nohup: ignoring input
Task: race-high, Checkpoint: None, Identifier: T5-large-ft-random
Output directory () already exists and is not empty.
03/12/2022 15:43:36 - INFO - __main__ - Namespace(task_dir='data/race-high/', task_name='race-high', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-race-high', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/12/2022 15:43:36 - INFO - __main__ - models/T5-large-ft-random/singletask-race-high
03/12/2022 15:43:36 - INFO - __main__ - Namespace(task_dir='data/race-high/', task_name='race-high', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-race-high', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/12/2022 15:43:36 - INFO - __main__ - models/T5-large-ft-random/singletask-race-high
03/12/2022 15:43:36 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/12/2022 15:43:36 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/12/2022 15:43:36 - INFO - __main__ - args.device: cuda:1
03/12/2022 15:43:36 - INFO - __main__ - args.device: cuda:0
03/12/2022 15:43:36 - INFO - __main__ - Using 2 gpus
03/12/2022 15:43:36 - INFO - __main__ - Using 2 gpus
03/12/2022 15:43:36 - INFO - __main__ - Fine-tuning the following samples: ['race-high_32_100', 'race-high_32_13', 'race-high_32_21', 'race-high_32_42', 'race-high_32_87']
03/12/2022 15:43:36 - INFO - __main__ - Fine-tuning the following samples: ['race-high_32_100', 'race-high_32_13', 'race-high_32_21', 'race-high_32_42', 'race-high_32_87']
03/12/2022 15:43:40 - INFO - __main__ - Running ... prefix=race-high_32_100, lr=0.0005, bsz=8 ...
03/12/2022 15:43:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 15:43:41 - INFO - __main__ - Printing 3 examples
03/12/2022 15:43:41 - INFO - __main__ -  [race-high] Which of the following leads to Jobs's downfall at Apple? (A) The Apple III. (B) The Macintosh (C) The NeXT. (D) The Lisa. [SEP] Steven Jobs was born on February 24,1955,in  San Francisco,California,and was adopted by Paul and Clara Jobs.In 1961 the family moved to Mountain View, California.At that time people started to refer to the area as "Silicon Valley". As a child, Jobs preferred doing things by himself, not interested in team sports or other group activities.He enrolled in the HewlettPackard Explorer Club. There he saw his first computer at the age of twelve. He was very impressed, and knew right away that he wanted to work with computers. At that time almost all computers were so large that one could fill a room, and so costly that individuals could not afford to buy them.Advances in electronics, however, meant that the parts of a computer were getting smaller and the power of the computer was increasing. By 1980 the personal computer era was well underway.Apple was continually forced to improve its products to remain ahead, as more competitors entered the marketplace. Apple introduced the Apple III, but the new model suffered technical and marketing problems.It was removed from the market. Early in 1983 Jobs unveiled the Lisa.It did not sell well,however,  because  it  was  more  expensive  than  personal computers sold by competitors.Apple's biggest competitor was International Business Machines  (IBM). By 1983 it was estimated that Apple had lost half of its market share ( part of an industry's sales that a specific company has) to IBM. In 1984 Apple introduced a revolutionary new model, the Macintosh.The Macintosh did not sell well to businesses,however.It lacked features other personal computers had, such as a corresponding high quality printer. The failure of the Macintosh signaled the beginning of Jobs's downfall at Apple. Late in 1988 the NeXT computer was introduced, aimed at the educational market.The product was very userfriendly,and had a fast processing speed, excellent graphics displays,and an outstanding sound system.Despite the warm reception,however, the NeXT machine never caught on. It was too costly, had a blackandwhite screen, and could not be linked to other computers or run common software.
03/12/2022 15:43:41 - INFO - __main__ - ['The Macintosh']
03/12/2022 15:43:41 - INFO - __main__ -  [race-high] All the statements are true except   _  . (A) More than 50% of women ages 18 to 64 have jobs. (B) 18% of working women took a vacation away from home last year. (C) 32% of working women have college education. (D) 80% of working women drive a car to work. [SEP] Many American women are earning money outside their homes today. Among women who are eighteen to sixty-four years old, more than fifty per cent have jobs.     In general, working women have had more education then those who stay at home. Of those who work, thirty-two per cent have attended college, compared with twenty per cent of those who do not have jobs.     Among women with jobs, eight out of ten drive a car to work, and eight per cent took a vacation a way from home during the past year. Much of their traveling was by air.     These figures come from a report which was written for advertisers. The report gives advertisers a new picture of women today. For instance, it tells advertisers that fifty-one per cent of all American women have traveled by air--along with fifty-nine per cent of all American men.     The lesson for American business is that many women now have other interests in addition to their homes. They like advertisements which show women in office, planes, and cars.
03/12/2022 15:43:41 - INFO - __main__ - ['18% of working women took a vacation away from home last year.']
03/12/2022 15:43:41 - INFO - __main__ -  [race-high] In the passage the author argues that   _  . (A) it is unfair to blame English teachers for language deficiencies of students (B) to improve the level of English requires the effort of several generations (C) English should not be the target of the blame of language deficiencies (D) to rid language deficiencies one should have sensitive eyes and ears [SEP] The speaker, a teacher from a community college, addressed a sympathetic audience. Heads nodded in agreement when he said, "High school English teachers are not doing their jobs." He described the weaknesses of his students, all high school graduates who can use language only at a grade 9 level. I was unable to determine from his answers to my questions how this grade 9 level had been established. What the speaker was really saying is that he is no longer young; he has been teaching for sixteen years, and is able to think and speak like a mature adult. My point is that the frequent complaint of one generation about the one immediately following it is unavoidable. It is also human nature to look for the reasons for our dissatisfaction. Before English became a school subject in the late nineteenth century, it was difficult to find the target of the blame for language deficiencies  . But since then, English teachers have been under constant attack. The complainers think they have hit upon an original idea. As their own command of the language improves, they notice that young people do not have this same ability. Unaware that their own ability has developed through the years, they suppose the new generation of young people must be hopeless in this respect. To the eyes and ears of sensitive adults, the language of the young always seems inadequate . Since this concern about the decline and fall of the English language is not recognized as a generational phenomenon but rather as something new and strange to today's young people, it naturally follows that today's English teachers cannot be doing their jobs. Otherwise, young people would not commit crimes against the language.
03/12/2022 15:43:41 - INFO - __main__ - ['it is unfair to blame English teachers for language deficiencies of students']
03/12/2022 15:43:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 15:43:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 15:43:41 - INFO - __main__ - Printing 3 examples
03/12/2022 15:43:41 - INFO - __main__ -  [race-high] Which of the following leads to Jobs's downfall at Apple? (A) The Apple III. (B) The Macintosh (C) The NeXT. (D) The Lisa. [SEP] Steven Jobs was born on February 24,1955,in  San Francisco,California,and was adopted by Paul and Clara Jobs.In 1961 the family moved to Mountain View, California.At that time people started to refer to the area as "Silicon Valley". As a child, Jobs preferred doing things by himself, not interested in team sports or other group activities.He enrolled in the HewlettPackard Explorer Club. There he saw his first computer at the age of twelve. He was very impressed, and knew right away that he wanted to work with computers. At that time almost all computers were so large that one could fill a room, and so costly that individuals could not afford to buy them.Advances in electronics, however, meant that the parts of a computer were getting smaller and the power of the computer was increasing. By 1980 the personal computer era was well underway.Apple was continually forced to improve its products to remain ahead, as more competitors entered the marketplace. Apple introduced the Apple III, but the new model suffered technical and marketing problems.It was removed from the market. Early in 1983 Jobs unveiled the Lisa.It did not sell well,however,  because  it  was  more  expensive  than  personal computers sold by competitors.Apple's biggest competitor was International Business Machines  (IBM). By 1983 it was estimated that Apple had lost half of its market share ( part of an industry's sales that a specific company has) to IBM. In 1984 Apple introduced a revolutionary new model, the Macintosh.The Macintosh did not sell well to businesses,however.It lacked features other personal computers had, such as a corresponding high quality printer. The failure of the Macintosh signaled the beginning of Jobs's downfall at Apple. Late in 1988 the NeXT computer was introduced, aimed at the educational market.The product was very userfriendly,and had a fast processing speed, excellent graphics displays,and an outstanding sound system.Despite the warm reception,however, the NeXT machine never caught on. It was too costly, had a blackandwhite screen, and could not be linked to other computers or run common software.
03/12/2022 15:43:41 - INFO - __main__ - ['The Macintosh']
03/12/2022 15:43:41 - INFO - __main__ -  [race-high] All the statements are true except   _  . (A) More than 50% of women ages 18 to 64 have jobs. (B) 18% of working women took a vacation away from home last year. (C) 32% of working women have college education. (D) 80% of working women drive a car to work. [SEP] Many American women are earning money outside their homes today. Among women who are eighteen to sixty-four years old, more than fifty per cent have jobs.     In general, working women have had more education then those who stay at home. Of those who work, thirty-two per cent have attended college, compared with twenty per cent of those who do not have jobs.     Among women with jobs, eight out of ten drive a car to work, and eight per cent took a vacation a way from home during the past year. Much of their traveling was by air.     These figures come from a report which was written for advertisers. The report gives advertisers a new picture of women today. For instance, it tells advertisers that fifty-one per cent of all American women have traveled by air--along with fifty-nine per cent of all American men.     The lesson for American business is that many women now have other interests in addition to their homes. They like advertisements which show women in office, planes, and cars.
03/12/2022 15:43:41 - INFO - __main__ - ['18% of working women took a vacation away from home last year.']
03/12/2022 15:43:41 - INFO - __main__ -  [race-high] In the passage the author argues that   _  . (A) it is unfair to blame English teachers for language deficiencies of students (B) to improve the level of English requires the effort of several generations (C) English should not be the target of the blame of language deficiencies (D) to rid language deficiencies one should have sensitive eyes and ears [SEP] The speaker, a teacher from a community college, addressed a sympathetic audience. Heads nodded in agreement when he said, "High school English teachers are not doing their jobs." He described the weaknesses of his students, all high school graduates who can use language only at a grade 9 level. I was unable to determine from his answers to my questions how this grade 9 level had been established. What the speaker was really saying is that he is no longer young; he has been teaching for sixteen years, and is able to think and speak like a mature adult. My point is that the frequent complaint of one generation about the one immediately following it is unavoidable. It is also human nature to look for the reasons for our dissatisfaction. Before English became a school subject in the late nineteenth century, it was difficult to find the target of the blame for language deficiencies  . But since then, English teachers have been under constant attack. The complainers think they have hit upon an original idea. As their own command of the language improves, they notice that young people do not have this same ability. Unaware that their own ability has developed through the years, they suppose the new generation of young people must be hopeless in this respect. To the eyes and ears of sensitive adults, the language of the young always seems inadequate . Since this concern about the decline and fall of the English language is not recognized as a generational phenomenon but rather as something new and strange to today's young people, it naturally follows that today's English teachers cannot be doing their jobs. Otherwise, young people would not commit crimes against the language.
03/12/2022 15:43:41 - INFO - __main__ - ['it is unfair to blame English teachers for language deficiencies of students']
03/12/2022 15:43:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 15:43:41 - INFO - __main__ - Tokenizing Output ...
03/12/2022 15:43:41 - INFO - __main__ - Tokenizing Output ...
03/12/2022 15:43:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 15:43:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 15:43:41 - INFO - __main__ - Printing 3 examples
03/12/2022 15:43:41 - INFO - __main__ -  [race-high] How many artists' paintings are on show at the special exhibition? (A) Three. (B) Five. (C) Seven. (D) Eight. [SEP] New picture The National Gallery now has a fine _ by the 18th century Dutch painter Jan van Os.This large picture (89.1 cm x 71cm) of flowers and fruit is painted in light bright colours on wood.It is one of the first pictures of this type in the Gallery.The picture is signed and dated 1777 and 1778.It is not unusual for a picture to be dated two years: the artist waited for particular flowers to come out in their different seasons in order to paint them.This picture was given to the Gallery by Miss Violet Churchman in memory of her sister Ida Nancy. It is now on show in Room 25. Special exhibition The exhibition "Painting in Spain During the Late 18th Century" opened in the Sunley Room on 15 March.Recently the Gallery has bought works by three Spanish painters of this period--Paret, Melendez and Francisco Bayeu, who are the focus of the exhibition.These three artists are also joined by Francisco's brother Ramon, by Antonio Gonzalez and two Italians who worked in Spain during these years--Corrado Giaquinto and Giovanni Battista Tiepolo.The exhibition runs until 31 May. Lecture news    Lectures will be given along with the special exhibition every Wednesday.On 8 April, Lizzie Barker will discuss the work of Melendez, while on 15 April, Sarah Symmons will lecture on Luis Patter.On 22 and 29 April, Juliet Wilson will talk about Francisco Bayeu.On 25 April, a Saturday, Erika Langmuir will explain how artists often "tell a story" through their pictures.
03/12/2022 15:43:41 - INFO - __main__ - ['Seven.']
03/12/2022 15:43:41 - INFO - __main__ -  [race-high] We can learn from the passage that   _  . (A) the problem of student suicides is getting worse according to a research on the accurate statistics (B) teachers have enough ability to sense the emotional distress of students (C) parents place neither pressure nor care on their children (D) both teachers and parents should learn more to deal with the problem of student suicides. [SEP] SHANGHAI, June 7(AP)--A 16-year-old girl's suicide after she was barred from a key exam draw attention to increasing worries over academic pressures, as millions of Chinese students began annual college entrance tests on Wednesday.  The three-day exam, viewed as important to future career and financial success, has a record 9.5 million high school students across prefix = st1 /Chinacompeting for just 2.6 million university places. For kids and parents alike, it's a difficulty that experts say causes extreme emotional distress. "Pressure from study and exams is a top reason for psychological problems among Chinese youth," said Jin Wuguan, director of the Youth Psychological Counseling Center at Shanghai'sRuijinHospital.  In China's increasingly success oriented, pressure-cooker cities, academic stress is seen as a rising cause of youth suicides and even murders of parents by children who are driven crazy by intolerable pressure to perform.  According to her family and newspaper accounts, 16-year-old Wu Wenwen drowned herself after she was stopped at the exam room door because her hair wasn't tied back as her school required. Returning in tied hair, she was then told the end-of-term exam had already started and she was too late to take it. In tears, Wu called her mother, and then disappeared. Her body was found the same night in a nearby lake.  China doesn't keep comprehensive statistics on student suicides, but Jin said health care professionals see the problem worsening, even among elementary students. Most Chinese schools still lack advisers and teachers receive little training in spotting symptoms of emotional distress, Jin said. Parents are little help, often piling on pressure while ignoring their children's emotional development, he said. "It's a basic unwillingness or inability to recognize and deal with with emotional problems," Jin said.  Wang Yufeng, of Peking University's Institute of Mental, estimates the rate of emotional disorders such as depression among Chinese students under age 17 at up to 32 percent , a total of 30 million students.  Others say that figure may be as high as 50 percent. A survey last year by the government's China Youth and ChildrenResearchCentershowed 57.6 percent of students felt highly distressed by academic pressures.
03/12/2022 15:43:41 - INFO - __main__ - ['both teachers and parents should learn more to deal with the problem of student suicides.']
03/12/2022 15:43:41 - INFO - __main__ -  [race-high] What is the aim of the Predator Compensation Program? (A) To protect people in the wild. (B) To protect Masai's farms. (C) To protect lions only. (D) To protect the wildlife. [SEP] At the beginning of the 20th century there were more than a million lions worldwide. Today there are less than 30,000 in the wild. The remaining lions are increasingly threatened by habitat loss, hunting and activities to protect farms and cattle. For generations, Masai tribesmen on the large African plains in southeastern Kenya have hunted lions -- to protect their farms and cattle. Today they celebrate the lions' life. Noah is an elder in the Masai community. "We have decided as a community of the Masai to lay down our spears, and there will be no more killing of lions in our community." He is part of a group of Masai visiting the United States promoting   the Predator Compensation Program. Conservation International's Frank Hawkins explains, "The Masai have been living with wildlife for many generations and it has been a conflicting ( ) relationship in many ways. They compete with the animals for food as lions eat their cattle. We're trying to find ways in which the wildlife will become something useful to them." They had the Predator Compensation Fund founded in 2003. After much discussion, a group of Masai farmers agreed to protect lions. In turn, if lions or other predators kill their cattle, the Masai owner will be paid market value for the dead animals from the fund. One man said that in the past, when a lion killed cattle, they killed it immediately. And now, after the start of the program, the Masai see the lion population growing. Since 2003, only four lions have been killed here.
03/12/2022 15:43:41 - INFO - __main__ - ['To protect the wildlife.']
03/12/2022 15:43:41 - INFO - __main__ - Tokenizing Input ...
03/12/2022 15:43:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 15:43:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 15:43:41 - INFO - __main__ - Printing 3 examples
03/12/2022 15:43:41 - INFO - __main__ -  [race-high] How many artists' paintings are on show at the special exhibition? (A) Three. (B) Five. (C) Seven. (D) Eight. [SEP] New picture The National Gallery now has a fine _ by the 18th century Dutch painter Jan van Os.This large picture (89.1 cm x 71cm) of flowers and fruit is painted in light bright colours on wood.It is one of the first pictures of this type in the Gallery.The picture is signed and dated 1777 and 1778.It is not unusual for a picture to be dated two years: the artist waited for particular flowers to come out in their different seasons in order to paint them.This picture was given to the Gallery by Miss Violet Churchman in memory of her sister Ida Nancy. It is now on show in Room 25. Special exhibition The exhibition "Painting in Spain During the Late 18th Century" opened in the Sunley Room on 15 March.Recently the Gallery has bought works by three Spanish painters of this period--Paret, Melendez and Francisco Bayeu, who are the focus of the exhibition.These three artists are also joined by Francisco's brother Ramon, by Antonio Gonzalez and two Italians who worked in Spain during these years--Corrado Giaquinto and Giovanni Battista Tiepolo.The exhibition runs until 31 May. Lecture news    Lectures will be given along with the special exhibition every Wednesday.On 8 April, Lizzie Barker will discuss the work of Melendez, while on 15 April, Sarah Symmons will lecture on Luis Patter.On 22 and 29 April, Juliet Wilson will talk about Francisco Bayeu.On 25 April, a Saturday, Erika Langmuir will explain how artists often "tell a story" through their pictures.
03/12/2022 15:43:41 - INFO - __main__ - ['Seven.']
03/12/2022 15:43:41 - INFO - __main__ -  [race-high] We can learn from the passage that   _  . (A) the problem of student suicides is getting worse according to a research on the accurate statistics (B) teachers have enough ability to sense the emotional distress of students (C) parents place neither pressure nor care on their children (D) both teachers and parents should learn more to deal with the problem of student suicides. [SEP] SHANGHAI, June 7(AP)--A 16-year-old girl's suicide after she was barred from a key exam draw attention to increasing worries over academic pressures, as millions of Chinese students began annual college entrance tests on Wednesday.  The three-day exam, viewed as important to future career and financial success, has a record 9.5 million high school students across prefix = st1 /Chinacompeting for just 2.6 million university places. For kids and parents alike, it's a difficulty that experts say causes extreme emotional distress. "Pressure from study and exams is a top reason for psychological problems among Chinese youth," said Jin Wuguan, director of the Youth Psychological Counseling Center at Shanghai'sRuijinHospital.  In China's increasingly success oriented, pressure-cooker cities, academic stress is seen as a rising cause of youth suicides and even murders of parents by children who are driven crazy by intolerable pressure to perform.  According to her family and newspaper accounts, 16-year-old Wu Wenwen drowned herself after she was stopped at the exam room door because her hair wasn't tied back as her school required. Returning in tied hair, she was then told the end-of-term exam had already started and she was too late to take it. In tears, Wu called her mother, and then disappeared. Her body was found the same night in a nearby lake.  China doesn't keep comprehensive statistics on student suicides, but Jin said health care professionals see the problem worsening, even among elementary students. Most Chinese schools still lack advisers and teachers receive little training in spotting symptoms of emotional distress, Jin said. Parents are little help, often piling on pressure while ignoring their children's emotional development, he said. "It's a basic unwillingness or inability to recognize and deal with with emotional problems," Jin said.  Wang Yufeng, of Peking University's Institute of Mental, estimates the rate of emotional disorders such as depression among Chinese students under age 17 at up to 32 percent , a total of 30 million students.  Others say that figure may be as high as 50 percent. A survey last year by the government's China Youth and ChildrenResearchCentershowed 57.6 percent of students felt highly distressed by academic pressures.
03/12/2022 15:43:41 - INFO - __main__ - ['both teachers and parents should learn more to deal with the problem of student suicides.']
03/12/2022 15:43:41 - INFO - __main__ -  [race-high] What is the aim of the Predator Compensation Program? (A) To protect people in the wild. (B) To protect Masai's farms. (C) To protect lions only. (D) To protect the wildlife. [SEP] At the beginning of the 20th century there were more than a million lions worldwide. Today there are less than 30,000 in the wild. The remaining lions are increasingly threatened by habitat loss, hunting and activities to protect farms and cattle. For generations, Masai tribesmen on the large African plains in southeastern Kenya have hunted lions -- to protect their farms and cattle. Today they celebrate the lions' life. Noah is an elder in the Masai community. "We have decided as a community of the Masai to lay down our spears, and there will be no more killing of lions in our community." He is part of a group of Masai visiting the United States promoting   the Predator Compensation Program. Conservation International's Frank Hawkins explains, "The Masai have been living with wildlife for many generations and it has been a conflicting ( ) relationship in many ways. They compete with the animals for food as lions eat their cattle. We're trying to find ways in which the wildlife will become something useful to them." They had the Predator Compensation Fund founded in 2003. After much discussion, a group of Masai farmers agreed to protect lions. In turn, if lions or other predators kill their cattle, the Masai owner will be paid market value for the dead animals from the fund. One man said that in the past, when a lion killed cattle, they killed it immediately. And now, after the start of the program, the Masai see the lion population growing. Since 2003, only four lions have been killed here.
03/12/2022 15:43:41 - INFO - __main__ - ['To protect the wildlife.']
03/12/2022 15:43:41 - INFO - __main__ - Tokenizing Input ...
03/12/2022 15:43:41 - INFO - __main__ - Tokenizing Output ...
03/12/2022 15:43:41 - INFO - __main__ - Tokenizing Output ...
03/12/2022 15:43:42 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 15:43:42 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 15:43:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 15:43:54 - INFO - __main__ - Starting training!
03/12/2022 15:43:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 15:43:55 - INFO - __main__ - Starting training!
03/12/2022 15:44:01 - INFO - __main__ - Step 10 Global step 10 Train loss 20.916855 on epoch=4
03/12/2022 15:44:06 - INFO - __main__ - Step 20 Global step 20 Train loss 18.765011 on epoch=9
03/12/2022 15:44:12 - INFO - __main__ - Step 30 Global step 30 Train loss 9.384947 on epoch=14
03/12/2022 15:44:19 - INFO - __main__ - Step 40 Global step 40 Train loss 6.932963 on epoch=19
03/12/2022 15:44:25 - INFO - __main__ - Step 50 Global step 50 Train loss 5.798910 on epoch=24
03/12/2022 15:44:38 - INFO - __main__ - Global step 50 Train loss 12.359735 ACC 0.0 on epoch=24
03/12/2022 15:44:46 - INFO - __main__ - Step 60 Global step 60 Train loss 4.433808 on epoch=29
03/12/2022 15:44:52 - INFO - __main__ - Step 70 Global step 70 Train loss 3.890112 on epoch=34
03/12/2022 15:44:58 - INFO - __main__ - Step 80 Global step 80 Train loss 4.742846 on epoch=39
03/12/2022 15:45:04 - INFO - __main__ - Step 90 Global step 90 Train loss 3.876919 on epoch=44
03/12/2022 15:45:10 - INFO - __main__ - Step 100 Global step 100 Train loss 2.283610 on epoch=49
03/12/2022 15:45:12 - INFO - __main__ - Global step 100 Train loss 3.845459 ACC 0.0 on epoch=49
03/12/2022 15:45:18 - INFO - __main__ - Step 110 Global step 110 Train loss 1.997476 on epoch=54
03/12/2022 15:45:24 - INFO - __main__ - Step 120 Global step 120 Train loss 1.568969 on epoch=59
03/12/2022 15:45:30 - INFO - __main__ - Step 130 Global step 130 Train loss 1.414568 on epoch=64
03/12/2022 15:45:36 - INFO - __main__ - Step 140 Global step 140 Train loss 1.199229 on epoch=69
03/12/2022 15:45:42 - INFO - __main__ - Step 150 Global step 150 Train loss 1.264840 on epoch=74
03/12/2022 15:45:44 - INFO - __main__ - Global step 150 Train loss 1.489016 ACC 0.0 on epoch=74
03/12/2022 15:45:50 - INFO - __main__ - Step 160 Global step 160 Train loss 1.143225 on epoch=79
03/12/2022 15:45:56 - INFO - __main__ - Step 170 Global step 170 Train loss 1.342092 on epoch=84
03/12/2022 15:46:02 - INFO - __main__ - Step 180 Global step 180 Train loss 1.134114 on epoch=89
03/12/2022 15:46:08 - INFO - __main__ - Step 190 Global step 190 Train loss 1.077758 on epoch=94
03/12/2022 15:46:14 - INFO - __main__ - Step 200 Global step 200 Train loss 1.058709 on epoch=99
03/12/2022 15:46:16 - INFO - __main__ - Global step 200 Train loss 1.151180 ACC 0.0 on epoch=99
03/12/2022 15:46:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.982944 on epoch=104
03/12/2022 15:46:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.987330 on epoch=109
03/12/2022 15:46:34 - INFO - __main__ - Step 230 Global step 230 Train loss 1.137064 on epoch=114
03/12/2022 15:46:40 - INFO - __main__ - Step 240 Global step 240 Train loss 1.030570 on epoch=119
03/12/2022 15:46:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.870020 on epoch=124
03/12/2022 15:46:48 - INFO - __main__ - Global step 250 Train loss 1.001586 ACC 0.0 on epoch=124
03/12/2022 15:46:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.815010 on epoch=129
03/12/2022 15:47:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.829117 on epoch=134
03/12/2022 15:47:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.738649 on epoch=139
03/12/2022 15:47:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.807807 on epoch=144
03/12/2022 15:47:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.680076 on epoch=149
03/12/2022 15:47:20 - INFO - __main__ - Global step 300 Train loss 0.774132 ACC 0.0 on epoch=149
03/12/2022 15:47:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.716473 on epoch=154
03/12/2022 15:47:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.668391 on epoch=159
03/12/2022 15:47:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.662809 on epoch=164
03/12/2022 15:47:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.626747 on epoch=169
03/12/2022 15:47:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.603188 on epoch=174
03/12/2022 15:47:52 - INFO - __main__ - Global step 350 Train loss 0.655522 ACC 0.0 on epoch=174
03/12/2022 15:47:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.653723 on epoch=179
03/12/2022 15:48:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.581274 on epoch=184
03/12/2022 15:48:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.578159 on epoch=189
03/12/2022 15:48:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.558665 on epoch=194
03/12/2022 15:48:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.664850 on epoch=199
03/12/2022 15:48:24 - INFO - __main__ - Global step 400 Train loss 0.607334 ACC 0.0 on epoch=199
03/12/2022 15:48:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.526090 on epoch=204
03/12/2022 15:48:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.538572 on epoch=209
03/12/2022 15:48:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.495541 on epoch=214
03/12/2022 15:48:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.583258 on epoch=219
03/12/2022 15:48:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.462041 on epoch=224
03/12/2022 15:48:56 - INFO - __main__ - Global step 450 Train loss 0.521101 ACC 0.0 on epoch=224
03/12/2022 15:49:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.525155 on epoch=229
03/12/2022 15:49:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.483062 on epoch=234
03/12/2022 15:49:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.518443 on epoch=239
03/12/2022 15:49:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.479399 on epoch=244
03/12/2022 15:49:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.490014 on epoch=249
03/12/2022 15:49:27 - INFO - __main__ - Global step 500 Train loss 0.499215 ACC 0.0 on epoch=249
03/12/2022 15:49:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.474743 on epoch=254
03/12/2022 15:49:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.465318 on epoch=259
03/12/2022 15:49:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.435149 on epoch=264
03/12/2022 15:49:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.476795 on epoch=269
03/12/2022 15:49:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.444435 on epoch=274
03/12/2022 15:49:59 - INFO - __main__ - Global step 550 Train loss 0.459288 ACC 0.0 on epoch=274
03/12/2022 15:50:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.447827 on epoch=279
03/12/2022 15:50:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.422056 on epoch=284
03/12/2022 15:50:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.451079 on epoch=289
03/12/2022 15:50:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.418045 on epoch=294
03/12/2022 15:50:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.442327 on epoch=299
03/12/2022 15:50:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 15:50:31 - INFO - __main__ - Printing 3 examples
03/12/2022 15:50:31 - INFO - __main__ -  [race-high] Which of the following leads to Jobs's downfall at Apple? (A) The Apple III. (B) The Macintosh (C) The NeXT. (D) The Lisa. [SEP] Steven Jobs was born on February 24,1955,in  San Francisco,California,and was adopted by Paul and Clara Jobs.In 1961 the family moved to Mountain View, California.At that time people started to refer to the area as "Silicon Valley". As a child, Jobs preferred doing things by himself, not interested in team sports or other group activities.He enrolled in the HewlettPackard Explorer Club. There he saw his first computer at the age of twelve. He was very impressed, and knew right away that he wanted to work with computers. At that time almost all computers were so large that one could fill a room, and so costly that individuals could not afford to buy them.Advances in electronics, however, meant that the parts of a computer were getting smaller and the power of the computer was increasing. By 1980 the personal computer era was well underway.Apple was continually forced to improve its products to remain ahead, as more competitors entered the marketplace. Apple introduced the Apple III, but the new model suffered technical and marketing problems.It was removed from the market. Early in 1983 Jobs unveiled the Lisa.It did not sell well,however,  because  it  was  more  expensive  than  personal computers sold by competitors.Apple's biggest competitor was International Business Machines  (IBM). By 1983 it was estimated that Apple had lost half of its market share ( part of an industry's sales that a specific company has) to IBM. In 1984 Apple introduced a revolutionary new model, the Macintosh.The Macintosh did not sell well to businesses,however.It lacked features other personal computers had, such as a corresponding high quality printer. The failure of the Macintosh signaled the beginning of Jobs's downfall at Apple. Late in 1988 the NeXT computer was introduced, aimed at the educational market.The product was very userfriendly,and had a fast processing speed, excellent graphics displays,and an outstanding sound system.Despite the warm reception,however, the NeXT machine never caught on. It was too costly, had a blackandwhite screen, and could not be linked to other computers or run common software.
03/12/2022 15:50:31 - INFO - __main__ - ['The Macintosh']
03/12/2022 15:50:31 - INFO - __main__ -  [race-high] All the statements are true except   _  . (A) More than 50% of women ages 18 to 64 have jobs. (B) 18% of working women took a vacation away from home last year. (C) 32% of working women have college education. (D) 80% of working women drive a car to work. [SEP] Many American women are earning money outside their homes today. Among women who are eighteen to sixty-four years old, more than fifty per cent have jobs.     In general, working women have had more education then those who stay at home. Of those who work, thirty-two per cent have attended college, compared with twenty per cent of those who do not have jobs.     Among women with jobs, eight out of ten drive a car to work, and eight per cent took a vacation a way from home during the past year. Much of their traveling was by air.     These figures come from a report which was written for advertisers. The report gives advertisers a new picture of women today. For instance, it tells advertisers that fifty-one per cent of all American women have traveled by air--along with fifty-nine per cent of all American men.     The lesson for American business is that many women now have other interests in addition to their homes. They like advertisements which show women in office, planes, and cars.
03/12/2022 15:50:31 - INFO - __main__ - ['18% of working women took a vacation away from home last year.']
03/12/2022 15:50:31 - INFO - __main__ -  [race-high] In the passage the author argues that   _  . (A) it is unfair to blame English teachers for language deficiencies of students (B) to improve the level of English requires the effort of several generations (C) English should not be the target of the blame of language deficiencies (D) to rid language deficiencies one should have sensitive eyes and ears [SEP] The speaker, a teacher from a community college, addressed a sympathetic audience. Heads nodded in agreement when he said, "High school English teachers are not doing their jobs." He described the weaknesses of his students, all high school graduates who can use language only at a grade 9 level. I was unable to determine from his answers to my questions how this grade 9 level had been established. What the speaker was really saying is that he is no longer young; he has been teaching for sixteen years, and is able to think and speak like a mature adult. My point is that the frequent complaint of one generation about the one immediately following it is unavoidable. It is also human nature to look for the reasons for our dissatisfaction. Before English became a school subject in the late nineteenth century, it was difficult to find the target of the blame for language deficiencies  . But since then, English teachers have been under constant attack. The complainers think they have hit upon an original idea. As their own command of the language improves, they notice that young people do not have this same ability. Unaware that their own ability has developed through the years, they suppose the new generation of young people must be hopeless in this respect. To the eyes and ears of sensitive adults, the language of the young always seems inadequate . Since this concern about the decline and fall of the English language is not recognized as a generational phenomenon but rather as something new and strange to today's young people, it naturally follows that today's English teachers cannot be doing their jobs. Otherwise, young people would not commit crimes against the language.
03/12/2022 15:50:31 - INFO - __main__ - ['it is unfair to blame English teachers for language deficiencies of students']
03/12/2022 15:50:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 15:50:31 - INFO - __main__ - Tokenizing Output ...
03/12/2022 15:50:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 15:50:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 15:50:31 - INFO - __main__ - Printing 3 examples
03/12/2022 15:50:31 - INFO - __main__ -  [race-high] How many artists' paintings are on show at the special exhibition? (A) Three. (B) Five. (C) Seven. (D) Eight. [SEP] New picture The National Gallery now has a fine _ by the 18th century Dutch painter Jan van Os.This large picture (89.1 cm x 71cm) of flowers and fruit is painted in light bright colours on wood.It is one of the first pictures of this type in the Gallery.The picture is signed and dated 1777 and 1778.It is not unusual for a picture to be dated two years: the artist waited for particular flowers to come out in their different seasons in order to paint them.This picture was given to the Gallery by Miss Violet Churchman in memory of her sister Ida Nancy. It is now on show in Room 25. Special exhibition The exhibition "Painting in Spain During the Late 18th Century" opened in the Sunley Room on 15 March.Recently the Gallery has bought works by three Spanish painters of this period--Paret, Melendez and Francisco Bayeu, who are the focus of the exhibition.These three artists are also joined by Francisco's brother Ramon, by Antonio Gonzalez and two Italians who worked in Spain during these years--Corrado Giaquinto and Giovanni Battista Tiepolo.The exhibition runs until 31 May. Lecture news    Lectures will be given along with the special exhibition every Wednesday.On 8 April, Lizzie Barker will discuss the work of Melendez, while on 15 April, Sarah Symmons will lecture on Luis Patter.On 22 and 29 April, Juliet Wilson will talk about Francisco Bayeu.On 25 April, a Saturday, Erika Langmuir will explain how artists often "tell a story" through their pictures.
03/12/2022 15:50:31 - INFO - __main__ - ['Seven.']
03/12/2022 15:50:31 - INFO - __main__ -  [race-high] We can learn from the passage that   _  . (A) the problem of student suicides is getting worse according to a research on the accurate statistics (B) teachers have enough ability to sense the emotional distress of students (C) parents place neither pressure nor care on their children (D) both teachers and parents should learn more to deal with the problem of student suicides. [SEP] SHANGHAI, June 7(AP)--A 16-year-old girl's suicide after she was barred from a key exam draw attention to increasing worries over academic pressures, as millions of Chinese students began annual college entrance tests on Wednesday.  The three-day exam, viewed as important to future career and financial success, has a record 9.5 million high school students across prefix = st1 /Chinacompeting for just 2.6 million university places. For kids and parents alike, it's a difficulty that experts say causes extreme emotional distress. "Pressure from study and exams is a top reason for psychological problems among Chinese youth," said Jin Wuguan, director of the Youth Psychological Counseling Center at Shanghai'sRuijinHospital.  In China's increasingly success oriented, pressure-cooker cities, academic stress is seen as a rising cause of youth suicides and even murders of parents by children who are driven crazy by intolerable pressure to perform.  According to her family and newspaper accounts, 16-year-old Wu Wenwen drowned herself after she was stopped at the exam room door because her hair wasn't tied back as her school required. Returning in tied hair, she was then told the end-of-term exam had already started and she was too late to take it. In tears, Wu called her mother, and then disappeared. Her body was found the same night in a nearby lake.  China doesn't keep comprehensive statistics on student suicides, but Jin said health care professionals see the problem worsening, even among elementary students. Most Chinese schools still lack advisers and teachers receive little training in spotting symptoms of emotional distress, Jin said. Parents are little help, often piling on pressure while ignoring their children's emotional development, he said. "It's a basic unwillingness or inability to recognize and deal with with emotional problems," Jin said.  Wang Yufeng, of Peking University's Institute of Mental, estimates the rate of emotional disorders such as depression among Chinese students under age 17 at up to 32 percent , a total of 30 million students.  Others say that figure may be as high as 50 percent. A survey last year by the government's China Youth and ChildrenResearchCentershowed 57.6 percent of students felt highly distressed by academic pressures.
03/12/2022 15:50:31 - INFO - __main__ - ['both teachers and parents should learn more to deal with the problem of student suicides.']
03/12/2022 15:50:31 - INFO - __main__ -  [race-high] What is the aim of the Predator Compensation Program? (A) To protect people in the wild. (B) To protect Masai's farms. (C) To protect lions only. (D) To protect the wildlife. [SEP] At the beginning of the 20th century there were more than a million lions worldwide. Today there are less than 30,000 in the wild. The remaining lions are increasingly threatened by habitat loss, hunting and activities to protect farms and cattle. For generations, Masai tribesmen on the large African plains in southeastern Kenya have hunted lions -- to protect their farms and cattle. Today they celebrate the lions' life. Noah is an elder in the Masai community. "We have decided as a community of the Masai to lay down our spears, and there will be no more killing of lions in our community." He is part of a group of Masai visiting the United States promoting   the Predator Compensation Program. Conservation International's Frank Hawkins explains, "The Masai have been living with wildlife for many generations and it has been a conflicting ( ) relationship in many ways. They compete with the animals for food as lions eat their cattle. We're trying to find ways in which the wildlife will become something useful to them." They had the Predator Compensation Fund founded in 2003. After much discussion, a group of Masai farmers agreed to protect lions. In turn, if lions or other predators kill their cattle, the Masai owner will be paid market value for the dead animals from the fund. One man said that in the past, when a lion killed cattle, they killed it immediately. And now, after the start of the program, the Masai see the lion population growing. Since 2003, only four lions have been killed here.
03/12/2022 15:50:31 - INFO - __main__ - ['To protect the wildlife.']
03/12/2022 15:50:31 - INFO - __main__ - Tokenizing Input ...
03/12/2022 15:50:31 - INFO - __main__ - Tokenizing Output ...
03/12/2022 15:50:31 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 15:50:31 - INFO - __main__ - Global step 600 Train loss 0.436267 ACC 0.0 on epoch=299
03/12/2022 15:50:31 - INFO - __main__ - save last model!
03/12/2022 15:50:41 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 15:50:42 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 15:50:42 - INFO - __main__ - Printing 3 examples
03/12/2022 15:50:42 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 15:50:42 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 15:50:42 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 15:50:42 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 15:50:42 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 15:50:42 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 15:50:42 - INFO - __main__ - Tokenizing Input ...
03/12/2022 15:50:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 15:50:44 - INFO - __main__ - Starting training!
03/12/2022 15:50:47 - INFO - __main__ - Tokenizing Output ...
03/12/2022 15:50:51 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 16:12:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_100_0.0005_8_predictions.txt
03/12/2022 16:12:23 - INFO - __main__ - ACC on test data: 0.0014
03/12/2022 16:12:23 - INFO - __main__ - prefix=race-high_32_100, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.0014488554042306578
03/12/2022 16:12:23 - INFO - __main__ - Running ... prefix=race-high_32_100, lr=0.0003, bsz=8 ...
03/12/2022 16:12:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 16:12:24 - INFO - __main__ - Printing 3 examples
03/12/2022 16:12:24 - INFO - __main__ -  [race-high] Which of the following leads to Jobs's downfall at Apple? (A) The Apple III. (B) The Macintosh (C) The NeXT. (D) The Lisa. [SEP] Steven Jobs was born on February 24,1955,in  San Francisco,California,and was adopted by Paul and Clara Jobs.In 1961 the family moved to Mountain View, California.At that time people started to refer to the area as "Silicon Valley". As a child, Jobs preferred doing things by himself, not interested in team sports or other group activities.He enrolled in the HewlettPackard Explorer Club. There he saw his first computer at the age of twelve. He was very impressed, and knew right away that he wanted to work with computers. At that time almost all computers were so large that one could fill a room, and so costly that individuals could not afford to buy them.Advances in electronics, however, meant that the parts of a computer were getting smaller and the power of the computer was increasing. By 1980 the personal computer era was well underway.Apple was continually forced to improve its products to remain ahead, as more competitors entered the marketplace. Apple introduced the Apple III, but the new model suffered technical and marketing problems.It was removed from the market. Early in 1983 Jobs unveiled the Lisa.It did not sell well,however,  because  it  was  more  expensive  than  personal computers sold by competitors.Apple's biggest competitor was International Business Machines  (IBM). By 1983 it was estimated that Apple had lost half of its market share ( part of an industry's sales that a specific company has) to IBM. In 1984 Apple introduced a revolutionary new model, the Macintosh.The Macintosh did not sell well to businesses,however.It lacked features other personal computers had, such as a corresponding high quality printer. The failure of the Macintosh signaled the beginning of Jobs's downfall at Apple. Late in 1988 the NeXT computer was introduced, aimed at the educational market.The product was very userfriendly,and had a fast processing speed, excellent graphics displays,and an outstanding sound system.Despite the warm reception,however, the NeXT machine never caught on. It was too costly, had a blackandwhite screen, and could not be linked to other computers or run common software.
03/12/2022 16:12:24 - INFO - __main__ - ['The Macintosh']
03/12/2022 16:12:24 - INFO - __main__ -  [race-high] All the statements are true except   _  . (A) More than 50% of women ages 18 to 64 have jobs. (B) 18% of working women took a vacation away from home last year. (C) 32% of working women have college education. (D) 80% of working women drive a car to work. [SEP] Many American women are earning money outside their homes today. Among women who are eighteen to sixty-four years old, more than fifty per cent have jobs.     In general, working women have had more education then those who stay at home. Of those who work, thirty-two per cent have attended college, compared with twenty per cent of those who do not have jobs.     Among women with jobs, eight out of ten drive a car to work, and eight per cent took a vacation a way from home during the past year. Much of their traveling was by air.     These figures come from a report which was written for advertisers. The report gives advertisers a new picture of women today. For instance, it tells advertisers that fifty-one per cent of all American women have traveled by air--along with fifty-nine per cent of all American men.     The lesson for American business is that many women now have other interests in addition to their homes. They like advertisements which show women in office, planes, and cars.
03/12/2022 16:12:24 - INFO - __main__ - ['18% of working women took a vacation away from home last year.']
03/12/2022 16:12:24 - INFO - __main__ -  [race-high] In the passage the author argues that   _  . (A) it is unfair to blame English teachers for language deficiencies of students (B) to improve the level of English requires the effort of several generations (C) English should not be the target of the blame of language deficiencies (D) to rid language deficiencies one should have sensitive eyes and ears [SEP] The speaker, a teacher from a community college, addressed a sympathetic audience. Heads nodded in agreement when he said, "High school English teachers are not doing their jobs." He described the weaknesses of his students, all high school graduates who can use language only at a grade 9 level. I was unable to determine from his answers to my questions how this grade 9 level had been established. What the speaker was really saying is that he is no longer young; he has been teaching for sixteen years, and is able to think and speak like a mature adult. My point is that the frequent complaint of one generation about the one immediately following it is unavoidable. It is also human nature to look for the reasons for our dissatisfaction. Before English became a school subject in the late nineteenth century, it was difficult to find the target of the blame for language deficiencies  . But since then, English teachers have been under constant attack. The complainers think they have hit upon an original idea. As their own command of the language improves, they notice that young people do not have this same ability. Unaware that their own ability has developed through the years, they suppose the new generation of young people must be hopeless in this respect. To the eyes and ears of sensitive adults, the language of the young always seems inadequate . Since this concern about the decline and fall of the English language is not recognized as a generational phenomenon but rather as something new and strange to today's young people, it naturally follows that today's English teachers cannot be doing their jobs. Otherwise, young people would not commit crimes against the language.
03/12/2022 16:12:24 - INFO - __main__ - ['it is unfair to blame English teachers for language deficiencies of students']
03/12/2022 16:12:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 16:12:24 - INFO - __main__ - Tokenizing Output ...
03/12/2022 16:12:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 16:12:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 16:12:24 - INFO - __main__ - Printing 3 examples
03/12/2022 16:12:24 - INFO - __main__ -  [race-high] How many artists' paintings are on show at the special exhibition? (A) Three. (B) Five. (C) Seven. (D) Eight. [SEP] New picture The National Gallery now has a fine _ by the 18th century Dutch painter Jan van Os.This large picture (89.1 cm x 71cm) of flowers and fruit is painted in light bright colours on wood.It is one of the first pictures of this type in the Gallery.The picture is signed and dated 1777 and 1778.It is not unusual for a picture to be dated two years: the artist waited for particular flowers to come out in their different seasons in order to paint them.This picture was given to the Gallery by Miss Violet Churchman in memory of her sister Ida Nancy. It is now on show in Room 25. Special exhibition The exhibition "Painting in Spain During the Late 18th Century" opened in the Sunley Room on 15 March.Recently the Gallery has bought works by three Spanish painters of this period--Paret, Melendez and Francisco Bayeu, who are the focus of the exhibition.These three artists are also joined by Francisco's brother Ramon, by Antonio Gonzalez and two Italians who worked in Spain during these years--Corrado Giaquinto and Giovanni Battista Tiepolo.The exhibition runs until 31 May. Lecture news    Lectures will be given along with the special exhibition every Wednesday.On 8 April, Lizzie Barker will discuss the work of Melendez, while on 15 April, Sarah Symmons will lecture on Luis Patter.On 22 and 29 April, Juliet Wilson will talk about Francisco Bayeu.On 25 April, a Saturday, Erika Langmuir will explain how artists often "tell a story" through their pictures.
03/12/2022 16:12:24 - INFO - __main__ - ['Seven.']
03/12/2022 16:12:24 - INFO - __main__ -  [race-high] We can learn from the passage that   _  . (A) the problem of student suicides is getting worse according to a research on the accurate statistics (B) teachers have enough ability to sense the emotional distress of students (C) parents place neither pressure nor care on their children (D) both teachers and parents should learn more to deal with the problem of student suicides. [SEP] SHANGHAI, June 7(AP)--A 16-year-old girl's suicide after she was barred from a key exam draw attention to increasing worries over academic pressures, as millions of Chinese students began annual college entrance tests on Wednesday.  The three-day exam, viewed as important to future career and financial success, has a record 9.5 million high school students across prefix = st1 /Chinacompeting for just 2.6 million university places. For kids and parents alike, it's a difficulty that experts say causes extreme emotional distress. "Pressure from study and exams is a top reason for psychological problems among Chinese youth," said Jin Wuguan, director of the Youth Psychological Counseling Center at Shanghai'sRuijinHospital.  In China's increasingly success oriented, pressure-cooker cities, academic stress is seen as a rising cause of youth suicides and even murders of parents by children who are driven crazy by intolerable pressure to perform.  According to her family and newspaper accounts, 16-year-old Wu Wenwen drowned herself after she was stopped at the exam room door because her hair wasn't tied back as her school required. Returning in tied hair, she was then told the end-of-term exam had already started and she was too late to take it. In tears, Wu called her mother, and then disappeared. Her body was found the same night in a nearby lake.  China doesn't keep comprehensive statistics on student suicides, but Jin said health care professionals see the problem worsening, even among elementary students. Most Chinese schools still lack advisers and teachers receive little training in spotting symptoms of emotional distress, Jin said. Parents are little help, often piling on pressure while ignoring their children's emotional development, he said. "It's a basic unwillingness or inability to recognize and deal with with emotional problems," Jin said.  Wang Yufeng, of Peking University's Institute of Mental, estimates the rate of emotional disorders such as depression among Chinese students under age 17 at up to 32 percent , a total of 30 million students.  Others say that figure may be as high as 50 percent. A survey last year by the government's China Youth and ChildrenResearchCentershowed 57.6 percent of students felt highly distressed by academic pressures.
03/12/2022 16:12:24 - INFO - __main__ - ['both teachers and parents should learn more to deal with the problem of student suicides.']
03/12/2022 16:12:24 - INFO - __main__ -  [race-high] What is the aim of the Predator Compensation Program? (A) To protect people in the wild. (B) To protect Masai's farms. (C) To protect lions only. (D) To protect the wildlife. [SEP] At the beginning of the 20th century there were more than a million lions worldwide. Today there are less than 30,000 in the wild. The remaining lions are increasingly threatened by habitat loss, hunting and activities to protect farms and cattle. For generations, Masai tribesmen on the large African plains in southeastern Kenya have hunted lions -- to protect their farms and cattle. Today they celebrate the lions' life. Noah is an elder in the Masai community. "We have decided as a community of the Masai to lay down our spears, and there will be no more killing of lions in our community." He is part of a group of Masai visiting the United States promoting   the Predator Compensation Program. Conservation International's Frank Hawkins explains, "The Masai have been living with wildlife for many generations and it has been a conflicting ( ) relationship in many ways. They compete with the animals for food as lions eat their cattle. We're trying to find ways in which the wildlife will become something useful to them." They had the Predator Compensation Fund founded in 2003. After much discussion, a group of Masai farmers agreed to protect lions. In turn, if lions or other predators kill their cattle, the Masai owner will be paid market value for the dead animals from the fund. One man said that in the past, when a lion killed cattle, they killed it immediately. And now, after the start of the program, the Masai see the lion population growing. Since 2003, only four lions have been killed here.
03/12/2022 16:12:24 - INFO - __main__ - ['To protect the wildlife.']
03/12/2022 16:12:24 - INFO - __main__ - Tokenizing Input ...
03/12/2022 16:12:24 - INFO - __main__ - Tokenizing Output ...
03/12/2022 16:12:24 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 16:12:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 16:12:36 - INFO - __main__ - Starting training!
03/12/2022 16:12:41 - INFO - __main__ - Step 10 Global step 10 Train loss 19.710550 on epoch=4
03/12/2022 16:12:46 - INFO - __main__ - Step 20 Global step 20 Train loss 16.753077 on epoch=9
03/12/2022 16:12:52 - INFO - __main__ - Step 30 Global step 30 Train loss 15.203344 on epoch=14
03/12/2022 16:12:58 - INFO - __main__ - Step 40 Global step 40 Train loss 12.728414 on epoch=19
03/12/2022 16:13:04 - INFO - __main__ - Step 50 Global step 50 Train loss 10.447657 on epoch=24
03/12/2022 16:13:15 - INFO - __main__ - Global step 50 Train loss 14.968607 ACC 0.0 on epoch=24
03/12/2022 16:13:22 - INFO - __main__ - Step 60 Global step 60 Train loss 8.324196 on epoch=29
03/12/2022 16:13:28 - INFO - __main__ - Step 70 Global step 70 Train loss 6.156241 on epoch=34
03/12/2022 16:13:34 - INFO - __main__ - Step 80 Global step 80 Train loss 4.647246 on epoch=39
03/12/2022 16:13:41 - INFO - __main__ - Step 90 Global step 90 Train loss 3.684661 on epoch=44
03/12/2022 16:13:47 - INFO - __main__ - Step 100 Global step 100 Train loss 3.275239 on epoch=49
03/12/2022 16:13:48 - INFO - __main__ - Global step 100 Train loss 5.217517 ACC 0.0 on epoch=49
03/12/2022 16:13:54 - INFO - __main__ - Step 110 Global step 110 Train loss 2.975137 on epoch=54
03/12/2022 16:14:00 - INFO - __main__ - Step 120 Global step 120 Train loss 2.778851 on epoch=59
03/12/2022 16:14:07 - INFO - __main__ - Step 130 Global step 130 Train loss 2.436787 on epoch=64
03/12/2022 16:14:13 - INFO - __main__ - Step 140 Global step 140 Train loss 2.280117 on epoch=69
03/12/2022 16:14:19 - INFO - __main__ - Step 150 Global step 150 Train loss 2.018281 on epoch=74
03/12/2022 16:14:21 - INFO - __main__ - Global step 150 Train loss 2.497835 ACC 0.0 on epoch=74
03/12/2022 16:14:27 - INFO - __main__ - Step 160 Global step 160 Train loss 1.809946 on epoch=79
03/12/2022 16:14:33 - INFO - __main__ - Step 170 Global step 170 Train loss 1.622313 on epoch=84
03/12/2022 16:14:39 - INFO - __main__ - Step 180 Global step 180 Train loss 1.411372 on epoch=89
03/12/2022 16:14:45 - INFO - __main__ - Step 190 Global step 190 Train loss 1.272791 on epoch=94
03/12/2022 16:14:51 - INFO - __main__ - Step 200 Global step 200 Train loss 1.459694 on epoch=99
03/12/2022 16:14:53 - INFO - __main__ - Global step 200 Train loss 1.515223 ACC 0.0 on epoch=99
03/12/2022 16:14:59 - INFO - __main__ - Step 210 Global step 210 Train loss 1.567165 on epoch=104
03/12/2022 16:15:05 - INFO - __main__ - Step 220 Global step 220 Train loss 1.326716 on epoch=109
03/12/2022 16:15:11 - INFO - __main__ - Step 230 Global step 230 Train loss 1.355381 on epoch=114
03/12/2022 16:15:17 - INFO - __main__ - Step 240 Global step 240 Train loss 1.300244 on epoch=119
03/12/2022 16:15:23 - INFO - __main__ - Step 250 Global step 250 Train loss 1.208520 on epoch=124
03/12/2022 16:15:25 - INFO - __main__ - Global step 250 Train loss 1.351605 ACC 0.0 on epoch=124
03/12/2022 16:15:31 - INFO - __main__ - Step 260 Global step 260 Train loss 1.122096 on epoch=129
03/12/2022 16:15:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.998509 on epoch=134
03/12/2022 16:15:43 - INFO - __main__ - Step 280 Global step 280 Train loss 1.036816 on epoch=139
03/12/2022 16:15:49 - INFO - __main__ - Step 290 Global step 290 Train loss 1.190346 on epoch=144
03/12/2022 16:15:55 - INFO - __main__ - Step 300 Global step 300 Train loss 1.134389 on epoch=149
03/12/2022 16:15:57 - INFO - __main__ - Global step 300 Train loss 1.096431 ACC 0.0 on epoch=149
03/12/2022 16:16:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.908638 on epoch=154
03/12/2022 16:16:09 - INFO - __main__ - Step 320 Global step 320 Train loss 1.033414 on epoch=159
03/12/2022 16:16:15 - INFO - __main__ - Step 330 Global step 330 Train loss 1.059543 on epoch=164
03/12/2022 16:16:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.864163 on epoch=169
03/12/2022 16:16:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.987652 on epoch=174
03/12/2022 16:16:29 - INFO - __main__ - Global step 350 Train loss 0.970682 ACC 0.0 on epoch=174
03/12/2022 16:16:35 - INFO - __main__ - Step 360 Global step 360 Train loss 1.050002 on epoch=179
03/12/2022 16:16:41 - INFO - __main__ - Step 370 Global step 370 Train loss 1.011531 on epoch=184
03/12/2022 16:16:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.848844 on epoch=189
03/12/2022 16:16:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.789276 on epoch=194
03/12/2022 16:16:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.858109 on epoch=199
03/12/2022 16:17:01 - INFO - __main__ - Global step 400 Train loss 0.911552 ACC 0.0 on epoch=199
03/12/2022 16:17:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.816628 on epoch=204
03/12/2022 16:17:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.751726 on epoch=209
03/12/2022 16:17:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.747476 on epoch=214
03/12/2022 16:17:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.691781 on epoch=219
03/12/2022 16:17:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.763427 on epoch=224
03/12/2022 16:17:33 - INFO - __main__ - Global step 450 Train loss 0.754208 ACC 0.0 on epoch=224
03/12/2022 16:17:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.782685 on epoch=229
03/12/2022 16:17:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.752824 on epoch=234
03/12/2022 16:17:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.729814 on epoch=239
03/12/2022 16:17:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.728302 on epoch=244
03/12/2022 16:18:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.760163 on epoch=249
03/12/2022 16:18:05 - INFO - __main__ - Global step 500 Train loss 0.750758 ACC 0.0 on epoch=249
03/12/2022 16:18:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.761909 on epoch=254
03/12/2022 16:18:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.758864 on epoch=259
03/12/2022 16:18:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.746135 on epoch=264
03/12/2022 16:18:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.643855 on epoch=269
03/12/2022 16:18:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.691794 on epoch=274
03/12/2022 16:18:37 - INFO - __main__ - Global step 550 Train loss 0.720511 ACC 0.0 on epoch=274
03/12/2022 16:18:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.603077 on epoch=279
03/12/2022 16:18:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.592958 on epoch=284
03/12/2022 16:18:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.633591 on epoch=289
03/12/2022 16:19:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.648835 on epoch=294
03/12/2022 16:19:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.651268 on epoch=299
03/12/2022 16:19:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 16:19:09 - INFO - __main__ - Printing 3 examples
03/12/2022 16:19:09 - INFO - __main__ -  [race-high] Which of the following leads to Jobs's downfall at Apple? (A) The Apple III. (B) The Macintosh (C) The NeXT. (D) The Lisa. [SEP] Steven Jobs was born on February 24,1955,in  San Francisco,California,and was adopted by Paul and Clara Jobs.In 1961 the family moved to Mountain View, California.At that time people started to refer to the area as "Silicon Valley". As a child, Jobs preferred doing things by himself, not interested in team sports or other group activities.He enrolled in the HewlettPackard Explorer Club. There he saw his first computer at the age of twelve. He was very impressed, and knew right away that he wanted to work with computers. At that time almost all computers were so large that one could fill a room, and so costly that individuals could not afford to buy them.Advances in electronics, however, meant that the parts of a computer were getting smaller and the power of the computer was increasing. By 1980 the personal computer era was well underway.Apple was continually forced to improve its products to remain ahead, as more competitors entered the marketplace. Apple introduced the Apple III, but the new model suffered technical and marketing problems.It was removed from the market. Early in 1983 Jobs unveiled the Lisa.It did not sell well,however,  because  it  was  more  expensive  than  personal computers sold by competitors.Apple's biggest competitor was International Business Machines  (IBM). By 1983 it was estimated that Apple had lost half of its market share ( part of an industry's sales that a specific company has) to IBM. In 1984 Apple introduced a revolutionary new model, the Macintosh.The Macintosh did not sell well to businesses,however.It lacked features other personal computers had, such as a corresponding high quality printer. The failure of the Macintosh signaled the beginning of Jobs's downfall at Apple. Late in 1988 the NeXT computer was introduced, aimed at the educational market.The product was very userfriendly,and had a fast processing speed, excellent graphics displays,and an outstanding sound system.Despite the warm reception,however, the NeXT machine never caught on. It was too costly, had a blackandwhite screen, and could not be linked to other computers or run common software.
03/12/2022 16:19:09 - INFO - __main__ - ['The Macintosh']
03/12/2022 16:19:09 - INFO - __main__ -  [race-high] All the statements are true except   _  . (A) More than 50% of women ages 18 to 64 have jobs. (B) 18% of working women took a vacation away from home last year. (C) 32% of working women have college education. (D) 80% of working women drive a car to work. [SEP] Many American women are earning money outside their homes today. Among women who are eighteen to sixty-four years old, more than fifty per cent have jobs.     In general, working women have had more education then those who stay at home. Of those who work, thirty-two per cent have attended college, compared with twenty per cent of those who do not have jobs.     Among women with jobs, eight out of ten drive a car to work, and eight per cent took a vacation a way from home during the past year. Much of their traveling was by air.     These figures come from a report which was written for advertisers. The report gives advertisers a new picture of women today. For instance, it tells advertisers that fifty-one per cent of all American women have traveled by air--along with fifty-nine per cent of all American men.     The lesson for American business is that many women now have other interests in addition to their homes. They like advertisements which show women in office, planes, and cars.
03/12/2022 16:19:09 - INFO - __main__ - ['18% of working women took a vacation away from home last year.']
03/12/2022 16:19:09 - INFO - __main__ -  [race-high] In the passage the author argues that   _  . (A) it is unfair to blame English teachers for language deficiencies of students (B) to improve the level of English requires the effort of several generations (C) English should not be the target of the blame of language deficiencies (D) to rid language deficiencies one should have sensitive eyes and ears [SEP] The speaker, a teacher from a community college, addressed a sympathetic audience. Heads nodded in agreement when he said, "High school English teachers are not doing their jobs." He described the weaknesses of his students, all high school graduates who can use language only at a grade 9 level. I was unable to determine from his answers to my questions how this grade 9 level had been established. What the speaker was really saying is that he is no longer young; he has been teaching for sixteen years, and is able to think and speak like a mature adult. My point is that the frequent complaint of one generation about the one immediately following it is unavoidable. It is also human nature to look for the reasons for our dissatisfaction. Before English became a school subject in the late nineteenth century, it was difficult to find the target of the blame for language deficiencies  . But since then, English teachers have been under constant attack. The complainers think they have hit upon an original idea. As their own command of the language improves, they notice that young people do not have this same ability. Unaware that their own ability has developed through the years, they suppose the new generation of young people must be hopeless in this respect. To the eyes and ears of sensitive adults, the language of the young always seems inadequate . Since this concern about the decline and fall of the English language is not recognized as a generational phenomenon but rather as something new and strange to today's young people, it naturally follows that today's English teachers cannot be doing their jobs. Otherwise, young people would not commit crimes against the language.
03/12/2022 16:19:09 - INFO - __main__ - ['it is unfair to blame English teachers for language deficiencies of students']
03/12/2022 16:19:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 16:19:09 - INFO - __main__ - Tokenizing Output ...
03/12/2022 16:19:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 16:19:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 16:19:09 - INFO - __main__ - Printing 3 examples
03/12/2022 16:19:09 - INFO - __main__ -  [race-high] How many artists' paintings are on show at the special exhibition? (A) Three. (B) Five. (C) Seven. (D) Eight. [SEP] New picture The National Gallery now has a fine _ by the 18th century Dutch painter Jan van Os.This large picture (89.1 cm x 71cm) of flowers and fruit is painted in light bright colours on wood.It is one of the first pictures of this type in the Gallery.The picture is signed and dated 1777 and 1778.It is not unusual for a picture to be dated two years: the artist waited for particular flowers to come out in their different seasons in order to paint them.This picture was given to the Gallery by Miss Violet Churchman in memory of her sister Ida Nancy. It is now on show in Room 25. Special exhibition The exhibition "Painting in Spain During the Late 18th Century" opened in the Sunley Room on 15 March.Recently the Gallery has bought works by three Spanish painters of this period--Paret, Melendez and Francisco Bayeu, who are the focus of the exhibition.These three artists are also joined by Francisco's brother Ramon, by Antonio Gonzalez and two Italians who worked in Spain during these years--Corrado Giaquinto and Giovanni Battista Tiepolo.The exhibition runs until 31 May. Lecture news    Lectures will be given along with the special exhibition every Wednesday.On 8 April, Lizzie Barker will discuss the work of Melendez, while on 15 April, Sarah Symmons will lecture on Luis Patter.On 22 and 29 April, Juliet Wilson will talk about Francisco Bayeu.On 25 April, a Saturday, Erika Langmuir will explain how artists often "tell a story" through their pictures.
03/12/2022 16:19:09 - INFO - __main__ - ['Seven.']
03/12/2022 16:19:09 - INFO - __main__ -  [race-high] We can learn from the passage that   _  . (A) the problem of student suicides is getting worse according to a research on the accurate statistics (B) teachers have enough ability to sense the emotional distress of students (C) parents place neither pressure nor care on their children (D) both teachers and parents should learn more to deal with the problem of student suicides. [SEP] SHANGHAI, June 7(AP)--A 16-year-old girl's suicide after she was barred from a key exam draw attention to increasing worries over academic pressures, as millions of Chinese students began annual college entrance tests on Wednesday.  The three-day exam, viewed as important to future career and financial success, has a record 9.5 million high school students across prefix = st1 /Chinacompeting for just 2.6 million university places. For kids and parents alike, it's a difficulty that experts say causes extreme emotional distress. "Pressure from study and exams is a top reason for psychological problems among Chinese youth," said Jin Wuguan, director of the Youth Psychological Counseling Center at Shanghai'sRuijinHospital.  In China's increasingly success oriented, pressure-cooker cities, academic stress is seen as a rising cause of youth suicides and even murders of parents by children who are driven crazy by intolerable pressure to perform.  According to her family and newspaper accounts, 16-year-old Wu Wenwen drowned herself after she was stopped at the exam room door because her hair wasn't tied back as her school required. Returning in tied hair, she was then told the end-of-term exam had already started and she was too late to take it. In tears, Wu called her mother, and then disappeared. Her body was found the same night in a nearby lake.  China doesn't keep comprehensive statistics on student suicides, but Jin said health care professionals see the problem worsening, even among elementary students. Most Chinese schools still lack advisers and teachers receive little training in spotting symptoms of emotional distress, Jin said. Parents are little help, often piling on pressure while ignoring their children's emotional development, he said. "It's a basic unwillingness or inability to recognize and deal with with emotional problems," Jin said.  Wang Yufeng, of Peking University's Institute of Mental, estimates the rate of emotional disorders such as depression among Chinese students under age 17 at up to 32 percent , a total of 30 million students.  Others say that figure may be as high as 50 percent. A survey last year by the government's China Youth and ChildrenResearchCentershowed 57.6 percent of students felt highly distressed by academic pressures.
03/12/2022 16:19:09 - INFO - __main__ - ['both teachers and parents should learn more to deal with the problem of student suicides.']
03/12/2022 16:19:09 - INFO - __main__ -  [race-high] What is the aim of the Predator Compensation Program? (A) To protect people in the wild. (B) To protect Masai's farms. (C) To protect lions only. (D) To protect the wildlife. [SEP] At the beginning of the 20th century there were more than a million lions worldwide. Today there are less than 30,000 in the wild. The remaining lions are increasingly threatened by habitat loss, hunting and activities to protect farms and cattle. For generations, Masai tribesmen on the large African plains in southeastern Kenya have hunted lions -- to protect their farms and cattle. Today they celebrate the lions' life. Noah is an elder in the Masai community. "We have decided as a community of the Masai to lay down our spears, and there will be no more killing of lions in our community." He is part of a group of Masai visiting the United States promoting   the Predator Compensation Program. Conservation International's Frank Hawkins explains, "The Masai have been living with wildlife for many generations and it has been a conflicting ( ) relationship in many ways. They compete with the animals for food as lions eat their cattle. We're trying to find ways in which the wildlife will become something useful to them." They had the Predator Compensation Fund founded in 2003. After much discussion, a group of Masai farmers agreed to protect lions. In turn, if lions or other predators kill their cattle, the Masai owner will be paid market value for the dead animals from the fund. One man said that in the past, when a lion killed cattle, they killed it immediately. And now, after the start of the program, the Masai see the lion population growing. Since 2003, only four lions have been killed here.
03/12/2022 16:19:09 - INFO - __main__ - ['To protect the wildlife.']
03/12/2022 16:19:09 - INFO - __main__ - Tokenizing Input ...
03/12/2022 16:19:09 - INFO - __main__ - Tokenizing Output ...
03/12/2022 16:19:09 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 16:19:09 - INFO - __main__ - Global step 600 Train loss 0.625946 ACC 0.0 on epoch=299
03/12/2022 16:19:09 - INFO - __main__ - save last model!
03/12/2022 16:19:18 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 16:19:19 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 16:19:19 - INFO - __main__ - Printing 3 examples
03/12/2022 16:19:19 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 16:19:19 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 16:19:19 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 16:19:19 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 16:19:19 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 16:19:19 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 16:19:19 - INFO - __main__ - Tokenizing Input ...
03/12/2022 16:19:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 16:19:22 - INFO - __main__ - Starting training!
03/12/2022 16:19:24 - INFO - __main__ - Tokenizing Output ...
03/12/2022 16:19:28 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 16:36:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_100_0.0003_8_predictions.txt
03/12/2022 16:36:34 - INFO - __main__ - ACC on test data: 0.0000
03/12/2022 16:36:34 - INFO - __main__ - prefix=race-high_32_100, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.0
03/12/2022 16:36:34 - INFO - __main__ - Running ... prefix=race-high_32_100, lr=0.0002, bsz=8 ...
03/12/2022 16:36:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 16:36:35 - INFO - __main__ - Printing 3 examples
03/12/2022 16:36:35 - INFO - __main__ -  [race-high] Which of the following leads to Jobs's downfall at Apple? (A) The Apple III. (B) The Macintosh (C) The NeXT. (D) The Lisa. [SEP] Steven Jobs was born on February 24,1955,in  San Francisco,California,and was adopted by Paul and Clara Jobs.In 1961 the family moved to Mountain View, California.At that time people started to refer to the area as "Silicon Valley". As a child, Jobs preferred doing things by himself, not interested in team sports or other group activities.He enrolled in the HewlettPackard Explorer Club. There he saw his first computer at the age of twelve. He was very impressed, and knew right away that he wanted to work with computers. At that time almost all computers were so large that one could fill a room, and so costly that individuals could not afford to buy them.Advances in electronics, however, meant that the parts of a computer were getting smaller and the power of the computer was increasing. By 1980 the personal computer era was well underway.Apple was continually forced to improve its products to remain ahead, as more competitors entered the marketplace. Apple introduced the Apple III, but the new model suffered technical and marketing problems.It was removed from the market. Early in 1983 Jobs unveiled the Lisa.It did not sell well,however,  because  it  was  more  expensive  than  personal computers sold by competitors.Apple's biggest competitor was International Business Machines  (IBM). By 1983 it was estimated that Apple had lost half of its market share ( part of an industry's sales that a specific company has) to IBM. In 1984 Apple introduced a revolutionary new model, the Macintosh.The Macintosh did not sell well to businesses,however.It lacked features other personal computers had, such as a corresponding high quality printer. The failure of the Macintosh signaled the beginning of Jobs's downfall at Apple. Late in 1988 the NeXT computer was introduced, aimed at the educational market.The product was very userfriendly,and had a fast processing speed, excellent graphics displays,and an outstanding sound system.Despite the warm reception,however, the NeXT machine never caught on. It was too costly, had a blackandwhite screen, and could not be linked to other computers or run common software.
03/12/2022 16:36:35 - INFO - __main__ - ['The Macintosh']
03/12/2022 16:36:35 - INFO - __main__ -  [race-high] All the statements are true except   _  . (A) More than 50% of women ages 18 to 64 have jobs. (B) 18% of working women took a vacation away from home last year. (C) 32% of working women have college education. (D) 80% of working women drive a car to work. [SEP] Many American women are earning money outside their homes today. Among women who are eighteen to sixty-four years old, more than fifty per cent have jobs.     In general, working women have had more education then those who stay at home. Of those who work, thirty-two per cent have attended college, compared with twenty per cent of those who do not have jobs.     Among women with jobs, eight out of ten drive a car to work, and eight per cent took a vacation a way from home during the past year. Much of their traveling was by air.     These figures come from a report which was written for advertisers. The report gives advertisers a new picture of women today. For instance, it tells advertisers that fifty-one per cent of all American women have traveled by air--along with fifty-nine per cent of all American men.     The lesson for American business is that many women now have other interests in addition to their homes. They like advertisements which show women in office, planes, and cars.
03/12/2022 16:36:35 - INFO - __main__ - ['18% of working women took a vacation away from home last year.']
03/12/2022 16:36:35 - INFO - __main__ -  [race-high] In the passage the author argues that   _  . (A) it is unfair to blame English teachers for language deficiencies of students (B) to improve the level of English requires the effort of several generations (C) English should not be the target of the blame of language deficiencies (D) to rid language deficiencies one should have sensitive eyes and ears [SEP] The speaker, a teacher from a community college, addressed a sympathetic audience. Heads nodded in agreement when he said, "High school English teachers are not doing their jobs." He described the weaknesses of his students, all high school graduates who can use language only at a grade 9 level. I was unable to determine from his answers to my questions how this grade 9 level had been established. What the speaker was really saying is that he is no longer young; he has been teaching for sixteen years, and is able to think and speak like a mature adult. My point is that the frequent complaint of one generation about the one immediately following it is unavoidable. It is also human nature to look for the reasons for our dissatisfaction. Before English became a school subject in the late nineteenth century, it was difficult to find the target of the blame for language deficiencies  . But since then, English teachers have been under constant attack. The complainers think they have hit upon an original idea. As their own command of the language improves, they notice that young people do not have this same ability. Unaware that their own ability has developed through the years, they suppose the new generation of young people must be hopeless in this respect. To the eyes and ears of sensitive adults, the language of the young always seems inadequate . Since this concern about the decline and fall of the English language is not recognized as a generational phenomenon but rather as something new and strange to today's young people, it naturally follows that today's English teachers cannot be doing their jobs. Otherwise, young people would not commit crimes against the language.
03/12/2022 16:36:35 - INFO - __main__ - ['it is unfair to blame English teachers for language deficiencies of students']
03/12/2022 16:36:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 16:36:35 - INFO - __main__ - Tokenizing Output ...
03/12/2022 16:36:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 16:36:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 16:36:35 - INFO - __main__ - Printing 3 examples
03/12/2022 16:36:35 - INFO - __main__ -  [race-high] How many artists' paintings are on show at the special exhibition? (A) Three. (B) Five. (C) Seven. (D) Eight. [SEP] New picture The National Gallery now has a fine _ by the 18th century Dutch painter Jan van Os.This large picture (89.1 cm x 71cm) of flowers and fruit is painted in light bright colours on wood.It is one of the first pictures of this type in the Gallery.The picture is signed and dated 1777 and 1778.It is not unusual for a picture to be dated two years: the artist waited for particular flowers to come out in their different seasons in order to paint them.This picture was given to the Gallery by Miss Violet Churchman in memory of her sister Ida Nancy. It is now on show in Room 25. Special exhibition The exhibition "Painting in Spain During the Late 18th Century" opened in the Sunley Room on 15 March.Recently the Gallery has bought works by three Spanish painters of this period--Paret, Melendez and Francisco Bayeu, who are the focus of the exhibition.These three artists are also joined by Francisco's brother Ramon, by Antonio Gonzalez and two Italians who worked in Spain during these years--Corrado Giaquinto and Giovanni Battista Tiepolo.The exhibition runs until 31 May. Lecture news    Lectures will be given along with the special exhibition every Wednesday.On 8 April, Lizzie Barker will discuss the work of Melendez, while on 15 April, Sarah Symmons will lecture on Luis Patter.On 22 and 29 April, Juliet Wilson will talk about Francisco Bayeu.On 25 April, a Saturday, Erika Langmuir will explain how artists often "tell a story" through their pictures.
03/12/2022 16:36:35 - INFO - __main__ - ['Seven.']
03/12/2022 16:36:35 - INFO - __main__ -  [race-high] We can learn from the passage that   _  . (A) the problem of student suicides is getting worse according to a research on the accurate statistics (B) teachers have enough ability to sense the emotional distress of students (C) parents place neither pressure nor care on their children (D) both teachers and parents should learn more to deal with the problem of student suicides. [SEP] SHANGHAI, June 7(AP)--A 16-year-old girl's suicide after she was barred from a key exam draw attention to increasing worries over academic pressures, as millions of Chinese students began annual college entrance tests on Wednesday.  The three-day exam, viewed as important to future career and financial success, has a record 9.5 million high school students across prefix = st1 /Chinacompeting for just 2.6 million university places. For kids and parents alike, it's a difficulty that experts say causes extreme emotional distress. "Pressure from study and exams is a top reason for psychological problems among Chinese youth," said Jin Wuguan, director of the Youth Psychological Counseling Center at Shanghai'sRuijinHospital.  In China's increasingly success oriented, pressure-cooker cities, academic stress is seen as a rising cause of youth suicides and even murders of parents by children who are driven crazy by intolerable pressure to perform.  According to her family and newspaper accounts, 16-year-old Wu Wenwen drowned herself after she was stopped at the exam room door because her hair wasn't tied back as her school required. Returning in tied hair, she was then told the end-of-term exam had already started and she was too late to take it. In tears, Wu called her mother, and then disappeared. Her body was found the same night in a nearby lake.  China doesn't keep comprehensive statistics on student suicides, but Jin said health care professionals see the problem worsening, even among elementary students. Most Chinese schools still lack advisers and teachers receive little training in spotting symptoms of emotional distress, Jin said. Parents are little help, often piling on pressure while ignoring their children's emotional development, he said. "It's a basic unwillingness or inability to recognize and deal with with emotional problems," Jin said.  Wang Yufeng, of Peking University's Institute of Mental, estimates the rate of emotional disorders such as depression among Chinese students under age 17 at up to 32 percent , a total of 30 million students.  Others say that figure may be as high as 50 percent. A survey last year by the government's China Youth and ChildrenResearchCentershowed 57.6 percent of students felt highly distressed by academic pressures.
03/12/2022 16:36:35 - INFO - __main__ - ['both teachers and parents should learn more to deal with the problem of student suicides.']
03/12/2022 16:36:35 - INFO - __main__ -  [race-high] What is the aim of the Predator Compensation Program? (A) To protect people in the wild. (B) To protect Masai's farms. (C) To protect lions only. (D) To protect the wildlife. [SEP] At the beginning of the 20th century there were more than a million lions worldwide. Today there are less than 30,000 in the wild. The remaining lions are increasingly threatened by habitat loss, hunting and activities to protect farms and cattle. For generations, Masai tribesmen on the large African plains in southeastern Kenya have hunted lions -- to protect their farms and cattle. Today they celebrate the lions' life. Noah is an elder in the Masai community. "We have decided as a community of the Masai to lay down our spears, and there will be no more killing of lions in our community." He is part of a group of Masai visiting the United States promoting   the Predator Compensation Program. Conservation International's Frank Hawkins explains, "The Masai have been living with wildlife for many generations and it has been a conflicting ( ) relationship in many ways. They compete with the animals for food as lions eat their cattle. We're trying to find ways in which the wildlife will become something useful to them." They had the Predator Compensation Fund founded in 2003. After much discussion, a group of Masai farmers agreed to protect lions. In turn, if lions or other predators kill their cattle, the Masai owner will be paid market value for the dead animals from the fund. One man said that in the past, when a lion killed cattle, they killed it immediately. And now, after the start of the program, the Masai see the lion population growing. Since 2003, only four lions have been killed here.
03/12/2022 16:36:35 - INFO - __main__ - ['To protect the wildlife.']
03/12/2022 16:36:35 - INFO - __main__ - Tokenizing Input ...
03/12/2022 16:36:35 - INFO - __main__ - Tokenizing Output ...
03/12/2022 16:36:35 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 16:36:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 16:36:48 - INFO - __main__ - Starting training!
03/12/2022 16:36:56 - INFO - __main__ - Step 10 Global step 10 Train loss 19.560165 on epoch=4
03/12/2022 16:37:02 - INFO - __main__ - Step 20 Global step 20 Train loss 16.973789 on epoch=9
03/12/2022 16:37:08 - INFO - __main__ - Step 30 Global step 30 Train loss 12.170694 on epoch=14
03/12/2022 16:37:14 - INFO - __main__ - Step 40 Global step 40 Train loss 9.149379 on epoch=19
03/12/2022 16:37:20 - INFO - __main__ - Step 50 Global step 50 Train loss 7.537128 on epoch=24
03/12/2022 16:37:32 - INFO - __main__ - Global step 50 Train loss 13.078231 ACC 0.09375 on epoch=24
03/12/2022 16:37:38 - INFO - __main__ - Step 60 Global step 60 Train loss 5.950399 on epoch=29
03/12/2022 16:37:44 - INFO - __main__ - Step 70 Global step 70 Train loss 5.011496 on epoch=34
03/12/2022 16:37:50 - INFO - __main__ - Step 80 Global step 80 Train loss 4.089958 on epoch=39
03/12/2022 16:37:56 - INFO - __main__ - Step 90 Global step 90 Train loss 3.766047 on epoch=44
03/12/2022 16:38:03 - INFO - __main__ - Step 100 Global step 100 Train loss 3.547241 on epoch=49
03/12/2022 16:38:05 - INFO - __main__ - Global step 100 Train loss 4.473028 ACC 0.03125 on epoch=49
03/12/2022 16:38:11 - INFO - __main__ - Step 110 Global step 110 Train loss 3.174309 on epoch=54
03/12/2022 16:38:17 - INFO - __main__ - Step 120 Global step 120 Train loss 2.975522 on epoch=59
03/12/2022 16:38:23 - INFO - __main__ - Step 130 Global step 130 Train loss 2.855971 on epoch=64
03/12/2022 16:38:29 - INFO - __main__ - Step 140 Global step 140 Train loss 2.720743 on epoch=69
03/12/2022 16:38:35 - INFO - __main__ - Step 150 Global step 150 Train loss 2.601049 on epoch=74
03/12/2022 16:38:37 - INFO - __main__ - Global step 150 Train loss 2.865519 ACC 0.09375 on epoch=74
03/12/2022 16:38:43 - INFO - __main__ - Step 160 Global step 160 Train loss 2.121537 on epoch=79
03/12/2022 16:38:50 - INFO - __main__ - Step 170 Global step 170 Train loss 2.168976 on epoch=84
03/12/2022 16:38:56 - INFO - __main__ - Step 180 Global step 180 Train loss 2.022306 on epoch=89
03/12/2022 16:39:02 - INFO - __main__ - Step 190 Global step 190 Train loss 1.803145 on epoch=94
03/12/2022 16:39:08 - INFO - __main__ - Step 200 Global step 200 Train loss 1.599191 on epoch=99
03/12/2022 16:39:10 - INFO - __main__ - Global step 200 Train loss 1.943031 ACC 0.0625 on epoch=99
03/12/2022 16:39:16 - INFO - __main__ - Step 210 Global step 210 Train loss 1.656405 on epoch=104
03/12/2022 16:39:23 - INFO - __main__ - Step 220 Global step 220 Train loss 1.466886 on epoch=109
03/12/2022 16:39:29 - INFO - __main__ - Step 230 Global step 230 Train loss 1.500799 on epoch=114
03/12/2022 16:39:35 - INFO - __main__ - Step 240 Global step 240 Train loss 1.468016 on epoch=119
03/12/2022 16:39:41 - INFO - __main__ - Step 250 Global step 250 Train loss 1.413597 on epoch=124
03/12/2022 16:39:43 - INFO - __main__ - Global step 250 Train loss 1.501141 ACC 0.0625 on epoch=124
03/12/2022 16:39:49 - INFO - __main__ - Step 260 Global step 260 Train loss 1.069618 on epoch=129
03/12/2022 16:39:56 - INFO - __main__ - Step 270 Global step 270 Train loss 1.207351 on epoch=134
03/12/2022 16:40:02 - INFO - __main__ - Step 280 Global step 280 Train loss 1.185253 on epoch=139
03/12/2022 16:40:08 - INFO - __main__ - Step 290 Global step 290 Train loss 1.135396 on epoch=144
03/12/2022 16:40:14 - INFO - __main__ - Step 300 Global step 300 Train loss 1.055983 on epoch=149
03/12/2022 16:40:16 - INFO - __main__ - Global step 300 Train loss 1.130720 ACC 0.0 on epoch=149
03/12/2022 16:40:22 - INFO - __main__ - Step 310 Global step 310 Train loss 1.022992 on epoch=154
03/12/2022 16:40:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.960675 on epoch=159
03/12/2022 16:40:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.988790 on epoch=164
03/12/2022 16:40:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.932986 on epoch=169
03/12/2022 16:40:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.982684 on epoch=174
03/12/2022 16:40:49 - INFO - __main__ - Global step 350 Train loss 0.977625 ACC 0.0 on epoch=174
03/12/2022 16:40:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.982731 on epoch=179
03/12/2022 16:41:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.968620 on epoch=184
03/12/2022 16:41:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.930245 on epoch=189
03/12/2022 16:41:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.938160 on epoch=194
03/12/2022 16:41:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.860805 on epoch=199
03/12/2022 16:41:21 - INFO - __main__ - Global step 400 Train loss 0.936112 ACC 0.0 on epoch=199
03/12/2022 16:41:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.914541 on epoch=204
03/12/2022 16:41:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.888291 on epoch=209
03/12/2022 16:41:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.915503 on epoch=214
03/12/2022 16:41:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.760463 on epoch=219
03/12/2022 16:41:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.858866 on epoch=224
03/12/2022 16:41:54 - INFO - __main__ - Global step 450 Train loss 0.867533 ACC 0.0 on epoch=224
03/12/2022 16:42:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.951500 on epoch=229
03/12/2022 16:42:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.808260 on epoch=234
03/12/2022 16:42:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.820159 on epoch=239
03/12/2022 16:42:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.787285 on epoch=244
03/12/2022 16:42:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.757057 on epoch=249
03/12/2022 16:42:26 - INFO - __main__ - Global step 500 Train loss 0.824852 ACC 0.0 on epoch=249
03/12/2022 16:42:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.700069 on epoch=254
03/12/2022 16:42:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.632214 on epoch=259
03/12/2022 16:42:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.765448 on epoch=264
03/12/2022 16:42:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.736241 on epoch=269
03/12/2022 16:42:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.717414 on epoch=274
03/12/2022 16:42:59 - INFO - __main__ - Global step 550 Train loss 0.710277 ACC 0.0 on epoch=274
03/12/2022 16:43:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.663930 on epoch=279
03/12/2022 16:43:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.730226 on epoch=284
03/12/2022 16:43:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.685905 on epoch=289
03/12/2022 16:43:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.678962 on epoch=294
03/12/2022 16:43:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.723660 on epoch=299
03/12/2022 16:43:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 16:43:31 - INFO - __main__ - Printing 3 examples
03/12/2022 16:43:31 - INFO - __main__ -  [race-high] Which of the following leads to Jobs's downfall at Apple? (A) The Apple III. (B) The Macintosh (C) The NeXT. (D) The Lisa. [SEP] Steven Jobs was born on February 24,1955,in  San Francisco,California,and was adopted by Paul and Clara Jobs.In 1961 the family moved to Mountain View, California.At that time people started to refer to the area as "Silicon Valley". As a child, Jobs preferred doing things by himself, not interested in team sports or other group activities.He enrolled in the HewlettPackard Explorer Club. There he saw his first computer at the age of twelve. He was very impressed, and knew right away that he wanted to work with computers. At that time almost all computers were so large that one could fill a room, and so costly that individuals could not afford to buy them.Advances in electronics, however, meant that the parts of a computer were getting smaller and the power of the computer was increasing. By 1980 the personal computer era was well underway.Apple was continually forced to improve its products to remain ahead, as more competitors entered the marketplace. Apple introduced the Apple III, but the new model suffered technical and marketing problems.It was removed from the market. Early in 1983 Jobs unveiled the Lisa.It did not sell well,however,  because  it  was  more  expensive  than  personal computers sold by competitors.Apple's biggest competitor was International Business Machines  (IBM). By 1983 it was estimated that Apple had lost half of its market share ( part of an industry's sales that a specific company has) to IBM. In 1984 Apple introduced a revolutionary new model, the Macintosh.The Macintosh did not sell well to businesses,however.It lacked features other personal computers had, such as a corresponding high quality printer. The failure of the Macintosh signaled the beginning of Jobs's downfall at Apple. Late in 1988 the NeXT computer was introduced, aimed at the educational market.The product was very userfriendly,and had a fast processing speed, excellent graphics displays,and an outstanding sound system.Despite the warm reception,however, the NeXT machine never caught on. It was too costly, had a blackandwhite screen, and could not be linked to other computers or run common software.
03/12/2022 16:43:31 - INFO - __main__ - ['The Macintosh']
03/12/2022 16:43:31 - INFO - __main__ -  [race-high] All the statements are true except   _  . (A) More than 50% of women ages 18 to 64 have jobs. (B) 18% of working women took a vacation away from home last year. (C) 32% of working women have college education. (D) 80% of working women drive a car to work. [SEP] Many American women are earning money outside their homes today. Among women who are eighteen to sixty-four years old, more than fifty per cent have jobs.     In general, working women have had more education then those who stay at home. Of those who work, thirty-two per cent have attended college, compared with twenty per cent of those who do not have jobs.     Among women with jobs, eight out of ten drive a car to work, and eight per cent took a vacation a way from home during the past year. Much of their traveling was by air.     These figures come from a report which was written for advertisers. The report gives advertisers a new picture of women today. For instance, it tells advertisers that fifty-one per cent of all American women have traveled by air--along with fifty-nine per cent of all American men.     The lesson for American business is that many women now have other interests in addition to their homes. They like advertisements which show women in office, planes, and cars.
03/12/2022 16:43:31 - INFO - __main__ - ['18% of working women took a vacation away from home last year.']
03/12/2022 16:43:31 - INFO - __main__ -  [race-high] In the passage the author argues that   _  . (A) it is unfair to blame English teachers for language deficiencies of students (B) to improve the level of English requires the effort of several generations (C) English should not be the target of the blame of language deficiencies (D) to rid language deficiencies one should have sensitive eyes and ears [SEP] The speaker, a teacher from a community college, addressed a sympathetic audience. Heads nodded in agreement when he said, "High school English teachers are not doing their jobs." He described the weaknesses of his students, all high school graduates who can use language only at a grade 9 level. I was unable to determine from his answers to my questions how this grade 9 level had been established. What the speaker was really saying is that he is no longer young; he has been teaching for sixteen years, and is able to think and speak like a mature adult. My point is that the frequent complaint of one generation about the one immediately following it is unavoidable. It is also human nature to look for the reasons for our dissatisfaction. Before English became a school subject in the late nineteenth century, it was difficult to find the target of the blame for language deficiencies  . But since then, English teachers have been under constant attack. The complainers think they have hit upon an original idea. As their own command of the language improves, they notice that young people do not have this same ability. Unaware that their own ability has developed through the years, they suppose the new generation of young people must be hopeless in this respect. To the eyes and ears of sensitive adults, the language of the young always seems inadequate . Since this concern about the decline and fall of the English language is not recognized as a generational phenomenon but rather as something new and strange to today's young people, it naturally follows that today's English teachers cannot be doing their jobs. Otherwise, young people would not commit crimes against the language.
03/12/2022 16:43:31 - INFO - __main__ - ['it is unfair to blame English teachers for language deficiencies of students']
03/12/2022 16:43:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 16:43:31 - INFO - __main__ - Tokenizing Output ...
03/12/2022 16:43:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 16:43:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 16:43:31 - INFO - __main__ - Printing 3 examples
03/12/2022 16:43:31 - INFO - __main__ -  [race-high] How many artists' paintings are on show at the special exhibition? (A) Three. (B) Five. (C) Seven. (D) Eight. [SEP] New picture The National Gallery now has a fine _ by the 18th century Dutch painter Jan van Os.This large picture (89.1 cm x 71cm) of flowers and fruit is painted in light bright colours on wood.It is one of the first pictures of this type in the Gallery.The picture is signed and dated 1777 and 1778.It is not unusual for a picture to be dated two years: the artist waited for particular flowers to come out in their different seasons in order to paint them.This picture was given to the Gallery by Miss Violet Churchman in memory of her sister Ida Nancy. It is now on show in Room 25. Special exhibition The exhibition "Painting in Spain During the Late 18th Century" opened in the Sunley Room on 15 March.Recently the Gallery has bought works by three Spanish painters of this period--Paret, Melendez and Francisco Bayeu, who are the focus of the exhibition.These three artists are also joined by Francisco's brother Ramon, by Antonio Gonzalez and two Italians who worked in Spain during these years--Corrado Giaquinto and Giovanni Battista Tiepolo.The exhibition runs until 31 May. Lecture news    Lectures will be given along with the special exhibition every Wednesday.On 8 April, Lizzie Barker will discuss the work of Melendez, while on 15 April, Sarah Symmons will lecture on Luis Patter.On 22 and 29 April, Juliet Wilson will talk about Francisco Bayeu.On 25 April, a Saturday, Erika Langmuir will explain how artists often "tell a story" through their pictures.
03/12/2022 16:43:31 - INFO - __main__ - ['Seven.']
03/12/2022 16:43:31 - INFO - __main__ -  [race-high] We can learn from the passage that   _  . (A) the problem of student suicides is getting worse according to a research on the accurate statistics (B) teachers have enough ability to sense the emotional distress of students (C) parents place neither pressure nor care on their children (D) both teachers and parents should learn more to deal with the problem of student suicides. [SEP] SHANGHAI, June 7(AP)--A 16-year-old girl's suicide after she was barred from a key exam draw attention to increasing worries over academic pressures, as millions of Chinese students began annual college entrance tests on Wednesday.  The three-day exam, viewed as important to future career and financial success, has a record 9.5 million high school students across prefix = st1 /Chinacompeting for just 2.6 million university places. For kids and parents alike, it's a difficulty that experts say causes extreme emotional distress. "Pressure from study and exams is a top reason for psychological problems among Chinese youth," said Jin Wuguan, director of the Youth Psychological Counseling Center at Shanghai'sRuijinHospital.  In China's increasingly success oriented, pressure-cooker cities, academic stress is seen as a rising cause of youth suicides and even murders of parents by children who are driven crazy by intolerable pressure to perform.  According to her family and newspaper accounts, 16-year-old Wu Wenwen drowned herself after she was stopped at the exam room door because her hair wasn't tied back as her school required. Returning in tied hair, she was then told the end-of-term exam had already started and she was too late to take it. In tears, Wu called her mother, and then disappeared. Her body was found the same night in a nearby lake.  China doesn't keep comprehensive statistics on student suicides, but Jin said health care professionals see the problem worsening, even among elementary students. Most Chinese schools still lack advisers and teachers receive little training in spotting symptoms of emotional distress, Jin said. Parents are little help, often piling on pressure while ignoring their children's emotional development, he said. "It's a basic unwillingness or inability to recognize and deal with with emotional problems," Jin said.  Wang Yufeng, of Peking University's Institute of Mental, estimates the rate of emotional disorders such as depression among Chinese students under age 17 at up to 32 percent , a total of 30 million students.  Others say that figure may be as high as 50 percent. A survey last year by the government's China Youth and ChildrenResearchCentershowed 57.6 percent of students felt highly distressed by academic pressures.
03/12/2022 16:43:31 - INFO - __main__ - ['both teachers and parents should learn more to deal with the problem of student suicides.']
03/12/2022 16:43:31 - INFO - __main__ -  [race-high] What is the aim of the Predator Compensation Program? (A) To protect people in the wild. (B) To protect Masai's farms. (C) To protect lions only. (D) To protect the wildlife. [SEP] At the beginning of the 20th century there were more than a million lions worldwide. Today there are less than 30,000 in the wild. The remaining lions are increasingly threatened by habitat loss, hunting and activities to protect farms and cattle. For generations, Masai tribesmen on the large African plains in southeastern Kenya have hunted lions -- to protect their farms and cattle. Today they celebrate the lions' life. Noah is an elder in the Masai community. "We have decided as a community of the Masai to lay down our spears, and there will be no more killing of lions in our community." He is part of a group of Masai visiting the United States promoting   the Predator Compensation Program. Conservation International's Frank Hawkins explains, "The Masai have been living with wildlife for many generations and it has been a conflicting ( ) relationship in many ways. They compete with the animals for food as lions eat their cattle. We're trying to find ways in which the wildlife will become something useful to them." They had the Predator Compensation Fund founded in 2003. After much discussion, a group of Masai farmers agreed to protect lions. In turn, if lions or other predators kill their cattle, the Masai owner will be paid market value for the dead animals from the fund. One man said that in the past, when a lion killed cattle, they killed it immediately. And now, after the start of the program, the Masai see the lion population growing. Since 2003, only four lions have been killed here.
03/12/2022 16:43:31 - INFO - __main__ - ['To protect the wildlife.']
03/12/2022 16:43:31 - INFO - __main__ - Tokenizing Input ...
03/12/2022 16:43:31 - INFO - __main__ - Tokenizing Output ...
03/12/2022 16:43:31 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 16:43:31 - INFO - __main__ - Global step 600 Train loss 0.696536 ACC 0.0 on epoch=299
03/12/2022 16:43:31 - INFO - __main__ - save last model!
03/12/2022 16:43:38 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 16:43:39 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 16:43:39 - INFO - __main__ - Printing 3 examples
03/12/2022 16:43:39 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 16:43:39 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 16:43:39 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 16:43:39 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 16:43:39 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 16:43:39 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 16:43:39 - INFO - __main__ - Tokenizing Input ...
03/12/2022 16:43:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 16:43:44 - INFO - __main__ - Starting training!
03/12/2022 16:43:45 - INFO - __main__ - Tokenizing Output ...
03/12/2022 16:43:48 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 17:00:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_100_0.0002_8_predictions.txt
03/12/2022 17:00:27 - INFO - __main__ - ACC on test data: 0.0243
03/12/2022 17:00:28 - INFO - __main__ - prefix=race-high_32_100, lr=0.0002, bsz=8, dev_performance=0.09375, test_performance=0.02434077079107505
03/12/2022 17:00:28 - INFO - __main__ - Running ... prefix=race-high_32_100, lr=0.0001, bsz=8 ...
03/12/2022 17:00:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:00:29 - INFO - __main__ - Printing 3 examples
03/12/2022 17:00:29 - INFO - __main__ -  [race-high] Which of the following leads to Jobs's downfall at Apple? (A) The Apple III. (B) The Macintosh (C) The NeXT. (D) The Lisa. [SEP] Steven Jobs was born on February 24,1955,in  San Francisco,California,and was adopted by Paul and Clara Jobs.In 1961 the family moved to Mountain View, California.At that time people started to refer to the area as "Silicon Valley". As a child, Jobs preferred doing things by himself, not interested in team sports or other group activities.He enrolled in the HewlettPackard Explorer Club. There he saw his first computer at the age of twelve. He was very impressed, and knew right away that he wanted to work with computers. At that time almost all computers were so large that one could fill a room, and so costly that individuals could not afford to buy them.Advances in electronics, however, meant that the parts of a computer were getting smaller and the power of the computer was increasing. By 1980 the personal computer era was well underway.Apple was continually forced to improve its products to remain ahead, as more competitors entered the marketplace. Apple introduced the Apple III, but the new model suffered technical and marketing problems.It was removed from the market. Early in 1983 Jobs unveiled the Lisa.It did not sell well,however,  because  it  was  more  expensive  than  personal computers sold by competitors.Apple's biggest competitor was International Business Machines  (IBM). By 1983 it was estimated that Apple had lost half of its market share ( part of an industry's sales that a specific company has) to IBM. In 1984 Apple introduced a revolutionary new model, the Macintosh.The Macintosh did not sell well to businesses,however.It lacked features other personal computers had, such as a corresponding high quality printer. The failure of the Macintosh signaled the beginning of Jobs's downfall at Apple. Late in 1988 the NeXT computer was introduced, aimed at the educational market.The product was very userfriendly,and had a fast processing speed, excellent graphics displays,and an outstanding sound system.Despite the warm reception,however, the NeXT machine never caught on. It was too costly, had a blackandwhite screen, and could not be linked to other computers or run common software.
03/12/2022 17:00:29 - INFO - __main__ - ['The Macintosh']
03/12/2022 17:00:29 - INFO - __main__ -  [race-high] All the statements are true except   _  . (A) More than 50% of women ages 18 to 64 have jobs. (B) 18% of working women took a vacation away from home last year. (C) 32% of working women have college education. (D) 80% of working women drive a car to work. [SEP] Many American women are earning money outside their homes today. Among women who are eighteen to sixty-four years old, more than fifty per cent have jobs.     In general, working women have had more education then those who stay at home. Of those who work, thirty-two per cent have attended college, compared with twenty per cent of those who do not have jobs.     Among women with jobs, eight out of ten drive a car to work, and eight per cent took a vacation a way from home during the past year. Much of their traveling was by air.     These figures come from a report which was written for advertisers. The report gives advertisers a new picture of women today. For instance, it tells advertisers that fifty-one per cent of all American women have traveled by air--along with fifty-nine per cent of all American men.     The lesson for American business is that many women now have other interests in addition to their homes. They like advertisements which show women in office, planes, and cars.
03/12/2022 17:00:29 - INFO - __main__ - ['18% of working women took a vacation away from home last year.']
03/12/2022 17:00:29 - INFO - __main__ -  [race-high] In the passage the author argues that   _  . (A) it is unfair to blame English teachers for language deficiencies of students (B) to improve the level of English requires the effort of several generations (C) English should not be the target of the blame of language deficiencies (D) to rid language deficiencies one should have sensitive eyes and ears [SEP] The speaker, a teacher from a community college, addressed a sympathetic audience. Heads nodded in agreement when he said, "High school English teachers are not doing their jobs." He described the weaknesses of his students, all high school graduates who can use language only at a grade 9 level. I was unable to determine from his answers to my questions how this grade 9 level had been established. What the speaker was really saying is that he is no longer young; he has been teaching for sixteen years, and is able to think and speak like a mature adult. My point is that the frequent complaint of one generation about the one immediately following it is unavoidable. It is also human nature to look for the reasons for our dissatisfaction. Before English became a school subject in the late nineteenth century, it was difficult to find the target of the blame for language deficiencies  . But since then, English teachers have been under constant attack. The complainers think they have hit upon an original idea. As their own command of the language improves, they notice that young people do not have this same ability. Unaware that their own ability has developed through the years, they suppose the new generation of young people must be hopeless in this respect. To the eyes and ears of sensitive adults, the language of the young always seems inadequate . Since this concern about the decline and fall of the English language is not recognized as a generational phenomenon but rather as something new and strange to today's young people, it naturally follows that today's English teachers cannot be doing their jobs. Otherwise, young people would not commit crimes against the language.
03/12/2022 17:00:29 - INFO - __main__ - ['it is unfair to blame English teachers for language deficiencies of students']
03/12/2022 17:00:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 17:00:29 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:00:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 17:00:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:00:29 - INFO - __main__ - Printing 3 examples
03/12/2022 17:00:29 - INFO - __main__ -  [race-high] How many artists' paintings are on show at the special exhibition? (A) Three. (B) Five. (C) Seven. (D) Eight. [SEP] New picture The National Gallery now has a fine _ by the 18th century Dutch painter Jan van Os.This large picture (89.1 cm x 71cm) of flowers and fruit is painted in light bright colours on wood.It is one of the first pictures of this type in the Gallery.The picture is signed and dated 1777 and 1778.It is not unusual for a picture to be dated two years: the artist waited for particular flowers to come out in their different seasons in order to paint them.This picture was given to the Gallery by Miss Violet Churchman in memory of her sister Ida Nancy. It is now on show in Room 25. Special exhibition The exhibition "Painting in Spain During the Late 18th Century" opened in the Sunley Room on 15 March.Recently the Gallery has bought works by three Spanish painters of this period--Paret, Melendez and Francisco Bayeu, who are the focus of the exhibition.These three artists are also joined by Francisco's brother Ramon, by Antonio Gonzalez and two Italians who worked in Spain during these years--Corrado Giaquinto and Giovanni Battista Tiepolo.The exhibition runs until 31 May. Lecture news    Lectures will be given along with the special exhibition every Wednesday.On 8 April, Lizzie Barker will discuss the work of Melendez, while on 15 April, Sarah Symmons will lecture on Luis Patter.On 22 and 29 April, Juliet Wilson will talk about Francisco Bayeu.On 25 April, a Saturday, Erika Langmuir will explain how artists often "tell a story" through their pictures.
03/12/2022 17:00:29 - INFO - __main__ - ['Seven.']
03/12/2022 17:00:29 - INFO - __main__ -  [race-high] We can learn from the passage that   _  . (A) the problem of student suicides is getting worse according to a research on the accurate statistics (B) teachers have enough ability to sense the emotional distress of students (C) parents place neither pressure nor care on their children (D) both teachers and parents should learn more to deal with the problem of student suicides. [SEP] SHANGHAI, June 7(AP)--A 16-year-old girl's suicide after she was barred from a key exam draw attention to increasing worries over academic pressures, as millions of Chinese students began annual college entrance tests on Wednesday.  The three-day exam, viewed as important to future career and financial success, has a record 9.5 million high school students across prefix = st1 /Chinacompeting for just 2.6 million university places. For kids and parents alike, it's a difficulty that experts say causes extreme emotional distress. "Pressure from study and exams is a top reason for psychological problems among Chinese youth," said Jin Wuguan, director of the Youth Psychological Counseling Center at Shanghai'sRuijinHospital.  In China's increasingly success oriented, pressure-cooker cities, academic stress is seen as a rising cause of youth suicides and even murders of parents by children who are driven crazy by intolerable pressure to perform.  According to her family and newspaper accounts, 16-year-old Wu Wenwen drowned herself after she was stopped at the exam room door because her hair wasn't tied back as her school required. Returning in tied hair, she was then told the end-of-term exam had already started and she was too late to take it. In tears, Wu called her mother, and then disappeared. Her body was found the same night in a nearby lake.  China doesn't keep comprehensive statistics on student suicides, but Jin said health care professionals see the problem worsening, even among elementary students. Most Chinese schools still lack advisers and teachers receive little training in spotting symptoms of emotional distress, Jin said. Parents are little help, often piling on pressure while ignoring their children's emotional development, he said. "It's a basic unwillingness or inability to recognize and deal with with emotional problems," Jin said.  Wang Yufeng, of Peking University's Institute of Mental, estimates the rate of emotional disorders such as depression among Chinese students under age 17 at up to 32 percent , a total of 30 million students.  Others say that figure may be as high as 50 percent. A survey last year by the government's China Youth and ChildrenResearchCentershowed 57.6 percent of students felt highly distressed by academic pressures.
03/12/2022 17:00:29 - INFO - __main__ - ['both teachers and parents should learn more to deal with the problem of student suicides.']
03/12/2022 17:00:29 - INFO - __main__ -  [race-high] What is the aim of the Predator Compensation Program? (A) To protect people in the wild. (B) To protect Masai's farms. (C) To protect lions only. (D) To protect the wildlife. [SEP] At the beginning of the 20th century there were more than a million lions worldwide. Today there are less than 30,000 in the wild. The remaining lions are increasingly threatened by habitat loss, hunting and activities to protect farms and cattle. For generations, Masai tribesmen on the large African plains in southeastern Kenya have hunted lions -- to protect their farms and cattle. Today they celebrate the lions' life. Noah is an elder in the Masai community. "We have decided as a community of the Masai to lay down our spears, and there will be no more killing of lions in our community." He is part of a group of Masai visiting the United States promoting   the Predator Compensation Program. Conservation International's Frank Hawkins explains, "The Masai have been living with wildlife for many generations and it has been a conflicting ( ) relationship in many ways. They compete with the animals for food as lions eat their cattle. We're trying to find ways in which the wildlife will become something useful to them." They had the Predator Compensation Fund founded in 2003. After much discussion, a group of Masai farmers agreed to protect lions. In turn, if lions or other predators kill their cattle, the Masai owner will be paid market value for the dead animals from the fund. One man said that in the past, when a lion killed cattle, they killed it immediately. And now, after the start of the program, the Masai see the lion population growing. Since 2003, only four lions have been killed here.
03/12/2022 17:00:29 - INFO - __main__ - ['To protect the wildlife.']
03/12/2022 17:00:29 - INFO - __main__ - Tokenizing Input ...
03/12/2022 17:00:29 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:00:29 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 17:00:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 17:00:41 - INFO - __main__ - Starting training!
03/12/2022 17:00:46 - INFO - __main__ - Step 10 Global step 10 Train loss 20.032581 on epoch=4
03/12/2022 17:00:52 - INFO - __main__ - Step 20 Global step 20 Train loss 18.154980 on epoch=9
03/12/2022 17:00:59 - INFO - __main__ - Step 30 Global step 30 Train loss 11.969333 on epoch=14
03/12/2022 17:01:05 - INFO - __main__ - Step 40 Global step 40 Train loss 8.635262 on epoch=19
03/12/2022 17:01:11 - INFO - __main__ - Step 50 Global step 50 Train loss 6.201839 on epoch=24
03/12/2022 17:01:14 - INFO - __main__ - Global step 50 Train loss 12.998800 ACC 0.09375 on epoch=24
03/12/2022 17:01:21 - INFO - __main__ - Step 60 Global step 60 Train loss 5.268986 on epoch=29
03/12/2022 17:01:27 - INFO - __main__ - Step 70 Global step 70 Train loss 4.801692 on epoch=34
03/12/2022 17:01:33 - INFO - __main__ - Step 80 Global step 80 Train loss 4.522625 on epoch=39
03/12/2022 17:01:40 - INFO - __main__ - Step 90 Global step 90 Train loss 3.874231 on epoch=44
03/12/2022 17:01:46 - INFO - __main__ - Step 100 Global step 100 Train loss 4.567384 on epoch=49
03/12/2022 17:01:48 - INFO - __main__ - Global step 100 Train loss 4.606984 ACC 0.15625 on epoch=49
03/12/2022 17:01:55 - INFO - __main__ - Step 110 Global step 110 Train loss 4.122949 on epoch=54
03/12/2022 17:02:01 - INFO - __main__ - Step 120 Global step 120 Train loss 3.762245 on epoch=59
03/12/2022 17:02:07 - INFO - __main__ - Step 130 Global step 130 Train loss 3.752000 on epoch=64
03/12/2022 17:02:14 - INFO - __main__ - Step 140 Global step 140 Train loss 3.621702 on epoch=69
03/12/2022 17:02:20 - INFO - __main__ - Step 150 Global step 150 Train loss 3.391158 on epoch=74
03/12/2022 17:02:22 - INFO - __main__ - Global step 150 Train loss 3.730011 ACC 0.15625 on epoch=74
03/12/2022 17:02:28 - INFO - __main__ - Step 160 Global step 160 Train loss 3.295499 on epoch=79
03/12/2022 17:02:34 - INFO - __main__ - Step 170 Global step 170 Train loss 3.180766 on epoch=84
03/12/2022 17:02:40 - INFO - __main__ - Step 180 Global step 180 Train loss 3.302952 on epoch=89
03/12/2022 17:02:47 - INFO - __main__ - Step 190 Global step 190 Train loss 3.254160 on epoch=94
03/12/2022 17:02:52 - INFO - __main__ - Step 200 Global step 200 Train loss 3.060128 on epoch=99
03/12/2022 17:02:55 - INFO - __main__ - Global step 200 Train loss 3.218701 ACC 0.21875 on epoch=99
03/12/2022 17:03:01 - INFO - __main__ - Step 210 Global step 210 Train loss 3.043169 on epoch=104
03/12/2022 17:03:07 - INFO - __main__ - Step 220 Global step 220 Train loss 2.814444 on epoch=109
03/12/2022 17:03:13 - INFO - __main__ - Step 230 Global step 230 Train loss 2.635296 on epoch=114
03/12/2022 17:03:19 - INFO - __main__ - Step 240 Global step 240 Train loss 2.737661 on epoch=119
03/12/2022 17:03:25 - INFO - __main__ - Step 250 Global step 250 Train loss 2.612061 on epoch=124
03/12/2022 17:03:27 - INFO - __main__ - Global step 250 Train loss 2.768526 ACC 0.1875 on epoch=124
03/12/2022 17:03:33 - INFO - __main__ - Step 260 Global step 260 Train loss 2.589648 on epoch=129
03/12/2022 17:03:39 - INFO - __main__ - Step 270 Global step 270 Train loss 2.419675 on epoch=134
03/12/2022 17:03:45 - INFO - __main__ - Step 280 Global step 280 Train loss 2.572407 on epoch=139
03/12/2022 17:03:51 - INFO - __main__ - Step 290 Global step 290 Train loss 2.317928 on epoch=144
03/12/2022 17:03:57 - INFO - __main__ - Step 300 Global step 300 Train loss 2.190134 on epoch=149
03/12/2022 17:04:00 - INFO - __main__ - Global step 300 Train loss 2.417959 ACC 0.15625 on epoch=149
03/12/2022 17:04:06 - INFO - __main__ - Step 310 Global step 310 Train loss 2.177747 on epoch=154
03/12/2022 17:04:12 - INFO - __main__ - Step 320 Global step 320 Train loss 1.939242 on epoch=159
03/12/2022 17:04:18 - INFO - __main__ - Step 330 Global step 330 Train loss 1.867301 on epoch=164
03/12/2022 17:04:24 - INFO - __main__ - Step 340 Global step 340 Train loss 1.835989 on epoch=169
03/12/2022 17:04:30 - INFO - __main__ - Step 350 Global step 350 Train loss 1.835892 on epoch=174
03/12/2022 17:04:32 - INFO - __main__ - Global step 350 Train loss 1.931234 ACC 0.21875 on epoch=174
03/12/2022 17:04:38 - INFO - __main__ - Step 360 Global step 360 Train loss 1.576928 on epoch=179
03/12/2022 17:04:44 - INFO - __main__ - Step 370 Global step 370 Train loss 1.449555 on epoch=184
03/12/2022 17:04:50 - INFO - __main__ - Step 380 Global step 380 Train loss 1.698917 on epoch=189
03/12/2022 17:04:56 - INFO - __main__ - Step 390 Global step 390 Train loss 1.429559 on epoch=194
03/12/2022 17:05:02 - INFO - __main__ - Step 400 Global step 400 Train loss 1.487851 on epoch=199
03/12/2022 17:05:04 - INFO - __main__ - Global step 400 Train loss 1.528562 ACC 0.21875 on epoch=199
03/12/2022 17:05:10 - INFO - __main__ - Step 410 Global step 410 Train loss 1.369776 on epoch=204
03/12/2022 17:05:16 - INFO - __main__ - Step 420 Global step 420 Train loss 1.422531 on epoch=209
03/12/2022 17:05:22 - INFO - __main__ - Step 430 Global step 430 Train loss 1.432820 on epoch=214
03/12/2022 17:05:28 - INFO - __main__ - Step 440 Global step 440 Train loss 1.354270 on epoch=219
03/12/2022 17:05:34 - INFO - __main__ - Step 450 Global step 450 Train loss 1.431640 on epoch=224
03/12/2022 17:05:37 - INFO - __main__ - Global step 450 Train loss 1.402207 ACC 0.15625 on epoch=224
03/12/2022 17:05:43 - INFO - __main__ - Step 460 Global step 460 Train loss 1.297371 on epoch=229
03/12/2022 17:05:49 - INFO - __main__ - Step 470 Global step 470 Train loss 1.389821 on epoch=234
03/12/2022 17:05:55 - INFO - __main__ - Step 480 Global step 480 Train loss 1.323782 on epoch=239
03/12/2022 17:06:01 - INFO - __main__ - Step 490 Global step 490 Train loss 1.220502 on epoch=244
03/12/2022 17:06:07 - INFO - __main__ - Step 500 Global step 500 Train loss 1.447328 on epoch=249
03/12/2022 17:06:09 - INFO - __main__ - Global step 500 Train loss 1.335761 ACC 0.0 on epoch=249
03/12/2022 17:06:15 - INFO - __main__ - Step 510 Global step 510 Train loss 1.282651 on epoch=254
03/12/2022 17:06:21 - INFO - __main__ - Step 520 Global step 520 Train loss 1.168014 on epoch=259
03/12/2022 17:06:27 - INFO - __main__ - Step 530 Global step 530 Train loss 1.124113 on epoch=264
03/12/2022 17:06:33 - INFO - __main__ - Step 540 Global step 540 Train loss 1.248838 on epoch=269
03/12/2022 17:06:39 - INFO - __main__ - Step 550 Global step 550 Train loss 1.120349 on epoch=274
03/12/2022 17:06:42 - INFO - __main__ - Global step 550 Train loss 1.188793 ACC 0.0 on epoch=274
03/12/2022 17:06:48 - INFO - __main__ - Step 560 Global step 560 Train loss 1.201814 on epoch=279
03/12/2022 17:06:53 - INFO - __main__ - Step 570 Global step 570 Train loss 1.004801 on epoch=284
03/12/2022 17:06:59 - INFO - __main__ - Step 580 Global step 580 Train loss 1.015833 on epoch=289
03/12/2022 17:07:05 - INFO - __main__ - Step 590 Global step 590 Train loss 1.078343 on epoch=294
03/12/2022 17:07:11 - INFO - __main__ - Step 600 Global step 600 Train loss 1.232957 on epoch=299
03/12/2022 17:07:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:07:13 - INFO - __main__ - Printing 3 examples
03/12/2022 17:07:13 - INFO - __main__ -  [race-high] Dr La Farge's research is of great importance to   _  . (A) knowing what the plants during the Little Ice Age were like (B) understanding how ecosystems recover from glaciers. (C) regrowing many species that have been destroyed before. (D) figuring out the effects of melting ice caps on moss. [SEP] 400-year-old plants from the Little Ice Age were brought back to life, which could help us understand how the Earth will deal with climate change. Moss  found buried beneath the Teardrop glacier  on Ellesmere Island in Canada has been brought back to life. Findings suggest that these plants could help repopulate regions exposed by melting ice caps. Plants that were buried beneath thick ice in Canada more than 400 years ago and were thought to have frozen to death have been brought back to life by Canadian scientists. Samples of the moss plant, covered by the glacier during the Little Ice Age of 1550 to 1850 AD, were replanted in a lab at the University of Alberta and grew new stems . Researchers now think these findings can give indication as to how regions can recover as the ice covering them melts. Biologist Dr. Catherine La Farge and her team at the University of Alberta were exploring the region around the Teardrop glacier on Ellesmere Island. Ice on Ellesmere Island region has been melting at around four meters each year for the past nine years. This means that many areas of land that were previously covered by ice have since been exposed. Many ecosystems that were thought to have been destroyed during the Little Ice Age between 1550 and 1850 AD can now be studied, including many species that have never been studied before. While examining an exposed area of land, La Farge and her team discovered a small area of moss called Aulacomnium turgidum. It is a type of bryophyte  plant that mainly grows across Canada, the US and the Highlands of Scotland. Dr La Farge noticed that the moss had small patches of green stems, suggesting it is either growing again or can be encouraged to repopulate. Dr La Farge told the BBC, "When we looked at the samples in detail and brought them to the lab, I could see some of the stems actually had new growth of green branches, suggesting that these plants are growing again, and that _ When we think of thick areas of ice covering the landscape, we've always thought that plants have to come from refugia , never considering that land plants come from underneath a glacier. It's a whole world of what's coming out from underneath the glacier that really needs to be studied. The ice is disappearing pretty fast. We really have not examined all the biological systems that exist in the world; we don't know it all." Dr La Farge took samples of the moss and, using carbon-dating techniques, discovered that the plants date back to the Little Ice Age. Dr La Farge's team took the samples, planted them in dishes full of nutrient-rich potting soil and fed them with water. The samples were from four separate species including Aulacomnium turgidum, Distichium capillaceum, Encalypta procera and Syntrichia ruralis. The moss plants found by Dr La Farge are types of bryophytes. Bryophytes can survive long winters and regrow when the weather gets warmer. However, Dr La Farge was surprised that the plants buried under ice have survived into the twenty-first century. Her findings appear in proceedings of the National Academy of Sciences.
03/12/2022 17:07:13 - INFO - __main__ - ['understanding how ecosystems recover from glaciers.']
03/12/2022 17:07:13 - INFO - __main__ -  [race-high] Which of the following statements is TRUE? (A) The museum is opened all the year around. (B) You can usually pay your parking in check. (C) During bears home games visitors can park on the museum Campus for free. (D) After 4:00 p.m.you are not allowed to enter the museum. [SEP] The Field Museum Hours Regular hours are 9:00 a.m.--5:00 p.m. , daily. Last admission at 4:00 p.m. Open every day except Christmas. Admission We have several ticket choices for you to choose from. Advance tickets may be purchased at the will- call booth in person at the museum before 4:00 p.m. Getting Here The Field Museum is located on Chicago's Museum Campus; at 1400 S. Lake Shore Drive, just south of Roosevelt Rd. How to get here : by car or public transit or free trolley. Parking Visitor parking in all lots on the Museum Campus is $ 15.00 per day .This includes the Adler lot, the north garage, the Waldron garage and the east museum lot. Hours for the north garage and Adler lot are 5:00 a.m.--5:00 p.m. Mon--Fri and 6:00a.m. --5:00p.m. weekends; east museum lot9:00 a.m.--3:00p.m. Designated handicapped parking is available in every lot. When all other lots are full , parking will be made available in the remote south lot for$ 8.00 per day. From Memorial Day to Labor Day , the parking lot will only accept cash payments , which will need to be paid upon entering the garage. Please note : These hours and rates are for daytime only and do not apply when special events are scheduled at the museums or Soldier Field. Getting Here During Chicago Bears Home Games During bears home games and other major special events at Soldier Field , access to the Museum Campus can be challenging. No museum visitor parking is available on the Museum Campus during bears home games. However, public transit remains a great way to get to the Campus every day of the year. For more information, call the Regional Transportation Authority at (312) 836 -- 7000 or visit www. rtachicago.com. Additional parking is available at the Monroe Street garage , located at 350 East Monroe Street.
03/12/2022 17:07:13 - INFO - __main__ - ['After 4:00 p.m.you are not allowed to enter the museum.']
03/12/2022 17:07:13 - INFO - __main__ -  [race-high] Why isn't it a plain sailing? (A) No one can treat his mother well. (B) Dr Syed was the wrong blood group. (C) They didn't have money to be in hospital. (D) Mrs. Syed was unwilling to receive the operation. [SEP] Dr Asim Syed, 32, has performed more than 100 operations at London's Hammersmith Hospital in the country's busiest transplant unit, but never imagined that he would one day become a donor himself. He stepped forward when was told his 64-year-old mother might be dead within months unless she got a new kidney  . The worried surgeon brought her to London to be cared for at his hospital. However, it was not all plain sailing. Tests showed Dr Syed was the wrong blood group, so the only way was to go through a special blood-washing process. He consulted colleagues about that, but they didn't agree, because the risk of rejection is still too high. Dr Syed and his mother were then advised to consider a new way of donating and receiving, called an organ-paired. That is, Dr Syed donated his kidney to an unknown person and another donor in the chain was a successful match for his mother. The chain of three transplants took place at the same time on July 31 with Dr Syed's kidney going to a recipient in the Midlands and Mrs. Syed receiving her kidney from a person in the south of England. Just hours after donating his own kidney, Dr Syed found himself recovering in bed next to his mother. Mrs Syed said, "When I came round from my operation Asim was in the next bed and the first thing he said was, 'Mum now all your worries are over.' Tears fell down." Now mother and son are recovering well with Dr Syed already back at work. Mrs. Syed is staying with him for several months while the hospital monitors her progress. He said, "I did what anyone would do when they see a relative suffering disease. Although I wasn't able to help mum directly, by agreeing to be part of a chain, I was also very happy."
03/12/2022 17:07:13 - INFO - __main__ - ['Dr Syed was the wrong blood group.']
03/12/2022 17:07:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 17:07:13 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:07:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 17:07:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:07:13 - INFO - __main__ - Printing 3 examples
03/12/2022 17:07:13 - INFO - __main__ -  [race-high] The passage implies that   _  . (A) modern technology is bridging the people (B) modern technology is separating the people (C) modern technology is developing too fast (D) modern technology is interrupting our communication [SEP] A funny thing happened on the way to the communications revolution: we stopped talking to one another. I was walking in the park with a friend recently, and his cell phone rang, interrupting our conversation. There we were walking and talking on a beautiful sunny day and... I became invisible, absent from the conversation. The telephone used to connect you to the absent. Now it makes people sitting next to you feel absent. Why is it that the more connected we get, the more disconnected I feel? Every advance in communications technology is a tragedy to the closeness of human interaction . With email and instant messaging over the internet, we can now communicate without seeing or talking to one another. With voice mail, you can conduct  entire conversations without ever reaching anyone. If my mom  has a question, I just leave the answer on her machine. As almost every contact we can imagine between human  beings gets automated , the alienation index  goes up. You can't  even call a person to get the phone number of another person  any more. Directory assistance is almost always fully automated. I am not against modern technology. I own a cell phone,  an ATM card, a voice mail system, and an email account. Giving them up isn't wise... they're a great help to us. It's some of  their possible consequences that make me feel uneasy. More and more. I find myself hiding behind email to do a job meant for conversation. Or being relieved that voice mail   picked up because I didn't really have time to talk. The industry devoted to helping me keep in touch is making me lonelier ...or at least facilitating my antisocial instincts. So I've put myself on technology restriction: no instant   messaging with people who live near me, no cell phoning in the presence of friends, no letting the voice mail pick up when I'm at home.
03/12/2022 17:07:13 - INFO - __main__ - ['modern technology is separating the people']
03/12/2022 17:07:13 - INFO - __main__ -  [race-high] What's the main idea of the passage? (A) Some manners on visiting British and American people's home. (B) Different table manners between British and American people. (C) Different ideas about the home between British and American people (D) Different ideas about how to get along well with neighbors between British and American people. [SEP] For the British, the home is a private place in which he or she goes to hide away from the troubles of life. It is very seldom that one would be invited to a British person's home. It is rude to knock on a person's door if you are not invited. If you are invited, don't ask to see more than the downstairs that your British host invites you into. Never ask how much the house or any of the items in it cost. To the American, most of them want their home to be a place where they can entertain   and share their lives with their friends. They may be delighted to give you a full tour of their houses. They may also be pleased when you show your interest and pleasure in their houses. Both British and American people will _ quite a bit of chat and a drink or two before the meal is served. After the first mouthful, you should say how delicious the food is and ask something about it. Remember, never eat with your mouth open and make very little noise while eating. It would be nice of you to help your host in any way. Maybe offer to pour some drinks or clear up after the meal.
03/12/2022 17:07:13 - INFO - __main__ - ["Some manners on visiting British and American people's home."]
03/12/2022 17:07:13 - INFO - __main__ -  [race-high] What's the title of the passage? (A) Parents' responsibilities. (B) Advice on self-control. (C) Bad influences of celebrities. (D) Media's bad influences. [SEP] As we know, many teen celebrities  feel and think that having a slimmer figure can do great good to them. But, does size really matter? Are teenage fans trying hard to become like their celebrity idols ? Do celebrities really have the power to influence people, especially teenagers? For the longest time, many parents blame teen idols for influencing the way their kids act. Have you noticed how teens idolize the celebrities these days? Even, their personal affairs are being followed by kids these days. Take for example the case of Lindsay Lohan of Mary Kate Ashley. They are definitely famous teen stars. But, since they are trying to project an image to satisfy a lot of people in show business, their health and body suffer. Many kids are aware of this problem. But they are easily influenced by these celebrities to exercise and eat less. It is a fact that the media, and especially famous teen celebrities, can influence people powerfully. But teenagers are easily influenced because teenage years are the period when our personality and identity developments take place. Teens watching TV shows and reading magazines are easily pulled into the dieting and harmful eating habits because the media have some ways to pull these acts. They use thin models and celebrities to endorse  products or to star in an up-and -coming shows or movies. With fierce competition, celebrities are forced to eat less and do extreme exercise routines to get the roles or offers that come their way. Living in today's time and generation is a bit disturbing to a lot of parents. Media, especially as well as the celebrities, have a very powerful influence to drive teenagers to good or bad. It's good that we can control ourselves to avoid bad things from happening. If not, parents should really be aware and guide their teens to determine what's in ad what's out.
03/12/2022 17:07:13 - INFO - __main__ - ['Bad influences of celebrities.']
03/12/2022 17:07:13 - INFO - __main__ - Tokenizing Input ...
03/12/2022 17:07:13 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:07:13 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 17:07:14 - INFO - __main__ - Global step 600 Train loss 1.106750 ACC 0.0 on epoch=299
03/12/2022 17:07:14 - INFO - __main__ - save last model!
03/12/2022 17:07:21 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 17:07:21 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 17:07:21 - INFO - __main__ - Printing 3 examples
03/12/2022 17:07:21 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 17:07:21 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 17:07:21 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 17:07:21 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 17:07:21 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 17:07:21 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 17:07:21 - INFO - __main__ - Tokenizing Input ...
03/12/2022 17:07:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 17:07:25 - INFO - __main__ - Starting training!
03/12/2022 17:07:27 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:07:31 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 17:11:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_100_0.0001_8_predictions.txt
03/12/2022 17:11:17 - INFO - __main__ - ACC on test data: 0.1391
03/12/2022 17:11:18 - INFO - __main__ - prefix=race-high_32_100, lr=0.0001, bsz=8, dev_performance=0.21875, test_performance=0.13909011880614314
03/12/2022 17:11:18 - INFO - __main__ - Running ... prefix=race-high_32_13, lr=0.0005, bsz=8 ...
03/12/2022 17:11:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:11:19 - INFO - __main__ - Printing 3 examples
03/12/2022 17:11:19 - INFO - __main__ -  [race-high] Dr La Farge's research is of great importance to   _  . (A) knowing what the plants during the Little Ice Age were like (B) understanding how ecosystems recover from glaciers. (C) regrowing many species that have been destroyed before. (D) figuring out the effects of melting ice caps on moss. [SEP] 400-year-old plants from the Little Ice Age were brought back to life, which could help us understand how the Earth will deal with climate change. Moss  found buried beneath the Teardrop glacier  on Ellesmere Island in Canada has been brought back to life. Findings suggest that these plants could help repopulate regions exposed by melting ice caps. Plants that were buried beneath thick ice in Canada more than 400 years ago and were thought to have frozen to death have been brought back to life by Canadian scientists. Samples of the moss plant, covered by the glacier during the Little Ice Age of 1550 to 1850 AD, were replanted in a lab at the University of Alberta and grew new stems . Researchers now think these findings can give indication as to how regions can recover as the ice covering them melts. Biologist Dr. Catherine La Farge and her team at the University of Alberta were exploring the region around the Teardrop glacier on Ellesmere Island. Ice on Ellesmere Island region has been melting at around four meters each year for the past nine years. This means that many areas of land that were previously covered by ice have since been exposed. Many ecosystems that were thought to have been destroyed during the Little Ice Age between 1550 and 1850 AD can now be studied, including many species that have never been studied before. While examining an exposed area of land, La Farge and her team discovered a small area of moss called Aulacomnium turgidum. It is a type of bryophyte  plant that mainly grows across Canada, the US and the Highlands of Scotland. Dr La Farge noticed that the moss had small patches of green stems, suggesting it is either growing again or can be encouraged to repopulate. Dr La Farge told the BBC, "When we looked at the samples in detail and brought them to the lab, I could see some of the stems actually had new growth of green branches, suggesting that these plants are growing again, and that _ When we think of thick areas of ice covering the landscape, we've always thought that plants have to come from refugia , never considering that land plants come from underneath a glacier. It's a whole world of what's coming out from underneath the glacier that really needs to be studied. The ice is disappearing pretty fast. We really have not examined all the biological systems that exist in the world; we don't know it all." Dr La Farge took samples of the moss and, using carbon-dating techniques, discovered that the plants date back to the Little Ice Age. Dr La Farge's team took the samples, planted them in dishes full of nutrient-rich potting soil and fed them with water. The samples were from four separate species including Aulacomnium turgidum, Distichium capillaceum, Encalypta procera and Syntrichia ruralis. The moss plants found by Dr La Farge are types of bryophytes. Bryophytes can survive long winters and regrow when the weather gets warmer. However, Dr La Farge was surprised that the plants buried under ice have survived into the twenty-first century. Her findings appear in proceedings of the National Academy of Sciences.
03/12/2022 17:11:19 - INFO - __main__ - ['understanding how ecosystems recover from glaciers.']
03/12/2022 17:11:19 - INFO - __main__ -  [race-high] Which of the following statements is TRUE? (A) The museum is opened all the year around. (B) You can usually pay your parking in check. (C) During bears home games visitors can park on the museum Campus for free. (D) After 4:00 p.m.you are not allowed to enter the museum. [SEP] The Field Museum Hours Regular hours are 9:00 a.m.--5:00 p.m. , daily. Last admission at 4:00 p.m. Open every day except Christmas. Admission We have several ticket choices for you to choose from. Advance tickets may be purchased at the will- call booth in person at the museum before 4:00 p.m. Getting Here The Field Museum is located on Chicago's Museum Campus; at 1400 S. Lake Shore Drive, just south of Roosevelt Rd. How to get here : by car or public transit or free trolley. Parking Visitor parking in all lots on the Museum Campus is $ 15.00 per day .This includes the Adler lot, the north garage, the Waldron garage and the east museum lot. Hours for the north garage and Adler lot are 5:00 a.m.--5:00 p.m. Mon--Fri and 6:00a.m. --5:00p.m. weekends; east museum lot9:00 a.m.--3:00p.m. Designated handicapped parking is available in every lot. When all other lots are full , parking will be made available in the remote south lot for$ 8.00 per day. From Memorial Day to Labor Day , the parking lot will only accept cash payments , which will need to be paid upon entering the garage. Please note : These hours and rates are for daytime only and do not apply when special events are scheduled at the museums or Soldier Field. Getting Here During Chicago Bears Home Games During bears home games and other major special events at Soldier Field , access to the Museum Campus can be challenging. No museum visitor parking is available on the Museum Campus during bears home games. However, public transit remains a great way to get to the Campus every day of the year. For more information, call the Regional Transportation Authority at (312) 836 -- 7000 or visit www. rtachicago.com. Additional parking is available at the Monroe Street garage , located at 350 East Monroe Street.
03/12/2022 17:11:19 - INFO - __main__ - ['After 4:00 p.m.you are not allowed to enter the museum.']
03/12/2022 17:11:19 - INFO - __main__ -  [race-high] Why isn't it a plain sailing? (A) No one can treat his mother well. (B) Dr Syed was the wrong blood group. (C) They didn't have money to be in hospital. (D) Mrs. Syed was unwilling to receive the operation. [SEP] Dr Asim Syed, 32, has performed more than 100 operations at London's Hammersmith Hospital in the country's busiest transplant unit, but never imagined that he would one day become a donor himself. He stepped forward when was told his 64-year-old mother might be dead within months unless she got a new kidney  . The worried surgeon brought her to London to be cared for at his hospital. However, it was not all plain sailing. Tests showed Dr Syed was the wrong blood group, so the only way was to go through a special blood-washing process. He consulted colleagues about that, but they didn't agree, because the risk of rejection is still too high. Dr Syed and his mother were then advised to consider a new way of donating and receiving, called an organ-paired. That is, Dr Syed donated his kidney to an unknown person and another donor in the chain was a successful match for his mother. The chain of three transplants took place at the same time on July 31 with Dr Syed's kidney going to a recipient in the Midlands and Mrs. Syed receiving her kidney from a person in the south of England. Just hours after donating his own kidney, Dr Syed found himself recovering in bed next to his mother. Mrs Syed said, "When I came round from my operation Asim was in the next bed and the first thing he said was, 'Mum now all your worries are over.' Tears fell down." Now mother and son are recovering well with Dr Syed already back at work. Mrs. Syed is staying with him for several months while the hospital monitors her progress. He said, "I did what anyone would do when they see a relative suffering disease. Although I wasn't able to help mum directly, by agreeing to be part of a chain, I was also very happy."
03/12/2022 17:11:19 - INFO - __main__ - ['Dr Syed was the wrong blood group.']
03/12/2022 17:11:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 17:11:19 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:11:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 17:11:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:11:19 - INFO - __main__ - Printing 3 examples
03/12/2022 17:11:19 - INFO - __main__ -  [race-high] The passage implies that   _  . (A) modern technology is bridging the people (B) modern technology is separating the people (C) modern technology is developing too fast (D) modern technology is interrupting our communication [SEP] A funny thing happened on the way to the communications revolution: we stopped talking to one another. I was walking in the park with a friend recently, and his cell phone rang, interrupting our conversation. There we were walking and talking on a beautiful sunny day and... I became invisible, absent from the conversation. The telephone used to connect you to the absent. Now it makes people sitting next to you feel absent. Why is it that the more connected we get, the more disconnected I feel? Every advance in communications technology is a tragedy to the closeness of human interaction . With email and instant messaging over the internet, we can now communicate without seeing or talking to one another. With voice mail, you can conduct  entire conversations without ever reaching anyone. If my mom  has a question, I just leave the answer on her machine. As almost every contact we can imagine between human  beings gets automated , the alienation index  goes up. You can't  even call a person to get the phone number of another person  any more. Directory assistance is almost always fully automated. I am not against modern technology. I own a cell phone,  an ATM card, a voice mail system, and an email account. Giving them up isn't wise... they're a great help to us. It's some of  their possible consequences that make me feel uneasy. More and more. I find myself hiding behind email to do a job meant for conversation. Or being relieved that voice mail   picked up because I didn't really have time to talk. The industry devoted to helping me keep in touch is making me lonelier ...or at least facilitating my antisocial instincts. So I've put myself on technology restriction: no instant   messaging with people who live near me, no cell phoning in the presence of friends, no letting the voice mail pick up when I'm at home.
03/12/2022 17:11:19 - INFO - __main__ - ['modern technology is separating the people']
03/12/2022 17:11:19 - INFO - __main__ -  [race-high] What's the main idea of the passage? (A) Some manners on visiting British and American people's home. (B) Different table manners between British and American people. (C) Different ideas about the home between British and American people (D) Different ideas about how to get along well with neighbors between British and American people. [SEP] For the British, the home is a private place in which he or she goes to hide away from the troubles of life. It is very seldom that one would be invited to a British person's home. It is rude to knock on a person's door if you are not invited. If you are invited, don't ask to see more than the downstairs that your British host invites you into. Never ask how much the house or any of the items in it cost. To the American, most of them want their home to be a place where they can entertain   and share their lives with their friends. They may be delighted to give you a full tour of their houses. They may also be pleased when you show your interest and pleasure in their houses. Both British and American people will _ quite a bit of chat and a drink or two before the meal is served. After the first mouthful, you should say how delicious the food is and ask something about it. Remember, never eat with your mouth open and make very little noise while eating. It would be nice of you to help your host in any way. Maybe offer to pour some drinks or clear up after the meal.
03/12/2022 17:11:19 - INFO - __main__ - ["Some manners on visiting British and American people's home."]
03/12/2022 17:11:19 - INFO - __main__ -  [race-high] What's the title of the passage? (A) Parents' responsibilities. (B) Advice on self-control. (C) Bad influences of celebrities. (D) Media's bad influences. [SEP] As we know, many teen celebrities  feel and think that having a slimmer figure can do great good to them. But, does size really matter? Are teenage fans trying hard to become like their celebrity idols ? Do celebrities really have the power to influence people, especially teenagers? For the longest time, many parents blame teen idols for influencing the way their kids act. Have you noticed how teens idolize the celebrities these days? Even, their personal affairs are being followed by kids these days. Take for example the case of Lindsay Lohan of Mary Kate Ashley. They are definitely famous teen stars. But, since they are trying to project an image to satisfy a lot of people in show business, their health and body suffer. Many kids are aware of this problem. But they are easily influenced by these celebrities to exercise and eat less. It is a fact that the media, and especially famous teen celebrities, can influence people powerfully. But teenagers are easily influenced because teenage years are the period when our personality and identity developments take place. Teens watching TV shows and reading magazines are easily pulled into the dieting and harmful eating habits because the media have some ways to pull these acts. They use thin models and celebrities to endorse  products or to star in an up-and -coming shows or movies. With fierce competition, celebrities are forced to eat less and do extreme exercise routines to get the roles or offers that come their way. Living in today's time and generation is a bit disturbing to a lot of parents. Media, especially as well as the celebrities, have a very powerful influence to drive teenagers to good or bad. It's good that we can control ourselves to avoid bad things from happening. If not, parents should really be aware and guide their teens to determine what's in ad what's out.
03/12/2022 17:11:19 - INFO - __main__ - ['Bad influences of celebrities.']
03/12/2022 17:11:19 - INFO - __main__ - Tokenizing Input ...
03/12/2022 17:11:19 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:11:19 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 17:11:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 17:11:30 - INFO - __main__ - Starting training!
03/12/2022 17:11:35 - INFO - __main__ - Step 10 Global step 10 Train loss 19.132030 on epoch=4
03/12/2022 17:11:41 - INFO - __main__ - Step 20 Global step 20 Train loss 16.478012 on epoch=9
03/12/2022 17:11:46 - INFO - __main__ - Step 30 Global step 30 Train loss 12.573313 on epoch=14
03/12/2022 17:11:53 - INFO - __main__ - Step 40 Global step 40 Train loss 7.550172 on epoch=19
03/12/2022 17:11:59 - INFO - __main__ - Step 50 Global step 50 Train loss 5.058710 on epoch=24
03/12/2022 17:12:11 - INFO - __main__ - Global step 50 Train loss 12.158447 ACC 0.0 on epoch=24
03/12/2022 17:12:18 - INFO - __main__ - Step 60 Global step 60 Train loss 3.647725 on epoch=29
03/12/2022 17:12:24 - INFO - __main__ - Step 70 Global step 70 Train loss 3.214272 on epoch=34
03/12/2022 17:12:31 - INFO - __main__ - Step 80 Global step 80 Train loss 2.761586 on epoch=39
03/12/2022 17:12:37 - INFO - __main__ - Step 90 Global step 90 Train loss 2.267635 on epoch=44
03/12/2022 17:12:43 - INFO - __main__ - Step 100 Global step 100 Train loss 2.120941 on epoch=49
03/12/2022 17:12:51 - INFO - __main__ - Global step 100 Train loss 2.802432 ACC 0.0 on epoch=49
03/12/2022 17:12:57 - INFO - __main__ - Step 110 Global step 110 Train loss 1.609746 on epoch=54
03/12/2022 17:13:03 - INFO - __main__ - Step 120 Global step 120 Train loss 1.438362 on epoch=59
03/12/2022 17:13:09 - INFO - __main__ - Step 130 Global step 130 Train loss 1.199407 on epoch=64
03/12/2022 17:13:16 - INFO - __main__ - Step 140 Global step 140 Train loss 1.118542 on epoch=69
03/12/2022 17:13:22 - INFO - __main__ - Step 150 Global step 150 Train loss 1.268918 on epoch=74
03/12/2022 17:13:25 - INFO - __main__ - Global step 150 Train loss 1.326995 ACC 0.0 on epoch=74
03/12/2022 17:13:31 - INFO - __main__ - Step 160 Global step 160 Train loss 1.123066 on epoch=79
03/12/2022 17:13:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.085229 on epoch=84
03/12/2022 17:13:44 - INFO - __main__ - Step 180 Global step 180 Train loss 1.101571 on epoch=89
03/12/2022 17:13:50 - INFO - __main__ - Step 190 Global step 190 Train loss 1.005664 on epoch=94
03/12/2022 17:13:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.944575 on epoch=99
03/12/2022 17:13:58 - INFO - __main__ - Global step 200 Train loss 1.052021 ACC 0.0 on epoch=99
03/12/2022 17:14:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.777621 on epoch=104
03/12/2022 17:14:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.851487 on epoch=109
03/12/2022 17:14:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.822206 on epoch=114
03/12/2022 17:14:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.816177 on epoch=119
03/12/2022 17:14:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.750469 on epoch=124
03/12/2022 17:14:31 - INFO - __main__ - Global step 250 Train loss 0.803592 ACC 0.0 on epoch=124
03/12/2022 17:14:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.691636 on epoch=129
03/12/2022 17:14:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.723208 on epoch=134
03/12/2022 17:14:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.704312 on epoch=139
03/12/2022 17:14:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.677334 on epoch=144
03/12/2022 17:15:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.649378 on epoch=149
03/12/2022 17:15:04 - INFO - __main__ - Global step 300 Train loss 0.689174 ACC 0.0 on epoch=149
03/12/2022 17:15:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.638643 on epoch=154
03/12/2022 17:15:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.596103 on epoch=159
03/12/2022 17:15:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.669708 on epoch=164
03/12/2022 17:15:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.625536 on epoch=169
03/12/2022 17:15:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.639185 on epoch=174
03/12/2022 17:15:37 - INFO - __main__ - Global step 350 Train loss 0.633835 ACC 0.0 on epoch=174
03/12/2022 17:15:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.538859 on epoch=179
03/12/2022 17:15:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.621705 on epoch=184
03/12/2022 17:15:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.571629 on epoch=189
03/12/2022 17:16:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.546106 on epoch=194
03/12/2022 17:16:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.560836 on epoch=199
03/12/2022 17:16:10 - INFO - __main__ - Global step 400 Train loss 0.567827 ACC 0.0 on epoch=199
03/12/2022 17:16:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.518046 on epoch=204
03/12/2022 17:16:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.512614 on epoch=209
03/12/2022 17:16:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.537196 on epoch=214
03/12/2022 17:16:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.482791 on epoch=219
03/12/2022 17:16:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.520801 on epoch=224
03/12/2022 17:16:43 - INFO - __main__ - Global step 450 Train loss 0.514290 ACC 0.0 on epoch=224
03/12/2022 17:16:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.448133 on epoch=229
03/12/2022 17:16:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.442667 on epoch=234
03/12/2022 17:17:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.496995 on epoch=239
03/12/2022 17:17:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.432149 on epoch=244
03/12/2022 17:17:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.470086 on epoch=249
03/12/2022 17:17:15 - INFO - __main__ - Global step 500 Train loss 0.458006 ACC 0.0 on epoch=249
03/12/2022 17:17:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.449823 on epoch=254
03/12/2022 17:17:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.451054 on epoch=259
03/12/2022 17:17:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.462355 on epoch=264
03/12/2022 17:17:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.450551 on epoch=269
03/12/2022 17:17:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.435987 on epoch=274
03/12/2022 17:17:48 - INFO - __main__ - Global step 550 Train loss 0.449954 ACC 0.0 on epoch=274
03/12/2022 17:17:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.445783 on epoch=279
03/12/2022 17:18:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.429444 on epoch=284
03/12/2022 17:18:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.473662 on epoch=289
03/12/2022 17:18:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.440989 on epoch=294
03/12/2022 17:18:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.456642 on epoch=299
03/12/2022 17:18:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:18:20 - INFO - __main__ - Printing 3 examples
03/12/2022 17:18:20 - INFO - __main__ -  [race-high] Dr La Farge's research is of great importance to   _  . (A) knowing what the plants during the Little Ice Age were like (B) understanding how ecosystems recover from glaciers. (C) regrowing many species that have been destroyed before. (D) figuring out the effects of melting ice caps on moss. [SEP] 400-year-old plants from the Little Ice Age were brought back to life, which could help us understand how the Earth will deal with climate change. Moss  found buried beneath the Teardrop glacier  on Ellesmere Island in Canada has been brought back to life. Findings suggest that these plants could help repopulate regions exposed by melting ice caps. Plants that were buried beneath thick ice in Canada more than 400 years ago and were thought to have frozen to death have been brought back to life by Canadian scientists. Samples of the moss plant, covered by the glacier during the Little Ice Age of 1550 to 1850 AD, were replanted in a lab at the University of Alberta and grew new stems . Researchers now think these findings can give indication as to how regions can recover as the ice covering them melts. Biologist Dr. Catherine La Farge and her team at the University of Alberta were exploring the region around the Teardrop glacier on Ellesmere Island. Ice on Ellesmere Island region has been melting at around four meters each year for the past nine years. This means that many areas of land that were previously covered by ice have since been exposed. Many ecosystems that were thought to have been destroyed during the Little Ice Age between 1550 and 1850 AD can now be studied, including many species that have never been studied before. While examining an exposed area of land, La Farge and her team discovered a small area of moss called Aulacomnium turgidum. It is a type of bryophyte  plant that mainly grows across Canada, the US and the Highlands of Scotland. Dr La Farge noticed that the moss had small patches of green stems, suggesting it is either growing again or can be encouraged to repopulate. Dr La Farge told the BBC, "When we looked at the samples in detail and brought them to the lab, I could see some of the stems actually had new growth of green branches, suggesting that these plants are growing again, and that _ When we think of thick areas of ice covering the landscape, we've always thought that plants have to come from refugia , never considering that land plants come from underneath a glacier. It's a whole world of what's coming out from underneath the glacier that really needs to be studied. The ice is disappearing pretty fast. We really have not examined all the biological systems that exist in the world; we don't know it all." Dr La Farge took samples of the moss and, using carbon-dating techniques, discovered that the plants date back to the Little Ice Age. Dr La Farge's team took the samples, planted them in dishes full of nutrient-rich potting soil and fed them with water. The samples were from four separate species including Aulacomnium turgidum, Distichium capillaceum, Encalypta procera and Syntrichia ruralis. The moss plants found by Dr La Farge are types of bryophytes. Bryophytes can survive long winters and regrow when the weather gets warmer. However, Dr La Farge was surprised that the plants buried under ice have survived into the twenty-first century. Her findings appear in proceedings of the National Academy of Sciences.
03/12/2022 17:18:20 - INFO - __main__ - ['understanding how ecosystems recover from glaciers.']
03/12/2022 17:18:20 - INFO - __main__ -  [race-high] Which of the following statements is TRUE? (A) The museum is opened all the year around. (B) You can usually pay your parking in check. (C) During bears home games visitors can park on the museum Campus for free. (D) After 4:00 p.m.you are not allowed to enter the museum. [SEP] The Field Museum Hours Regular hours are 9:00 a.m.--5:00 p.m. , daily. Last admission at 4:00 p.m. Open every day except Christmas. Admission We have several ticket choices for you to choose from. Advance tickets may be purchased at the will- call booth in person at the museum before 4:00 p.m. Getting Here The Field Museum is located on Chicago's Museum Campus; at 1400 S. Lake Shore Drive, just south of Roosevelt Rd. How to get here : by car or public transit or free trolley. Parking Visitor parking in all lots on the Museum Campus is $ 15.00 per day .This includes the Adler lot, the north garage, the Waldron garage and the east museum lot. Hours for the north garage and Adler lot are 5:00 a.m.--5:00 p.m. Mon--Fri and 6:00a.m. --5:00p.m. weekends; east museum lot9:00 a.m.--3:00p.m. Designated handicapped parking is available in every lot. When all other lots are full , parking will be made available in the remote south lot for$ 8.00 per day. From Memorial Day to Labor Day , the parking lot will only accept cash payments , which will need to be paid upon entering the garage. Please note : These hours and rates are for daytime only and do not apply when special events are scheduled at the museums or Soldier Field. Getting Here During Chicago Bears Home Games During bears home games and other major special events at Soldier Field , access to the Museum Campus can be challenging. No museum visitor parking is available on the Museum Campus during bears home games. However, public transit remains a great way to get to the Campus every day of the year. For more information, call the Regional Transportation Authority at (312) 836 -- 7000 or visit www. rtachicago.com. Additional parking is available at the Monroe Street garage , located at 350 East Monroe Street.
03/12/2022 17:18:20 - INFO - __main__ - ['After 4:00 p.m.you are not allowed to enter the museum.']
03/12/2022 17:18:20 - INFO - __main__ -  [race-high] Why isn't it a plain sailing? (A) No one can treat his mother well. (B) Dr Syed was the wrong blood group. (C) They didn't have money to be in hospital. (D) Mrs. Syed was unwilling to receive the operation. [SEP] Dr Asim Syed, 32, has performed more than 100 operations at London's Hammersmith Hospital in the country's busiest transplant unit, but never imagined that he would one day become a donor himself. He stepped forward when was told his 64-year-old mother might be dead within months unless she got a new kidney  . The worried surgeon brought her to London to be cared for at his hospital. However, it was not all plain sailing. Tests showed Dr Syed was the wrong blood group, so the only way was to go through a special blood-washing process. He consulted colleagues about that, but they didn't agree, because the risk of rejection is still too high. Dr Syed and his mother were then advised to consider a new way of donating and receiving, called an organ-paired. That is, Dr Syed donated his kidney to an unknown person and another donor in the chain was a successful match for his mother. The chain of three transplants took place at the same time on July 31 with Dr Syed's kidney going to a recipient in the Midlands and Mrs. Syed receiving her kidney from a person in the south of England. Just hours after donating his own kidney, Dr Syed found himself recovering in bed next to his mother. Mrs Syed said, "When I came round from my operation Asim was in the next bed and the first thing he said was, 'Mum now all your worries are over.' Tears fell down." Now mother and son are recovering well with Dr Syed already back at work. Mrs. Syed is staying with him for several months while the hospital monitors her progress. He said, "I did what anyone would do when they see a relative suffering disease. Although I wasn't able to help mum directly, by agreeing to be part of a chain, I was also very happy."
03/12/2022 17:18:20 - INFO - __main__ - ['Dr Syed was the wrong blood group.']
03/12/2022 17:18:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 17:18:20 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:18:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 17:18:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:18:20 - INFO - __main__ - Printing 3 examples
03/12/2022 17:18:20 - INFO - __main__ -  [race-high] The passage implies that   _  . (A) modern technology is bridging the people (B) modern technology is separating the people (C) modern technology is developing too fast (D) modern technology is interrupting our communication [SEP] A funny thing happened on the way to the communications revolution: we stopped talking to one another. I was walking in the park with a friend recently, and his cell phone rang, interrupting our conversation. There we were walking and talking on a beautiful sunny day and... I became invisible, absent from the conversation. The telephone used to connect you to the absent. Now it makes people sitting next to you feel absent. Why is it that the more connected we get, the more disconnected I feel? Every advance in communications technology is a tragedy to the closeness of human interaction . With email and instant messaging over the internet, we can now communicate without seeing or talking to one another. With voice mail, you can conduct  entire conversations without ever reaching anyone. If my mom  has a question, I just leave the answer on her machine. As almost every contact we can imagine between human  beings gets automated , the alienation index  goes up. You can't  even call a person to get the phone number of another person  any more. Directory assistance is almost always fully automated. I am not against modern technology. I own a cell phone,  an ATM card, a voice mail system, and an email account. Giving them up isn't wise... they're a great help to us. It's some of  their possible consequences that make me feel uneasy. More and more. I find myself hiding behind email to do a job meant for conversation. Or being relieved that voice mail   picked up because I didn't really have time to talk. The industry devoted to helping me keep in touch is making me lonelier ...or at least facilitating my antisocial instincts. So I've put myself on technology restriction: no instant   messaging with people who live near me, no cell phoning in the presence of friends, no letting the voice mail pick up when I'm at home.
03/12/2022 17:18:20 - INFO - __main__ - ['modern technology is separating the people']
03/12/2022 17:18:20 - INFO - __main__ -  [race-high] What's the main idea of the passage? (A) Some manners on visiting British and American people's home. (B) Different table manners between British and American people. (C) Different ideas about the home between British and American people (D) Different ideas about how to get along well with neighbors between British and American people. [SEP] For the British, the home is a private place in which he or she goes to hide away from the troubles of life. It is very seldom that one would be invited to a British person's home. It is rude to knock on a person's door if you are not invited. If you are invited, don't ask to see more than the downstairs that your British host invites you into. Never ask how much the house or any of the items in it cost. To the American, most of them want their home to be a place where they can entertain   and share their lives with their friends. They may be delighted to give you a full tour of their houses. They may also be pleased when you show your interest and pleasure in their houses. Both British and American people will _ quite a bit of chat and a drink or two before the meal is served. After the first mouthful, you should say how delicious the food is and ask something about it. Remember, never eat with your mouth open and make very little noise while eating. It would be nice of you to help your host in any way. Maybe offer to pour some drinks or clear up after the meal.
03/12/2022 17:18:20 - INFO - __main__ - ["Some manners on visiting British and American people's home."]
03/12/2022 17:18:20 - INFO - __main__ -  [race-high] What's the title of the passage? (A) Parents' responsibilities. (B) Advice on self-control. (C) Bad influences of celebrities. (D) Media's bad influences. [SEP] As we know, many teen celebrities  feel and think that having a slimmer figure can do great good to them. But, does size really matter? Are teenage fans trying hard to become like their celebrity idols ? Do celebrities really have the power to influence people, especially teenagers? For the longest time, many parents blame teen idols for influencing the way their kids act. Have you noticed how teens idolize the celebrities these days? Even, their personal affairs are being followed by kids these days. Take for example the case of Lindsay Lohan of Mary Kate Ashley. They are definitely famous teen stars. But, since they are trying to project an image to satisfy a lot of people in show business, their health and body suffer. Many kids are aware of this problem. But they are easily influenced by these celebrities to exercise and eat less. It is a fact that the media, and especially famous teen celebrities, can influence people powerfully. But teenagers are easily influenced because teenage years are the period when our personality and identity developments take place. Teens watching TV shows and reading magazines are easily pulled into the dieting and harmful eating habits because the media have some ways to pull these acts. They use thin models and celebrities to endorse  products or to star in an up-and -coming shows or movies. With fierce competition, celebrities are forced to eat less and do extreme exercise routines to get the roles or offers that come their way. Living in today's time and generation is a bit disturbing to a lot of parents. Media, especially as well as the celebrities, have a very powerful influence to drive teenagers to good or bad. It's good that we can control ourselves to avoid bad things from happening. If not, parents should really be aware and guide their teens to determine what's in ad what's out.
03/12/2022 17:18:20 - INFO - __main__ - ['Bad influences of celebrities.']
03/12/2022 17:18:20 - INFO - __main__ - Tokenizing Input ...
03/12/2022 17:18:20 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:18:20 - INFO - __main__ - Global step 600 Train loss 0.449304 ACC 0.0 on epoch=299
03/12/2022 17:18:20 - INFO - __main__ - save last model!
03/12/2022 17:18:20 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 17:18:27 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 17:18:28 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 17:18:28 - INFO - __main__ - Printing 3 examples
03/12/2022 17:18:28 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 17:18:28 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 17:18:28 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 17:18:28 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 17:18:28 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 17:18:28 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 17:18:28 - INFO - __main__ - Tokenizing Input ...
03/12/2022 17:18:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 17:18:33 - INFO - __main__ - Starting training!
03/12/2022 17:18:34 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:18:37 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 17:40:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_13_0.0005_8_predictions.txt
03/12/2022 17:40:44 - INFO - __main__ - ACC on test data: 0.0000
03/12/2022 17:40:45 - INFO - __main__ - prefix=race-high_32_13, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.0
03/12/2022 17:40:45 - INFO - __main__ - Running ... prefix=race-high_32_13, lr=0.0003, bsz=8 ...
03/12/2022 17:40:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:40:45 - INFO - __main__ - Printing 3 examples
03/12/2022 17:40:45 - INFO - __main__ -  [race-high] Dr La Farge's research is of great importance to   _  . (A) knowing what the plants during the Little Ice Age were like (B) understanding how ecosystems recover from glaciers. (C) regrowing many species that have been destroyed before. (D) figuring out the effects of melting ice caps on moss. [SEP] 400-year-old plants from the Little Ice Age were brought back to life, which could help us understand how the Earth will deal with climate change. Moss  found buried beneath the Teardrop glacier  on Ellesmere Island in Canada has been brought back to life. Findings suggest that these plants could help repopulate regions exposed by melting ice caps. Plants that were buried beneath thick ice in Canada more than 400 years ago and were thought to have frozen to death have been brought back to life by Canadian scientists. Samples of the moss plant, covered by the glacier during the Little Ice Age of 1550 to 1850 AD, were replanted in a lab at the University of Alberta and grew new stems . Researchers now think these findings can give indication as to how regions can recover as the ice covering them melts. Biologist Dr. Catherine La Farge and her team at the University of Alberta were exploring the region around the Teardrop glacier on Ellesmere Island. Ice on Ellesmere Island region has been melting at around four meters each year for the past nine years. This means that many areas of land that were previously covered by ice have since been exposed. Many ecosystems that were thought to have been destroyed during the Little Ice Age between 1550 and 1850 AD can now be studied, including many species that have never been studied before. While examining an exposed area of land, La Farge and her team discovered a small area of moss called Aulacomnium turgidum. It is a type of bryophyte  plant that mainly grows across Canada, the US and the Highlands of Scotland. Dr La Farge noticed that the moss had small patches of green stems, suggesting it is either growing again or can be encouraged to repopulate. Dr La Farge told the BBC, "When we looked at the samples in detail and brought them to the lab, I could see some of the stems actually had new growth of green branches, suggesting that these plants are growing again, and that _ When we think of thick areas of ice covering the landscape, we've always thought that plants have to come from refugia , never considering that land plants come from underneath a glacier. It's a whole world of what's coming out from underneath the glacier that really needs to be studied. The ice is disappearing pretty fast. We really have not examined all the biological systems that exist in the world; we don't know it all." Dr La Farge took samples of the moss and, using carbon-dating techniques, discovered that the plants date back to the Little Ice Age. Dr La Farge's team took the samples, planted them in dishes full of nutrient-rich potting soil and fed them with water. The samples were from four separate species including Aulacomnium turgidum, Distichium capillaceum, Encalypta procera and Syntrichia ruralis. The moss plants found by Dr La Farge are types of bryophytes. Bryophytes can survive long winters and regrow when the weather gets warmer. However, Dr La Farge was surprised that the plants buried under ice have survived into the twenty-first century. Her findings appear in proceedings of the National Academy of Sciences.
03/12/2022 17:40:45 - INFO - __main__ - ['understanding how ecosystems recover from glaciers.']
03/12/2022 17:40:45 - INFO - __main__ -  [race-high] Which of the following statements is TRUE? (A) The museum is opened all the year around. (B) You can usually pay your parking in check. (C) During bears home games visitors can park on the museum Campus for free. (D) After 4:00 p.m.you are not allowed to enter the museum. [SEP] The Field Museum Hours Regular hours are 9:00 a.m.--5:00 p.m. , daily. Last admission at 4:00 p.m. Open every day except Christmas. Admission We have several ticket choices for you to choose from. Advance tickets may be purchased at the will- call booth in person at the museum before 4:00 p.m. Getting Here The Field Museum is located on Chicago's Museum Campus; at 1400 S. Lake Shore Drive, just south of Roosevelt Rd. How to get here : by car or public transit or free trolley. Parking Visitor parking in all lots on the Museum Campus is $ 15.00 per day .This includes the Adler lot, the north garage, the Waldron garage and the east museum lot. Hours for the north garage and Adler lot are 5:00 a.m.--5:00 p.m. Mon--Fri and 6:00a.m. --5:00p.m. weekends; east museum lot9:00 a.m.--3:00p.m. Designated handicapped parking is available in every lot. When all other lots are full , parking will be made available in the remote south lot for$ 8.00 per day. From Memorial Day to Labor Day , the parking lot will only accept cash payments , which will need to be paid upon entering the garage. Please note : These hours and rates are for daytime only and do not apply when special events are scheduled at the museums or Soldier Field. Getting Here During Chicago Bears Home Games During bears home games and other major special events at Soldier Field , access to the Museum Campus can be challenging. No museum visitor parking is available on the Museum Campus during bears home games. However, public transit remains a great way to get to the Campus every day of the year. For more information, call the Regional Transportation Authority at (312) 836 -- 7000 or visit www. rtachicago.com. Additional parking is available at the Monroe Street garage , located at 350 East Monroe Street.
03/12/2022 17:40:45 - INFO - __main__ - ['After 4:00 p.m.you are not allowed to enter the museum.']
03/12/2022 17:40:45 - INFO - __main__ -  [race-high] Why isn't it a plain sailing? (A) No one can treat his mother well. (B) Dr Syed was the wrong blood group. (C) They didn't have money to be in hospital. (D) Mrs. Syed was unwilling to receive the operation. [SEP] Dr Asim Syed, 32, has performed more than 100 operations at London's Hammersmith Hospital in the country's busiest transplant unit, but never imagined that he would one day become a donor himself. He stepped forward when was told his 64-year-old mother might be dead within months unless she got a new kidney  . The worried surgeon brought her to London to be cared for at his hospital. However, it was not all plain sailing. Tests showed Dr Syed was the wrong blood group, so the only way was to go through a special blood-washing process. He consulted colleagues about that, but they didn't agree, because the risk of rejection is still too high. Dr Syed and his mother were then advised to consider a new way of donating and receiving, called an organ-paired. That is, Dr Syed donated his kidney to an unknown person and another donor in the chain was a successful match for his mother. The chain of three transplants took place at the same time on July 31 with Dr Syed's kidney going to a recipient in the Midlands and Mrs. Syed receiving her kidney from a person in the south of England. Just hours after donating his own kidney, Dr Syed found himself recovering in bed next to his mother. Mrs Syed said, "When I came round from my operation Asim was in the next bed and the first thing he said was, 'Mum now all your worries are over.' Tears fell down." Now mother and son are recovering well with Dr Syed already back at work. Mrs. Syed is staying with him for several months while the hospital monitors her progress. He said, "I did what anyone would do when they see a relative suffering disease. Although I wasn't able to help mum directly, by agreeing to be part of a chain, I was also very happy."
03/12/2022 17:40:45 - INFO - __main__ - ['Dr Syed was the wrong blood group.']
03/12/2022 17:40:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 17:40:45 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:40:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 17:40:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:40:46 - INFO - __main__ - Printing 3 examples
03/12/2022 17:40:46 - INFO - __main__ -  [race-high] The passage implies that   _  . (A) modern technology is bridging the people (B) modern technology is separating the people (C) modern technology is developing too fast (D) modern technology is interrupting our communication [SEP] A funny thing happened on the way to the communications revolution: we stopped talking to one another. I was walking in the park with a friend recently, and his cell phone rang, interrupting our conversation. There we were walking and talking on a beautiful sunny day and... I became invisible, absent from the conversation. The telephone used to connect you to the absent. Now it makes people sitting next to you feel absent. Why is it that the more connected we get, the more disconnected I feel? Every advance in communications technology is a tragedy to the closeness of human interaction . With email and instant messaging over the internet, we can now communicate without seeing or talking to one another. With voice mail, you can conduct  entire conversations without ever reaching anyone. If my mom  has a question, I just leave the answer on her machine. As almost every contact we can imagine between human  beings gets automated , the alienation index  goes up. You can't  even call a person to get the phone number of another person  any more. Directory assistance is almost always fully automated. I am not against modern technology. I own a cell phone,  an ATM card, a voice mail system, and an email account. Giving them up isn't wise... they're a great help to us. It's some of  their possible consequences that make me feel uneasy. More and more. I find myself hiding behind email to do a job meant for conversation. Or being relieved that voice mail   picked up because I didn't really have time to talk. The industry devoted to helping me keep in touch is making me lonelier ...or at least facilitating my antisocial instincts. So I've put myself on technology restriction: no instant   messaging with people who live near me, no cell phoning in the presence of friends, no letting the voice mail pick up when I'm at home.
03/12/2022 17:40:46 - INFO - __main__ - ['modern technology is separating the people']
03/12/2022 17:40:46 - INFO - __main__ -  [race-high] What's the main idea of the passage? (A) Some manners on visiting British and American people's home. (B) Different table manners between British and American people. (C) Different ideas about the home between British and American people (D) Different ideas about how to get along well with neighbors between British and American people. [SEP] For the British, the home is a private place in which he or she goes to hide away from the troubles of life. It is very seldom that one would be invited to a British person's home. It is rude to knock on a person's door if you are not invited. If you are invited, don't ask to see more than the downstairs that your British host invites you into. Never ask how much the house or any of the items in it cost. To the American, most of them want their home to be a place where they can entertain   and share their lives with their friends. They may be delighted to give you a full tour of their houses. They may also be pleased when you show your interest and pleasure in their houses. Both British and American people will _ quite a bit of chat and a drink or two before the meal is served. After the first mouthful, you should say how delicious the food is and ask something about it. Remember, never eat with your mouth open and make very little noise while eating. It would be nice of you to help your host in any way. Maybe offer to pour some drinks or clear up after the meal.
03/12/2022 17:40:46 - INFO - __main__ - ["Some manners on visiting British and American people's home."]
03/12/2022 17:40:46 - INFO - __main__ -  [race-high] What's the title of the passage? (A) Parents' responsibilities. (B) Advice on self-control. (C) Bad influences of celebrities. (D) Media's bad influences. [SEP] As we know, many teen celebrities  feel and think that having a slimmer figure can do great good to them. But, does size really matter? Are teenage fans trying hard to become like their celebrity idols ? Do celebrities really have the power to influence people, especially teenagers? For the longest time, many parents blame teen idols for influencing the way their kids act. Have you noticed how teens idolize the celebrities these days? Even, their personal affairs are being followed by kids these days. Take for example the case of Lindsay Lohan of Mary Kate Ashley. They are definitely famous teen stars. But, since they are trying to project an image to satisfy a lot of people in show business, their health and body suffer. Many kids are aware of this problem. But they are easily influenced by these celebrities to exercise and eat less. It is a fact that the media, and especially famous teen celebrities, can influence people powerfully. But teenagers are easily influenced because teenage years are the period when our personality and identity developments take place. Teens watching TV shows and reading magazines are easily pulled into the dieting and harmful eating habits because the media have some ways to pull these acts. They use thin models and celebrities to endorse  products or to star in an up-and -coming shows or movies. With fierce competition, celebrities are forced to eat less and do extreme exercise routines to get the roles or offers that come their way. Living in today's time and generation is a bit disturbing to a lot of parents. Media, especially as well as the celebrities, have a very powerful influence to drive teenagers to good or bad. It's good that we can control ourselves to avoid bad things from happening. If not, parents should really be aware and guide their teens to determine what's in ad what's out.
03/12/2022 17:40:46 - INFO - __main__ - ['Bad influences of celebrities.']
03/12/2022 17:40:46 - INFO - __main__ - Tokenizing Input ...
03/12/2022 17:40:46 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:40:46 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 17:40:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 17:40:58 - INFO - __main__ - Starting training!
03/12/2022 17:41:03 - INFO - __main__ - Step 10 Global step 10 Train loss 19.245691 on epoch=4
03/12/2022 17:41:09 - INFO - __main__ - Step 20 Global step 20 Train loss 13.894602 on epoch=9
03/12/2022 17:41:16 - INFO - __main__ - Step 30 Global step 30 Train loss 6.317237 on epoch=14
03/12/2022 17:41:22 - INFO - __main__ - Step 40 Global step 40 Train loss 4.120196 on epoch=19
03/12/2022 17:41:28 - INFO - __main__ - Step 50 Global step 50 Train loss 3.684822 on epoch=24
03/12/2022 17:41:31 - INFO - __main__ - Global step 50 Train loss 9.452510 ACC 0.09375 on epoch=24
03/12/2022 17:41:38 - INFO - __main__ - Step 60 Global step 60 Train loss 3.151664 on epoch=29
03/12/2022 17:41:44 - INFO - __main__ - Step 70 Global step 70 Train loss 2.974743 on epoch=34
03/12/2022 17:41:50 - INFO - __main__ - Step 80 Global step 80 Train loss 2.881906 on epoch=39
03/12/2022 17:41:56 - INFO - __main__ - Step 90 Global step 90 Train loss 2.624358 on epoch=44
03/12/2022 17:42:02 - INFO - __main__ - Step 100 Global step 100 Train loss 2.566174 on epoch=49
03/12/2022 17:42:05 - INFO - __main__ - Global step 100 Train loss 2.839769 ACC 0.09375 on epoch=49
03/12/2022 17:42:11 - INFO - __main__ - Step 110 Global step 110 Train loss 2.305932 on epoch=54
03/12/2022 17:42:17 - INFO - __main__ - Step 120 Global step 120 Train loss 2.044477 on epoch=59
03/12/2022 17:42:23 - INFO - __main__ - Step 130 Global step 130 Train loss 2.024698 on epoch=64
03/12/2022 17:42:30 - INFO - __main__ - Step 140 Global step 140 Train loss 1.686990 on epoch=69
03/12/2022 17:42:36 - INFO - __main__ - Step 150 Global step 150 Train loss 1.481049 on epoch=74
03/12/2022 17:42:38 - INFO - __main__ - Global step 150 Train loss 1.908629 ACC 0.15625 on epoch=74
03/12/2022 17:42:45 - INFO - __main__ - Step 160 Global step 160 Train loss 1.369050 on epoch=79
03/12/2022 17:42:52 - INFO - __main__ - Step 170 Global step 170 Train loss 1.205489 on epoch=84
03/12/2022 17:42:58 - INFO - __main__ - Step 180 Global step 180 Train loss 1.163510 on epoch=89
03/12/2022 17:43:04 - INFO - __main__ - Step 190 Global step 190 Train loss 1.180131 on epoch=94
03/12/2022 17:43:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.946199 on epoch=99
03/12/2022 17:43:13 - INFO - __main__ - Global step 200 Train loss 1.172876 ACC 0.09375 on epoch=99
03/12/2022 17:43:19 - INFO - __main__ - Step 210 Global step 210 Train loss 1.078628 on epoch=104
03/12/2022 17:43:25 - INFO - __main__ - Step 220 Global step 220 Train loss 1.006213 on epoch=109
03/12/2022 17:43:31 - INFO - __main__ - Step 230 Global step 230 Train loss 1.003016 on epoch=114
03/12/2022 17:43:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.928096 on epoch=119
03/12/2022 17:43:44 - INFO - __main__ - Step 250 Global step 250 Train loss 1.035236 on epoch=124
03/12/2022 17:43:46 - INFO - __main__ - Global step 250 Train loss 1.010238 ACC 0.0625 on epoch=124
03/12/2022 17:43:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.898617 on epoch=129
03/12/2022 17:43:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.801433 on epoch=134
03/12/2022 17:44:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.796768 on epoch=139
03/12/2022 17:44:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.805827 on epoch=144
03/12/2022 17:44:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.785334 on epoch=149
03/12/2022 17:44:20 - INFO - __main__ - Global step 300 Train loss 0.817596 ACC 0.0625 on epoch=149
03/12/2022 17:44:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.963804 on epoch=154
03/12/2022 17:44:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.731014 on epoch=159
03/12/2022 17:44:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.818490 on epoch=164
03/12/2022 17:44:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.672631 on epoch=169
03/12/2022 17:44:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.735124 on epoch=174
03/12/2022 17:44:54 - INFO - __main__ - Global step 350 Train loss 0.784213 ACC 0.03125 on epoch=174
03/12/2022 17:45:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.664193 on epoch=179
03/12/2022 17:45:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.687144 on epoch=184
03/12/2022 17:45:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.695518 on epoch=189
03/12/2022 17:45:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.703208 on epoch=194
03/12/2022 17:45:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.612880 on epoch=199
03/12/2022 17:45:27 - INFO - __main__ - Global step 400 Train loss 0.672589 ACC 0.0625 on epoch=199
03/12/2022 17:45:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.586261 on epoch=204
03/12/2022 17:45:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.635722 on epoch=209
03/12/2022 17:45:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.649661 on epoch=214
03/12/2022 17:45:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.509690 on epoch=219
03/12/2022 17:45:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.556299 on epoch=224
03/12/2022 17:46:01 - INFO - __main__ - Global step 450 Train loss 0.587526 ACC 0.09375 on epoch=224
03/12/2022 17:46:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.583015 on epoch=229
03/12/2022 17:46:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.628514 on epoch=234
03/12/2022 17:46:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.620310 on epoch=239
03/12/2022 17:46:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.463555 on epoch=244
03/12/2022 17:46:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.562266 on epoch=249
03/12/2022 17:46:34 - INFO - __main__ - Global step 500 Train loss 0.571532 ACC 0.09375 on epoch=249
03/12/2022 17:46:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.548260 on epoch=254
03/12/2022 17:46:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.514321 on epoch=259
03/12/2022 17:46:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.512792 on epoch=264
03/12/2022 17:46:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.540442 on epoch=269
03/12/2022 17:47:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.467470 on epoch=274
03/12/2022 17:47:07 - INFO - __main__ - Global step 550 Train loss 0.516657 ACC 0.09375 on epoch=274
03/12/2022 17:47:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.440605 on epoch=279
03/12/2022 17:47:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.461416 on epoch=284
03/12/2022 17:47:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.523602 on epoch=289
03/12/2022 17:47:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.453527 on epoch=294
03/12/2022 17:47:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.431248 on epoch=299
03/12/2022 17:47:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:47:40 - INFO - __main__ - Printing 3 examples
03/12/2022 17:47:40 - INFO - __main__ -  [race-high] Dr La Farge's research is of great importance to   _  . (A) knowing what the plants during the Little Ice Age were like (B) understanding how ecosystems recover from glaciers. (C) regrowing many species that have been destroyed before. (D) figuring out the effects of melting ice caps on moss. [SEP] 400-year-old plants from the Little Ice Age were brought back to life, which could help us understand how the Earth will deal with climate change. Moss  found buried beneath the Teardrop glacier  on Ellesmere Island in Canada has been brought back to life. Findings suggest that these plants could help repopulate regions exposed by melting ice caps. Plants that were buried beneath thick ice in Canada more than 400 years ago and were thought to have frozen to death have been brought back to life by Canadian scientists. Samples of the moss plant, covered by the glacier during the Little Ice Age of 1550 to 1850 AD, were replanted in a lab at the University of Alberta and grew new stems . Researchers now think these findings can give indication as to how regions can recover as the ice covering them melts. Biologist Dr. Catherine La Farge and her team at the University of Alberta were exploring the region around the Teardrop glacier on Ellesmere Island. Ice on Ellesmere Island region has been melting at around four meters each year for the past nine years. This means that many areas of land that were previously covered by ice have since been exposed. Many ecosystems that were thought to have been destroyed during the Little Ice Age between 1550 and 1850 AD can now be studied, including many species that have never been studied before. While examining an exposed area of land, La Farge and her team discovered a small area of moss called Aulacomnium turgidum. It is a type of bryophyte  plant that mainly grows across Canada, the US and the Highlands of Scotland. Dr La Farge noticed that the moss had small patches of green stems, suggesting it is either growing again or can be encouraged to repopulate. Dr La Farge told the BBC, "When we looked at the samples in detail and brought them to the lab, I could see some of the stems actually had new growth of green branches, suggesting that these plants are growing again, and that _ When we think of thick areas of ice covering the landscape, we've always thought that plants have to come from refugia , never considering that land plants come from underneath a glacier. It's a whole world of what's coming out from underneath the glacier that really needs to be studied. The ice is disappearing pretty fast. We really have not examined all the biological systems that exist in the world; we don't know it all." Dr La Farge took samples of the moss and, using carbon-dating techniques, discovered that the plants date back to the Little Ice Age. Dr La Farge's team took the samples, planted them in dishes full of nutrient-rich potting soil and fed them with water. The samples were from four separate species including Aulacomnium turgidum, Distichium capillaceum, Encalypta procera and Syntrichia ruralis. The moss plants found by Dr La Farge are types of bryophytes. Bryophytes can survive long winters and regrow when the weather gets warmer. However, Dr La Farge was surprised that the plants buried under ice have survived into the twenty-first century. Her findings appear in proceedings of the National Academy of Sciences.
03/12/2022 17:47:40 - INFO - __main__ - ['understanding how ecosystems recover from glaciers.']
03/12/2022 17:47:40 - INFO - __main__ -  [race-high] Which of the following statements is TRUE? (A) The museum is opened all the year around. (B) You can usually pay your parking in check. (C) During bears home games visitors can park on the museum Campus for free. (D) After 4:00 p.m.you are not allowed to enter the museum. [SEP] The Field Museum Hours Regular hours are 9:00 a.m.--5:00 p.m. , daily. Last admission at 4:00 p.m. Open every day except Christmas. Admission We have several ticket choices for you to choose from. Advance tickets may be purchased at the will- call booth in person at the museum before 4:00 p.m. Getting Here The Field Museum is located on Chicago's Museum Campus; at 1400 S. Lake Shore Drive, just south of Roosevelt Rd. How to get here : by car or public transit or free trolley. Parking Visitor parking in all lots on the Museum Campus is $ 15.00 per day .This includes the Adler lot, the north garage, the Waldron garage and the east museum lot. Hours for the north garage and Adler lot are 5:00 a.m.--5:00 p.m. Mon--Fri and 6:00a.m. --5:00p.m. weekends; east museum lot9:00 a.m.--3:00p.m. Designated handicapped parking is available in every lot. When all other lots are full , parking will be made available in the remote south lot for$ 8.00 per day. From Memorial Day to Labor Day , the parking lot will only accept cash payments , which will need to be paid upon entering the garage. Please note : These hours and rates are for daytime only and do not apply when special events are scheduled at the museums or Soldier Field. Getting Here During Chicago Bears Home Games During bears home games and other major special events at Soldier Field , access to the Museum Campus can be challenging. No museum visitor parking is available on the Museum Campus during bears home games. However, public transit remains a great way to get to the Campus every day of the year. For more information, call the Regional Transportation Authority at (312) 836 -- 7000 or visit www. rtachicago.com. Additional parking is available at the Monroe Street garage , located at 350 East Monroe Street.
03/12/2022 17:47:40 - INFO - __main__ - ['After 4:00 p.m.you are not allowed to enter the museum.']
03/12/2022 17:47:40 - INFO - __main__ -  [race-high] Why isn't it a plain sailing? (A) No one can treat his mother well. (B) Dr Syed was the wrong blood group. (C) They didn't have money to be in hospital. (D) Mrs. Syed was unwilling to receive the operation. [SEP] Dr Asim Syed, 32, has performed more than 100 operations at London's Hammersmith Hospital in the country's busiest transplant unit, but never imagined that he would one day become a donor himself. He stepped forward when was told his 64-year-old mother might be dead within months unless she got a new kidney  . The worried surgeon brought her to London to be cared for at his hospital. However, it was not all plain sailing. Tests showed Dr Syed was the wrong blood group, so the only way was to go through a special blood-washing process. He consulted colleagues about that, but they didn't agree, because the risk of rejection is still too high. Dr Syed and his mother were then advised to consider a new way of donating and receiving, called an organ-paired. That is, Dr Syed donated his kidney to an unknown person and another donor in the chain was a successful match for his mother. The chain of three transplants took place at the same time on July 31 with Dr Syed's kidney going to a recipient in the Midlands and Mrs. Syed receiving her kidney from a person in the south of England. Just hours after donating his own kidney, Dr Syed found himself recovering in bed next to his mother. Mrs Syed said, "When I came round from my operation Asim was in the next bed and the first thing he said was, 'Mum now all your worries are over.' Tears fell down." Now mother and son are recovering well with Dr Syed already back at work. Mrs. Syed is staying with him for several months while the hospital monitors her progress. He said, "I did what anyone would do when they see a relative suffering disease. Although I wasn't able to help mum directly, by agreeing to be part of a chain, I was also very happy."
03/12/2022 17:47:40 - INFO - __main__ - ['Dr Syed was the wrong blood group.']
03/12/2022 17:47:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 17:47:40 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:47:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 17:47:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:47:40 - INFO - __main__ - Printing 3 examples
03/12/2022 17:47:40 - INFO - __main__ -  [race-high] The passage implies that   _  . (A) modern technology is bridging the people (B) modern technology is separating the people (C) modern technology is developing too fast (D) modern technology is interrupting our communication [SEP] A funny thing happened on the way to the communications revolution: we stopped talking to one another. I was walking in the park with a friend recently, and his cell phone rang, interrupting our conversation. There we were walking and talking on a beautiful sunny day and... I became invisible, absent from the conversation. The telephone used to connect you to the absent. Now it makes people sitting next to you feel absent. Why is it that the more connected we get, the more disconnected I feel? Every advance in communications technology is a tragedy to the closeness of human interaction . With email and instant messaging over the internet, we can now communicate without seeing or talking to one another. With voice mail, you can conduct  entire conversations without ever reaching anyone. If my mom  has a question, I just leave the answer on her machine. As almost every contact we can imagine between human  beings gets automated , the alienation index  goes up. You can't  even call a person to get the phone number of another person  any more. Directory assistance is almost always fully automated. I am not against modern technology. I own a cell phone,  an ATM card, a voice mail system, and an email account. Giving them up isn't wise... they're a great help to us. It's some of  their possible consequences that make me feel uneasy. More and more. I find myself hiding behind email to do a job meant for conversation. Or being relieved that voice mail   picked up because I didn't really have time to talk. The industry devoted to helping me keep in touch is making me lonelier ...or at least facilitating my antisocial instincts. So I've put myself on technology restriction: no instant   messaging with people who live near me, no cell phoning in the presence of friends, no letting the voice mail pick up when I'm at home.
03/12/2022 17:47:40 - INFO - __main__ - ['modern technology is separating the people']
03/12/2022 17:47:40 - INFO - __main__ -  [race-high] What's the main idea of the passage? (A) Some manners on visiting British and American people's home. (B) Different table manners between British and American people. (C) Different ideas about the home between British and American people (D) Different ideas about how to get along well with neighbors between British and American people. [SEP] For the British, the home is a private place in which he or she goes to hide away from the troubles of life. It is very seldom that one would be invited to a British person's home. It is rude to knock on a person's door if you are not invited. If you are invited, don't ask to see more than the downstairs that your British host invites you into. Never ask how much the house or any of the items in it cost. To the American, most of them want their home to be a place where they can entertain   and share their lives with their friends. They may be delighted to give you a full tour of their houses. They may also be pleased when you show your interest and pleasure in their houses. Both British and American people will _ quite a bit of chat and a drink or two before the meal is served. After the first mouthful, you should say how delicious the food is and ask something about it. Remember, never eat with your mouth open and make very little noise while eating. It would be nice of you to help your host in any way. Maybe offer to pour some drinks or clear up after the meal.
03/12/2022 17:47:40 - INFO - __main__ - ["Some manners on visiting British and American people's home."]
03/12/2022 17:47:40 - INFO - __main__ -  [race-high] What's the title of the passage? (A) Parents' responsibilities. (B) Advice on self-control. (C) Bad influences of celebrities. (D) Media's bad influences. [SEP] As we know, many teen celebrities  feel and think that having a slimmer figure can do great good to them. But, does size really matter? Are teenage fans trying hard to become like their celebrity idols ? Do celebrities really have the power to influence people, especially teenagers? For the longest time, many parents blame teen idols for influencing the way their kids act. Have you noticed how teens idolize the celebrities these days? Even, their personal affairs are being followed by kids these days. Take for example the case of Lindsay Lohan of Mary Kate Ashley. They are definitely famous teen stars. But, since they are trying to project an image to satisfy a lot of people in show business, their health and body suffer. Many kids are aware of this problem. But they are easily influenced by these celebrities to exercise and eat less. It is a fact that the media, and especially famous teen celebrities, can influence people powerfully. But teenagers are easily influenced because teenage years are the period when our personality and identity developments take place. Teens watching TV shows and reading magazines are easily pulled into the dieting and harmful eating habits because the media have some ways to pull these acts. They use thin models and celebrities to endorse  products or to star in an up-and -coming shows or movies. With fierce competition, celebrities are forced to eat less and do extreme exercise routines to get the roles or offers that come their way. Living in today's time and generation is a bit disturbing to a lot of parents. Media, especially as well as the celebrities, have a very powerful influence to drive teenagers to good or bad. It's good that we can control ourselves to avoid bad things from happening. If not, parents should really be aware and guide their teens to determine what's in ad what's out.
03/12/2022 17:47:40 - INFO - __main__ - ['Bad influences of celebrities.']
03/12/2022 17:47:40 - INFO - __main__ - Tokenizing Input ...
03/12/2022 17:47:40 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:47:40 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 17:47:41 - INFO - __main__ - Global step 600 Train loss 0.462080 ACC 0.0625 on epoch=299
03/12/2022 17:47:41 - INFO - __main__ - save last model!
03/12/2022 17:47:48 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 17:47:49 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 17:47:49 - INFO - __main__ - Printing 3 examples
03/12/2022 17:47:49 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 17:47:49 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 17:47:49 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 17:47:49 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 17:47:49 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 17:47:49 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 17:47:49 - INFO - __main__ - Tokenizing Input ...
03/12/2022 17:47:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 17:47:53 - INFO - __main__ - Starting training!
03/12/2022 17:47:54 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:47:58 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 17:51:49 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_13_0.0003_8_predictions.txt
03/12/2022 17:51:49 - INFO - __main__ - ACC on test data: 0.1307
03/12/2022 17:51:50 - INFO - __main__ - prefix=race-high_32_13, lr=0.0003, bsz=8, dev_performance=0.15625, test_performance=0.13068675746160532
03/12/2022 17:51:50 - INFO - __main__ - Running ... prefix=race-high_32_13, lr=0.0002, bsz=8 ...
03/12/2022 17:51:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:51:51 - INFO - __main__ - Printing 3 examples
03/12/2022 17:51:51 - INFO - __main__ -  [race-high] Dr La Farge's research is of great importance to   _  . (A) knowing what the plants during the Little Ice Age were like (B) understanding how ecosystems recover from glaciers. (C) regrowing many species that have been destroyed before. (D) figuring out the effects of melting ice caps on moss. [SEP] 400-year-old plants from the Little Ice Age were brought back to life, which could help us understand how the Earth will deal with climate change. Moss  found buried beneath the Teardrop glacier  on Ellesmere Island in Canada has been brought back to life. Findings suggest that these plants could help repopulate regions exposed by melting ice caps. Plants that were buried beneath thick ice in Canada more than 400 years ago and were thought to have frozen to death have been brought back to life by Canadian scientists. Samples of the moss plant, covered by the glacier during the Little Ice Age of 1550 to 1850 AD, were replanted in a lab at the University of Alberta and grew new stems . Researchers now think these findings can give indication as to how regions can recover as the ice covering them melts. Biologist Dr. Catherine La Farge and her team at the University of Alberta were exploring the region around the Teardrop glacier on Ellesmere Island. Ice on Ellesmere Island region has been melting at around four meters each year for the past nine years. This means that many areas of land that were previously covered by ice have since been exposed. Many ecosystems that were thought to have been destroyed during the Little Ice Age between 1550 and 1850 AD can now be studied, including many species that have never been studied before. While examining an exposed area of land, La Farge and her team discovered a small area of moss called Aulacomnium turgidum. It is a type of bryophyte  plant that mainly grows across Canada, the US and the Highlands of Scotland. Dr La Farge noticed that the moss had small patches of green stems, suggesting it is either growing again or can be encouraged to repopulate. Dr La Farge told the BBC, "When we looked at the samples in detail and brought them to the lab, I could see some of the stems actually had new growth of green branches, suggesting that these plants are growing again, and that _ When we think of thick areas of ice covering the landscape, we've always thought that plants have to come from refugia , never considering that land plants come from underneath a glacier. It's a whole world of what's coming out from underneath the glacier that really needs to be studied. The ice is disappearing pretty fast. We really have not examined all the biological systems that exist in the world; we don't know it all." Dr La Farge took samples of the moss and, using carbon-dating techniques, discovered that the plants date back to the Little Ice Age. Dr La Farge's team took the samples, planted them in dishes full of nutrient-rich potting soil and fed them with water. The samples were from four separate species including Aulacomnium turgidum, Distichium capillaceum, Encalypta procera and Syntrichia ruralis. The moss plants found by Dr La Farge are types of bryophytes. Bryophytes can survive long winters and regrow when the weather gets warmer. However, Dr La Farge was surprised that the plants buried under ice have survived into the twenty-first century. Her findings appear in proceedings of the National Academy of Sciences.
03/12/2022 17:51:51 - INFO - __main__ - ['understanding how ecosystems recover from glaciers.']
03/12/2022 17:51:51 - INFO - __main__ -  [race-high] Which of the following statements is TRUE? (A) The museum is opened all the year around. (B) You can usually pay your parking in check. (C) During bears home games visitors can park on the museum Campus for free. (D) After 4:00 p.m.you are not allowed to enter the museum. [SEP] The Field Museum Hours Regular hours are 9:00 a.m.--5:00 p.m. , daily. Last admission at 4:00 p.m. Open every day except Christmas. Admission We have several ticket choices for you to choose from. Advance tickets may be purchased at the will- call booth in person at the museum before 4:00 p.m. Getting Here The Field Museum is located on Chicago's Museum Campus; at 1400 S. Lake Shore Drive, just south of Roosevelt Rd. How to get here : by car or public transit or free trolley. Parking Visitor parking in all lots on the Museum Campus is $ 15.00 per day .This includes the Adler lot, the north garage, the Waldron garage and the east museum lot. Hours for the north garage and Adler lot are 5:00 a.m.--5:00 p.m. Mon--Fri and 6:00a.m. --5:00p.m. weekends; east museum lot9:00 a.m.--3:00p.m. Designated handicapped parking is available in every lot. When all other lots are full , parking will be made available in the remote south lot for$ 8.00 per day. From Memorial Day to Labor Day , the parking lot will only accept cash payments , which will need to be paid upon entering the garage. Please note : These hours and rates are for daytime only and do not apply when special events are scheduled at the museums or Soldier Field. Getting Here During Chicago Bears Home Games During bears home games and other major special events at Soldier Field , access to the Museum Campus can be challenging. No museum visitor parking is available on the Museum Campus during bears home games. However, public transit remains a great way to get to the Campus every day of the year. For more information, call the Regional Transportation Authority at (312) 836 -- 7000 or visit www. rtachicago.com. Additional parking is available at the Monroe Street garage , located at 350 East Monroe Street.
03/12/2022 17:51:51 - INFO - __main__ - ['After 4:00 p.m.you are not allowed to enter the museum.']
03/12/2022 17:51:51 - INFO - __main__ -  [race-high] Why isn't it a plain sailing? (A) No one can treat his mother well. (B) Dr Syed was the wrong blood group. (C) They didn't have money to be in hospital. (D) Mrs. Syed was unwilling to receive the operation. [SEP] Dr Asim Syed, 32, has performed more than 100 operations at London's Hammersmith Hospital in the country's busiest transplant unit, but never imagined that he would one day become a donor himself. He stepped forward when was told his 64-year-old mother might be dead within months unless she got a new kidney  . The worried surgeon brought her to London to be cared for at his hospital. However, it was not all plain sailing. Tests showed Dr Syed was the wrong blood group, so the only way was to go through a special blood-washing process. He consulted colleagues about that, but they didn't agree, because the risk of rejection is still too high. Dr Syed and his mother were then advised to consider a new way of donating and receiving, called an organ-paired. That is, Dr Syed donated his kidney to an unknown person and another donor in the chain was a successful match for his mother. The chain of three transplants took place at the same time on July 31 with Dr Syed's kidney going to a recipient in the Midlands and Mrs. Syed receiving her kidney from a person in the south of England. Just hours after donating his own kidney, Dr Syed found himself recovering in bed next to his mother. Mrs Syed said, "When I came round from my operation Asim was in the next bed and the first thing he said was, 'Mum now all your worries are over.' Tears fell down." Now mother and son are recovering well with Dr Syed already back at work. Mrs. Syed is staying with him for several months while the hospital monitors her progress. He said, "I did what anyone would do when they see a relative suffering disease. Although I wasn't able to help mum directly, by agreeing to be part of a chain, I was also very happy."
03/12/2022 17:51:51 - INFO - __main__ - ['Dr Syed was the wrong blood group.']
03/12/2022 17:51:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 17:51:51 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:51:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 17:51:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:51:51 - INFO - __main__ - Printing 3 examples
03/12/2022 17:51:51 - INFO - __main__ -  [race-high] The passage implies that   _  . (A) modern technology is bridging the people (B) modern technology is separating the people (C) modern technology is developing too fast (D) modern technology is interrupting our communication [SEP] A funny thing happened on the way to the communications revolution: we stopped talking to one another. I was walking in the park with a friend recently, and his cell phone rang, interrupting our conversation. There we were walking and talking on a beautiful sunny day and... I became invisible, absent from the conversation. The telephone used to connect you to the absent. Now it makes people sitting next to you feel absent. Why is it that the more connected we get, the more disconnected I feel? Every advance in communications technology is a tragedy to the closeness of human interaction . With email and instant messaging over the internet, we can now communicate without seeing or talking to one another. With voice mail, you can conduct  entire conversations without ever reaching anyone. If my mom  has a question, I just leave the answer on her machine. As almost every contact we can imagine between human  beings gets automated , the alienation index  goes up. You can't  even call a person to get the phone number of another person  any more. Directory assistance is almost always fully automated. I am not against modern technology. I own a cell phone,  an ATM card, a voice mail system, and an email account. Giving them up isn't wise... they're a great help to us. It's some of  their possible consequences that make me feel uneasy. More and more. I find myself hiding behind email to do a job meant for conversation. Or being relieved that voice mail   picked up because I didn't really have time to talk. The industry devoted to helping me keep in touch is making me lonelier ...or at least facilitating my antisocial instincts. So I've put myself on technology restriction: no instant   messaging with people who live near me, no cell phoning in the presence of friends, no letting the voice mail pick up when I'm at home.
03/12/2022 17:51:51 - INFO - __main__ - ['modern technology is separating the people']
03/12/2022 17:51:51 - INFO - __main__ -  [race-high] What's the main idea of the passage? (A) Some manners on visiting British and American people's home. (B) Different table manners between British and American people. (C) Different ideas about the home between British and American people (D) Different ideas about how to get along well with neighbors between British and American people. [SEP] For the British, the home is a private place in which he or she goes to hide away from the troubles of life. It is very seldom that one would be invited to a British person's home. It is rude to knock on a person's door if you are not invited. If you are invited, don't ask to see more than the downstairs that your British host invites you into. Never ask how much the house or any of the items in it cost. To the American, most of them want their home to be a place where they can entertain   and share their lives with their friends. They may be delighted to give you a full tour of their houses. They may also be pleased when you show your interest and pleasure in their houses. Both British and American people will _ quite a bit of chat and a drink or two before the meal is served. After the first mouthful, you should say how delicious the food is and ask something about it. Remember, never eat with your mouth open and make very little noise while eating. It would be nice of you to help your host in any way. Maybe offer to pour some drinks or clear up after the meal.
03/12/2022 17:51:51 - INFO - __main__ - ["Some manners on visiting British and American people's home."]
03/12/2022 17:51:51 - INFO - __main__ -  [race-high] What's the title of the passage? (A) Parents' responsibilities. (B) Advice on self-control. (C) Bad influences of celebrities. (D) Media's bad influences. [SEP] As we know, many teen celebrities  feel and think that having a slimmer figure can do great good to them. But, does size really matter? Are teenage fans trying hard to become like their celebrity idols ? Do celebrities really have the power to influence people, especially teenagers? For the longest time, many parents blame teen idols for influencing the way their kids act. Have you noticed how teens idolize the celebrities these days? Even, their personal affairs are being followed by kids these days. Take for example the case of Lindsay Lohan of Mary Kate Ashley. They are definitely famous teen stars. But, since they are trying to project an image to satisfy a lot of people in show business, their health and body suffer. Many kids are aware of this problem. But they are easily influenced by these celebrities to exercise and eat less. It is a fact that the media, and especially famous teen celebrities, can influence people powerfully. But teenagers are easily influenced because teenage years are the period when our personality and identity developments take place. Teens watching TV shows and reading magazines are easily pulled into the dieting and harmful eating habits because the media have some ways to pull these acts. They use thin models and celebrities to endorse  products or to star in an up-and -coming shows or movies. With fierce competition, celebrities are forced to eat less and do extreme exercise routines to get the roles or offers that come their way. Living in today's time and generation is a bit disturbing to a lot of parents. Media, especially as well as the celebrities, have a very powerful influence to drive teenagers to good or bad. It's good that we can control ourselves to avoid bad things from happening. If not, parents should really be aware and guide their teens to determine what's in ad what's out.
03/12/2022 17:51:51 - INFO - __main__ - ['Bad influences of celebrities.']
03/12/2022 17:51:51 - INFO - __main__ - Tokenizing Input ...
03/12/2022 17:51:51 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:51:51 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 17:52:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 17:52:02 - INFO - __main__ - Starting training!
03/12/2022 17:52:07 - INFO - __main__ - Step 10 Global step 10 Train loss 18.834143 on epoch=4
03/12/2022 17:52:13 - INFO - __main__ - Step 20 Global step 20 Train loss 17.521410 on epoch=9
03/12/2022 17:52:19 - INFO - __main__ - Step 30 Global step 30 Train loss 12.751428 on epoch=14
03/12/2022 17:52:25 - INFO - __main__ - Step 40 Global step 40 Train loss 9.970280 on epoch=19
03/12/2022 17:52:32 - INFO - __main__ - Step 50 Global step 50 Train loss 7.711379 on epoch=24
03/12/2022 17:52:34 - INFO - __main__ - Global step 50 Train loss 13.357728 ACC 0.0 on epoch=24
03/12/2022 17:52:41 - INFO - __main__ - Step 60 Global step 60 Train loss 6.881242 on epoch=29
03/12/2022 17:52:47 - INFO - __main__ - Step 70 Global step 70 Train loss 6.301672 on epoch=34
03/12/2022 17:52:53 - INFO - __main__ - Step 80 Global step 80 Train loss 5.333771 on epoch=39
03/12/2022 17:52:59 - INFO - __main__ - Step 90 Global step 90 Train loss 4.794295 on epoch=44
03/12/2022 17:53:05 - INFO - __main__ - Step 100 Global step 100 Train loss 4.249959 on epoch=49
03/12/2022 17:53:07 - INFO - __main__ - Global step 100 Train loss 5.512189 ACC 0.03125 on epoch=49
03/12/2022 17:53:14 - INFO - __main__ - Step 110 Global step 110 Train loss 3.746123 on epoch=54
03/12/2022 17:53:20 - INFO - __main__ - Step 120 Global step 120 Train loss 3.617901 on epoch=59
03/12/2022 17:53:27 - INFO - __main__ - Step 130 Global step 130 Train loss 3.228367 on epoch=64
03/12/2022 17:53:33 - INFO - __main__ - Step 140 Global step 140 Train loss 2.998132 on epoch=69
03/12/2022 17:53:39 - INFO - __main__ - Step 150 Global step 150 Train loss 2.636059 on epoch=74
03/12/2022 17:53:41 - INFO - __main__ - Global step 150 Train loss 3.245316 ACC 0.15625 on epoch=74
03/12/2022 17:53:48 - INFO - __main__ - Step 160 Global step 160 Train loss 2.559305 on epoch=79
03/12/2022 17:53:54 - INFO - __main__ - Step 170 Global step 170 Train loss 2.198173 on epoch=84
03/12/2022 17:54:01 - INFO - __main__ - Step 180 Global step 180 Train loss 1.998118 on epoch=89
03/12/2022 17:54:07 - INFO - __main__ - Step 190 Global step 190 Train loss 1.950280 on epoch=94
03/12/2022 17:54:13 - INFO - __main__ - Step 200 Global step 200 Train loss 1.757504 on epoch=99
03/12/2022 17:54:15 - INFO - __main__ - Global step 200 Train loss 2.092676 ACC 0.09375 on epoch=99
03/12/2022 17:54:21 - INFO - __main__ - Step 210 Global step 210 Train loss 1.739231 on epoch=104
03/12/2022 17:54:27 - INFO - __main__ - Step 220 Global step 220 Train loss 1.558378 on epoch=109
03/12/2022 17:54:33 - INFO - __main__ - Step 230 Global step 230 Train loss 1.574290 on epoch=114
03/12/2022 17:54:39 - INFO - __main__ - Step 240 Global step 240 Train loss 1.480953 on epoch=119
03/12/2022 17:54:45 - INFO - __main__ - Step 250 Global step 250 Train loss 1.253040 on epoch=124
03/12/2022 17:54:48 - INFO - __main__ - Global step 250 Train loss 1.521178 ACC 0.125 on epoch=124
03/12/2022 17:54:54 - INFO - __main__ - Step 260 Global step 260 Train loss 1.105008 on epoch=129
03/12/2022 17:55:00 - INFO - __main__ - Step 270 Global step 270 Train loss 1.233377 on epoch=134
03/12/2022 17:55:06 - INFO - __main__ - Step 280 Global step 280 Train loss 1.138710 on epoch=139
03/12/2022 17:55:12 - INFO - __main__ - Step 290 Global step 290 Train loss 1.285786 on epoch=144
03/12/2022 17:55:18 - INFO - __main__ - Step 300 Global step 300 Train loss 1.023709 on epoch=149
03/12/2022 17:55:20 - INFO - __main__ - Global step 300 Train loss 1.157318 ACC 0.09375 on epoch=149
03/12/2022 17:55:27 - INFO - __main__ - Step 310 Global step 310 Train loss 1.059867 on epoch=154
03/12/2022 17:55:33 - INFO - __main__ - Step 320 Global step 320 Train loss 1.168875 on epoch=159
03/12/2022 17:55:39 - INFO - __main__ - Step 330 Global step 330 Train loss 1.273642 on epoch=164
03/12/2022 17:55:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.947416 on epoch=169
03/12/2022 17:55:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.997478 on epoch=174
03/12/2022 17:55:54 - INFO - __main__ - Global step 350 Train loss 1.089456 ACC 0.03125 on epoch=174
03/12/2022 17:56:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.899981 on epoch=179
03/12/2022 17:56:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.910010 on epoch=184
03/12/2022 17:56:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.896043 on epoch=189
03/12/2022 17:56:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.905676 on epoch=194
03/12/2022 17:56:24 - INFO - __main__ - Step 400 Global step 400 Train loss 1.003841 on epoch=199
03/12/2022 17:56:27 - INFO - __main__ - Global step 400 Train loss 0.923110 ACC 0.03125 on epoch=199
03/12/2022 17:56:33 - INFO - __main__ - Step 410 Global step 410 Train loss 1.031913 on epoch=204
03/12/2022 17:56:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.924422 on epoch=209
03/12/2022 17:56:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.894303 on epoch=214
03/12/2022 17:56:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.947985 on epoch=219
03/12/2022 17:56:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.900382 on epoch=224
03/12/2022 17:57:00 - INFO - __main__ - Global step 450 Train loss 0.939801 ACC 0.0625 on epoch=224
03/12/2022 17:57:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.809045 on epoch=229
03/12/2022 17:57:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.819075 on epoch=234
03/12/2022 17:57:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.734938 on epoch=239
03/12/2022 17:57:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.805940 on epoch=244
03/12/2022 17:57:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.827823 on epoch=249
03/12/2022 17:57:32 - INFO - __main__ - Global step 500 Train loss 0.799364 ACC 0.03125 on epoch=249
03/12/2022 17:57:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.764558 on epoch=254
03/12/2022 17:57:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.766219 on epoch=259
03/12/2022 17:57:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.679341 on epoch=264
03/12/2022 17:57:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.714561 on epoch=269
03/12/2022 17:58:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.697718 on epoch=274
03/12/2022 17:58:04 - INFO - __main__ - Global step 550 Train loss 0.724479 ACC 0.0625 on epoch=274
03/12/2022 17:58:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.710760 on epoch=279
03/12/2022 17:58:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.676943 on epoch=284
03/12/2022 17:58:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.667077 on epoch=289
03/12/2022 17:58:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.670827 on epoch=294
03/12/2022 17:58:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.617320 on epoch=299
03/12/2022 17:58:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:58:36 - INFO - __main__ - Printing 3 examples
03/12/2022 17:58:36 - INFO - __main__ -  [race-high] Dr La Farge's research is of great importance to   _  . (A) knowing what the plants during the Little Ice Age were like (B) understanding how ecosystems recover from glaciers. (C) regrowing many species that have been destroyed before. (D) figuring out the effects of melting ice caps on moss. [SEP] 400-year-old plants from the Little Ice Age were brought back to life, which could help us understand how the Earth will deal with climate change. Moss  found buried beneath the Teardrop glacier  on Ellesmere Island in Canada has been brought back to life. Findings suggest that these plants could help repopulate regions exposed by melting ice caps. Plants that were buried beneath thick ice in Canada more than 400 years ago and were thought to have frozen to death have been brought back to life by Canadian scientists. Samples of the moss plant, covered by the glacier during the Little Ice Age of 1550 to 1850 AD, were replanted in a lab at the University of Alberta and grew new stems . Researchers now think these findings can give indication as to how regions can recover as the ice covering them melts. Biologist Dr. Catherine La Farge and her team at the University of Alberta were exploring the region around the Teardrop glacier on Ellesmere Island. Ice on Ellesmere Island region has been melting at around four meters each year for the past nine years. This means that many areas of land that were previously covered by ice have since been exposed. Many ecosystems that were thought to have been destroyed during the Little Ice Age between 1550 and 1850 AD can now be studied, including many species that have never been studied before. While examining an exposed area of land, La Farge and her team discovered a small area of moss called Aulacomnium turgidum. It is a type of bryophyte  plant that mainly grows across Canada, the US and the Highlands of Scotland. Dr La Farge noticed that the moss had small patches of green stems, suggesting it is either growing again or can be encouraged to repopulate. Dr La Farge told the BBC, "When we looked at the samples in detail and brought them to the lab, I could see some of the stems actually had new growth of green branches, suggesting that these plants are growing again, and that _ When we think of thick areas of ice covering the landscape, we've always thought that plants have to come from refugia , never considering that land plants come from underneath a glacier. It's a whole world of what's coming out from underneath the glacier that really needs to be studied. The ice is disappearing pretty fast. We really have not examined all the biological systems that exist in the world; we don't know it all." Dr La Farge took samples of the moss and, using carbon-dating techniques, discovered that the plants date back to the Little Ice Age. Dr La Farge's team took the samples, planted them in dishes full of nutrient-rich potting soil and fed them with water. The samples were from four separate species including Aulacomnium turgidum, Distichium capillaceum, Encalypta procera and Syntrichia ruralis. The moss plants found by Dr La Farge are types of bryophytes. Bryophytes can survive long winters and regrow when the weather gets warmer. However, Dr La Farge was surprised that the plants buried under ice have survived into the twenty-first century. Her findings appear in proceedings of the National Academy of Sciences.
03/12/2022 17:58:36 - INFO - __main__ - ['understanding how ecosystems recover from glaciers.']
03/12/2022 17:58:36 - INFO - __main__ -  [race-high] Which of the following statements is TRUE? (A) The museum is opened all the year around. (B) You can usually pay your parking in check. (C) During bears home games visitors can park on the museum Campus for free. (D) After 4:00 p.m.you are not allowed to enter the museum. [SEP] The Field Museum Hours Regular hours are 9:00 a.m.--5:00 p.m. , daily. Last admission at 4:00 p.m. Open every day except Christmas. Admission We have several ticket choices for you to choose from. Advance tickets may be purchased at the will- call booth in person at the museum before 4:00 p.m. Getting Here The Field Museum is located on Chicago's Museum Campus; at 1400 S. Lake Shore Drive, just south of Roosevelt Rd. How to get here : by car or public transit or free trolley. Parking Visitor parking in all lots on the Museum Campus is $ 15.00 per day .This includes the Adler lot, the north garage, the Waldron garage and the east museum lot. Hours for the north garage and Adler lot are 5:00 a.m.--5:00 p.m. Mon--Fri and 6:00a.m. --5:00p.m. weekends; east museum lot9:00 a.m.--3:00p.m. Designated handicapped parking is available in every lot. When all other lots are full , parking will be made available in the remote south lot for$ 8.00 per day. From Memorial Day to Labor Day , the parking lot will only accept cash payments , which will need to be paid upon entering the garage. Please note : These hours and rates are for daytime only and do not apply when special events are scheduled at the museums or Soldier Field. Getting Here During Chicago Bears Home Games During bears home games and other major special events at Soldier Field , access to the Museum Campus can be challenging. No museum visitor parking is available on the Museum Campus during bears home games. However, public transit remains a great way to get to the Campus every day of the year. For more information, call the Regional Transportation Authority at (312) 836 -- 7000 or visit www. rtachicago.com. Additional parking is available at the Monroe Street garage , located at 350 East Monroe Street.
03/12/2022 17:58:36 - INFO - __main__ - ['After 4:00 p.m.you are not allowed to enter the museum.']
03/12/2022 17:58:36 - INFO - __main__ -  [race-high] Why isn't it a plain sailing? (A) No one can treat his mother well. (B) Dr Syed was the wrong blood group. (C) They didn't have money to be in hospital. (D) Mrs. Syed was unwilling to receive the operation. [SEP] Dr Asim Syed, 32, has performed more than 100 operations at London's Hammersmith Hospital in the country's busiest transplant unit, but never imagined that he would one day become a donor himself. He stepped forward when was told his 64-year-old mother might be dead within months unless she got a new kidney  . The worried surgeon brought her to London to be cared for at his hospital. However, it was not all plain sailing. Tests showed Dr Syed was the wrong blood group, so the only way was to go through a special blood-washing process. He consulted colleagues about that, but they didn't agree, because the risk of rejection is still too high. Dr Syed and his mother were then advised to consider a new way of donating and receiving, called an organ-paired. That is, Dr Syed donated his kidney to an unknown person and another donor in the chain was a successful match for his mother. The chain of three transplants took place at the same time on July 31 with Dr Syed's kidney going to a recipient in the Midlands and Mrs. Syed receiving her kidney from a person in the south of England. Just hours after donating his own kidney, Dr Syed found himself recovering in bed next to his mother. Mrs Syed said, "When I came round from my operation Asim was in the next bed and the first thing he said was, 'Mum now all your worries are over.' Tears fell down." Now mother and son are recovering well with Dr Syed already back at work. Mrs. Syed is staying with him for several months while the hospital monitors her progress. He said, "I did what anyone would do when they see a relative suffering disease. Although I wasn't able to help mum directly, by agreeing to be part of a chain, I was also very happy."
03/12/2022 17:58:36 - INFO - __main__ - ['Dr Syed was the wrong blood group.']
03/12/2022 17:58:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 17:58:36 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:58:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 17:58:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 17:58:36 - INFO - __main__ - Printing 3 examples
03/12/2022 17:58:36 - INFO - __main__ -  [race-high] The passage implies that   _  . (A) modern technology is bridging the people (B) modern technology is separating the people (C) modern technology is developing too fast (D) modern technology is interrupting our communication [SEP] A funny thing happened on the way to the communications revolution: we stopped talking to one another. I was walking in the park with a friend recently, and his cell phone rang, interrupting our conversation. There we were walking and talking on a beautiful sunny day and... I became invisible, absent from the conversation. The telephone used to connect you to the absent. Now it makes people sitting next to you feel absent. Why is it that the more connected we get, the more disconnected I feel? Every advance in communications technology is a tragedy to the closeness of human interaction . With email and instant messaging over the internet, we can now communicate without seeing or talking to one another. With voice mail, you can conduct  entire conversations without ever reaching anyone. If my mom  has a question, I just leave the answer on her machine. As almost every contact we can imagine between human  beings gets automated , the alienation index  goes up. You can't  even call a person to get the phone number of another person  any more. Directory assistance is almost always fully automated. I am not against modern technology. I own a cell phone,  an ATM card, a voice mail system, and an email account. Giving them up isn't wise... they're a great help to us. It's some of  their possible consequences that make me feel uneasy. More and more. I find myself hiding behind email to do a job meant for conversation. Or being relieved that voice mail   picked up because I didn't really have time to talk. The industry devoted to helping me keep in touch is making me lonelier ...or at least facilitating my antisocial instincts. So I've put myself on technology restriction: no instant   messaging with people who live near me, no cell phoning in the presence of friends, no letting the voice mail pick up when I'm at home.
03/12/2022 17:58:36 - INFO - __main__ - ['modern technology is separating the people']
03/12/2022 17:58:36 - INFO - __main__ -  [race-high] What's the main idea of the passage? (A) Some manners on visiting British and American people's home. (B) Different table manners between British and American people. (C) Different ideas about the home between British and American people (D) Different ideas about how to get along well with neighbors between British and American people. [SEP] For the British, the home is a private place in which he or she goes to hide away from the troubles of life. It is very seldom that one would be invited to a British person's home. It is rude to knock on a person's door if you are not invited. If you are invited, don't ask to see more than the downstairs that your British host invites you into. Never ask how much the house or any of the items in it cost. To the American, most of them want their home to be a place where they can entertain   and share their lives with their friends. They may be delighted to give you a full tour of their houses. They may also be pleased when you show your interest and pleasure in their houses. Both British and American people will _ quite a bit of chat and a drink or two before the meal is served. After the first mouthful, you should say how delicious the food is and ask something about it. Remember, never eat with your mouth open and make very little noise while eating. It would be nice of you to help your host in any way. Maybe offer to pour some drinks or clear up after the meal.
03/12/2022 17:58:36 - INFO - __main__ - ["Some manners on visiting British and American people's home."]
03/12/2022 17:58:36 - INFO - __main__ -  [race-high] What's the title of the passage? (A) Parents' responsibilities. (B) Advice on self-control. (C) Bad influences of celebrities. (D) Media's bad influences. [SEP] As we know, many teen celebrities  feel and think that having a slimmer figure can do great good to them. But, does size really matter? Are teenage fans trying hard to become like their celebrity idols ? Do celebrities really have the power to influence people, especially teenagers? For the longest time, many parents blame teen idols for influencing the way their kids act. Have you noticed how teens idolize the celebrities these days? Even, their personal affairs are being followed by kids these days. Take for example the case of Lindsay Lohan of Mary Kate Ashley. They are definitely famous teen stars. But, since they are trying to project an image to satisfy a lot of people in show business, their health and body suffer. Many kids are aware of this problem. But they are easily influenced by these celebrities to exercise and eat less. It is a fact that the media, and especially famous teen celebrities, can influence people powerfully. But teenagers are easily influenced because teenage years are the period when our personality and identity developments take place. Teens watching TV shows and reading magazines are easily pulled into the dieting and harmful eating habits because the media have some ways to pull these acts. They use thin models and celebrities to endorse  products or to star in an up-and -coming shows or movies. With fierce competition, celebrities are forced to eat less and do extreme exercise routines to get the roles or offers that come their way. Living in today's time and generation is a bit disturbing to a lot of parents. Media, especially as well as the celebrities, have a very powerful influence to drive teenagers to good or bad. It's good that we can control ourselves to avoid bad things from happening. If not, parents should really be aware and guide their teens to determine what's in ad what's out.
03/12/2022 17:58:36 - INFO - __main__ - ['Bad influences of celebrities.']
03/12/2022 17:58:36 - INFO - __main__ - Tokenizing Input ...
03/12/2022 17:58:36 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:58:36 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 17:58:37 - INFO - __main__ - Global step 600 Train loss 0.668585 ACC 0.03125 on epoch=299
03/12/2022 17:58:37 - INFO - __main__ - save last model!
03/12/2022 17:58:44 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 17:58:45 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 17:58:45 - INFO - __main__ - Printing 3 examples
03/12/2022 17:58:45 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 17:58:45 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 17:58:45 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 17:58:45 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 17:58:45 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 17:58:45 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 17:58:45 - INFO - __main__ - Tokenizing Input ...
03/12/2022 17:58:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 17:58:47 - INFO - __main__ - Starting training!
03/12/2022 17:58:51 - INFO - __main__ - Tokenizing Output ...
03/12/2022 17:58:54 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 18:02:39 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_13_0.0002_8_predictions.txt
03/12/2022 18:02:39 - INFO - __main__ - ACC on test data: 0.1345
03/12/2022 18:02:39 - INFO - __main__ - prefix=race-high_32_13, lr=0.0002, bsz=8, dev_performance=0.15625, test_performance=0.13445378151260504
03/12/2022 18:02:39 - INFO - __main__ - Running ... prefix=race-high_32_13, lr=0.0001, bsz=8 ...
03/12/2022 18:02:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:02:40 - INFO - __main__ - Printing 3 examples
03/12/2022 18:02:40 - INFO - __main__ -  [race-high] Dr La Farge's research is of great importance to   _  . (A) knowing what the plants during the Little Ice Age were like (B) understanding how ecosystems recover from glaciers. (C) regrowing many species that have been destroyed before. (D) figuring out the effects of melting ice caps on moss. [SEP] 400-year-old plants from the Little Ice Age were brought back to life, which could help us understand how the Earth will deal with climate change. Moss  found buried beneath the Teardrop glacier  on Ellesmere Island in Canada has been brought back to life. Findings suggest that these plants could help repopulate regions exposed by melting ice caps. Plants that were buried beneath thick ice in Canada more than 400 years ago and were thought to have frozen to death have been brought back to life by Canadian scientists. Samples of the moss plant, covered by the glacier during the Little Ice Age of 1550 to 1850 AD, were replanted in a lab at the University of Alberta and grew new stems . Researchers now think these findings can give indication as to how regions can recover as the ice covering them melts. Biologist Dr. Catherine La Farge and her team at the University of Alberta were exploring the region around the Teardrop glacier on Ellesmere Island. Ice on Ellesmere Island region has been melting at around four meters each year for the past nine years. This means that many areas of land that were previously covered by ice have since been exposed. Many ecosystems that were thought to have been destroyed during the Little Ice Age between 1550 and 1850 AD can now be studied, including many species that have never been studied before. While examining an exposed area of land, La Farge and her team discovered a small area of moss called Aulacomnium turgidum. It is a type of bryophyte  plant that mainly grows across Canada, the US and the Highlands of Scotland. Dr La Farge noticed that the moss had small patches of green stems, suggesting it is either growing again or can be encouraged to repopulate. Dr La Farge told the BBC, "When we looked at the samples in detail and brought them to the lab, I could see some of the stems actually had new growth of green branches, suggesting that these plants are growing again, and that _ When we think of thick areas of ice covering the landscape, we've always thought that plants have to come from refugia , never considering that land plants come from underneath a glacier. It's a whole world of what's coming out from underneath the glacier that really needs to be studied. The ice is disappearing pretty fast. We really have not examined all the biological systems that exist in the world; we don't know it all." Dr La Farge took samples of the moss and, using carbon-dating techniques, discovered that the plants date back to the Little Ice Age. Dr La Farge's team took the samples, planted them in dishes full of nutrient-rich potting soil and fed them with water. The samples were from four separate species including Aulacomnium turgidum, Distichium capillaceum, Encalypta procera and Syntrichia ruralis. The moss plants found by Dr La Farge are types of bryophytes. Bryophytes can survive long winters and regrow when the weather gets warmer. However, Dr La Farge was surprised that the plants buried under ice have survived into the twenty-first century. Her findings appear in proceedings of the National Academy of Sciences.
03/12/2022 18:02:40 - INFO - __main__ - ['understanding how ecosystems recover from glaciers.']
03/12/2022 18:02:40 - INFO - __main__ -  [race-high] Which of the following statements is TRUE? (A) The museum is opened all the year around. (B) You can usually pay your parking in check. (C) During bears home games visitors can park on the museum Campus for free. (D) After 4:00 p.m.you are not allowed to enter the museum. [SEP] The Field Museum Hours Regular hours are 9:00 a.m.--5:00 p.m. , daily. Last admission at 4:00 p.m. Open every day except Christmas. Admission We have several ticket choices for you to choose from. Advance tickets may be purchased at the will- call booth in person at the museum before 4:00 p.m. Getting Here The Field Museum is located on Chicago's Museum Campus; at 1400 S. Lake Shore Drive, just south of Roosevelt Rd. How to get here : by car or public transit or free trolley. Parking Visitor parking in all lots on the Museum Campus is $ 15.00 per day .This includes the Adler lot, the north garage, the Waldron garage and the east museum lot. Hours for the north garage and Adler lot are 5:00 a.m.--5:00 p.m. Mon--Fri and 6:00a.m. --5:00p.m. weekends; east museum lot9:00 a.m.--3:00p.m. Designated handicapped parking is available in every lot. When all other lots are full , parking will be made available in the remote south lot for$ 8.00 per day. From Memorial Day to Labor Day , the parking lot will only accept cash payments , which will need to be paid upon entering the garage. Please note : These hours and rates are for daytime only and do not apply when special events are scheduled at the museums or Soldier Field. Getting Here During Chicago Bears Home Games During bears home games and other major special events at Soldier Field , access to the Museum Campus can be challenging. No museum visitor parking is available on the Museum Campus during bears home games. However, public transit remains a great way to get to the Campus every day of the year. For more information, call the Regional Transportation Authority at (312) 836 -- 7000 or visit www. rtachicago.com. Additional parking is available at the Monroe Street garage , located at 350 East Monroe Street.
03/12/2022 18:02:40 - INFO - __main__ - ['After 4:00 p.m.you are not allowed to enter the museum.']
03/12/2022 18:02:40 - INFO - __main__ -  [race-high] Why isn't it a plain sailing? (A) No one can treat his mother well. (B) Dr Syed was the wrong blood group. (C) They didn't have money to be in hospital. (D) Mrs. Syed was unwilling to receive the operation. [SEP] Dr Asim Syed, 32, has performed more than 100 operations at London's Hammersmith Hospital in the country's busiest transplant unit, but never imagined that he would one day become a donor himself. He stepped forward when was told his 64-year-old mother might be dead within months unless she got a new kidney  . The worried surgeon brought her to London to be cared for at his hospital. However, it was not all plain sailing. Tests showed Dr Syed was the wrong blood group, so the only way was to go through a special blood-washing process. He consulted colleagues about that, but they didn't agree, because the risk of rejection is still too high. Dr Syed and his mother were then advised to consider a new way of donating and receiving, called an organ-paired. That is, Dr Syed donated his kidney to an unknown person and another donor in the chain was a successful match for his mother. The chain of three transplants took place at the same time on July 31 with Dr Syed's kidney going to a recipient in the Midlands and Mrs. Syed receiving her kidney from a person in the south of England. Just hours after donating his own kidney, Dr Syed found himself recovering in bed next to his mother. Mrs Syed said, "When I came round from my operation Asim was in the next bed and the first thing he said was, 'Mum now all your worries are over.' Tears fell down." Now mother and son are recovering well with Dr Syed already back at work. Mrs. Syed is staying with him for several months while the hospital monitors her progress. He said, "I did what anyone would do when they see a relative suffering disease. Although I wasn't able to help mum directly, by agreeing to be part of a chain, I was also very happy."
03/12/2022 18:02:40 - INFO - __main__ - ['Dr Syed was the wrong blood group.']
03/12/2022 18:02:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 18:02:40 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:02:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 18:02:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:02:40 - INFO - __main__ - Printing 3 examples
03/12/2022 18:02:40 - INFO - __main__ -  [race-high] The passage implies that   _  . (A) modern technology is bridging the people (B) modern technology is separating the people (C) modern technology is developing too fast (D) modern technology is interrupting our communication [SEP] A funny thing happened on the way to the communications revolution: we stopped talking to one another. I was walking in the park with a friend recently, and his cell phone rang, interrupting our conversation. There we were walking and talking on a beautiful sunny day and... I became invisible, absent from the conversation. The telephone used to connect you to the absent. Now it makes people sitting next to you feel absent. Why is it that the more connected we get, the more disconnected I feel? Every advance in communications technology is a tragedy to the closeness of human interaction . With email and instant messaging over the internet, we can now communicate without seeing or talking to one another. With voice mail, you can conduct  entire conversations without ever reaching anyone. If my mom  has a question, I just leave the answer on her machine. As almost every contact we can imagine between human  beings gets automated , the alienation index  goes up. You can't  even call a person to get the phone number of another person  any more. Directory assistance is almost always fully automated. I am not against modern technology. I own a cell phone,  an ATM card, a voice mail system, and an email account. Giving them up isn't wise... they're a great help to us. It's some of  their possible consequences that make me feel uneasy. More and more. I find myself hiding behind email to do a job meant for conversation. Or being relieved that voice mail   picked up because I didn't really have time to talk. The industry devoted to helping me keep in touch is making me lonelier ...or at least facilitating my antisocial instincts. So I've put myself on technology restriction: no instant   messaging with people who live near me, no cell phoning in the presence of friends, no letting the voice mail pick up when I'm at home.
03/12/2022 18:02:40 - INFO - __main__ - ['modern technology is separating the people']
03/12/2022 18:02:40 - INFO - __main__ -  [race-high] What's the main idea of the passage? (A) Some manners on visiting British and American people's home. (B) Different table manners between British and American people. (C) Different ideas about the home between British and American people (D) Different ideas about how to get along well with neighbors between British and American people. [SEP] For the British, the home is a private place in which he or she goes to hide away from the troubles of life. It is very seldom that one would be invited to a British person's home. It is rude to knock on a person's door if you are not invited. If you are invited, don't ask to see more than the downstairs that your British host invites you into. Never ask how much the house or any of the items in it cost. To the American, most of them want their home to be a place where they can entertain   and share their lives with their friends. They may be delighted to give you a full tour of their houses. They may also be pleased when you show your interest and pleasure in their houses. Both British and American people will _ quite a bit of chat and a drink or two before the meal is served. After the first mouthful, you should say how delicious the food is and ask something about it. Remember, never eat with your mouth open and make very little noise while eating. It would be nice of you to help your host in any way. Maybe offer to pour some drinks or clear up after the meal.
03/12/2022 18:02:40 - INFO - __main__ - ["Some manners on visiting British and American people's home."]
03/12/2022 18:02:40 - INFO - __main__ -  [race-high] What's the title of the passage? (A) Parents' responsibilities. (B) Advice on self-control. (C) Bad influences of celebrities. (D) Media's bad influences. [SEP] As we know, many teen celebrities  feel and think that having a slimmer figure can do great good to them. But, does size really matter? Are teenage fans trying hard to become like their celebrity idols ? Do celebrities really have the power to influence people, especially teenagers? For the longest time, many parents blame teen idols for influencing the way their kids act. Have you noticed how teens idolize the celebrities these days? Even, their personal affairs are being followed by kids these days. Take for example the case of Lindsay Lohan of Mary Kate Ashley. They are definitely famous teen stars. But, since they are trying to project an image to satisfy a lot of people in show business, their health and body suffer. Many kids are aware of this problem. But they are easily influenced by these celebrities to exercise and eat less. It is a fact that the media, and especially famous teen celebrities, can influence people powerfully. But teenagers are easily influenced because teenage years are the period when our personality and identity developments take place. Teens watching TV shows and reading magazines are easily pulled into the dieting and harmful eating habits because the media have some ways to pull these acts. They use thin models and celebrities to endorse  products or to star in an up-and -coming shows or movies. With fierce competition, celebrities are forced to eat less and do extreme exercise routines to get the roles or offers that come their way. Living in today's time and generation is a bit disturbing to a lot of parents. Media, especially as well as the celebrities, have a very powerful influence to drive teenagers to good or bad. It's good that we can control ourselves to avoid bad things from happening. If not, parents should really be aware and guide their teens to determine what's in ad what's out.
03/12/2022 18:02:40 - INFO - __main__ - ['Bad influences of celebrities.']
03/12/2022 18:02:40 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:02:40 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:02:40 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 18:02:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 18:02:51 - INFO - __main__ - Starting training!
03/12/2022 18:02:57 - INFO - __main__ - Step 10 Global step 10 Train loss 19.664906 on epoch=4
03/12/2022 18:03:02 - INFO - __main__ - Step 20 Global step 20 Train loss 18.658560 on epoch=9
03/12/2022 18:03:08 - INFO - __main__ - Step 30 Global step 30 Train loss 14.496260 on epoch=14
03/12/2022 18:03:14 - INFO - __main__ - Step 40 Global step 40 Train loss 13.299292 on epoch=19
03/12/2022 18:03:20 - INFO - __main__ - Step 50 Global step 50 Train loss 11.451723 on epoch=24
03/12/2022 18:03:34 - INFO - __main__ - Global step 50 Train loss 15.514149 ACC 0.0 on epoch=24
03/12/2022 18:03:41 - INFO - __main__ - Step 60 Global step 60 Train loss 9.948249 on epoch=29
03/12/2022 18:03:47 - INFO - __main__ - Step 70 Global step 70 Train loss 8.211252 on epoch=34
03/12/2022 18:03:53 - INFO - __main__ - Step 80 Global step 80 Train loss 7.367646 on epoch=39
03/12/2022 18:03:59 - INFO - __main__ - Step 90 Global step 90 Train loss 6.974280 on epoch=44
03/12/2022 18:04:05 - INFO - __main__ - Step 100 Global step 100 Train loss 6.402796 on epoch=49
03/12/2022 18:04:07 - INFO - __main__ - Global step 100 Train loss 7.780845 ACC 0.0625 on epoch=49
03/12/2022 18:04:14 - INFO - __main__ - Step 110 Global step 110 Train loss 5.301650 on epoch=54
03/12/2022 18:04:20 - INFO - __main__ - Step 120 Global step 120 Train loss 4.514147 on epoch=59
03/12/2022 18:04:26 - INFO - __main__ - Step 130 Global step 130 Train loss 4.294959 on epoch=64
03/12/2022 18:04:32 - INFO - __main__ - Step 140 Global step 140 Train loss 3.546277 on epoch=69
03/12/2022 18:04:38 - INFO - __main__ - Step 150 Global step 150 Train loss 3.519766 on epoch=74
03/12/2022 18:04:40 - INFO - __main__ - Global step 150 Train loss 4.235360 ACC 0.125 on epoch=74
03/12/2022 18:04:47 - INFO - __main__ - Step 160 Global step 160 Train loss 3.655179 on epoch=79
03/12/2022 18:04:53 - INFO - __main__ - Step 170 Global step 170 Train loss 3.323558 on epoch=84
03/12/2022 18:05:00 - INFO - __main__ - Step 180 Global step 180 Train loss 3.065001 on epoch=89
03/12/2022 18:05:06 - INFO - __main__ - Step 190 Global step 190 Train loss 3.056721 on epoch=94
03/12/2022 18:05:12 - INFO - __main__ - Step 200 Global step 200 Train loss 2.906128 on epoch=99
03/12/2022 18:05:14 - INFO - __main__ - Global step 200 Train loss 3.201318 ACC 0.0625 on epoch=99
03/12/2022 18:05:20 - INFO - __main__ - Step 210 Global step 210 Train loss 2.986165 on epoch=104
03/12/2022 18:05:27 - INFO - __main__ - Step 220 Global step 220 Train loss 2.830606 on epoch=109
03/12/2022 18:05:33 - INFO - __main__ - Step 230 Global step 230 Train loss 2.520565 on epoch=114
03/12/2022 18:05:39 - INFO - __main__ - Step 240 Global step 240 Train loss 2.643960 on epoch=119
03/12/2022 18:05:45 - INFO - __main__ - Step 250 Global step 250 Train loss 2.500521 on epoch=124
03/12/2022 18:05:47 - INFO - __main__ - Global step 250 Train loss 2.696363 ACC 0.09375 on epoch=124
03/12/2022 18:05:54 - INFO - __main__ - Step 260 Global step 260 Train loss 2.409932 on epoch=129
03/12/2022 18:06:00 - INFO - __main__ - Step 270 Global step 270 Train loss 2.400959 on epoch=134
03/12/2022 18:06:06 - INFO - __main__ - Step 280 Global step 280 Train loss 2.369668 on epoch=139
03/12/2022 18:06:12 - INFO - __main__ - Step 290 Global step 290 Train loss 2.269460 on epoch=144
03/12/2022 18:06:18 - INFO - __main__ - Step 300 Global step 300 Train loss 2.223023 on epoch=149
03/12/2022 18:06:21 - INFO - __main__ - Global step 300 Train loss 2.334608 ACC 0.15625 on epoch=149
03/12/2022 18:06:28 - INFO - __main__ - Step 310 Global step 310 Train loss 2.198172 on epoch=154
03/12/2022 18:06:34 - INFO - __main__ - Step 320 Global step 320 Train loss 2.069130 on epoch=159
03/12/2022 18:06:40 - INFO - __main__ - Step 330 Global step 330 Train loss 1.900909 on epoch=164
03/12/2022 18:06:46 - INFO - __main__ - Step 340 Global step 340 Train loss 1.871315 on epoch=169
03/12/2022 18:06:52 - INFO - __main__ - Step 350 Global step 350 Train loss 1.830680 on epoch=174
03/12/2022 18:06:54 - INFO - __main__ - Global step 350 Train loss 1.974041 ACC 0.125 on epoch=174
03/12/2022 18:07:01 - INFO - __main__ - Step 360 Global step 360 Train loss 1.755164 on epoch=179
03/12/2022 18:07:07 - INFO - __main__ - Step 370 Global step 370 Train loss 1.681200 on epoch=184
03/12/2022 18:07:13 - INFO - __main__ - Step 380 Global step 380 Train loss 1.549230 on epoch=189
03/12/2022 18:07:19 - INFO - __main__ - Step 390 Global step 390 Train loss 1.596995 on epoch=194
03/12/2022 18:07:25 - INFO - __main__ - Step 400 Global step 400 Train loss 1.345889 on epoch=199
03/12/2022 18:07:27 - INFO - __main__ - Global step 400 Train loss 1.585695 ACC 0.125 on epoch=199
03/12/2022 18:07:33 - INFO - __main__ - Step 410 Global step 410 Train loss 1.603435 on epoch=204
03/12/2022 18:07:39 - INFO - __main__ - Step 420 Global step 420 Train loss 1.394224 on epoch=209
03/12/2022 18:07:45 - INFO - __main__ - Step 430 Global step 430 Train loss 1.540027 on epoch=214
03/12/2022 18:07:52 - INFO - __main__ - Step 440 Global step 440 Train loss 1.449148 on epoch=219
03/12/2022 18:07:58 - INFO - __main__ - Step 450 Global step 450 Train loss 1.304475 on epoch=224
03/12/2022 18:08:00 - INFO - __main__ - Global step 450 Train loss 1.458262 ACC 0.125 on epoch=224
03/12/2022 18:08:06 - INFO - __main__ - Step 460 Global step 460 Train loss 1.370225 on epoch=229
03/12/2022 18:08:12 - INFO - __main__ - Step 470 Global step 470 Train loss 1.311700 on epoch=234
03/12/2022 18:08:18 - INFO - __main__ - Step 480 Global step 480 Train loss 1.161155 on epoch=239
03/12/2022 18:08:24 - INFO - __main__ - Step 490 Global step 490 Train loss 1.207803 on epoch=244
03/12/2022 18:08:30 - INFO - __main__ - Step 500 Global step 500 Train loss 1.176776 on epoch=249
03/12/2022 18:08:32 - INFO - __main__ - Global step 500 Train loss 1.245532 ACC 0.09375 on epoch=249
03/12/2022 18:08:38 - INFO - __main__ - Step 510 Global step 510 Train loss 1.199374 on epoch=254
03/12/2022 18:08:44 - INFO - __main__ - Step 520 Global step 520 Train loss 1.136468 on epoch=259
03/12/2022 18:08:50 - INFO - __main__ - Step 530 Global step 530 Train loss 1.129008 on epoch=264
03/12/2022 18:08:56 - INFO - __main__ - Step 540 Global step 540 Train loss 1.215498 on epoch=269
03/12/2022 18:09:02 - INFO - __main__ - Step 550 Global step 550 Train loss 1.125143 on epoch=274
03/12/2022 18:09:05 - INFO - __main__ - Global step 550 Train loss 1.161098 ACC 0.0625 on epoch=274
03/12/2022 18:09:11 - INFO - __main__ - Step 560 Global step 560 Train loss 1.041912 on epoch=279
03/12/2022 18:09:17 - INFO - __main__ - Step 570 Global step 570 Train loss 1.006856 on epoch=284
03/12/2022 18:09:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.994277 on epoch=289
03/12/2022 18:09:29 - INFO - __main__ - Step 590 Global step 590 Train loss 1.121881 on epoch=294
03/12/2022 18:09:36 - INFO - __main__ - Step 600 Global step 600 Train loss 1.053382 on epoch=299
03/12/2022 18:09:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:09:37 - INFO - __main__ - Printing 3 examples
03/12/2022 18:09:37 - INFO - __main__ -  [race-high] What is special about Sydney's Great Coastal Walk? (A) It starts from Royal National Park in the south. (B) It takes about more than five hours to complete. (C) It really has the longest coastline in the world. (D) It provides visitors a variety of great landscapes. [SEP] Beaches are not only great for lying on and doing water sports, and in fact one of the best ways of enjoying them is a classic beach walk. Here at iWantSun. Co. Uk, we've been searching the globe to find you the world's best and most glorious beach walks, and here's our pick of the top. The Footpath of the Gods, Amalfi Coast, Italy The name says it all really and you truly do feel up there to walking along this wonderful mountain coastal path, which offers some of the most striking views on the planet. The path begins at town of Bomerano to charming Positano along the UNESCO World Heritage area of the Amalfi Coast. The whole walk will take you approximately four and a half hours to complete and pass over narrow rocky paths, past sheer cliffs and shining blue bays. Sydney's Great Coastal Walk, Australia Sydney's coastline is one of the most beautiful and diverse in the world. Here you have national parks, historic sites, steep cliffs, sparkling beaches and quiet bays all in one place. Sydney's Great Walk runs all the way from Barrenjoey in the north to Royal National Park in the south and takes an incredible seven days to complete. However, if you're not up to doing the full walk, then there are many different parts of the walk that you can do right in the city. Walking from the city's famous Bondi Beach to the sweeping curve of Bronte Beach takes just an hour, which takes in some top scenery. Great Ocean Walk, Australia The Great Ocean Walk stretches 104 km along Victoria's famous Great Ocean Road, located on the southern coast of Australia, from the resort town Apollo Bay to the magnificent Twelve Apostles. The Twelve Apostles are the area's famous stone landmarks which stand out like giants from the sea. The walk passes through a range of landscapes and sights, from national parks, famous surfing spots and deserted beaches, to wild coastlines, cascading waterfalls, lush forests, historic lighthouses and ghostly shipwrecks. Day walks and shorter three-hour walks such as the Wreck Beach Walk or the Lighthouse Cemetery and Lookout Walk can also be enjoyed. So next time when you're looking for a beach holiday don't just think about the resorts and the sand, but consider a more active sun holiday, discovering some of the best beaches in the world.
03/12/2022 18:09:37 - INFO - __main__ - ['It provides visitors a variety of great landscapes.']
03/12/2022 18:09:37 - INFO - __main__ -  [race-high] In the author's opinion, which of the following is NOT true? (A) Nearly everyone has been told a wrong number. (B) It's necessary for everyone to have a telephone. (C) He himself can not decide whether to answer a call. (D) A telephone directory may bring in unexpected calls. [SEP] Many people think a telephone is essential. But I think it is a pest and a time waster. Very often you find it impossible to escape from some idle or curious chatter-box, or from somebody who wants something for nothing. If you have a telephone in your own house, you will admit that it tends to ring when you are asleep, or in the middle of a meal or a conversation, or when you are just going out, or when you are taking your bath. Are you strong minded enough to ignore it, to say to yourself,"Ah, well, it will all be the same in a hundred years' time" You are not. You think there may be some important news or messages for you. I can assure you that if a message is really important it will reach you sooner or later. Have you never rushed dripping from the bath, or chewing from the table, or dazed from the bed, only to be told that you are a wrong number? But you will say, you need not have your name printed in the telephone directory, and you can have a telephone which is only usable for outgoing calls. Besides, you will say, isn't it important to have a telephone in case of emergency--illness, an accident, or fire? Of course, you are right, but here in a thickly populated country like England one is seldom far from a telephone in case of dreadful necessity. I think perhaps I had better try to justify myself by trying to prove that what I like is good. I admit that in different circumstances--if I were a tycoon(business VIP),for instance, or bed ridden I might find a telephone essential. But then if I were a taxi-driver I should find a car essential. Let me put it another way: there are two things for which the English seem to show particular talent; one is mechanical invention, the other is literature. My own business happens to be with the use of words but I see I must now stop using them. For I have just been handed a slip of paper to say that somebody is waiting to speak to me on the telephone. I think I had better answer it. After all, one never knows, it may be something important.
03/12/2022 18:09:37 - INFO - __main__ - ["It's necessary for everyone to have a telephone."]
03/12/2022 18:09:37 - INFO - __main__ -  [race-high] Visitors who use Fast Pass tickets   _ (A) can get discounts on tickets (B) will have a well-planned ride time (C) will often be checked at the exit (D) have to pay more for their tickets [SEP] As for visiting Walt Disney World. People usually have trouble in arranging. The following will give you some practical suggestions on paying a valuable visit to the famous park. When to go? While it's always busy, Disney World is slightly slower between mid-January and March. It's summer when the visitors , combined with the heat and humidity , are crowded. Your best choice is either early in the morning or late in the day--- often you can enjoy more rides in the first or last two hours of the day than you can the entire rest of the day combined. Where to stay? The question: to stay on Disney World Resort property or not. Disney World offers three resort pricing: Deluxe , Moderate, and Value, with good facilities and in-hotel attractions. That said, they all offer "Extra Magic Hours"(early access to the theme parks), guaranteed entry when the parks are over-crowded, and frequent shuttle service to the many attractions. Off-property hotels will have shuttle transfers, but often at less regular intervals ,wasting more time in transport. You'll also need a rental car, which adds to the expense, making up for saving in the hotel rate. The "Magic Your Way Package" is a good value if you have a large family. What to do? Make use of the "Fast Pass" , which , for no extra cost, allow you to insert your entrance ticket into a particular machine at the attraction and receive a schedule ride time a little later on. For example, use your Fast Pass ticket at a ride just before getting lunch. Once you're done eating, you can return to the ride and jump right on during the busiest time of day. At the Animal Kingdom, the animals are most active either early or late in the day, so plan accordingly. Thankfully _ is fit for the least busy times of day in the park. Study the map of the park the night before to plan your visit. You'll save time by knowing where things are and where you're headed.
03/12/2022 18:09:37 - INFO - __main__ - ['will have a well-planned ride time']
03/12/2022 18:09:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 18:09:37 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:09:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 18:09:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:09:37 - INFO - __main__ - Printing 3 examples
03/12/2022 18:09:37 - INFO - __main__ -  [race-high] Which of the following best serves as the best title of the story? (A) Touching by Feeling (B) To See or to Feel (C) Seeing by Feeling (D) Seeing Is Believing [SEP] "If you want to see something well, reach out and touch it!" That may seem a strange thing to say.But touching things can help you to see them better. Your eyes can tell you that a glass ball is round.But by holding it in your hands, you can feel how smooth and cool the ball is.You can feel how heavy the glass is.When you feel all these about the ball, you really see it. With your skin, you can feel better.For example, your fingers can tell the difference between two coins in your pockets.You can feel a little drop of water on the back of your hand, too. You can even feel sounds against your skin.Have you ever wanted to know why some people like very loud music? They must like to feel the sounds of music. All children soon learn what "Don't touch!" means.They hear it often.Yet most of us keep on touching things as we grow up.In shops, we often have to touch things before we buy them. The bottoms of our feet can feel things, too.You know this when you walk on warm sand, cool grass or a hard floor.All feel different under your feet. There are ways of learning to see well by feeling.One way is to close your eyes and try to feel everything that is touching your skin.Feel the shoes on your feet, the clothes on your body, the air on your skin...... Most museums are just for looking.But today some museums have some things to touch.Their signs say, "Do touch!" There you can feel everything on show.
03/12/2022 18:09:37 - INFO - __main__ - ['Seeing by Feeling']
03/12/2022 18:09:37 - INFO - __main__ -  [race-high] According to the author, the flu  _  . (A) doesn't cause sneezes (B) should be paid serious attention to (C) can be caused by spreading much time indoors (D) only spreads among the very young and the very old. [SEP] It's flu season.  What are you doing about it? David Oreck, Founder  My Oreck Air Purifier captures and destroys viruses, bacteria and germs.  2005 ORECK HOLDINGS,LLC. All Rights Reserved. All word marks, logos, product configurations and registered trademarks are owned and used under the authority of Oreck Holdings, LLC. For the very young and the very old and virtually everyone in between, the flu is nothing to sneeze at. So here's what you can do. Check with your doctor to see if a flu shot is right for you. Wash your hands frequently. Maintain a healthy diet and regimen of exercise. And because you're spending most of your time indoors, it makes sense that the air in your home is as fresh, clean and pure as it can be.  My Oreck XL(tm) Professional Air Purifier captures and destroys viruses, bacteria and germs. It removes mold spores, pollen, pet dander, household odors and other airborne particulates as small as 0.1 microns. (The flu virus measures 0.3 microns.) The Oreck Professional Air Purifier uses the same advanced technology as the prefix = st1 /U.S.submarine fleet where mariners are submerged for up to six months at a time. Because the permanent electronic filter never needs replacing, you can rinse it off as often as you like and it's as good as new. My Oreck Professional Air Purifier will literally pay for itself in what you pay for costly HEPA replacement filters that other manufacturers require. Besides HEPA filters don't destroy germs. They can only capture and hold them. So this flu season, take the Oreck Challenge and try my Oreck Professional Air Purifier risk-free for three full months. Then decide. CALL NOW AND RECEIVE A $100 GIFT-FREE.  Just for trying an Oreck Air Purifier risk-free for 90 days, we'll send you our $100 Oreck Cord-Free Electric Broom  ly free. It's a floor vac and a hand vac in one. If you don't feel the difference simply send the Air Purifier back--but keep the Electric Broom--there's no obligation. The shipping is free. Call 1-800-522-5961 ext. CR589 or visit www. oreck. com/decairNothing gets by an Oreck.
03/12/2022 18:09:37 - INFO - __main__ - ['should be paid serious attention to']
03/12/2022 18:09:37 - INFO - __main__ -  [race-high] We can draw the conclusion that Mr.Rady's work in Iraq is very_. (A) happy (B) tiring (C) busy (D) dangerous [SEP] When he stopped his car at a traffic light in Basra, Bassam Rady noticed the motorbike with two riders on it passed by him slowly. Suddenly the bike turned round and Mr. Rady, feeling dangerous, tried to drive off. Before he could, a man on the back of the bike took out a gun from his jacket and fired. The bullet went through the windscreen and just missed him. As he sped away another shot was fired, but missed the car.  Mr. Rady was an interpreter for British army in Iraq. As such, his life is in danger from the militia that once controlled the Iraqi city and is now returning.  More than a year ago Iraqi soldiers, backed by US, brought peace and stability to Basra by driving militants over the Iranian border in an operation called Charge of the Knights.  According to local estimates, however, about half have returned. Although they have not become as active as before, the militants are trying to shoot Iraqi citizens who worked with British. Most in danger are translators such as Mr. Rady.  The 31-year-old father worked with soldiers on dangerous missions but was refused resettlement in Britain at the end of his employment. He worked with nine translators. Seven of them have been killed.  "I'm like a cancer patient -- now that the militia is back, my family is just waiting for me to die," Mr. Rady said. "I see reports in the media that Basra is safe but it's not true. I know these militia people. I went to school with some of them. I didn't see them for a year but now they are around again. They have told me, 'Your day will come soon'." He takes security precautions and never follows a routine. This means that he cannot work because that would give the militants a better chance of success.  Militiamen belonging to the alMahdi Army took control of Basra between 2004 and 2007 with other Shia fighters. They enforced strict Islamic rules at the same time as running criminal rackets. British army was unable or unwilling to fight back and eventually went back to their base at the airport.
03/12/2022 18:09:37 - INFO - __main__ - ['dangerous']
03/12/2022 18:09:37 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:09:37 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:09:37 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 18:09:38 - INFO - __main__ - Global step 600 Train loss 1.043661 ACC 0.0625 on epoch=299
03/12/2022 18:09:38 - INFO - __main__ - save last model!
03/12/2022 18:09:45 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 18:09:46 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 18:09:46 - INFO - __main__ - Printing 3 examples
03/12/2022 18:09:46 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:09:46 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 18:09:46 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:09:46 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 18:09:46 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:09:46 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 18:09:46 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:09:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 18:09:48 - INFO - __main__ - Starting training!
03/12/2022 18:09:52 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:09:55 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 18:13:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_13_0.0001_8_predictions.txt
03/12/2022 18:13:43 - INFO - __main__ - ACC on test data: 0.1507
03/12/2022 18:13:43 - INFO - __main__ - prefix=race-high_32_13, lr=0.0001, bsz=8, dev_performance=0.15625, test_performance=0.1506809620399884
03/12/2022 18:13:43 - INFO - __main__ - Running ... prefix=race-high_32_21, lr=0.0005, bsz=8 ...
03/12/2022 18:13:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:13:44 - INFO - __main__ - Printing 3 examples
03/12/2022 18:13:44 - INFO - __main__ -  [race-high] What is special about Sydney's Great Coastal Walk? (A) It starts from Royal National Park in the south. (B) It takes about more than five hours to complete. (C) It really has the longest coastline in the world. (D) It provides visitors a variety of great landscapes. [SEP] Beaches are not only great for lying on and doing water sports, and in fact one of the best ways of enjoying them is a classic beach walk. Here at iWantSun. Co. Uk, we've been searching the globe to find you the world's best and most glorious beach walks, and here's our pick of the top. The Footpath of the Gods, Amalfi Coast, Italy The name says it all really and you truly do feel up there to walking along this wonderful mountain coastal path, which offers some of the most striking views on the planet. The path begins at town of Bomerano to charming Positano along the UNESCO World Heritage area of the Amalfi Coast. The whole walk will take you approximately four and a half hours to complete and pass over narrow rocky paths, past sheer cliffs and shining blue bays. Sydney's Great Coastal Walk, Australia Sydney's coastline is one of the most beautiful and diverse in the world. Here you have national parks, historic sites, steep cliffs, sparkling beaches and quiet bays all in one place. Sydney's Great Walk runs all the way from Barrenjoey in the north to Royal National Park in the south and takes an incredible seven days to complete. However, if you're not up to doing the full walk, then there are many different parts of the walk that you can do right in the city. Walking from the city's famous Bondi Beach to the sweeping curve of Bronte Beach takes just an hour, which takes in some top scenery. Great Ocean Walk, Australia The Great Ocean Walk stretches 104 km along Victoria's famous Great Ocean Road, located on the southern coast of Australia, from the resort town Apollo Bay to the magnificent Twelve Apostles. The Twelve Apostles are the area's famous stone landmarks which stand out like giants from the sea. The walk passes through a range of landscapes and sights, from national parks, famous surfing spots and deserted beaches, to wild coastlines, cascading waterfalls, lush forests, historic lighthouses and ghostly shipwrecks. Day walks and shorter three-hour walks such as the Wreck Beach Walk or the Lighthouse Cemetery and Lookout Walk can also be enjoyed. So next time when you're looking for a beach holiday don't just think about the resorts and the sand, but consider a more active sun holiday, discovering some of the best beaches in the world.
03/12/2022 18:13:44 - INFO - __main__ - ['It provides visitors a variety of great landscapes.']
03/12/2022 18:13:44 - INFO - __main__ -  [race-high] In the author's opinion, which of the following is NOT true? (A) Nearly everyone has been told a wrong number. (B) It's necessary for everyone to have a telephone. (C) He himself can not decide whether to answer a call. (D) A telephone directory may bring in unexpected calls. [SEP] Many people think a telephone is essential. But I think it is a pest and a time waster. Very often you find it impossible to escape from some idle or curious chatter-box, or from somebody who wants something for nothing. If you have a telephone in your own house, you will admit that it tends to ring when you are asleep, or in the middle of a meal or a conversation, or when you are just going out, or when you are taking your bath. Are you strong minded enough to ignore it, to say to yourself,"Ah, well, it will all be the same in a hundred years' time" You are not. You think there may be some important news or messages for you. I can assure you that if a message is really important it will reach you sooner or later. Have you never rushed dripping from the bath, or chewing from the table, or dazed from the bed, only to be told that you are a wrong number? But you will say, you need not have your name printed in the telephone directory, and you can have a telephone which is only usable for outgoing calls. Besides, you will say, isn't it important to have a telephone in case of emergency--illness, an accident, or fire? Of course, you are right, but here in a thickly populated country like England one is seldom far from a telephone in case of dreadful necessity. I think perhaps I had better try to justify myself by trying to prove that what I like is good. I admit that in different circumstances--if I were a tycoon(business VIP),for instance, or bed ridden I might find a telephone essential. But then if I were a taxi-driver I should find a car essential. Let me put it another way: there are two things for which the English seem to show particular talent; one is mechanical invention, the other is literature. My own business happens to be with the use of words but I see I must now stop using them. For I have just been handed a slip of paper to say that somebody is waiting to speak to me on the telephone. I think I had better answer it. After all, one never knows, it may be something important.
03/12/2022 18:13:44 - INFO - __main__ - ["It's necessary for everyone to have a telephone."]
03/12/2022 18:13:44 - INFO - __main__ -  [race-high] Visitors who use Fast Pass tickets   _ (A) can get discounts on tickets (B) will have a well-planned ride time (C) will often be checked at the exit (D) have to pay more for their tickets [SEP] As for visiting Walt Disney World. People usually have trouble in arranging. The following will give you some practical suggestions on paying a valuable visit to the famous park. When to go? While it's always busy, Disney World is slightly slower between mid-January and March. It's summer when the visitors , combined with the heat and humidity , are crowded. Your best choice is either early in the morning or late in the day--- often you can enjoy more rides in the first or last two hours of the day than you can the entire rest of the day combined. Where to stay? The question: to stay on Disney World Resort property or not. Disney World offers three resort pricing: Deluxe , Moderate, and Value, with good facilities and in-hotel attractions. That said, they all offer "Extra Magic Hours"(early access to the theme parks), guaranteed entry when the parks are over-crowded, and frequent shuttle service to the many attractions. Off-property hotels will have shuttle transfers, but often at less regular intervals ,wasting more time in transport. You'll also need a rental car, which adds to the expense, making up for saving in the hotel rate. The "Magic Your Way Package" is a good value if you have a large family. What to do? Make use of the "Fast Pass" , which , for no extra cost, allow you to insert your entrance ticket into a particular machine at the attraction and receive a schedule ride time a little later on. For example, use your Fast Pass ticket at a ride just before getting lunch. Once you're done eating, you can return to the ride and jump right on during the busiest time of day. At the Animal Kingdom, the animals are most active either early or late in the day, so plan accordingly. Thankfully _ is fit for the least busy times of day in the park. Study the map of the park the night before to plan your visit. You'll save time by knowing where things are and where you're headed.
03/12/2022 18:13:44 - INFO - __main__ - ['will have a well-planned ride time']
03/12/2022 18:13:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 18:13:44 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:13:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 18:13:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:13:44 - INFO - __main__ - Printing 3 examples
03/12/2022 18:13:44 - INFO - __main__ -  [race-high] Which of the following best serves as the best title of the story? (A) Touching by Feeling (B) To See or to Feel (C) Seeing by Feeling (D) Seeing Is Believing [SEP] "If you want to see something well, reach out and touch it!" That may seem a strange thing to say.But touching things can help you to see them better. Your eyes can tell you that a glass ball is round.But by holding it in your hands, you can feel how smooth and cool the ball is.You can feel how heavy the glass is.When you feel all these about the ball, you really see it. With your skin, you can feel better.For example, your fingers can tell the difference between two coins in your pockets.You can feel a little drop of water on the back of your hand, too. You can even feel sounds against your skin.Have you ever wanted to know why some people like very loud music? They must like to feel the sounds of music. All children soon learn what "Don't touch!" means.They hear it often.Yet most of us keep on touching things as we grow up.In shops, we often have to touch things before we buy them. The bottoms of our feet can feel things, too.You know this when you walk on warm sand, cool grass or a hard floor.All feel different under your feet. There are ways of learning to see well by feeling.One way is to close your eyes and try to feel everything that is touching your skin.Feel the shoes on your feet, the clothes on your body, the air on your skin...... Most museums are just for looking.But today some museums have some things to touch.Their signs say, "Do touch!" There you can feel everything on show.
03/12/2022 18:13:44 - INFO - __main__ - ['Seeing by Feeling']
03/12/2022 18:13:44 - INFO - __main__ -  [race-high] According to the author, the flu  _  . (A) doesn't cause sneezes (B) should be paid serious attention to (C) can be caused by spreading much time indoors (D) only spreads among the very young and the very old. [SEP] It's flu season.  What are you doing about it? David Oreck, Founder  My Oreck Air Purifier captures and destroys viruses, bacteria and germs.  2005 ORECK HOLDINGS,LLC. All Rights Reserved. All word marks, logos, product configurations and registered trademarks are owned and used under the authority of Oreck Holdings, LLC. For the very young and the very old and virtually everyone in between, the flu is nothing to sneeze at. So here's what you can do. Check with your doctor to see if a flu shot is right for you. Wash your hands frequently. Maintain a healthy diet and regimen of exercise. And because you're spending most of your time indoors, it makes sense that the air in your home is as fresh, clean and pure as it can be.  My Oreck XL(tm) Professional Air Purifier captures and destroys viruses, bacteria and germs. It removes mold spores, pollen, pet dander, household odors and other airborne particulates as small as 0.1 microns. (The flu virus measures 0.3 microns.) The Oreck Professional Air Purifier uses the same advanced technology as the prefix = st1 /U.S.submarine fleet where mariners are submerged for up to six months at a time. Because the permanent electronic filter never needs replacing, you can rinse it off as often as you like and it's as good as new. My Oreck Professional Air Purifier will literally pay for itself in what you pay for costly HEPA replacement filters that other manufacturers require. Besides HEPA filters don't destroy germs. They can only capture and hold them. So this flu season, take the Oreck Challenge and try my Oreck Professional Air Purifier risk-free for three full months. Then decide. CALL NOW AND RECEIVE A $100 GIFT-FREE.  Just for trying an Oreck Air Purifier risk-free for 90 days, we'll send you our $100 Oreck Cord-Free Electric Broom  ly free. It's a floor vac and a hand vac in one. If you don't feel the difference simply send the Air Purifier back--but keep the Electric Broom--there's no obligation. The shipping is free. Call 1-800-522-5961 ext. CR589 or visit www. oreck. com/decairNothing gets by an Oreck.
03/12/2022 18:13:44 - INFO - __main__ - ['should be paid serious attention to']
03/12/2022 18:13:44 - INFO - __main__ -  [race-high] We can draw the conclusion that Mr.Rady's work in Iraq is very_. (A) happy (B) tiring (C) busy (D) dangerous [SEP] When he stopped his car at a traffic light in Basra, Bassam Rady noticed the motorbike with two riders on it passed by him slowly. Suddenly the bike turned round and Mr. Rady, feeling dangerous, tried to drive off. Before he could, a man on the back of the bike took out a gun from his jacket and fired. The bullet went through the windscreen and just missed him. As he sped away another shot was fired, but missed the car.  Mr. Rady was an interpreter for British army in Iraq. As such, his life is in danger from the militia that once controlled the Iraqi city and is now returning.  More than a year ago Iraqi soldiers, backed by US, brought peace and stability to Basra by driving militants over the Iranian border in an operation called Charge of the Knights.  According to local estimates, however, about half have returned. Although they have not become as active as before, the militants are trying to shoot Iraqi citizens who worked with British. Most in danger are translators such as Mr. Rady.  The 31-year-old father worked with soldiers on dangerous missions but was refused resettlement in Britain at the end of his employment. He worked with nine translators. Seven of them have been killed.  "I'm like a cancer patient -- now that the militia is back, my family is just waiting for me to die," Mr. Rady said. "I see reports in the media that Basra is safe but it's not true. I know these militia people. I went to school with some of them. I didn't see them for a year but now they are around again. They have told me, 'Your day will come soon'." He takes security precautions and never follows a routine. This means that he cannot work because that would give the militants a better chance of success.  Militiamen belonging to the alMahdi Army took control of Basra between 2004 and 2007 with other Shia fighters. They enforced strict Islamic rules at the same time as running criminal rackets. British army was unable or unwilling to fight back and eventually went back to their base at the airport.
03/12/2022 18:13:44 - INFO - __main__ - ['dangerous']
03/12/2022 18:13:44 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:13:44 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:13:44 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 18:13:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 18:13:55 - INFO - __main__ - Starting training!
03/12/2022 18:14:01 - INFO - __main__ - Step 10 Global step 10 Train loss 19.026829 on epoch=4
03/12/2022 18:14:07 - INFO - __main__ - Step 20 Global step 20 Train loss 14.918169 on epoch=9
03/12/2022 18:14:12 - INFO - __main__ - Step 30 Global step 30 Train loss 11.371945 on epoch=14
03/12/2022 18:14:18 - INFO - __main__ - Step 40 Global step 40 Train loss 8.444285 on epoch=19
03/12/2022 18:14:25 - INFO - __main__ - Step 50 Global step 50 Train loss 5.710305 on epoch=24
03/12/2022 18:14:27 - INFO - __main__ - Global step 50 Train loss 11.894306 ACC 0.0 on epoch=24
03/12/2022 18:14:33 - INFO - __main__ - Step 60 Global step 60 Train loss 5.228012 on epoch=29
03/12/2022 18:14:40 - INFO - __main__ - Step 70 Global step 70 Train loss 3.877021 on epoch=34
03/12/2022 18:14:46 - INFO - __main__ - Step 80 Global step 80 Train loss 3.187966 on epoch=39
03/12/2022 18:14:52 - INFO - __main__ - Step 90 Global step 90 Train loss 2.735271 on epoch=44
03/12/2022 18:14:59 - INFO - __main__ - Step 100 Global step 100 Train loss 2.448304 on epoch=49
03/12/2022 18:15:00 - INFO - __main__ - Global step 100 Train loss 3.495315 ACC 0.0 on epoch=49
03/12/2022 18:15:07 - INFO - __main__ - Step 110 Global step 110 Train loss 2.002865 on epoch=54
03/12/2022 18:15:13 - INFO - __main__ - Step 120 Global step 120 Train loss 1.824220 on epoch=59
03/12/2022 18:15:19 - INFO - __main__ - Step 130 Global step 130 Train loss 1.585530 on epoch=64
03/12/2022 18:15:25 - INFO - __main__ - Step 140 Global step 140 Train loss 1.561277 on epoch=69
03/12/2022 18:15:32 - INFO - __main__ - Step 150 Global step 150 Train loss 1.401188 on epoch=74
03/12/2022 18:15:34 - INFO - __main__ - Global step 150 Train loss 1.675016 ACC 0.0 on epoch=74
03/12/2022 18:15:40 - INFO - __main__ - Step 160 Global step 160 Train loss 1.483486 on epoch=79
03/12/2022 18:15:46 - INFO - __main__ - Step 170 Global step 170 Train loss 1.273854 on epoch=84
03/12/2022 18:15:53 - INFO - __main__ - Step 180 Global step 180 Train loss 1.189719 on epoch=89
03/12/2022 18:15:59 - INFO - __main__ - Step 190 Global step 190 Train loss 1.154176 on epoch=94
03/12/2022 18:16:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.952744 on epoch=99
03/12/2022 18:16:07 - INFO - __main__ - Global step 200 Train loss 1.210796 ACC 0.0 on epoch=99
03/12/2022 18:16:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.970664 on epoch=104
03/12/2022 18:16:20 - INFO - __main__ - Step 220 Global step 220 Train loss 1.006691 on epoch=109
03/12/2022 18:16:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.849443 on epoch=114
03/12/2022 18:16:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.881079 on epoch=119
03/12/2022 18:16:39 - INFO - __main__ - Step 250 Global step 250 Train loss 1.218872 on epoch=124
03/12/2022 18:16:40 - INFO - __main__ - Global step 250 Train loss 0.985350 ACC 0.0 on epoch=124
03/12/2022 18:16:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.991280 on epoch=129
03/12/2022 18:16:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.821455 on epoch=134
03/12/2022 18:16:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.748814 on epoch=139
03/12/2022 18:17:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.857946 on epoch=144
03/12/2022 18:17:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.775547 on epoch=149
03/12/2022 18:17:13 - INFO - __main__ - Global step 300 Train loss 0.839009 ACC 0.0 on epoch=149
03/12/2022 18:17:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.827630 on epoch=154
03/12/2022 18:17:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.634473 on epoch=159
03/12/2022 18:17:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.790533 on epoch=164
03/12/2022 18:17:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.641781 on epoch=169
03/12/2022 18:17:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.629324 on epoch=174
03/12/2022 18:17:46 - INFO - __main__ - Global step 350 Train loss 0.704748 ACC 0.0 on epoch=174
03/12/2022 18:17:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.652653 on epoch=179
03/12/2022 18:17:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.672914 on epoch=184
03/12/2022 18:18:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.619994 on epoch=189
03/12/2022 18:18:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.571082 on epoch=194
03/12/2022 18:18:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.575274 on epoch=199
03/12/2022 18:18:19 - INFO - __main__ - Global step 400 Train loss 0.618383 ACC 0.0 on epoch=199
03/12/2022 18:18:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.585271 on epoch=204
03/12/2022 18:18:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.585273 on epoch=209
03/12/2022 18:18:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.583241 on epoch=214
03/12/2022 18:18:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.580033 on epoch=219
03/12/2022 18:18:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.624477 on epoch=224
03/12/2022 18:18:52 - INFO - __main__ - Global step 450 Train loss 0.591659 ACC 0.0 on epoch=224
03/12/2022 18:18:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.634220 on epoch=229
03/12/2022 18:19:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.588552 on epoch=234
03/12/2022 18:19:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.565000 on epoch=239
03/12/2022 18:19:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.606744 on epoch=244
03/12/2022 18:19:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.537873 on epoch=249
03/12/2022 18:19:25 - INFO - __main__ - Global step 500 Train loss 0.586478 ACC 0.0 on epoch=249
03/12/2022 18:19:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.521765 on epoch=254
03/12/2022 18:19:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.494775 on epoch=259
03/12/2022 18:19:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.614096 on epoch=264
03/12/2022 18:19:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.571929 on epoch=269
03/12/2022 18:19:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.551929 on epoch=274
03/12/2022 18:19:58 - INFO - __main__ - Global step 550 Train loss 0.550899 ACC 0.0 on epoch=274
03/12/2022 18:20:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.572563 on epoch=279
03/12/2022 18:20:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.514843 on epoch=284
03/12/2022 18:20:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.555574 on epoch=289
03/12/2022 18:20:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.609596 on epoch=294
03/12/2022 18:20:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.561987 on epoch=299
03/12/2022 18:20:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:20:30 - INFO - __main__ - Printing 3 examples
03/12/2022 18:20:30 - INFO - __main__ -  [race-high] What is special about Sydney's Great Coastal Walk? (A) It starts from Royal National Park in the south. (B) It takes about more than five hours to complete. (C) It really has the longest coastline in the world. (D) It provides visitors a variety of great landscapes. [SEP] Beaches are not only great for lying on and doing water sports, and in fact one of the best ways of enjoying them is a classic beach walk. Here at iWantSun. Co. Uk, we've been searching the globe to find you the world's best and most glorious beach walks, and here's our pick of the top. The Footpath of the Gods, Amalfi Coast, Italy The name says it all really and you truly do feel up there to walking along this wonderful mountain coastal path, which offers some of the most striking views on the planet. The path begins at town of Bomerano to charming Positano along the UNESCO World Heritage area of the Amalfi Coast. The whole walk will take you approximately four and a half hours to complete and pass over narrow rocky paths, past sheer cliffs and shining blue bays. Sydney's Great Coastal Walk, Australia Sydney's coastline is one of the most beautiful and diverse in the world. Here you have national parks, historic sites, steep cliffs, sparkling beaches and quiet bays all in one place. Sydney's Great Walk runs all the way from Barrenjoey in the north to Royal National Park in the south and takes an incredible seven days to complete. However, if you're not up to doing the full walk, then there are many different parts of the walk that you can do right in the city. Walking from the city's famous Bondi Beach to the sweeping curve of Bronte Beach takes just an hour, which takes in some top scenery. Great Ocean Walk, Australia The Great Ocean Walk stretches 104 km along Victoria's famous Great Ocean Road, located on the southern coast of Australia, from the resort town Apollo Bay to the magnificent Twelve Apostles. The Twelve Apostles are the area's famous stone landmarks which stand out like giants from the sea. The walk passes through a range of landscapes and sights, from national parks, famous surfing spots and deserted beaches, to wild coastlines, cascading waterfalls, lush forests, historic lighthouses and ghostly shipwrecks. Day walks and shorter three-hour walks such as the Wreck Beach Walk or the Lighthouse Cemetery and Lookout Walk can also be enjoyed. So next time when you're looking for a beach holiday don't just think about the resorts and the sand, but consider a more active sun holiday, discovering some of the best beaches in the world.
03/12/2022 18:20:30 - INFO - __main__ - ['It provides visitors a variety of great landscapes.']
03/12/2022 18:20:30 - INFO - __main__ -  [race-high] In the author's opinion, which of the following is NOT true? (A) Nearly everyone has been told a wrong number. (B) It's necessary for everyone to have a telephone. (C) He himself can not decide whether to answer a call. (D) A telephone directory may bring in unexpected calls. [SEP] Many people think a telephone is essential. But I think it is a pest and a time waster. Very often you find it impossible to escape from some idle or curious chatter-box, or from somebody who wants something for nothing. If you have a telephone in your own house, you will admit that it tends to ring when you are asleep, or in the middle of a meal or a conversation, or when you are just going out, or when you are taking your bath. Are you strong minded enough to ignore it, to say to yourself,"Ah, well, it will all be the same in a hundred years' time" You are not. You think there may be some important news or messages for you. I can assure you that if a message is really important it will reach you sooner or later. Have you never rushed dripping from the bath, or chewing from the table, or dazed from the bed, only to be told that you are a wrong number? But you will say, you need not have your name printed in the telephone directory, and you can have a telephone which is only usable for outgoing calls. Besides, you will say, isn't it important to have a telephone in case of emergency--illness, an accident, or fire? Of course, you are right, but here in a thickly populated country like England one is seldom far from a telephone in case of dreadful necessity. I think perhaps I had better try to justify myself by trying to prove that what I like is good. I admit that in different circumstances--if I were a tycoon(business VIP),for instance, or bed ridden I might find a telephone essential. But then if I were a taxi-driver I should find a car essential. Let me put it another way: there are two things for which the English seem to show particular talent; one is mechanical invention, the other is literature. My own business happens to be with the use of words but I see I must now stop using them. For I have just been handed a slip of paper to say that somebody is waiting to speak to me on the telephone. I think I had better answer it. After all, one never knows, it may be something important.
03/12/2022 18:20:30 - INFO - __main__ - ["It's necessary for everyone to have a telephone."]
03/12/2022 18:20:30 - INFO - __main__ -  [race-high] Visitors who use Fast Pass tickets   _ (A) can get discounts on tickets (B) will have a well-planned ride time (C) will often be checked at the exit (D) have to pay more for their tickets [SEP] As for visiting Walt Disney World. People usually have trouble in arranging. The following will give you some practical suggestions on paying a valuable visit to the famous park. When to go? While it's always busy, Disney World is slightly slower between mid-January and March. It's summer when the visitors , combined with the heat and humidity , are crowded. Your best choice is either early in the morning or late in the day--- often you can enjoy more rides in the first or last two hours of the day than you can the entire rest of the day combined. Where to stay? The question: to stay on Disney World Resort property or not. Disney World offers three resort pricing: Deluxe , Moderate, and Value, with good facilities and in-hotel attractions. That said, they all offer "Extra Magic Hours"(early access to the theme parks), guaranteed entry when the parks are over-crowded, and frequent shuttle service to the many attractions. Off-property hotels will have shuttle transfers, but often at less regular intervals ,wasting more time in transport. You'll also need a rental car, which adds to the expense, making up for saving in the hotel rate. The "Magic Your Way Package" is a good value if you have a large family. What to do? Make use of the "Fast Pass" , which , for no extra cost, allow you to insert your entrance ticket into a particular machine at the attraction and receive a schedule ride time a little later on. For example, use your Fast Pass ticket at a ride just before getting lunch. Once you're done eating, you can return to the ride and jump right on during the busiest time of day. At the Animal Kingdom, the animals are most active either early or late in the day, so plan accordingly. Thankfully _ is fit for the least busy times of day in the park. Study the map of the park the night before to plan your visit. You'll save time by knowing where things are and where you're headed.
03/12/2022 18:20:30 - INFO - __main__ - ['will have a well-planned ride time']
03/12/2022 18:20:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 18:20:30 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:20:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 18:20:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:20:30 - INFO - __main__ - Printing 3 examples
03/12/2022 18:20:30 - INFO - __main__ -  [race-high] Which of the following best serves as the best title of the story? (A) Touching by Feeling (B) To See or to Feel (C) Seeing by Feeling (D) Seeing Is Believing [SEP] "If you want to see something well, reach out and touch it!" That may seem a strange thing to say.But touching things can help you to see them better. Your eyes can tell you that a glass ball is round.But by holding it in your hands, you can feel how smooth and cool the ball is.You can feel how heavy the glass is.When you feel all these about the ball, you really see it. With your skin, you can feel better.For example, your fingers can tell the difference between two coins in your pockets.You can feel a little drop of water on the back of your hand, too. You can even feel sounds against your skin.Have you ever wanted to know why some people like very loud music? They must like to feel the sounds of music. All children soon learn what "Don't touch!" means.They hear it often.Yet most of us keep on touching things as we grow up.In shops, we often have to touch things before we buy them. The bottoms of our feet can feel things, too.You know this when you walk on warm sand, cool grass or a hard floor.All feel different under your feet. There are ways of learning to see well by feeling.One way is to close your eyes and try to feel everything that is touching your skin.Feel the shoes on your feet, the clothes on your body, the air on your skin...... Most museums are just for looking.But today some museums have some things to touch.Their signs say, "Do touch!" There you can feel everything on show.
03/12/2022 18:20:30 - INFO - __main__ - ['Seeing by Feeling']
03/12/2022 18:20:30 - INFO - __main__ -  [race-high] According to the author, the flu  _  . (A) doesn't cause sneezes (B) should be paid serious attention to (C) can be caused by spreading much time indoors (D) only spreads among the very young and the very old. [SEP] It's flu season.  What are you doing about it? David Oreck, Founder  My Oreck Air Purifier captures and destroys viruses, bacteria and germs.  2005 ORECK HOLDINGS,LLC. All Rights Reserved. All word marks, logos, product configurations and registered trademarks are owned and used under the authority of Oreck Holdings, LLC. For the very young and the very old and virtually everyone in between, the flu is nothing to sneeze at. So here's what you can do. Check with your doctor to see if a flu shot is right for you. Wash your hands frequently. Maintain a healthy diet and regimen of exercise. And because you're spending most of your time indoors, it makes sense that the air in your home is as fresh, clean and pure as it can be.  My Oreck XL(tm) Professional Air Purifier captures and destroys viruses, bacteria and germs. It removes mold spores, pollen, pet dander, household odors and other airborne particulates as small as 0.1 microns. (The flu virus measures 0.3 microns.) The Oreck Professional Air Purifier uses the same advanced technology as the prefix = st1 /U.S.submarine fleet where mariners are submerged for up to six months at a time. Because the permanent electronic filter never needs replacing, you can rinse it off as often as you like and it's as good as new. My Oreck Professional Air Purifier will literally pay for itself in what you pay for costly HEPA replacement filters that other manufacturers require. Besides HEPA filters don't destroy germs. They can only capture and hold them. So this flu season, take the Oreck Challenge and try my Oreck Professional Air Purifier risk-free for three full months. Then decide. CALL NOW AND RECEIVE A $100 GIFT-FREE.  Just for trying an Oreck Air Purifier risk-free for 90 days, we'll send you our $100 Oreck Cord-Free Electric Broom  ly free. It's a floor vac and a hand vac in one. If you don't feel the difference simply send the Air Purifier back--but keep the Electric Broom--there's no obligation. The shipping is free. Call 1-800-522-5961 ext. CR589 or visit www. oreck. com/decairNothing gets by an Oreck.
03/12/2022 18:20:30 - INFO - __main__ - ['should be paid serious attention to']
03/12/2022 18:20:30 - INFO - __main__ -  [race-high] We can draw the conclusion that Mr.Rady's work in Iraq is very_. (A) happy (B) tiring (C) busy (D) dangerous [SEP] When he stopped his car at a traffic light in Basra, Bassam Rady noticed the motorbike with two riders on it passed by him slowly. Suddenly the bike turned round and Mr. Rady, feeling dangerous, tried to drive off. Before he could, a man on the back of the bike took out a gun from his jacket and fired. The bullet went through the windscreen and just missed him. As he sped away another shot was fired, but missed the car.  Mr. Rady was an interpreter for British army in Iraq. As such, his life is in danger from the militia that once controlled the Iraqi city and is now returning.  More than a year ago Iraqi soldiers, backed by US, brought peace and stability to Basra by driving militants over the Iranian border in an operation called Charge of the Knights.  According to local estimates, however, about half have returned. Although they have not become as active as before, the militants are trying to shoot Iraqi citizens who worked with British. Most in danger are translators such as Mr. Rady.  The 31-year-old father worked with soldiers on dangerous missions but was refused resettlement in Britain at the end of his employment. He worked with nine translators. Seven of them have been killed.  "I'm like a cancer patient -- now that the militia is back, my family is just waiting for me to die," Mr. Rady said. "I see reports in the media that Basra is safe but it's not true. I know these militia people. I went to school with some of them. I didn't see them for a year but now they are around again. They have told me, 'Your day will come soon'." He takes security precautions and never follows a routine. This means that he cannot work because that would give the militants a better chance of success.  Militiamen belonging to the alMahdi Army took control of Basra between 2004 and 2007 with other Shia fighters. They enforced strict Islamic rules at the same time as running criminal rackets. British army was unable or unwilling to fight back and eventually went back to their base at the airport.
03/12/2022 18:20:30 - INFO - __main__ - ['dangerous']
03/12/2022 18:20:30 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:20:30 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:20:30 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 18:20:31 - INFO - __main__ - Global step 600 Train loss 0.562912 ACC 0.0 on epoch=299
03/12/2022 18:20:31 - INFO - __main__ - save last model!
03/12/2022 18:20:38 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 18:20:39 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 18:20:39 - INFO - __main__ - Printing 3 examples
03/12/2022 18:20:39 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:20:39 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 18:20:39 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:20:39 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 18:20:39 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:20:39 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 18:20:39 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:20:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 18:20:43 - INFO - __main__ - Starting training!
03/12/2022 18:20:45 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:20:48 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 18:23:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_21_0.0005_8_predictions.txt
03/12/2022 18:23:44 - INFO - __main__ - ACC on test data: 0.0151
03/12/2022 18:23:45 - INFO - __main__ - prefix=race-high_32_21, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.015068096203998842
03/12/2022 18:23:45 - INFO - __main__ - Running ... prefix=race-high_32_21, lr=0.0003, bsz=8 ...
03/12/2022 18:23:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:23:46 - INFO - __main__ - Printing 3 examples
03/12/2022 18:23:46 - INFO - __main__ -  [race-high] What is special about Sydney's Great Coastal Walk? (A) It starts from Royal National Park in the south. (B) It takes about more than five hours to complete. (C) It really has the longest coastline in the world. (D) It provides visitors a variety of great landscapes. [SEP] Beaches are not only great for lying on and doing water sports, and in fact one of the best ways of enjoying them is a classic beach walk. Here at iWantSun. Co. Uk, we've been searching the globe to find you the world's best and most glorious beach walks, and here's our pick of the top. The Footpath of the Gods, Amalfi Coast, Italy The name says it all really and you truly do feel up there to walking along this wonderful mountain coastal path, which offers some of the most striking views on the planet. The path begins at town of Bomerano to charming Positano along the UNESCO World Heritage area of the Amalfi Coast. The whole walk will take you approximately four and a half hours to complete and pass over narrow rocky paths, past sheer cliffs and shining blue bays. Sydney's Great Coastal Walk, Australia Sydney's coastline is one of the most beautiful and diverse in the world. Here you have national parks, historic sites, steep cliffs, sparkling beaches and quiet bays all in one place. Sydney's Great Walk runs all the way from Barrenjoey in the north to Royal National Park in the south and takes an incredible seven days to complete. However, if you're not up to doing the full walk, then there are many different parts of the walk that you can do right in the city. Walking from the city's famous Bondi Beach to the sweeping curve of Bronte Beach takes just an hour, which takes in some top scenery. Great Ocean Walk, Australia The Great Ocean Walk stretches 104 km along Victoria's famous Great Ocean Road, located on the southern coast of Australia, from the resort town Apollo Bay to the magnificent Twelve Apostles. The Twelve Apostles are the area's famous stone landmarks which stand out like giants from the sea. The walk passes through a range of landscapes and sights, from national parks, famous surfing spots and deserted beaches, to wild coastlines, cascading waterfalls, lush forests, historic lighthouses and ghostly shipwrecks. Day walks and shorter three-hour walks such as the Wreck Beach Walk or the Lighthouse Cemetery and Lookout Walk can also be enjoyed. So next time when you're looking for a beach holiday don't just think about the resorts and the sand, but consider a more active sun holiday, discovering some of the best beaches in the world.
03/12/2022 18:23:46 - INFO - __main__ - ['It provides visitors a variety of great landscapes.']
03/12/2022 18:23:46 - INFO - __main__ -  [race-high] In the author's opinion, which of the following is NOT true? (A) Nearly everyone has been told a wrong number. (B) It's necessary for everyone to have a telephone. (C) He himself can not decide whether to answer a call. (D) A telephone directory may bring in unexpected calls. [SEP] Many people think a telephone is essential. But I think it is a pest and a time waster. Very often you find it impossible to escape from some idle or curious chatter-box, or from somebody who wants something for nothing. If you have a telephone in your own house, you will admit that it tends to ring when you are asleep, or in the middle of a meal or a conversation, or when you are just going out, or when you are taking your bath. Are you strong minded enough to ignore it, to say to yourself,"Ah, well, it will all be the same in a hundred years' time" You are not. You think there may be some important news or messages for you. I can assure you that if a message is really important it will reach you sooner or later. Have you never rushed dripping from the bath, or chewing from the table, or dazed from the bed, only to be told that you are a wrong number? But you will say, you need not have your name printed in the telephone directory, and you can have a telephone which is only usable for outgoing calls. Besides, you will say, isn't it important to have a telephone in case of emergency--illness, an accident, or fire? Of course, you are right, but here in a thickly populated country like England one is seldom far from a telephone in case of dreadful necessity. I think perhaps I had better try to justify myself by trying to prove that what I like is good. I admit that in different circumstances--if I were a tycoon(business VIP),for instance, or bed ridden I might find a telephone essential. But then if I were a taxi-driver I should find a car essential. Let me put it another way: there are two things for which the English seem to show particular talent; one is mechanical invention, the other is literature. My own business happens to be with the use of words but I see I must now stop using them. For I have just been handed a slip of paper to say that somebody is waiting to speak to me on the telephone. I think I had better answer it. After all, one never knows, it may be something important.
03/12/2022 18:23:46 - INFO - __main__ - ["It's necessary for everyone to have a telephone."]
03/12/2022 18:23:46 - INFO - __main__ -  [race-high] Visitors who use Fast Pass tickets   _ (A) can get discounts on tickets (B) will have a well-planned ride time (C) will often be checked at the exit (D) have to pay more for their tickets [SEP] As for visiting Walt Disney World. People usually have trouble in arranging. The following will give you some practical suggestions on paying a valuable visit to the famous park. When to go? While it's always busy, Disney World is slightly slower between mid-January and March. It's summer when the visitors , combined with the heat and humidity , are crowded. Your best choice is either early in the morning or late in the day--- often you can enjoy more rides in the first or last two hours of the day than you can the entire rest of the day combined. Where to stay? The question: to stay on Disney World Resort property or not. Disney World offers three resort pricing: Deluxe , Moderate, and Value, with good facilities and in-hotel attractions. That said, they all offer "Extra Magic Hours"(early access to the theme parks), guaranteed entry when the parks are over-crowded, and frequent shuttle service to the many attractions. Off-property hotels will have shuttle transfers, but often at less regular intervals ,wasting more time in transport. You'll also need a rental car, which adds to the expense, making up for saving in the hotel rate. The "Magic Your Way Package" is a good value if you have a large family. What to do? Make use of the "Fast Pass" , which , for no extra cost, allow you to insert your entrance ticket into a particular machine at the attraction and receive a schedule ride time a little later on. For example, use your Fast Pass ticket at a ride just before getting lunch. Once you're done eating, you can return to the ride and jump right on during the busiest time of day. At the Animal Kingdom, the animals are most active either early or late in the day, so plan accordingly. Thankfully _ is fit for the least busy times of day in the park. Study the map of the park the night before to plan your visit. You'll save time by knowing where things are and where you're headed.
03/12/2022 18:23:46 - INFO - __main__ - ['will have a well-planned ride time']
03/12/2022 18:23:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 18:23:46 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:23:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 18:23:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:23:46 - INFO - __main__ - Printing 3 examples
03/12/2022 18:23:46 - INFO - __main__ -  [race-high] Which of the following best serves as the best title of the story? (A) Touching by Feeling (B) To See or to Feel (C) Seeing by Feeling (D) Seeing Is Believing [SEP] "If you want to see something well, reach out and touch it!" That may seem a strange thing to say.But touching things can help you to see them better. Your eyes can tell you that a glass ball is round.But by holding it in your hands, you can feel how smooth and cool the ball is.You can feel how heavy the glass is.When you feel all these about the ball, you really see it. With your skin, you can feel better.For example, your fingers can tell the difference between two coins in your pockets.You can feel a little drop of water on the back of your hand, too. You can even feel sounds against your skin.Have you ever wanted to know why some people like very loud music? They must like to feel the sounds of music. All children soon learn what "Don't touch!" means.They hear it often.Yet most of us keep on touching things as we grow up.In shops, we often have to touch things before we buy them. The bottoms of our feet can feel things, too.You know this when you walk on warm sand, cool grass or a hard floor.All feel different under your feet. There are ways of learning to see well by feeling.One way is to close your eyes and try to feel everything that is touching your skin.Feel the shoes on your feet, the clothes on your body, the air on your skin...... Most museums are just for looking.But today some museums have some things to touch.Their signs say, "Do touch!" There you can feel everything on show.
03/12/2022 18:23:46 - INFO - __main__ - ['Seeing by Feeling']
03/12/2022 18:23:46 - INFO - __main__ -  [race-high] According to the author, the flu  _  . (A) doesn't cause sneezes (B) should be paid serious attention to (C) can be caused by spreading much time indoors (D) only spreads among the very young and the very old. [SEP] It's flu season.  What are you doing about it? David Oreck, Founder  My Oreck Air Purifier captures and destroys viruses, bacteria and germs.  2005 ORECK HOLDINGS,LLC. All Rights Reserved. All word marks, logos, product configurations and registered trademarks are owned and used under the authority of Oreck Holdings, LLC. For the very young and the very old and virtually everyone in between, the flu is nothing to sneeze at. So here's what you can do. Check with your doctor to see if a flu shot is right for you. Wash your hands frequently. Maintain a healthy diet and regimen of exercise. And because you're spending most of your time indoors, it makes sense that the air in your home is as fresh, clean and pure as it can be.  My Oreck XL(tm) Professional Air Purifier captures and destroys viruses, bacteria and germs. It removes mold spores, pollen, pet dander, household odors and other airborne particulates as small as 0.1 microns. (The flu virus measures 0.3 microns.) The Oreck Professional Air Purifier uses the same advanced technology as the prefix = st1 /U.S.submarine fleet where mariners are submerged for up to six months at a time. Because the permanent electronic filter never needs replacing, you can rinse it off as often as you like and it's as good as new. My Oreck Professional Air Purifier will literally pay for itself in what you pay for costly HEPA replacement filters that other manufacturers require. Besides HEPA filters don't destroy germs. They can only capture and hold them. So this flu season, take the Oreck Challenge and try my Oreck Professional Air Purifier risk-free for three full months. Then decide. CALL NOW AND RECEIVE A $100 GIFT-FREE.  Just for trying an Oreck Air Purifier risk-free for 90 days, we'll send you our $100 Oreck Cord-Free Electric Broom  ly free. It's a floor vac and a hand vac in one. If you don't feel the difference simply send the Air Purifier back--but keep the Electric Broom--there's no obligation. The shipping is free. Call 1-800-522-5961 ext. CR589 or visit www. oreck. com/decairNothing gets by an Oreck.
03/12/2022 18:23:46 - INFO - __main__ - ['should be paid serious attention to']
03/12/2022 18:23:46 - INFO - __main__ -  [race-high] We can draw the conclusion that Mr.Rady's work in Iraq is very_. (A) happy (B) tiring (C) busy (D) dangerous [SEP] When he stopped his car at a traffic light in Basra, Bassam Rady noticed the motorbike with two riders on it passed by him slowly. Suddenly the bike turned round and Mr. Rady, feeling dangerous, tried to drive off. Before he could, a man on the back of the bike took out a gun from his jacket and fired. The bullet went through the windscreen and just missed him. As he sped away another shot was fired, but missed the car.  Mr. Rady was an interpreter for British army in Iraq. As such, his life is in danger from the militia that once controlled the Iraqi city and is now returning.  More than a year ago Iraqi soldiers, backed by US, brought peace and stability to Basra by driving militants over the Iranian border in an operation called Charge of the Knights.  According to local estimates, however, about half have returned. Although they have not become as active as before, the militants are trying to shoot Iraqi citizens who worked with British. Most in danger are translators such as Mr. Rady.  The 31-year-old father worked with soldiers on dangerous missions but was refused resettlement in Britain at the end of his employment. He worked with nine translators. Seven of them have been killed.  "I'm like a cancer patient -- now that the militia is back, my family is just waiting for me to die," Mr. Rady said. "I see reports in the media that Basra is safe but it's not true. I know these militia people. I went to school with some of them. I didn't see them for a year but now they are around again. They have told me, 'Your day will come soon'." He takes security precautions and never follows a routine. This means that he cannot work because that would give the militants a better chance of success.  Militiamen belonging to the alMahdi Army took control of Basra between 2004 and 2007 with other Shia fighters. They enforced strict Islamic rules at the same time as running criminal rackets. British army was unable or unwilling to fight back and eventually went back to their base at the airport.
03/12/2022 18:23:46 - INFO - __main__ - ['dangerous']
03/12/2022 18:23:46 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:23:46 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:23:46 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 18:23:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 18:23:57 - INFO - __main__ - Starting training!
03/12/2022 18:24:02 - INFO - __main__ - Step 10 Global step 10 Train loss 19.061001 on epoch=4
03/12/2022 18:24:08 - INFO - __main__ - Step 20 Global step 20 Train loss 15.291718 on epoch=9
03/12/2022 18:24:14 - INFO - __main__ - Step 30 Global step 30 Train loss 10.862955 on epoch=14
03/12/2022 18:24:21 - INFO - __main__ - Step 40 Global step 40 Train loss 8.186896 on epoch=19
03/12/2022 18:24:27 - INFO - __main__ - Step 50 Global step 50 Train loss 6.890719 on epoch=24
03/12/2022 18:24:29 - INFO - __main__ - Global step 50 Train loss 12.058659 ACC 0.0 on epoch=24
03/12/2022 18:24:36 - INFO - __main__ - Step 60 Global step 60 Train loss 5.970629 on epoch=29
03/12/2022 18:24:43 - INFO - __main__ - Step 70 Global step 70 Train loss 5.724202 on epoch=34
03/12/2022 18:24:49 - INFO - __main__ - Step 80 Global step 80 Train loss 4.955506 on epoch=39
03/12/2022 18:24:55 - INFO - __main__ - Step 90 Global step 90 Train loss 4.019996 on epoch=44
03/12/2022 18:25:01 - INFO - __main__ - Step 100 Global step 100 Train loss 3.367621 on epoch=49
03/12/2022 18:25:04 - INFO - __main__ - Global step 100 Train loss 4.807590 ACC 0.09375 on epoch=49
03/12/2022 18:25:11 - INFO - __main__ - Step 110 Global step 110 Train loss 2.811470 on epoch=54
03/12/2022 18:25:17 - INFO - __main__ - Step 120 Global step 120 Train loss 2.633257 on epoch=59
03/12/2022 18:25:23 - INFO - __main__ - Step 130 Global step 130 Train loss 2.213046 on epoch=64
03/12/2022 18:25:29 - INFO - __main__ - Step 140 Global step 140 Train loss 1.879307 on epoch=69
03/12/2022 18:25:36 - INFO - __main__ - Step 150 Global step 150 Train loss 1.576825 on epoch=74
03/12/2022 18:25:38 - INFO - __main__ - Global step 150 Train loss 2.222781 ACC 0.0625 on epoch=74
03/12/2022 18:25:44 - INFO - __main__ - Step 160 Global step 160 Train loss 1.384568 on epoch=79
03/12/2022 18:25:50 - INFO - __main__ - Step 170 Global step 170 Train loss 1.366239 on epoch=84
03/12/2022 18:25:57 - INFO - __main__ - Step 180 Global step 180 Train loss 1.422201 on epoch=89
03/12/2022 18:26:03 - INFO - __main__ - Step 190 Global step 190 Train loss 1.194638 on epoch=94
03/12/2022 18:26:09 - INFO - __main__ - Step 200 Global step 200 Train loss 1.144132 on epoch=99
03/12/2022 18:26:12 - INFO - __main__ - Global step 200 Train loss 1.302356 ACC 0.03125 on epoch=99
03/12/2022 18:26:18 - INFO - __main__ - Step 210 Global step 210 Train loss 1.130043 on epoch=104
03/12/2022 18:26:24 - INFO - __main__ - Step 220 Global step 220 Train loss 1.073227 on epoch=109
03/12/2022 18:26:31 - INFO - __main__ - Step 230 Global step 230 Train loss 1.125563 on epoch=114
03/12/2022 18:26:37 - INFO - __main__ - Step 240 Global step 240 Train loss 1.207785 on epoch=119
03/12/2022 18:26:43 - INFO - __main__ - Step 250 Global step 250 Train loss 1.020770 on epoch=124
03/12/2022 18:26:45 - INFO - __main__ - Global step 250 Train loss 1.111478 ACC 0.0 on epoch=124
03/12/2022 18:26:51 - INFO - __main__ - Step 260 Global step 260 Train loss 1.034443 on epoch=129
03/12/2022 18:26:58 - INFO - __main__ - Step 270 Global step 270 Train loss 1.081413 on epoch=134
03/12/2022 18:27:04 - INFO - __main__ - Step 280 Global step 280 Train loss 1.056741 on epoch=139
03/12/2022 18:27:10 - INFO - __main__ - Step 290 Global step 290 Train loss 1.070525 on epoch=144
03/12/2022 18:27:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.891175 on epoch=149
03/12/2022 18:27:19 - INFO - __main__ - Global step 300 Train loss 1.026859 ACC 0.0 on epoch=149
03/12/2022 18:27:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.970811 on epoch=154
03/12/2022 18:27:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.949556 on epoch=159
03/12/2022 18:27:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.898640 on epoch=164
03/12/2022 18:27:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.866306 on epoch=169
03/12/2022 18:27:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.675068 on epoch=174
03/12/2022 18:27:53 - INFO - __main__ - Global step 350 Train loss 0.872076 ACC 0.0 on epoch=174
03/12/2022 18:27:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.730718 on epoch=179
03/12/2022 18:28:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.760162 on epoch=184
03/12/2022 18:28:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.816189 on epoch=189
03/12/2022 18:28:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.720571 on epoch=194
03/12/2022 18:28:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.672650 on epoch=199
03/12/2022 18:28:26 - INFO - __main__ - Global step 400 Train loss 0.740058 ACC 0.0 on epoch=199
03/12/2022 18:28:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.675877 on epoch=204
03/12/2022 18:28:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.682886 on epoch=209
03/12/2022 18:28:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.773103 on epoch=214
03/12/2022 18:28:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.687113 on epoch=219
03/12/2022 18:28:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.623272 on epoch=224
03/12/2022 18:29:00 - INFO - __main__ - Global step 450 Train loss 0.688450 ACC 0.03125 on epoch=224
03/12/2022 18:29:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.644505 on epoch=229
03/12/2022 18:29:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.684055 on epoch=234
03/12/2022 18:29:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.612888 on epoch=239
03/12/2022 18:29:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.590635 on epoch=244
03/12/2022 18:29:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.610211 on epoch=249
03/12/2022 18:29:34 - INFO - __main__ - Global step 500 Train loss 0.628459 ACC 0.03125 on epoch=249
03/12/2022 18:29:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.610563 on epoch=254
03/12/2022 18:29:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.589947 on epoch=259
03/12/2022 18:29:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.525148 on epoch=264
03/12/2022 18:29:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.571666 on epoch=269
03/12/2022 18:30:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.500827 on epoch=274
03/12/2022 18:30:07 - INFO - __main__ - Global step 550 Train loss 0.559630 ACC 0.03125 on epoch=274
03/12/2022 18:30:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.493718 on epoch=279
03/12/2022 18:30:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.524484 on epoch=284
03/12/2022 18:30:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.562949 on epoch=289
03/12/2022 18:30:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.487346 on epoch=294
03/12/2022 18:30:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.479685 on epoch=299
03/12/2022 18:30:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:30:40 - INFO - __main__ - Printing 3 examples
03/12/2022 18:30:40 - INFO - __main__ -  [race-high] What is special about Sydney's Great Coastal Walk? (A) It starts from Royal National Park in the south. (B) It takes about more than five hours to complete. (C) It really has the longest coastline in the world. (D) It provides visitors a variety of great landscapes. [SEP] Beaches are not only great for lying on and doing water sports, and in fact one of the best ways of enjoying them is a classic beach walk. Here at iWantSun. Co. Uk, we've been searching the globe to find you the world's best and most glorious beach walks, and here's our pick of the top. The Footpath of the Gods, Amalfi Coast, Italy The name says it all really and you truly do feel up there to walking along this wonderful mountain coastal path, which offers some of the most striking views on the planet. The path begins at town of Bomerano to charming Positano along the UNESCO World Heritage area of the Amalfi Coast. The whole walk will take you approximately four and a half hours to complete and pass over narrow rocky paths, past sheer cliffs and shining blue bays. Sydney's Great Coastal Walk, Australia Sydney's coastline is one of the most beautiful and diverse in the world. Here you have national parks, historic sites, steep cliffs, sparkling beaches and quiet bays all in one place. Sydney's Great Walk runs all the way from Barrenjoey in the north to Royal National Park in the south and takes an incredible seven days to complete. However, if you're not up to doing the full walk, then there are many different parts of the walk that you can do right in the city. Walking from the city's famous Bondi Beach to the sweeping curve of Bronte Beach takes just an hour, which takes in some top scenery. Great Ocean Walk, Australia The Great Ocean Walk stretches 104 km along Victoria's famous Great Ocean Road, located on the southern coast of Australia, from the resort town Apollo Bay to the magnificent Twelve Apostles. The Twelve Apostles are the area's famous stone landmarks which stand out like giants from the sea. The walk passes through a range of landscapes and sights, from national parks, famous surfing spots and deserted beaches, to wild coastlines, cascading waterfalls, lush forests, historic lighthouses and ghostly shipwrecks. Day walks and shorter three-hour walks such as the Wreck Beach Walk or the Lighthouse Cemetery and Lookout Walk can also be enjoyed. So next time when you're looking for a beach holiday don't just think about the resorts and the sand, but consider a more active sun holiday, discovering some of the best beaches in the world.
03/12/2022 18:30:40 - INFO - __main__ - ['It provides visitors a variety of great landscapes.']
03/12/2022 18:30:40 - INFO - __main__ -  [race-high] In the author's opinion, which of the following is NOT true? (A) Nearly everyone has been told a wrong number. (B) It's necessary for everyone to have a telephone. (C) He himself can not decide whether to answer a call. (D) A telephone directory may bring in unexpected calls. [SEP] Many people think a telephone is essential. But I think it is a pest and a time waster. Very often you find it impossible to escape from some idle or curious chatter-box, or from somebody who wants something for nothing. If you have a telephone in your own house, you will admit that it tends to ring when you are asleep, or in the middle of a meal or a conversation, or when you are just going out, or when you are taking your bath. Are you strong minded enough to ignore it, to say to yourself,"Ah, well, it will all be the same in a hundred years' time" You are not. You think there may be some important news or messages for you. I can assure you that if a message is really important it will reach you sooner or later. Have you never rushed dripping from the bath, or chewing from the table, or dazed from the bed, only to be told that you are a wrong number? But you will say, you need not have your name printed in the telephone directory, and you can have a telephone which is only usable for outgoing calls. Besides, you will say, isn't it important to have a telephone in case of emergency--illness, an accident, or fire? Of course, you are right, but here in a thickly populated country like England one is seldom far from a telephone in case of dreadful necessity. I think perhaps I had better try to justify myself by trying to prove that what I like is good. I admit that in different circumstances--if I were a tycoon(business VIP),for instance, or bed ridden I might find a telephone essential. But then if I were a taxi-driver I should find a car essential. Let me put it another way: there are two things for which the English seem to show particular talent; one is mechanical invention, the other is literature. My own business happens to be with the use of words but I see I must now stop using them. For I have just been handed a slip of paper to say that somebody is waiting to speak to me on the telephone. I think I had better answer it. After all, one never knows, it may be something important.
03/12/2022 18:30:40 - INFO - __main__ - ["It's necessary for everyone to have a telephone."]
03/12/2022 18:30:40 - INFO - __main__ -  [race-high] Visitors who use Fast Pass tickets   _ (A) can get discounts on tickets (B) will have a well-planned ride time (C) will often be checked at the exit (D) have to pay more for their tickets [SEP] As for visiting Walt Disney World. People usually have trouble in arranging. The following will give you some practical suggestions on paying a valuable visit to the famous park. When to go? While it's always busy, Disney World is slightly slower between mid-January and March. It's summer when the visitors , combined with the heat and humidity , are crowded. Your best choice is either early in the morning or late in the day--- often you can enjoy more rides in the first or last two hours of the day than you can the entire rest of the day combined. Where to stay? The question: to stay on Disney World Resort property or not. Disney World offers three resort pricing: Deluxe , Moderate, and Value, with good facilities and in-hotel attractions. That said, they all offer "Extra Magic Hours"(early access to the theme parks), guaranteed entry when the parks are over-crowded, and frequent shuttle service to the many attractions. Off-property hotels will have shuttle transfers, but often at less regular intervals ,wasting more time in transport. You'll also need a rental car, which adds to the expense, making up for saving in the hotel rate. The "Magic Your Way Package" is a good value if you have a large family. What to do? Make use of the "Fast Pass" , which , for no extra cost, allow you to insert your entrance ticket into a particular machine at the attraction and receive a schedule ride time a little later on. For example, use your Fast Pass ticket at a ride just before getting lunch. Once you're done eating, you can return to the ride and jump right on during the busiest time of day. At the Animal Kingdom, the animals are most active either early or late in the day, so plan accordingly. Thankfully _ is fit for the least busy times of day in the park. Study the map of the park the night before to plan your visit. You'll save time by knowing where things are and where you're headed.
03/12/2022 18:30:40 - INFO - __main__ - ['will have a well-planned ride time']
03/12/2022 18:30:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 18:30:40 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:30:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 18:30:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:30:40 - INFO - __main__ - Printing 3 examples
03/12/2022 18:30:40 - INFO - __main__ -  [race-high] Which of the following best serves as the best title of the story? (A) Touching by Feeling (B) To See or to Feel (C) Seeing by Feeling (D) Seeing Is Believing [SEP] "If you want to see something well, reach out and touch it!" That may seem a strange thing to say.But touching things can help you to see them better. Your eyes can tell you that a glass ball is round.But by holding it in your hands, you can feel how smooth and cool the ball is.You can feel how heavy the glass is.When you feel all these about the ball, you really see it. With your skin, you can feel better.For example, your fingers can tell the difference between two coins in your pockets.You can feel a little drop of water on the back of your hand, too. You can even feel sounds against your skin.Have you ever wanted to know why some people like very loud music? They must like to feel the sounds of music. All children soon learn what "Don't touch!" means.They hear it often.Yet most of us keep on touching things as we grow up.In shops, we often have to touch things before we buy them. The bottoms of our feet can feel things, too.You know this when you walk on warm sand, cool grass or a hard floor.All feel different under your feet. There are ways of learning to see well by feeling.One way is to close your eyes and try to feel everything that is touching your skin.Feel the shoes on your feet, the clothes on your body, the air on your skin...... Most museums are just for looking.But today some museums have some things to touch.Their signs say, "Do touch!" There you can feel everything on show.
03/12/2022 18:30:40 - INFO - __main__ - ['Seeing by Feeling']
03/12/2022 18:30:40 - INFO - __main__ -  [race-high] According to the author, the flu  _  . (A) doesn't cause sneezes (B) should be paid serious attention to (C) can be caused by spreading much time indoors (D) only spreads among the very young and the very old. [SEP] It's flu season.  What are you doing about it? David Oreck, Founder  My Oreck Air Purifier captures and destroys viruses, bacteria and germs.  2005 ORECK HOLDINGS,LLC. All Rights Reserved. All word marks, logos, product configurations and registered trademarks are owned and used under the authority of Oreck Holdings, LLC. For the very young and the very old and virtually everyone in between, the flu is nothing to sneeze at. So here's what you can do. Check with your doctor to see if a flu shot is right for you. Wash your hands frequently. Maintain a healthy diet and regimen of exercise. And because you're spending most of your time indoors, it makes sense that the air in your home is as fresh, clean and pure as it can be.  My Oreck XL(tm) Professional Air Purifier captures and destroys viruses, bacteria and germs. It removes mold spores, pollen, pet dander, household odors and other airborne particulates as small as 0.1 microns. (The flu virus measures 0.3 microns.) The Oreck Professional Air Purifier uses the same advanced technology as the prefix = st1 /U.S.submarine fleet where mariners are submerged for up to six months at a time. Because the permanent electronic filter never needs replacing, you can rinse it off as often as you like and it's as good as new. My Oreck Professional Air Purifier will literally pay for itself in what you pay for costly HEPA replacement filters that other manufacturers require. Besides HEPA filters don't destroy germs. They can only capture and hold them. So this flu season, take the Oreck Challenge and try my Oreck Professional Air Purifier risk-free for three full months. Then decide. CALL NOW AND RECEIVE A $100 GIFT-FREE.  Just for trying an Oreck Air Purifier risk-free for 90 days, we'll send you our $100 Oreck Cord-Free Electric Broom  ly free. It's a floor vac and a hand vac in one. If you don't feel the difference simply send the Air Purifier back--but keep the Electric Broom--there's no obligation. The shipping is free. Call 1-800-522-5961 ext. CR589 or visit www. oreck. com/decairNothing gets by an Oreck.
03/12/2022 18:30:40 - INFO - __main__ - ['should be paid serious attention to']
03/12/2022 18:30:40 - INFO - __main__ -  [race-high] We can draw the conclusion that Mr.Rady's work in Iraq is very_. (A) happy (B) tiring (C) busy (D) dangerous [SEP] When he stopped his car at a traffic light in Basra, Bassam Rady noticed the motorbike with two riders on it passed by him slowly. Suddenly the bike turned round and Mr. Rady, feeling dangerous, tried to drive off. Before he could, a man on the back of the bike took out a gun from his jacket and fired. The bullet went through the windscreen and just missed him. As he sped away another shot was fired, but missed the car.  Mr. Rady was an interpreter for British army in Iraq. As such, his life is in danger from the militia that once controlled the Iraqi city and is now returning.  More than a year ago Iraqi soldiers, backed by US, brought peace and stability to Basra by driving militants over the Iranian border in an operation called Charge of the Knights.  According to local estimates, however, about half have returned. Although they have not become as active as before, the militants are trying to shoot Iraqi citizens who worked with British. Most in danger are translators such as Mr. Rady.  The 31-year-old father worked with soldiers on dangerous missions but was refused resettlement in Britain at the end of his employment. He worked with nine translators. Seven of them have been killed.  "I'm like a cancer patient -- now that the militia is back, my family is just waiting for me to die," Mr. Rady said. "I see reports in the media that Basra is safe but it's not true. I know these militia people. I went to school with some of them. I didn't see them for a year but now they are around again. They have told me, 'Your day will come soon'." He takes security precautions and never follows a routine. This means that he cannot work because that would give the militants a better chance of success.  Militiamen belonging to the alMahdi Army took control of Basra between 2004 and 2007 with other Shia fighters. They enforced strict Islamic rules at the same time as running criminal rackets. British army was unable or unwilling to fight back and eventually went back to their base at the airport.
03/12/2022 18:30:40 - INFO - __main__ - ['dangerous']
03/12/2022 18:30:40 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:30:40 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:30:40 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 18:30:41 - INFO - __main__ - Global step 600 Train loss 0.509636 ACC 0.03125 on epoch=299
03/12/2022 18:30:41 - INFO - __main__ - save last model!
03/12/2022 18:30:48 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 18:30:49 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 18:30:49 - INFO - __main__ - Printing 3 examples
03/12/2022 18:30:49 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:30:49 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 18:30:49 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:30:49 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 18:30:49 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:30:49 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 18:30:49 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:30:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 18:30:53 - INFO - __main__ - Starting training!
03/12/2022 18:30:54 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:30:58 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 18:34:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_21_0.0003_8_predictions.txt
03/12/2022 18:34:34 - INFO - __main__ - ACC on test data: 0.0548
03/12/2022 18:34:34 - INFO - __main__ - prefix=race-high_32_21, lr=0.0003, bsz=8, dev_performance=0.09375, test_performance=0.05476673427991886
03/12/2022 18:34:34 - INFO - __main__ - Running ... prefix=race-high_32_21, lr=0.0002, bsz=8 ...
03/12/2022 18:34:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:34:35 - INFO - __main__ - Printing 3 examples
03/12/2022 18:34:35 - INFO - __main__ -  [race-high] What is special about Sydney's Great Coastal Walk? (A) It starts from Royal National Park in the south. (B) It takes about more than five hours to complete. (C) It really has the longest coastline in the world. (D) It provides visitors a variety of great landscapes. [SEP] Beaches are not only great for lying on and doing water sports, and in fact one of the best ways of enjoying them is a classic beach walk. Here at iWantSun. Co. Uk, we've been searching the globe to find you the world's best and most glorious beach walks, and here's our pick of the top. The Footpath of the Gods, Amalfi Coast, Italy The name says it all really and you truly do feel up there to walking along this wonderful mountain coastal path, which offers some of the most striking views on the planet. The path begins at town of Bomerano to charming Positano along the UNESCO World Heritage area of the Amalfi Coast. The whole walk will take you approximately four and a half hours to complete and pass over narrow rocky paths, past sheer cliffs and shining blue bays. Sydney's Great Coastal Walk, Australia Sydney's coastline is one of the most beautiful and diverse in the world. Here you have national parks, historic sites, steep cliffs, sparkling beaches and quiet bays all in one place. Sydney's Great Walk runs all the way from Barrenjoey in the north to Royal National Park in the south and takes an incredible seven days to complete. However, if you're not up to doing the full walk, then there are many different parts of the walk that you can do right in the city. Walking from the city's famous Bondi Beach to the sweeping curve of Bronte Beach takes just an hour, which takes in some top scenery. Great Ocean Walk, Australia The Great Ocean Walk stretches 104 km along Victoria's famous Great Ocean Road, located on the southern coast of Australia, from the resort town Apollo Bay to the magnificent Twelve Apostles. The Twelve Apostles are the area's famous stone landmarks which stand out like giants from the sea. The walk passes through a range of landscapes and sights, from national parks, famous surfing spots and deserted beaches, to wild coastlines, cascading waterfalls, lush forests, historic lighthouses and ghostly shipwrecks. Day walks and shorter three-hour walks such as the Wreck Beach Walk or the Lighthouse Cemetery and Lookout Walk can also be enjoyed. So next time when you're looking for a beach holiday don't just think about the resorts and the sand, but consider a more active sun holiday, discovering some of the best beaches in the world.
03/12/2022 18:34:35 - INFO - __main__ - ['It provides visitors a variety of great landscapes.']
03/12/2022 18:34:35 - INFO - __main__ -  [race-high] In the author's opinion, which of the following is NOT true? (A) Nearly everyone has been told a wrong number. (B) It's necessary for everyone to have a telephone. (C) He himself can not decide whether to answer a call. (D) A telephone directory may bring in unexpected calls. [SEP] Many people think a telephone is essential. But I think it is a pest and a time waster. Very often you find it impossible to escape from some idle or curious chatter-box, or from somebody who wants something for nothing. If you have a telephone in your own house, you will admit that it tends to ring when you are asleep, or in the middle of a meal or a conversation, or when you are just going out, or when you are taking your bath. Are you strong minded enough to ignore it, to say to yourself,"Ah, well, it will all be the same in a hundred years' time" You are not. You think there may be some important news or messages for you. I can assure you that if a message is really important it will reach you sooner or later. Have you never rushed dripping from the bath, or chewing from the table, or dazed from the bed, only to be told that you are a wrong number? But you will say, you need not have your name printed in the telephone directory, and you can have a telephone which is only usable for outgoing calls. Besides, you will say, isn't it important to have a telephone in case of emergency--illness, an accident, or fire? Of course, you are right, but here in a thickly populated country like England one is seldom far from a telephone in case of dreadful necessity. I think perhaps I had better try to justify myself by trying to prove that what I like is good. I admit that in different circumstances--if I were a tycoon(business VIP),for instance, or bed ridden I might find a telephone essential. But then if I were a taxi-driver I should find a car essential. Let me put it another way: there are two things for which the English seem to show particular talent; one is mechanical invention, the other is literature. My own business happens to be with the use of words but I see I must now stop using them. For I have just been handed a slip of paper to say that somebody is waiting to speak to me on the telephone. I think I had better answer it. After all, one never knows, it may be something important.
03/12/2022 18:34:35 - INFO - __main__ - ["It's necessary for everyone to have a telephone."]
03/12/2022 18:34:35 - INFO - __main__ -  [race-high] Visitors who use Fast Pass tickets   _ (A) can get discounts on tickets (B) will have a well-planned ride time (C) will often be checked at the exit (D) have to pay more for their tickets [SEP] As for visiting Walt Disney World. People usually have trouble in arranging. The following will give you some practical suggestions on paying a valuable visit to the famous park. When to go? While it's always busy, Disney World is slightly slower between mid-January and March. It's summer when the visitors , combined with the heat and humidity , are crowded. Your best choice is either early in the morning or late in the day--- often you can enjoy more rides in the first or last two hours of the day than you can the entire rest of the day combined. Where to stay? The question: to stay on Disney World Resort property or not. Disney World offers three resort pricing: Deluxe , Moderate, and Value, with good facilities and in-hotel attractions. That said, they all offer "Extra Magic Hours"(early access to the theme parks), guaranteed entry when the parks are over-crowded, and frequent shuttle service to the many attractions. Off-property hotels will have shuttle transfers, but often at less regular intervals ,wasting more time in transport. You'll also need a rental car, which adds to the expense, making up for saving in the hotel rate. The "Magic Your Way Package" is a good value if you have a large family. What to do? Make use of the "Fast Pass" , which , for no extra cost, allow you to insert your entrance ticket into a particular machine at the attraction and receive a schedule ride time a little later on. For example, use your Fast Pass ticket at a ride just before getting lunch. Once you're done eating, you can return to the ride and jump right on during the busiest time of day. At the Animal Kingdom, the animals are most active either early or late in the day, so plan accordingly. Thankfully _ is fit for the least busy times of day in the park. Study the map of the park the night before to plan your visit. You'll save time by knowing where things are and where you're headed.
03/12/2022 18:34:35 - INFO - __main__ - ['will have a well-planned ride time']
03/12/2022 18:34:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 18:34:35 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:34:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 18:34:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:34:35 - INFO - __main__ - Printing 3 examples
03/12/2022 18:34:35 - INFO - __main__ -  [race-high] Which of the following best serves as the best title of the story? (A) Touching by Feeling (B) To See or to Feel (C) Seeing by Feeling (D) Seeing Is Believing [SEP] "If you want to see something well, reach out and touch it!" That may seem a strange thing to say.But touching things can help you to see them better. Your eyes can tell you that a glass ball is round.But by holding it in your hands, you can feel how smooth and cool the ball is.You can feel how heavy the glass is.When you feel all these about the ball, you really see it. With your skin, you can feel better.For example, your fingers can tell the difference between two coins in your pockets.You can feel a little drop of water on the back of your hand, too. You can even feel sounds against your skin.Have you ever wanted to know why some people like very loud music? They must like to feel the sounds of music. All children soon learn what "Don't touch!" means.They hear it often.Yet most of us keep on touching things as we grow up.In shops, we often have to touch things before we buy them. The bottoms of our feet can feel things, too.You know this when you walk on warm sand, cool grass or a hard floor.All feel different under your feet. There are ways of learning to see well by feeling.One way is to close your eyes and try to feel everything that is touching your skin.Feel the shoes on your feet, the clothes on your body, the air on your skin...... Most museums are just for looking.But today some museums have some things to touch.Their signs say, "Do touch!" There you can feel everything on show.
03/12/2022 18:34:35 - INFO - __main__ - ['Seeing by Feeling']
03/12/2022 18:34:35 - INFO - __main__ -  [race-high] According to the author, the flu  _  . (A) doesn't cause sneezes (B) should be paid serious attention to (C) can be caused by spreading much time indoors (D) only spreads among the very young and the very old. [SEP] It's flu season.  What are you doing about it? David Oreck, Founder  My Oreck Air Purifier captures and destroys viruses, bacteria and germs.  2005 ORECK HOLDINGS,LLC. All Rights Reserved. All word marks, logos, product configurations and registered trademarks are owned and used under the authority of Oreck Holdings, LLC. For the very young and the very old and virtually everyone in between, the flu is nothing to sneeze at. So here's what you can do. Check with your doctor to see if a flu shot is right for you. Wash your hands frequently. Maintain a healthy diet and regimen of exercise. And because you're spending most of your time indoors, it makes sense that the air in your home is as fresh, clean and pure as it can be.  My Oreck XL(tm) Professional Air Purifier captures and destroys viruses, bacteria and germs. It removes mold spores, pollen, pet dander, household odors and other airborne particulates as small as 0.1 microns. (The flu virus measures 0.3 microns.) The Oreck Professional Air Purifier uses the same advanced technology as the prefix = st1 /U.S.submarine fleet where mariners are submerged for up to six months at a time. Because the permanent electronic filter never needs replacing, you can rinse it off as often as you like and it's as good as new. My Oreck Professional Air Purifier will literally pay for itself in what you pay for costly HEPA replacement filters that other manufacturers require. Besides HEPA filters don't destroy germs. They can only capture and hold them. So this flu season, take the Oreck Challenge and try my Oreck Professional Air Purifier risk-free for three full months. Then decide. CALL NOW AND RECEIVE A $100 GIFT-FREE.  Just for trying an Oreck Air Purifier risk-free for 90 days, we'll send you our $100 Oreck Cord-Free Electric Broom  ly free. It's a floor vac and a hand vac in one. If you don't feel the difference simply send the Air Purifier back--but keep the Electric Broom--there's no obligation. The shipping is free. Call 1-800-522-5961 ext. CR589 or visit www. oreck. com/decairNothing gets by an Oreck.
03/12/2022 18:34:35 - INFO - __main__ - ['should be paid serious attention to']
03/12/2022 18:34:35 - INFO - __main__ -  [race-high] We can draw the conclusion that Mr.Rady's work in Iraq is very_. (A) happy (B) tiring (C) busy (D) dangerous [SEP] When he stopped his car at a traffic light in Basra, Bassam Rady noticed the motorbike with two riders on it passed by him slowly. Suddenly the bike turned round and Mr. Rady, feeling dangerous, tried to drive off. Before he could, a man on the back of the bike took out a gun from his jacket and fired. The bullet went through the windscreen and just missed him. As he sped away another shot was fired, but missed the car.  Mr. Rady was an interpreter for British army in Iraq. As such, his life is in danger from the militia that once controlled the Iraqi city and is now returning.  More than a year ago Iraqi soldiers, backed by US, brought peace and stability to Basra by driving militants over the Iranian border in an operation called Charge of the Knights.  According to local estimates, however, about half have returned. Although they have not become as active as before, the militants are trying to shoot Iraqi citizens who worked with British. Most in danger are translators such as Mr. Rady.  The 31-year-old father worked with soldiers on dangerous missions but was refused resettlement in Britain at the end of his employment. He worked with nine translators. Seven of them have been killed.  "I'm like a cancer patient -- now that the militia is back, my family is just waiting for me to die," Mr. Rady said. "I see reports in the media that Basra is safe but it's not true. I know these militia people. I went to school with some of them. I didn't see them for a year but now they are around again. They have told me, 'Your day will come soon'." He takes security precautions and never follows a routine. This means that he cannot work because that would give the militants a better chance of success.  Militiamen belonging to the alMahdi Army took control of Basra between 2004 and 2007 with other Shia fighters. They enforced strict Islamic rules at the same time as running criminal rackets. British army was unable or unwilling to fight back and eventually went back to their base at the airport.
03/12/2022 18:34:35 - INFO - __main__ - ['dangerous']
03/12/2022 18:34:35 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:34:35 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:34:35 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 18:34:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 18:34:48 - INFO - __main__ - Starting training!
03/12/2022 18:34:56 - INFO - __main__ - Step 10 Global step 10 Train loss 19.828089 on epoch=4
03/12/2022 18:35:02 - INFO - __main__ - Step 20 Global step 20 Train loss 16.775354 on epoch=9
03/12/2022 18:35:08 - INFO - __main__ - Step 30 Global step 30 Train loss 10.059702 on epoch=14
03/12/2022 18:35:15 - INFO - __main__ - Step 40 Global step 40 Train loss 6.242435 on epoch=19
03/12/2022 18:35:21 - INFO - __main__ - Step 50 Global step 50 Train loss 4.646835 on epoch=24
03/12/2022 18:35:24 - INFO - __main__ - Global step 50 Train loss 11.510484 ACC 0.0625 on epoch=24
03/12/2022 18:35:30 - INFO - __main__ - Step 60 Global step 60 Train loss 4.129294 on epoch=29
03/12/2022 18:35:37 - INFO - __main__ - Step 70 Global step 70 Train loss 4.203478 on epoch=34
03/12/2022 18:35:43 - INFO - __main__ - Step 80 Global step 80 Train loss 3.782959 on epoch=39
03/12/2022 18:35:50 - INFO - __main__ - Step 90 Global step 90 Train loss 3.626109 on epoch=44
03/12/2022 18:35:56 - INFO - __main__ - Step 100 Global step 100 Train loss 3.459195 on epoch=49
03/12/2022 18:35:58 - INFO - __main__ - Global step 100 Train loss 3.840207 ACC 0.125 on epoch=49
03/12/2022 18:36:05 - INFO - __main__ - Step 110 Global step 110 Train loss 3.129451 on epoch=54
03/12/2022 18:36:11 - INFO - __main__ - Step 120 Global step 120 Train loss 2.960785 on epoch=59
03/12/2022 18:36:17 - INFO - __main__ - Step 130 Global step 130 Train loss 2.733516 on epoch=64
03/12/2022 18:36:24 - INFO - __main__ - Step 140 Global step 140 Train loss 2.552934 on epoch=69
03/12/2022 18:36:30 - INFO - __main__ - Step 150 Global step 150 Train loss 2.535916 on epoch=74
03/12/2022 18:36:32 - INFO - __main__ - Global step 150 Train loss 2.782521 ACC 0.09375 on epoch=74
03/12/2022 18:36:38 - INFO - __main__ - Step 160 Global step 160 Train loss 2.192738 on epoch=79
03/12/2022 18:36:45 - INFO - __main__ - Step 170 Global step 170 Train loss 2.395967 on epoch=84
03/12/2022 18:36:51 - INFO - __main__ - Step 180 Global step 180 Train loss 2.010138 on epoch=89
03/12/2022 18:36:57 - INFO - __main__ - Step 190 Global step 190 Train loss 1.941984 on epoch=94
03/12/2022 18:37:03 - INFO - __main__ - Step 200 Global step 200 Train loss 1.726555 on epoch=99
03/12/2022 18:37:05 - INFO - __main__ - Global step 200 Train loss 2.053477 ACC 0.09375 on epoch=99
03/12/2022 18:37:12 - INFO - __main__ - Step 210 Global step 210 Train loss 1.691562 on epoch=104
03/12/2022 18:37:18 - INFO - __main__ - Step 220 Global step 220 Train loss 1.442606 on epoch=109
03/12/2022 18:37:24 - INFO - __main__ - Step 230 Global step 230 Train loss 1.547469 on epoch=114
03/12/2022 18:37:31 - INFO - __main__ - Step 240 Global step 240 Train loss 1.329360 on epoch=119
03/12/2022 18:37:37 - INFO - __main__ - Step 250 Global step 250 Train loss 1.451490 on epoch=124
03/12/2022 18:37:39 - INFO - __main__ - Global step 250 Train loss 1.492498 ACC 0.0 on epoch=124
03/12/2022 18:37:46 - INFO - __main__ - Step 260 Global step 260 Train loss 1.433229 on epoch=129
03/12/2022 18:37:52 - INFO - __main__ - Step 270 Global step 270 Train loss 1.342255 on epoch=134
03/12/2022 18:37:58 - INFO - __main__ - Step 280 Global step 280 Train loss 1.172504 on epoch=139
03/12/2022 18:38:05 - INFO - __main__ - Step 290 Global step 290 Train loss 1.184140 on epoch=144
03/12/2022 18:38:11 - INFO - __main__ - Step 300 Global step 300 Train loss 1.252389 on epoch=149
03/12/2022 18:38:14 - INFO - __main__ - Global step 300 Train loss 1.276904 ACC 0.0 on epoch=149
03/12/2022 18:38:20 - INFO - __main__ - Step 310 Global step 310 Train loss 1.196182 on epoch=154
03/12/2022 18:38:26 - INFO - __main__ - Step 320 Global step 320 Train loss 1.314450 on epoch=159
03/12/2022 18:38:32 - INFO - __main__ - Step 330 Global step 330 Train loss 1.130617 on epoch=164
03/12/2022 18:38:38 - INFO - __main__ - Step 340 Global step 340 Train loss 1.248831 on epoch=169
03/12/2022 18:38:45 - INFO - __main__ - Step 350 Global step 350 Train loss 1.209915 on epoch=174
03/12/2022 18:38:47 - INFO - __main__ - Global step 350 Train loss 1.219999 ACC 0.0625 on epoch=174
03/12/2022 18:38:54 - INFO - __main__ - Step 360 Global step 360 Train loss 1.005784 on epoch=179
03/12/2022 18:39:00 - INFO - __main__ - Step 370 Global step 370 Train loss 1.218861 on epoch=184
03/12/2022 18:39:06 - INFO - __main__ - Step 380 Global step 380 Train loss 1.097695 on epoch=189
03/12/2022 18:39:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.971288 on epoch=194
03/12/2022 18:39:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.946435 on epoch=199
03/12/2022 18:39:21 - INFO - __main__ - Global step 400 Train loss 1.048013 ACC 0.03125 on epoch=199
03/12/2022 18:39:27 - INFO - __main__ - Step 410 Global step 410 Train loss 1.019876 on epoch=204
03/12/2022 18:39:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.881608 on epoch=209
03/12/2022 18:39:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.860544 on epoch=214
03/12/2022 18:39:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.840112 on epoch=219
03/12/2022 18:39:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.845139 on epoch=224
03/12/2022 18:39:54 - INFO - __main__ - Global step 450 Train loss 0.889456 ACC 0.0 on epoch=224
03/12/2022 18:40:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.843277 on epoch=229
03/12/2022 18:40:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.713811 on epoch=234
03/12/2022 18:40:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.765427 on epoch=239
03/12/2022 18:40:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.793537 on epoch=244
03/12/2022 18:40:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.799372 on epoch=249
03/12/2022 18:40:27 - INFO - __main__ - Global step 500 Train loss 0.783085 ACC 0.03125 on epoch=249
03/12/2022 18:40:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.862659 on epoch=254
03/12/2022 18:40:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.814583 on epoch=259
03/12/2022 18:40:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.680893 on epoch=264
03/12/2022 18:40:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.750810 on epoch=269
03/12/2022 18:40:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.776970 on epoch=274
03/12/2022 18:41:00 - INFO - __main__ - Global step 550 Train loss 0.777183 ACC 0.03125 on epoch=274
03/12/2022 18:41:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.651318 on epoch=279
03/12/2022 18:41:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.736704 on epoch=284
03/12/2022 18:41:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.589246 on epoch=289
03/12/2022 18:41:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.623998 on epoch=294
03/12/2022 18:41:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.749085 on epoch=299
03/12/2022 18:41:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:41:32 - INFO - __main__ - Printing 3 examples
03/12/2022 18:41:32 - INFO - __main__ -  [race-high] What is special about Sydney's Great Coastal Walk? (A) It starts from Royal National Park in the south. (B) It takes about more than five hours to complete. (C) It really has the longest coastline in the world. (D) It provides visitors a variety of great landscapes. [SEP] Beaches are not only great for lying on and doing water sports, and in fact one of the best ways of enjoying them is a classic beach walk. Here at iWantSun. Co. Uk, we've been searching the globe to find you the world's best and most glorious beach walks, and here's our pick of the top. The Footpath of the Gods, Amalfi Coast, Italy The name says it all really and you truly do feel up there to walking along this wonderful mountain coastal path, which offers some of the most striking views on the planet. The path begins at town of Bomerano to charming Positano along the UNESCO World Heritage area of the Amalfi Coast. The whole walk will take you approximately four and a half hours to complete and pass over narrow rocky paths, past sheer cliffs and shining blue bays. Sydney's Great Coastal Walk, Australia Sydney's coastline is one of the most beautiful and diverse in the world. Here you have national parks, historic sites, steep cliffs, sparkling beaches and quiet bays all in one place. Sydney's Great Walk runs all the way from Barrenjoey in the north to Royal National Park in the south and takes an incredible seven days to complete. However, if you're not up to doing the full walk, then there are many different parts of the walk that you can do right in the city. Walking from the city's famous Bondi Beach to the sweeping curve of Bronte Beach takes just an hour, which takes in some top scenery. Great Ocean Walk, Australia The Great Ocean Walk stretches 104 km along Victoria's famous Great Ocean Road, located on the southern coast of Australia, from the resort town Apollo Bay to the magnificent Twelve Apostles. The Twelve Apostles are the area's famous stone landmarks which stand out like giants from the sea. The walk passes through a range of landscapes and sights, from national parks, famous surfing spots and deserted beaches, to wild coastlines, cascading waterfalls, lush forests, historic lighthouses and ghostly shipwrecks. Day walks and shorter three-hour walks such as the Wreck Beach Walk or the Lighthouse Cemetery and Lookout Walk can also be enjoyed. So next time when you're looking for a beach holiday don't just think about the resorts and the sand, but consider a more active sun holiday, discovering some of the best beaches in the world.
03/12/2022 18:41:32 - INFO - __main__ - ['It provides visitors a variety of great landscapes.']
03/12/2022 18:41:32 - INFO - __main__ -  [race-high] In the author's opinion, which of the following is NOT true? (A) Nearly everyone has been told a wrong number. (B) It's necessary for everyone to have a telephone. (C) He himself can not decide whether to answer a call. (D) A telephone directory may bring in unexpected calls. [SEP] Many people think a telephone is essential. But I think it is a pest and a time waster. Very often you find it impossible to escape from some idle or curious chatter-box, or from somebody who wants something for nothing. If you have a telephone in your own house, you will admit that it tends to ring when you are asleep, or in the middle of a meal or a conversation, or when you are just going out, or when you are taking your bath. Are you strong minded enough to ignore it, to say to yourself,"Ah, well, it will all be the same in a hundred years' time" You are not. You think there may be some important news or messages for you. I can assure you that if a message is really important it will reach you sooner or later. Have you never rushed dripping from the bath, or chewing from the table, or dazed from the bed, only to be told that you are a wrong number? But you will say, you need not have your name printed in the telephone directory, and you can have a telephone which is only usable for outgoing calls. Besides, you will say, isn't it important to have a telephone in case of emergency--illness, an accident, or fire? Of course, you are right, but here in a thickly populated country like England one is seldom far from a telephone in case of dreadful necessity. I think perhaps I had better try to justify myself by trying to prove that what I like is good. I admit that in different circumstances--if I were a tycoon(business VIP),for instance, or bed ridden I might find a telephone essential. But then if I were a taxi-driver I should find a car essential. Let me put it another way: there are two things for which the English seem to show particular talent; one is mechanical invention, the other is literature. My own business happens to be with the use of words but I see I must now stop using them. For I have just been handed a slip of paper to say that somebody is waiting to speak to me on the telephone. I think I had better answer it. After all, one never knows, it may be something important.
03/12/2022 18:41:32 - INFO - __main__ - ["It's necessary for everyone to have a telephone."]
03/12/2022 18:41:32 - INFO - __main__ -  [race-high] Visitors who use Fast Pass tickets   _ (A) can get discounts on tickets (B) will have a well-planned ride time (C) will often be checked at the exit (D) have to pay more for their tickets [SEP] As for visiting Walt Disney World. People usually have trouble in arranging. The following will give you some practical suggestions on paying a valuable visit to the famous park. When to go? While it's always busy, Disney World is slightly slower between mid-January and March. It's summer when the visitors , combined with the heat and humidity , are crowded. Your best choice is either early in the morning or late in the day--- often you can enjoy more rides in the first or last two hours of the day than you can the entire rest of the day combined. Where to stay? The question: to stay on Disney World Resort property or not. Disney World offers three resort pricing: Deluxe , Moderate, and Value, with good facilities and in-hotel attractions. That said, they all offer "Extra Magic Hours"(early access to the theme parks), guaranteed entry when the parks are over-crowded, and frequent shuttle service to the many attractions. Off-property hotels will have shuttle transfers, but often at less regular intervals ,wasting more time in transport. You'll also need a rental car, which adds to the expense, making up for saving in the hotel rate. The "Magic Your Way Package" is a good value if you have a large family. What to do? Make use of the "Fast Pass" , which , for no extra cost, allow you to insert your entrance ticket into a particular machine at the attraction and receive a schedule ride time a little later on. For example, use your Fast Pass ticket at a ride just before getting lunch. Once you're done eating, you can return to the ride and jump right on during the busiest time of day. At the Animal Kingdom, the animals are most active either early or late in the day, so plan accordingly. Thankfully _ is fit for the least busy times of day in the park. Study the map of the park the night before to plan your visit. You'll save time by knowing where things are and where you're headed.
03/12/2022 18:41:32 - INFO - __main__ - ['will have a well-planned ride time']
03/12/2022 18:41:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 18:41:32 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:41:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 18:41:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:41:32 - INFO - __main__ - Printing 3 examples
03/12/2022 18:41:32 - INFO - __main__ -  [race-high] Which of the following best serves as the best title of the story? (A) Touching by Feeling (B) To See or to Feel (C) Seeing by Feeling (D) Seeing Is Believing [SEP] "If you want to see something well, reach out and touch it!" That may seem a strange thing to say.But touching things can help you to see them better. Your eyes can tell you that a glass ball is round.But by holding it in your hands, you can feel how smooth and cool the ball is.You can feel how heavy the glass is.When you feel all these about the ball, you really see it. With your skin, you can feel better.For example, your fingers can tell the difference between two coins in your pockets.You can feel a little drop of water on the back of your hand, too. You can even feel sounds against your skin.Have you ever wanted to know why some people like very loud music? They must like to feel the sounds of music. All children soon learn what "Don't touch!" means.They hear it often.Yet most of us keep on touching things as we grow up.In shops, we often have to touch things before we buy them. The bottoms of our feet can feel things, too.You know this when you walk on warm sand, cool grass or a hard floor.All feel different under your feet. There are ways of learning to see well by feeling.One way is to close your eyes and try to feel everything that is touching your skin.Feel the shoes on your feet, the clothes on your body, the air on your skin...... Most museums are just for looking.But today some museums have some things to touch.Their signs say, "Do touch!" There you can feel everything on show.
03/12/2022 18:41:32 - INFO - __main__ - ['Seeing by Feeling']
03/12/2022 18:41:32 - INFO - __main__ -  [race-high] According to the author, the flu  _  . (A) doesn't cause sneezes (B) should be paid serious attention to (C) can be caused by spreading much time indoors (D) only spreads among the very young and the very old. [SEP] It's flu season.  What are you doing about it? David Oreck, Founder  My Oreck Air Purifier captures and destroys viruses, bacteria and germs.  2005 ORECK HOLDINGS,LLC. All Rights Reserved. All word marks, logos, product configurations and registered trademarks are owned and used under the authority of Oreck Holdings, LLC. For the very young and the very old and virtually everyone in between, the flu is nothing to sneeze at. So here's what you can do. Check with your doctor to see if a flu shot is right for you. Wash your hands frequently. Maintain a healthy diet and regimen of exercise. And because you're spending most of your time indoors, it makes sense that the air in your home is as fresh, clean and pure as it can be.  My Oreck XL(tm) Professional Air Purifier captures and destroys viruses, bacteria and germs. It removes mold spores, pollen, pet dander, household odors and other airborne particulates as small as 0.1 microns. (The flu virus measures 0.3 microns.) The Oreck Professional Air Purifier uses the same advanced technology as the prefix = st1 /U.S.submarine fleet where mariners are submerged for up to six months at a time. Because the permanent electronic filter never needs replacing, you can rinse it off as often as you like and it's as good as new. My Oreck Professional Air Purifier will literally pay for itself in what you pay for costly HEPA replacement filters that other manufacturers require. Besides HEPA filters don't destroy germs. They can only capture and hold them. So this flu season, take the Oreck Challenge and try my Oreck Professional Air Purifier risk-free for three full months. Then decide. CALL NOW AND RECEIVE A $100 GIFT-FREE.  Just for trying an Oreck Air Purifier risk-free for 90 days, we'll send you our $100 Oreck Cord-Free Electric Broom  ly free. It's a floor vac and a hand vac in one. If you don't feel the difference simply send the Air Purifier back--but keep the Electric Broom--there's no obligation. The shipping is free. Call 1-800-522-5961 ext. CR589 or visit www. oreck. com/decairNothing gets by an Oreck.
03/12/2022 18:41:32 - INFO - __main__ - ['should be paid serious attention to']
03/12/2022 18:41:32 - INFO - __main__ -  [race-high] We can draw the conclusion that Mr.Rady's work in Iraq is very_. (A) happy (B) tiring (C) busy (D) dangerous [SEP] When he stopped his car at a traffic light in Basra, Bassam Rady noticed the motorbike with two riders on it passed by him slowly. Suddenly the bike turned round and Mr. Rady, feeling dangerous, tried to drive off. Before he could, a man on the back of the bike took out a gun from his jacket and fired. The bullet went through the windscreen and just missed him. As he sped away another shot was fired, but missed the car.  Mr. Rady was an interpreter for British army in Iraq. As such, his life is in danger from the militia that once controlled the Iraqi city and is now returning.  More than a year ago Iraqi soldiers, backed by US, brought peace and stability to Basra by driving militants over the Iranian border in an operation called Charge of the Knights.  According to local estimates, however, about half have returned. Although they have not become as active as before, the militants are trying to shoot Iraqi citizens who worked with British. Most in danger are translators such as Mr. Rady.  The 31-year-old father worked with soldiers on dangerous missions but was refused resettlement in Britain at the end of his employment. He worked with nine translators. Seven of them have been killed.  "I'm like a cancer patient -- now that the militia is back, my family is just waiting for me to die," Mr. Rady said. "I see reports in the media that Basra is safe but it's not true. I know these militia people. I went to school with some of them. I didn't see them for a year but now they are around again. They have told me, 'Your day will come soon'." He takes security precautions and never follows a routine. This means that he cannot work because that would give the militants a better chance of success.  Militiamen belonging to the alMahdi Army took control of Basra between 2004 and 2007 with other Shia fighters. They enforced strict Islamic rules at the same time as running criminal rackets. British army was unable or unwilling to fight back and eventually went back to their base at the airport.
03/12/2022 18:41:32 - INFO - __main__ - ['dangerous']
03/12/2022 18:41:32 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:41:32 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:41:32 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 18:41:33 - INFO - __main__ - Global step 600 Train loss 0.670070 ACC 0.09375 on epoch=299
03/12/2022 18:41:33 - INFO - __main__ - save last model!
03/12/2022 18:41:40 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 18:41:41 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 18:41:41 - INFO - __main__ - Printing 3 examples
03/12/2022 18:41:41 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:41:41 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 18:41:41 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:41:41 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 18:41:41 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:41:41 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 18:41:41 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:41:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 18:41:44 - INFO - __main__ - Starting training!
03/12/2022 18:41:47 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:41:50 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 18:45:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_21_0.0002_8_predictions.txt
03/12/2022 18:45:25 - INFO - __main__ - ACC on test data: 0.0872
03/12/2022 18:45:25 - INFO - __main__ - prefix=race-high_32_21, lr=0.0002, bsz=8, dev_performance=0.125, test_performance=0.0872210953346856
03/12/2022 18:45:25 - INFO - __main__ - Running ... prefix=race-high_32_21, lr=0.0001, bsz=8 ...
03/12/2022 18:45:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:45:26 - INFO - __main__ - Printing 3 examples
03/12/2022 18:45:26 - INFO - __main__ -  [race-high] What is special about Sydney's Great Coastal Walk? (A) It starts from Royal National Park in the south. (B) It takes about more than five hours to complete. (C) It really has the longest coastline in the world. (D) It provides visitors a variety of great landscapes. [SEP] Beaches are not only great for lying on and doing water sports, and in fact one of the best ways of enjoying them is a classic beach walk. Here at iWantSun. Co. Uk, we've been searching the globe to find you the world's best and most glorious beach walks, and here's our pick of the top. The Footpath of the Gods, Amalfi Coast, Italy The name says it all really and you truly do feel up there to walking along this wonderful mountain coastal path, which offers some of the most striking views on the planet. The path begins at town of Bomerano to charming Positano along the UNESCO World Heritage area of the Amalfi Coast. The whole walk will take you approximately four and a half hours to complete and pass over narrow rocky paths, past sheer cliffs and shining blue bays. Sydney's Great Coastal Walk, Australia Sydney's coastline is one of the most beautiful and diverse in the world. Here you have national parks, historic sites, steep cliffs, sparkling beaches and quiet bays all in one place. Sydney's Great Walk runs all the way from Barrenjoey in the north to Royal National Park in the south and takes an incredible seven days to complete. However, if you're not up to doing the full walk, then there are many different parts of the walk that you can do right in the city. Walking from the city's famous Bondi Beach to the sweeping curve of Bronte Beach takes just an hour, which takes in some top scenery. Great Ocean Walk, Australia The Great Ocean Walk stretches 104 km along Victoria's famous Great Ocean Road, located on the southern coast of Australia, from the resort town Apollo Bay to the magnificent Twelve Apostles. The Twelve Apostles are the area's famous stone landmarks which stand out like giants from the sea. The walk passes through a range of landscapes and sights, from national parks, famous surfing spots and deserted beaches, to wild coastlines, cascading waterfalls, lush forests, historic lighthouses and ghostly shipwrecks. Day walks and shorter three-hour walks such as the Wreck Beach Walk or the Lighthouse Cemetery and Lookout Walk can also be enjoyed. So next time when you're looking for a beach holiday don't just think about the resorts and the sand, but consider a more active sun holiday, discovering some of the best beaches in the world.
03/12/2022 18:45:26 - INFO - __main__ - ['It provides visitors a variety of great landscapes.']
03/12/2022 18:45:26 - INFO - __main__ -  [race-high] In the author's opinion, which of the following is NOT true? (A) Nearly everyone has been told a wrong number. (B) It's necessary for everyone to have a telephone. (C) He himself can not decide whether to answer a call. (D) A telephone directory may bring in unexpected calls. [SEP] Many people think a telephone is essential. But I think it is a pest and a time waster. Very often you find it impossible to escape from some idle or curious chatter-box, or from somebody who wants something for nothing. If you have a telephone in your own house, you will admit that it tends to ring when you are asleep, or in the middle of a meal or a conversation, or when you are just going out, or when you are taking your bath. Are you strong minded enough to ignore it, to say to yourself,"Ah, well, it will all be the same in a hundred years' time" You are not. You think there may be some important news or messages for you. I can assure you that if a message is really important it will reach you sooner or later. Have you never rushed dripping from the bath, or chewing from the table, or dazed from the bed, only to be told that you are a wrong number? But you will say, you need not have your name printed in the telephone directory, and you can have a telephone which is only usable for outgoing calls. Besides, you will say, isn't it important to have a telephone in case of emergency--illness, an accident, or fire? Of course, you are right, but here in a thickly populated country like England one is seldom far from a telephone in case of dreadful necessity. I think perhaps I had better try to justify myself by trying to prove that what I like is good. I admit that in different circumstances--if I were a tycoon(business VIP),for instance, or bed ridden I might find a telephone essential. But then if I were a taxi-driver I should find a car essential. Let me put it another way: there are two things for which the English seem to show particular talent; one is mechanical invention, the other is literature. My own business happens to be with the use of words but I see I must now stop using them. For I have just been handed a slip of paper to say that somebody is waiting to speak to me on the telephone. I think I had better answer it. After all, one never knows, it may be something important.
03/12/2022 18:45:26 - INFO - __main__ - ["It's necessary for everyone to have a telephone."]
03/12/2022 18:45:26 - INFO - __main__ -  [race-high] Visitors who use Fast Pass tickets   _ (A) can get discounts on tickets (B) will have a well-planned ride time (C) will often be checked at the exit (D) have to pay more for their tickets [SEP] As for visiting Walt Disney World. People usually have trouble in arranging. The following will give you some practical suggestions on paying a valuable visit to the famous park. When to go? While it's always busy, Disney World is slightly slower between mid-January and March. It's summer when the visitors , combined with the heat and humidity , are crowded. Your best choice is either early in the morning or late in the day--- often you can enjoy more rides in the first or last two hours of the day than you can the entire rest of the day combined. Where to stay? The question: to stay on Disney World Resort property or not. Disney World offers three resort pricing: Deluxe , Moderate, and Value, with good facilities and in-hotel attractions. That said, they all offer "Extra Magic Hours"(early access to the theme parks), guaranteed entry when the parks are over-crowded, and frequent shuttle service to the many attractions. Off-property hotels will have shuttle transfers, but often at less regular intervals ,wasting more time in transport. You'll also need a rental car, which adds to the expense, making up for saving in the hotel rate. The "Magic Your Way Package" is a good value if you have a large family. What to do? Make use of the "Fast Pass" , which , for no extra cost, allow you to insert your entrance ticket into a particular machine at the attraction and receive a schedule ride time a little later on. For example, use your Fast Pass ticket at a ride just before getting lunch. Once you're done eating, you can return to the ride and jump right on during the busiest time of day. At the Animal Kingdom, the animals are most active either early or late in the day, so plan accordingly. Thankfully _ is fit for the least busy times of day in the park. Study the map of the park the night before to plan your visit. You'll save time by knowing where things are and where you're headed.
03/12/2022 18:45:26 - INFO - __main__ - ['will have a well-planned ride time']
03/12/2022 18:45:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 18:45:26 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:45:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 18:45:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:45:26 - INFO - __main__ - Printing 3 examples
03/12/2022 18:45:26 - INFO - __main__ -  [race-high] Which of the following best serves as the best title of the story? (A) Touching by Feeling (B) To See or to Feel (C) Seeing by Feeling (D) Seeing Is Believing [SEP] "If you want to see something well, reach out and touch it!" That may seem a strange thing to say.But touching things can help you to see them better. Your eyes can tell you that a glass ball is round.But by holding it in your hands, you can feel how smooth and cool the ball is.You can feel how heavy the glass is.When you feel all these about the ball, you really see it. With your skin, you can feel better.For example, your fingers can tell the difference between two coins in your pockets.You can feel a little drop of water on the back of your hand, too. You can even feel sounds against your skin.Have you ever wanted to know why some people like very loud music? They must like to feel the sounds of music. All children soon learn what "Don't touch!" means.They hear it often.Yet most of us keep on touching things as we grow up.In shops, we often have to touch things before we buy them. The bottoms of our feet can feel things, too.You know this when you walk on warm sand, cool grass or a hard floor.All feel different under your feet. There are ways of learning to see well by feeling.One way is to close your eyes and try to feel everything that is touching your skin.Feel the shoes on your feet, the clothes on your body, the air on your skin...... Most museums are just for looking.But today some museums have some things to touch.Their signs say, "Do touch!" There you can feel everything on show.
03/12/2022 18:45:26 - INFO - __main__ - ['Seeing by Feeling']
03/12/2022 18:45:26 - INFO - __main__ -  [race-high] According to the author, the flu  _  . (A) doesn't cause sneezes (B) should be paid serious attention to (C) can be caused by spreading much time indoors (D) only spreads among the very young and the very old. [SEP] It's flu season.  What are you doing about it? David Oreck, Founder  My Oreck Air Purifier captures and destroys viruses, bacteria and germs.  2005 ORECK HOLDINGS,LLC. All Rights Reserved. All word marks, logos, product configurations and registered trademarks are owned and used under the authority of Oreck Holdings, LLC. For the very young and the very old and virtually everyone in between, the flu is nothing to sneeze at. So here's what you can do. Check with your doctor to see if a flu shot is right for you. Wash your hands frequently. Maintain a healthy diet and regimen of exercise. And because you're spending most of your time indoors, it makes sense that the air in your home is as fresh, clean and pure as it can be.  My Oreck XL(tm) Professional Air Purifier captures and destroys viruses, bacteria and germs. It removes mold spores, pollen, pet dander, household odors and other airborne particulates as small as 0.1 microns. (The flu virus measures 0.3 microns.) The Oreck Professional Air Purifier uses the same advanced technology as the prefix = st1 /U.S.submarine fleet where mariners are submerged for up to six months at a time. Because the permanent electronic filter never needs replacing, you can rinse it off as often as you like and it's as good as new. My Oreck Professional Air Purifier will literally pay for itself in what you pay for costly HEPA replacement filters that other manufacturers require. Besides HEPA filters don't destroy germs. They can only capture and hold them. So this flu season, take the Oreck Challenge and try my Oreck Professional Air Purifier risk-free for three full months. Then decide. CALL NOW AND RECEIVE A $100 GIFT-FREE.  Just for trying an Oreck Air Purifier risk-free for 90 days, we'll send you our $100 Oreck Cord-Free Electric Broom  ly free. It's a floor vac and a hand vac in one. If you don't feel the difference simply send the Air Purifier back--but keep the Electric Broom--there's no obligation. The shipping is free. Call 1-800-522-5961 ext. CR589 or visit www. oreck. com/decairNothing gets by an Oreck.
03/12/2022 18:45:26 - INFO - __main__ - ['should be paid serious attention to']
03/12/2022 18:45:26 - INFO - __main__ -  [race-high] We can draw the conclusion that Mr.Rady's work in Iraq is very_. (A) happy (B) tiring (C) busy (D) dangerous [SEP] When he stopped his car at a traffic light in Basra, Bassam Rady noticed the motorbike with two riders on it passed by him slowly. Suddenly the bike turned round and Mr. Rady, feeling dangerous, tried to drive off. Before he could, a man on the back of the bike took out a gun from his jacket and fired. The bullet went through the windscreen and just missed him. As he sped away another shot was fired, but missed the car.  Mr. Rady was an interpreter for British army in Iraq. As such, his life is in danger from the militia that once controlled the Iraqi city and is now returning.  More than a year ago Iraqi soldiers, backed by US, brought peace and stability to Basra by driving militants over the Iranian border in an operation called Charge of the Knights.  According to local estimates, however, about half have returned. Although they have not become as active as before, the militants are trying to shoot Iraqi citizens who worked with British. Most in danger are translators such as Mr. Rady.  The 31-year-old father worked with soldiers on dangerous missions but was refused resettlement in Britain at the end of his employment. He worked with nine translators. Seven of them have been killed.  "I'm like a cancer patient -- now that the militia is back, my family is just waiting for me to die," Mr. Rady said. "I see reports in the media that Basra is safe but it's not true. I know these militia people. I went to school with some of them. I didn't see them for a year but now they are around again. They have told me, 'Your day will come soon'." He takes security precautions and never follows a routine. This means that he cannot work because that would give the militants a better chance of success.  Militiamen belonging to the alMahdi Army took control of Basra between 2004 and 2007 with other Shia fighters. They enforced strict Islamic rules at the same time as running criminal rackets. British army was unable or unwilling to fight back and eventually went back to their base at the airport.
03/12/2022 18:45:26 - INFO - __main__ - ['dangerous']
03/12/2022 18:45:26 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:45:26 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:45:26 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 18:45:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 18:45:37 - INFO - __main__ - Starting training!
03/12/2022 18:45:43 - INFO - __main__ - Step 10 Global step 10 Train loss 19.585777 on epoch=4
03/12/2022 18:45:48 - INFO - __main__ - Step 20 Global step 20 Train loss 17.855932 on epoch=9
03/12/2022 18:45:54 - INFO - __main__ - Step 30 Global step 30 Train loss 13.355105 on epoch=14
03/12/2022 18:46:01 - INFO - __main__ - Step 40 Global step 40 Train loss 11.419742 on epoch=19
03/12/2022 18:46:07 - INFO - __main__ - Step 50 Global step 50 Train loss 8.861404 on epoch=24
03/12/2022 18:46:20 - INFO - __main__ - Global step 50 Train loss 14.215592 ACC 0.0 on epoch=24
03/12/2022 18:46:27 - INFO - __main__ - Step 60 Global step 60 Train loss 6.307436 on epoch=29
03/12/2022 18:46:33 - INFO - __main__ - Step 70 Global step 70 Train loss 5.637355 on epoch=34
03/12/2022 18:46:40 - INFO - __main__ - Step 80 Global step 80 Train loss 5.340322 on epoch=39
03/12/2022 18:46:46 - INFO - __main__ - Step 90 Global step 90 Train loss 4.615144 on epoch=44
03/12/2022 18:46:52 - INFO - __main__ - Step 100 Global step 100 Train loss 4.456239 on epoch=49
03/12/2022 18:46:54 - INFO - __main__ - Global step 100 Train loss 5.271299 ACC 0.0625 on epoch=49
03/12/2022 18:47:01 - INFO - __main__ - Step 110 Global step 110 Train loss 4.376353 on epoch=54
03/12/2022 18:47:07 - INFO - __main__ - Step 120 Global step 120 Train loss 3.842547 on epoch=59
03/12/2022 18:47:13 - INFO - __main__ - Step 130 Global step 130 Train loss 4.225536 on epoch=64
03/12/2022 18:47:19 - INFO - __main__ - Step 140 Global step 140 Train loss 3.799601 on epoch=69
03/12/2022 18:47:26 - INFO - __main__ - Step 150 Global step 150 Train loss 3.593538 on epoch=74
03/12/2022 18:47:28 - INFO - __main__ - Global step 150 Train loss 3.967515 ACC 0.125 on epoch=74
03/12/2022 18:47:35 - INFO - __main__ - Step 160 Global step 160 Train loss 3.474063 on epoch=79
03/12/2022 18:47:41 - INFO - __main__ - Step 170 Global step 170 Train loss 3.560322 on epoch=84
03/12/2022 18:47:47 - INFO - __main__ - Step 180 Global step 180 Train loss 3.384881 on epoch=89
03/12/2022 18:47:54 - INFO - __main__ - Step 190 Global step 190 Train loss 3.305496 on epoch=94
03/12/2022 18:48:00 - INFO - __main__ - Step 200 Global step 200 Train loss 3.348259 on epoch=99
03/12/2022 18:48:02 - INFO - __main__ - Global step 200 Train loss 3.414604 ACC 0.15625 on epoch=99
03/12/2022 18:48:09 - INFO - __main__ - Step 210 Global step 210 Train loss 3.189593 on epoch=104
03/12/2022 18:48:15 - INFO - __main__ - Step 220 Global step 220 Train loss 2.927934 on epoch=109
03/12/2022 18:48:21 - INFO - __main__ - Step 230 Global step 230 Train loss 2.971461 on epoch=114
03/12/2022 18:48:27 - INFO - __main__ - Step 240 Global step 240 Train loss 3.040193 on epoch=119
03/12/2022 18:48:34 - INFO - __main__ - Step 250 Global step 250 Train loss 2.787646 on epoch=124
03/12/2022 18:48:36 - INFO - __main__ - Global step 250 Train loss 2.983366 ACC 0.09375 on epoch=124
03/12/2022 18:48:42 - INFO - __main__ - Step 260 Global step 260 Train loss 2.775760 on epoch=129
03/12/2022 18:48:48 - INFO - __main__ - Step 270 Global step 270 Train loss 2.855357 on epoch=134
03/12/2022 18:48:54 - INFO - __main__ - Step 280 Global step 280 Train loss 2.663396 on epoch=139
03/12/2022 18:49:01 - INFO - __main__ - Step 290 Global step 290 Train loss 2.591403 on epoch=144
03/12/2022 18:49:07 - INFO - __main__ - Step 300 Global step 300 Train loss 2.464678 on epoch=149
03/12/2022 18:49:09 - INFO - __main__ - Global step 300 Train loss 2.670119 ACC 0.09375 on epoch=149
03/12/2022 18:49:15 - INFO - __main__ - Step 310 Global step 310 Train loss 2.423718 on epoch=154
03/12/2022 18:49:21 - INFO - __main__ - Step 320 Global step 320 Train loss 2.231770 on epoch=159
03/12/2022 18:49:27 - INFO - __main__ - Step 330 Global step 330 Train loss 2.260358 on epoch=164
03/12/2022 18:49:34 - INFO - __main__ - Step 340 Global step 340 Train loss 2.263781 on epoch=169
03/12/2022 18:49:40 - INFO - __main__ - Step 350 Global step 350 Train loss 1.976551 on epoch=174
03/12/2022 18:49:42 - INFO - __main__ - Global step 350 Train loss 2.231236 ACC 0.15625 on epoch=174
03/12/2022 18:49:48 - INFO - __main__ - Step 360 Global step 360 Train loss 1.845695 on epoch=179
03/12/2022 18:49:54 - INFO - __main__ - Step 370 Global step 370 Train loss 1.910139 on epoch=184
03/12/2022 18:50:01 - INFO - __main__ - Step 380 Global step 380 Train loss 2.037587 on epoch=189
03/12/2022 18:50:07 - INFO - __main__ - Step 390 Global step 390 Train loss 1.784053 on epoch=194
03/12/2022 18:50:13 - INFO - __main__ - Step 400 Global step 400 Train loss 2.001136 on epoch=199
03/12/2022 18:50:15 - INFO - __main__ - Global step 400 Train loss 1.915722 ACC 0.15625 on epoch=199
03/12/2022 18:50:22 - INFO - __main__ - Step 410 Global step 410 Train loss 1.600686 on epoch=204
03/12/2022 18:50:28 - INFO - __main__ - Step 420 Global step 420 Train loss 1.890159 on epoch=209
03/12/2022 18:50:34 - INFO - __main__ - Step 430 Global step 430 Train loss 1.600443 on epoch=214
03/12/2022 18:50:40 - INFO - __main__ - Step 440 Global step 440 Train loss 1.710384 on epoch=219
03/12/2022 18:50:47 - INFO - __main__ - Step 450 Global step 450 Train loss 1.517801 on epoch=224
03/12/2022 18:50:49 - INFO - __main__ - Global step 450 Train loss 1.663895 ACC 0.1875 on epoch=224
03/12/2022 18:50:56 - INFO - __main__ - Step 460 Global step 460 Train loss 1.362342 on epoch=229
03/12/2022 18:51:02 - INFO - __main__ - Step 470 Global step 470 Train loss 1.364738 on epoch=234
03/12/2022 18:51:08 - INFO - __main__ - Step 480 Global step 480 Train loss 1.433472 on epoch=239
03/12/2022 18:51:14 - INFO - __main__ - Step 490 Global step 490 Train loss 1.451856 on epoch=244
03/12/2022 18:51:21 - INFO - __main__ - Step 500 Global step 500 Train loss 1.562768 on epoch=249
03/12/2022 18:51:23 - INFO - __main__ - Global step 500 Train loss 1.435035 ACC 0.0625 on epoch=249
03/12/2022 18:51:29 - INFO - __main__ - Step 510 Global step 510 Train loss 1.386202 on epoch=254
03/12/2022 18:51:35 - INFO - __main__ - Step 520 Global step 520 Train loss 1.504733 on epoch=259
03/12/2022 18:51:42 - INFO - __main__ - Step 530 Global step 530 Train loss 1.307747 on epoch=264
03/12/2022 18:51:48 - INFO - __main__ - Step 540 Global step 540 Train loss 1.209909 on epoch=269
03/12/2022 18:51:54 - INFO - __main__ - Step 550 Global step 550 Train loss 1.278207 on epoch=274
03/12/2022 18:51:56 - INFO - __main__ - Global step 550 Train loss 1.337359 ACC 0.03125 on epoch=274
03/12/2022 18:52:03 - INFO - __main__ - Step 560 Global step 560 Train loss 1.418235 on epoch=279
03/12/2022 18:52:09 - INFO - __main__ - Step 570 Global step 570 Train loss 1.277806 on epoch=284
03/12/2022 18:52:15 - INFO - __main__ - Step 580 Global step 580 Train loss 1.172883 on epoch=289
03/12/2022 18:52:21 - INFO - __main__ - Step 590 Global step 590 Train loss 1.484496 on epoch=294
03/12/2022 18:52:28 - INFO - __main__ - Step 600 Global step 600 Train loss 1.166862 on epoch=299
03/12/2022 18:52:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:52:29 - INFO - __main__ - Printing 3 examples
03/12/2022 18:52:29 - INFO - __main__ -  [race-high] In Hawaii the mountains are in the center of the islands because   _  . (A) of the weather (B) the islands were created by volcanic eruptions (C) of the geological conditions (D) of the islands' location in the Pacific [SEP] The Hawaiian Islands are situated about two thousand miles away from North America, right in the middle of the Pacific Ocean. Despite  the distance the islands actually make up the fiftieth state of the United States. The islands were all formed by volcanic eruption and on the largest of the islands, Hawaii, or the Big Island, there are still two active volcanoes, the Mauna Loa and the Kilauea, which still erupt every few years. On the Hawaiian islands the natives have a particularly strange way of indicating directions. They don't use the north, south, east and west system common to the rest of the world. They use the mauka andmakaisystem.Maukameans "mountain". The mountains in Hawaii are always at the center of the islands, as these are volcanic islands.Makaimeans "the sea". The islands are small and the system is simple. Wherever you want to visit, it can always be described in terms of where it lies in relation to the mountains and the sea. A typical conversation between a native Hawaiian and a tourist might go as follow. TOURIST: Excuse me! Could you tell me where the Sheraton Hotel is? HAWAIIAN: Well... Let me see! From here it's two blocks mauka, and then one blockmakai. You can't miss it!
03/12/2022 18:52:29 - INFO - __main__ - ['the islands were created by volcanic eruptions']
03/12/2022 18:52:29 - INFO - __main__ -  [race-high] Which of the following is true about Planet English according to the passage? (A) Planet English can support the students who study computer. (B) Planet English can help the students and their teachers interact. (C) Planet English makes it possible for students to communicate with speakers from around the world. (D) Planet English offers only a range of spoken communication for students. [SEP] Planet English is the world's leading interactive multimedia software package for English language teaching and learning. For Students Planet English uses the latest in multimedia and information technology to support students who wish to learn English for international communication. Planet English is an exciting, high-tech, interactive way of learning English. It contains more than 40 hours of video and audio recordings, over 2,500 0riginal graphics, 3,000 interactive screens and 80 different activity types including real time student voice recordings. For Teachers Planet English is more than just a computer program. It includes a package of resources to complement any Eng-lish language teaching programme. Teachers can easily integrate Planet English with classroom activities using the detailed Teacher's Manual  and Student Workbooks. Teachers can also manage the learning experience for students using the unique Planet English Courseware Management System (CMS). The CMS allows teachers to tailor courses to their syllabus  and to students' needs by "mapping" content for classes or individuals. Activities and exercises that are relevant to the center's syllabus are then delivered to students in the appropriate lesson, ensuring students "navigate" to the right area of the programme of each lesson. For Educational Managers Planet English is the world's leading Computer Assisted Language Learning (CALL) program. It allows English language teaching centers to enhance the educational quality of teaching programmes, improve learning outcomes and provide professional development for teaching staff. Implementing  Planet English allows English language teaching centers to maximize the benefits of computor hardware because it provides teachers and learners with an easy-to-use and highly productive CALL resource.
03/12/2022 18:52:29 - INFO - __main__ - ['Planet English makes it possible for students to communicate with speakers from around the world.']
03/12/2022 18:52:29 - INFO - __main__ -  [race-high] The Corcovado Hill is famous for  _  . (A) the samba parade (B) the beautiful view of the city (C) the statue of Jesus (D) the small church [SEP] On the coast of Brazil lies its most famous city Rio de Janeiro. Rio is a beautiful place to visit. It is full of exciting parades, historical architecture and natural beauty. The most famous time of year in Rio is the spring when the samba schools organize a giant parade of dance, music and costumes. Samba is a type of fast Brazilian music, which is perfect for dancing. The carnival lasts two days and over 20 samba schools perform, a total of 70,000 people in brightly coloured costumes, all dancing and singing! It is a magnificent spectacle and millions of people watch the carnival on television around the world. Rio de Janeiro is a religious city and it is full of churches. Some, like the former Imperial Church, were built over 400 years ago while others, like the Metropolitan Cathedral, are very modern. The Metropolitan Cathedral was built in 1976 and is so large that 20,000 people can stand inside at one time! Another important religious site in Rio is the Corcovado hill. On the top of the hill, along with a small church, stands a 30-metre high statue of Jesus with his arms stretched out. From this point you can see every part of the city.
03/12/2022 18:52:29 - INFO - __main__ - ['the beautiful view of the city']
03/12/2022 18:52:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 18:52:29 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:52:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 18:52:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:52:29 - INFO - __main__ - Printing 3 examples
03/12/2022 18:52:29 - INFO - __main__ -  [race-high] Which of the following is true according to the text? (A) Ieoh Ming Pei was born in America. (B) Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young. (C) Ieoh Ming Pei got a degree for architecture in 1948. (D) Ieoh Ming Pei got a Harvard undergraduate Degree in 1946. [SEP] On this vivid planet, it appears color1ful with many world famous buildings. Among these largest artificial articles in the world, many were designed by the same architect--Ieoh Ming Pei. Pei, the 1983 Laureate of the Pritzker, Architecture Prize, is a founding partner of I. M. Pei & Partners based in New York City. He was born in China in 1917, the son of a banker. He came to the United States in 1935 to study architecture at the Massachusetts Institute of Technology and the Harvard Graduate School of Design (M. Arch. 1946). From 1945 to 1948, Pei taught at Harvard. In 1948 he accepted the newly created post of director of Architecture at Webb & Knapp, Inc., and this association resulted in major architectural and planning projects in big cities. In 1958, he formed the partnership of I. M. Pei & Associates, which became I. M. Pei & Partners in 1966. The partnership received the 1968 Architectural Firm Award of The American Institute of Architects. Pei has designed over forty projects in the world, twenty of which have been award winners. His outstanding projects have included the East Building of the National Gallery of Art, Washington, D.C.; the John Fitzgerald Kennedy Library near Boston; the Fragrant Hill Hotel near Beijing, China. Pei is now a member of the National Council on the Arts, and before served on the National Council on the Humanities. He is a Fellow of the American Institute of Architects, a member of the Royal Institute of British Architects, and an elected member of the American Academy of Arts and Letters. He is a member of the Corporation of the Massachusetts Institute of Technology. As a student, he was awarded the MIT Traveling Fellowship, at Harvard. He later won a lot of honors. In 1982, the deans of the architectural schools of America chose I. M. Pei as the best designer of significant non-residential   structures.
03/12/2022 18:52:29 - INFO - __main__ - ['Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young.']
03/12/2022 18:52:29 - INFO - __main__ -  [race-high] According to the recently discovered evidence,   _  . (A) different regions of the brain have to work separately (B) people with phonagnosia are good at facial recognition (C) regions of the brain for personal recognition are connected (D) person recognition is controlled by the same part of the brain [SEP] James Cooke, of Islip,N.Y.,can't recognize other people. "I see eyes, nose, cheekbones, but no face," he said. "I've even passed by my son and daughter without recognizing them."  He is not the only one. Those with prosopagnosia, also known as face blindness, can see perfectly well, but their brains are unable to piece together the information needed to understand that a collection of features represents an individual's face. The condition is a neurological mystery, but new research has shed light on this strange disease.  Some scientists had believed that faces and voices, the two main ways people recognize one another, were processed separately by the brain. Indeed, a condition parallel to prosopagnosia, called  _ , similarly leaves a person unable to distinguish a familiar voice from an unfamiliar one. But by testing for these two conditions at the same time, researchers at the Max Planck Institute recently found evidence that face and voice recognition may be linked in a unique person-recognition system. The scientists observed the brain activity of 19 healthy volunteers as they were led through tasks that tested their ability to recognize both faces and voices. The researchers found that regions of the brain, associated with facial recognition, are directly linked to regions responsible for voice recognition.  This research helps explain why a person with prosopagnosia may still have difficulty determining who a person is even after he has begun to speak. The challenge for scientists is to find out where this system breaks down. Are these connections in the brain missing entirely, or are people unable to recognize faces and voices simply unable to use these links in some way?  It is unclear how many people have these conditions. Many don't even realize they have problems with facial or voice recognition. While some develop these difficulties after a brain injury, others develop it in childhood.
03/12/2022 18:52:29 - INFO - __main__ - ['regions of the brain for personal recognition are connected']
03/12/2022 18:52:29 - INFO - __main__ -  [race-high] By writing the passage, the author mainly wants to tell us    _   . (A) to think twice before we act (B) to be grateful for what we have (C) it is no use making complaints (D) saying "thank you" has many disadvantages [SEP] Throughout our childhood our parents taught us to say "thank you" and it has become a habit -- something we say automatically  , along with "please". And because of this we have forgotten just how important gratitude is and how essential it is in leading fulfilled   lives. Just for a minute, think of all the things you are grateful for, such as loving friends, good health, great holidays as well as everyday items such as a comfortable home, TV, and clean water. The list, in fact, could go on and on. Now focus on events that have made you angry -- it's raining, the car won't start, and a colleague   makes you annoyed. You start to feel unhappy, and that is something that certainly does not make you feel good! In fact, we have the ability to choose how we feel -- it's just our perception   of how things are. But for most of us, it just doesn't seem easy. Let me give you an example: it's a rainy day, and immediately most people will start to complain, telling everyone who will listen what a miserable day it is, with the result that they end up feeling miserable themselves. But look at it another way and despite wet clothes and hair, both will dry perfectly well and no lasting harm has been done. And in addition to this, because of rain, we not only live in a green and beautiful landscape, we are also able to grow a lot of fruit and vegetables. There really is no obvious reason for feeling miserable -- in fact there is a great deal to be grateful for. It all depends on what we think about things. Realize what a difference having gratitude can make to your life. That's why gratitude is so special -- use it to feel good!
03/12/2022 18:52:29 - INFO - __main__ - ['to be grateful for what we have']
03/12/2022 18:52:29 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:52:29 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:52:29 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 18:52:30 - INFO - __main__ - Global step 600 Train loss 1.304056 ACC 0.03125 on epoch=299
03/12/2022 18:52:30 - INFO - __main__ - save last model!
03/12/2022 18:52:37 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 18:52:37 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 18:52:37 - INFO - __main__ - Printing 3 examples
03/12/2022 18:52:37 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:52:37 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 18:52:37 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:52:37 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 18:52:37 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 18:52:37 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 18:52:37 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:52:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 18:52:42 - INFO - __main__ - Starting training!
03/12/2022 18:52:43 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:52:47 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 18:56:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_21_0.0001_8_predictions.txt
03/12/2022 18:56:35 - INFO - __main__ - ACC on test data: 0.1156
03/12/2022 18:56:35 - INFO - __main__ - prefix=race-high_32_21, lr=0.0001, bsz=8, dev_performance=0.1875, test_performance=0.11561866125760649
03/12/2022 18:56:35 - INFO - __main__ - Running ... prefix=race-high_32_42, lr=0.0005, bsz=8 ...
03/12/2022 18:56:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:56:36 - INFO - __main__ - Printing 3 examples
03/12/2022 18:56:36 - INFO - __main__ -  [race-high] In Hawaii the mountains are in the center of the islands because   _  . (A) of the weather (B) the islands were created by volcanic eruptions (C) of the geological conditions (D) of the islands' location in the Pacific [SEP] The Hawaiian Islands are situated about two thousand miles away from North America, right in the middle of the Pacific Ocean. Despite  the distance the islands actually make up the fiftieth state of the United States. The islands were all formed by volcanic eruption and on the largest of the islands, Hawaii, or the Big Island, there are still two active volcanoes, the Mauna Loa and the Kilauea, which still erupt every few years. On the Hawaiian islands the natives have a particularly strange way of indicating directions. They don't use the north, south, east and west system common to the rest of the world. They use the mauka andmakaisystem.Maukameans "mountain". The mountains in Hawaii are always at the center of the islands, as these are volcanic islands.Makaimeans "the sea". The islands are small and the system is simple. Wherever you want to visit, it can always be described in terms of where it lies in relation to the mountains and the sea. A typical conversation between a native Hawaiian and a tourist might go as follow. TOURIST: Excuse me! Could you tell me where the Sheraton Hotel is? HAWAIIAN: Well... Let me see! From here it's two blocks mauka, and then one blockmakai. You can't miss it!
03/12/2022 18:56:36 - INFO - __main__ - ['the islands were created by volcanic eruptions']
03/12/2022 18:56:36 - INFO - __main__ -  [race-high] Which of the following is true about Planet English according to the passage? (A) Planet English can support the students who study computer. (B) Planet English can help the students and their teachers interact. (C) Planet English makes it possible for students to communicate with speakers from around the world. (D) Planet English offers only a range of spoken communication for students. [SEP] Planet English is the world's leading interactive multimedia software package for English language teaching and learning. For Students Planet English uses the latest in multimedia and information technology to support students who wish to learn English for international communication. Planet English is an exciting, high-tech, interactive way of learning English. It contains more than 40 hours of video and audio recordings, over 2,500 0riginal graphics, 3,000 interactive screens and 80 different activity types including real time student voice recordings. For Teachers Planet English is more than just a computer program. It includes a package of resources to complement any Eng-lish language teaching programme. Teachers can easily integrate Planet English with classroom activities using the detailed Teacher's Manual  and Student Workbooks. Teachers can also manage the learning experience for students using the unique Planet English Courseware Management System (CMS). The CMS allows teachers to tailor courses to their syllabus  and to students' needs by "mapping" content for classes or individuals. Activities and exercises that are relevant to the center's syllabus are then delivered to students in the appropriate lesson, ensuring students "navigate" to the right area of the programme of each lesson. For Educational Managers Planet English is the world's leading Computer Assisted Language Learning (CALL) program. It allows English language teaching centers to enhance the educational quality of teaching programmes, improve learning outcomes and provide professional development for teaching staff. Implementing  Planet English allows English language teaching centers to maximize the benefits of computor hardware because it provides teachers and learners with an easy-to-use and highly productive CALL resource.
03/12/2022 18:56:36 - INFO - __main__ - ['Planet English makes it possible for students to communicate with speakers from around the world.']
03/12/2022 18:56:36 - INFO - __main__ -  [race-high] The Corcovado Hill is famous for  _  . (A) the samba parade (B) the beautiful view of the city (C) the statue of Jesus (D) the small church [SEP] On the coast of Brazil lies its most famous city Rio de Janeiro. Rio is a beautiful place to visit. It is full of exciting parades, historical architecture and natural beauty. The most famous time of year in Rio is the spring when the samba schools organize a giant parade of dance, music and costumes. Samba is a type of fast Brazilian music, which is perfect for dancing. The carnival lasts two days and over 20 samba schools perform, a total of 70,000 people in brightly coloured costumes, all dancing and singing! It is a magnificent spectacle and millions of people watch the carnival on television around the world. Rio de Janeiro is a religious city and it is full of churches. Some, like the former Imperial Church, were built over 400 years ago while others, like the Metropolitan Cathedral, are very modern. The Metropolitan Cathedral was built in 1976 and is so large that 20,000 people can stand inside at one time! Another important religious site in Rio is the Corcovado hill. On the top of the hill, along with a small church, stands a 30-metre high statue of Jesus with his arms stretched out. From this point you can see every part of the city.
03/12/2022 18:56:36 - INFO - __main__ - ['the beautiful view of the city']
03/12/2022 18:56:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 18:56:36 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:56:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 18:56:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 18:56:36 - INFO - __main__ - Printing 3 examples
03/12/2022 18:56:36 - INFO - __main__ -  [race-high] Which of the following is true according to the text? (A) Ieoh Ming Pei was born in America. (B) Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young. (C) Ieoh Ming Pei got a degree for architecture in 1948. (D) Ieoh Ming Pei got a Harvard undergraduate Degree in 1946. [SEP] On this vivid planet, it appears color1ful with many world famous buildings. Among these largest artificial articles in the world, many were designed by the same architect--Ieoh Ming Pei. Pei, the 1983 Laureate of the Pritzker, Architecture Prize, is a founding partner of I. M. Pei & Partners based in New York City. He was born in China in 1917, the son of a banker. He came to the United States in 1935 to study architecture at the Massachusetts Institute of Technology and the Harvard Graduate School of Design (M. Arch. 1946). From 1945 to 1948, Pei taught at Harvard. In 1948 he accepted the newly created post of director of Architecture at Webb & Knapp, Inc., and this association resulted in major architectural and planning projects in big cities. In 1958, he formed the partnership of I. M. Pei & Associates, which became I. M. Pei & Partners in 1966. The partnership received the 1968 Architectural Firm Award of The American Institute of Architects. Pei has designed over forty projects in the world, twenty of which have been award winners. His outstanding projects have included the East Building of the National Gallery of Art, Washington, D.C.; the John Fitzgerald Kennedy Library near Boston; the Fragrant Hill Hotel near Beijing, China. Pei is now a member of the National Council on the Arts, and before served on the National Council on the Humanities. He is a Fellow of the American Institute of Architects, a member of the Royal Institute of British Architects, and an elected member of the American Academy of Arts and Letters. He is a member of the Corporation of the Massachusetts Institute of Technology. As a student, he was awarded the MIT Traveling Fellowship, at Harvard. He later won a lot of honors. In 1982, the deans of the architectural schools of America chose I. M. Pei as the best designer of significant non-residential   structures.
03/12/2022 18:56:36 - INFO - __main__ - ['Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young.']
03/12/2022 18:56:36 - INFO - __main__ -  [race-high] According to the recently discovered evidence,   _  . (A) different regions of the brain have to work separately (B) people with phonagnosia are good at facial recognition (C) regions of the brain for personal recognition are connected (D) person recognition is controlled by the same part of the brain [SEP] James Cooke, of Islip,N.Y.,can't recognize other people. "I see eyes, nose, cheekbones, but no face," he said. "I've even passed by my son and daughter without recognizing them."  He is not the only one. Those with prosopagnosia, also known as face blindness, can see perfectly well, but their brains are unable to piece together the information needed to understand that a collection of features represents an individual's face. The condition is a neurological mystery, but new research has shed light on this strange disease.  Some scientists had believed that faces and voices, the two main ways people recognize one another, were processed separately by the brain. Indeed, a condition parallel to prosopagnosia, called  _ , similarly leaves a person unable to distinguish a familiar voice from an unfamiliar one. But by testing for these two conditions at the same time, researchers at the Max Planck Institute recently found evidence that face and voice recognition may be linked in a unique person-recognition system. The scientists observed the brain activity of 19 healthy volunteers as they were led through tasks that tested their ability to recognize both faces and voices. The researchers found that regions of the brain, associated with facial recognition, are directly linked to regions responsible for voice recognition.  This research helps explain why a person with prosopagnosia may still have difficulty determining who a person is even after he has begun to speak. The challenge for scientists is to find out where this system breaks down. Are these connections in the brain missing entirely, or are people unable to recognize faces and voices simply unable to use these links in some way?  It is unclear how many people have these conditions. Many don't even realize they have problems with facial or voice recognition. While some develop these difficulties after a brain injury, others develop it in childhood.
03/12/2022 18:56:36 - INFO - __main__ - ['regions of the brain for personal recognition are connected']
03/12/2022 18:56:36 - INFO - __main__ -  [race-high] By writing the passage, the author mainly wants to tell us    _   . (A) to think twice before we act (B) to be grateful for what we have (C) it is no use making complaints (D) saying "thank you" has many disadvantages [SEP] Throughout our childhood our parents taught us to say "thank you" and it has become a habit -- something we say automatically  , along with "please". And because of this we have forgotten just how important gratitude is and how essential it is in leading fulfilled   lives. Just for a minute, think of all the things you are grateful for, such as loving friends, good health, great holidays as well as everyday items such as a comfortable home, TV, and clean water. The list, in fact, could go on and on. Now focus on events that have made you angry -- it's raining, the car won't start, and a colleague   makes you annoyed. You start to feel unhappy, and that is something that certainly does not make you feel good! In fact, we have the ability to choose how we feel -- it's just our perception   of how things are. But for most of us, it just doesn't seem easy. Let me give you an example: it's a rainy day, and immediately most people will start to complain, telling everyone who will listen what a miserable day it is, with the result that they end up feeling miserable themselves. But look at it another way and despite wet clothes and hair, both will dry perfectly well and no lasting harm has been done. And in addition to this, because of rain, we not only live in a green and beautiful landscape, we are also able to grow a lot of fruit and vegetables. There really is no obvious reason for feeling miserable -- in fact there is a great deal to be grateful for. It all depends on what we think about things. Realize what a difference having gratitude can make to your life. That's why gratitude is so special -- use it to feel good!
03/12/2022 18:56:36 - INFO - __main__ - ['to be grateful for what we have']
03/12/2022 18:56:36 - INFO - __main__ - Tokenizing Input ...
03/12/2022 18:56:36 - INFO - __main__ - Tokenizing Output ...
03/12/2022 18:56:36 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 18:56:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 18:56:49 - INFO - __main__ - Starting training!
03/12/2022 18:56:56 - INFO - __main__ - Step 10 Global step 10 Train loss 20.802673 on epoch=4
03/12/2022 18:57:02 - INFO - __main__ - Step 20 Global step 20 Train loss 12.440931 on epoch=9
03/12/2022 18:57:08 - INFO - __main__ - Step 30 Global step 30 Train loss 6.628545 on epoch=14
03/12/2022 18:57:14 - INFO - __main__ - Step 40 Global step 40 Train loss 3.696647 on epoch=19
03/12/2022 18:57:20 - INFO - __main__ - Step 50 Global step 50 Train loss 3.022943 on epoch=24
03/12/2022 18:57:23 - INFO - __main__ - Global step 50 Train loss 9.318348 ACC 0.09375 on epoch=24
03/12/2022 18:57:29 - INFO - __main__ - Step 60 Global step 60 Train loss 2.656931 on epoch=29
03/12/2022 18:57:35 - INFO - __main__ - Step 70 Global step 70 Train loss 2.392351 on epoch=34
03/12/2022 18:57:41 - INFO - __main__ - Step 80 Global step 80 Train loss 1.926584 on epoch=39
03/12/2022 18:57:48 - INFO - __main__ - Step 90 Global step 90 Train loss 1.624146 on epoch=44
03/12/2022 18:57:54 - INFO - __main__ - Step 100 Global step 100 Train loss 1.467335 on epoch=49
03/12/2022 18:57:56 - INFO - __main__ - Global step 100 Train loss 2.013469 ACC 0.03125 on epoch=49
03/12/2022 18:58:02 - INFO - __main__ - Step 110 Global step 110 Train loss 1.373814 on epoch=54
03/12/2022 18:58:08 - INFO - __main__ - Step 120 Global step 120 Train loss 1.188137 on epoch=59
03/12/2022 18:58:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.994348 on epoch=64
03/12/2022 18:58:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.936470 on epoch=69
03/12/2022 18:58:26 - INFO - __main__ - Step 150 Global step 150 Train loss 1.054115 on epoch=74
03/12/2022 18:58:28 - INFO - __main__ - Global step 150 Train loss 1.109377 ACC 0.0 on epoch=74
03/12/2022 18:58:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.930798 on epoch=79
03/12/2022 18:58:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.777819 on epoch=84
03/12/2022 18:58:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.808558 on epoch=89
03/12/2022 18:58:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.775163 on epoch=94
03/12/2022 18:58:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.757115 on epoch=99
03/12/2022 18:59:01 - INFO - __main__ - Global step 200 Train loss 0.809890 ACC 0.0 on epoch=99
03/12/2022 18:59:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.658873 on epoch=104
03/12/2022 18:59:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.745162 on epoch=109
03/12/2022 18:59:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.695639 on epoch=114
03/12/2022 18:59:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.707613 on epoch=119
03/12/2022 18:59:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.607670 on epoch=124
03/12/2022 18:59:34 - INFO - __main__ - Global step 250 Train loss 0.682992 ACC 0.0 on epoch=124
03/12/2022 18:59:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.686971 on epoch=129
03/12/2022 18:59:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.654549 on epoch=134
03/12/2022 18:59:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.680614 on epoch=139
03/12/2022 18:59:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.643424 on epoch=144
03/12/2022 19:00:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.601402 on epoch=149
03/12/2022 19:00:07 - INFO - __main__ - Global step 300 Train loss 0.653392 ACC 0.0 on epoch=149
03/12/2022 19:00:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.605674 on epoch=154
03/12/2022 19:00:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.548050 on epoch=159
03/12/2022 19:00:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.572192 on epoch=164
03/12/2022 19:00:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.517579 on epoch=169
03/12/2022 19:00:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.495167 on epoch=174
03/12/2022 19:00:40 - INFO - __main__ - Global step 350 Train loss 0.547733 ACC 0.0 on epoch=174
03/12/2022 19:00:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.476773 on epoch=179
03/12/2022 19:00:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.521391 on epoch=184
03/12/2022 19:00:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.502093 on epoch=189
03/12/2022 19:01:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.554477 on epoch=194
03/12/2022 19:01:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.511546 on epoch=199
03/12/2022 19:01:13 - INFO - __main__ - Global step 400 Train loss 0.513256 ACC 0.0 on epoch=199
03/12/2022 19:01:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.467349 on epoch=204
03/12/2022 19:01:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.484115 on epoch=209
03/12/2022 19:01:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.418514 on epoch=214
03/12/2022 19:01:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.415959 on epoch=219
03/12/2022 19:01:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.415542 on epoch=224
03/12/2022 19:01:46 - INFO - __main__ - Global step 450 Train loss 0.440296 ACC 0.0 on epoch=224
03/12/2022 19:01:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.411327 on epoch=229
03/12/2022 19:01:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.435810 on epoch=234
03/12/2022 19:02:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.421919 on epoch=239
03/12/2022 19:02:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.408881 on epoch=244
03/12/2022 19:02:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.382993 on epoch=249
03/12/2022 19:02:19 - INFO - __main__ - Global step 500 Train loss 0.412186 ACC 0.0 on epoch=249
03/12/2022 19:02:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.439082 on epoch=254
03/12/2022 19:02:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.401784 on epoch=259
03/12/2022 19:02:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.385214 on epoch=264
03/12/2022 19:02:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.392131 on epoch=269
03/12/2022 19:02:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.412494 on epoch=274
03/12/2022 19:02:52 - INFO - __main__ - Global step 550 Train loss 0.406141 ACC 0.0 on epoch=274
03/12/2022 19:02:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.369214 on epoch=279
03/12/2022 19:03:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.401768 on epoch=284
03/12/2022 19:03:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.377342 on epoch=289
03/12/2022 19:03:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.384600 on epoch=294
03/12/2022 19:03:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.388926 on epoch=299
03/12/2022 19:03:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:03:24 - INFO - __main__ - Printing 3 examples
03/12/2022 19:03:24 - INFO - __main__ -  [race-high] In Hawaii the mountains are in the center of the islands because   _  . (A) of the weather (B) the islands were created by volcanic eruptions (C) of the geological conditions (D) of the islands' location in the Pacific [SEP] The Hawaiian Islands are situated about two thousand miles away from North America, right in the middle of the Pacific Ocean. Despite  the distance the islands actually make up the fiftieth state of the United States. The islands were all formed by volcanic eruption and on the largest of the islands, Hawaii, or the Big Island, there are still two active volcanoes, the Mauna Loa and the Kilauea, which still erupt every few years. On the Hawaiian islands the natives have a particularly strange way of indicating directions. They don't use the north, south, east and west system common to the rest of the world. They use the mauka andmakaisystem.Maukameans "mountain". The mountains in Hawaii are always at the center of the islands, as these are volcanic islands.Makaimeans "the sea". The islands are small and the system is simple. Wherever you want to visit, it can always be described in terms of where it lies in relation to the mountains and the sea. A typical conversation between a native Hawaiian and a tourist might go as follow. TOURIST: Excuse me! Could you tell me where the Sheraton Hotel is? HAWAIIAN: Well... Let me see! From here it's two blocks mauka, and then one blockmakai. You can't miss it!
03/12/2022 19:03:24 - INFO - __main__ - ['the islands were created by volcanic eruptions']
03/12/2022 19:03:24 - INFO - __main__ -  [race-high] Which of the following is true about Planet English according to the passage? (A) Planet English can support the students who study computer. (B) Planet English can help the students and their teachers interact. (C) Planet English makes it possible for students to communicate with speakers from around the world. (D) Planet English offers only a range of spoken communication for students. [SEP] Planet English is the world's leading interactive multimedia software package for English language teaching and learning. For Students Planet English uses the latest in multimedia and information technology to support students who wish to learn English for international communication. Planet English is an exciting, high-tech, interactive way of learning English. It contains more than 40 hours of video and audio recordings, over 2,500 0riginal graphics, 3,000 interactive screens and 80 different activity types including real time student voice recordings. For Teachers Planet English is more than just a computer program. It includes a package of resources to complement any Eng-lish language teaching programme. Teachers can easily integrate Planet English with classroom activities using the detailed Teacher's Manual  and Student Workbooks. Teachers can also manage the learning experience for students using the unique Planet English Courseware Management System (CMS). The CMS allows teachers to tailor courses to their syllabus  and to students' needs by "mapping" content for classes or individuals. Activities and exercises that are relevant to the center's syllabus are then delivered to students in the appropriate lesson, ensuring students "navigate" to the right area of the programme of each lesson. For Educational Managers Planet English is the world's leading Computer Assisted Language Learning (CALL) program. It allows English language teaching centers to enhance the educational quality of teaching programmes, improve learning outcomes and provide professional development for teaching staff. Implementing  Planet English allows English language teaching centers to maximize the benefits of computor hardware because it provides teachers and learners with an easy-to-use and highly productive CALL resource.
03/12/2022 19:03:24 - INFO - __main__ - ['Planet English makes it possible for students to communicate with speakers from around the world.']
03/12/2022 19:03:24 - INFO - __main__ -  [race-high] The Corcovado Hill is famous for  _  . (A) the samba parade (B) the beautiful view of the city (C) the statue of Jesus (D) the small church [SEP] On the coast of Brazil lies its most famous city Rio de Janeiro. Rio is a beautiful place to visit. It is full of exciting parades, historical architecture and natural beauty. The most famous time of year in Rio is the spring when the samba schools organize a giant parade of dance, music and costumes. Samba is a type of fast Brazilian music, which is perfect for dancing. The carnival lasts two days and over 20 samba schools perform, a total of 70,000 people in brightly coloured costumes, all dancing and singing! It is a magnificent spectacle and millions of people watch the carnival on television around the world. Rio de Janeiro is a religious city and it is full of churches. Some, like the former Imperial Church, were built over 400 years ago while others, like the Metropolitan Cathedral, are very modern. The Metropolitan Cathedral was built in 1976 and is so large that 20,000 people can stand inside at one time! Another important religious site in Rio is the Corcovado hill. On the top of the hill, along with a small church, stands a 30-metre high statue of Jesus with his arms stretched out. From this point you can see every part of the city.
03/12/2022 19:03:24 - INFO - __main__ - ['the beautiful view of the city']
03/12/2022 19:03:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 19:03:24 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:03:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 19:03:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:03:24 - INFO - __main__ - Printing 3 examples
03/12/2022 19:03:24 - INFO - __main__ -  [race-high] Which of the following is true according to the text? (A) Ieoh Ming Pei was born in America. (B) Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young. (C) Ieoh Ming Pei got a degree for architecture in 1948. (D) Ieoh Ming Pei got a Harvard undergraduate Degree in 1946. [SEP] On this vivid planet, it appears color1ful with many world famous buildings. Among these largest artificial articles in the world, many were designed by the same architect--Ieoh Ming Pei. Pei, the 1983 Laureate of the Pritzker, Architecture Prize, is a founding partner of I. M. Pei & Partners based in New York City. He was born in China in 1917, the son of a banker. He came to the United States in 1935 to study architecture at the Massachusetts Institute of Technology and the Harvard Graduate School of Design (M. Arch. 1946). From 1945 to 1948, Pei taught at Harvard. In 1948 he accepted the newly created post of director of Architecture at Webb & Knapp, Inc., and this association resulted in major architectural and planning projects in big cities. In 1958, he formed the partnership of I. M. Pei & Associates, which became I. M. Pei & Partners in 1966. The partnership received the 1968 Architectural Firm Award of The American Institute of Architects. Pei has designed over forty projects in the world, twenty of which have been award winners. His outstanding projects have included the East Building of the National Gallery of Art, Washington, D.C.; the John Fitzgerald Kennedy Library near Boston; the Fragrant Hill Hotel near Beijing, China. Pei is now a member of the National Council on the Arts, and before served on the National Council on the Humanities. He is a Fellow of the American Institute of Architects, a member of the Royal Institute of British Architects, and an elected member of the American Academy of Arts and Letters. He is a member of the Corporation of the Massachusetts Institute of Technology. As a student, he was awarded the MIT Traveling Fellowship, at Harvard. He later won a lot of honors. In 1982, the deans of the architectural schools of America chose I. M. Pei as the best designer of significant non-residential   structures.
03/12/2022 19:03:24 - INFO - __main__ - ['Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young.']
03/12/2022 19:03:24 - INFO - __main__ -  [race-high] According to the recently discovered evidence,   _  . (A) different regions of the brain have to work separately (B) people with phonagnosia are good at facial recognition (C) regions of the brain for personal recognition are connected (D) person recognition is controlled by the same part of the brain [SEP] James Cooke, of Islip,N.Y.,can't recognize other people. "I see eyes, nose, cheekbones, but no face," he said. "I've even passed by my son and daughter without recognizing them."  He is not the only one. Those with prosopagnosia, also known as face blindness, can see perfectly well, but their brains are unable to piece together the information needed to understand that a collection of features represents an individual's face. The condition is a neurological mystery, but new research has shed light on this strange disease.  Some scientists had believed that faces and voices, the two main ways people recognize one another, were processed separately by the brain. Indeed, a condition parallel to prosopagnosia, called  _ , similarly leaves a person unable to distinguish a familiar voice from an unfamiliar one. But by testing for these two conditions at the same time, researchers at the Max Planck Institute recently found evidence that face and voice recognition may be linked in a unique person-recognition system. The scientists observed the brain activity of 19 healthy volunteers as they were led through tasks that tested their ability to recognize both faces and voices. The researchers found that regions of the brain, associated with facial recognition, are directly linked to regions responsible for voice recognition.  This research helps explain why a person with prosopagnosia may still have difficulty determining who a person is even after he has begun to speak. The challenge for scientists is to find out where this system breaks down. Are these connections in the brain missing entirely, or are people unable to recognize faces and voices simply unable to use these links in some way?  It is unclear how many people have these conditions. Many don't even realize they have problems with facial or voice recognition. While some develop these difficulties after a brain injury, others develop it in childhood.
03/12/2022 19:03:24 - INFO - __main__ - ['regions of the brain for personal recognition are connected']
03/12/2022 19:03:24 - INFO - __main__ -  [race-high] By writing the passage, the author mainly wants to tell us    _   . (A) to think twice before we act (B) to be grateful for what we have (C) it is no use making complaints (D) saying "thank you" has many disadvantages [SEP] Throughout our childhood our parents taught us to say "thank you" and it has become a habit -- something we say automatically  , along with "please". And because of this we have forgotten just how important gratitude is and how essential it is in leading fulfilled   lives. Just for a minute, think of all the things you are grateful for, such as loving friends, good health, great holidays as well as everyday items such as a comfortable home, TV, and clean water. The list, in fact, could go on and on. Now focus on events that have made you angry -- it's raining, the car won't start, and a colleague   makes you annoyed. You start to feel unhappy, and that is something that certainly does not make you feel good! In fact, we have the ability to choose how we feel -- it's just our perception   of how things are. But for most of us, it just doesn't seem easy. Let me give you an example: it's a rainy day, and immediately most people will start to complain, telling everyone who will listen what a miserable day it is, with the result that they end up feeling miserable themselves. But look at it another way and despite wet clothes and hair, both will dry perfectly well and no lasting harm has been done. And in addition to this, because of rain, we not only live in a green and beautiful landscape, we are also able to grow a lot of fruit and vegetables. There really is no obvious reason for feeling miserable -- in fact there is a great deal to be grateful for. It all depends on what we think about things. Realize what a difference having gratitude can make to your life. That's why gratitude is so special -- use it to feel good!
03/12/2022 19:03:24 - INFO - __main__ - ['to be grateful for what we have']
03/12/2022 19:03:24 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:03:24 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:03:24 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 19:03:25 - INFO - __main__ - Global step 600 Train loss 0.384370 ACC 0.0 on epoch=299
03/12/2022 19:03:25 - INFO - __main__ - save last model!
03/12/2022 19:03:32 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 19:03:33 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 19:03:33 - INFO - __main__ - Printing 3 examples
03/12/2022 19:03:33 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:03:33 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 19:03:33 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:03:33 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 19:03:33 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:03:33 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 19:03:33 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:03:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 19:03:37 - INFO - __main__ - Starting training!
03/12/2022 19:03:38 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:03:42 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 19:07:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_42_0.0005_8_predictions.txt
03/12/2022 19:07:31 - INFO - __main__ - ACC on test data: 0.0768
03/12/2022 19:07:31 - INFO - __main__ - prefix=race-high_32_42, lr=0.0005, bsz=8, dev_performance=0.09375, test_performance=0.07678933642422486
03/12/2022 19:07:31 - INFO - __main__ - Running ... prefix=race-high_32_42, lr=0.0003, bsz=8 ...
03/12/2022 19:07:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:07:32 - INFO - __main__ - Printing 3 examples
03/12/2022 19:07:32 - INFO - __main__ -  [race-high] In Hawaii the mountains are in the center of the islands because   _  . (A) of the weather (B) the islands were created by volcanic eruptions (C) of the geological conditions (D) of the islands' location in the Pacific [SEP] The Hawaiian Islands are situated about two thousand miles away from North America, right in the middle of the Pacific Ocean. Despite  the distance the islands actually make up the fiftieth state of the United States. The islands were all formed by volcanic eruption and on the largest of the islands, Hawaii, or the Big Island, there are still two active volcanoes, the Mauna Loa and the Kilauea, which still erupt every few years. On the Hawaiian islands the natives have a particularly strange way of indicating directions. They don't use the north, south, east and west system common to the rest of the world. They use the mauka andmakaisystem.Maukameans "mountain". The mountains in Hawaii are always at the center of the islands, as these are volcanic islands.Makaimeans "the sea". The islands are small and the system is simple. Wherever you want to visit, it can always be described in terms of where it lies in relation to the mountains and the sea. A typical conversation between a native Hawaiian and a tourist might go as follow. TOURIST: Excuse me! Could you tell me where the Sheraton Hotel is? HAWAIIAN: Well... Let me see! From here it's two blocks mauka, and then one blockmakai. You can't miss it!
03/12/2022 19:07:32 - INFO - __main__ - ['the islands were created by volcanic eruptions']
03/12/2022 19:07:32 - INFO - __main__ -  [race-high] Which of the following is true about Planet English according to the passage? (A) Planet English can support the students who study computer. (B) Planet English can help the students and their teachers interact. (C) Planet English makes it possible for students to communicate with speakers from around the world. (D) Planet English offers only a range of spoken communication for students. [SEP] Planet English is the world's leading interactive multimedia software package for English language teaching and learning. For Students Planet English uses the latest in multimedia and information technology to support students who wish to learn English for international communication. Planet English is an exciting, high-tech, interactive way of learning English. It contains more than 40 hours of video and audio recordings, over 2,500 0riginal graphics, 3,000 interactive screens and 80 different activity types including real time student voice recordings. For Teachers Planet English is more than just a computer program. It includes a package of resources to complement any Eng-lish language teaching programme. Teachers can easily integrate Planet English with classroom activities using the detailed Teacher's Manual  and Student Workbooks. Teachers can also manage the learning experience for students using the unique Planet English Courseware Management System (CMS). The CMS allows teachers to tailor courses to their syllabus  and to students' needs by "mapping" content for classes or individuals. Activities and exercises that are relevant to the center's syllabus are then delivered to students in the appropriate lesson, ensuring students "navigate" to the right area of the programme of each lesson. For Educational Managers Planet English is the world's leading Computer Assisted Language Learning (CALL) program. It allows English language teaching centers to enhance the educational quality of teaching programmes, improve learning outcomes and provide professional development for teaching staff. Implementing  Planet English allows English language teaching centers to maximize the benefits of computor hardware because it provides teachers and learners with an easy-to-use and highly productive CALL resource.
03/12/2022 19:07:32 - INFO - __main__ - ['Planet English makes it possible for students to communicate with speakers from around the world.']
03/12/2022 19:07:32 - INFO - __main__ -  [race-high] The Corcovado Hill is famous for  _  . (A) the samba parade (B) the beautiful view of the city (C) the statue of Jesus (D) the small church [SEP] On the coast of Brazil lies its most famous city Rio de Janeiro. Rio is a beautiful place to visit. It is full of exciting parades, historical architecture and natural beauty. The most famous time of year in Rio is the spring when the samba schools organize a giant parade of dance, music and costumes. Samba is a type of fast Brazilian music, which is perfect for dancing. The carnival lasts two days and over 20 samba schools perform, a total of 70,000 people in brightly coloured costumes, all dancing and singing! It is a magnificent spectacle and millions of people watch the carnival on television around the world. Rio de Janeiro is a religious city and it is full of churches. Some, like the former Imperial Church, were built over 400 years ago while others, like the Metropolitan Cathedral, are very modern. The Metropolitan Cathedral was built in 1976 and is so large that 20,000 people can stand inside at one time! Another important religious site in Rio is the Corcovado hill. On the top of the hill, along with a small church, stands a 30-metre high statue of Jesus with his arms stretched out. From this point you can see every part of the city.
03/12/2022 19:07:32 - INFO - __main__ - ['the beautiful view of the city']
03/12/2022 19:07:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 19:07:32 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:07:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 19:07:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:07:32 - INFO - __main__ - Printing 3 examples
03/12/2022 19:07:32 - INFO - __main__ -  [race-high] Which of the following is true according to the text? (A) Ieoh Ming Pei was born in America. (B) Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young. (C) Ieoh Ming Pei got a degree for architecture in 1948. (D) Ieoh Ming Pei got a Harvard undergraduate Degree in 1946. [SEP] On this vivid planet, it appears color1ful with many world famous buildings. Among these largest artificial articles in the world, many were designed by the same architect--Ieoh Ming Pei. Pei, the 1983 Laureate of the Pritzker, Architecture Prize, is a founding partner of I. M. Pei & Partners based in New York City. He was born in China in 1917, the son of a banker. He came to the United States in 1935 to study architecture at the Massachusetts Institute of Technology and the Harvard Graduate School of Design (M. Arch. 1946). From 1945 to 1948, Pei taught at Harvard. In 1948 he accepted the newly created post of director of Architecture at Webb & Knapp, Inc., and this association resulted in major architectural and planning projects in big cities. In 1958, he formed the partnership of I. M. Pei & Associates, which became I. M. Pei & Partners in 1966. The partnership received the 1968 Architectural Firm Award of The American Institute of Architects. Pei has designed over forty projects in the world, twenty of which have been award winners. His outstanding projects have included the East Building of the National Gallery of Art, Washington, D.C.; the John Fitzgerald Kennedy Library near Boston; the Fragrant Hill Hotel near Beijing, China. Pei is now a member of the National Council on the Arts, and before served on the National Council on the Humanities. He is a Fellow of the American Institute of Architects, a member of the Royal Institute of British Architects, and an elected member of the American Academy of Arts and Letters. He is a member of the Corporation of the Massachusetts Institute of Technology. As a student, he was awarded the MIT Traveling Fellowship, at Harvard. He later won a lot of honors. In 1982, the deans of the architectural schools of America chose I. M. Pei as the best designer of significant non-residential   structures.
03/12/2022 19:07:32 - INFO - __main__ - ['Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young.']
03/12/2022 19:07:32 - INFO - __main__ -  [race-high] According to the recently discovered evidence,   _  . (A) different regions of the brain have to work separately (B) people with phonagnosia are good at facial recognition (C) regions of the brain for personal recognition are connected (D) person recognition is controlled by the same part of the brain [SEP] James Cooke, of Islip,N.Y.,can't recognize other people. "I see eyes, nose, cheekbones, but no face," he said. "I've even passed by my son and daughter without recognizing them."  He is not the only one. Those with prosopagnosia, also known as face blindness, can see perfectly well, but their brains are unable to piece together the information needed to understand that a collection of features represents an individual's face. The condition is a neurological mystery, but new research has shed light on this strange disease.  Some scientists had believed that faces and voices, the two main ways people recognize one another, were processed separately by the brain. Indeed, a condition parallel to prosopagnosia, called  _ , similarly leaves a person unable to distinguish a familiar voice from an unfamiliar one. But by testing for these two conditions at the same time, researchers at the Max Planck Institute recently found evidence that face and voice recognition may be linked in a unique person-recognition system. The scientists observed the brain activity of 19 healthy volunteers as they were led through tasks that tested their ability to recognize both faces and voices. The researchers found that regions of the brain, associated with facial recognition, are directly linked to regions responsible for voice recognition.  This research helps explain why a person with prosopagnosia may still have difficulty determining who a person is even after he has begun to speak. The challenge for scientists is to find out where this system breaks down. Are these connections in the brain missing entirely, or are people unable to recognize faces and voices simply unable to use these links in some way?  It is unclear how many people have these conditions. Many don't even realize they have problems with facial or voice recognition. While some develop these difficulties after a brain injury, others develop it in childhood.
03/12/2022 19:07:32 - INFO - __main__ - ['regions of the brain for personal recognition are connected']
03/12/2022 19:07:32 - INFO - __main__ -  [race-high] By writing the passage, the author mainly wants to tell us    _   . (A) to think twice before we act (B) to be grateful for what we have (C) it is no use making complaints (D) saying "thank you" has many disadvantages [SEP] Throughout our childhood our parents taught us to say "thank you" and it has become a habit -- something we say automatically  , along with "please". And because of this we have forgotten just how important gratitude is and how essential it is in leading fulfilled   lives. Just for a minute, think of all the things you are grateful for, such as loving friends, good health, great holidays as well as everyday items such as a comfortable home, TV, and clean water. The list, in fact, could go on and on. Now focus on events that have made you angry -- it's raining, the car won't start, and a colleague   makes you annoyed. You start to feel unhappy, and that is something that certainly does not make you feel good! In fact, we have the ability to choose how we feel -- it's just our perception   of how things are. But for most of us, it just doesn't seem easy. Let me give you an example: it's a rainy day, and immediately most people will start to complain, telling everyone who will listen what a miserable day it is, with the result that they end up feeling miserable themselves. But look at it another way and despite wet clothes and hair, both will dry perfectly well and no lasting harm has been done. And in addition to this, because of rain, we not only live in a green and beautiful landscape, we are also able to grow a lot of fruit and vegetables. There really is no obvious reason for feeling miserable -- in fact there is a great deal to be grateful for. It all depends on what we think about things. Realize what a difference having gratitude can make to your life. That's why gratitude is so special -- use it to feel good!
03/12/2022 19:07:32 - INFO - __main__ - ['to be grateful for what we have']
03/12/2022 19:07:32 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:07:32 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:07:32 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 19:07:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 19:07:44 - INFO - __main__ - Starting training!
03/12/2022 19:07:50 - INFO - __main__ - Step 10 Global step 10 Train loss 19.208309 on epoch=4
03/12/2022 19:07:56 - INFO - __main__ - Step 20 Global step 20 Train loss 17.508902 on epoch=9
03/12/2022 19:08:02 - INFO - __main__ - Step 30 Global step 30 Train loss 11.718946 on epoch=14
03/12/2022 19:08:08 - INFO - __main__ - Step 40 Global step 40 Train loss 8.617293 on epoch=19
03/12/2022 19:08:14 - INFO - __main__ - Step 50 Global step 50 Train loss 5.283593 on epoch=24
03/12/2022 19:08:16 - INFO - __main__ - Global step 50 Train loss 12.467408 ACC 0.0 on epoch=24
03/12/2022 19:08:23 - INFO - __main__ - Step 60 Global step 60 Train loss 4.134990 on epoch=29
03/12/2022 19:08:29 - INFO - __main__ - Step 70 Global step 70 Train loss 3.257141 on epoch=34
03/12/2022 19:08:35 - INFO - __main__ - Step 80 Global step 80 Train loss 2.814983 on epoch=39
03/12/2022 19:08:42 - INFO - __main__ - Step 90 Global step 90 Train loss 2.615700 on epoch=44
03/12/2022 19:08:48 - INFO - __main__ - Step 100 Global step 100 Train loss 2.308123 on epoch=49
03/12/2022 19:08:50 - INFO - __main__ - Global step 100 Train loss 3.026187 ACC 0.0 on epoch=49
03/12/2022 19:08:56 - INFO - __main__ - Step 110 Global step 110 Train loss 2.100266 on epoch=54
03/12/2022 19:09:02 - INFO - __main__ - Step 120 Global step 120 Train loss 1.986515 on epoch=59
03/12/2022 19:09:08 - INFO - __main__ - Step 130 Global step 130 Train loss 1.823448 on epoch=64
03/12/2022 19:09:14 - INFO - __main__ - Step 140 Global step 140 Train loss 1.555452 on epoch=69
03/12/2022 19:09:20 - INFO - __main__ - Step 150 Global step 150 Train loss 1.335857 on epoch=74
03/12/2022 19:09:23 - INFO - __main__ - Global step 150 Train loss 1.760307 ACC 0.0 on epoch=74
03/12/2022 19:09:29 - INFO - __main__ - Step 160 Global step 160 Train loss 1.345818 on epoch=79
03/12/2022 19:09:35 - INFO - __main__ - Step 170 Global step 170 Train loss 1.225896 on epoch=84
03/12/2022 19:09:41 - INFO - __main__ - Step 180 Global step 180 Train loss 1.109152 on epoch=89
03/12/2022 19:09:47 - INFO - __main__ - Step 190 Global step 190 Train loss 1.185175 on epoch=94
03/12/2022 19:09:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.907475 on epoch=99
03/12/2022 19:09:56 - INFO - __main__ - Global step 200 Train loss 1.154703 ACC 0.0 on epoch=99
03/12/2022 19:10:02 - INFO - __main__ - Step 210 Global step 210 Train loss 1.039053 on epoch=104
03/12/2022 19:10:08 - INFO - __main__ - Step 220 Global step 220 Train loss 1.199065 on epoch=109
03/12/2022 19:10:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.983820 on epoch=114
03/12/2022 19:10:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.941238 on epoch=119
03/12/2022 19:10:27 - INFO - __main__ - Step 250 Global step 250 Train loss 1.068880 on epoch=124
03/12/2022 19:10:29 - INFO - __main__ - Global step 250 Train loss 1.046411 ACC 0.03125 on epoch=124
03/12/2022 19:10:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.918855 on epoch=129
03/12/2022 19:10:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.831235 on epoch=134
03/12/2022 19:10:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.812194 on epoch=139
03/12/2022 19:10:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.888365 on epoch=144
03/12/2022 19:11:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.801374 on epoch=149
03/12/2022 19:11:02 - INFO - __main__ - Global step 300 Train loss 0.850405 ACC 0.03125 on epoch=149
03/12/2022 19:11:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.949464 on epoch=154
03/12/2022 19:11:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.806601 on epoch=159
03/12/2022 19:11:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.760821 on epoch=164
03/12/2022 19:11:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.757532 on epoch=169
03/12/2022 19:11:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.655525 on epoch=174
03/12/2022 19:11:35 - INFO - __main__ - Global step 350 Train loss 0.785989 ACC 0.03125 on epoch=174
03/12/2022 19:11:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.641457 on epoch=179
03/12/2022 19:11:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.720978 on epoch=184
03/12/2022 19:11:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.721319 on epoch=189
03/12/2022 19:11:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.688666 on epoch=194
03/12/2022 19:12:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.681328 on epoch=199
03/12/2022 19:12:07 - INFO - __main__ - Global step 400 Train loss 0.690750 ACC 0.0 on epoch=199
03/12/2022 19:12:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.645537 on epoch=204
03/12/2022 19:12:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.615740 on epoch=209
03/12/2022 19:12:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.609314 on epoch=214
03/12/2022 19:12:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.542630 on epoch=219
03/12/2022 19:12:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.636192 on epoch=224
03/12/2022 19:12:40 - INFO - __main__ - Global step 450 Train loss 0.609883 ACC 0.0 on epoch=224
03/12/2022 19:12:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.610239 on epoch=229
03/12/2022 19:12:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.585003 on epoch=234
03/12/2022 19:12:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.549288 on epoch=239
03/12/2022 19:13:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.515133 on epoch=244
03/12/2022 19:13:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.531609 on epoch=249
03/12/2022 19:13:13 - INFO - __main__ - Global step 500 Train loss 0.558255 ACC 0.0 on epoch=249
03/12/2022 19:13:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.544167 on epoch=254
03/12/2022 19:13:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.517446 on epoch=259
03/12/2022 19:13:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.501708 on epoch=264
03/12/2022 19:13:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.510702 on epoch=269
03/12/2022 19:13:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.562025 on epoch=274
03/12/2022 19:13:46 - INFO - __main__ - Global step 550 Train loss 0.527210 ACC 0.0 on epoch=274
03/12/2022 19:13:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.485831 on epoch=279
03/12/2022 19:13:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.507783 on epoch=284
03/12/2022 19:14:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.471005 on epoch=289
03/12/2022 19:14:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.469786 on epoch=294
03/12/2022 19:14:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.482477 on epoch=299
03/12/2022 19:14:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:14:19 - INFO - __main__ - Printing 3 examples
03/12/2022 19:14:19 - INFO - __main__ -  [race-high] In Hawaii the mountains are in the center of the islands because   _  . (A) of the weather (B) the islands were created by volcanic eruptions (C) of the geological conditions (D) of the islands' location in the Pacific [SEP] The Hawaiian Islands are situated about two thousand miles away from North America, right in the middle of the Pacific Ocean. Despite  the distance the islands actually make up the fiftieth state of the United States. The islands were all formed by volcanic eruption and on the largest of the islands, Hawaii, or the Big Island, there are still two active volcanoes, the Mauna Loa and the Kilauea, which still erupt every few years. On the Hawaiian islands the natives have a particularly strange way of indicating directions. They don't use the north, south, east and west system common to the rest of the world. They use the mauka andmakaisystem.Maukameans "mountain". The mountains in Hawaii are always at the center of the islands, as these are volcanic islands.Makaimeans "the sea". The islands are small and the system is simple. Wherever you want to visit, it can always be described in terms of where it lies in relation to the mountains and the sea. A typical conversation between a native Hawaiian and a tourist might go as follow. TOURIST: Excuse me! Could you tell me where the Sheraton Hotel is? HAWAIIAN: Well... Let me see! From here it's two blocks mauka, and then one blockmakai. You can't miss it!
03/12/2022 19:14:19 - INFO - __main__ - ['the islands were created by volcanic eruptions']
03/12/2022 19:14:19 - INFO - __main__ -  [race-high] Which of the following is true about Planet English according to the passage? (A) Planet English can support the students who study computer. (B) Planet English can help the students and their teachers interact. (C) Planet English makes it possible for students to communicate with speakers from around the world. (D) Planet English offers only a range of spoken communication for students. [SEP] Planet English is the world's leading interactive multimedia software package for English language teaching and learning. For Students Planet English uses the latest in multimedia and information technology to support students who wish to learn English for international communication. Planet English is an exciting, high-tech, interactive way of learning English. It contains more than 40 hours of video and audio recordings, over 2,500 0riginal graphics, 3,000 interactive screens and 80 different activity types including real time student voice recordings. For Teachers Planet English is more than just a computer program. It includes a package of resources to complement any Eng-lish language teaching programme. Teachers can easily integrate Planet English with classroom activities using the detailed Teacher's Manual  and Student Workbooks. Teachers can also manage the learning experience for students using the unique Planet English Courseware Management System (CMS). The CMS allows teachers to tailor courses to their syllabus  and to students' needs by "mapping" content for classes or individuals. Activities and exercises that are relevant to the center's syllabus are then delivered to students in the appropriate lesson, ensuring students "navigate" to the right area of the programme of each lesson. For Educational Managers Planet English is the world's leading Computer Assisted Language Learning (CALL) program. It allows English language teaching centers to enhance the educational quality of teaching programmes, improve learning outcomes and provide professional development for teaching staff. Implementing  Planet English allows English language teaching centers to maximize the benefits of computor hardware because it provides teachers and learners with an easy-to-use and highly productive CALL resource.
03/12/2022 19:14:19 - INFO - __main__ - ['Planet English makes it possible for students to communicate with speakers from around the world.']
03/12/2022 19:14:19 - INFO - __main__ -  [race-high] The Corcovado Hill is famous for  _  . (A) the samba parade (B) the beautiful view of the city (C) the statue of Jesus (D) the small church [SEP] On the coast of Brazil lies its most famous city Rio de Janeiro. Rio is a beautiful place to visit. It is full of exciting parades, historical architecture and natural beauty. The most famous time of year in Rio is the spring when the samba schools organize a giant parade of dance, music and costumes. Samba is a type of fast Brazilian music, which is perfect for dancing. The carnival lasts two days and over 20 samba schools perform, a total of 70,000 people in brightly coloured costumes, all dancing and singing! It is a magnificent spectacle and millions of people watch the carnival on television around the world. Rio de Janeiro is a religious city and it is full of churches. Some, like the former Imperial Church, were built over 400 years ago while others, like the Metropolitan Cathedral, are very modern. The Metropolitan Cathedral was built in 1976 and is so large that 20,000 people can stand inside at one time! Another important religious site in Rio is the Corcovado hill. On the top of the hill, along with a small church, stands a 30-metre high statue of Jesus with his arms stretched out. From this point you can see every part of the city.
03/12/2022 19:14:19 - INFO - __main__ - ['the beautiful view of the city']
03/12/2022 19:14:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 19:14:19 - INFO - __main__ - Global step 600 Train loss 0.483376 ACC 0.0 on epoch=299
03/12/2022 19:14:19 - INFO - __main__ - save last model!
03/12/2022 19:14:19 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:14:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 19:14:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:14:19 - INFO - __main__ - Printing 3 examples
03/12/2022 19:14:19 - INFO - __main__ -  [race-high] Which of the following is true according to the text? (A) Ieoh Ming Pei was born in America. (B) Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young. (C) Ieoh Ming Pei got a degree for architecture in 1948. (D) Ieoh Ming Pei got a Harvard undergraduate Degree in 1946. [SEP] On this vivid planet, it appears color1ful with many world famous buildings. Among these largest artificial articles in the world, many were designed by the same architect--Ieoh Ming Pei. Pei, the 1983 Laureate of the Pritzker, Architecture Prize, is a founding partner of I. M. Pei & Partners based in New York City. He was born in China in 1917, the son of a banker. He came to the United States in 1935 to study architecture at the Massachusetts Institute of Technology and the Harvard Graduate School of Design (M. Arch. 1946). From 1945 to 1948, Pei taught at Harvard. In 1948 he accepted the newly created post of director of Architecture at Webb & Knapp, Inc., and this association resulted in major architectural and planning projects in big cities. In 1958, he formed the partnership of I. M. Pei & Associates, which became I. M. Pei & Partners in 1966. The partnership received the 1968 Architectural Firm Award of The American Institute of Architects. Pei has designed over forty projects in the world, twenty of which have been award winners. His outstanding projects have included the East Building of the National Gallery of Art, Washington, D.C.; the John Fitzgerald Kennedy Library near Boston; the Fragrant Hill Hotel near Beijing, China. Pei is now a member of the National Council on the Arts, and before served on the National Council on the Humanities. He is a Fellow of the American Institute of Architects, a member of the Royal Institute of British Architects, and an elected member of the American Academy of Arts and Letters. He is a member of the Corporation of the Massachusetts Institute of Technology. As a student, he was awarded the MIT Traveling Fellowship, at Harvard. He later won a lot of honors. In 1982, the deans of the architectural schools of America chose I. M. Pei as the best designer of significant non-residential   structures.
03/12/2022 19:14:19 - INFO - __main__ - ['Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young.']
03/12/2022 19:14:19 - INFO - __main__ -  [race-high] According to the recently discovered evidence,   _  . (A) different regions of the brain have to work separately (B) people with phonagnosia are good at facial recognition (C) regions of the brain for personal recognition are connected (D) person recognition is controlled by the same part of the brain [SEP] James Cooke, of Islip,N.Y.,can't recognize other people. "I see eyes, nose, cheekbones, but no face," he said. "I've even passed by my son and daughter without recognizing them."  He is not the only one. Those with prosopagnosia, also known as face blindness, can see perfectly well, but their brains are unable to piece together the information needed to understand that a collection of features represents an individual's face. The condition is a neurological mystery, but new research has shed light on this strange disease.  Some scientists had believed that faces and voices, the two main ways people recognize one another, were processed separately by the brain. Indeed, a condition parallel to prosopagnosia, called  _ , similarly leaves a person unable to distinguish a familiar voice from an unfamiliar one. But by testing for these two conditions at the same time, researchers at the Max Planck Institute recently found evidence that face and voice recognition may be linked in a unique person-recognition system. The scientists observed the brain activity of 19 healthy volunteers as they were led through tasks that tested their ability to recognize both faces and voices. The researchers found that regions of the brain, associated with facial recognition, are directly linked to regions responsible for voice recognition.  This research helps explain why a person with prosopagnosia may still have difficulty determining who a person is even after he has begun to speak. The challenge for scientists is to find out where this system breaks down. Are these connections in the brain missing entirely, or are people unable to recognize faces and voices simply unable to use these links in some way?  It is unclear how many people have these conditions. Many don't even realize they have problems with facial or voice recognition. While some develop these difficulties after a brain injury, others develop it in childhood.
03/12/2022 19:14:19 - INFO - __main__ - ['regions of the brain for personal recognition are connected']
03/12/2022 19:14:19 - INFO - __main__ -  [race-high] By writing the passage, the author mainly wants to tell us    _   . (A) to think twice before we act (B) to be grateful for what we have (C) it is no use making complaints (D) saying "thank you" has many disadvantages [SEP] Throughout our childhood our parents taught us to say "thank you" and it has become a habit -- something we say automatically  , along with "please". And because of this we have forgotten just how important gratitude is and how essential it is in leading fulfilled   lives. Just for a minute, think of all the things you are grateful for, such as loving friends, good health, great holidays as well as everyday items such as a comfortable home, TV, and clean water. The list, in fact, could go on and on. Now focus on events that have made you angry -- it's raining, the car won't start, and a colleague   makes you annoyed. You start to feel unhappy, and that is something that certainly does not make you feel good! In fact, we have the ability to choose how we feel -- it's just our perception   of how things are. But for most of us, it just doesn't seem easy. Let me give you an example: it's a rainy day, and immediately most people will start to complain, telling everyone who will listen what a miserable day it is, with the result that they end up feeling miserable themselves. But look at it another way and despite wet clothes and hair, both will dry perfectly well and no lasting harm has been done. And in addition to this, because of rain, we not only live in a green and beautiful landscape, we are also able to grow a lot of fruit and vegetables. There really is no obvious reason for feeling miserable -- in fact there is a great deal to be grateful for. It all depends on what we think about things. Realize what a difference having gratitude can make to your life. That's why gratitude is so special -- use it to feel good!
03/12/2022 19:14:19 - INFO - __main__ - ['to be grateful for what we have']
03/12/2022 19:14:19 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:14:19 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:14:19 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 19:14:26 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 19:14:26 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 19:14:26 - INFO - __main__ - Printing 3 examples
03/12/2022 19:14:26 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:14:26 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 19:14:26 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:14:26 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 19:14:26 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:14:26 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 19:14:26 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:14:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 19:14:31 - INFO - __main__ - Starting training!
03/12/2022 19:14:32 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:14:36 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 19:18:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_42_0.0003_8_predictions.txt
03/12/2022 19:18:25 - INFO - __main__ - ACC on test data: 0.0136
03/12/2022 19:18:26 - INFO - __main__ - prefix=race-high_32_42, lr=0.0003, bsz=8, dev_performance=0.03125, test_performance=0.013619240799768183
03/12/2022 19:18:26 - INFO - __main__ - Running ... prefix=race-high_32_42, lr=0.0002, bsz=8 ...
03/12/2022 19:18:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:18:26 - INFO - __main__ - Printing 3 examples
03/12/2022 19:18:26 - INFO - __main__ -  [race-high] In Hawaii the mountains are in the center of the islands because   _  . (A) of the weather (B) the islands were created by volcanic eruptions (C) of the geological conditions (D) of the islands' location in the Pacific [SEP] The Hawaiian Islands are situated about two thousand miles away from North America, right in the middle of the Pacific Ocean. Despite  the distance the islands actually make up the fiftieth state of the United States. The islands were all formed by volcanic eruption and on the largest of the islands, Hawaii, or the Big Island, there are still two active volcanoes, the Mauna Loa and the Kilauea, which still erupt every few years. On the Hawaiian islands the natives have a particularly strange way of indicating directions. They don't use the north, south, east and west system common to the rest of the world. They use the mauka andmakaisystem.Maukameans "mountain". The mountains in Hawaii are always at the center of the islands, as these are volcanic islands.Makaimeans "the sea". The islands are small and the system is simple. Wherever you want to visit, it can always be described in terms of where it lies in relation to the mountains and the sea. A typical conversation between a native Hawaiian and a tourist might go as follow. TOURIST: Excuse me! Could you tell me where the Sheraton Hotel is? HAWAIIAN: Well... Let me see! From here it's two blocks mauka, and then one blockmakai. You can't miss it!
03/12/2022 19:18:26 - INFO - __main__ - ['the islands were created by volcanic eruptions']
03/12/2022 19:18:26 - INFO - __main__ -  [race-high] Which of the following is true about Planet English according to the passage? (A) Planet English can support the students who study computer. (B) Planet English can help the students and their teachers interact. (C) Planet English makes it possible for students to communicate with speakers from around the world. (D) Planet English offers only a range of spoken communication for students. [SEP] Planet English is the world's leading interactive multimedia software package for English language teaching and learning. For Students Planet English uses the latest in multimedia and information technology to support students who wish to learn English for international communication. Planet English is an exciting, high-tech, interactive way of learning English. It contains more than 40 hours of video and audio recordings, over 2,500 0riginal graphics, 3,000 interactive screens and 80 different activity types including real time student voice recordings. For Teachers Planet English is more than just a computer program. It includes a package of resources to complement any Eng-lish language teaching programme. Teachers can easily integrate Planet English with classroom activities using the detailed Teacher's Manual  and Student Workbooks. Teachers can also manage the learning experience for students using the unique Planet English Courseware Management System (CMS). The CMS allows teachers to tailor courses to their syllabus  and to students' needs by "mapping" content for classes or individuals. Activities and exercises that are relevant to the center's syllabus are then delivered to students in the appropriate lesson, ensuring students "navigate" to the right area of the programme of each lesson. For Educational Managers Planet English is the world's leading Computer Assisted Language Learning (CALL) program. It allows English language teaching centers to enhance the educational quality of teaching programmes, improve learning outcomes and provide professional development for teaching staff. Implementing  Planet English allows English language teaching centers to maximize the benefits of computor hardware because it provides teachers and learners with an easy-to-use and highly productive CALL resource.
03/12/2022 19:18:26 - INFO - __main__ - ['Planet English makes it possible for students to communicate with speakers from around the world.']
03/12/2022 19:18:26 - INFO - __main__ -  [race-high] The Corcovado Hill is famous for  _  . (A) the samba parade (B) the beautiful view of the city (C) the statue of Jesus (D) the small church [SEP] On the coast of Brazil lies its most famous city Rio de Janeiro. Rio is a beautiful place to visit. It is full of exciting parades, historical architecture and natural beauty. The most famous time of year in Rio is the spring when the samba schools organize a giant parade of dance, music and costumes. Samba is a type of fast Brazilian music, which is perfect for dancing. The carnival lasts two days and over 20 samba schools perform, a total of 70,000 people in brightly coloured costumes, all dancing and singing! It is a magnificent spectacle and millions of people watch the carnival on television around the world. Rio de Janeiro is a religious city and it is full of churches. Some, like the former Imperial Church, were built over 400 years ago while others, like the Metropolitan Cathedral, are very modern. The Metropolitan Cathedral was built in 1976 and is so large that 20,000 people can stand inside at one time! Another important religious site in Rio is the Corcovado hill. On the top of the hill, along with a small church, stands a 30-metre high statue of Jesus with his arms stretched out. From this point you can see every part of the city.
03/12/2022 19:18:26 - INFO - __main__ - ['the beautiful view of the city']
03/12/2022 19:18:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 19:18:27 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:18:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 19:18:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:18:27 - INFO - __main__ - Printing 3 examples
03/12/2022 19:18:27 - INFO - __main__ -  [race-high] Which of the following is true according to the text? (A) Ieoh Ming Pei was born in America. (B) Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young. (C) Ieoh Ming Pei got a degree for architecture in 1948. (D) Ieoh Ming Pei got a Harvard undergraduate Degree in 1946. [SEP] On this vivid planet, it appears color1ful with many world famous buildings. Among these largest artificial articles in the world, many were designed by the same architect--Ieoh Ming Pei. Pei, the 1983 Laureate of the Pritzker, Architecture Prize, is a founding partner of I. M. Pei & Partners based in New York City. He was born in China in 1917, the son of a banker. He came to the United States in 1935 to study architecture at the Massachusetts Institute of Technology and the Harvard Graduate School of Design (M. Arch. 1946). From 1945 to 1948, Pei taught at Harvard. In 1948 he accepted the newly created post of director of Architecture at Webb & Knapp, Inc., and this association resulted in major architectural and planning projects in big cities. In 1958, he formed the partnership of I. M. Pei & Associates, which became I. M. Pei & Partners in 1966. The partnership received the 1968 Architectural Firm Award of The American Institute of Architects. Pei has designed over forty projects in the world, twenty of which have been award winners. His outstanding projects have included the East Building of the National Gallery of Art, Washington, D.C.; the John Fitzgerald Kennedy Library near Boston; the Fragrant Hill Hotel near Beijing, China. Pei is now a member of the National Council on the Arts, and before served on the National Council on the Humanities. He is a Fellow of the American Institute of Architects, a member of the Royal Institute of British Architects, and an elected member of the American Academy of Arts and Letters. He is a member of the Corporation of the Massachusetts Institute of Technology. As a student, he was awarded the MIT Traveling Fellowship, at Harvard. He later won a lot of honors. In 1982, the deans of the architectural schools of America chose I. M. Pei as the best designer of significant non-residential   structures.
03/12/2022 19:18:27 - INFO - __main__ - ['Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young.']
03/12/2022 19:18:27 - INFO - __main__ -  [race-high] According to the recently discovered evidence,   _  . (A) different regions of the brain have to work separately (B) people with phonagnosia are good at facial recognition (C) regions of the brain for personal recognition are connected (D) person recognition is controlled by the same part of the brain [SEP] James Cooke, of Islip,N.Y.,can't recognize other people. "I see eyes, nose, cheekbones, but no face," he said. "I've even passed by my son and daughter without recognizing them."  He is not the only one. Those with prosopagnosia, also known as face blindness, can see perfectly well, but their brains are unable to piece together the information needed to understand that a collection of features represents an individual's face. The condition is a neurological mystery, but new research has shed light on this strange disease.  Some scientists had believed that faces and voices, the two main ways people recognize one another, were processed separately by the brain. Indeed, a condition parallel to prosopagnosia, called  _ , similarly leaves a person unable to distinguish a familiar voice from an unfamiliar one. But by testing for these two conditions at the same time, researchers at the Max Planck Institute recently found evidence that face and voice recognition may be linked in a unique person-recognition system. The scientists observed the brain activity of 19 healthy volunteers as they were led through tasks that tested their ability to recognize both faces and voices. The researchers found that regions of the brain, associated with facial recognition, are directly linked to regions responsible for voice recognition.  This research helps explain why a person with prosopagnosia may still have difficulty determining who a person is even after he has begun to speak. The challenge for scientists is to find out where this system breaks down. Are these connections in the brain missing entirely, or are people unable to recognize faces and voices simply unable to use these links in some way?  It is unclear how many people have these conditions. Many don't even realize they have problems with facial or voice recognition. While some develop these difficulties after a brain injury, others develop it in childhood.
03/12/2022 19:18:27 - INFO - __main__ - ['regions of the brain for personal recognition are connected']
03/12/2022 19:18:27 - INFO - __main__ -  [race-high] By writing the passage, the author mainly wants to tell us    _   . (A) to think twice before we act (B) to be grateful for what we have (C) it is no use making complaints (D) saying "thank you" has many disadvantages [SEP] Throughout our childhood our parents taught us to say "thank you" and it has become a habit -- something we say automatically  , along with "please". And because of this we have forgotten just how important gratitude is and how essential it is in leading fulfilled   lives. Just for a minute, think of all the things you are grateful for, such as loving friends, good health, great holidays as well as everyday items such as a comfortable home, TV, and clean water. The list, in fact, could go on and on. Now focus on events that have made you angry -- it's raining, the car won't start, and a colleague   makes you annoyed. You start to feel unhappy, and that is something that certainly does not make you feel good! In fact, we have the ability to choose how we feel -- it's just our perception   of how things are. But for most of us, it just doesn't seem easy. Let me give you an example: it's a rainy day, and immediately most people will start to complain, telling everyone who will listen what a miserable day it is, with the result that they end up feeling miserable themselves. But look at it another way and despite wet clothes and hair, both will dry perfectly well and no lasting harm has been done. And in addition to this, because of rain, we not only live in a green and beautiful landscape, we are also able to grow a lot of fruit and vegetables. There really is no obvious reason for feeling miserable -- in fact there is a great deal to be grateful for. It all depends on what we think about things. Realize what a difference having gratitude can make to your life. That's why gratitude is so special -- use it to feel good!
03/12/2022 19:18:27 - INFO - __main__ - ['to be grateful for what we have']
03/12/2022 19:18:27 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:18:27 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:18:27 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 19:18:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 19:18:39 - INFO - __main__ - Starting training!
03/12/2022 19:18:44 - INFO - __main__ - Step 10 Global step 10 Train loss 20.245144 on epoch=4
03/12/2022 19:18:50 - INFO - __main__ - Step 20 Global step 20 Train loss 16.067326 on epoch=9
03/12/2022 19:18:56 - INFO - __main__ - Step 30 Global step 30 Train loss 9.880794 on epoch=14
03/12/2022 19:19:02 - INFO - __main__ - Step 40 Global step 40 Train loss 6.952089 on epoch=19
03/12/2022 19:19:08 - INFO - __main__ - Step 50 Global step 50 Train loss 5.140201 on epoch=24
03/12/2022 19:19:10 - INFO - __main__ - Global step 50 Train loss 11.657109 ACC 0.0625 on epoch=24
03/12/2022 19:19:17 - INFO - __main__ - Step 60 Global step 60 Train loss 4.321980 on epoch=29
03/12/2022 19:19:23 - INFO - __main__ - Step 70 Global step 70 Train loss 3.892380 on epoch=34
03/12/2022 19:19:29 - INFO - __main__ - Step 80 Global step 80 Train loss 3.560546 on epoch=39
03/12/2022 19:19:35 - INFO - __main__ - Step 90 Global step 90 Train loss 3.423787 on epoch=44
03/12/2022 19:19:41 - INFO - __main__ - Step 100 Global step 100 Train loss 3.393536 on epoch=49
03/12/2022 19:19:43 - INFO - __main__ - Global step 100 Train loss 3.718446 ACC 0.09375 on epoch=49
03/12/2022 19:19:50 - INFO - __main__ - Step 110 Global step 110 Train loss 3.100416 on epoch=54
03/12/2022 19:19:56 - INFO - __main__ - Step 120 Global step 120 Train loss 2.770931 on epoch=59
03/12/2022 19:20:02 - INFO - __main__ - Step 130 Global step 130 Train loss 2.777321 on epoch=64
03/12/2022 19:20:08 - INFO - __main__ - Step 140 Global step 140 Train loss 2.514630 on epoch=69
03/12/2022 19:20:15 - INFO - __main__ - Step 150 Global step 150 Train loss 2.096062 on epoch=74
03/12/2022 19:20:17 - INFO - __main__ - Global step 150 Train loss 2.651872 ACC 0.09375 on epoch=74
03/12/2022 19:20:23 - INFO - __main__ - Step 160 Global step 160 Train loss 2.089373 on epoch=79
03/12/2022 19:20:30 - INFO - __main__ - Step 170 Global step 170 Train loss 1.925807 on epoch=84
03/12/2022 19:20:36 - INFO - __main__ - Step 180 Global step 180 Train loss 1.801089 on epoch=89
03/12/2022 19:20:42 - INFO - __main__ - Step 190 Global step 190 Train loss 1.666048 on epoch=94
03/12/2022 19:20:48 - INFO - __main__ - Step 200 Global step 200 Train loss 1.759214 on epoch=99
03/12/2022 19:20:51 - INFO - __main__ - Global step 200 Train loss 1.848306 ACC 0.09375 on epoch=99
03/12/2022 19:20:57 - INFO - __main__ - Step 210 Global step 210 Train loss 1.360175 on epoch=104
03/12/2022 19:21:03 - INFO - __main__ - Step 220 Global step 220 Train loss 1.375776 on epoch=109
03/12/2022 19:21:09 - INFO - __main__ - Step 230 Global step 230 Train loss 1.346752 on epoch=114
03/12/2022 19:21:16 - INFO - __main__ - Step 240 Global step 240 Train loss 1.232456 on epoch=119
03/12/2022 19:21:22 - INFO - __main__ - Step 250 Global step 250 Train loss 1.256652 on epoch=124
03/12/2022 19:21:24 - INFO - __main__ - Global step 250 Train loss 1.314362 ACC 0.09375 on epoch=124
03/12/2022 19:21:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.163800 on epoch=129
03/12/2022 19:21:37 - INFO - __main__ - Step 270 Global step 270 Train loss 1.046060 on epoch=134
03/12/2022 19:21:43 - INFO - __main__ - Step 280 Global step 280 Train loss 1.069446 on epoch=139
03/12/2022 19:21:49 - INFO - __main__ - Step 290 Global step 290 Train loss 1.011252 on epoch=144
03/12/2022 19:21:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.955042 on epoch=149
03/12/2022 19:21:58 - INFO - __main__ - Global step 300 Train loss 1.049120 ACC 0.03125 on epoch=149
03/12/2022 19:22:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.972182 on epoch=154
03/12/2022 19:22:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.956722 on epoch=159
03/12/2022 19:22:16 - INFO - __main__ - Step 330 Global step 330 Train loss 1.008569 on epoch=164
03/12/2022 19:22:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.982083 on epoch=169
03/12/2022 19:22:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.925121 on epoch=174
03/12/2022 19:22:31 - INFO - __main__ - Global step 350 Train loss 0.968935 ACC 0.0 on epoch=174
03/12/2022 19:22:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.868358 on epoch=179
03/12/2022 19:22:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.839755 on epoch=184
03/12/2022 19:22:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.877650 on epoch=189
03/12/2022 19:22:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.917208 on epoch=194
03/12/2022 19:23:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.892003 on epoch=199
03/12/2022 19:23:04 - INFO - __main__ - Global step 400 Train loss 0.878995 ACC 0.0 on epoch=199
03/12/2022 19:23:10 - INFO - __main__ - Step 410 Global step 410 Train loss 1.042184 on epoch=204
03/12/2022 19:23:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.884540 on epoch=209
03/12/2022 19:23:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.882799 on epoch=214
03/12/2022 19:23:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.771347 on epoch=219
03/12/2022 19:23:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.802126 on epoch=224
03/12/2022 19:23:38 - INFO - __main__ - Global step 450 Train loss 0.876599 ACC 0.0 on epoch=224
03/12/2022 19:23:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.867724 on epoch=229
03/12/2022 19:23:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.789518 on epoch=234
03/12/2022 19:23:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.743918 on epoch=239
03/12/2022 19:24:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.735676 on epoch=244
03/12/2022 19:24:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.587430 on epoch=249
03/12/2022 19:24:11 - INFO - __main__ - Global step 500 Train loss 0.744853 ACC 0.0 on epoch=249
03/12/2022 19:24:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.718181 on epoch=254
03/12/2022 19:24:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.694800 on epoch=259
03/12/2022 19:24:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.790380 on epoch=264
03/12/2022 19:24:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.741566 on epoch=269
03/12/2022 19:24:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.799130 on epoch=274
03/12/2022 19:24:45 - INFO - __main__ - Global step 550 Train loss 0.748811 ACC 0.0 on epoch=274
03/12/2022 19:24:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.685231 on epoch=279
03/12/2022 19:24:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.755843 on epoch=284
03/12/2022 19:25:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.632630 on epoch=289
03/12/2022 19:25:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.623686 on epoch=294
03/12/2022 19:25:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.719653 on epoch=299
03/12/2022 19:25:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:25:17 - INFO - __main__ - Printing 3 examples
03/12/2022 19:25:17 - INFO - __main__ -  [race-high] In Hawaii the mountains are in the center of the islands because   _  . (A) of the weather (B) the islands were created by volcanic eruptions (C) of the geological conditions (D) of the islands' location in the Pacific [SEP] The Hawaiian Islands are situated about two thousand miles away from North America, right in the middle of the Pacific Ocean. Despite  the distance the islands actually make up the fiftieth state of the United States. The islands were all formed by volcanic eruption and on the largest of the islands, Hawaii, or the Big Island, there are still two active volcanoes, the Mauna Loa and the Kilauea, which still erupt every few years. On the Hawaiian islands the natives have a particularly strange way of indicating directions. They don't use the north, south, east and west system common to the rest of the world. They use the mauka andmakaisystem.Maukameans "mountain". The mountains in Hawaii are always at the center of the islands, as these are volcanic islands.Makaimeans "the sea". The islands are small and the system is simple. Wherever you want to visit, it can always be described in terms of where it lies in relation to the mountains and the sea. A typical conversation between a native Hawaiian and a tourist might go as follow. TOURIST: Excuse me! Could you tell me where the Sheraton Hotel is? HAWAIIAN: Well... Let me see! From here it's two blocks mauka, and then one blockmakai. You can't miss it!
03/12/2022 19:25:17 - INFO - __main__ - ['the islands were created by volcanic eruptions']
03/12/2022 19:25:17 - INFO - __main__ -  [race-high] Which of the following is true about Planet English according to the passage? (A) Planet English can support the students who study computer. (B) Planet English can help the students and their teachers interact. (C) Planet English makes it possible for students to communicate with speakers from around the world. (D) Planet English offers only a range of spoken communication for students. [SEP] Planet English is the world's leading interactive multimedia software package for English language teaching and learning. For Students Planet English uses the latest in multimedia and information technology to support students who wish to learn English for international communication. Planet English is an exciting, high-tech, interactive way of learning English. It contains more than 40 hours of video and audio recordings, over 2,500 0riginal graphics, 3,000 interactive screens and 80 different activity types including real time student voice recordings. For Teachers Planet English is more than just a computer program. It includes a package of resources to complement any Eng-lish language teaching programme. Teachers can easily integrate Planet English with classroom activities using the detailed Teacher's Manual  and Student Workbooks. Teachers can also manage the learning experience for students using the unique Planet English Courseware Management System (CMS). The CMS allows teachers to tailor courses to their syllabus  and to students' needs by "mapping" content for classes or individuals. Activities and exercises that are relevant to the center's syllabus are then delivered to students in the appropriate lesson, ensuring students "navigate" to the right area of the programme of each lesson. For Educational Managers Planet English is the world's leading Computer Assisted Language Learning (CALL) program. It allows English language teaching centers to enhance the educational quality of teaching programmes, improve learning outcomes and provide professional development for teaching staff. Implementing  Planet English allows English language teaching centers to maximize the benefits of computor hardware because it provides teachers and learners with an easy-to-use and highly productive CALL resource.
03/12/2022 19:25:17 - INFO - __main__ - ['Planet English makes it possible for students to communicate with speakers from around the world.']
03/12/2022 19:25:17 - INFO - __main__ -  [race-high] The Corcovado Hill is famous for  _  . (A) the samba parade (B) the beautiful view of the city (C) the statue of Jesus (D) the small church [SEP] On the coast of Brazil lies its most famous city Rio de Janeiro. Rio is a beautiful place to visit. It is full of exciting parades, historical architecture and natural beauty. The most famous time of year in Rio is the spring when the samba schools organize a giant parade of dance, music and costumes. Samba is a type of fast Brazilian music, which is perfect for dancing. The carnival lasts two days and over 20 samba schools perform, a total of 70,000 people in brightly coloured costumes, all dancing and singing! It is a magnificent spectacle and millions of people watch the carnival on television around the world. Rio de Janeiro is a religious city and it is full of churches. Some, like the former Imperial Church, were built over 400 years ago while others, like the Metropolitan Cathedral, are very modern. The Metropolitan Cathedral was built in 1976 and is so large that 20,000 people can stand inside at one time! Another important religious site in Rio is the Corcovado hill. On the top of the hill, along with a small church, stands a 30-metre high statue of Jesus with his arms stretched out. From this point you can see every part of the city.
03/12/2022 19:25:17 - INFO - __main__ - ['the beautiful view of the city']
03/12/2022 19:25:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 19:25:17 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:25:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 19:25:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:25:17 - INFO - __main__ - Printing 3 examples
03/12/2022 19:25:17 - INFO - __main__ -  [race-high] Which of the following is true according to the text? (A) Ieoh Ming Pei was born in America. (B) Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young. (C) Ieoh Ming Pei got a degree for architecture in 1948. (D) Ieoh Ming Pei got a Harvard undergraduate Degree in 1946. [SEP] On this vivid planet, it appears color1ful with many world famous buildings. Among these largest artificial articles in the world, many were designed by the same architect--Ieoh Ming Pei. Pei, the 1983 Laureate of the Pritzker, Architecture Prize, is a founding partner of I. M. Pei & Partners based in New York City. He was born in China in 1917, the son of a banker. He came to the United States in 1935 to study architecture at the Massachusetts Institute of Technology and the Harvard Graduate School of Design (M. Arch. 1946). From 1945 to 1948, Pei taught at Harvard. In 1948 he accepted the newly created post of director of Architecture at Webb & Knapp, Inc., and this association resulted in major architectural and planning projects in big cities. In 1958, he formed the partnership of I. M. Pei & Associates, which became I. M. Pei & Partners in 1966. The partnership received the 1968 Architectural Firm Award of The American Institute of Architects. Pei has designed over forty projects in the world, twenty of which have been award winners. His outstanding projects have included the East Building of the National Gallery of Art, Washington, D.C.; the John Fitzgerald Kennedy Library near Boston; the Fragrant Hill Hotel near Beijing, China. Pei is now a member of the National Council on the Arts, and before served on the National Council on the Humanities. He is a Fellow of the American Institute of Architects, a member of the Royal Institute of British Architects, and an elected member of the American Academy of Arts and Letters. He is a member of the Corporation of the Massachusetts Institute of Technology. As a student, he was awarded the MIT Traveling Fellowship, at Harvard. He later won a lot of honors. In 1982, the deans of the architectural schools of America chose I. M. Pei as the best designer of significant non-residential   structures.
03/12/2022 19:25:17 - INFO - __main__ - ['Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young.']
03/12/2022 19:25:17 - INFO - __main__ -  [race-high] According to the recently discovered evidence,   _  . (A) different regions of the brain have to work separately (B) people with phonagnosia are good at facial recognition (C) regions of the brain for personal recognition are connected (D) person recognition is controlled by the same part of the brain [SEP] James Cooke, of Islip,N.Y.,can't recognize other people. "I see eyes, nose, cheekbones, but no face," he said. "I've even passed by my son and daughter without recognizing them."  He is not the only one. Those with prosopagnosia, also known as face blindness, can see perfectly well, but their brains are unable to piece together the information needed to understand that a collection of features represents an individual's face. The condition is a neurological mystery, but new research has shed light on this strange disease.  Some scientists had believed that faces and voices, the two main ways people recognize one another, were processed separately by the brain. Indeed, a condition parallel to prosopagnosia, called  _ , similarly leaves a person unable to distinguish a familiar voice from an unfamiliar one. But by testing for these two conditions at the same time, researchers at the Max Planck Institute recently found evidence that face and voice recognition may be linked in a unique person-recognition system. The scientists observed the brain activity of 19 healthy volunteers as they were led through tasks that tested their ability to recognize both faces and voices. The researchers found that regions of the brain, associated with facial recognition, are directly linked to regions responsible for voice recognition.  This research helps explain why a person with prosopagnosia may still have difficulty determining who a person is even after he has begun to speak. The challenge for scientists is to find out where this system breaks down. Are these connections in the brain missing entirely, or are people unable to recognize faces and voices simply unable to use these links in some way?  It is unclear how many people have these conditions. Many don't even realize they have problems with facial or voice recognition. While some develop these difficulties after a brain injury, others develop it in childhood.
03/12/2022 19:25:17 - INFO - __main__ - ['regions of the brain for personal recognition are connected']
03/12/2022 19:25:17 - INFO - __main__ -  [race-high] By writing the passage, the author mainly wants to tell us    _   . (A) to think twice before we act (B) to be grateful for what we have (C) it is no use making complaints (D) saying "thank you" has many disadvantages [SEP] Throughout our childhood our parents taught us to say "thank you" and it has become a habit -- something we say automatically  , along with "please". And because of this we have forgotten just how important gratitude is and how essential it is in leading fulfilled   lives. Just for a minute, think of all the things you are grateful for, such as loving friends, good health, great holidays as well as everyday items such as a comfortable home, TV, and clean water. The list, in fact, could go on and on. Now focus on events that have made you angry -- it's raining, the car won't start, and a colleague   makes you annoyed. You start to feel unhappy, and that is something that certainly does not make you feel good! In fact, we have the ability to choose how we feel -- it's just our perception   of how things are. But for most of us, it just doesn't seem easy. Let me give you an example: it's a rainy day, and immediately most people will start to complain, telling everyone who will listen what a miserable day it is, with the result that they end up feeling miserable themselves. But look at it another way and despite wet clothes and hair, both will dry perfectly well and no lasting harm has been done. And in addition to this, because of rain, we not only live in a green and beautiful landscape, we are also able to grow a lot of fruit and vegetables. There really is no obvious reason for feeling miserable -- in fact there is a great deal to be grateful for. It all depends on what we think about things. Realize what a difference having gratitude can make to your life. That's why gratitude is so special -- use it to feel good!
03/12/2022 19:25:17 - INFO - __main__ - ['to be grateful for what we have']
03/12/2022 19:25:17 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:25:17 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:25:17 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 19:25:18 - INFO - __main__ - Global step 600 Train loss 0.683409 ACC 0.0 on epoch=299
03/12/2022 19:25:18 - INFO - __main__ - save last model!
03/12/2022 19:25:25 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 19:25:26 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 19:25:26 - INFO - __main__ - Printing 3 examples
03/12/2022 19:25:26 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:25:26 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 19:25:26 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:25:26 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 19:25:26 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:25:26 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 19:25:26 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:25:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 19:25:28 - INFO - __main__ - Starting training!
03/12/2022 19:25:32 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:25:35 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 19:29:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_42_0.0002_8_predictions.txt
03/12/2022 19:29:32 - INFO - __main__ - ACC on test data: 0.1281
03/12/2022 19:29:32 - INFO - __main__ - prefix=race-high_32_42, lr=0.0002, bsz=8, dev_performance=0.09375, test_performance=0.12807881773399016
03/12/2022 19:29:32 - INFO - __main__ - Running ... prefix=race-high_32_42, lr=0.0001, bsz=8 ...
03/12/2022 19:29:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:29:33 - INFO - __main__ - Printing 3 examples
03/12/2022 19:29:33 - INFO - __main__ -  [race-high] In Hawaii the mountains are in the center of the islands because   _  . (A) of the weather (B) the islands were created by volcanic eruptions (C) of the geological conditions (D) of the islands' location in the Pacific [SEP] The Hawaiian Islands are situated about two thousand miles away from North America, right in the middle of the Pacific Ocean. Despite  the distance the islands actually make up the fiftieth state of the United States. The islands were all formed by volcanic eruption and on the largest of the islands, Hawaii, or the Big Island, there are still two active volcanoes, the Mauna Loa and the Kilauea, which still erupt every few years. On the Hawaiian islands the natives have a particularly strange way of indicating directions. They don't use the north, south, east and west system common to the rest of the world. They use the mauka andmakaisystem.Maukameans "mountain". The mountains in Hawaii are always at the center of the islands, as these are volcanic islands.Makaimeans "the sea". The islands are small and the system is simple. Wherever you want to visit, it can always be described in terms of where it lies in relation to the mountains and the sea. A typical conversation between a native Hawaiian and a tourist might go as follow. TOURIST: Excuse me! Could you tell me where the Sheraton Hotel is? HAWAIIAN: Well... Let me see! From here it's two blocks mauka, and then one blockmakai. You can't miss it!
03/12/2022 19:29:33 - INFO - __main__ - ['the islands were created by volcanic eruptions']
03/12/2022 19:29:33 - INFO - __main__ -  [race-high] Which of the following is true about Planet English according to the passage? (A) Planet English can support the students who study computer. (B) Planet English can help the students and their teachers interact. (C) Planet English makes it possible for students to communicate with speakers from around the world. (D) Planet English offers only a range of spoken communication for students. [SEP] Planet English is the world's leading interactive multimedia software package for English language teaching and learning. For Students Planet English uses the latest in multimedia and information technology to support students who wish to learn English for international communication. Planet English is an exciting, high-tech, interactive way of learning English. It contains more than 40 hours of video and audio recordings, over 2,500 0riginal graphics, 3,000 interactive screens and 80 different activity types including real time student voice recordings. For Teachers Planet English is more than just a computer program. It includes a package of resources to complement any Eng-lish language teaching programme. Teachers can easily integrate Planet English with classroom activities using the detailed Teacher's Manual  and Student Workbooks. Teachers can also manage the learning experience for students using the unique Planet English Courseware Management System (CMS). The CMS allows teachers to tailor courses to their syllabus  and to students' needs by "mapping" content for classes or individuals. Activities and exercises that are relevant to the center's syllabus are then delivered to students in the appropriate lesson, ensuring students "navigate" to the right area of the programme of each lesson. For Educational Managers Planet English is the world's leading Computer Assisted Language Learning (CALL) program. It allows English language teaching centers to enhance the educational quality of teaching programmes, improve learning outcomes and provide professional development for teaching staff. Implementing  Planet English allows English language teaching centers to maximize the benefits of computor hardware because it provides teachers and learners with an easy-to-use and highly productive CALL resource.
03/12/2022 19:29:33 - INFO - __main__ - ['Planet English makes it possible for students to communicate with speakers from around the world.']
03/12/2022 19:29:33 - INFO - __main__ -  [race-high] The Corcovado Hill is famous for  _  . (A) the samba parade (B) the beautiful view of the city (C) the statue of Jesus (D) the small church [SEP] On the coast of Brazil lies its most famous city Rio de Janeiro. Rio is a beautiful place to visit. It is full of exciting parades, historical architecture and natural beauty. The most famous time of year in Rio is the spring when the samba schools organize a giant parade of dance, music and costumes. Samba is a type of fast Brazilian music, which is perfect for dancing. The carnival lasts two days and over 20 samba schools perform, a total of 70,000 people in brightly coloured costumes, all dancing and singing! It is a magnificent spectacle and millions of people watch the carnival on television around the world. Rio de Janeiro is a religious city and it is full of churches. Some, like the former Imperial Church, were built over 400 years ago while others, like the Metropolitan Cathedral, are very modern. The Metropolitan Cathedral was built in 1976 and is so large that 20,000 people can stand inside at one time! Another important religious site in Rio is the Corcovado hill. On the top of the hill, along with a small church, stands a 30-metre high statue of Jesus with his arms stretched out. From this point you can see every part of the city.
03/12/2022 19:29:33 - INFO - __main__ - ['the beautiful view of the city']
03/12/2022 19:29:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 19:29:33 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:29:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 19:29:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:29:33 - INFO - __main__ - Printing 3 examples
03/12/2022 19:29:33 - INFO - __main__ -  [race-high] Which of the following is true according to the text? (A) Ieoh Ming Pei was born in America. (B) Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young. (C) Ieoh Ming Pei got a degree for architecture in 1948. (D) Ieoh Ming Pei got a Harvard undergraduate Degree in 1946. [SEP] On this vivid planet, it appears color1ful with many world famous buildings. Among these largest artificial articles in the world, many were designed by the same architect--Ieoh Ming Pei. Pei, the 1983 Laureate of the Pritzker, Architecture Prize, is a founding partner of I. M. Pei & Partners based in New York City. He was born in China in 1917, the son of a banker. He came to the United States in 1935 to study architecture at the Massachusetts Institute of Technology and the Harvard Graduate School of Design (M. Arch. 1946). From 1945 to 1948, Pei taught at Harvard. In 1948 he accepted the newly created post of director of Architecture at Webb & Knapp, Inc., and this association resulted in major architectural and planning projects in big cities. In 1958, he formed the partnership of I. M. Pei & Associates, which became I. M. Pei & Partners in 1966. The partnership received the 1968 Architectural Firm Award of The American Institute of Architects. Pei has designed over forty projects in the world, twenty of which have been award winners. His outstanding projects have included the East Building of the National Gallery of Art, Washington, D.C.; the John Fitzgerald Kennedy Library near Boston; the Fragrant Hill Hotel near Beijing, China. Pei is now a member of the National Council on the Arts, and before served on the National Council on the Humanities. He is a Fellow of the American Institute of Architects, a member of the Royal Institute of British Architects, and an elected member of the American Academy of Arts and Letters. He is a member of the Corporation of the Massachusetts Institute of Technology. As a student, he was awarded the MIT Traveling Fellowship, at Harvard. He later won a lot of honors. In 1982, the deans of the architectural schools of America chose I. M. Pei as the best designer of significant non-residential   structures.
03/12/2022 19:29:33 - INFO - __main__ - ['Ieoh Ming Pei studied architecture at the Massachusetts Institute of Technology when he was young.']
03/12/2022 19:29:33 - INFO - __main__ -  [race-high] According to the recently discovered evidence,   _  . (A) different regions of the brain have to work separately (B) people with phonagnosia are good at facial recognition (C) regions of the brain for personal recognition are connected (D) person recognition is controlled by the same part of the brain [SEP] James Cooke, of Islip,N.Y.,can't recognize other people. "I see eyes, nose, cheekbones, but no face," he said. "I've even passed by my son and daughter without recognizing them."  He is not the only one. Those with prosopagnosia, also known as face blindness, can see perfectly well, but their brains are unable to piece together the information needed to understand that a collection of features represents an individual's face. The condition is a neurological mystery, but new research has shed light on this strange disease.  Some scientists had believed that faces and voices, the two main ways people recognize one another, were processed separately by the brain. Indeed, a condition parallel to prosopagnosia, called  _ , similarly leaves a person unable to distinguish a familiar voice from an unfamiliar one. But by testing for these two conditions at the same time, researchers at the Max Planck Institute recently found evidence that face and voice recognition may be linked in a unique person-recognition system. The scientists observed the brain activity of 19 healthy volunteers as they were led through tasks that tested their ability to recognize both faces and voices. The researchers found that regions of the brain, associated with facial recognition, are directly linked to regions responsible for voice recognition.  This research helps explain why a person with prosopagnosia may still have difficulty determining who a person is even after he has begun to speak. The challenge for scientists is to find out where this system breaks down. Are these connections in the brain missing entirely, or are people unable to recognize faces and voices simply unable to use these links in some way?  It is unclear how many people have these conditions. Many don't even realize they have problems with facial or voice recognition. While some develop these difficulties after a brain injury, others develop it in childhood.
03/12/2022 19:29:33 - INFO - __main__ - ['regions of the brain for personal recognition are connected']
03/12/2022 19:29:33 - INFO - __main__ -  [race-high] By writing the passage, the author mainly wants to tell us    _   . (A) to think twice before we act (B) to be grateful for what we have (C) it is no use making complaints (D) saying "thank you" has many disadvantages [SEP] Throughout our childhood our parents taught us to say "thank you" and it has become a habit -- something we say automatically  , along with "please". And because of this we have forgotten just how important gratitude is and how essential it is in leading fulfilled   lives. Just for a minute, think of all the things you are grateful for, such as loving friends, good health, great holidays as well as everyday items such as a comfortable home, TV, and clean water. The list, in fact, could go on and on. Now focus on events that have made you angry -- it's raining, the car won't start, and a colleague   makes you annoyed. You start to feel unhappy, and that is something that certainly does not make you feel good! In fact, we have the ability to choose how we feel -- it's just our perception   of how things are. But for most of us, it just doesn't seem easy. Let me give you an example: it's a rainy day, and immediately most people will start to complain, telling everyone who will listen what a miserable day it is, with the result that they end up feeling miserable themselves. But look at it another way and despite wet clothes and hair, both will dry perfectly well and no lasting harm has been done. And in addition to this, because of rain, we not only live in a green and beautiful landscape, we are also able to grow a lot of fruit and vegetables. There really is no obvious reason for feeling miserable -- in fact there is a great deal to be grateful for. It all depends on what we think about things. Realize what a difference having gratitude can make to your life. That's why gratitude is so special -- use it to feel good!
03/12/2022 19:29:33 - INFO - __main__ - ['to be grateful for what we have']
03/12/2022 19:29:33 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:29:33 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:29:33 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 19:29:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 19:29:44 - INFO - __main__ - Starting training!
03/12/2022 19:29:50 - INFO - __main__ - Step 10 Global step 10 Train loss 20.096033 on epoch=4
03/12/2022 19:29:56 - INFO - __main__ - Step 20 Global step 20 Train loss 17.608532 on epoch=9
03/12/2022 19:30:02 - INFO - __main__ - Step 30 Global step 30 Train loss 12.907720 on epoch=14
03/12/2022 19:30:08 - INFO - __main__ - Step 40 Global step 40 Train loss 10.550651 on epoch=19
03/12/2022 19:30:14 - INFO - __main__ - Step 50 Global step 50 Train loss 8.622916 on epoch=24
03/12/2022 19:30:29 - INFO - __main__ - Global step 50 Train loss 13.957170 ACC 0.0 on epoch=24
03/12/2022 19:30:36 - INFO - __main__ - Step 60 Global step 60 Train loss 7.580520 on epoch=29
03/12/2022 19:30:42 - INFO - __main__ - Step 70 Global step 70 Train loss 6.964209 on epoch=34
03/12/2022 19:30:48 - INFO - __main__ - Step 80 Global step 80 Train loss 5.419684 on epoch=39
03/12/2022 19:30:54 - INFO - __main__ - Step 90 Global step 90 Train loss 4.602209 on epoch=44
03/12/2022 19:31:00 - INFO - __main__ - Step 100 Global step 100 Train loss 4.348808 on epoch=49
03/12/2022 19:31:02 - INFO - __main__ - Global step 100 Train loss 5.783085 ACC 0.0625 on epoch=49
03/12/2022 19:31:09 - INFO - __main__ - Step 110 Global step 110 Train loss 3.891942 on epoch=54
03/12/2022 19:31:15 - INFO - __main__ - Step 120 Global step 120 Train loss 3.883099 on epoch=59
03/12/2022 19:31:21 - INFO - __main__ - Step 130 Global step 130 Train loss 3.643779 on epoch=64
03/12/2022 19:31:27 - INFO - __main__ - Step 140 Global step 140 Train loss 3.176533 on epoch=69
03/12/2022 19:31:33 - INFO - __main__ - Step 150 Global step 150 Train loss 3.484369 on epoch=74
03/12/2022 19:31:36 - INFO - __main__ - Global step 150 Train loss 3.615944 ACC 0.1875 on epoch=74
03/12/2022 19:31:43 - INFO - __main__ - Step 160 Global step 160 Train loss 3.006632 on epoch=79
03/12/2022 19:31:49 - INFO - __main__ - Step 170 Global step 170 Train loss 3.061843 on epoch=84
03/12/2022 19:31:55 - INFO - __main__ - Step 180 Global step 180 Train loss 2.981085 on epoch=89
03/12/2022 19:32:01 - INFO - __main__ - Step 190 Global step 190 Train loss 3.005686 on epoch=94
03/12/2022 19:32:07 - INFO - __main__ - Step 200 Global step 200 Train loss 2.798606 on epoch=99
03/12/2022 19:32:10 - INFO - __main__ - Global step 200 Train loss 2.970771 ACC 0.15625 on epoch=99
03/12/2022 19:32:16 - INFO - __main__ - Step 210 Global step 210 Train loss 2.580122 on epoch=104
03/12/2022 19:32:22 - INFO - __main__ - Step 220 Global step 220 Train loss 2.549219 on epoch=109
03/12/2022 19:32:28 - INFO - __main__ - Step 230 Global step 230 Train loss 2.459321 on epoch=114
03/12/2022 19:32:34 - INFO - __main__ - Step 240 Global step 240 Train loss 2.728329 on epoch=119
03/12/2022 19:32:40 - INFO - __main__ - Step 250 Global step 250 Train loss 2.447435 on epoch=124
03/12/2022 19:32:43 - INFO - __main__ - Global step 250 Train loss 2.552886 ACC 0.15625 on epoch=124
03/12/2022 19:32:49 - INFO - __main__ - Step 260 Global step 260 Train loss 2.339188 on epoch=129
03/12/2022 19:32:55 - INFO - __main__ - Step 270 Global step 270 Train loss 2.198485 on epoch=134
03/12/2022 19:33:01 - INFO - __main__ - Step 280 Global step 280 Train loss 2.400603 on epoch=139
03/12/2022 19:33:07 - INFO - __main__ - Step 290 Global step 290 Train loss 2.012151 on epoch=144
03/12/2022 19:33:13 - INFO - __main__ - Step 300 Global step 300 Train loss 2.098756 on epoch=149
03/12/2022 19:33:15 - INFO - __main__ - Global step 300 Train loss 2.209836 ACC 0.15625 on epoch=149
03/12/2022 19:33:21 - INFO - __main__ - Step 310 Global step 310 Train loss 2.115115 on epoch=154
03/12/2022 19:33:27 - INFO - __main__ - Step 320 Global step 320 Train loss 1.936661 on epoch=159
03/12/2022 19:33:33 - INFO - __main__ - Step 330 Global step 330 Train loss 1.845824 on epoch=164
03/12/2022 19:33:39 - INFO - __main__ - Step 340 Global step 340 Train loss 1.915325 on epoch=169
03/12/2022 19:33:45 - INFO - __main__ - Step 350 Global step 350 Train loss 1.674799 on epoch=174
03/12/2022 19:33:48 - INFO - __main__ - Global step 350 Train loss 1.897545 ACC 0.21875 on epoch=174
03/12/2022 19:33:54 - INFO - __main__ - Step 360 Global step 360 Train loss 1.615270 on epoch=179
03/12/2022 19:34:01 - INFO - __main__ - Step 370 Global step 370 Train loss 1.717580 on epoch=184
03/12/2022 19:34:07 - INFO - __main__ - Step 380 Global step 380 Train loss 1.551791 on epoch=189
03/12/2022 19:34:13 - INFO - __main__ - Step 390 Global step 390 Train loss 1.473761 on epoch=194
03/12/2022 19:34:19 - INFO - __main__ - Step 400 Global step 400 Train loss 1.450003 on epoch=199
03/12/2022 19:34:22 - INFO - __main__ - Global step 400 Train loss 1.561681 ACC 0.15625 on epoch=199
03/12/2022 19:34:28 - INFO - __main__ - Step 410 Global step 410 Train loss 1.430699 on epoch=204
03/12/2022 19:34:34 - INFO - __main__ - Step 420 Global step 420 Train loss 1.335737 on epoch=209
03/12/2022 19:34:41 - INFO - __main__ - Step 430 Global step 430 Train loss 1.310559 on epoch=214
03/12/2022 19:34:47 - INFO - __main__ - Step 440 Global step 440 Train loss 1.353244 on epoch=219
03/12/2022 19:34:53 - INFO - __main__ - Step 450 Global step 450 Train loss 1.162049 on epoch=224
03/12/2022 19:34:56 - INFO - __main__ - Global step 450 Train loss 1.318457 ACC 0.125 on epoch=224
03/12/2022 19:35:02 - INFO - __main__ - Step 460 Global step 460 Train loss 1.215630 on epoch=229
03/12/2022 19:35:08 - INFO - __main__ - Step 470 Global step 470 Train loss 1.082030 on epoch=234
03/12/2022 19:35:14 - INFO - __main__ - Step 480 Global step 480 Train loss 1.092458 on epoch=239
03/12/2022 19:35:20 - INFO - __main__ - Step 490 Global step 490 Train loss 1.169903 on epoch=244
03/12/2022 19:35:27 - INFO - __main__ - Step 500 Global step 500 Train loss 1.106179 on epoch=249
03/12/2022 19:35:29 - INFO - __main__ - Global step 500 Train loss 1.133240 ACC 0.0625 on epoch=249
03/12/2022 19:35:35 - INFO - __main__ - Step 510 Global step 510 Train loss 1.004441 on epoch=254
03/12/2022 19:35:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.956958 on epoch=259
03/12/2022 19:35:47 - INFO - __main__ - Step 530 Global step 530 Train loss 1.199253 on epoch=264
03/12/2022 19:35:54 - INFO - __main__ - Step 540 Global step 540 Train loss 1.022569 on epoch=269
03/12/2022 19:36:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.933554 on epoch=274
03/12/2022 19:36:02 - INFO - __main__ - Global step 550 Train loss 1.023355 ACC 0.03125 on epoch=274
03/12/2022 19:36:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.992123 on epoch=279
03/12/2022 19:36:14 - INFO - __main__ - Step 570 Global step 570 Train loss 1.023313 on epoch=284
03/12/2022 19:36:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.886800 on epoch=289
03/12/2022 19:36:27 - INFO - __main__ - Step 590 Global step 590 Train loss 1.001009 on epoch=294
03/12/2022 19:36:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.948865 on epoch=299
03/12/2022 19:36:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:36:34 - INFO - __main__ - Printing 3 examples
03/12/2022 19:36:34 - INFO - __main__ -  [race-high] Which statement is NOT true according to the article? (A) In the West, people think laws and customs are rather different. (B) In the West, there is little difference between "sins" and "crimes". (C) An action that is considered a crime in one country may be socially acceptable in another. (D) There is far less use of the civil justice system in Japan than in the United States. [SEP] The idea of "law" exists in every culture. All societies have some kind of law to keep order and to control the interactions of people with those around them. The laws of any culture tell people three things: what they can do (their right), what they must do (their duties), and what they may not do. In addition, there are usually specific types of punishment for those who break the law. Although all societies have laws, not all have the same idea of justice--which is "right" and "wrong" and how "wrong" should be punished. In most Western cultures, it is thought that punishing criminals will prevent them from committing other crimes. Also, it is hoped that the fear of punishment will act as a deterrent  that prevents other people from committing similar crimes; in other words, people who are considering a life of crime will decide against it because of fear of punishment. In most non-Western cultures, by contrast, punishment is not seen as a deterrent. Instead, great importance is placed on restoring balance in the situation. A thief, for example, may be ordered to return the things he has stolen instead of, as in Western societies, spending time in prison. Another difference in the concept of justice lies in various societies' ideas of what laws are. In the West, people consider "laws" quite different from "customs". There is also a great contrast between "sins" (breaking religious laws) and "crimes" (breaking laws of the government). In many non-Western cultures, on the other hand, there is little separation of customs, laws, and religious beliefs; in other cultures, these three may be quite separate from one another, but still very much different from those in the West. For these reasons, an action may be considered a crime in one country, but be socially acceptable in others. For instance, although a thief is viewed as a criminal in much of the world, in a small village where there is considerable communal  living and sharing of objects, the word thief may have little meaning. Someone who has taken something without asking is simply considered an impolite person. Most countries have two kinds of law: criminal and civil. People who have been accused of acts such as murder or theft are heard in the criminal justice system, while civil justice deals with people who are believed to have violated others' rights. The use of the civil system reflects the values of the society in which it exists. In the United States where personal, individual justice is considered very important, civil law has become "big business." There are over 600,000 lawyers in the United States, and many of them keep busy with civil lawsuits; that is, they work for people who want to sue others. If a man falls over a torn rug in a hotel and breaks his arm, for instance, he might decide to sue the hotel owners so that they will pay his medical costs. In a country like Japan, by contrast, there is very little use of the civil justice system. Lawsuits are not very popular in Japan, where social harmony is even more important than individual rights, and where people would rather reach agreement outside court.
03/12/2022 19:36:34 - INFO - __main__ - ['In the West, there is little difference between "sins" and "crimes".']
03/12/2022 19:36:34 - INFO - __main__ -  [race-high] according to the passage, perhaps the followings are our ordinary ways of relaxation for common people except  _  . (A) listening to music (B) playing card (C) going out for fishing (D) boxing match [SEP] "All work and no play makes Jack a dull boy" is a popular saying in the United States. Other countries have similar sayings. It is true that all of us need _ We cannot work all the time if we are going to keep good health and enjoy life. Everyone has his own way of relaxing. Perhaps the most popular way is to take part in sports. there are team sports, such as baseball, basketball, and football. There are individual sports, also, such as golf and swimming. In addition hiking, fishing, skiing, and mountain climbing have a great attraction for people who like to be outdoors. Not everyone who enjoys sports events likes to take part in them. Many people prefer to be onlookers, either watching them on television, or listening to them on the radio. When there is an important baseball game or boxing match it is almost impossible to get tickets; everyone wants to attend. Chess, card-playing, and dancing are forms of indoor recreation enjoyed by many people. It doesn't matter whether we play a fast game of ping-pong, concentrate over the bridge table, or go walking through the woods on a brisk autumn afternoon. It is important for every one to relax from time to time and enjoy some form of recreation.
03/12/2022 19:36:34 - INFO - __main__ - ['boxing match']
03/12/2022 19:36:34 - INFO - __main__ -  [race-high] Before he became a full time writer Mark Twain had not been    _ (A) a printer. (B) a miner (C) a tailor (D) a soldier [SEP] Mark Twain left school when he was twelve. Though he had little school education, he became the most famous writer of his time. He made millions of dollars by writing. His real name was Samuel Langhorne Clements, but he is better known all over the world as Mark Twain, his penname. Mark Twain was born in 1835 and he was not a healthy baby. In fact, he was not expected to live through the first winter. But with his mother's tender care, he managed to survive. As a boy, he caused much trouble to his parents. He used to play jokes on all his friends and neighbors. He didn't like to go to school, and he ran away from home from time to time. He always went in the direction of the nearby Mississippi. He was nearly drowned  nine times. After his father's death in 1847, Mark Twain began to work for a printer, who only provided him with food and clothing. Then, he worked as a river-boat pilot and later joined the army. But shortly after that he became a miner, during this period, he started to write short stories.  Afterwards he became a full-time writer. In 1870, Mark Twain got married. In the years that followed he wrote many books including in 1876, and in 1884, which made him famous, and brought him a great fortune .Unfortunately, Mark Twain got into debts in bad investments  and he had to write large numbers of stories to pay these debts. In 1904, his wife died and then three of their children passed away. He died on April 21, 1910 at the age of 70.
03/12/2022 19:36:34 - INFO - __main__ - ['a tailor']
03/12/2022 19:36:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 19:36:35 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:36:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 19:36:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:36:35 - INFO - __main__ - Printing 3 examples
03/12/2022 19:36:35 - INFO - __main__ -  [race-high] Compared to Emily, Zach is more interested in   . (A) design (B) sports (C) beaches (D) history [SEP] Emily and Zach are confused!  Their parents told them they could choose between Massachusetts and Arizona for their vacation this summer. Emily has always wanted to visit Boston, the capital of Massachusetts. Zach and she both agree that walking along the Freedom Trail would enable them to see Boston's most famous places of historic interest, like the site of the school Ben Franklin attended and the Old State House. If Emily and Zach go to Massachusetts, they could spend a few days at the beaches on Cape Cod. Emily loves body boarding, and Zach is great at body surfing. They both enjoy building sandcastles with their mom and dad. Zach finds learning about Native Americans wonderful and has always wanted to travel along the Apache Trail in Arizona. This mountain highway passes Native American ruins in Tonto National Forest. Emily is not as interested in traveling along this trail as Zach, but they both would like to visit Phoenix, the capital, and then travel to Grand Canyon National Park and Meteor Crater .Zach learned in science class that Meteor Crater is a hole over 4,000 feet wide and 520 feet deep that was created when a huge object from space fell to Earth. The object went so deep that it has never been found. Zach would really like to try to discover it. But Emily thinks if experienced scientists and researchers cannot find it, Zach might as well not even bother to try. The only drawback for Zach and Emily if they choose Arizona would be the heat. It is very hot and dry in this southwestern state. Massachusetts, on the other hand, is in northeastern United States. Here Zach and Emily and their parents could enjoy mild temperatures. Their parents love hot weather, but Zach and Emily do not really like to sweat. How will they ever decide to which state they should travel? If only they could take two trips!
03/12/2022 19:36:35 - INFO - __main__ - ['history']
03/12/2022 19:36:35 - INFO - __main__ -  [race-high] The purpose of writing the text is to   _  . (A) advertise some popular summer programmes (B) encourage readers to have a good time relaxing (C) offer some tips on how to enjoy a learning vacation (D) attract more readers to spend summer time learning [SEP] Is there something that you've always wanted to try but just never had the time?Well,make plans to try it now since you are on summer vacation.Not all vacations call for taking a tour bus to take photos of famous landmarks.Some vacations allow you plenty of opportunities to learn. The most difficult aspect of a learning vacation may be choosing one because the possibilities are endless.If you enjoy cooking,various companies can take you to Italy,France,Spain,Mexico or even Peru.Once there,you can learn to prepare the local cuisine .Trips are often planned to fit in with local food festivals or special events. The term"learning vacation"often brings language to mind.The best way to learn a language is in an environment where it' s spoken.Study Spanish,French or English.Or attempt a more unusual language like Polish,Estonian or Thai.You'll be able to learn about the country and absorb the culture at the same time. If you are fond of sports,you can polish your skills or learn new ones.Golf and tennis schools welcome players of all levels.If you want a bigger thrill,you can learn to surf,go climbing or race cars.It' s even possible to learn the art and techniques of bull fighting while on vacation! You can also discover our inner artist.Many places offer painting classes in different mediums.The scenic locations of the schools offer plenty of subjects that provide inspiration for practice. If you prefer capturing the world on film,take a photography vacation.Travel with a small group to photograph beautiful animals or scenery .You can also practise your technique on people or at historical sights. Once you decide on a vacation,choose a company carefully.Request names of recent customers you can contact,and then ask them for an evaluation.The more you know before you go,the better prepared you'll be.Then go out and learn something!
03/12/2022 19:36:35 - INFO - __main__ - ['offer some tips on how to enjoy a learning vacation']
03/12/2022 19:36:35 - INFO - __main__ -  [race-high] What kind of people will benefit a lot more from this passage? (A) Scientists (B) Teachers (C) Designers (D) Lawyers [SEP] Trends come and go but style is eternal. So what is the latest fashion trend that defines this season? Let's take a look at the fashion trend in 2016 and be the first ones to embrace the latest trend. Head----It seems like everyone will be wearing a bucket hat this season. Bucket hat might sound a bit old-fashioned for some people, but the 90s trend is in season again! Spring and summer are always the seasons for casual clothes, so pairing the bucket hat with casual shorts or skirts would be your outfit to go in this spring and summer in 2016. Bottom--Summer is the season when everyone gets ready for the humid weather. The current season trend is making everyone have a more comfortable summer because wide-leg bottoms is in trend again. Perhaps not everyone likes wide-leg bottoms because this could possible make people look shorter, choosing a high-waist wide-leg bottom and matching it with crop top will definitely make you look taller and trendy in 2016. Dress---Be prepared for embracing the feminine details this season. Ruffles around the sleeve or ruffles all-over the dress will be everyone's favorite in SS 2016. All these little details will transform your look in season. Simple colors like pink and black are designer's favorites in this season too. Shoes---Many people thought wedges are also the shoes to go in spring and summer time, but in SS 2016, very flat flats are in trend again. Shoe designers are trying to have more combinations flats or low-heels with less of gender identity. Accessories----No outfit goes perfect without accessories. Adding statement accessories to your simple and natural outlook this summer is your day-to-night outfit. These jewels are doing to make you feel completed in day time and sparkled at night.
03/12/2022 19:36:35 - INFO - __main__ - ['Designers']
03/12/2022 19:36:35 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:36:35 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:36:35 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 19:36:35 - INFO - __main__ - Global step 600 Train loss 0.970422 ACC 0.03125 on epoch=299
03/12/2022 19:36:35 - INFO - __main__ - save last model!
03/12/2022 19:36:42 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 19:36:43 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 19:36:43 - INFO - __main__ - Printing 3 examples
03/12/2022 19:36:43 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:36:43 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 19:36:43 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:36:43 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 19:36:43 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:36:43 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 19:36:43 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:36:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 19:36:45 - INFO - __main__ - Starting training!
03/12/2022 19:36:49 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:36:52 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 19:41:01 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_42_0.0001_8_predictions.txt
03/12/2022 19:41:01 - INFO - __main__ - ACC on test data: 0.2046
03/12/2022 19:41:02 - INFO - __main__ - prefix=race-high_32_42, lr=0.0001, bsz=8, dev_performance=0.21875, test_performance=0.2045783830773689
03/12/2022 19:41:02 - INFO - __main__ - Running ... prefix=race-high_32_87, lr=0.0005, bsz=8 ...
03/12/2022 19:41:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:41:03 - INFO - __main__ - Printing 3 examples
03/12/2022 19:41:03 - INFO - __main__ -  [race-high] Which statement is NOT true according to the article? (A) In the West, people think laws and customs are rather different. (B) In the West, there is little difference between "sins" and "crimes". (C) An action that is considered a crime in one country may be socially acceptable in another. (D) There is far less use of the civil justice system in Japan than in the United States. [SEP] The idea of "law" exists in every culture. All societies have some kind of law to keep order and to control the interactions of people with those around them. The laws of any culture tell people three things: what they can do (their right), what they must do (their duties), and what they may not do. In addition, there are usually specific types of punishment for those who break the law. Although all societies have laws, not all have the same idea of justice--which is "right" and "wrong" and how "wrong" should be punished. In most Western cultures, it is thought that punishing criminals will prevent them from committing other crimes. Also, it is hoped that the fear of punishment will act as a deterrent  that prevents other people from committing similar crimes; in other words, people who are considering a life of crime will decide against it because of fear of punishment. In most non-Western cultures, by contrast, punishment is not seen as a deterrent. Instead, great importance is placed on restoring balance in the situation. A thief, for example, may be ordered to return the things he has stolen instead of, as in Western societies, spending time in prison. Another difference in the concept of justice lies in various societies' ideas of what laws are. In the West, people consider "laws" quite different from "customs". There is also a great contrast between "sins" (breaking religious laws) and "crimes" (breaking laws of the government). In many non-Western cultures, on the other hand, there is little separation of customs, laws, and religious beliefs; in other cultures, these three may be quite separate from one another, but still very much different from those in the West. For these reasons, an action may be considered a crime in one country, but be socially acceptable in others. For instance, although a thief is viewed as a criminal in much of the world, in a small village where there is considerable communal  living and sharing of objects, the word thief may have little meaning. Someone who has taken something without asking is simply considered an impolite person. Most countries have two kinds of law: criminal and civil. People who have been accused of acts such as murder or theft are heard in the criminal justice system, while civil justice deals with people who are believed to have violated others' rights. The use of the civil system reflects the values of the society in which it exists. In the United States where personal, individual justice is considered very important, civil law has become "big business." There are over 600,000 lawyers in the United States, and many of them keep busy with civil lawsuits; that is, they work for people who want to sue others. If a man falls over a torn rug in a hotel and breaks his arm, for instance, he might decide to sue the hotel owners so that they will pay his medical costs. In a country like Japan, by contrast, there is very little use of the civil justice system. Lawsuits are not very popular in Japan, where social harmony is even more important than individual rights, and where people would rather reach agreement outside court.
03/12/2022 19:41:03 - INFO - __main__ - ['In the West, there is little difference between "sins" and "crimes".']
03/12/2022 19:41:03 - INFO - __main__ -  [race-high] according to the passage, perhaps the followings are our ordinary ways of relaxation for common people except  _  . (A) listening to music (B) playing card (C) going out for fishing (D) boxing match [SEP] "All work and no play makes Jack a dull boy" is a popular saying in the United States. Other countries have similar sayings. It is true that all of us need _ We cannot work all the time if we are going to keep good health and enjoy life. Everyone has his own way of relaxing. Perhaps the most popular way is to take part in sports. there are team sports, such as baseball, basketball, and football. There are individual sports, also, such as golf and swimming. In addition hiking, fishing, skiing, and mountain climbing have a great attraction for people who like to be outdoors. Not everyone who enjoys sports events likes to take part in them. Many people prefer to be onlookers, either watching them on television, or listening to them on the radio. When there is an important baseball game or boxing match it is almost impossible to get tickets; everyone wants to attend. Chess, card-playing, and dancing are forms of indoor recreation enjoyed by many people. It doesn't matter whether we play a fast game of ping-pong, concentrate over the bridge table, or go walking through the woods on a brisk autumn afternoon. It is important for every one to relax from time to time and enjoy some form of recreation.
03/12/2022 19:41:03 - INFO - __main__ - ['boxing match']
03/12/2022 19:41:03 - INFO - __main__ -  [race-high] Before he became a full time writer Mark Twain had not been    _ (A) a printer. (B) a miner (C) a tailor (D) a soldier [SEP] Mark Twain left school when he was twelve. Though he had little school education, he became the most famous writer of his time. He made millions of dollars by writing. His real name was Samuel Langhorne Clements, but he is better known all over the world as Mark Twain, his penname. Mark Twain was born in 1835 and he was not a healthy baby. In fact, he was not expected to live through the first winter. But with his mother's tender care, he managed to survive. As a boy, he caused much trouble to his parents. He used to play jokes on all his friends and neighbors. He didn't like to go to school, and he ran away from home from time to time. He always went in the direction of the nearby Mississippi. He was nearly drowned  nine times. After his father's death in 1847, Mark Twain began to work for a printer, who only provided him with food and clothing. Then, he worked as a river-boat pilot and later joined the army. But shortly after that he became a miner, during this period, he started to write short stories.  Afterwards he became a full-time writer. In 1870, Mark Twain got married. In the years that followed he wrote many books including in 1876, and in 1884, which made him famous, and brought him a great fortune .Unfortunately, Mark Twain got into debts in bad investments  and he had to write large numbers of stories to pay these debts. In 1904, his wife died and then three of their children passed away. He died on April 21, 1910 at the age of 70.
03/12/2022 19:41:03 - INFO - __main__ - ['a tailor']
03/12/2022 19:41:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 19:41:03 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:41:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 19:41:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:41:03 - INFO - __main__ - Printing 3 examples
03/12/2022 19:41:03 - INFO - __main__ -  [race-high] Compared to Emily, Zach is more interested in   . (A) design (B) sports (C) beaches (D) history [SEP] Emily and Zach are confused!  Their parents told them they could choose between Massachusetts and Arizona for their vacation this summer. Emily has always wanted to visit Boston, the capital of Massachusetts. Zach and she both agree that walking along the Freedom Trail would enable them to see Boston's most famous places of historic interest, like the site of the school Ben Franklin attended and the Old State House. If Emily and Zach go to Massachusetts, they could spend a few days at the beaches on Cape Cod. Emily loves body boarding, and Zach is great at body surfing. They both enjoy building sandcastles with their mom and dad. Zach finds learning about Native Americans wonderful and has always wanted to travel along the Apache Trail in Arizona. This mountain highway passes Native American ruins in Tonto National Forest. Emily is not as interested in traveling along this trail as Zach, but they both would like to visit Phoenix, the capital, and then travel to Grand Canyon National Park and Meteor Crater .Zach learned in science class that Meteor Crater is a hole over 4,000 feet wide and 520 feet deep that was created when a huge object from space fell to Earth. The object went so deep that it has never been found. Zach would really like to try to discover it. But Emily thinks if experienced scientists and researchers cannot find it, Zach might as well not even bother to try. The only drawback for Zach and Emily if they choose Arizona would be the heat. It is very hot and dry in this southwestern state. Massachusetts, on the other hand, is in northeastern United States. Here Zach and Emily and their parents could enjoy mild temperatures. Their parents love hot weather, but Zach and Emily do not really like to sweat. How will they ever decide to which state they should travel? If only they could take two trips!
03/12/2022 19:41:03 - INFO - __main__ - ['history']
03/12/2022 19:41:03 - INFO - __main__ -  [race-high] The purpose of writing the text is to   _  . (A) advertise some popular summer programmes (B) encourage readers to have a good time relaxing (C) offer some tips on how to enjoy a learning vacation (D) attract more readers to spend summer time learning [SEP] Is there something that you've always wanted to try but just never had the time?Well,make plans to try it now since you are on summer vacation.Not all vacations call for taking a tour bus to take photos of famous landmarks.Some vacations allow you plenty of opportunities to learn. The most difficult aspect of a learning vacation may be choosing one because the possibilities are endless.If you enjoy cooking,various companies can take you to Italy,France,Spain,Mexico or even Peru.Once there,you can learn to prepare the local cuisine .Trips are often planned to fit in with local food festivals or special events. The term"learning vacation"often brings language to mind.The best way to learn a language is in an environment where it' s spoken.Study Spanish,French or English.Or attempt a more unusual language like Polish,Estonian or Thai.You'll be able to learn about the country and absorb the culture at the same time. If you are fond of sports,you can polish your skills or learn new ones.Golf and tennis schools welcome players of all levels.If you want a bigger thrill,you can learn to surf,go climbing or race cars.It' s even possible to learn the art and techniques of bull fighting while on vacation! You can also discover our inner artist.Many places offer painting classes in different mediums.The scenic locations of the schools offer plenty of subjects that provide inspiration for practice. If you prefer capturing the world on film,take a photography vacation.Travel with a small group to photograph beautiful animals or scenery .You can also practise your technique on people or at historical sights. Once you decide on a vacation,choose a company carefully.Request names of recent customers you can contact,and then ask them for an evaluation.The more you know before you go,the better prepared you'll be.Then go out and learn something!
03/12/2022 19:41:03 - INFO - __main__ - ['offer some tips on how to enjoy a learning vacation']
03/12/2022 19:41:03 - INFO - __main__ -  [race-high] What kind of people will benefit a lot more from this passage? (A) Scientists (B) Teachers (C) Designers (D) Lawyers [SEP] Trends come and go but style is eternal. So what is the latest fashion trend that defines this season? Let's take a look at the fashion trend in 2016 and be the first ones to embrace the latest trend. Head----It seems like everyone will be wearing a bucket hat this season. Bucket hat might sound a bit old-fashioned for some people, but the 90s trend is in season again! Spring and summer are always the seasons for casual clothes, so pairing the bucket hat with casual shorts or skirts would be your outfit to go in this spring and summer in 2016. Bottom--Summer is the season when everyone gets ready for the humid weather. The current season trend is making everyone have a more comfortable summer because wide-leg bottoms is in trend again. Perhaps not everyone likes wide-leg bottoms because this could possible make people look shorter, choosing a high-waist wide-leg bottom and matching it with crop top will definitely make you look taller and trendy in 2016. Dress---Be prepared for embracing the feminine details this season. Ruffles around the sleeve or ruffles all-over the dress will be everyone's favorite in SS 2016. All these little details will transform your look in season. Simple colors like pink and black are designer's favorites in this season too. Shoes---Many people thought wedges are also the shoes to go in spring and summer time, but in SS 2016, very flat flats are in trend again. Shoe designers are trying to have more combinations flats or low-heels with less of gender identity. Accessories----No outfit goes perfect without accessories. Adding statement accessories to your simple and natural outlook this summer is your day-to-night outfit. These jewels are doing to make you feel completed in day time and sparkled at night.
03/12/2022 19:41:03 - INFO - __main__ - ['Designers']
03/12/2022 19:41:03 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:41:03 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:41:03 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 19:41:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 19:41:14 - INFO - __main__ - Starting training!
03/12/2022 19:41:19 - INFO - __main__ - Step 10 Global step 10 Train loss 20.228291 on epoch=4
03/12/2022 19:41:25 - INFO - __main__ - Step 20 Global step 20 Train loss 14.481085 on epoch=9
03/12/2022 19:41:31 - INFO - __main__ - Step 30 Global step 30 Train loss 4.396997 on epoch=14
03/12/2022 19:41:37 - INFO - __main__ - Step 40 Global step 40 Train loss 3.526928 on epoch=19
03/12/2022 19:41:44 - INFO - __main__ - Step 50 Global step 50 Train loss 3.219636 on epoch=24
03/12/2022 19:41:46 - INFO - __main__ - Global step 50 Train loss 9.170588 ACC 0.03125 on epoch=24
03/12/2022 19:41:53 - INFO - __main__ - Step 60 Global step 60 Train loss 2.701971 on epoch=29
03/12/2022 19:41:59 - INFO - __main__ - Step 70 Global step 70 Train loss 2.469593 on epoch=34
03/12/2022 19:42:05 - INFO - __main__ - Step 80 Global step 80 Train loss 1.993527 on epoch=39
03/12/2022 19:42:11 - INFO - __main__ - Step 90 Global step 90 Train loss 1.624414 on epoch=44
03/12/2022 19:42:17 - INFO - __main__ - Step 100 Global step 100 Train loss 1.534632 on epoch=49
03/12/2022 19:42:20 - INFO - __main__ - Global step 100 Train loss 2.064827 ACC 0.03125 on epoch=49
03/12/2022 19:42:26 - INFO - __main__ - Step 110 Global step 110 Train loss 1.270559 on epoch=54
03/12/2022 19:42:32 - INFO - __main__ - Step 120 Global step 120 Train loss 1.302540 on epoch=59
03/12/2022 19:42:38 - INFO - __main__ - Step 130 Global step 130 Train loss 1.195241 on epoch=64
03/12/2022 19:42:44 - INFO - __main__ - Step 140 Global step 140 Train loss 1.008644 on epoch=69
03/12/2022 19:42:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.972707 on epoch=74
03/12/2022 19:42:52 - INFO - __main__ - Global step 150 Train loss 1.149938 ACC 0.0625 on epoch=74
03/12/2022 19:42:59 - INFO - __main__ - Step 160 Global step 160 Train loss 1.017761 on epoch=79
03/12/2022 19:43:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.825451 on epoch=84
03/12/2022 19:43:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.955113 on epoch=89
03/12/2022 19:43:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.895670 on epoch=94
03/12/2022 19:43:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.885812 on epoch=99
03/12/2022 19:43:26 - INFO - __main__ - Global step 200 Train loss 0.915962 ACC 0.09375 on epoch=99
03/12/2022 19:43:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.784340 on epoch=104
03/12/2022 19:43:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.704460 on epoch=109
03/12/2022 19:43:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.679104 on epoch=114
03/12/2022 19:43:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.636001 on epoch=119
03/12/2022 19:43:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.719003 on epoch=124
03/12/2022 19:43:59 - INFO - __main__ - Global step 250 Train loss 0.704582 ACC 0.09375 on epoch=124
03/12/2022 19:44:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.619756 on epoch=129
03/12/2022 19:44:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.639946 on epoch=134
03/12/2022 19:44:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.614448 on epoch=139
03/12/2022 19:44:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.642530 on epoch=144
03/12/2022 19:44:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.519922 on epoch=149
03/12/2022 19:44:31 - INFO - __main__ - Global step 300 Train loss 0.607320 ACC 0.03125 on epoch=149
03/12/2022 19:44:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.616810 on epoch=154
03/12/2022 19:44:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.555551 on epoch=159
03/12/2022 19:44:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.565941 on epoch=164
03/12/2022 19:44:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.563817 on epoch=169
03/12/2022 19:45:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.489163 on epoch=174
03/12/2022 19:45:03 - INFO - __main__ - Global step 350 Train loss 0.558256 ACC 0.0625 on epoch=174
03/12/2022 19:45:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.443806 on epoch=179
03/12/2022 19:45:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.430271 on epoch=184
03/12/2022 19:45:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.437000 on epoch=189
03/12/2022 19:45:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.426969 on epoch=194
03/12/2022 19:45:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.417535 on epoch=199
03/12/2022 19:45:36 - INFO - __main__ - Global step 400 Train loss 0.431116 ACC 0.09375 on epoch=199
03/12/2022 19:45:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.519548 on epoch=204
03/12/2022 19:45:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.454883 on epoch=209
03/12/2022 19:45:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.441300 on epoch=214
03/12/2022 19:45:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.418026 on epoch=219
03/12/2022 19:46:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.401182 on epoch=224
03/12/2022 19:46:08 - INFO - __main__ - Global step 450 Train loss 0.446988 ACC 0.03125 on epoch=224
03/12/2022 19:46:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.398943 on epoch=229
03/12/2022 19:46:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.382124 on epoch=234
03/12/2022 19:46:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.403777 on epoch=239
03/12/2022 19:46:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.396606 on epoch=244
03/12/2022 19:46:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.365497 on epoch=249
03/12/2022 19:46:40 - INFO - __main__ - Global step 500 Train loss 0.389390 ACC 0.03125 on epoch=249
03/12/2022 19:46:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.345768 on epoch=254
03/12/2022 19:46:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.334249 on epoch=259
03/12/2022 19:46:58 - INFO - __main__ - Step 530 Global step 530 Train loss 1.455999 on epoch=264
03/12/2022 19:47:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.334818 on epoch=269
03/12/2022 19:47:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.274643 on epoch=274
03/12/2022 19:47:12 - INFO - __main__ - Global step 550 Train loss 0.549095 ACC 0.03125 on epoch=274
03/12/2022 19:47:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.262825 on epoch=279
03/12/2022 19:47:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.048423 on epoch=284
03/12/2022 19:47:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.027561 on epoch=289
03/12/2022 19:47:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.009313 on epoch=294
03/12/2022 19:47:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.011188 on epoch=299
03/12/2022 19:47:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:47:43 - INFO - __main__ - Printing 3 examples
03/12/2022 19:47:43 - INFO - __main__ -  [race-high] Which statement is NOT true according to the article? (A) In the West, people think laws and customs are rather different. (B) In the West, there is little difference between "sins" and "crimes". (C) An action that is considered a crime in one country may be socially acceptable in another. (D) There is far less use of the civil justice system in Japan than in the United States. [SEP] The idea of "law" exists in every culture. All societies have some kind of law to keep order and to control the interactions of people with those around them. The laws of any culture tell people three things: what they can do (their right), what they must do (their duties), and what they may not do. In addition, there are usually specific types of punishment for those who break the law. Although all societies have laws, not all have the same idea of justice--which is "right" and "wrong" and how "wrong" should be punished. In most Western cultures, it is thought that punishing criminals will prevent them from committing other crimes. Also, it is hoped that the fear of punishment will act as a deterrent  that prevents other people from committing similar crimes; in other words, people who are considering a life of crime will decide against it because of fear of punishment. In most non-Western cultures, by contrast, punishment is not seen as a deterrent. Instead, great importance is placed on restoring balance in the situation. A thief, for example, may be ordered to return the things he has stolen instead of, as in Western societies, spending time in prison. Another difference in the concept of justice lies in various societies' ideas of what laws are. In the West, people consider "laws" quite different from "customs". There is also a great contrast between "sins" (breaking religious laws) and "crimes" (breaking laws of the government). In many non-Western cultures, on the other hand, there is little separation of customs, laws, and religious beliefs; in other cultures, these three may be quite separate from one another, but still very much different from those in the West. For these reasons, an action may be considered a crime in one country, but be socially acceptable in others. For instance, although a thief is viewed as a criminal in much of the world, in a small village where there is considerable communal  living and sharing of objects, the word thief may have little meaning. Someone who has taken something without asking is simply considered an impolite person. Most countries have two kinds of law: criminal and civil. People who have been accused of acts such as murder or theft are heard in the criminal justice system, while civil justice deals with people who are believed to have violated others' rights. The use of the civil system reflects the values of the society in which it exists. In the United States where personal, individual justice is considered very important, civil law has become "big business." There are over 600,000 lawyers in the United States, and many of them keep busy with civil lawsuits; that is, they work for people who want to sue others. If a man falls over a torn rug in a hotel and breaks his arm, for instance, he might decide to sue the hotel owners so that they will pay his medical costs. In a country like Japan, by contrast, there is very little use of the civil justice system. Lawsuits are not very popular in Japan, where social harmony is even more important than individual rights, and where people would rather reach agreement outside court.
03/12/2022 19:47:43 - INFO - __main__ - ['In the West, there is little difference between "sins" and "crimes".']
03/12/2022 19:47:43 - INFO - __main__ -  [race-high] according to the passage, perhaps the followings are our ordinary ways of relaxation for common people except  _  . (A) listening to music (B) playing card (C) going out for fishing (D) boxing match [SEP] "All work and no play makes Jack a dull boy" is a popular saying in the United States. Other countries have similar sayings. It is true that all of us need _ We cannot work all the time if we are going to keep good health and enjoy life. Everyone has his own way of relaxing. Perhaps the most popular way is to take part in sports. there are team sports, such as baseball, basketball, and football. There are individual sports, also, such as golf and swimming. In addition hiking, fishing, skiing, and mountain climbing have a great attraction for people who like to be outdoors. Not everyone who enjoys sports events likes to take part in them. Many people prefer to be onlookers, either watching them on television, or listening to them on the radio. When there is an important baseball game or boxing match it is almost impossible to get tickets; everyone wants to attend. Chess, card-playing, and dancing are forms of indoor recreation enjoyed by many people. It doesn't matter whether we play a fast game of ping-pong, concentrate over the bridge table, or go walking through the woods on a brisk autumn afternoon. It is important for every one to relax from time to time and enjoy some form of recreation.
03/12/2022 19:47:43 - INFO - __main__ - ['boxing match']
03/12/2022 19:47:43 - INFO - __main__ -  [race-high] Before he became a full time writer Mark Twain had not been    _ (A) a printer. (B) a miner (C) a tailor (D) a soldier [SEP] Mark Twain left school when he was twelve. Though he had little school education, he became the most famous writer of his time. He made millions of dollars by writing. His real name was Samuel Langhorne Clements, but he is better known all over the world as Mark Twain, his penname. Mark Twain was born in 1835 and he was not a healthy baby. In fact, he was not expected to live through the first winter. But with his mother's tender care, he managed to survive. As a boy, he caused much trouble to his parents. He used to play jokes on all his friends and neighbors. He didn't like to go to school, and he ran away from home from time to time. He always went in the direction of the nearby Mississippi. He was nearly drowned  nine times. After his father's death in 1847, Mark Twain began to work for a printer, who only provided him with food and clothing. Then, he worked as a river-boat pilot and later joined the army. But shortly after that he became a miner, during this period, he started to write short stories.  Afterwards he became a full-time writer. In 1870, Mark Twain got married. In the years that followed he wrote many books including in 1876, and in 1884, which made him famous, and brought him a great fortune .Unfortunately, Mark Twain got into debts in bad investments  and he had to write large numbers of stories to pay these debts. In 1904, his wife died and then three of their children passed away. He died on April 21, 1910 at the age of 70.
03/12/2022 19:47:43 - INFO - __main__ - ['a tailor']
03/12/2022 19:47:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 19:47:43 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:47:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 19:47:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:47:43 - INFO - __main__ - Printing 3 examples
03/12/2022 19:47:43 - INFO - __main__ -  [race-high] Compared to Emily, Zach is more interested in   . (A) design (B) sports (C) beaches (D) history [SEP] Emily and Zach are confused!  Their parents told them they could choose between Massachusetts and Arizona for their vacation this summer. Emily has always wanted to visit Boston, the capital of Massachusetts. Zach and she both agree that walking along the Freedom Trail would enable them to see Boston's most famous places of historic interest, like the site of the school Ben Franklin attended and the Old State House. If Emily and Zach go to Massachusetts, they could spend a few days at the beaches on Cape Cod. Emily loves body boarding, and Zach is great at body surfing. They both enjoy building sandcastles with their mom and dad. Zach finds learning about Native Americans wonderful and has always wanted to travel along the Apache Trail in Arizona. This mountain highway passes Native American ruins in Tonto National Forest. Emily is not as interested in traveling along this trail as Zach, but they both would like to visit Phoenix, the capital, and then travel to Grand Canyon National Park and Meteor Crater .Zach learned in science class that Meteor Crater is a hole over 4,000 feet wide and 520 feet deep that was created when a huge object from space fell to Earth. The object went so deep that it has never been found. Zach would really like to try to discover it. But Emily thinks if experienced scientists and researchers cannot find it, Zach might as well not even bother to try. The only drawback for Zach and Emily if they choose Arizona would be the heat. It is very hot and dry in this southwestern state. Massachusetts, on the other hand, is in northeastern United States. Here Zach and Emily and their parents could enjoy mild temperatures. Their parents love hot weather, but Zach and Emily do not really like to sweat. How will they ever decide to which state they should travel? If only they could take two trips!
03/12/2022 19:47:43 - INFO - __main__ - ['history']
03/12/2022 19:47:43 - INFO - __main__ -  [race-high] The purpose of writing the text is to   _  . (A) advertise some popular summer programmes (B) encourage readers to have a good time relaxing (C) offer some tips on how to enjoy a learning vacation (D) attract more readers to spend summer time learning [SEP] Is there something that you've always wanted to try but just never had the time?Well,make plans to try it now since you are on summer vacation.Not all vacations call for taking a tour bus to take photos of famous landmarks.Some vacations allow you plenty of opportunities to learn. The most difficult aspect of a learning vacation may be choosing one because the possibilities are endless.If you enjoy cooking,various companies can take you to Italy,France,Spain,Mexico or even Peru.Once there,you can learn to prepare the local cuisine .Trips are often planned to fit in with local food festivals or special events. The term"learning vacation"often brings language to mind.The best way to learn a language is in an environment where it' s spoken.Study Spanish,French or English.Or attempt a more unusual language like Polish,Estonian or Thai.You'll be able to learn about the country and absorb the culture at the same time. If you are fond of sports,you can polish your skills or learn new ones.Golf and tennis schools welcome players of all levels.If you want a bigger thrill,you can learn to surf,go climbing or race cars.It' s even possible to learn the art and techniques of bull fighting while on vacation! You can also discover our inner artist.Many places offer painting classes in different mediums.The scenic locations of the schools offer plenty of subjects that provide inspiration for practice. If you prefer capturing the world on film,take a photography vacation.Travel with a small group to photograph beautiful animals or scenery .You can also practise your technique on people or at historical sights. Once you decide on a vacation,choose a company carefully.Request names of recent customers you can contact,and then ask them for an evaluation.The more you know before you go,the better prepared you'll be.Then go out and learn something!
03/12/2022 19:47:43 - INFO - __main__ - ['offer some tips on how to enjoy a learning vacation']
03/12/2022 19:47:43 - INFO - __main__ -  [race-high] What kind of people will benefit a lot more from this passage? (A) Scientists (B) Teachers (C) Designers (D) Lawyers [SEP] Trends come and go but style is eternal. So what is the latest fashion trend that defines this season? Let's take a look at the fashion trend in 2016 and be the first ones to embrace the latest trend. Head----It seems like everyone will be wearing a bucket hat this season. Bucket hat might sound a bit old-fashioned for some people, but the 90s trend is in season again! Spring and summer are always the seasons for casual clothes, so pairing the bucket hat with casual shorts or skirts would be your outfit to go in this spring and summer in 2016. Bottom--Summer is the season when everyone gets ready for the humid weather. The current season trend is making everyone have a more comfortable summer because wide-leg bottoms is in trend again. Perhaps not everyone likes wide-leg bottoms because this could possible make people look shorter, choosing a high-waist wide-leg bottom and matching it with crop top will definitely make you look taller and trendy in 2016. Dress---Be prepared for embracing the feminine details this season. Ruffles around the sleeve or ruffles all-over the dress will be everyone's favorite in SS 2016. All these little details will transform your look in season. Simple colors like pink and black are designer's favorites in this season too. Shoes---Many people thought wedges are also the shoes to go in spring and summer time, but in SS 2016, very flat flats are in trend again. Shoe designers are trying to have more combinations flats or low-heels with less of gender identity. Accessories----No outfit goes perfect without accessories. Adding statement accessories to your simple and natural outlook this summer is your day-to-night outfit. These jewels are doing to make you feel completed in day time and sparkled at night.
03/12/2022 19:47:43 - INFO - __main__ - ['Designers']
03/12/2022 19:47:43 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:47:43 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:47:43 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 19:47:45 - INFO - __main__ - Global step 600 Train loss 0.071862 ACC 0.03125 on epoch=299
03/12/2022 19:47:45 - INFO - __main__ - save last model!
03/12/2022 19:47:52 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 19:47:53 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 19:47:53 - INFO - __main__ - Printing 3 examples
03/12/2022 19:47:53 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:47:53 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 19:47:53 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:47:53 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 19:47:53 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 19:47:53 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 19:47:53 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:47:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 19:47:56 - INFO - __main__ - Starting training!
03/12/2022 19:47:58 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:48:02 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 19:52:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_87_0.0005_8_predictions.txt
03/12/2022 19:52:25 - INFO - __main__ - ACC on test data: 0.0336
03/12/2022 19:52:25 - INFO - __main__ - prefix=race-high_32_87, lr=0.0005, bsz=8, dev_performance=0.09375, test_performance=0.03361344537815126
03/12/2022 19:52:25 - INFO - __main__ - Running ... prefix=race-high_32_87, lr=0.0003, bsz=8 ...
03/12/2022 19:52:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:52:26 - INFO - __main__ - Printing 3 examples
03/12/2022 19:52:26 - INFO - __main__ -  [race-high] Which statement is NOT true according to the article? (A) In the West, people think laws and customs are rather different. (B) In the West, there is little difference between "sins" and "crimes". (C) An action that is considered a crime in one country may be socially acceptable in another. (D) There is far less use of the civil justice system in Japan than in the United States. [SEP] The idea of "law" exists in every culture. All societies have some kind of law to keep order and to control the interactions of people with those around them. The laws of any culture tell people three things: what they can do (their right), what they must do (their duties), and what they may not do. In addition, there are usually specific types of punishment for those who break the law. Although all societies have laws, not all have the same idea of justice--which is "right" and "wrong" and how "wrong" should be punished. In most Western cultures, it is thought that punishing criminals will prevent them from committing other crimes. Also, it is hoped that the fear of punishment will act as a deterrent  that prevents other people from committing similar crimes; in other words, people who are considering a life of crime will decide against it because of fear of punishment. In most non-Western cultures, by contrast, punishment is not seen as a deterrent. Instead, great importance is placed on restoring balance in the situation. A thief, for example, may be ordered to return the things he has stolen instead of, as in Western societies, spending time in prison. Another difference in the concept of justice lies in various societies' ideas of what laws are. In the West, people consider "laws" quite different from "customs". There is also a great contrast between "sins" (breaking religious laws) and "crimes" (breaking laws of the government). In many non-Western cultures, on the other hand, there is little separation of customs, laws, and religious beliefs; in other cultures, these three may be quite separate from one another, but still very much different from those in the West. For these reasons, an action may be considered a crime in one country, but be socially acceptable in others. For instance, although a thief is viewed as a criminal in much of the world, in a small village where there is considerable communal  living and sharing of objects, the word thief may have little meaning. Someone who has taken something without asking is simply considered an impolite person. Most countries have two kinds of law: criminal and civil. People who have been accused of acts such as murder or theft are heard in the criminal justice system, while civil justice deals with people who are believed to have violated others' rights. The use of the civil system reflects the values of the society in which it exists. In the United States where personal, individual justice is considered very important, civil law has become "big business." There are over 600,000 lawyers in the United States, and many of them keep busy with civil lawsuits; that is, they work for people who want to sue others. If a man falls over a torn rug in a hotel and breaks his arm, for instance, he might decide to sue the hotel owners so that they will pay his medical costs. In a country like Japan, by contrast, there is very little use of the civil justice system. Lawsuits are not very popular in Japan, where social harmony is even more important than individual rights, and where people would rather reach agreement outside court.
03/12/2022 19:52:26 - INFO - __main__ - ['In the West, there is little difference between "sins" and "crimes".']
03/12/2022 19:52:26 - INFO - __main__ -  [race-high] according to the passage, perhaps the followings are our ordinary ways of relaxation for common people except  _  . (A) listening to music (B) playing card (C) going out for fishing (D) boxing match [SEP] "All work and no play makes Jack a dull boy" is a popular saying in the United States. Other countries have similar sayings. It is true that all of us need _ We cannot work all the time if we are going to keep good health and enjoy life. Everyone has his own way of relaxing. Perhaps the most popular way is to take part in sports. there are team sports, such as baseball, basketball, and football. There are individual sports, also, such as golf and swimming. In addition hiking, fishing, skiing, and mountain climbing have a great attraction for people who like to be outdoors. Not everyone who enjoys sports events likes to take part in them. Many people prefer to be onlookers, either watching them on television, or listening to them on the radio. When there is an important baseball game or boxing match it is almost impossible to get tickets; everyone wants to attend. Chess, card-playing, and dancing are forms of indoor recreation enjoyed by many people. It doesn't matter whether we play a fast game of ping-pong, concentrate over the bridge table, or go walking through the woods on a brisk autumn afternoon. It is important for every one to relax from time to time and enjoy some form of recreation.
03/12/2022 19:52:26 - INFO - __main__ - ['boxing match']
03/12/2022 19:52:26 - INFO - __main__ -  [race-high] Before he became a full time writer Mark Twain had not been    _ (A) a printer. (B) a miner (C) a tailor (D) a soldier [SEP] Mark Twain left school when he was twelve. Though he had little school education, he became the most famous writer of his time. He made millions of dollars by writing. His real name was Samuel Langhorne Clements, but he is better known all over the world as Mark Twain, his penname. Mark Twain was born in 1835 and he was not a healthy baby. In fact, he was not expected to live through the first winter. But with his mother's tender care, he managed to survive. As a boy, he caused much trouble to his parents. He used to play jokes on all his friends and neighbors. He didn't like to go to school, and he ran away from home from time to time. He always went in the direction of the nearby Mississippi. He was nearly drowned  nine times. After his father's death in 1847, Mark Twain began to work for a printer, who only provided him with food and clothing. Then, he worked as a river-boat pilot and later joined the army. But shortly after that he became a miner, during this period, he started to write short stories.  Afterwards he became a full-time writer. In 1870, Mark Twain got married. In the years that followed he wrote many books including in 1876, and in 1884, which made him famous, and brought him a great fortune .Unfortunately, Mark Twain got into debts in bad investments  and he had to write large numbers of stories to pay these debts. In 1904, his wife died and then three of their children passed away. He died on April 21, 1910 at the age of 70.
03/12/2022 19:52:26 - INFO - __main__ - ['a tailor']
03/12/2022 19:52:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 19:52:26 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:52:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 19:52:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 19:52:26 - INFO - __main__ - Printing 3 examples
03/12/2022 19:52:26 - INFO - __main__ -  [race-high] Compared to Emily, Zach is more interested in   . (A) design (B) sports (C) beaches (D) history [SEP] Emily and Zach are confused!  Their parents told them they could choose between Massachusetts and Arizona for their vacation this summer. Emily has always wanted to visit Boston, the capital of Massachusetts. Zach and she both agree that walking along the Freedom Trail would enable them to see Boston's most famous places of historic interest, like the site of the school Ben Franklin attended and the Old State House. If Emily and Zach go to Massachusetts, they could spend a few days at the beaches on Cape Cod. Emily loves body boarding, and Zach is great at body surfing. They both enjoy building sandcastles with their mom and dad. Zach finds learning about Native Americans wonderful and has always wanted to travel along the Apache Trail in Arizona. This mountain highway passes Native American ruins in Tonto National Forest. Emily is not as interested in traveling along this trail as Zach, but they both would like to visit Phoenix, the capital, and then travel to Grand Canyon National Park and Meteor Crater .Zach learned in science class that Meteor Crater is a hole over 4,000 feet wide and 520 feet deep that was created when a huge object from space fell to Earth. The object went so deep that it has never been found. Zach would really like to try to discover it. But Emily thinks if experienced scientists and researchers cannot find it, Zach might as well not even bother to try. The only drawback for Zach and Emily if they choose Arizona would be the heat. It is very hot and dry in this southwestern state. Massachusetts, on the other hand, is in northeastern United States. Here Zach and Emily and their parents could enjoy mild temperatures. Their parents love hot weather, but Zach and Emily do not really like to sweat. How will they ever decide to which state they should travel? If only they could take two trips!
03/12/2022 19:52:26 - INFO - __main__ - ['history']
03/12/2022 19:52:26 - INFO - __main__ -  [race-high] The purpose of writing the text is to   _  . (A) advertise some popular summer programmes (B) encourage readers to have a good time relaxing (C) offer some tips on how to enjoy a learning vacation (D) attract more readers to spend summer time learning [SEP] Is there something that you've always wanted to try but just never had the time?Well,make plans to try it now since you are on summer vacation.Not all vacations call for taking a tour bus to take photos of famous landmarks.Some vacations allow you plenty of opportunities to learn. The most difficult aspect of a learning vacation may be choosing one because the possibilities are endless.If you enjoy cooking,various companies can take you to Italy,France,Spain,Mexico or even Peru.Once there,you can learn to prepare the local cuisine .Trips are often planned to fit in with local food festivals or special events. The term"learning vacation"often brings language to mind.The best way to learn a language is in an environment where it' s spoken.Study Spanish,French or English.Or attempt a more unusual language like Polish,Estonian or Thai.You'll be able to learn about the country and absorb the culture at the same time. If you are fond of sports,you can polish your skills or learn new ones.Golf and tennis schools welcome players of all levels.If you want a bigger thrill,you can learn to surf,go climbing or race cars.It' s even possible to learn the art and techniques of bull fighting while on vacation! You can also discover our inner artist.Many places offer painting classes in different mediums.The scenic locations of the schools offer plenty of subjects that provide inspiration for practice. If you prefer capturing the world on film,take a photography vacation.Travel with a small group to photograph beautiful animals or scenery .You can also practise your technique on people or at historical sights. Once you decide on a vacation,choose a company carefully.Request names of recent customers you can contact,and then ask them for an evaluation.The more you know before you go,the better prepared you'll be.Then go out and learn something!
03/12/2022 19:52:26 - INFO - __main__ - ['offer some tips on how to enjoy a learning vacation']
03/12/2022 19:52:26 - INFO - __main__ -  [race-high] What kind of people will benefit a lot more from this passage? (A) Scientists (B) Teachers (C) Designers (D) Lawyers [SEP] Trends come and go but style is eternal. So what is the latest fashion trend that defines this season? Let's take a look at the fashion trend in 2016 and be the first ones to embrace the latest trend. Head----It seems like everyone will be wearing a bucket hat this season. Bucket hat might sound a bit old-fashioned for some people, but the 90s trend is in season again! Spring and summer are always the seasons for casual clothes, so pairing the bucket hat with casual shorts or skirts would be your outfit to go in this spring and summer in 2016. Bottom--Summer is the season when everyone gets ready for the humid weather. The current season trend is making everyone have a more comfortable summer because wide-leg bottoms is in trend again. Perhaps not everyone likes wide-leg bottoms because this could possible make people look shorter, choosing a high-waist wide-leg bottom and matching it with crop top will definitely make you look taller and trendy in 2016. Dress---Be prepared for embracing the feminine details this season. Ruffles around the sleeve or ruffles all-over the dress will be everyone's favorite in SS 2016. All these little details will transform your look in season. Simple colors like pink and black are designer's favorites in this season too. Shoes---Many people thought wedges are also the shoes to go in spring and summer time, but in SS 2016, very flat flats are in trend again. Shoe designers are trying to have more combinations flats or low-heels with less of gender identity. Accessories----No outfit goes perfect without accessories. Adding statement accessories to your simple and natural outlook this summer is your day-to-night outfit. These jewels are doing to make you feel completed in day time and sparkled at night.
03/12/2022 19:52:26 - INFO - __main__ - ['Designers']
03/12/2022 19:52:26 - INFO - __main__ - Tokenizing Input ...
03/12/2022 19:52:26 - INFO - __main__ - Tokenizing Output ...
03/12/2022 19:52:26 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 19:52:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 19:52:37 - INFO - __main__ - Starting training!
03/12/2022 19:52:43 - INFO - __main__ - Step 10 Global step 10 Train loss 20.079554 on epoch=4
03/12/2022 19:52:48 - INFO - __main__ - Step 20 Global step 20 Train loss 14.834513 on epoch=9
03/12/2022 19:52:53 - INFO - __main__ - Step 30 Global step 30 Train loss 13.450559 on epoch=14
03/12/2022 19:52:59 - INFO - __main__ - Step 40 Global step 40 Train loss 12.078174 on epoch=19
03/12/2022 19:53:05 - INFO - __main__ - Step 50 Global step 50 Train loss 11.260469 on epoch=24
03/12/2022 19:53:18 - INFO - __main__ - Global step 50 Train loss 14.340652 ACC 0.0 on epoch=24
03/12/2022 19:53:25 - INFO - __main__ - Step 60 Global step 60 Train loss 9.718728 on epoch=29
03/12/2022 19:53:31 - INFO - __main__ - Step 70 Global step 70 Train loss 7.927769 on epoch=34
03/12/2022 19:53:37 - INFO - __main__ - Step 80 Global step 80 Train loss 5.936326 on epoch=39
03/12/2022 19:53:43 - INFO - __main__ - Step 90 Global step 90 Train loss 4.460035 on epoch=44
03/12/2022 19:53:50 - INFO - __main__ - Step 100 Global step 100 Train loss 3.447886 on epoch=49
03/12/2022 19:54:02 - INFO - __main__ - Global step 100 Train loss 6.298149 ACC 0.0 on epoch=49
03/12/2022 19:54:08 - INFO - __main__ - Step 110 Global step 110 Train loss 2.929758 on epoch=54
03/12/2022 19:54:14 - INFO - __main__ - Step 120 Global step 120 Train loss 2.750191 on epoch=59
03/12/2022 19:54:20 - INFO - __main__ - Step 130 Global step 130 Train loss 2.499280 on epoch=64
03/12/2022 19:54:26 - INFO - __main__ - Step 140 Global step 140 Train loss 2.251355 on epoch=69
03/12/2022 19:54:33 - INFO - __main__ - Step 150 Global step 150 Train loss 2.087495 on epoch=74
03/12/2022 19:54:45 - INFO - __main__ - Global step 150 Train loss 2.503616 ACC 0.0 on epoch=74
03/12/2022 19:54:51 - INFO - __main__ - Step 160 Global step 160 Train loss 1.719675 on epoch=79
03/12/2022 19:54:57 - INFO - __main__ - Step 170 Global step 170 Train loss 1.774343 on epoch=84
03/12/2022 19:55:03 - INFO - __main__ - Step 180 Global step 180 Train loss 1.517360 on epoch=89
03/12/2022 19:55:09 - INFO - __main__ - Step 190 Global step 190 Train loss 1.477118 on epoch=94
03/12/2022 19:55:15 - INFO - __main__ - Step 200 Global step 200 Train loss 1.357232 on epoch=99
03/12/2022 19:55:28 - INFO - __main__ - Global step 200 Train loss 1.569146 ACC 0.0 on epoch=99
03/12/2022 19:55:34 - INFO - __main__ - Step 210 Global step 210 Train loss 1.243387 on epoch=104
03/12/2022 19:55:40 - INFO - __main__ - Step 220 Global step 220 Train loss 1.270973 on epoch=109
03/12/2022 19:55:46 - INFO - __main__ - Step 230 Global step 230 Train loss 1.109428 on epoch=114
03/12/2022 19:55:52 - INFO - __main__ - Step 240 Global step 240 Train loss 1.145187 on epoch=119
03/12/2022 19:55:58 - INFO - __main__ - Step 250 Global step 250 Train loss 1.196470 on epoch=124
03/12/2022 19:56:10 - INFO - __main__ - Global step 250 Train loss 1.193089 ACC 0.0 on epoch=124
03/12/2022 19:56:16 - INFO - __main__ - Step 260 Global step 260 Train loss 1.137868 on epoch=129
03/12/2022 19:56:22 - INFO - __main__ - Step 270 Global step 270 Train loss 1.095707 on epoch=134
03/12/2022 19:56:28 - INFO - __main__ - Step 280 Global step 280 Train loss 1.115001 on epoch=139
03/12/2022 19:56:35 - INFO - __main__ - Step 290 Global step 290 Train loss 1.099849 on epoch=144
03/12/2022 19:56:41 - INFO - __main__ - Step 300 Global step 300 Train loss 1.037194 on epoch=149
03/12/2022 19:56:52 - INFO - __main__ - Global step 300 Train loss 1.097124 ACC 0.0 on epoch=149
03/12/2022 19:56:58 - INFO - __main__ - Step 310 Global step 310 Train loss 1.084193 on epoch=154
03/12/2022 19:57:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.955532 on epoch=159
03/12/2022 19:57:11 - INFO - __main__ - Step 330 Global step 330 Train loss 1.149088 on epoch=164
03/12/2022 19:57:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.993321 on epoch=169
03/12/2022 19:57:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.964605 on epoch=174
03/12/2022 19:57:36 - INFO - __main__ - Global step 350 Train loss 1.029348 ACC 0.0 on epoch=174
03/12/2022 19:57:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.791557 on epoch=179
03/12/2022 19:57:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.802157 on epoch=184
03/12/2022 19:57:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.697707 on epoch=189
03/12/2022 19:58:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.833051 on epoch=194
03/12/2022 19:58:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.826990 on epoch=199
03/12/2022 19:58:16 - INFO - __main__ - Global step 400 Train loss 0.790292 ACC 0.0 on epoch=199
03/12/2022 19:58:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.787437 on epoch=204
03/12/2022 19:58:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.730264 on epoch=209
03/12/2022 19:58:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.753908 on epoch=214
03/12/2022 19:58:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.722198 on epoch=219
03/12/2022 19:58:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.784395 on epoch=224
03/12/2022 19:58:56 - INFO - __main__ - Global step 450 Train loss 0.755640 ACC 0.0 on epoch=224
03/12/2022 19:59:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.724770 on epoch=229
03/12/2022 19:59:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.676309 on epoch=234
03/12/2022 19:59:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.787999 on epoch=239
03/12/2022 19:59:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.685805 on epoch=244
03/12/2022 19:59:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.656872 on epoch=249
03/12/2022 19:59:34 - INFO - __main__ - Global step 500 Train loss 0.706351 ACC 0.0 on epoch=249
03/12/2022 19:59:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.630659 on epoch=254
03/12/2022 19:59:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.738979 on epoch=259
03/12/2022 19:59:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.681192 on epoch=264
03/12/2022 19:59:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.638347 on epoch=269
03/12/2022 20:00:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.684251 on epoch=274
03/12/2022 20:00:10 - INFO - __main__ - Global step 550 Train loss 0.674685 ACC 0.0 on epoch=274
03/12/2022 20:00:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.615688 on epoch=279
03/12/2022 20:00:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.580299 on epoch=284
03/12/2022 20:00:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.601934 on epoch=289
03/12/2022 20:00:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.646419 on epoch=294
03/12/2022 20:00:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.572002 on epoch=299
03/12/2022 20:00:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:00:43 - INFO - __main__ - Printing 3 examples
03/12/2022 20:00:43 - INFO - __main__ -  [race-high] Which statement is NOT true according to the article? (A) In the West, people think laws and customs are rather different. (B) In the West, there is little difference between "sins" and "crimes". (C) An action that is considered a crime in one country may be socially acceptable in another. (D) There is far less use of the civil justice system in Japan than in the United States. [SEP] The idea of "law" exists in every culture. All societies have some kind of law to keep order and to control the interactions of people with those around them. The laws of any culture tell people three things: what they can do (their right), what they must do (their duties), and what they may not do. In addition, there are usually specific types of punishment for those who break the law. Although all societies have laws, not all have the same idea of justice--which is "right" and "wrong" and how "wrong" should be punished. In most Western cultures, it is thought that punishing criminals will prevent them from committing other crimes. Also, it is hoped that the fear of punishment will act as a deterrent  that prevents other people from committing similar crimes; in other words, people who are considering a life of crime will decide against it because of fear of punishment. In most non-Western cultures, by contrast, punishment is not seen as a deterrent. Instead, great importance is placed on restoring balance in the situation. A thief, for example, may be ordered to return the things he has stolen instead of, as in Western societies, spending time in prison. Another difference in the concept of justice lies in various societies' ideas of what laws are. In the West, people consider "laws" quite different from "customs". There is also a great contrast between "sins" (breaking religious laws) and "crimes" (breaking laws of the government). In many non-Western cultures, on the other hand, there is little separation of customs, laws, and religious beliefs; in other cultures, these three may be quite separate from one another, but still very much different from those in the West. For these reasons, an action may be considered a crime in one country, but be socially acceptable in others. For instance, although a thief is viewed as a criminal in much of the world, in a small village where there is considerable communal  living and sharing of objects, the word thief may have little meaning. Someone who has taken something without asking is simply considered an impolite person. Most countries have two kinds of law: criminal and civil. People who have been accused of acts such as murder or theft are heard in the criminal justice system, while civil justice deals with people who are believed to have violated others' rights. The use of the civil system reflects the values of the society in which it exists. In the United States where personal, individual justice is considered very important, civil law has become "big business." There are over 600,000 lawyers in the United States, and many of them keep busy with civil lawsuits; that is, they work for people who want to sue others. If a man falls over a torn rug in a hotel and breaks his arm, for instance, he might decide to sue the hotel owners so that they will pay his medical costs. In a country like Japan, by contrast, there is very little use of the civil justice system. Lawsuits are not very popular in Japan, where social harmony is even more important than individual rights, and where people would rather reach agreement outside court.
03/12/2022 20:00:43 - INFO - __main__ - ['In the West, there is little difference between "sins" and "crimes".']
03/12/2022 20:00:43 - INFO - __main__ -  [race-high] according to the passage, perhaps the followings are our ordinary ways of relaxation for common people except  _  . (A) listening to music (B) playing card (C) going out for fishing (D) boxing match [SEP] "All work and no play makes Jack a dull boy" is a popular saying in the United States. Other countries have similar sayings. It is true that all of us need _ We cannot work all the time if we are going to keep good health and enjoy life. Everyone has his own way of relaxing. Perhaps the most popular way is to take part in sports. there are team sports, such as baseball, basketball, and football. There are individual sports, also, such as golf and swimming. In addition hiking, fishing, skiing, and mountain climbing have a great attraction for people who like to be outdoors. Not everyone who enjoys sports events likes to take part in them. Many people prefer to be onlookers, either watching them on television, or listening to them on the radio. When there is an important baseball game or boxing match it is almost impossible to get tickets; everyone wants to attend. Chess, card-playing, and dancing are forms of indoor recreation enjoyed by many people. It doesn't matter whether we play a fast game of ping-pong, concentrate over the bridge table, or go walking through the woods on a brisk autumn afternoon. It is important for every one to relax from time to time and enjoy some form of recreation.
03/12/2022 20:00:43 - INFO - __main__ - ['boxing match']
03/12/2022 20:00:43 - INFO - __main__ -  [race-high] Before he became a full time writer Mark Twain had not been    _ (A) a printer. (B) a miner (C) a tailor (D) a soldier [SEP] Mark Twain left school when he was twelve. Though he had little school education, he became the most famous writer of his time. He made millions of dollars by writing. His real name was Samuel Langhorne Clements, but he is better known all over the world as Mark Twain, his penname. Mark Twain was born in 1835 and he was not a healthy baby. In fact, he was not expected to live through the first winter. But with his mother's tender care, he managed to survive. As a boy, he caused much trouble to his parents. He used to play jokes on all his friends and neighbors. He didn't like to go to school, and he ran away from home from time to time. He always went in the direction of the nearby Mississippi. He was nearly drowned  nine times. After his father's death in 1847, Mark Twain began to work for a printer, who only provided him with food and clothing. Then, he worked as a river-boat pilot and later joined the army. But shortly after that he became a miner, during this period, he started to write short stories.  Afterwards he became a full-time writer. In 1870, Mark Twain got married. In the years that followed he wrote many books including in 1876, and in 1884, which made him famous, and brought him a great fortune .Unfortunately, Mark Twain got into debts in bad investments  and he had to write large numbers of stories to pay these debts. In 1904, his wife died and then three of their children passed away. He died on April 21, 1910 at the age of 70.
03/12/2022 20:00:43 - INFO - __main__ - ['a tailor']
03/12/2022 20:00:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 20:00:43 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:00:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 20:00:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:00:43 - INFO - __main__ - Printing 3 examples
03/12/2022 20:00:43 - INFO - __main__ -  [race-high] Compared to Emily, Zach is more interested in   . (A) design (B) sports (C) beaches (D) history [SEP] Emily and Zach are confused!  Their parents told them they could choose between Massachusetts and Arizona for their vacation this summer. Emily has always wanted to visit Boston, the capital of Massachusetts. Zach and she both agree that walking along the Freedom Trail would enable them to see Boston's most famous places of historic interest, like the site of the school Ben Franklin attended and the Old State House. If Emily and Zach go to Massachusetts, they could spend a few days at the beaches on Cape Cod. Emily loves body boarding, and Zach is great at body surfing. They both enjoy building sandcastles with their mom and dad. Zach finds learning about Native Americans wonderful and has always wanted to travel along the Apache Trail in Arizona. This mountain highway passes Native American ruins in Tonto National Forest. Emily is not as interested in traveling along this trail as Zach, but they both would like to visit Phoenix, the capital, and then travel to Grand Canyon National Park and Meteor Crater .Zach learned in science class that Meteor Crater is a hole over 4,000 feet wide and 520 feet deep that was created when a huge object from space fell to Earth. The object went so deep that it has never been found. Zach would really like to try to discover it. But Emily thinks if experienced scientists and researchers cannot find it, Zach might as well not even bother to try. The only drawback for Zach and Emily if they choose Arizona would be the heat. It is very hot and dry in this southwestern state. Massachusetts, on the other hand, is in northeastern United States. Here Zach and Emily and their parents could enjoy mild temperatures. Their parents love hot weather, but Zach and Emily do not really like to sweat. How will they ever decide to which state they should travel? If only they could take two trips!
03/12/2022 20:00:43 - INFO - __main__ - ['history']
03/12/2022 20:00:43 - INFO - __main__ -  [race-high] The purpose of writing the text is to   _  . (A) advertise some popular summer programmes (B) encourage readers to have a good time relaxing (C) offer some tips on how to enjoy a learning vacation (D) attract more readers to spend summer time learning [SEP] Is there something that you've always wanted to try but just never had the time?Well,make plans to try it now since you are on summer vacation.Not all vacations call for taking a tour bus to take photos of famous landmarks.Some vacations allow you plenty of opportunities to learn. The most difficult aspect of a learning vacation may be choosing one because the possibilities are endless.If you enjoy cooking,various companies can take you to Italy,France,Spain,Mexico or even Peru.Once there,you can learn to prepare the local cuisine .Trips are often planned to fit in with local food festivals or special events. The term"learning vacation"often brings language to mind.The best way to learn a language is in an environment where it' s spoken.Study Spanish,French or English.Or attempt a more unusual language like Polish,Estonian or Thai.You'll be able to learn about the country and absorb the culture at the same time. If you are fond of sports,you can polish your skills or learn new ones.Golf and tennis schools welcome players of all levels.If you want a bigger thrill,you can learn to surf,go climbing or race cars.It' s even possible to learn the art and techniques of bull fighting while on vacation! You can also discover our inner artist.Many places offer painting classes in different mediums.The scenic locations of the schools offer plenty of subjects that provide inspiration for practice. If you prefer capturing the world on film,take a photography vacation.Travel with a small group to photograph beautiful animals or scenery .You can also practise your technique on people or at historical sights. Once you decide on a vacation,choose a company carefully.Request names of recent customers you can contact,and then ask them for an evaluation.The more you know before you go,the better prepared you'll be.Then go out and learn something!
03/12/2022 20:00:43 - INFO - __main__ - ['offer some tips on how to enjoy a learning vacation']
03/12/2022 20:00:43 - INFO - __main__ -  [race-high] What kind of people will benefit a lot more from this passage? (A) Scientists (B) Teachers (C) Designers (D) Lawyers [SEP] Trends come and go but style is eternal. So what is the latest fashion trend that defines this season? Let's take a look at the fashion trend in 2016 and be the first ones to embrace the latest trend. Head----It seems like everyone will be wearing a bucket hat this season. Bucket hat might sound a bit old-fashioned for some people, but the 90s trend is in season again! Spring and summer are always the seasons for casual clothes, so pairing the bucket hat with casual shorts or skirts would be your outfit to go in this spring and summer in 2016. Bottom--Summer is the season when everyone gets ready for the humid weather. The current season trend is making everyone have a more comfortable summer because wide-leg bottoms is in trend again. Perhaps not everyone likes wide-leg bottoms because this could possible make people look shorter, choosing a high-waist wide-leg bottom and matching it with crop top will definitely make you look taller and trendy in 2016. Dress---Be prepared for embracing the feminine details this season. Ruffles around the sleeve or ruffles all-over the dress will be everyone's favorite in SS 2016. All these little details will transform your look in season. Simple colors like pink and black are designer's favorites in this season too. Shoes---Many people thought wedges are also the shoes to go in spring and summer time, but in SS 2016, very flat flats are in trend again. Shoe designers are trying to have more combinations flats or low-heels with less of gender identity. Accessories----No outfit goes perfect without accessories. Adding statement accessories to your simple and natural outlook this summer is your day-to-night outfit. These jewels are doing to make you feel completed in day time and sparkled at night.
03/12/2022 20:00:43 - INFO - __main__ - ['Designers']
03/12/2022 20:00:43 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:00:43 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:00:43 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 20:00:46 - INFO - __main__ - Global step 600 Train loss 0.603268 ACC 0.0 on epoch=299
03/12/2022 20:00:46 - INFO - __main__ - save last model!
03/12/2022 20:00:53 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 20:00:53 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 20:00:53 - INFO - __main__ - Printing 3 examples
03/12/2022 20:00:53 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 20:00:53 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 20:00:53 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 20:00:53 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 20:00:53 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 20:00:53 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 20:00:53 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:00:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 20:00:55 - INFO - __main__ - Starting training!
03/12/2022 20:00:59 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:01:03 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 20:24:14 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_87_0.0003_8_predictions.txt
03/12/2022 20:24:14 - INFO - __main__ - ACC on test data: 0.0000
03/12/2022 20:24:14 - INFO - __main__ - prefix=race-high_32_87, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.0
03/12/2022 20:24:14 - INFO - __main__ - Running ... prefix=race-high_32_87, lr=0.0002, bsz=8 ...
03/12/2022 20:24:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:24:15 - INFO - __main__ - Printing 3 examples
03/12/2022 20:24:15 - INFO - __main__ -  [race-high] Which statement is NOT true according to the article? (A) In the West, people think laws and customs are rather different. (B) In the West, there is little difference between "sins" and "crimes". (C) An action that is considered a crime in one country may be socially acceptable in another. (D) There is far less use of the civil justice system in Japan than in the United States. [SEP] The idea of "law" exists in every culture. All societies have some kind of law to keep order and to control the interactions of people with those around them. The laws of any culture tell people three things: what they can do (their right), what they must do (their duties), and what they may not do. In addition, there are usually specific types of punishment for those who break the law. Although all societies have laws, not all have the same idea of justice--which is "right" and "wrong" and how "wrong" should be punished. In most Western cultures, it is thought that punishing criminals will prevent them from committing other crimes. Also, it is hoped that the fear of punishment will act as a deterrent  that prevents other people from committing similar crimes; in other words, people who are considering a life of crime will decide against it because of fear of punishment. In most non-Western cultures, by contrast, punishment is not seen as a deterrent. Instead, great importance is placed on restoring balance in the situation. A thief, for example, may be ordered to return the things he has stolen instead of, as in Western societies, spending time in prison. Another difference in the concept of justice lies in various societies' ideas of what laws are. In the West, people consider "laws" quite different from "customs". There is also a great contrast between "sins" (breaking religious laws) and "crimes" (breaking laws of the government). In many non-Western cultures, on the other hand, there is little separation of customs, laws, and religious beliefs; in other cultures, these three may be quite separate from one another, but still very much different from those in the West. For these reasons, an action may be considered a crime in one country, but be socially acceptable in others. For instance, although a thief is viewed as a criminal in much of the world, in a small village where there is considerable communal  living and sharing of objects, the word thief may have little meaning. Someone who has taken something without asking is simply considered an impolite person. Most countries have two kinds of law: criminal and civil. People who have been accused of acts such as murder or theft are heard in the criminal justice system, while civil justice deals with people who are believed to have violated others' rights. The use of the civil system reflects the values of the society in which it exists. In the United States where personal, individual justice is considered very important, civil law has become "big business." There are over 600,000 lawyers in the United States, and many of them keep busy with civil lawsuits; that is, they work for people who want to sue others. If a man falls over a torn rug in a hotel and breaks his arm, for instance, he might decide to sue the hotel owners so that they will pay his medical costs. In a country like Japan, by contrast, there is very little use of the civil justice system. Lawsuits are not very popular in Japan, where social harmony is even more important than individual rights, and where people would rather reach agreement outside court.
03/12/2022 20:24:15 - INFO - __main__ - ['In the West, there is little difference between "sins" and "crimes".']
03/12/2022 20:24:15 - INFO - __main__ -  [race-high] according to the passage, perhaps the followings are our ordinary ways of relaxation for common people except  _  . (A) listening to music (B) playing card (C) going out for fishing (D) boxing match [SEP] "All work and no play makes Jack a dull boy" is a popular saying in the United States. Other countries have similar sayings. It is true that all of us need _ We cannot work all the time if we are going to keep good health and enjoy life. Everyone has his own way of relaxing. Perhaps the most popular way is to take part in sports. there are team sports, such as baseball, basketball, and football. There are individual sports, also, such as golf and swimming. In addition hiking, fishing, skiing, and mountain climbing have a great attraction for people who like to be outdoors. Not everyone who enjoys sports events likes to take part in them. Many people prefer to be onlookers, either watching them on television, or listening to them on the radio. When there is an important baseball game or boxing match it is almost impossible to get tickets; everyone wants to attend. Chess, card-playing, and dancing are forms of indoor recreation enjoyed by many people. It doesn't matter whether we play a fast game of ping-pong, concentrate over the bridge table, or go walking through the woods on a brisk autumn afternoon. It is important for every one to relax from time to time and enjoy some form of recreation.
03/12/2022 20:24:15 - INFO - __main__ - ['boxing match']
03/12/2022 20:24:15 - INFO - __main__ -  [race-high] Before he became a full time writer Mark Twain had not been    _ (A) a printer. (B) a miner (C) a tailor (D) a soldier [SEP] Mark Twain left school when he was twelve. Though he had little school education, he became the most famous writer of his time. He made millions of dollars by writing. His real name was Samuel Langhorne Clements, but he is better known all over the world as Mark Twain, his penname. Mark Twain was born in 1835 and he was not a healthy baby. In fact, he was not expected to live through the first winter. But with his mother's tender care, he managed to survive. As a boy, he caused much trouble to his parents. He used to play jokes on all his friends and neighbors. He didn't like to go to school, and he ran away from home from time to time. He always went in the direction of the nearby Mississippi. He was nearly drowned  nine times. After his father's death in 1847, Mark Twain began to work for a printer, who only provided him with food and clothing. Then, he worked as a river-boat pilot and later joined the army. But shortly after that he became a miner, during this period, he started to write short stories.  Afterwards he became a full-time writer. In 1870, Mark Twain got married. In the years that followed he wrote many books including in 1876, and in 1884, which made him famous, and brought him a great fortune .Unfortunately, Mark Twain got into debts in bad investments  and he had to write large numbers of stories to pay these debts. In 1904, his wife died and then three of their children passed away. He died on April 21, 1910 at the age of 70.
03/12/2022 20:24:15 - INFO - __main__ - ['a tailor']
03/12/2022 20:24:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 20:24:15 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:24:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 20:24:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:24:15 - INFO - __main__ - Printing 3 examples
03/12/2022 20:24:15 - INFO - __main__ -  [race-high] Compared to Emily, Zach is more interested in   . (A) design (B) sports (C) beaches (D) history [SEP] Emily and Zach are confused!  Their parents told them they could choose between Massachusetts and Arizona for their vacation this summer. Emily has always wanted to visit Boston, the capital of Massachusetts. Zach and she both agree that walking along the Freedom Trail would enable them to see Boston's most famous places of historic interest, like the site of the school Ben Franklin attended and the Old State House. If Emily and Zach go to Massachusetts, they could spend a few days at the beaches on Cape Cod. Emily loves body boarding, and Zach is great at body surfing. They both enjoy building sandcastles with their mom and dad. Zach finds learning about Native Americans wonderful and has always wanted to travel along the Apache Trail in Arizona. This mountain highway passes Native American ruins in Tonto National Forest. Emily is not as interested in traveling along this trail as Zach, but they both would like to visit Phoenix, the capital, and then travel to Grand Canyon National Park and Meteor Crater .Zach learned in science class that Meteor Crater is a hole over 4,000 feet wide and 520 feet deep that was created when a huge object from space fell to Earth. The object went so deep that it has never been found. Zach would really like to try to discover it. But Emily thinks if experienced scientists and researchers cannot find it, Zach might as well not even bother to try. The only drawback for Zach and Emily if they choose Arizona would be the heat. It is very hot and dry in this southwestern state. Massachusetts, on the other hand, is in northeastern United States. Here Zach and Emily and their parents could enjoy mild temperatures. Their parents love hot weather, but Zach and Emily do not really like to sweat. How will they ever decide to which state they should travel? If only they could take two trips!
03/12/2022 20:24:15 - INFO - __main__ - ['history']
03/12/2022 20:24:15 - INFO - __main__ -  [race-high] The purpose of writing the text is to   _  . (A) advertise some popular summer programmes (B) encourage readers to have a good time relaxing (C) offer some tips on how to enjoy a learning vacation (D) attract more readers to spend summer time learning [SEP] Is there something that you've always wanted to try but just never had the time?Well,make plans to try it now since you are on summer vacation.Not all vacations call for taking a tour bus to take photos of famous landmarks.Some vacations allow you plenty of opportunities to learn. The most difficult aspect of a learning vacation may be choosing one because the possibilities are endless.If you enjoy cooking,various companies can take you to Italy,France,Spain,Mexico or even Peru.Once there,you can learn to prepare the local cuisine .Trips are often planned to fit in with local food festivals or special events. The term"learning vacation"often brings language to mind.The best way to learn a language is in an environment where it' s spoken.Study Spanish,French or English.Or attempt a more unusual language like Polish,Estonian or Thai.You'll be able to learn about the country and absorb the culture at the same time. If you are fond of sports,you can polish your skills or learn new ones.Golf and tennis schools welcome players of all levels.If you want a bigger thrill,you can learn to surf,go climbing or race cars.It' s even possible to learn the art and techniques of bull fighting while on vacation! You can also discover our inner artist.Many places offer painting classes in different mediums.The scenic locations of the schools offer plenty of subjects that provide inspiration for practice. If you prefer capturing the world on film,take a photography vacation.Travel with a small group to photograph beautiful animals or scenery .You can also practise your technique on people or at historical sights. Once you decide on a vacation,choose a company carefully.Request names of recent customers you can contact,and then ask them for an evaluation.The more you know before you go,the better prepared you'll be.Then go out and learn something!
03/12/2022 20:24:15 - INFO - __main__ - ['offer some tips on how to enjoy a learning vacation']
03/12/2022 20:24:15 - INFO - __main__ -  [race-high] What kind of people will benefit a lot more from this passage? (A) Scientists (B) Teachers (C) Designers (D) Lawyers [SEP] Trends come and go but style is eternal. So what is the latest fashion trend that defines this season? Let's take a look at the fashion trend in 2016 and be the first ones to embrace the latest trend. Head----It seems like everyone will be wearing a bucket hat this season. Bucket hat might sound a bit old-fashioned for some people, but the 90s trend is in season again! Spring and summer are always the seasons for casual clothes, so pairing the bucket hat with casual shorts or skirts would be your outfit to go in this spring and summer in 2016. Bottom--Summer is the season when everyone gets ready for the humid weather. The current season trend is making everyone have a more comfortable summer because wide-leg bottoms is in trend again. Perhaps not everyone likes wide-leg bottoms because this could possible make people look shorter, choosing a high-waist wide-leg bottom and matching it with crop top will definitely make you look taller and trendy in 2016. Dress---Be prepared for embracing the feminine details this season. Ruffles around the sleeve or ruffles all-over the dress will be everyone's favorite in SS 2016. All these little details will transform your look in season. Simple colors like pink and black are designer's favorites in this season too. Shoes---Many people thought wedges are also the shoes to go in spring and summer time, but in SS 2016, very flat flats are in trend again. Shoe designers are trying to have more combinations flats or low-heels with less of gender identity. Accessories----No outfit goes perfect without accessories. Adding statement accessories to your simple and natural outlook this summer is your day-to-night outfit. These jewels are doing to make you feel completed in day time and sparkled at night.
03/12/2022 20:24:15 - INFO - __main__ - ['Designers']
03/12/2022 20:24:15 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:24:15 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:24:15 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 20:24:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 20:24:26 - INFO - __main__ - Starting training!
03/12/2022 20:24:32 - INFO - __main__ - Step 10 Global step 10 Train loss 19.291590 on epoch=4
03/12/2022 20:24:38 - INFO - __main__ - Step 20 Global step 20 Train loss 16.062572 on epoch=9
03/12/2022 20:24:44 - INFO - __main__ - Step 30 Global step 30 Train loss 8.706273 on epoch=14
03/12/2022 20:24:50 - INFO - __main__ - Step 40 Global step 40 Train loss 6.310134 on epoch=19
03/12/2022 20:24:56 - INFO - __main__ - Step 50 Global step 50 Train loss 5.533732 on epoch=24
03/12/2022 20:24:58 - INFO - __main__ - Global step 50 Train loss 11.180860 ACC 0.03125 on epoch=24
03/12/2022 20:25:05 - INFO - __main__ - Step 60 Global step 60 Train loss 4.675153 on epoch=29
03/12/2022 20:25:11 - INFO - __main__ - Step 70 Global step 70 Train loss 5.853807 on epoch=34
03/12/2022 20:25:17 - INFO - __main__ - Step 80 Global step 80 Train loss 6.249072 on epoch=39
03/12/2022 20:25:23 - INFO - __main__ - Step 90 Global step 90 Train loss 4.618377 on epoch=44
03/12/2022 20:25:29 - INFO - __main__ - Step 100 Global step 100 Train loss 3.867321 on epoch=49
03/12/2022 20:25:32 - INFO - __main__ - Global step 100 Train loss 5.052746 ACC 0.0 on epoch=49
03/12/2022 20:25:38 - INFO - __main__ - Step 110 Global step 110 Train loss 3.508418 on epoch=54
03/12/2022 20:25:44 - INFO - __main__ - Step 120 Global step 120 Train loss 3.177025 on epoch=59
03/12/2022 20:25:50 - INFO - __main__ - Step 130 Global step 130 Train loss 3.204963 on epoch=64
03/12/2022 20:25:57 - INFO - __main__ - Step 140 Global step 140 Train loss 3.072377 on epoch=69
03/12/2022 20:26:03 - INFO - __main__ - Step 150 Global step 150 Train loss 2.881714 on epoch=74
03/12/2022 20:26:05 - INFO - __main__ - Global step 150 Train loss 3.168900 ACC 0.0 on epoch=74
03/12/2022 20:26:11 - INFO - __main__ - Step 160 Global step 160 Train loss 2.705018 on epoch=79
03/12/2022 20:26:17 - INFO - __main__ - Step 170 Global step 170 Train loss 2.419936 on epoch=84
03/12/2022 20:26:24 - INFO - __main__ - Step 180 Global step 180 Train loss 2.269128 on epoch=89
03/12/2022 20:26:30 - INFO - __main__ - Step 190 Global step 190 Train loss 2.212564 on epoch=94
03/12/2022 20:26:36 - INFO - __main__ - Step 200 Global step 200 Train loss 2.190428 on epoch=99
03/12/2022 20:26:39 - INFO - __main__ - Global step 200 Train loss 2.359415 ACC 0.0 on epoch=99
03/12/2022 20:26:45 - INFO - __main__ - Step 210 Global step 210 Train loss 1.872373 on epoch=104
03/12/2022 20:26:51 - INFO - __main__ - Step 220 Global step 220 Train loss 1.787391 on epoch=109
03/12/2022 20:26:57 - INFO - __main__ - Step 230 Global step 230 Train loss 1.782351 on epoch=114
03/12/2022 20:27:04 - INFO - __main__ - Step 240 Global step 240 Train loss 1.571998 on epoch=119
03/12/2022 20:27:10 - INFO - __main__ - Step 250 Global step 250 Train loss 1.458860 on epoch=124
03/12/2022 20:27:12 - INFO - __main__ - Global step 250 Train loss 1.694595 ACC 0.0 on epoch=124
03/12/2022 20:27:18 - INFO - __main__ - Step 260 Global step 260 Train loss 1.195495 on epoch=129
03/12/2022 20:27:24 - INFO - __main__ - Step 270 Global step 270 Train loss 1.357956 on epoch=134
03/12/2022 20:27:31 - INFO - __main__ - Step 280 Global step 280 Train loss 1.297731 on epoch=139
03/12/2022 20:27:37 - INFO - __main__ - Step 290 Global step 290 Train loss 1.220645 on epoch=144
03/12/2022 20:27:43 - INFO - __main__ - Step 300 Global step 300 Train loss 1.176060 on epoch=149
03/12/2022 20:27:46 - INFO - __main__ - Global step 300 Train loss 1.249577 ACC 0.0 on epoch=149
03/12/2022 20:27:52 - INFO - __main__ - Step 310 Global step 310 Train loss 1.148536 on epoch=154
03/12/2022 20:27:58 - INFO - __main__ - Step 320 Global step 320 Train loss 1.125451 on epoch=159
03/12/2022 20:28:04 - INFO - __main__ - Step 330 Global step 330 Train loss 1.149261 on epoch=164
03/12/2022 20:28:10 - INFO - __main__ - Step 340 Global step 340 Train loss 1.130448 on epoch=169
03/12/2022 20:28:17 - INFO - __main__ - Step 350 Global step 350 Train loss 1.041955 on epoch=174
03/12/2022 20:28:19 - INFO - __main__ - Global step 350 Train loss 1.119130 ACC 0.0 on epoch=174
03/12/2022 20:28:25 - INFO - __main__ - Step 360 Global step 360 Train loss 1.159829 on epoch=179
03/12/2022 20:28:31 - INFO - __main__ - Step 370 Global step 370 Train loss 1.113239 on epoch=184
03/12/2022 20:28:37 - INFO - __main__ - Step 380 Global step 380 Train loss 1.117308 on epoch=189
03/12/2022 20:28:44 - INFO - __main__ - Step 390 Global step 390 Train loss 1.042119 on epoch=194
03/12/2022 20:28:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.990000 on epoch=199
03/12/2022 20:28:52 - INFO - __main__ - Global step 400 Train loss 1.084499 ACC 0.0 on epoch=199
03/12/2022 20:28:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.982585 on epoch=204
03/12/2022 20:29:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.896250 on epoch=209
03/12/2022 20:29:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.918433 on epoch=214
03/12/2022 20:29:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.988101 on epoch=219
03/12/2022 20:29:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.838983 on epoch=224
03/12/2022 20:29:26 - INFO - __main__ - Global step 450 Train loss 0.924871 ACC 0.0 on epoch=224
03/12/2022 20:29:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.949433 on epoch=229
03/12/2022 20:29:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.962018 on epoch=234
03/12/2022 20:29:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.821654 on epoch=239
03/12/2022 20:29:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.798382 on epoch=244
03/12/2022 20:29:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.987105 on epoch=249
03/12/2022 20:29:59 - INFO - __main__ - Global step 500 Train loss 0.903719 ACC 0.0 on epoch=249
03/12/2022 20:30:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.793593 on epoch=254
03/12/2022 20:30:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.892951 on epoch=259
03/12/2022 20:30:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.802053 on epoch=264
03/12/2022 20:30:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.851231 on epoch=269
03/12/2022 20:30:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.729150 on epoch=274
03/12/2022 20:30:33 - INFO - __main__ - Global step 550 Train loss 0.813796 ACC 0.03125 on epoch=274
03/12/2022 20:30:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.766344 on epoch=279
03/12/2022 20:30:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.817368 on epoch=284
03/12/2022 20:30:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.803844 on epoch=289
03/12/2022 20:30:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.631962 on epoch=294
03/12/2022 20:31:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.744905 on epoch=299
03/12/2022 20:31:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:31:05 - INFO - __main__ - Printing 3 examples
03/12/2022 20:31:05 - INFO - __main__ -  [race-high] Which statement is NOT true according to the article? (A) In the West, people think laws and customs are rather different. (B) In the West, there is little difference between "sins" and "crimes". (C) An action that is considered a crime in one country may be socially acceptable in another. (D) There is far less use of the civil justice system in Japan than in the United States. [SEP] The idea of "law" exists in every culture. All societies have some kind of law to keep order and to control the interactions of people with those around them. The laws of any culture tell people three things: what they can do (their right), what they must do (their duties), and what they may not do. In addition, there are usually specific types of punishment for those who break the law. Although all societies have laws, not all have the same idea of justice--which is "right" and "wrong" and how "wrong" should be punished. In most Western cultures, it is thought that punishing criminals will prevent them from committing other crimes. Also, it is hoped that the fear of punishment will act as a deterrent  that prevents other people from committing similar crimes; in other words, people who are considering a life of crime will decide against it because of fear of punishment. In most non-Western cultures, by contrast, punishment is not seen as a deterrent. Instead, great importance is placed on restoring balance in the situation. A thief, for example, may be ordered to return the things he has stolen instead of, as in Western societies, spending time in prison. Another difference in the concept of justice lies in various societies' ideas of what laws are. In the West, people consider "laws" quite different from "customs". There is also a great contrast between "sins" (breaking religious laws) and "crimes" (breaking laws of the government). In many non-Western cultures, on the other hand, there is little separation of customs, laws, and religious beliefs; in other cultures, these three may be quite separate from one another, but still very much different from those in the West. For these reasons, an action may be considered a crime in one country, but be socially acceptable in others. For instance, although a thief is viewed as a criminal in much of the world, in a small village where there is considerable communal  living and sharing of objects, the word thief may have little meaning. Someone who has taken something without asking is simply considered an impolite person. Most countries have two kinds of law: criminal and civil. People who have been accused of acts such as murder or theft are heard in the criminal justice system, while civil justice deals with people who are believed to have violated others' rights. The use of the civil system reflects the values of the society in which it exists. In the United States where personal, individual justice is considered very important, civil law has become "big business." There are over 600,000 lawyers in the United States, and many of them keep busy with civil lawsuits; that is, they work for people who want to sue others. If a man falls over a torn rug in a hotel and breaks his arm, for instance, he might decide to sue the hotel owners so that they will pay his medical costs. In a country like Japan, by contrast, there is very little use of the civil justice system. Lawsuits are not very popular in Japan, where social harmony is even more important than individual rights, and where people would rather reach agreement outside court.
03/12/2022 20:31:05 - INFO - __main__ - ['In the West, there is little difference between "sins" and "crimes".']
03/12/2022 20:31:05 - INFO - __main__ -  [race-high] according to the passage, perhaps the followings are our ordinary ways of relaxation for common people except  _  . (A) listening to music (B) playing card (C) going out for fishing (D) boxing match [SEP] "All work and no play makes Jack a dull boy" is a popular saying in the United States. Other countries have similar sayings. It is true that all of us need _ We cannot work all the time if we are going to keep good health and enjoy life. Everyone has his own way of relaxing. Perhaps the most popular way is to take part in sports. there are team sports, such as baseball, basketball, and football. There are individual sports, also, such as golf and swimming. In addition hiking, fishing, skiing, and mountain climbing have a great attraction for people who like to be outdoors. Not everyone who enjoys sports events likes to take part in them. Many people prefer to be onlookers, either watching them on television, or listening to them on the radio. When there is an important baseball game or boxing match it is almost impossible to get tickets; everyone wants to attend. Chess, card-playing, and dancing are forms of indoor recreation enjoyed by many people. It doesn't matter whether we play a fast game of ping-pong, concentrate over the bridge table, or go walking through the woods on a brisk autumn afternoon. It is important for every one to relax from time to time and enjoy some form of recreation.
03/12/2022 20:31:05 - INFO - __main__ - ['boxing match']
03/12/2022 20:31:05 - INFO - __main__ -  [race-high] Before he became a full time writer Mark Twain had not been    _ (A) a printer. (B) a miner (C) a tailor (D) a soldier [SEP] Mark Twain left school when he was twelve. Though he had little school education, he became the most famous writer of his time. He made millions of dollars by writing. His real name was Samuel Langhorne Clements, but he is better known all over the world as Mark Twain, his penname. Mark Twain was born in 1835 and he was not a healthy baby. In fact, he was not expected to live through the first winter. But with his mother's tender care, he managed to survive. As a boy, he caused much trouble to his parents. He used to play jokes on all his friends and neighbors. He didn't like to go to school, and he ran away from home from time to time. He always went in the direction of the nearby Mississippi. He was nearly drowned  nine times. After his father's death in 1847, Mark Twain began to work for a printer, who only provided him with food and clothing. Then, he worked as a river-boat pilot and later joined the army. But shortly after that he became a miner, during this period, he started to write short stories.  Afterwards he became a full-time writer. In 1870, Mark Twain got married. In the years that followed he wrote many books including in 1876, and in 1884, which made him famous, and brought him a great fortune .Unfortunately, Mark Twain got into debts in bad investments  and he had to write large numbers of stories to pay these debts. In 1904, his wife died and then three of their children passed away. He died on April 21, 1910 at the age of 70.
03/12/2022 20:31:05 - INFO - __main__ - ['a tailor']
03/12/2022 20:31:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 20:31:05 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:31:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 20:31:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:31:05 - INFO - __main__ - Printing 3 examples
03/12/2022 20:31:05 - INFO - __main__ -  [race-high] Compared to Emily, Zach is more interested in   . (A) design (B) sports (C) beaches (D) history [SEP] Emily and Zach are confused!  Their parents told them they could choose between Massachusetts and Arizona for their vacation this summer. Emily has always wanted to visit Boston, the capital of Massachusetts. Zach and she both agree that walking along the Freedom Trail would enable them to see Boston's most famous places of historic interest, like the site of the school Ben Franklin attended and the Old State House. If Emily and Zach go to Massachusetts, they could spend a few days at the beaches on Cape Cod. Emily loves body boarding, and Zach is great at body surfing. They both enjoy building sandcastles with their mom and dad. Zach finds learning about Native Americans wonderful and has always wanted to travel along the Apache Trail in Arizona. This mountain highway passes Native American ruins in Tonto National Forest. Emily is not as interested in traveling along this trail as Zach, but they both would like to visit Phoenix, the capital, and then travel to Grand Canyon National Park and Meteor Crater .Zach learned in science class that Meteor Crater is a hole over 4,000 feet wide and 520 feet deep that was created when a huge object from space fell to Earth. The object went so deep that it has never been found. Zach would really like to try to discover it. But Emily thinks if experienced scientists and researchers cannot find it, Zach might as well not even bother to try. The only drawback for Zach and Emily if they choose Arizona would be the heat. It is very hot and dry in this southwestern state. Massachusetts, on the other hand, is in northeastern United States. Here Zach and Emily and their parents could enjoy mild temperatures. Their parents love hot weather, but Zach and Emily do not really like to sweat. How will they ever decide to which state they should travel? If only they could take two trips!
03/12/2022 20:31:05 - INFO - __main__ - ['history']
03/12/2022 20:31:05 - INFO - __main__ -  [race-high] The purpose of writing the text is to   _  . (A) advertise some popular summer programmes (B) encourage readers to have a good time relaxing (C) offer some tips on how to enjoy a learning vacation (D) attract more readers to spend summer time learning [SEP] Is there something that you've always wanted to try but just never had the time?Well,make plans to try it now since you are on summer vacation.Not all vacations call for taking a tour bus to take photos of famous landmarks.Some vacations allow you plenty of opportunities to learn. The most difficult aspect of a learning vacation may be choosing one because the possibilities are endless.If you enjoy cooking,various companies can take you to Italy,France,Spain,Mexico or even Peru.Once there,you can learn to prepare the local cuisine .Trips are often planned to fit in with local food festivals or special events. The term"learning vacation"often brings language to mind.The best way to learn a language is in an environment where it' s spoken.Study Spanish,French or English.Or attempt a more unusual language like Polish,Estonian or Thai.You'll be able to learn about the country and absorb the culture at the same time. If you are fond of sports,you can polish your skills or learn new ones.Golf and tennis schools welcome players of all levels.If you want a bigger thrill,you can learn to surf,go climbing or race cars.It' s even possible to learn the art and techniques of bull fighting while on vacation! You can also discover our inner artist.Many places offer painting classes in different mediums.The scenic locations of the schools offer plenty of subjects that provide inspiration for practice. If you prefer capturing the world on film,take a photography vacation.Travel with a small group to photograph beautiful animals or scenery .You can also practise your technique on people or at historical sights. Once you decide on a vacation,choose a company carefully.Request names of recent customers you can contact,and then ask them for an evaluation.The more you know before you go,the better prepared you'll be.Then go out and learn something!
03/12/2022 20:31:05 - INFO - __main__ - ['offer some tips on how to enjoy a learning vacation']
03/12/2022 20:31:05 - INFO - __main__ -  [race-high] What kind of people will benefit a lot more from this passage? (A) Scientists (B) Teachers (C) Designers (D) Lawyers [SEP] Trends come and go but style is eternal. So what is the latest fashion trend that defines this season? Let's take a look at the fashion trend in 2016 and be the first ones to embrace the latest trend. Head----It seems like everyone will be wearing a bucket hat this season. Bucket hat might sound a bit old-fashioned for some people, but the 90s trend is in season again! Spring and summer are always the seasons for casual clothes, so pairing the bucket hat with casual shorts or skirts would be your outfit to go in this spring and summer in 2016. Bottom--Summer is the season when everyone gets ready for the humid weather. The current season trend is making everyone have a more comfortable summer because wide-leg bottoms is in trend again. Perhaps not everyone likes wide-leg bottoms because this could possible make people look shorter, choosing a high-waist wide-leg bottom and matching it with crop top will definitely make you look taller and trendy in 2016. Dress---Be prepared for embracing the feminine details this season. Ruffles around the sleeve or ruffles all-over the dress will be everyone's favorite in SS 2016. All these little details will transform your look in season. Simple colors like pink and black are designer's favorites in this season too. Shoes---Many people thought wedges are also the shoes to go in spring and summer time, but in SS 2016, very flat flats are in trend again. Shoe designers are trying to have more combinations flats or low-heels with less of gender identity. Accessories----No outfit goes perfect without accessories. Adding statement accessories to your simple and natural outlook this summer is your day-to-night outfit. These jewels are doing to make you feel completed in day time and sparkled at night.
03/12/2022 20:31:05 - INFO - __main__ - ['Designers']
03/12/2022 20:31:05 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:31:05 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:31:05 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 20:31:06 - INFO - __main__ - Global step 600 Train loss 0.752885 ACC 0.0 on epoch=299
03/12/2022 20:31:06 - INFO - __main__ - save last model!
03/12/2022 20:31:13 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 20:31:14 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 20:31:14 - INFO - __main__ - Printing 3 examples
03/12/2022 20:31:14 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 20:31:14 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 20:31:14 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 20:31:14 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 20:31:14 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 20:31:14 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 20:31:14 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:31:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 20:31:18 - INFO - __main__ - Starting training!
03/12/2022 20:31:19 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:31:23 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 20:34:58 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_87_0.0002_8_predictions.txt
03/12/2022 20:34:58 - INFO - __main__ - ACC on test data: 0.0504
03/12/2022 20:34:59 - INFO - __main__ - prefix=race-high_32_87, lr=0.0002, bsz=8, dev_performance=0.03125, test_performance=0.05042016806722689
03/12/2022 20:34:59 - INFO - __main__ - Running ... prefix=race-high_32_87, lr=0.0001, bsz=8 ...
03/12/2022 20:35:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:35:00 - INFO - __main__ - Printing 3 examples
03/12/2022 20:35:00 - INFO - __main__ -  [race-high] Which statement is NOT true according to the article? (A) In the West, people think laws and customs are rather different. (B) In the West, there is little difference between "sins" and "crimes". (C) An action that is considered a crime in one country may be socially acceptable in another. (D) There is far less use of the civil justice system in Japan than in the United States. [SEP] The idea of "law" exists in every culture. All societies have some kind of law to keep order and to control the interactions of people with those around them. The laws of any culture tell people three things: what they can do (their right), what they must do (their duties), and what they may not do. In addition, there are usually specific types of punishment for those who break the law. Although all societies have laws, not all have the same idea of justice--which is "right" and "wrong" and how "wrong" should be punished. In most Western cultures, it is thought that punishing criminals will prevent them from committing other crimes. Also, it is hoped that the fear of punishment will act as a deterrent  that prevents other people from committing similar crimes; in other words, people who are considering a life of crime will decide against it because of fear of punishment. In most non-Western cultures, by contrast, punishment is not seen as a deterrent. Instead, great importance is placed on restoring balance in the situation. A thief, for example, may be ordered to return the things he has stolen instead of, as in Western societies, spending time in prison. Another difference in the concept of justice lies in various societies' ideas of what laws are. In the West, people consider "laws" quite different from "customs". There is also a great contrast between "sins" (breaking religious laws) and "crimes" (breaking laws of the government). In many non-Western cultures, on the other hand, there is little separation of customs, laws, and religious beliefs; in other cultures, these three may be quite separate from one another, but still very much different from those in the West. For these reasons, an action may be considered a crime in one country, but be socially acceptable in others. For instance, although a thief is viewed as a criminal in much of the world, in a small village where there is considerable communal  living and sharing of objects, the word thief may have little meaning. Someone who has taken something without asking is simply considered an impolite person. Most countries have two kinds of law: criminal and civil. People who have been accused of acts such as murder or theft are heard in the criminal justice system, while civil justice deals with people who are believed to have violated others' rights. The use of the civil system reflects the values of the society in which it exists. In the United States where personal, individual justice is considered very important, civil law has become "big business." There are over 600,000 lawyers in the United States, and many of them keep busy with civil lawsuits; that is, they work for people who want to sue others. If a man falls over a torn rug in a hotel and breaks his arm, for instance, he might decide to sue the hotel owners so that they will pay his medical costs. In a country like Japan, by contrast, there is very little use of the civil justice system. Lawsuits are not very popular in Japan, where social harmony is even more important than individual rights, and where people would rather reach agreement outside court.
03/12/2022 20:35:00 - INFO - __main__ - ['In the West, there is little difference between "sins" and "crimes".']
03/12/2022 20:35:00 - INFO - __main__ -  [race-high] according to the passage, perhaps the followings are our ordinary ways of relaxation for common people except  _  . (A) listening to music (B) playing card (C) going out for fishing (D) boxing match [SEP] "All work and no play makes Jack a dull boy" is a popular saying in the United States. Other countries have similar sayings. It is true that all of us need _ We cannot work all the time if we are going to keep good health and enjoy life. Everyone has his own way of relaxing. Perhaps the most popular way is to take part in sports. there are team sports, such as baseball, basketball, and football. There are individual sports, also, such as golf and swimming. In addition hiking, fishing, skiing, and mountain climbing have a great attraction for people who like to be outdoors. Not everyone who enjoys sports events likes to take part in them. Many people prefer to be onlookers, either watching them on television, or listening to them on the radio. When there is an important baseball game or boxing match it is almost impossible to get tickets; everyone wants to attend. Chess, card-playing, and dancing are forms of indoor recreation enjoyed by many people. It doesn't matter whether we play a fast game of ping-pong, concentrate over the bridge table, or go walking through the woods on a brisk autumn afternoon. It is important for every one to relax from time to time and enjoy some form of recreation.
03/12/2022 20:35:00 - INFO - __main__ - ['boxing match']
03/12/2022 20:35:00 - INFO - __main__ -  [race-high] Before he became a full time writer Mark Twain had not been    _ (A) a printer. (B) a miner (C) a tailor (D) a soldier [SEP] Mark Twain left school when he was twelve. Though he had little school education, he became the most famous writer of his time. He made millions of dollars by writing. His real name was Samuel Langhorne Clements, but he is better known all over the world as Mark Twain, his penname. Mark Twain was born in 1835 and he was not a healthy baby. In fact, he was not expected to live through the first winter. But with his mother's tender care, he managed to survive. As a boy, he caused much trouble to his parents. He used to play jokes on all his friends and neighbors. He didn't like to go to school, and he ran away from home from time to time. He always went in the direction of the nearby Mississippi. He was nearly drowned  nine times. After his father's death in 1847, Mark Twain began to work for a printer, who only provided him with food and clothing. Then, he worked as a river-boat pilot and later joined the army. But shortly after that he became a miner, during this period, he started to write short stories.  Afterwards he became a full-time writer. In 1870, Mark Twain got married. In the years that followed he wrote many books including in 1876, and in 1884, which made him famous, and brought him a great fortune .Unfortunately, Mark Twain got into debts in bad investments  and he had to write large numbers of stories to pay these debts. In 1904, his wife died and then three of their children passed away. He died on April 21, 1910 at the age of 70.
03/12/2022 20:35:00 - INFO - __main__ - ['a tailor']
03/12/2022 20:35:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 20:35:00 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:35:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 20:35:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:35:00 - INFO - __main__ - Printing 3 examples
03/12/2022 20:35:00 - INFO - __main__ -  [race-high] Compared to Emily, Zach is more interested in   . (A) design (B) sports (C) beaches (D) history [SEP] Emily and Zach are confused!  Their parents told them they could choose between Massachusetts and Arizona for their vacation this summer. Emily has always wanted to visit Boston, the capital of Massachusetts. Zach and she both agree that walking along the Freedom Trail would enable them to see Boston's most famous places of historic interest, like the site of the school Ben Franklin attended and the Old State House. If Emily and Zach go to Massachusetts, they could spend a few days at the beaches on Cape Cod. Emily loves body boarding, and Zach is great at body surfing. They both enjoy building sandcastles with their mom and dad. Zach finds learning about Native Americans wonderful and has always wanted to travel along the Apache Trail in Arizona. This mountain highway passes Native American ruins in Tonto National Forest. Emily is not as interested in traveling along this trail as Zach, but they both would like to visit Phoenix, the capital, and then travel to Grand Canyon National Park and Meteor Crater .Zach learned in science class that Meteor Crater is a hole over 4,000 feet wide and 520 feet deep that was created when a huge object from space fell to Earth. The object went so deep that it has never been found. Zach would really like to try to discover it. But Emily thinks if experienced scientists and researchers cannot find it, Zach might as well not even bother to try. The only drawback for Zach and Emily if they choose Arizona would be the heat. It is very hot and dry in this southwestern state. Massachusetts, on the other hand, is in northeastern United States. Here Zach and Emily and their parents could enjoy mild temperatures. Their parents love hot weather, but Zach and Emily do not really like to sweat. How will they ever decide to which state they should travel? If only they could take two trips!
03/12/2022 20:35:00 - INFO - __main__ - ['history']
03/12/2022 20:35:00 - INFO - __main__ -  [race-high] The purpose of writing the text is to   _  . (A) advertise some popular summer programmes (B) encourage readers to have a good time relaxing (C) offer some tips on how to enjoy a learning vacation (D) attract more readers to spend summer time learning [SEP] Is there something that you've always wanted to try but just never had the time?Well,make plans to try it now since you are on summer vacation.Not all vacations call for taking a tour bus to take photos of famous landmarks.Some vacations allow you plenty of opportunities to learn. The most difficult aspect of a learning vacation may be choosing one because the possibilities are endless.If you enjoy cooking,various companies can take you to Italy,France,Spain,Mexico or even Peru.Once there,you can learn to prepare the local cuisine .Trips are often planned to fit in with local food festivals or special events. The term"learning vacation"often brings language to mind.The best way to learn a language is in an environment where it' s spoken.Study Spanish,French or English.Or attempt a more unusual language like Polish,Estonian or Thai.You'll be able to learn about the country and absorb the culture at the same time. If you are fond of sports,you can polish your skills or learn new ones.Golf and tennis schools welcome players of all levels.If you want a bigger thrill,you can learn to surf,go climbing or race cars.It' s even possible to learn the art and techniques of bull fighting while on vacation! You can also discover our inner artist.Many places offer painting classes in different mediums.The scenic locations of the schools offer plenty of subjects that provide inspiration for practice. If you prefer capturing the world on film,take a photography vacation.Travel with a small group to photograph beautiful animals or scenery .You can also practise your technique on people or at historical sights. Once you decide on a vacation,choose a company carefully.Request names of recent customers you can contact,and then ask them for an evaluation.The more you know before you go,the better prepared you'll be.Then go out and learn something!
03/12/2022 20:35:00 - INFO - __main__ - ['offer some tips on how to enjoy a learning vacation']
03/12/2022 20:35:00 - INFO - __main__ -  [race-high] What kind of people will benefit a lot more from this passage? (A) Scientists (B) Teachers (C) Designers (D) Lawyers [SEP] Trends come and go but style is eternal. So what is the latest fashion trend that defines this season? Let's take a look at the fashion trend in 2016 and be the first ones to embrace the latest trend. Head----It seems like everyone will be wearing a bucket hat this season. Bucket hat might sound a bit old-fashioned for some people, but the 90s trend is in season again! Spring and summer are always the seasons for casual clothes, so pairing the bucket hat with casual shorts or skirts would be your outfit to go in this spring and summer in 2016. Bottom--Summer is the season when everyone gets ready for the humid weather. The current season trend is making everyone have a more comfortable summer because wide-leg bottoms is in trend again. Perhaps not everyone likes wide-leg bottoms because this could possible make people look shorter, choosing a high-waist wide-leg bottom and matching it with crop top will definitely make you look taller and trendy in 2016. Dress---Be prepared for embracing the feminine details this season. Ruffles around the sleeve or ruffles all-over the dress will be everyone's favorite in SS 2016. All these little details will transform your look in season. Simple colors like pink and black are designer's favorites in this season too. Shoes---Many people thought wedges are also the shoes to go in spring and summer time, but in SS 2016, very flat flats are in trend again. Shoe designers are trying to have more combinations flats or low-heels with less of gender identity. Accessories----No outfit goes perfect without accessories. Adding statement accessories to your simple and natural outlook this summer is your day-to-night outfit. These jewels are doing to make you feel completed in day time and sparkled at night.
03/12/2022 20:35:00 - INFO - __main__ - ['Designers']
03/12/2022 20:35:00 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:35:00 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:35:00 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 20:35:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 20:35:12 - INFO - __main__ - Starting training!
03/12/2022 20:35:18 - INFO - __main__ - Step 10 Global step 10 Train loss 19.616285 on epoch=4
03/12/2022 20:35:24 - INFO - __main__ - Step 20 Global step 20 Train loss 17.594446 on epoch=9
03/12/2022 20:35:30 - INFO - __main__ - Step 30 Global step 30 Train loss 14.781891 on epoch=14
03/12/2022 20:35:36 - INFO - __main__ - Step 40 Global step 40 Train loss 11.345653 on epoch=19
03/12/2022 20:35:43 - INFO - __main__ - Step 50 Global step 50 Train loss 9.050559 on epoch=24
03/12/2022 20:35:56 - INFO - __main__ - Global step 50 Train loss 14.477766 ACC 0.0 on epoch=24
03/12/2022 20:36:03 - INFO - __main__ - Step 60 Global step 60 Train loss 7.880496 on epoch=29
03/12/2022 20:36:09 - INFO - __main__ - Step 70 Global step 70 Train loss 6.106851 on epoch=34
03/12/2022 20:36:15 - INFO - __main__ - Step 80 Global step 80 Train loss 5.163693 on epoch=39
03/12/2022 20:36:21 - INFO - __main__ - Step 90 Global step 90 Train loss 4.698727 on epoch=44
03/12/2022 20:36:28 - INFO - __main__ - Step 100 Global step 100 Train loss 4.086399 on epoch=49
03/12/2022 20:36:31 - INFO - __main__ - Global step 100 Train loss 5.587233 ACC 0.03125 on epoch=49
03/12/2022 20:36:37 - INFO - __main__ - Step 110 Global step 110 Train loss 3.771175 on epoch=54
03/12/2022 20:36:43 - INFO - __main__ - Step 120 Global step 120 Train loss 3.775692 on epoch=59
03/12/2022 20:36:50 - INFO - __main__ - Step 130 Global step 130 Train loss 3.592431 on epoch=64
03/12/2022 20:36:56 - INFO - __main__ - Step 140 Global step 140 Train loss 3.613525 on epoch=69
03/12/2022 20:37:02 - INFO - __main__ - Step 150 Global step 150 Train loss 3.284978 on epoch=74
03/12/2022 20:37:05 - INFO - __main__ - Global step 150 Train loss 3.607560 ACC 0.0625 on epoch=74
03/12/2022 20:37:12 - INFO - __main__ - Step 160 Global step 160 Train loss 3.439347 on epoch=79
03/12/2022 20:37:18 - INFO - __main__ - Step 170 Global step 170 Train loss 2.962238 on epoch=84
03/12/2022 20:37:24 - INFO - __main__ - Step 180 Global step 180 Train loss 3.074759 on epoch=89
03/12/2022 20:37:30 - INFO - __main__ - Step 190 Global step 190 Train loss 2.948818 on epoch=94
03/12/2022 20:37:37 - INFO - __main__ - Step 200 Global step 200 Train loss 2.959115 on epoch=99
03/12/2022 20:37:39 - INFO - __main__ - Global step 200 Train loss 3.076855 ACC 0.0625 on epoch=99
03/12/2022 20:37:45 - INFO - __main__ - Step 210 Global step 210 Train loss 2.834624 on epoch=104
03/12/2022 20:37:52 - INFO - __main__ - Step 220 Global step 220 Train loss 2.780160 on epoch=109
03/12/2022 20:37:58 - INFO - __main__ - Step 230 Global step 230 Train loss 2.705022 on epoch=114
03/12/2022 20:38:04 - INFO - __main__ - Step 240 Global step 240 Train loss 2.734656 on epoch=119
03/12/2022 20:38:10 - INFO - __main__ - Step 250 Global step 250 Train loss 2.666857 on epoch=124
03/12/2022 20:38:13 - INFO - __main__ - Global step 250 Train loss 2.744264 ACC 0.125 on epoch=124
03/12/2022 20:38:20 - INFO - __main__ - Step 260 Global step 260 Train loss 2.384466 on epoch=129
03/12/2022 20:38:26 - INFO - __main__ - Step 270 Global step 270 Train loss 2.458741 on epoch=134
03/12/2022 20:38:32 - INFO - __main__ - Step 280 Global step 280 Train loss 2.443971 on epoch=139
03/12/2022 20:38:38 - INFO - __main__ - Step 290 Global step 290 Train loss 2.328163 on epoch=144
03/12/2022 20:38:44 - INFO - __main__ - Step 300 Global step 300 Train loss 2.260209 on epoch=149
03/12/2022 20:38:47 - INFO - __main__ - Global step 300 Train loss 2.375110 ACC 0.125 on epoch=149
03/12/2022 20:38:53 - INFO - __main__ - Step 310 Global step 310 Train loss 2.225273 on epoch=154
03/12/2022 20:39:00 - INFO - __main__ - Step 320 Global step 320 Train loss 2.098920 on epoch=159
03/12/2022 20:39:06 - INFO - __main__ - Step 330 Global step 330 Train loss 2.091757 on epoch=164
03/12/2022 20:39:12 - INFO - __main__ - Step 340 Global step 340 Train loss 2.070965 on epoch=169
03/12/2022 20:39:18 - INFO - __main__ - Step 350 Global step 350 Train loss 1.850953 on epoch=174
03/12/2022 20:39:21 - INFO - __main__ - Global step 350 Train loss 2.067573 ACC 0.0625 on epoch=174
03/12/2022 20:39:27 - INFO - __main__ - Step 360 Global step 360 Train loss 1.839042 on epoch=179
03/12/2022 20:39:34 - INFO - __main__ - Step 370 Global step 370 Train loss 1.707155 on epoch=184
03/12/2022 20:39:40 - INFO - __main__ - Step 380 Global step 380 Train loss 1.741750 on epoch=189
03/12/2022 20:39:46 - INFO - __main__ - Step 390 Global step 390 Train loss 1.482381 on epoch=194
03/12/2022 20:39:52 - INFO - __main__ - Step 400 Global step 400 Train loss 1.463915 on epoch=199
03/12/2022 20:39:55 - INFO - __main__ - Global step 400 Train loss 1.646849 ACC 0.21875 on epoch=199
03/12/2022 20:40:02 - INFO - __main__ - Step 410 Global step 410 Train loss 1.655712 on epoch=204
03/12/2022 20:40:08 - INFO - __main__ - Step 420 Global step 420 Train loss 1.346330 on epoch=209
03/12/2022 20:40:14 - INFO - __main__ - Step 430 Global step 430 Train loss 1.246728 on epoch=214
03/12/2022 20:40:20 - INFO - __main__ - Step 440 Global step 440 Train loss 1.441303 on epoch=219
03/12/2022 20:40:27 - INFO - __main__ - Step 450 Global step 450 Train loss 1.381056 on epoch=224
03/12/2022 20:40:30 - INFO - __main__ - Global step 450 Train loss 1.414226 ACC 0.1875 on epoch=224
03/12/2022 20:40:36 - INFO - __main__ - Step 460 Global step 460 Train loss 1.323828 on epoch=229
03/12/2022 20:40:42 - INFO - __main__ - Step 470 Global step 470 Train loss 1.299712 on epoch=234
03/12/2022 20:40:48 - INFO - __main__ - Step 480 Global step 480 Train loss 1.163755 on epoch=239
03/12/2022 20:40:54 - INFO - __main__ - Step 490 Global step 490 Train loss 1.318171 on epoch=244
03/12/2022 20:41:00 - INFO - __main__ - Step 500 Global step 500 Train loss 1.291377 on epoch=249
03/12/2022 20:41:03 - INFO - __main__ - Global step 500 Train loss 1.279369 ACC 0.0 on epoch=249
03/12/2022 20:41:09 - INFO - __main__ - Step 510 Global step 510 Train loss 1.235039 on epoch=254
03/12/2022 20:41:15 - INFO - __main__ - Step 520 Global step 520 Train loss 1.210707 on epoch=259
03/12/2022 20:41:22 - INFO - __main__ - Step 530 Global step 530 Train loss 1.140791 on epoch=264
03/12/2022 20:41:28 - INFO - __main__ - Step 540 Global step 540 Train loss 1.184734 on epoch=269
03/12/2022 20:41:34 - INFO - __main__ - Step 550 Global step 550 Train loss 1.112211 on epoch=274
03/12/2022 20:41:37 - INFO - __main__ - Global step 550 Train loss 1.176696 ACC 0.0 on epoch=274
03/12/2022 20:41:43 - INFO - __main__ - Step 560 Global step 560 Train loss 1.114206 on epoch=279
03/12/2022 20:41:50 - INFO - __main__ - Step 570 Global step 570 Train loss 1.158610 on epoch=284
03/12/2022 20:41:56 - INFO - __main__ - Step 580 Global step 580 Train loss 1.152610 on epoch=289
03/12/2022 20:42:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.955349 on epoch=294
03/12/2022 20:42:08 - INFO - __main__ - Step 600 Global step 600 Train loss 1.094576 on epoch=299
03/12/2022 20:42:11 - INFO - __main__ - Global step 600 Train loss 1.095070 ACC 0.0 on epoch=299
03/12/2022 20:42:11 - INFO - __main__ - save last model!
03/12/2022 20:42:17 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 20:42:18 - INFO - __main__ - Start tokenizing ... 3451 instances
03/12/2022 20:42:18 - INFO - __main__ - Printing 3 examples
03/12/2022 20:42:18 - INFO - __main__ -  [race-high] The Sherman Antitrust Act  _  . (A) affected only the companies doing business within state lines (B) sought to eliminate monopolies in favor of competition in the market-place (C) promoted trade with a large number of nations (D) provides a financial advantage to the buyer [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 20:42:18 - INFO - __main__ - ['sought to eliminate monopolies in favor of competition in the market-place']
03/12/2022 20:42:18 - INFO - __main__ -  [race-high] One might infer from this passage that lower prices   _  . (A) are more likely to exist in a competitive market economy (B) usually can be found only in an economy based on monopolies (C) matter only to people who are poor and living below the poverty level (D) are regulated by the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 20:42:18 - INFO - __main__ - ['are more likely to exist in a competitive market economy']
03/12/2022 20:42:18 - INFO - __main__ -  [race-high] It seems likely that many Americans  _  . (A) believed that the trusts had little influence over government (B) expected the wealthy magnates to share money with the poor (C) did little to build up American business (D) were worried that trusts might manipulate the government [SEP] One thinks of princes and presidents as some of the most powerful people in the world; however, governments, elected or otherwise, sometimes have had to struggle with the financial powerhouses called tycoons. The word tycoon is relatively new to the English language. It is Chinese in origin but was given as a title to some Japanese generals. The term was brought to the United States, in the late nineteenth century, where it eventually was used to refer to magnates who acquired immense fortunes from sugar and cattle, coal and oil, rubber and steel, and railroads. Some people called these tycoons "capitals of industry" and praised them for their contributions to U.S. wealth and international reputation. Others criticized them as cruel "robber barons", who would stop at nothing in pursuit of personal wealth. The early tycoons built successful businesses, often taking over smaller companies to eliminate competition. A single company that came to control an entire market was called a monopoly. Monopolies made a few families very wealthy, but they also placed a heavy financial burden on consumers and the economy at large. As the country expanded and railroads linked the East Coast to the West Coast, local monopolies turned into national corporations called trusts. A trust is a group of companies that join together under the control of a board of trustees. Railroad trusts are an excellent example. Railroads were privately owned and operated and often monopolized various routes, setting rates as high as they desired. The financial burden this placed on passengers and businesses increased when railroads formed trusts. Farmers, for example, had no choice but to pay, as railroads were the only means they could use to get their grain to buyers. Exorbitant   goods rates put some farmers out of business. There were even accusations that the trusts controlled government itself by buying votes and manipulating elected officials. In 1890 Congress passed the Sherman Antitrust. Act, legislation aimed at breaking the power of such trusts. The Sherman Antitrust Act focused on two main issues. First of all, it made illegal any effort to interfere with the normal conduct of interstate trade. It also made it illegal to monopolize any part of business that operates across state lines. Over the next 60 years or so, Congress passed other antitrust laws in an effort to encourage competition and restrict the power of larger corporations.
03/12/2022 20:42:18 - INFO - __main__ - ['were worried that trusts might manipulate the government']
03/12/2022 20:42:18 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:42:24 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:42:27 - INFO - __main__ - Loaded 3451 examples from test data
03/12/2022 20:46:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-race-high/race-high_32_87_0.0001_8_predictions.txt
03/12/2022 20:46:30 - INFO - __main__ - ACC on test data: 0.1895
03/12/2022 20:46:30 - INFO - __main__ - prefix=race-high_32_87, lr=0.0001, bsz=8, dev_performance=0.21875, test_performance=0.18951028687337004
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (16636): No such process
Task: glue-qnli, Checkpoint: None, Identifier: T5-large-ft-random
Output directory () already exists and is not empty.
03/12/2022 20:46:37 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/12/2022 20:46:37 - INFO - __main__ - models/T5-large-ft-random/singletask-glue-qnli
03/12/2022 20:46:37 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/12/2022 20:46:37 - INFO - __main__ - models/T5-large-ft-random/singletask-glue-qnli
03/12/2022 20:46:38 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/12/2022 20:46:38 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/12/2022 20:46:38 - INFO - __main__ - args.device: cuda:0
03/12/2022 20:46:38 - INFO - __main__ - Using 2 gpus
03/12/2022 20:46:38 - INFO - __main__ - args.device: cuda:1
03/12/2022 20:46:38 - INFO - __main__ - Using 2 gpus
03/12/2022 20:46:38 - INFO - __main__ - Fine-tuning the following samples: ['glue-qnli_16_100', 'glue-qnli_16_13', 'glue-qnli_16_21', 'glue-qnli_16_42', 'glue-qnli_16_87']
03/12/2022 20:46:38 - INFO - __main__ - Fine-tuning the following samples: ['glue-qnli_16_100', 'glue-qnli_16_13', 'glue-qnli_16_21', 'glue-qnli_16_42', 'glue-qnli_16_87']
03/12/2022 20:46:43 - INFO - __main__ - Running ... prefix=glue-qnli_16_100, lr=0.0005, bsz=8 ...
03/12/2022 20:46:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:46:44 - INFO - __main__ - Printing 3 examples
03/12/2022 20:46:44 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/12/2022 20:46:44 - INFO - __main__ - ['entailment']
03/12/2022 20:46:44 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/12/2022 20:46:44 - INFO - __main__ - ['entailment']
03/12/2022 20:46:44 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/12/2022 20:46:44 - INFO - __main__ - ['entailment']
03/12/2022 20:46:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 20:46:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:46:44 - INFO - __main__ - Printing 3 examples
03/12/2022 20:46:44 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/12/2022 20:46:44 - INFO - __main__ - ['entailment']
03/12/2022 20:46:44 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/12/2022 20:46:44 - INFO - __main__ - ['entailment']
03/12/2022 20:46:44 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/12/2022 20:46:44 - INFO - __main__ - ['entailment']
03/12/2022 20:46:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 20:46:44 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:46:44 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:46:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 20:46:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:46:44 - INFO - __main__ - Printing 3 examples
03/12/2022 20:46:44 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/12/2022 20:46:44 - INFO - __main__ - ['entailment']
03/12/2022 20:46:44 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/12/2022 20:46:44 - INFO - __main__ - ['entailment']
03/12/2022 20:46:44 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/12/2022 20:46:44 - INFO - __main__ - ['entailment']
03/12/2022 20:46:44 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:46:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 20:46:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:46:44 - INFO - __main__ - Printing 3 examples
03/12/2022 20:46:44 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/12/2022 20:46:44 - INFO - __main__ - ['entailment']
03/12/2022 20:46:44 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/12/2022 20:46:44 - INFO - __main__ - ['entailment']
03/12/2022 20:46:44 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/12/2022 20:46:44 - INFO - __main__ - ['entailment']
03/12/2022 20:46:44 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:46:44 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:46:44 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:46:44 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 20:46:44 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 20:46:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 20:46:57 - INFO - __main__ - Starting training!
03/12/2022 20:46:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 20:46:57 - INFO - __main__ - Starting training!
03/12/2022 20:47:01 - INFO - __main__ - Step 10 Global step 10 Train loss 21.410828 on epoch=4
03/12/2022 20:47:06 - INFO - __main__ - Step 20 Global step 20 Train loss 13.321132 on epoch=9
03/12/2022 20:47:11 - INFO - __main__ - Step 30 Global step 30 Train loss 5.594616 on epoch=14
03/12/2022 20:47:16 - INFO - __main__ - Step 40 Global step 40 Train loss 5.017888 on epoch=19
03/12/2022 20:47:20 - INFO - __main__ - Step 50 Global step 50 Train loss 4.370830 on epoch=24
03/12/2022 20:47:21 - INFO - __main__ - Global step 50 Train loss 9.943059 ACC 0.25 on epoch=24
03/12/2022 20:47:28 - INFO - __main__ - Step 60 Global step 60 Train loss 3.379274 on epoch=29
03/12/2022 20:47:33 - INFO - __main__ - Step 70 Global step 70 Train loss 2.297843 on epoch=34
03/12/2022 20:47:38 - INFO - __main__ - Step 80 Global step 80 Train loss 1.401194 on epoch=39
03/12/2022 20:47:43 - INFO - __main__ - Step 90 Global step 90 Train loss 1.338067 on epoch=44
03/12/2022 20:47:48 - INFO - __main__ - Step 100 Global step 100 Train loss 1.284331 on epoch=49
03/12/2022 20:47:49 - INFO - __main__ - Global step 100 Train loss 1.940142 ACC 0.5 on epoch=49
03/12/2022 20:47:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.857839 on epoch=54
03/12/2022 20:48:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.851108 on epoch=59
03/12/2022 20:48:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.997530 on epoch=64
03/12/2022 20:48:11 - INFO - __main__ - Step 140 Global step 140 Train loss 1.049208 on epoch=69
03/12/2022 20:48:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.695270 on epoch=74
03/12/2022 20:48:16 - INFO - __main__ - Global step 150 Train loss 0.890191 ACC 0.5 on epoch=74
03/12/2022 20:48:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.941840 on epoch=79
03/12/2022 20:48:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.867232 on epoch=84
03/12/2022 20:48:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.655356 on epoch=89
03/12/2022 20:48:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.491644 on epoch=94
03/12/2022 20:48:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.540909 on epoch=99
03/12/2022 20:48:42 - INFO - __main__ - Global step 200 Train loss 0.699396 ACC 0.5 on epoch=99
03/12/2022 20:48:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.528899 on epoch=104
03/12/2022 20:48:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.576644 on epoch=109
03/12/2022 20:48:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.588748 on epoch=114
03/12/2022 20:49:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.410674 on epoch=119
03/12/2022 20:49:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.363737 on epoch=124
03/12/2022 20:49:07 - INFO - __main__ - Global step 250 Train loss 0.493740 ACC 0.5 on epoch=124
03/12/2022 20:49:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.409873 on epoch=129
03/12/2022 20:49:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.350451 on epoch=134
03/12/2022 20:49:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.369851 on epoch=139
03/12/2022 20:49:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.208232 on epoch=144
03/12/2022 20:49:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.247201 on epoch=149
03/12/2022 20:49:32 - INFO - __main__ - Global step 300 Train loss 0.317122 ACC 0.53125 on epoch=149
03/12/2022 20:49:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.268485 on epoch=154
03/12/2022 20:49:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.219667 on epoch=159
03/12/2022 20:49:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.210691 on epoch=164
03/12/2022 20:49:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.224691 on epoch=169
03/12/2022 20:49:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.206564 on epoch=174
03/12/2022 20:49:58 - INFO - __main__ - Global step 350 Train loss 0.226020 ACC 0.53125 on epoch=174
03/12/2022 20:50:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.274821 on epoch=179
03/12/2022 20:50:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.168788 on epoch=184
03/12/2022 20:50:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.208742 on epoch=189
03/12/2022 20:50:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.180019 on epoch=194
03/12/2022 20:50:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.161280 on epoch=199
03/12/2022 20:50:23 - INFO - __main__ - Global step 400 Train loss 0.198730 ACC 0.5 on epoch=199
03/12/2022 20:50:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.123178 on epoch=204
03/12/2022 20:50:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.133936 on epoch=209
03/12/2022 20:50:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.140335 on epoch=214
03/12/2022 20:50:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.086648 on epoch=219
03/12/2022 20:50:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.153947 on epoch=224
03/12/2022 20:50:49 - INFO - __main__ - Global step 450 Train loss 0.127609 ACC 0.0625 on epoch=224
03/12/2022 20:50:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.250760 on epoch=229
03/12/2022 20:50:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.031759 on epoch=234
03/12/2022 20:51:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.015293 on epoch=239
03/12/2022 20:51:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.162658 on epoch=244
03/12/2022 20:51:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.017592 on epoch=249
03/12/2022 20:51:14 - INFO - __main__ - Global step 500 Train loss 0.095612 ACC 0.6875 on epoch=249
03/12/2022 20:51:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001707 on epoch=254
03/12/2022 20:51:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001028 on epoch=259
03/12/2022 20:51:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000733 on epoch=264
03/12/2022 20:51:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002021 on epoch=269
03/12/2022 20:51:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000774 on epoch=274
03/12/2022 20:51:40 - INFO - __main__ - Global step 550 Train loss 0.001253 ACC 0.65625 on epoch=274
03/12/2022 20:51:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000204 on epoch=279
03/12/2022 20:51:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004795 on epoch=284
03/12/2022 20:51:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000779 on epoch=289
03/12/2022 20:51:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000216 on epoch=294
03/12/2022 20:52:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000352 on epoch=299
03/12/2022 20:52:05 - INFO - __main__ - Global step 600 Train loss 0.001269 ACC 0.625 on epoch=299
03/12/2022 20:52:05 - INFO - __main__ - save last model!
03/12/2022 20:52:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:52:05 - INFO - __main__ - Printing 3 examples
03/12/2022 20:52:05 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/12/2022 20:52:05 - INFO - __main__ - ['entailment']
03/12/2022 20:52:05 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/12/2022 20:52:05 - INFO - __main__ - ['entailment']
03/12/2022 20:52:05 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/12/2022 20:52:05 - INFO - __main__ - ['entailment']
03/12/2022 20:52:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 20:52:05 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:52:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 20:52:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:52:05 - INFO - __main__ - Printing 3 examples
03/12/2022 20:52:05 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/12/2022 20:52:05 - INFO - __main__ - ['entailment']
03/12/2022 20:52:05 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/12/2022 20:52:05 - INFO - __main__ - ['entailment']
03/12/2022 20:52:05 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/12/2022 20:52:05 - INFO - __main__ - ['entailment']
03/12/2022 20:52:05 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:52:05 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:52:05 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 20:52:12 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 20:52:12 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 20:52:12 - INFO - __main__ - Printing 3 examples
03/12/2022 20:52:12 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 20:52:12 - INFO - __main__ - ['entailment']
03/12/2022 20:52:12 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 20:52:12 - INFO - __main__ - ['not_entailment']
03/12/2022 20:52:12 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 20:52:12 - INFO - __main__ - ['not_entailment']
03/12/2022 20:52:12 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:52:15 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:52:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 20:52:16 - INFO - __main__ - Starting training!
03/12/2022 20:52:20 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 20:54:21 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_100_0.0005_8_predictions.txt
03/12/2022 20:54:21 - INFO - __main__ - ACC on test data: 0.5471
03/12/2022 20:54:22 - INFO - __main__ - prefix=glue-qnli_16_100, lr=0.0005, bsz=8, dev_performance=0.6875, test_performance=0.5471352736591616
03/12/2022 20:54:22 - INFO - __main__ - Running ... prefix=glue-qnli_16_100, lr=0.0003, bsz=8 ...
03/12/2022 20:54:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:54:23 - INFO - __main__ - Printing 3 examples
03/12/2022 20:54:23 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/12/2022 20:54:23 - INFO - __main__ - ['entailment']
03/12/2022 20:54:23 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/12/2022 20:54:23 - INFO - __main__ - ['entailment']
03/12/2022 20:54:23 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/12/2022 20:54:23 - INFO - __main__ - ['entailment']
03/12/2022 20:54:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 20:54:23 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:54:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 20:54:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:54:23 - INFO - __main__ - Printing 3 examples
03/12/2022 20:54:23 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/12/2022 20:54:23 - INFO - __main__ - ['entailment']
03/12/2022 20:54:23 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/12/2022 20:54:23 - INFO - __main__ - ['entailment']
03/12/2022 20:54:23 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/12/2022 20:54:23 - INFO - __main__ - ['entailment']
03/12/2022 20:54:23 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:54:23 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:54:23 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 20:54:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 20:54:36 - INFO - __main__ - Starting training!
03/12/2022 20:54:40 - INFO - __main__ - Step 10 Global step 10 Train loss 21.933670 on epoch=4
03/12/2022 20:54:45 - INFO - __main__ - Step 20 Global step 20 Train loss 15.433952 on epoch=9
03/12/2022 20:54:50 - INFO - __main__ - Step 30 Global step 30 Train loss 7.026520 on epoch=14
03/12/2022 20:54:55 - INFO - __main__ - Step 40 Global step 40 Train loss 5.934366 on epoch=19
03/12/2022 20:55:00 - INFO - __main__ - Step 50 Global step 50 Train loss 5.193590 on epoch=24
03/12/2022 20:55:00 - INFO - __main__ - Global step 50 Train loss 11.104419 ACC 0.21875 on epoch=24
03/12/2022 20:55:06 - INFO - __main__ - Step 60 Global step 60 Train loss 4.361529 on epoch=29
03/12/2022 20:55:11 - INFO - __main__ - Step 70 Global step 70 Train loss 4.183685 on epoch=34
03/12/2022 20:55:16 - INFO - __main__ - Step 80 Global step 80 Train loss 3.949400 on epoch=39
03/12/2022 20:55:21 - INFO - __main__ - Step 90 Global step 90 Train loss 3.197067 on epoch=44
03/12/2022 20:55:26 - INFO - __main__ - Step 100 Global step 100 Train loss 2.518240 on epoch=49
03/12/2022 20:55:26 - INFO - __main__ - Global step 100 Train loss 3.641984 ACC 0.125 on epoch=49
03/12/2022 20:55:31 - INFO - __main__ - Step 110 Global step 110 Train loss 1.616386 on epoch=54
03/12/2022 20:55:36 - INFO - __main__ - Step 120 Global step 120 Train loss 1.453039 on epoch=59
03/12/2022 20:55:41 - INFO - __main__ - Step 130 Global step 130 Train loss 1.073118 on epoch=64
03/12/2022 20:55:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.189408 on epoch=69
03/12/2022 20:55:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.195801 on epoch=74
03/12/2022 20:55:52 - INFO - __main__ - Global step 150 Train loss 0.905550 ACC 0.53125 on epoch=74
03/12/2022 20:55:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.148920 on epoch=79
03/12/2022 20:56:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.148601 on epoch=84
03/12/2022 20:56:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.122265 on epoch=89
03/12/2022 20:56:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.084734 on epoch=94
03/12/2022 20:56:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.047608 on epoch=99
03/12/2022 20:56:19 - INFO - __main__ - Global step 200 Train loss 0.110425 ACC 0.625 on epoch=99
03/12/2022 20:56:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.033620 on epoch=104
03/12/2022 20:56:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.033844 on epoch=109
03/12/2022 20:56:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.035450 on epoch=114
03/12/2022 20:56:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.025669 on epoch=119
03/12/2022 20:56:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.007138 on epoch=124
03/12/2022 20:56:45 - INFO - __main__ - Global step 250 Train loss 0.027144 ACC 0.5625 on epoch=124
03/12/2022 20:56:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.004607 on epoch=129
03/12/2022 20:56:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.004057 on epoch=134
03/12/2022 20:57:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.003548 on epoch=139
03/12/2022 20:57:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.008036 on epoch=144
03/12/2022 20:57:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.003840 on epoch=149
03/12/2022 20:57:11 - INFO - __main__ - Global step 300 Train loss 0.004818 ACC 0.5625 on epoch=149
03/12/2022 20:57:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.002683 on epoch=154
03/12/2022 20:57:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.001158 on epoch=159
03/12/2022 20:57:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000832 on epoch=164
03/12/2022 20:57:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000293 on epoch=169
03/12/2022 20:57:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.001722 on epoch=174
03/12/2022 20:57:38 - INFO - __main__ - Global step 350 Train loss 0.001338 ACC 0.5625 on epoch=174
03/12/2022 20:57:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000218 on epoch=179
03/12/2022 20:57:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000344 on epoch=184
03/12/2022 20:57:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000245 on epoch=189
03/12/2022 20:57:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000234 on epoch=194
03/12/2022 20:58:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000315 on epoch=199
03/12/2022 20:58:04 - INFO - __main__ - Global step 400 Train loss 0.000271 ACC 0.65625 on epoch=199
03/12/2022 20:58:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000215 on epoch=204
03/12/2022 20:58:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000990 on epoch=209
03/12/2022 20:58:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000241 on epoch=214
03/12/2022 20:58:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000435 on epoch=219
03/12/2022 20:58:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.021236 on epoch=224
03/12/2022 20:58:31 - INFO - __main__ - Global step 450 Train loss 0.004623 ACC 0.59375 on epoch=224
03/12/2022 20:58:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000244 on epoch=229
03/12/2022 20:58:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.062087 on epoch=234
03/12/2022 20:58:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000081 on epoch=239
03/12/2022 20:58:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000060 on epoch=244
03/12/2022 20:58:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000153 on epoch=249
03/12/2022 20:58:57 - INFO - __main__ - Global step 500 Train loss 0.012525 ACC 0.625 on epoch=249
03/12/2022 20:59:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000070 on epoch=254
03/12/2022 20:59:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000179 on epoch=259
03/12/2022 20:59:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000449 on epoch=264
03/12/2022 20:59:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000204 on epoch=269
03/12/2022 20:59:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000125 on epoch=274
03/12/2022 20:59:23 - INFO - __main__ - Global step 550 Train loss 0.000206 ACC 0.5625 on epoch=274
03/12/2022 20:59:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000072 on epoch=279
03/12/2022 20:59:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000032 on epoch=284
03/12/2022 20:59:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001038 on epoch=289
03/12/2022 20:59:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000058 on epoch=294
03/12/2022 20:59:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000044 on epoch=299
03/12/2022 20:59:49 - INFO - __main__ - Global step 600 Train loss 0.000249 ACC 0.65625 on epoch=299
03/12/2022 20:59:49 - INFO - __main__ - save last model!
03/12/2022 20:59:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:59:50 - INFO - __main__ - Printing 3 examples
03/12/2022 20:59:50 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/12/2022 20:59:50 - INFO - __main__ - ['entailment']
03/12/2022 20:59:50 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/12/2022 20:59:50 - INFO - __main__ - ['entailment']
03/12/2022 20:59:50 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/12/2022 20:59:50 - INFO - __main__ - ['entailment']
03/12/2022 20:59:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 20:59:50 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:59:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 20:59:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 20:59:50 - INFO - __main__ - Printing 3 examples
03/12/2022 20:59:50 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/12/2022 20:59:50 - INFO - __main__ - ['entailment']
03/12/2022 20:59:50 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/12/2022 20:59:50 - INFO - __main__ - ['entailment']
03/12/2022 20:59:50 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/12/2022 20:59:50 - INFO - __main__ - ['entailment']
03/12/2022 20:59:50 - INFO - __main__ - Tokenizing Input ...
03/12/2022 20:59:50 - INFO - __main__ - Tokenizing Output ...
03/12/2022 20:59:50 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 20:59:56 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 20:59:57 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 20:59:57 - INFO - __main__ - Printing 3 examples
03/12/2022 20:59:57 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 20:59:57 - INFO - __main__ - ['entailment']
03/12/2022 20:59:57 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 20:59:57 - INFO - __main__ - ['not_entailment']
03/12/2022 20:59:57 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 20:59:57 - INFO - __main__ - ['not_entailment']
03/12/2022 20:59:57 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:00:00 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:00:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:00:00 - INFO - __main__ - Starting training!
03/12/2022 21:00:05 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 21:02:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_100_0.0003_8_predictions.txt
03/12/2022 21:02:07 - INFO - __main__ - ACC on test data: 0.5136
03/12/2022 21:02:08 - INFO - __main__ - prefix=glue-qnli_16_100, lr=0.0003, bsz=8, dev_performance=0.65625, test_performance=0.5136371956800293
03/12/2022 21:02:08 - INFO - __main__ - Running ... prefix=glue-qnli_16_100, lr=0.0002, bsz=8 ...
03/12/2022 21:02:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:02:08 - INFO - __main__ - Printing 3 examples
03/12/2022 21:02:08 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/12/2022 21:02:08 - INFO - __main__ - ['entailment']
03/12/2022 21:02:08 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/12/2022 21:02:08 - INFO - __main__ - ['entailment']
03/12/2022 21:02:08 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/12/2022 21:02:08 - INFO - __main__ - ['entailment']
03/12/2022 21:02:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 21:02:08 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:02:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:02:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:02:08 - INFO - __main__ - Printing 3 examples
03/12/2022 21:02:08 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/12/2022 21:02:08 - INFO - __main__ - ['entailment']
03/12/2022 21:02:08 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/12/2022 21:02:08 - INFO - __main__ - ['entailment']
03/12/2022 21:02:08 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/12/2022 21:02:08 - INFO - __main__ - ['entailment']
03/12/2022 21:02:08 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:02:08 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:02:09 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:02:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:02:19 - INFO - __main__ - Starting training!
03/12/2022 21:02:25 - INFO - __main__ - Step 10 Global step 10 Train loss 21.518105 on epoch=4
03/12/2022 21:02:30 - INFO - __main__ - Step 20 Global step 20 Train loss 15.536827 on epoch=9
03/12/2022 21:02:35 - INFO - __main__ - Step 30 Global step 30 Train loss 7.219402 on epoch=14
03/12/2022 21:02:39 - INFO - __main__ - Step 40 Global step 40 Train loss 6.029533 on epoch=19
03/12/2022 21:02:44 - INFO - __main__ - Step 50 Global step 50 Train loss 5.645701 on epoch=24
03/12/2022 21:02:45 - INFO - __main__ - Global step 50 Train loss 11.189915 ACC 0.46875 on epoch=24
03/12/2022 21:02:51 - INFO - __main__ - Step 60 Global step 60 Train loss 5.466196 on epoch=29
03/12/2022 21:02:55 - INFO - __main__ - Step 70 Global step 70 Train loss 5.145659 on epoch=34
03/12/2022 21:03:00 - INFO - __main__ - Step 80 Global step 80 Train loss 4.850070 on epoch=39
03/12/2022 21:03:05 - INFO - __main__ - Step 90 Global step 90 Train loss 4.456782 on epoch=44
03/12/2022 21:03:10 - INFO - __main__ - Step 100 Global step 100 Train loss 3.990781 on epoch=49
03/12/2022 21:03:11 - INFO - __main__ - Global step 100 Train loss 4.781898 ACC 0.375 on epoch=49
03/12/2022 21:03:15 - INFO - __main__ - Step 110 Global step 110 Train loss 3.981504 on epoch=54
03/12/2022 21:03:20 - INFO - __main__ - Step 120 Global step 120 Train loss 3.568607 on epoch=59
03/12/2022 21:03:25 - INFO - __main__ - Step 130 Global step 130 Train loss 2.640430 on epoch=64
03/12/2022 21:03:30 - INFO - __main__ - Step 140 Global step 140 Train loss 2.681711 on epoch=69
03/12/2022 21:03:35 - INFO - __main__ - Step 150 Global step 150 Train loss 2.113089 on epoch=74
03/12/2022 21:03:36 - INFO - __main__ - Global step 150 Train loss 2.997068 ACC 0.46875 on epoch=74
03/12/2022 21:03:40 - INFO - __main__ - Step 160 Global step 160 Train loss 1.625206 on epoch=79
03/12/2022 21:03:45 - INFO - __main__ - Step 170 Global step 170 Train loss 1.332146 on epoch=84
03/12/2022 21:03:50 - INFO - __main__ - Step 180 Global step 180 Train loss 1.264448 on epoch=89
03/12/2022 21:03:55 - INFO - __main__ - Step 190 Global step 190 Train loss 1.344292 on epoch=94
03/12/2022 21:04:00 - INFO - __main__ - Step 200 Global step 200 Train loss 1.288242 on epoch=99
03/12/2022 21:04:01 - INFO - __main__ - Global step 200 Train loss 1.370867 ACC 0.5 on epoch=99
03/12/2022 21:04:06 - INFO - __main__ - Step 210 Global step 210 Train loss 1.049300 on epoch=104
03/12/2022 21:04:11 - INFO - __main__ - Step 220 Global step 220 Train loss 1.376585 on epoch=109
03/12/2022 21:04:16 - INFO - __main__ - Step 230 Global step 230 Train loss 1.044330 on epoch=114
03/12/2022 21:04:21 - INFO - __main__ - Step 240 Global step 240 Train loss 1.211242 on epoch=119
03/12/2022 21:04:26 - INFO - __main__ - Step 250 Global step 250 Train loss 1.131466 on epoch=124
03/12/2022 21:04:26 - INFO - __main__ - Global step 250 Train loss 1.162585 ACC 0.5 on epoch=124
03/12/2022 21:04:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.897803 on epoch=129
03/12/2022 21:04:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.962470 on epoch=134
03/12/2022 21:04:41 - INFO - __main__ - Step 280 Global step 280 Train loss 1.127869 on epoch=139
03/12/2022 21:04:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.955936 on epoch=144
03/12/2022 21:04:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.904345 on epoch=149
03/12/2022 21:04:51 - INFO - __main__ - Global step 300 Train loss 0.969685 ACC 0.5 on epoch=149
03/12/2022 21:04:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.887475 on epoch=154
03/12/2022 21:05:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.951451 on epoch=159
03/12/2022 21:05:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.613790 on epoch=164
03/12/2022 21:05:11 - INFO - __main__ - Step 340 Global step 340 Train loss 1.031964 on epoch=169
03/12/2022 21:05:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.868694 on epoch=174
03/12/2022 21:05:16 - INFO - __main__ - Global step 350 Train loss 0.870675 ACC 0.5 on epoch=174
03/12/2022 21:05:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.903199 on epoch=179
03/12/2022 21:05:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.850417 on epoch=184
03/12/2022 21:05:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.653137 on epoch=189
03/12/2022 21:05:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.884702 on epoch=194
03/12/2022 21:05:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.777654 on epoch=199
03/12/2022 21:05:41 - INFO - __main__ - Global step 400 Train loss 0.813822 ACC 0.5 on epoch=199
03/12/2022 21:05:46 - INFO - __main__ - Step 410 Global step 410 Train loss 1.015165 on epoch=204
03/12/2022 21:05:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.729209 on epoch=209
03/12/2022 21:05:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.672619 on epoch=214
03/12/2022 21:06:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.708157 on epoch=219
03/12/2022 21:06:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.675152 on epoch=224
03/12/2022 21:06:06 - INFO - __main__ - Global step 450 Train loss 0.760060 ACC 0.5 on epoch=224
03/12/2022 21:06:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.532586 on epoch=229
03/12/2022 21:06:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.538327 on epoch=234
03/12/2022 21:06:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.654653 on epoch=239
03/12/2022 21:06:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.531862 on epoch=244
03/12/2022 21:06:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.447162 on epoch=249
03/12/2022 21:06:31 - INFO - __main__ - Global step 500 Train loss 0.540918 ACC 0.5 on epoch=249
03/12/2022 21:06:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.437358 on epoch=254
03/12/2022 21:06:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.526483 on epoch=259
03/12/2022 21:06:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.463234 on epoch=264
03/12/2022 21:06:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.360546 on epoch=269
03/12/2022 21:06:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.342071 on epoch=274
03/12/2022 21:06:56 - INFO - __main__ - Global step 550 Train loss 0.425938 ACC 0.5625 on epoch=274
03/12/2022 21:07:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.482447 on epoch=279
03/12/2022 21:07:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.275023 on epoch=284
03/12/2022 21:07:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.287329 on epoch=289
03/12/2022 21:07:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.399613 on epoch=294
03/12/2022 21:07:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.321485 on epoch=299
03/12/2022 21:07:22 - INFO - __main__ - Global step 600 Train loss 0.353180 ACC 0.4375 on epoch=299
03/12/2022 21:07:22 - INFO - __main__ - save last model!
03/12/2022 21:07:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:07:23 - INFO - __main__ - Printing 3 examples
03/12/2022 21:07:23 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/12/2022 21:07:23 - INFO - __main__ - ['entailment']
03/12/2022 21:07:23 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/12/2022 21:07:23 - INFO - __main__ - ['entailment']
03/12/2022 21:07:23 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/12/2022 21:07:23 - INFO - __main__ - ['entailment']
03/12/2022 21:07:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 21:07:23 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:07:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:07:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:07:23 - INFO - __main__ - Printing 3 examples
03/12/2022 21:07:23 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/12/2022 21:07:23 - INFO - __main__ - ['entailment']
03/12/2022 21:07:23 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/12/2022 21:07:23 - INFO - __main__ - ['entailment']
03/12/2022 21:07:23 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/12/2022 21:07:23 - INFO - __main__ - ['entailment']
03/12/2022 21:07:23 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:07:23 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:07:23 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:07:29 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 21:07:29 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 21:07:29 - INFO - __main__ - Printing 3 examples
03/12/2022 21:07:29 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 21:07:29 - INFO - __main__ - ['entailment']
03/12/2022 21:07:29 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 21:07:29 - INFO - __main__ - ['not_entailment']
03/12/2022 21:07:29 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 21:07:29 - INFO - __main__ - ['not_entailment']
03/12/2022 21:07:29 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:07:32 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:07:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:07:33 - INFO - __main__ - Starting training!
03/12/2022 21:07:38 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 21:09:26 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_100_0.0002_8_predictions.txt
03/12/2022 21:09:26 - INFO - __main__ - ACC on test data: 0.4953
03/12/2022 21:09:27 - INFO - __main__ - prefix=glue-qnli_16_100, lr=0.0002, bsz=8, dev_performance=0.5625, test_performance=0.4953322350356947
03/12/2022 21:09:27 - INFO - __main__ - Running ... prefix=glue-qnli_16_100, lr=0.0001, bsz=8 ...
03/12/2022 21:09:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:09:27 - INFO - __main__ - Printing 3 examples
03/12/2022 21:09:27 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/12/2022 21:09:27 - INFO - __main__ - ['entailment']
03/12/2022 21:09:27 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/12/2022 21:09:27 - INFO - __main__ - ['entailment']
03/12/2022 21:09:27 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/12/2022 21:09:27 - INFO - __main__ - ['entailment']
03/12/2022 21:09:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 21:09:28 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:09:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:09:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:09:28 - INFO - __main__ - Printing 3 examples
03/12/2022 21:09:28 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/12/2022 21:09:28 - INFO - __main__ - ['entailment']
03/12/2022 21:09:28 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/12/2022 21:09:28 - INFO - __main__ - ['entailment']
03/12/2022 21:09:28 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/12/2022 21:09:28 - INFO - __main__ - ['entailment']
03/12/2022 21:09:28 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:09:28 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:09:28 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:09:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:09:38 - INFO - __main__ - Starting training!
03/12/2022 21:09:42 - INFO - __main__ - Step 10 Global step 10 Train loss 21.585365 on epoch=4
03/12/2022 21:09:46 - INFO - __main__ - Step 20 Global step 20 Train loss 20.355684 on epoch=9
03/12/2022 21:09:51 - INFO - __main__ - Step 30 Global step 30 Train loss 15.076243 on epoch=14
03/12/2022 21:09:56 - INFO - __main__ - Step 40 Global step 40 Train loss 10.130903 on epoch=19
03/12/2022 21:10:01 - INFO - __main__ - Step 50 Global step 50 Train loss 8.446329 on epoch=24
03/12/2022 21:10:03 - INFO - __main__ - Global step 50 Train loss 15.118904 ACC 0.03125 on epoch=24
03/12/2022 21:10:09 - INFO - __main__ - Step 60 Global step 60 Train loss 7.409998 on epoch=29
03/12/2022 21:10:14 - INFO - __main__ - Step 70 Global step 70 Train loss 6.418433 on epoch=34
03/12/2022 21:10:18 - INFO - __main__ - Step 80 Global step 80 Train loss 6.395039 on epoch=39
03/12/2022 21:10:23 - INFO - __main__ - Step 90 Global step 90 Train loss 6.041268 on epoch=44
03/12/2022 21:10:28 - INFO - __main__ - Step 100 Global step 100 Train loss 5.524459 on epoch=49
03/12/2022 21:10:31 - INFO - __main__ - Global step 100 Train loss 6.357840 ACC 0.0 on epoch=49
03/12/2022 21:10:35 - INFO - __main__ - Step 110 Global step 110 Train loss 5.523259 on epoch=54
03/12/2022 21:10:40 - INFO - __main__ - Step 120 Global step 120 Train loss 5.461751 on epoch=59
03/12/2022 21:10:45 - INFO - __main__ - Step 130 Global step 130 Train loss 5.395226 on epoch=64
03/12/2022 21:10:50 - INFO - __main__ - Step 140 Global step 140 Train loss 4.966592 on epoch=69
03/12/2022 21:10:55 - INFO - __main__ - Step 150 Global step 150 Train loss 4.892347 on epoch=74
03/12/2022 21:10:56 - INFO - __main__ - Global step 150 Train loss 5.247835 ACC 0.0 on epoch=74
03/12/2022 21:11:01 - INFO - __main__ - Step 160 Global step 160 Train loss 4.481225 on epoch=79
03/12/2022 21:11:05 - INFO - __main__ - Step 170 Global step 170 Train loss 4.410089 on epoch=84
03/12/2022 21:11:10 - INFO - __main__ - Step 180 Global step 180 Train loss 4.071599 on epoch=89
03/12/2022 21:11:15 - INFO - __main__ - Step 190 Global step 190 Train loss 4.408760 on epoch=94
03/12/2022 21:11:20 - INFO - __main__ - Step 200 Global step 200 Train loss 3.958471 on epoch=99
03/12/2022 21:11:21 - INFO - __main__ - Global step 200 Train loss 4.266029 ACC 0.0 on epoch=99
03/12/2022 21:11:26 - INFO - __main__ - Step 210 Global step 210 Train loss 3.741200 on epoch=104
03/12/2022 21:11:31 - INFO - __main__ - Step 220 Global step 220 Train loss 3.455552 on epoch=109
03/12/2022 21:11:36 - INFO - __main__ - Step 230 Global step 230 Train loss 3.223005 on epoch=114
03/12/2022 21:11:41 - INFO - __main__ - Step 240 Global step 240 Train loss 3.400250 on epoch=119
03/12/2022 21:11:46 - INFO - __main__ - Step 250 Global step 250 Train loss 2.762102 on epoch=124
03/12/2022 21:11:46 - INFO - __main__ - Global step 250 Train loss 3.316422 ACC 0.0 on epoch=124
03/12/2022 21:11:51 - INFO - __main__ - Step 260 Global step 260 Train loss 2.814266 on epoch=129
03/12/2022 21:11:56 - INFO - __main__ - Step 270 Global step 270 Train loss 2.544428 on epoch=134
03/12/2022 21:12:01 - INFO - __main__ - Step 280 Global step 280 Train loss 2.399459 on epoch=139
03/12/2022 21:12:06 - INFO - __main__ - Step 290 Global step 290 Train loss 2.418955 on epoch=144
03/12/2022 21:12:10 - INFO - __main__ - Step 300 Global step 300 Train loss 2.221091 on epoch=149
03/12/2022 21:12:11 - INFO - __main__ - Global step 300 Train loss 2.479640 ACC 0.03125 on epoch=149
03/12/2022 21:12:16 - INFO - __main__ - Step 310 Global step 310 Train loss 1.980969 on epoch=154
03/12/2022 21:12:21 - INFO - __main__ - Step 320 Global step 320 Train loss 1.383070 on epoch=159
03/12/2022 21:12:25 - INFO - __main__ - Step 330 Global step 330 Train loss 1.671191 on epoch=164
03/12/2022 21:12:30 - INFO - __main__ - Step 340 Global step 340 Train loss 1.677392 on epoch=169
03/12/2022 21:12:35 - INFO - __main__ - Step 350 Global step 350 Train loss 1.431037 on epoch=174
03/12/2022 21:12:36 - INFO - __main__ - Global step 350 Train loss 1.628732 ACC 0.40625 on epoch=174
03/12/2022 21:12:41 - INFO - __main__ - Step 360 Global step 360 Train loss 1.345769 on epoch=179
03/12/2022 21:12:46 - INFO - __main__ - Step 370 Global step 370 Train loss 1.382308 on epoch=184
03/12/2022 21:12:51 - INFO - __main__ - Step 380 Global step 380 Train loss 1.309331 on epoch=189
03/12/2022 21:12:56 - INFO - __main__ - Step 390 Global step 390 Train loss 1.306339 on epoch=194
03/12/2022 21:13:01 - INFO - __main__ - Step 400 Global step 400 Train loss 1.020102 on epoch=199
03/12/2022 21:13:02 - INFO - __main__ - Global step 400 Train loss 1.272770 ACC 0.5 on epoch=199
03/12/2022 21:13:07 - INFO - __main__ - Step 410 Global step 410 Train loss 1.269747 on epoch=204
03/12/2022 21:13:12 - INFO - __main__ - Step 420 Global step 420 Train loss 1.024693 on epoch=209
03/12/2022 21:13:17 - INFO - __main__ - Step 430 Global step 430 Train loss 1.316801 on epoch=214
03/12/2022 21:13:22 - INFO - __main__ - Step 440 Global step 440 Train loss 1.359588 on epoch=219
03/12/2022 21:13:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.731113 on epoch=224
03/12/2022 21:13:27 - INFO - __main__ - Global step 450 Train loss 1.140388 ACC 0.5 on epoch=224
03/12/2022 21:13:32 - INFO - __main__ - Step 460 Global step 460 Train loss 1.040259 on epoch=229
03/12/2022 21:13:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.837269 on epoch=234
03/12/2022 21:13:42 - INFO - __main__ - Step 480 Global step 480 Train loss 1.318179 on epoch=239
03/12/2022 21:13:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.933526 on epoch=244
03/12/2022 21:13:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.701854 on epoch=249
03/12/2022 21:13:52 - INFO - __main__ - Global step 500 Train loss 0.966217 ACC 0.5 on epoch=249
03/12/2022 21:13:57 - INFO - __main__ - Step 510 Global step 510 Train loss 1.064390 on epoch=254
03/12/2022 21:14:02 - INFO - __main__ - Step 520 Global step 520 Train loss 1.220904 on epoch=259
03/12/2022 21:14:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.992569 on epoch=264
03/12/2022 21:14:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.795023 on epoch=269
03/12/2022 21:14:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.988412 on epoch=274
03/12/2022 21:14:18 - INFO - __main__ - Global step 550 Train loss 1.012259 ACC 0.5 on epoch=274
03/12/2022 21:14:23 - INFO - __main__ - Step 560 Global step 560 Train loss 1.107034 on epoch=279
03/12/2022 21:14:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.975465 on epoch=284
03/12/2022 21:14:33 - INFO - __main__ - Step 580 Global step 580 Train loss 1.103587 on epoch=289
03/12/2022 21:14:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.884438 on epoch=294
03/12/2022 21:14:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.937805 on epoch=299
03/12/2022 21:14:44 - INFO - __main__ - Global step 600 Train loss 1.001666 ACC 0.5 on epoch=299
03/12/2022 21:14:44 - INFO - __main__ - save last model!
03/12/2022 21:14:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:14:44 - INFO - __main__ - Printing 3 examples
03/12/2022 21:14:44 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/snskrt/; Sanskrit: sasktam [smskrtm] or saskta, originally saskt vk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/12/2022 21:14:44 - INFO - __main__ - ['entailment']
03/12/2022 21:14:44 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/12/2022 21:14:44 - INFO - __main__ - ['entailment']
03/12/2022 21:14:44 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyonc was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyonc at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/12/2022 21:14:44 - INFO - __main__ - ['entailment']
03/12/2022 21:14:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 21:14:44 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:14:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:14:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:14:44 - INFO - __main__ - Printing 3 examples
03/12/2022 21:14:44 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/12/2022 21:14:44 - INFO - __main__ - ['entailment']
03/12/2022 21:14:44 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/12/2022 21:14:44 - INFO - __main__ - ['entailment']
03/12/2022 21:14:44 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/12/2022 21:14:44 - INFO - __main__ - ['entailment']
03/12/2022 21:14:44 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:14:44 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:14:44 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:14:50 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 21:14:51 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 21:14:51 - INFO - __main__ - Printing 3 examples
03/12/2022 21:14:51 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 21:14:51 - INFO - __main__ - ['entailment']
03/12/2022 21:14:51 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 21:14:51 - INFO - __main__ - ['not_entailment']
03/12/2022 21:14:51 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 21:14:51 - INFO - __main__ - ['not_entailment']
03/12/2022 21:14:51 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:14:53 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:14:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:14:55 - INFO - __main__ - Starting training!
03/12/2022 21:14:59 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 21:16:49 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_100_0.0001_8_predictions.txt
03/12/2022 21:16:49 - INFO - __main__ - ACC on test data: 0.5041
03/12/2022 21:16:50 - INFO - __main__ - prefix=glue-qnli_16_100, lr=0.0001, bsz=8, dev_performance=0.5, test_performance=0.5041186161449753
03/12/2022 21:16:50 - INFO - __main__ - Running ... prefix=glue-qnli_16_13, lr=0.0005, bsz=8 ...
03/12/2022 21:16:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:16:51 - INFO - __main__ - Printing 3 examples
03/12/2022 21:16:51 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/snskrt/; Sanskrit: sasktam [smskrtm] or saskta, originally saskt vk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/12/2022 21:16:51 - INFO - __main__ - ['entailment']
03/12/2022 21:16:51 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/12/2022 21:16:51 - INFO - __main__ - ['entailment']
03/12/2022 21:16:51 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyonc was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyonc at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/12/2022 21:16:51 - INFO - __main__ - ['entailment']
03/12/2022 21:16:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 21:16:51 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:16:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:16:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:16:51 - INFO - __main__ - Printing 3 examples
03/12/2022 21:16:51 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/12/2022 21:16:51 - INFO - __main__ - ['entailment']
03/12/2022 21:16:51 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/12/2022 21:16:51 - INFO - __main__ - ['entailment']
03/12/2022 21:16:51 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/12/2022 21:16:51 - INFO - __main__ - ['entailment']
03/12/2022 21:16:51 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:16:51 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:16:51 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:17:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:17:01 - INFO - __main__ - Starting training!
03/12/2022 21:17:05 - INFO - __main__ - Step 10 Global step 10 Train loss 22.357906 on epoch=4
03/12/2022 21:17:10 - INFO - __main__ - Step 20 Global step 20 Train loss 16.737329 on epoch=9
03/12/2022 21:17:15 - INFO - __main__ - Step 30 Global step 30 Train loss 7.875866 on epoch=14
03/12/2022 21:17:19 - INFO - __main__ - Step 40 Global step 40 Train loss 6.430245 on epoch=19
03/12/2022 21:17:24 - INFO - __main__ - Step 50 Global step 50 Train loss 4.820193 on epoch=24
03/12/2022 21:17:25 - INFO - __main__ - Global step 50 Train loss 11.644309 ACC 0.0 on epoch=24
03/12/2022 21:17:31 - INFO - __main__ - Step 60 Global step 60 Train loss 3.890907 on epoch=29
03/12/2022 21:17:36 - INFO - __main__ - Step 70 Global step 70 Train loss 2.723878 on epoch=34
03/12/2022 21:17:41 - INFO - __main__ - Step 80 Global step 80 Train loss 2.152040 on epoch=39
03/12/2022 21:17:46 - INFO - __main__ - Step 90 Global step 90 Train loss 1.617146 on epoch=44
03/12/2022 21:17:51 - INFO - __main__ - Step 100 Global step 100 Train loss 1.100090 on epoch=49
03/12/2022 21:17:51 - INFO - __main__ - Global step 100 Train loss 2.296813 ACC 0.5 on epoch=49
03/12/2022 21:17:57 - INFO - __main__ - Step 110 Global step 110 Train loss 1.089497 on epoch=54
03/12/2022 21:18:02 - INFO - __main__ - Step 120 Global step 120 Train loss 1.128489 on epoch=59
03/12/2022 21:18:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.968559 on epoch=64
03/12/2022 21:18:12 - INFO - __main__ - Step 140 Global step 140 Train loss 1.043310 on epoch=69
03/12/2022 21:18:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.778450 on epoch=74
03/12/2022 21:18:18 - INFO - __main__ - Global step 150 Train loss 1.001661 ACC 0.5 on epoch=74
03/12/2022 21:18:23 - INFO - __main__ - Step 160 Global step 160 Train loss 1.107257 on epoch=79
03/12/2022 21:18:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.715220 on epoch=84
03/12/2022 21:18:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.738725 on epoch=89
03/12/2022 21:18:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.711429 on epoch=94
03/12/2022 21:18:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.768932 on epoch=99
03/12/2022 21:18:43 - INFO - __main__ - Global step 200 Train loss 0.808313 ACC 0.5 on epoch=99
03/12/2022 21:18:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.597773 on epoch=104
03/12/2022 21:18:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.452057 on epoch=109
03/12/2022 21:18:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.725295 on epoch=114
03/12/2022 21:19:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.670908 on epoch=119
03/12/2022 21:19:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.445741 on epoch=124
03/12/2022 21:19:09 - INFO - __main__ - Global step 250 Train loss 0.578355 ACC 0.5 on epoch=124
03/12/2022 21:19:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.421467 on epoch=129
03/12/2022 21:19:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.328422 on epoch=134
03/12/2022 21:19:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.448990 on epoch=139
03/12/2022 21:19:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.303732 on epoch=144
03/12/2022 21:19:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.351641 on epoch=149
03/12/2022 21:19:35 - INFO - __main__ - Global step 300 Train loss 0.370851 ACC 0.5 on epoch=149
03/12/2022 21:19:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.221733 on epoch=154
03/12/2022 21:19:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.298397 on epoch=159
03/12/2022 21:19:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.202471 on epoch=164
03/12/2022 21:19:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.225565 on epoch=169
03/12/2022 21:20:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.223850 on epoch=174
03/12/2022 21:20:00 - INFO - __main__ - Global step 350 Train loss 0.234403 ACC 0.5 on epoch=174
03/12/2022 21:20:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.227464 on epoch=179
03/12/2022 21:20:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.235943 on epoch=184
03/12/2022 21:20:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.270946 on epoch=189
03/12/2022 21:20:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.197865 on epoch=194
03/12/2022 21:20:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.223580 on epoch=199
03/12/2022 21:20:26 - INFO - __main__ - Global step 400 Train loss 0.231159 ACC 0.5 on epoch=199
03/12/2022 21:20:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.219651 on epoch=204
03/12/2022 21:20:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.206075 on epoch=209
03/12/2022 21:20:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.190057 on epoch=214
03/12/2022 21:20:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.169867 on epoch=219
03/12/2022 21:20:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.182578 on epoch=224
03/12/2022 21:20:52 - INFO - __main__ - Global step 450 Train loss 0.193646 ACC 0.5 on epoch=224
03/12/2022 21:20:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.159651 on epoch=229
03/12/2022 21:21:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.214123 on epoch=234
03/12/2022 21:21:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.186734 on epoch=239
03/12/2022 21:21:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.158762 on epoch=244
03/12/2022 21:21:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.156171 on epoch=249
03/12/2022 21:21:17 - INFO - __main__ - Global step 500 Train loss 0.175088 ACC 0.5 on epoch=249
03/12/2022 21:21:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.160915 on epoch=254
03/12/2022 21:21:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.165897 on epoch=259
03/12/2022 21:21:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.219877 on epoch=264
03/12/2022 21:21:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.146212 on epoch=269
03/12/2022 21:21:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.165778 on epoch=274
03/12/2022 21:21:43 - INFO - __main__ - Global step 550 Train loss 0.171736 ACC 0.5 on epoch=274
03/12/2022 21:21:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.139231 on epoch=279
03/12/2022 21:21:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.167764 on epoch=284
03/12/2022 21:21:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.129963 on epoch=289
03/12/2022 21:22:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.162066 on epoch=294
03/12/2022 21:22:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.162385 on epoch=299
03/12/2022 21:22:08 - INFO - __main__ - Global step 600 Train loss 0.152282 ACC 0.53125 on epoch=299
03/12/2022 21:22:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:22:09 - INFO - __main__ - Printing 3 examples
03/12/2022 21:22:09 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/snskrt/; Sanskrit: sasktam [smskrtm] or saskta, originally saskt vk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/12/2022 21:22:09 - INFO - __main__ - ['entailment']
03/12/2022 21:22:09 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/12/2022 21:22:09 - INFO - __main__ - ['entailment']
03/12/2022 21:22:09 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyonc was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyonc at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/12/2022 21:22:09 - INFO - __main__ - ['entailment']
03/12/2022 21:22:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 21:22:09 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:22:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:22:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:22:09 - INFO - __main__ - Printing 3 examples
03/12/2022 21:22:09 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/12/2022 21:22:09 - INFO - __main__ - ['entailment']
03/12/2022 21:22:09 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/12/2022 21:22:09 - INFO - __main__ - ['entailment']
03/12/2022 21:22:09 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/12/2022 21:22:09 - INFO - __main__ - ['entailment']
03/12/2022 21:22:09 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:22:09 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:22:09 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:22:09 - INFO - __main__ - save last model!
03/12/2022 21:22:16 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 21:22:17 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 21:22:17 - INFO - __main__ - Printing 3 examples
03/12/2022 21:22:17 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 21:22:17 - INFO - __main__ - ['entailment']
03/12/2022 21:22:17 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 21:22:17 - INFO - __main__ - ['not_entailment']
03/12/2022 21:22:17 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 21:22:17 - INFO - __main__ - ['not_entailment']
03/12/2022 21:22:17 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:22:20 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:22:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:22:21 - INFO - __main__ - Starting training!
03/12/2022 21:22:25 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 21:24:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_13_0.0005_8_predictions.txt
03/12/2022 21:24:17 - INFO - __main__ - ACC on test data: 0.5074
03/12/2022 21:24:17 - INFO - __main__ - prefix=glue-qnli_16_13, lr=0.0005, bsz=8, dev_performance=0.53125, test_performance=0.5074135090609555
03/12/2022 21:24:17 - INFO - __main__ - Running ... prefix=glue-qnli_16_13, lr=0.0003, bsz=8 ...
03/12/2022 21:24:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:24:18 - INFO - __main__ - Printing 3 examples
03/12/2022 21:24:18 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/snskrt/; Sanskrit: sasktam [smskrtm] or saskta, originally saskt vk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/12/2022 21:24:18 - INFO - __main__ - ['entailment']
03/12/2022 21:24:18 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/12/2022 21:24:18 - INFO - __main__ - ['entailment']
03/12/2022 21:24:18 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyonc was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyonc at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/12/2022 21:24:18 - INFO - __main__ - ['entailment']
03/12/2022 21:24:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 21:24:18 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:24:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:24:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:24:18 - INFO - __main__ - Printing 3 examples
03/12/2022 21:24:18 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/12/2022 21:24:18 - INFO - __main__ - ['entailment']
03/12/2022 21:24:18 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/12/2022 21:24:18 - INFO - __main__ - ['entailment']
03/12/2022 21:24:18 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/12/2022 21:24:18 - INFO - __main__ - ['entailment']
03/12/2022 21:24:18 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:24:19 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:24:19 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:24:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:24:31 - INFO - __main__ - Starting training!
03/12/2022 21:24:35 - INFO - __main__ - Step 10 Global step 10 Train loss 21.041952 on epoch=4
03/12/2022 21:24:40 - INFO - __main__ - Step 20 Global step 20 Train loss 17.768143 on epoch=9
03/12/2022 21:24:45 - INFO - __main__ - Step 30 Global step 30 Train loss 8.743747 on epoch=14
03/12/2022 21:24:50 - INFO - __main__ - Step 40 Global step 40 Train loss 6.924517 on epoch=19
03/12/2022 21:24:55 - INFO - __main__ - Step 50 Global step 50 Train loss 6.003273 on epoch=24
03/12/2022 21:24:56 - INFO - __main__ - Global step 50 Train loss 12.096327 ACC 0.34375 on epoch=24
03/12/2022 21:25:01 - INFO - __main__ - Step 60 Global step 60 Train loss 5.510869 on epoch=29
03/12/2022 21:25:06 - INFO - __main__ - Step 70 Global step 70 Train loss 5.060130 on epoch=34
03/12/2022 21:25:11 - INFO - __main__ - Step 80 Global step 80 Train loss 4.234763 on epoch=39
03/12/2022 21:25:16 - INFO - __main__ - Step 90 Global step 90 Train loss 3.772336 on epoch=44
03/12/2022 21:25:21 - INFO - __main__ - Step 100 Global step 100 Train loss 3.357327 on epoch=49
03/12/2022 21:25:22 - INFO - __main__ - Global step 100 Train loss 4.387085 ACC 0.4375 on epoch=49
03/12/2022 21:25:28 - INFO - __main__ - Step 110 Global step 110 Train loss 2.562570 on epoch=54
03/12/2022 21:25:33 - INFO - __main__ - Step 120 Global step 120 Train loss 1.972387 on epoch=59
03/12/2022 21:25:38 - INFO - __main__ - Step 130 Global step 130 Train loss 1.301862 on epoch=64
03/12/2022 21:25:43 - INFO - __main__ - Step 140 Global step 140 Train loss 1.055330 on epoch=69
03/12/2022 21:25:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.939723 on epoch=74
03/12/2022 21:25:48 - INFO - __main__ - Global step 150 Train loss 1.566374 ACC 0.5 on epoch=74
03/12/2022 21:25:54 - INFO - __main__ - Step 160 Global step 160 Train loss 1.242202 on epoch=79
03/12/2022 21:25:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.916020 on epoch=84
03/12/2022 21:26:04 - INFO - __main__ - Step 180 Global step 180 Train loss 1.028442 on epoch=89
03/12/2022 21:26:09 - INFO - __main__ - Step 190 Global step 190 Train loss 1.199443 on epoch=94
03/12/2022 21:26:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.719158 on epoch=99
03/12/2022 21:26:14 - INFO - __main__ - Global step 200 Train loss 1.021053 ACC 0.5 on epoch=99
03/12/2022 21:26:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.995860 on epoch=104
03/12/2022 21:26:24 - INFO - __main__ - Step 220 Global step 220 Train loss 1.005666 on epoch=109
03/12/2022 21:26:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.924554 on epoch=114
03/12/2022 21:26:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.921192 on epoch=119
03/12/2022 21:26:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.802058 on epoch=124
03/12/2022 21:26:40 - INFO - __main__ - Global step 250 Train loss 0.929866 ACC 0.5 on epoch=124
03/12/2022 21:26:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.674975 on epoch=129
03/12/2022 21:26:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.590152 on epoch=134
03/12/2022 21:26:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.985625 on epoch=139
03/12/2022 21:27:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.912507 on epoch=144
03/12/2022 21:27:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.630745 on epoch=149
03/12/2022 21:27:06 - INFO - __main__ - Global step 300 Train loss 0.758801 ACC 0.5 on epoch=149
03/12/2022 21:27:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.736817 on epoch=154
03/12/2022 21:27:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.667621 on epoch=159
03/12/2022 21:27:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.606923 on epoch=164
03/12/2022 21:27:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.549501 on epoch=169
03/12/2022 21:27:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.734968 on epoch=174
03/12/2022 21:27:32 - INFO - __main__ - Global step 350 Train loss 0.659166 ACC 0.5 on epoch=174
03/12/2022 21:27:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.669569 on epoch=179
03/12/2022 21:27:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.432976 on epoch=184
03/12/2022 21:27:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.547614 on epoch=189
03/12/2022 21:27:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.589567 on epoch=194
03/12/2022 21:27:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.474123 on epoch=199
03/12/2022 21:27:57 - INFO - __main__ - Global step 400 Train loss 0.542770 ACC 0.5 on epoch=199
03/12/2022 21:28:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.444235 on epoch=204
03/12/2022 21:28:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.469406 on epoch=209
03/12/2022 21:28:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.365388 on epoch=214
03/12/2022 21:28:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.422413 on epoch=219
03/12/2022 21:28:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.335089 on epoch=224
03/12/2022 21:28:23 - INFO - __main__ - Global step 450 Train loss 0.407306 ACC 0.5 on epoch=224
03/12/2022 21:28:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.288432 on epoch=229
03/12/2022 21:28:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.351835 on epoch=234
03/12/2022 21:28:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.294558 on epoch=239
03/12/2022 21:28:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.354787 on epoch=244
03/12/2022 21:28:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.321555 on epoch=249
03/12/2022 21:28:48 - INFO - __main__ - Global step 500 Train loss 0.322233 ACC 0.5 on epoch=249
03/12/2022 21:28:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.316409 on epoch=254
03/12/2022 21:28:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.325557 on epoch=259
03/12/2022 21:29:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.320983 on epoch=264
03/12/2022 21:29:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.314867 on epoch=269
03/12/2022 21:29:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.276749 on epoch=274
03/12/2022 21:29:14 - INFO - __main__ - Global step 550 Train loss 0.310913 ACC 0.5 on epoch=274
03/12/2022 21:29:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.277919 on epoch=279
03/12/2022 21:29:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.272543 on epoch=284
03/12/2022 21:29:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.223414 on epoch=289
03/12/2022 21:29:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.247789 on epoch=294
03/12/2022 21:29:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.199200 on epoch=299
03/12/2022 21:29:39 - INFO - __main__ - Global step 600 Train loss 0.244173 ACC 0.5 on epoch=299
03/12/2022 21:29:39 - INFO - __main__ - save last model!
03/12/2022 21:29:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:29:40 - INFO - __main__ - Printing 3 examples
03/12/2022 21:29:40 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/snskrt/; Sanskrit: sasktam [smskrtm] or saskta, originally saskt vk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/12/2022 21:29:40 - INFO - __main__ - ['entailment']
03/12/2022 21:29:40 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/12/2022 21:29:40 - INFO - __main__ - ['entailment']
03/12/2022 21:29:40 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyonc was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyonc at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/12/2022 21:29:40 - INFO - __main__ - ['entailment']
03/12/2022 21:29:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 21:29:40 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:29:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:29:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:29:40 - INFO - __main__ - Printing 3 examples
03/12/2022 21:29:40 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/12/2022 21:29:40 - INFO - __main__ - ['entailment']
03/12/2022 21:29:40 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/12/2022 21:29:40 - INFO - __main__ - ['entailment']
03/12/2022 21:29:40 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/12/2022 21:29:40 - INFO - __main__ - ['entailment']
03/12/2022 21:29:40 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:29:40 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:29:40 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:29:47 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 21:29:47 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 21:29:47 - INFO - __main__ - Printing 3 examples
03/12/2022 21:29:47 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 21:29:47 - INFO - __main__ - ['entailment']
03/12/2022 21:29:47 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 21:29:47 - INFO - __main__ - ['not_entailment']
03/12/2022 21:29:47 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 21:29:47 - INFO - __main__ - ['not_entailment']
03/12/2022 21:29:47 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:29:50 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:29:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:29:53 - INFO - __main__ - Starting training!
03/12/2022 21:29:55 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 21:31:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_13_0.0003_8_predictions.txt
03/12/2022 21:31:47 - INFO - __main__ - ACC on test data: 0.5047
03/12/2022 21:31:47 - INFO - __main__ - prefix=glue-qnli_16_13, lr=0.0003, bsz=8, dev_performance=0.5, test_performance=0.5046677649643053
03/12/2022 21:31:47 - INFO - __main__ - Running ... prefix=glue-qnli_16_13, lr=0.0002, bsz=8 ...
03/12/2022 21:31:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:31:48 - INFO - __main__ - Printing 3 examples
03/12/2022 21:31:48 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/snskrt/; Sanskrit: sasktam [smskrtm] or saskta, originally saskt vk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/12/2022 21:31:48 - INFO - __main__ - ['entailment']
03/12/2022 21:31:48 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/12/2022 21:31:48 - INFO - __main__ - ['entailment']
03/12/2022 21:31:48 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyonc was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyonc at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/12/2022 21:31:48 - INFO - __main__ - ['entailment']
03/12/2022 21:31:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 21:31:48 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:31:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:31:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:31:48 - INFO - __main__ - Printing 3 examples
03/12/2022 21:31:48 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/12/2022 21:31:48 - INFO - __main__ - ['entailment']
03/12/2022 21:31:48 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/12/2022 21:31:48 - INFO - __main__ - ['entailment']
03/12/2022 21:31:48 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/12/2022 21:31:48 - INFO - __main__ - ['entailment']
03/12/2022 21:31:48 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:31:48 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:31:48 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:31:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:31:58 - INFO - __main__ - Starting training!
03/12/2022 21:32:02 - INFO - __main__ - Step 10 Global step 10 Train loss 21.996519 on epoch=4
03/12/2022 21:32:07 - INFO - __main__ - Step 20 Global step 20 Train loss 18.601654 on epoch=9
03/12/2022 21:32:12 - INFO - __main__ - Step 30 Global step 30 Train loss 10.171251 on epoch=14
03/12/2022 21:32:17 - INFO - __main__ - Step 40 Global step 40 Train loss 7.601960 on epoch=19
03/12/2022 21:32:22 - INFO - __main__ - Step 50 Global step 50 Train loss 6.040713 on epoch=24
03/12/2022 21:32:23 - INFO - __main__ - Global step 50 Train loss 12.882421 ACC 0.0 on epoch=24
03/12/2022 21:32:29 - INFO - __main__ - Step 60 Global step 60 Train loss 5.887527 on epoch=29
03/12/2022 21:32:34 - INFO - __main__ - Step 70 Global step 70 Train loss 5.343717 on epoch=34
03/12/2022 21:32:39 - INFO - __main__ - Step 80 Global step 80 Train loss 4.982873 on epoch=39
03/12/2022 21:32:44 - INFO - __main__ - Step 90 Global step 90 Train loss 4.673558 on epoch=44
03/12/2022 21:32:49 - INFO - __main__ - Step 100 Global step 100 Train loss 4.520436 on epoch=49
03/12/2022 21:32:50 - INFO - __main__ - Global step 100 Train loss 5.081623 ACC 0.125 on epoch=49
03/12/2022 21:32:56 - INFO - __main__ - Step 110 Global step 110 Train loss 3.961313 on epoch=54
03/12/2022 21:33:01 - INFO - __main__ - Step 120 Global step 120 Train loss 3.691952 on epoch=59
03/12/2022 21:33:06 - INFO - __main__ - Step 130 Global step 130 Train loss 3.648994 on epoch=64
03/12/2022 21:33:11 - INFO - __main__ - Step 140 Global step 140 Train loss 3.100931 on epoch=69
03/12/2022 21:33:16 - INFO - __main__ - Step 150 Global step 150 Train loss 2.438942 on epoch=74
03/12/2022 21:33:16 - INFO - __main__ - Global step 150 Train loss 3.368427 ACC 0.0625 on epoch=74
03/12/2022 21:33:21 - INFO - __main__ - Step 160 Global step 160 Train loss 2.066325 on epoch=79
03/12/2022 21:33:26 - INFO - __main__ - Step 170 Global step 170 Train loss 1.875299 on epoch=84
03/12/2022 21:33:31 - INFO - __main__ - Step 180 Global step 180 Train loss 1.466542 on epoch=89
03/12/2022 21:33:36 - INFO - __main__ - Step 190 Global step 190 Train loss 1.118249 on epoch=94
03/12/2022 21:33:41 - INFO - __main__ - Step 200 Global step 200 Train loss 1.051641 on epoch=99
03/12/2022 21:33:42 - INFO - __main__ - Global step 200 Train loss 1.515611 ACC 0.5 on epoch=99
03/12/2022 21:33:48 - INFO - __main__ - Step 210 Global step 210 Train loss 1.602543 on epoch=104
03/12/2022 21:33:53 - INFO - __main__ - Step 220 Global step 220 Train loss 1.475107 on epoch=109
03/12/2022 21:33:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.908525 on epoch=114
03/12/2022 21:34:03 - INFO - __main__ - Step 240 Global step 240 Train loss 1.370299 on epoch=119
03/12/2022 21:34:08 - INFO - __main__ - Step 250 Global step 250 Train loss 1.198323 on epoch=124
03/12/2022 21:34:08 - INFO - __main__ - Global step 250 Train loss 1.310959 ACC 0.5 on epoch=124
03/12/2022 21:34:13 - INFO - __main__ - Step 260 Global step 260 Train loss 1.106823 on epoch=129
03/12/2022 21:34:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.957928 on epoch=134
03/12/2022 21:34:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.932042 on epoch=139
03/12/2022 21:34:28 - INFO - __main__ - Step 290 Global step 290 Train loss 1.051874 on epoch=144
03/12/2022 21:34:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.954647 on epoch=149
03/12/2022 21:34:34 - INFO - __main__ - Global step 300 Train loss 1.000663 ACC 0.5 on epoch=149
03/12/2022 21:34:39 - INFO - __main__ - Step 310 Global step 310 Train loss 1.058274 on epoch=154
03/12/2022 21:34:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.830858 on epoch=159
03/12/2022 21:34:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.599140 on epoch=164
03/12/2022 21:34:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.964953 on epoch=169
03/12/2022 21:34:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.970353 on epoch=174
03/12/2022 21:35:00 - INFO - __main__ - Global step 350 Train loss 0.884716 ACC 0.5 on epoch=174
03/12/2022 21:35:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.764561 on epoch=179
03/12/2022 21:35:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.719915 on epoch=184
03/12/2022 21:35:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.993532 on epoch=189
03/12/2022 21:35:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.887934 on epoch=194
03/12/2022 21:35:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.708811 on epoch=199
03/12/2022 21:35:26 - INFO - __main__ - Global step 400 Train loss 0.814950 ACC 0.46875 on epoch=199
03/12/2022 21:35:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.862076 on epoch=204
03/12/2022 21:35:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.574783 on epoch=209
03/12/2022 21:35:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.772309 on epoch=214
03/12/2022 21:35:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.676918 on epoch=219
03/12/2022 21:35:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.723786 on epoch=224
03/12/2022 21:35:51 - INFO - __main__ - Global step 450 Train loss 0.721975 ACC 0.5 on epoch=224
03/12/2022 21:35:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.550797 on epoch=229
03/12/2022 21:36:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.723690 on epoch=234
03/12/2022 21:36:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.749675 on epoch=239
03/12/2022 21:36:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.588080 on epoch=244
03/12/2022 21:36:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.540449 on epoch=249
03/12/2022 21:36:17 - INFO - __main__ - Global step 500 Train loss 0.630538 ACC 0.5 on epoch=249
03/12/2022 21:36:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.564653 on epoch=254
03/12/2022 21:36:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.645537 on epoch=259
03/12/2022 21:36:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.658984 on epoch=264
03/12/2022 21:36:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.601818 on epoch=269
03/12/2022 21:36:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.409388 on epoch=274
03/12/2022 21:36:43 - INFO - __main__ - Global step 550 Train loss 0.576076 ACC 0.5 on epoch=274
03/12/2022 21:36:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.543991 on epoch=279
03/12/2022 21:36:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.459992 on epoch=284
03/12/2022 21:36:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.433035 on epoch=289
03/12/2022 21:37:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.450854 on epoch=294
03/12/2022 21:37:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.459800 on epoch=299
03/12/2022 21:37:09 - INFO - __main__ - Global step 600 Train loss 0.469534 ACC 0.5 on epoch=299
03/12/2022 21:37:09 - INFO - __main__ - save last model!
03/12/2022 21:37:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:37:09 - INFO - __main__ - Printing 3 examples
03/12/2022 21:37:09 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/snskrt/; Sanskrit: sasktam [smskrtm] or saskta, originally saskt vk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/12/2022 21:37:09 - INFO - __main__ - ['entailment']
03/12/2022 21:37:09 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/12/2022 21:37:09 - INFO - __main__ - ['entailment']
03/12/2022 21:37:09 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyonc was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyonc at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/12/2022 21:37:09 - INFO - __main__ - ['entailment']
03/12/2022 21:37:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 21:37:09 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:37:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:37:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:37:09 - INFO - __main__ - Printing 3 examples
03/12/2022 21:37:09 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/12/2022 21:37:09 - INFO - __main__ - ['entailment']
03/12/2022 21:37:09 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/12/2022 21:37:09 - INFO - __main__ - ['entailment']
03/12/2022 21:37:09 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/12/2022 21:37:09 - INFO - __main__ - ['entailment']
03/12/2022 21:37:09 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:37:09 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:37:09 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:37:16 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 21:37:16 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 21:37:16 - INFO - __main__ - Printing 3 examples
03/12/2022 21:37:16 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 21:37:16 - INFO - __main__ - ['entailment']
03/12/2022 21:37:16 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 21:37:16 - INFO - __main__ - ['not_entailment']
03/12/2022 21:37:16 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 21:37:16 - INFO - __main__ - ['not_entailment']
03/12/2022 21:37:16 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:37:19 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:37:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:37:20 - INFO - __main__ - Starting training!
03/12/2022 21:37:24 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 21:39:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_13_0.0002_8_predictions.txt
03/12/2022 21:39:17 - INFO - __main__ - ACC on test data: 0.5043
03/12/2022 21:39:18 - INFO - __main__ - prefix=glue-qnli_16_13, lr=0.0002, bsz=8, dev_performance=0.5, test_performance=0.5043016657514187
03/12/2022 21:39:18 - INFO - __main__ - Running ... prefix=glue-qnli_16_13, lr=0.0001, bsz=8 ...
03/12/2022 21:39:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:39:19 - INFO - __main__ - Printing 3 examples
03/12/2022 21:39:19 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/snskrt/; Sanskrit: sasktam [smskrtm] or saskta, originally saskt vk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/12/2022 21:39:19 - INFO - __main__ - ['entailment']
03/12/2022 21:39:19 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/12/2022 21:39:19 - INFO - __main__ - ['entailment']
03/12/2022 21:39:19 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyonc was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyonc at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/12/2022 21:39:19 - INFO - __main__ - ['entailment']
03/12/2022 21:39:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 21:39:19 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:39:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:39:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:39:19 - INFO - __main__ - Printing 3 examples
03/12/2022 21:39:19 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/12/2022 21:39:19 - INFO - __main__ - ['entailment']
03/12/2022 21:39:19 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/12/2022 21:39:19 - INFO - __main__ - ['entailment']
03/12/2022 21:39:19 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/12/2022 21:39:19 - INFO - __main__ - ['entailment']
03/12/2022 21:39:19 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:39:19 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:39:19 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:39:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:39:29 - INFO - __main__ - Starting training!
03/12/2022 21:39:33 - INFO - __main__ - Step 10 Global step 10 Train loss 21.731220 on epoch=4
03/12/2022 21:39:38 - INFO - __main__ - Step 20 Global step 20 Train loss 19.759336 on epoch=9
03/12/2022 21:39:43 - INFO - __main__ - Step 30 Global step 30 Train loss 13.819122 on epoch=14
03/12/2022 21:39:48 - INFO - __main__ - Step 40 Global step 40 Train loss 10.857665 on epoch=19
03/12/2022 21:39:53 - INFO - __main__ - Step 50 Global step 50 Train loss 8.577557 on epoch=24
03/12/2022 21:39:54 - INFO - __main__ - Global step 50 Train loss 14.948979 ACC 0.0 on epoch=24
03/12/2022 21:40:00 - INFO - __main__ - Step 60 Global step 60 Train loss 7.710532 on epoch=29
03/12/2022 21:40:05 - INFO - __main__ - Step 70 Global step 70 Train loss 6.912383 on epoch=34
03/12/2022 21:40:10 - INFO - __main__ - Step 80 Global step 80 Train loss 6.252937 on epoch=39
03/12/2022 21:40:15 - INFO - __main__ - Step 90 Global step 90 Train loss 5.975492 on epoch=44
03/12/2022 21:40:20 - INFO - __main__ - Step 100 Global step 100 Train loss 5.992714 on epoch=49
03/12/2022 21:40:21 - INFO - __main__ - Global step 100 Train loss 6.568811 ACC 0.0 on epoch=49
03/12/2022 21:40:26 - INFO - __main__ - Step 110 Global step 110 Train loss 5.635298 on epoch=54
03/12/2022 21:40:31 - INFO - __main__ - Step 120 Global step 120 Train loss 5.521762 on epoch=59
03/12/2022 21:40:36 - INFO - __main__ - Step 130 Global step 130 Train loss 4.998873 on epoch=64
03/12/2022 21:40:41 - INFO - __main__ - Step 140 Global step 140 Train loss 5.167637 on epoch=69
03/12/2022 21:40:46 - INFO - __main__ - Step 150 Global step 150 Train loss 4.817205 on epoch=74
03/12/2022 21:40:47 - INFO - __main__ - Global step 150 Train loss 5.228155 ACC 0.0 on epoch=74
03/12/2022 21:40:52 - INFO - __main__ - Step 160 Global step 160 Train loss 4.727007 on epoch=79
03/12/2022 21:40:57 - INFO - __main__ - Step 170 Global step 170 Train loss 4.605132 on epoch=84
03/12/2022 21:41:02 - INFO - __main__ - Step 180 Global step 180 Train loss 4.403512 on epoch=89
03/12/2022 21:41:07 - INFO - __main__ - Step 190 Global step 190 Train loss 3.963261 on epoch=94
03/12/2022 21:41:12 - INFO - __main__ - Step 200 Global step 200 Train loss 3.917845 on epoch=99
03/12/2022 21:41:13 - INFO - __main__ - Global step 200 Train loss 4.323352 ACC 0.0625 on epoch=99
03/12/2022 21:41:19 - INFO - __main__ - Step 210 Global step 210 Train loss 3.819606 on epoch=104
03/12/2022 21:41:24 - INFO - __main__ - Step 220 Global step 220 Train loss 3.481715 on epoch=109
03/12/2022 21:41:29 - INFO - __main__ - Step 230 Global step 230 Train loss 3.237298 on epoch=114
03/12/2022 21:41:34 - INFO - __main__ - Step 240 Global step 240 Train loss 2.936697 on epoch=119
03/12/2022 21:41:39 - INFO - __main__ - Step 250 Global step 250 Train loss 3.390027 on epoch=124
03/12/2022 21:41:40 - INFO - __main__ - Global step 250 Train loss 3.373069 ACC 0.0625 on epoch=124
03/12/2022 21:41:45 - INFO - __main__ - Step 260 Global step 260 Train loss 3.057022 on epoch=129
03/12/2022 21:41:50 - INFO - __main__ - Step 270 Global step 270 Train loss 2.785527 on epoch=134
03/12/2022 21:41:55 - INFO - __main__ - Step 280 Global step 280 Train loss 3.023947 on epoch=139
03/12/2022 21:42:00 - INFO - __main__ - Step 290 Global step 290 Train loss 2.417193 on epoch=144
03/12/2022 21:42:05 - INFO - __main__ - Step 300 Global step 300 Train loss 2.304515 on epoch=149
03/12/2022 21:42:06 - INFO - __main__ - Global step 300 Train loss 2.717641 ACC 0.15625 on epoch=149
03/12/2022 21:42:12 - INFO - __main__ - Step 310 Global step 310 Train loss 2.332575 on epoch=154
03/12/2022 21:42:17 - INFO - __main__ - Step 320 Global step 320 Train loss 1.663548 on epoch=159
03/12/2022 21:42:22 - INFO - __main__ - Step 330 Global step 330 Train loss 1.669997 on epoch=164
03/12/2022 21:42:27 - INFO - __main__ - Step 340 Global step 340 Train loss 1.429689 on epoch=169
03/12/2022 21:42:32 - INFO - __main__ - Step 350 Global step 350 Train loss 1.670081 on epoch=174
03/12/2022 21:42:33 - INFO - __main__ - Global step 350 Train loss 1.753178 ACC 0.34375 on epoch=174
03/12/2022 21:42:39 - INFO - __main__ - Step 360 Global step 360 Train loss 1.308497 on epoch=179
03/12/2022 21:42:44 - INFO - __main__ - Step 370 Global step 370 Train loss 1.248076 on epoch=184
03/12/2022 21:42:49 - INFO - __main__ - Step 380 Global step 380 Train loss 1.481287 on epoch=189
03/12/2022 21:42:54 - INFO - __main__ - Step 390 Global step 390 Train loss 1.245759 on epoch=194
03/12/2022 21:42:59 - INFO - __main__ - Step 400 Global step 400 Train loss 1.388074 on epoch=199
03/12/2022 21:43:00 - INFO - __main__ - Global step 400 Train loss 1.334339 ACC 0.5 on epoch=199
03/12/2022 21:43:05 - INFO - __main__ - Step 410 Global step 410 Train loss 1.256805 on epoch=204
03/12/2022 21:43:11 - INFO - __main__ - Step 420 Global step 420 Train loss 1.483847 on epoch=209
03/12/2022 21:43:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.976184 on epoch=214
03/12/2022 21:43:21 - INFO - __main__ - Step 440 Global step 440 Train loss 1.049604 on epoch=219
03/12/2022 21:43:26 - INFO - __main__ - Step 450 Global step 450 Train loss 1.546927 on epoch=224
03/12/2022 21:43:26 - INFO - __main__ - Global step 450 Train loss 1.262673 ACC 0.5 on epoch=224
03/12/2022 21:43:31 - INFO - __main__ - Step 460 Global step 460 Train loss 1.286595 on epoch=229
03/12/2022 21:43:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.656441 on epoch=234
03/12/2022 21:43:42 - INFO - __main__ - Step 480 Global step 480 Train loss 1.142683 on epoch=239
03/12/2022 21:43:47 - INFO - __main__ - Step 490 Global step 490 Train loss 1.052452 on epoch=244
03/12/2022 21:43:52 - INFO - __main__ - Step 500 Global step 500 Train loss 1.070286 on epoch=249
03/12/2022 21:43:52 - INFO - __main__ - Global step 500 Train loss 1.041692 ACC 0.5 on epoch=249
03/12/2022 21:43:57 - INFO - __main__ - Step 510 Global step 510 Train loss 1.071843 on epoch=254
03/12/2022 21:44:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.892995 on epoch=259
03/12/2022 21:44:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.951445 on epoch=264
03/12/2022 21:44:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.755523 on epoch=269
03/12/2022 21:44:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.787589 on epoch=274
03/12/2022 21:44:18 - INFO - __main__ - Global step 550 Train loss 0.891879 ACC 0.5 on epoch=274
03/12/2022 21:44:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.876079 on epoch=279
03/12/2022 21:44:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.812539 on epoch=284
03/12/2022 21:44:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.864918 on epoch=289
03/12/2022 21:44:39 - INFO - __main__ - Step 590 Global step 590 Train loss 1.292566 on epoch=294
03/12/2022 21:44:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.639598 on epoch=299
03/12/2022 21:44:44 - INFO - __main__ - Global step 600 Train loss 0.897140 ACC 0.5 on epoch=299
03/12/2022 21:44:44 - INFO - __main__ - save last model!
03/12/2022 21:44:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:44:45 - INFO - __main__ - Printing 3 examples
03/12/2022 21:44:45 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/12/2022 21:44:45 - INFO - __main__ - ['entailment']
03/12/2022 21:44:45 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/12/2022 21:44:45 - INFO - __main__ - ['entailment']
03/12/2022 21:44:45 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/12/2022 21:44:45 - INFO - __main__ - ['entailment']
03/12/2022 21:44:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 21:44:45 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:44:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:44:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:44:45 - INFO - __main__ - Printing 3 examples
03/12/2022 21:44:45 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/12/2022 21:44:45 - INFO - __main__ - ['entailment']
03/12/2022 21:44:45 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministersthe Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Airremained in direct operational control of their respective services, but ceased to attend Cabinet.
03/12/2022 21:44:45 - INFO - __main__ - ['entailment']
03/12/2022 21:44:45 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/12/2022 21:44:45 - INFO - __main__ - ['entailment']
03/12/2022 21:44:45 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:44:45 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:44:45 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:44:51 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 21:44:51 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 21:44:51 - INFO - __main__ - Printing 3 examples
03/12/2022 21:44:51 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 21:44:51 - INFO - __main__ - ['entailment']
03/12/2022 21:44:51 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 21:44:51 - INFO - __main__ - ['not_entailment']
03/12/2022 21:44:51 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 21:44:51 - INFO - __main__ - ['not_entailment']
03/12/2022 21:44:51 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:44:54 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:44:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:44:57 - INFO - __main__ - Starting training!
03/12/2022 21:45:00 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 21:46:51 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_13_0.0001_8_predictions.txt
03/12/2022 21:46:51 - INFO - __main__ - ACC on test data: 0.5067
03/12/2022 21:46:51 - INFO - __main__ - prefix=glue-qnli_16_13, lr=0.0001, bsz=8, dev_performance=0.5, test_performance=0.5066813106351822
03/12/2022 21:46:51 - INFO - __main__ - Running ... prefix=glue-qnli_16_21, lr=0.0005, bsz=8 ...
03/12/2022 21:46:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:46:52 - INFO - __main__ - Printing 3 examples
03/12/2022 21:46:52 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/12/2022 21:46:52 - INFO - __main__ - ['entailment']
03/12/2022 21:46:52 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/12/2022 21:46:52 - INFO - __main__ - ['entailment']
03/12/2022 21:46:52 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/12/2022 21:46:52 - INFO - __main__ - ['entailment']
03/12/2022 21:46:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 21:46:52 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:46:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:46:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:46:52 - INFO - __main__ - Printing 3 examples
03/12/2022 21:46:52 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/12/2022 21:46:52 - INFO - __main__ - ['entailment']
03/12/2022 21:46:52 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministersthe Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Airremained in direct operational control of their respective services, but ceased to attend Cabinet.
03/12/2022 21:46:52 - INFO - __main__ - ['entailment']
03/12/2022 21:46:52 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/12/2022 21:46:52 - INFO - __main__ - ['entailment']
03/12/2022 21:46:52 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:46:52 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:46:52 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:47:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:47:02 - INFO - __main__ - Starting training!
03/12/2022 21:47:06 - INFO - __main__ - Step 10 Global step 10 Train loss 20.671024 on epoch=4
03/12/2022 21:47:11 - INFO - __main__ - Step 20 Global step 20 Train loss 13.088846 on epoch=9
03/12/2022 21:47:15 - INFO - __main__ - Step 30 Global step 30 Train loss 9.580118 on epoch=14
03/12/2022 21:47:20 - INFO - __main__ - Step 40 Global step 40 Train loss 7.260657 on epoch=19
03/12/2022 21:47:25 - INFO - __main__ - Step 50 Global step 50 Train loss 5.504413 on epoch=24
03/12/2022 21:47:26 - INFO - __main__ - Global step 50 Train loss 11.221012 ACC 0.0 on epoch=24
03/12/2022 21:47:31 - INFO - __main__ - Step 60 Global step 60 Train loss 4.826818 on epoch=29
03/12/2022 21:47:36 - INFO - __main__ - Step 70 Global step 70 Train loss 3.828139 on epoch=34
03/12/2022 21:47:41 - INFO - __main__ - Step 80 Global step 80 Train loss 2.584486 on epoch=39
03/12/2022 21:47:46 - INFO - __main__ - Step 90 Global step 90 Train loss 1.885853 on epoch=44
03/12/2022 21:47:51 - INFO - __main__ - Step 100 Global step 100 Train loss 1.472496 on epoch=49
03/12/2022 21:47:52 - INFO - __main__ - Global step 100 Train loss 2.919559 ACC 0.34375 on epoch=49
03/12/2022 21:47:57 - INFO - __main__ - Step 110 Global step 110 Train loss 1.314394 on epoch=54
03/12/2022 21:48:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.992708 on epoch=59
03/12/2022 21:48:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.914681 on epoch=64
03/12/2022 21:48:12 - INFO - __main__ - Step 140 Global step 140 Train loss 1.077489 on epoch=69
03/12/2022 21:48:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.761330 on epoch=74
03/12/2022 21:48:18 - INFO - __main__ - Global step 150 Train loss 1.012120 ACC 0.5 on epoch=74
03/12/2022 21:48:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.767061 on epoch=79
03/12/2022 21:48:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.531328 on epoch=84
03/12/2022 21:48:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.651664 on epoch=89
03/12/2022 21:48:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.698309 on epoch=94
03/12/2022 21:48:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.662727 on epoch=99
03/12/2022 21:48:44 - INFO - __main__ - Global step 200 Train loss 0.662218 ACC 0.5 on epoch=99
03/12/2022 21:48:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.604006 on epoch=104
03/12/2022 21:48:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.697723 on epoch=109
03/12/2022 21:48:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.624646 on epoch=114
03/12/2022 21:49:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.543767 on epoch=119
03/12/2022 21:49:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.553367 on epoch=124
03/12/2022 21:49:09 - INFO - __main__ - Global step 250 Train loss 0.604702 ACC 0.5 on epoch=124
03/12/2022 21:49:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.428778 on epoch=129
03/12/2022 21:49:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.354958 on epoch=134
03/12/2022 21:49:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.579996 on epoch=139
03/12/2022 21:49:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.490060 on epoch=144
03/12/2022 21:49:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.343521 on epoch=149
03/12/2022 21:49:35 - INFO - __main__ - Global step 300 Train loss 0.439463 ACC 0.5 on epoch=149
03/12/2022 21:49:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.376613 on epoch=154
03/12/2022 21:49:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.371143 on epoch=159
03/12/2022 21:49:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.376666 on epoch=164
03/12/2022 21:49:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.319887 on epoch=169
03/12/2022 21:49:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.311363 on epoch=174
03/12/2022 21:50:00 - INFO - __main__ - Global step 350 Train loss 0.351134 ACC 0.5 on epoch=174
03/12/2022 21:50:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.235008 on epoch=179
03/12/2022 21:50:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.288440 on epoch=184
03/12/2022 21:50:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.302445 on epoch=189
03/12/2022 21:50:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.214091 on epoch=194
03/12/2022 21:50:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.230526 on epoch=199
03/12/2022 21:50:25 - INFO - __main__ - Global step 400 Train loss 0.254102 ACC 0.5 on epoch=199
03/12/2022 21:50:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.205900 on epoch=204
03/12/2022 21:50:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.323324 on epoch=209
03/12/2022 21:50:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.201108 on epoch=214
03/12/2022 21:50:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.156748 on epoch=219
03/12/2022 21:50:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.230719 on epoch=224
03/12/2022 21:50:50 - INFO - __main__ - Global step 450 Train loss 0.223560 ACC 0.5 on epoch=224
03/12/2022 21:50:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.263796 on epoch=229
03/12/2022 21:51:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.222558 on epoch=234
03/12/2022 21:51:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.262090 on epoch=239
03/12/2022 21:51:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.203732 on epoch=244
03/12/2022 21:51:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.213024 on epoch=249
03/12/2022 21:51:15 - INFO - __main__ - Global step 500 Train loss 0.233040 ACC 0.46875 on epoch=249
03/12/2022 21:51:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.183949 on epoch=254
03/12/2022 21:51:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.178168 on epoch=259
03/12/2022 21:51:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.176659 on epoch=264
03/12/2022 21:51:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.267195 on epoch=269
03/12/2022 21:51:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.219317 on epoch=274
03/12/2022 21:51:41 - INFO - __main__ - Global step 550 Train loss 0.205058 ACC 0.5 on epoch=274
03/12/2022 21:51:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.160029 on epoch=279
03/12/2022 21:51:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.162762 on epoch=284
03/12/2022 21:51:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.179714 on epoch=289
03/12/2022 21:52:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.184485 on epoch=294
03/12/2022 21:52:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.157826 on epoch=299
03/12/2022 21:52:06 - INFO - __main__ - Global step 600 Train loss 0.168963 ACC 0.5 on epoch=299
03/12/2022 21:52:06 - INFO - __main__ - save last model!
03/12/2022 21:52:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:52:06 - INFO - __main__ - Printing 3 examples
03/12/2022 21:52:06 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/12/2022 21:52:06 - INFO - __main__ - ['entailment']
03/12/2022 21:52:06 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/12/2022 21:52:06 - INFO - __main__ - ['entailment']
03/12/2022 21:52:06 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/12/2022 21:52:06 - INFO - __main__ - ['entailment']
03/12/2022 21:52:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 21:52:06 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:52:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:52:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:52:06 - INFO - __main__ - Printing 3 examples
03/12/2022 21:52:06 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/12/2022 21:52:06 - INFO - __main__ - ['entailment']
03/12/2022 21:52:06 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministersthe Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Airremained in direct operational control of their respective services, but ceased to attend Cabinet.
03/12/2022 21:52:06 - INFO - __main__ - ['entailment']
03/12/2022 21:52:06 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/12/2022 21:52:06 - INFO - __main__ - ['entailment']
03/12/2022 21:52:06 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:52:06 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:52:06 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:52:13 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 21:52:13 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 21:52:13 - INFO - __main__ - Printing 3 examples
03/12/2022 21:52:13 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 21:52:13 - INFO - __main__ - ['entailment']
03/12/2022 21:52:13 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 21:52:13 - INFO - __main__ - ['not_entailment']
03/12/2022 21:52:13 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 21:52:13 - INFO - __main__ - ['not_entailment']
03/12/2022 21:52:13 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:52:16 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:52:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:52:19 - INFO - __main__ - Starting training!
03/12/2022 21:52:21 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 21:54:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_21_0.0005_8_predictions.txt
03/12/2022 21:54:13 - INFO - __main__ - ACC on test data: 0.4946
03/12/2022 21:54:13 - INFO - __main__ - prefix=glue-qnli_16_21, lr=0.0005, bsz=8, dev_performance=0.5, test_performance=0.4946000366099213
03/12/2022 21:54:13 - INFO - __main__ - Running ... prefix=glue-qnli_16_21, lr=0.0003, bsz=8 ...
03/12/2022 21:54:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:54:14 - INFO - __main__ - Printing 3 examples
03/12/2022 21:54:14 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/12/2022 21:54:14 - INFO - __main__ - ['entailment']
03/12/2022 21:54:14 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/12/2022 21:54:14 - INFO - __main__ - ['entailment']
03/12/2022 21:54:14 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/12/2022 21:54:14 - INFO - __main__ - ['entailment']
03/12/2022 21:54:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 21:54:14 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:54:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:54:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:54:14 - INFO - __main__ - Printing 3 examples
03/12/2022 21:54:14 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/12/2022 21:54:14 - INFO - __main__ - ['entailment']
03/12/2022 21:54:14 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministersthe Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Airremained in direct operational control of their respective services, but ceased to attend Cabinet.
03/12/2022 21:54:14 - INFO - __main__ - ['entailment']
03/12/2022 21:54:14 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/12/2022 21:54:14 - INFO - __main__ - ['entailment']
03/12/2022 21:54:14 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:54:14 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:54:14 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:54:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:54:27 - INFO - __main__ - Starting training!
03/12/2022 21:54:31 - INFO - __main__ - Step 10 Global step 10 Train loss 21.549829 on epoch=4
03/12/2022 21:54:36 - INFO - __main__ - Step 20 Global step 20 Train loss 13.551216 on epoch=9
03/12/2022 21:54:41 - INFO - __main__ - Step 30 Global step 30 Train loss 6.887486 on epoch=14
03/12/2022 21:54:46 - INFO - __main__ - Step 40 Global step 40 Train loss 5.684844 on epoch=19
03/12/2022 21:54:51 - INFO - __main__ - Step 50 Global step 50 Train loss 5.565420 on epoch=24
03/12/2022 21:54:51 - INFO - __main__ - Global step 50 Train loss 10.647758 ACC 0.03125 on epoch=24
03/12/2022 21:54:57 - INFO - __main__ - Step 60 Global step 60 Train loss 5.034014 on epoch=29
03/12/2022 21:55:02 - INFO - __main__ - Step 70 Global step 70 Train loss 4.596276 on epoch=34
03/12/2022 21:55:07 - INFO - __main__ - Step 80 Global step 80 Train loss 3.652225 on epoch=39
03/12/2022 21:55:12 - INFO - __main__ - Step 90 Global step 90 Train loss 3.443479 on epoch=44
03/12/2022 21:55:17 - INFO - __main__ - Step 100 Global step 100 Train loss 2.691967 on epoch=49
03/12/2022 21:55:17 - INFO - __main__ - Global step 100 Train loss 3.883592 ACC 0.0625 on epoch=49
03/12/2022 21:55:23 - INFO - __main__ - Step 110 Global step 110 Train loss 2.425535 on epoch=54
03/12/2022 21:55:28 - INFO - __main__ - Step 120 Global step 120 Train loss 1.454277 on epoch=59
03/12/2022 21:55:33 - INFO - __main__ - Step 130 Global step 130 Train loss 1.426728 on epoch=64
03/12/2022 21:55:38 - INFO - __main__ - Step 140 Global step 140 Train loss 1.242181 on epoch=69
03/12/2022 21:55:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.862608 on epoch=74
03/12/2022 21:55:44 - INFO - __main__ - Global step 150 Train loss 1.482266 ACC 0.5 on epoch=74
03/12/2022 21:55:49 - INFO - __main__ - Step 160 Global step 160 Train loss 1.288006 on epoch=79
03/12/2022 21:55:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.965483 on epoch=84
03/12/2022 21:55:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.981689 on epoch=89
03/12/2022 21:56:04 - INFO - __main__ - Step 190 Global step 190 Train loss 1.151485 on epoch=94
03/12/2022 21:56:09 - INFO - __main__ - Step 200 Global step 200 Train loss 1.168111 on epoch=99
03/12/2022 21:56:10 - INFO - __main__ - Global step 200 Train loss 1.110955 ACC 0.5 on epoch=99
03/12/2022 21:56:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.814192 on epoch=104
03/12/2022 21:56:20 - INFO - __main__ - Step 220 Global step 220 Train loss 1.161973 on epoch=109
03/12/2022 21:56:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.846737 on epoch=114
03/12/2022 21:56:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.755202 on epoch=119
03/12/2022 21:56:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.761093 on epoch=124
03/12/2022 21:56:35 - INFO - __main__ - Global step 250 Train loss 0.867840 ACC 0.5 on epoch=124
03/12/2022 21:56:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.765391 on epoch=129
03/12/2022 21:56:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.705784 on epoch=134
03/12/2022 21:56:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.902520 on epoch=139
03/12/2022 21:56:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.661397 on epoch=144
03/12/2022 21:57:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.871981 on epoch=149
03/12/2022 21:57:01 - INFO - __main__ - Global step 300 Train loss 0.781415 ACC 0.5 on epoch=149
03/12/2022 21:57:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.603961 on epoch=154
03/12/2022 21:57:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.535607 on epoch=159
03/12/2022 21:57:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.655221 on epoch=164
03/12/2022 21:57:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.641752 on epoch=169
03/12/2022 21:57:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.818271 on epoch=174
03/12/2022 21:57:26 - INFO - __main__ - Global step 350 Train loss 0.650963 ACC 0.5 on epoch=174
03/12/2022 21:57:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.390442 on epoch=179
03/12/2022 21:57:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.492229 on epoch=184
03/12/2022 21:57:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.542140 on epoch=189
03/12/2022 21:57:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.647712 on epoch=194
03/12/2022 21:57:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.473613 on epoch=199
03/12/2022 21:57:52 - INFO - __main__ - Global step 400 Train loss 0.509227 ACC 0.46875 on epoch=199
03/12/2022 21:57:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.413022 on epoch=204
03/12/2022 21:58:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.374061 on epoch=209
03/12/2022 21:58:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.363054 on epoch=214
03/12/2022 21:58:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.419757 on epoch=219
03/12/2022 21:58:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.287267 on epoch=224
03/12/2022 21:58:17 - INFO - __main__ - Global step 450 Train loss 0.371432 ACC 0.5 on epoch=224
03/12/2022 21:58:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.395651 on epoch=229
03/12/2022 21:58:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.412844 on epoch=234
03/12/2022 21:58:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.393061 on epoch=239
03/12/2022 21:58:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.369531 on epoch=244
03/12/2022 21:58:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.403656 on epoch=249
03/12/2022 21:58:43 - INFO - __main__ - Global step 500 Train loss 0.394948 ACC 0.5 on epoch=249
03/12/2022 21:58:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.304552 on epoch=254
03/12/2022 21:58:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.323134 on epoch=259
03/12/2022 21:58:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.304494 on epoch=264
03/12/2022 21:59:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.250011 on epoch=269
03/12/2022 21:59:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.208097 on epoch=274
03/12/2022 21:59:08 - INFO - __main__ - Global step 550 Train loss 0.278058 ACC 0.5 on epoch=274
03/12/2022 21:59:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.216553 on epoch=279
03/12/2022 21:59:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.199076 on epoch=284
03/12/2022 21:59:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.231468 on epoch=289
03/12/2022 21:59:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.208101 on epoch=294
03/12/2022 21:59:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.185310 on epoch=299
03/12/2022 21:59:34 - INFO - __main__ - Global step 600 Train loss 0.208101 ACC 0.5 on epoch=299
03/12/2022 21:59:34 - INFO - __main__ - save last model!
03/12/2022 21:59:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:59:34 - INFO - __main__ - Printing 3 examples
03/12/2022 21:59:34 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/12/2022 21:59:34 - INFO - __main__ - ['entailment']
03/12/2022 21:59:34 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/12/2022 21:59:34 - INFO - __main__ - ['entailment']
03/12/2022 21:59:34 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/12/2022 21:59:34 - INFO - __main__ - ['entailment']
03/12/2022 21:59:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 21:59:34 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:59:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 21:59:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 21:59:34 - INFO - __main__ - Printing 3 examples
03/12/2022 21:59:34 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/12/2022 21:59:34 - INFO - __main__ - ['entailment']
03/12/2022 21:59:34 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministersthe Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Airremained in direct operational control of their respective services, but ceased to attend Cabinet.
03/12/2022 21:59:34 - INFO - __main__ - ['entailment']
03/12/2022 21:59:34 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/12/2022 21:59:34 - INFO - __main__ - ['entailment']
03/12/2022 21:59:34 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:59:34 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:59:35 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 21:59:41 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 21:59:41 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 21:59:41 - INFO - __main__ - Printing 3 examples
03/12/2022 21:59:41 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 21:59:41 - INFO - __main__ - ['entailment']
03/12/2022 21:59:41 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 21:59:41 - INFO - __main__ - ['not_entailment']
03/12/2022 21:59:41 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 21:59:41 - INFO - __main__ - ['not_entailment']
03/12/2022 21:59:41 - INFO - __main__ - Tokenizing Input ...
03/12/2022 21:59:44 - INFO - __main__ - Tokenizing Output ...
03/12/2022 21:59:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 21:59:47 - INFO - __main__ - Starting training!
03/12/2022 21:59:49 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 22:01:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_21_0.0003_8_predictions.txt
03/12/2022 22:01:38 - INFO - __main__ - ACC on test data: 0.4946
03/12/2022 22:01:39 - INFO - __main__ - prefix=glue-qnli_16_21, lr=0.0003, bsz=8, dev_performance=0.5, test_performance=0.4946000366099213
03/12/2022 22:01:39 - INFO - __main__ - Running ... prefix=glue-qnli_16_21, lr=0.0002, bsz=8 ...
03/12/2022 22:01:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:01:39 - INFO - __main__ - Printing 3 examples
03/12/2022 22:01:39 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/12/2022 22:01:39 - INFO - __main__ - ['entailment']
03/12/2022 22:01:39 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/12/2022 22:01:39 - INFO - __main__ - ['entailment']
03/12/2022 22:01:39 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/12/2022 22:01:39 - INFO - __main__ - ['entailment']
03/12/2022 22:01:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 22:01:39 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:01:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:01:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:01:39 - INFO - __main__ - Printing 3 examples
03/12/2022 22:01:39 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/12/2022 22:01:39 - INFO - __main__ - ['entailment']
03/12/2022 22:01:39 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministersthe Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Airremained in direct operational control of their respective services, but ceased to attend Cabinet.
03/12/2022 22:01:39 - INFO - __main__ - ['entailment']
03/12/2022 22:01:39 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/12/2022 22:01:39 - INFO - __main__ - ['entailment']
03/12/2022 22:01:39 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:01:39 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:01:40 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:01:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:01:50 - INFO - __main__ - Starting training!
03/12/2022 22:01:55 - INFO - __main__ - Step 10 Global step 10 Train loss 22.950283 on epoch=4
03/12/2022 22:02:00 - INFO - __main__ - Step 20 Global step 20 Train loss 14.538620 on epoch=9
03/12/2022 22:02:04 - INFO - __main__ - Step 30 Global step 30 Train loss 11.211725 on epoch=14
03/12/2022 22:02:09 - INFO - __main__ - Step 40 Global step 40 Train loss 7.400922 on epoch=19
03/12/2022 22:02:14 - INFO - __main__ - Step 50 Global step 50 Train loss 6.789047 on epoch=24
03/12/2022 22:02:15 - INFO - __main__ - Global step 50 Train loss 12.578120 ACC 0.0 on epoch=24
03/12/2022 22:02:21 - INFO - __main__ - Step 60 Global step 60 Train loss 6.082083 on epoch=29
03/12/2022 22:02:26 - INFO - __main__ - Step 70 Global step 70 Train loss 5.559707 on epoch=34
03/12/2022 22:02:31 - INFO - __main__ - Step 80 Global step 80 Train loss 5.245863 on epoch=39
03/12/2022 22:02:36 - INFO - __main__ - Step 90 Global step 90 Train loss 4.597682 on epoch=44
03/12/2022 22:02:41 - INFO - __main__ - Step 100 Global step 100 Train loss 4.091617 on epoch=49
03/12/2022 22:02:42 - INFO - __main__ - Global step 100 Train loss 5.115390 ACC 0.5 on epoch=49
03/12/2022 22:02:47 - INFO - __main__ - Step 110 Global step 110 Train loss 4.101248 on epoch=54
03/12/2022 22:02:53 - INFO - __main__ - Step 120 Global step 120 Train loss 3.585574 on epoch=59
03/12/2022 22:02:58 - INFO - __main__ - Step 130 Global step 130 Train loss 3.231787 on epoch=64
03/12/2022 22:03:03 - INFO - __main__ - Step 140 Global step 140 Train loss 3.106117 on epoch=69
03/12/2022 22:03:08 - INFO - __main__ - Step 150 Global step 150 Train loss 2.621103 on epoch=74
03/12/2022 22:03:08 - INFO - __main__ - Global step 150 Train loss 3.329166 ACC 0.5 on epoch=74
03/12/2022 22:03:13 - INFO - __main__ - Step 160 Global step 160 Train loss 2.148047 on epoch=79
03/12/2022 22:03:18 - INFO - __main__ - Step 170 Global step 170 Train loss 2.064684 on epoch=84
03/12/2022 22:03:23 - INFO - __main__ - Step 180 Global step 180 Train loss 1.700013 on epoch=89
03/12/2022 22:03:28 - INFO - __main__ - Step 190 Global step 190 Train loss 1.272316 on epoch=94
03/12/2022 22:03:33 - INFO - __main__ - Step 200 Global step 200 Train loss 1.462101 on epoch=99
03/12/2022 22:03:34 - INFO - __main__ - Global step 200 Train loss 1.729432 ACC 0.5 on epoch=99
03/12/2022 22:03:39 - INFO - __main__ - Step 210 Global step 210 Train loss 1.002746 on epoch=104
03/12/2022 22:03:44 - INFO - __main__ - Step 220 Global step 220 Train loss 1.060608 on epoch=109
03/12/2022 22:03:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.055728 on epoch=114
03/12/2022 22:03:54 - INFO - __main__ - Step 240 Global step 240 Train loss 1.157296 on epoch=119
03/12/2022 22:03:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.882388 on epoch=124
03/12/2022 22:04:00 - INFO - __main__ - Global step 250 Train loss 1.031753 ACC 0.5 on epoch=124
03/12/2022 22:04:05 - INFO - __main__ - Step 260 Global step 260 Train loss 1.000090 on epoch=129
03/12/2022 22:04:10 - INFO - __main__ - Step 270 Global step 270 Train loss 1.326261 on epoch=134
03/12/2022 22:04:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.700935 on epoch=139
03/12/2022 22:04:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.833004 on epoch=144
03/12/2022 22:04:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.720945 on epoch=149
03/12/2022 22:04:25 - INFO - __main__ - Global step 300 Train loss 0.916247 ACC 0.5 on epoch=149
03/12/2022 22:04:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.973811 on epoch=154
03/12/2022 22:04:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.869771 on epoch=159
03/12/2022 22:04:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.910417 on epoch=164
03/12/2022 22:04:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.722794 on epoch=169
03/12/2022 22:04:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.814685 on epoch=174
03/12/2022 22:04:51 - INFO - __main__ - Global step 350 Train loss 0.858296 ACC 0.5 on epoch=174
03/12/2022 22:04:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.637594 on epoch=179
03/12/2022 22:05:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.629594 on epoch=184
03/12/2022 22:05:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.792331 on epoch=189
03/12/2022 22:05:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.787419 on epoch=194
03/12/2022 22:05:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.603354 on epoch=199
03/12/2022 22:05:17 - INFO - __main__ - Global step 400 Train loss 0.690058 ACC 0.5 on epoch=199
03/12/2022 22:05:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.908737 on epoch=204
03/12/2022 22:05:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.628408 on epoch=209
03/12/2022 22:05:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.711867 on epoch=214
03/12/2022 22:05:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.687425 on epoch=219
03/12/2022 22:05:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.490350 on epoch=224
03/12/2022 22:05:43 - INFO - __main__ - Global step 450 Train loss 0.685357 ACC 0.5 on epoch=224
03/12/2022 22:05:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.666502 on epoch=229
03/12/2022 22:05:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.542782 on epoch=234
03/12/2022 22:05:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.707069 on epoch=239
03/12/2022 22:06:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.366090 on epoch=244
03/12/2022 22:06:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.563341 on epoch=249
03/12/2022 22:06:08 - INFO - __main__ - Global step 500 Train loss 0.569157 ACC 0.5 on epoch=249
03/12/2022 22:06:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.499354 on epoch=254
03/12/2022 22:06:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.510723 on epoch=259
03/12/2022 22:06:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.592659 on epoch=264
03/12/2022 22:06:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.538526 on epoch=269
03/12/2022 22:06:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.489009 on epoch=274
03/12/2022 22:06:34 - INFO - __main__ - Global step 550 Train loss 0.526054 ACC 0.5 on epoch=274
03/12/2022 22:06:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.534139 on epoch=279
03/12/2022 22:06:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.452840 on epoch=284
03/12/2022 22:06:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.402182 on epoch=289
03/12/2022 22:06:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.399583 on epoch=294
03/12/2022 22:06:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.462642 on epoch=299
03/12/2022 22:07:00 - INFO - __main__ - Global step 600 Train loss 0.450277 ACC 0.5 on epoch=299
03/12/2022 22:07:00 - INFO - __main__ - save last model!
03/12/2022 22:07:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:07:01 - INFO - __main__ - Printing 3 examples
03/12/2022 22:07:01 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/12/2022 22:07:01 - INFO - __main__ - ['entailment']
03/12/2022 22:07:01 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/12/2022 22:07:01 - INFO - __main__ - ['entailment']
03/12/2022 22:07:01 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/12/2022 22:07:01 - INFO - __main__ - ['entailment']
03/12/2022 22:07:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 22:07:01 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:07:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:07:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:07:01 - INFO - __main__ - Printing 3 examples
03/12/2022 22:07:01 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/12/2022 22:07:01 - INFO - __main__ - ['entailment']
03/12/2022 22:07:01 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministersthe Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Airremained in direct operational control of their respective services, but ceased to attend Cabinet.
03/12/2022 22:07:01 - INFO - __main__ - ['entailment']
03/12/2022 22:07:01 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/12/2022 22:07:01 - INFO - __main__ - ['entailment']
03/12/2022 22:07:01 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:07:01 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:07:01 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:07:07 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 22:07:07 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 22:07:07 - INFO - __main__ - Printing 3 examples
03/12/2022 22:07:07 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 22:07:07 - INFO - __main__ - ['entailment']
03/12/2022 22:07:07 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 22:07:07 - INFO - __main__ - ['not_entailment']
03/12/2022 22:07:07 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 22:07:07 - INFO - __main__ - ['not_entailment']
03/12/2022 22:07:07 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:07:10 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:07:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:07:12 - INFO - __main__ - Starting training!
03/12/2022 22:07:15 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 22:09:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_21_0.0002_8_predictions.txt
03/12/2022 22:09:12 - INFO - __main__ - ACC on test data: 0.4900
03/12/2022 22:09:13 - INFO - __main__ - prefix=glue-qnli_16_21, lr=0.0002, bsz=8, dev_performance=0.5, test_performance=0.49002379644883765
03/12/2022 22:09:13 - INFO - __main__ - Running ... prefix=glue-qnli_16_21, lr=0.0001, bsz=8 ...
03/12/2022 22:09:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:09:14 - INFO - __main__ - Printing 3 examples
03/12/2022 22:09:14 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/12/2022 22:09:14 - INFO - __main__ - ['entailment']
03/12/2022 22:09:14 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/12/2022 22:09:14 - INFO - __main__ - ['entailment']
03/12/2022 22:09:14 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/12/2022 22:09:14 - INFO - __main__ - ['entailment']
03/12/2022 22:09:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 22:09:14 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:09:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:09:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:09:14 - INFO - __main__ - Printing 3 examples
03/12/2022 22:09:14 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/12/2022 22:09:14 - INFO - __main__ - ['entailment']
03/12/2022 22:09:14 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministersthe Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Airremained in direct operational control of their respective services, but ceased to attend Cabinet.
03/12/2022 22:09:14 - INFO - __main__ - ['entailment']
03/12/2022 22:09:14 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/12/2022 22:09:14 - INFO - __main__ - ['entailment']
03/12/2022 22:09:14 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:09:14 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:09:14 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:09:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:09:27 - INFO - __main__ - Starting training!
03/12/2022 22:09:31 - INFO - __main__ - Step 10 Global step 10 Train loss 21.067263 on epoch=4
03/12/2022 22:09:36 - INFO - __main__ - Step 20 Global step 20 Train loss 19.636833 on epoch=9
03/12/2022 22:09:41 - INFO - __main__ - Step 30 Global step 30 Train loss 14.846843 on epoch=14
03/12/2022 22:09:46 - INFO - __main__ - Step 40 Global step 40 Train loss 11.259657 on epoch=19
03/12/2022 22:09:51 - INFO - __main__ - Step 50 Global step 50 Train loss 9.688759 on epoch=24
03/12/2022 22:10:02 - INFO - __main__ - Global step 50 Train loss 15.299870 ACC 0.0 on epoch=24
03/12/2022 22:10:08 - INFO - __main__ - Step 60 Global step 60 Train loss 7.779193 on epoch=29
03/12/2022 22:10:13 - INFO - __main__ - Step 70 Global step 70 Train loss 8.090852 on epoch=34
03/12/2022 22:10:18 - INFO - __main__ - Step 80 Global step 80 Train loss 7.409503 on epoch=39
03/12/2022 22:10:23 - INFO - __main__ - Step 90 Global step 90 Train loss 6.115355 on epoch=44
03/12/2022 22:10:28 - INFO - __main__ - Step 100 Global step 100 Train loss 6.047179 on epoch=49
03/12/2022 22:10:29 - INFO - __main__ - Global step 100 Train loss 7.088417 ACC 0.15625 on epoch=49
03/12/2022 22:10:35 - INFO - __main__ - Step 110 Global step 110 Train loss 6.191297 on epoch=54
03/12/2022 22:10:40 - INFO - __main__ - Step 120 Global step 120 Train loss 5.703753 on epoch=59
03/12/2022 22:10:46 - INFO - __main__ - Step 130 Global step 130 Train loss 5.293773 on epoch=64
03/12/2022 22:10:51 - INFO - __main__ - Step 140 Global step 140 Train loss 5.216525 on epoch=69
03/12/2022 22:10:56 - INFO - __main__ - Step 150 Global step 150 Train loss 5.279567 on epoch=74
03/12/2022 22:10:56 - INFO - __main__ - Global step 150 Train loss 5.536983 ACC 0.25 on epoch=74
03/12/2022 22:11:02 - INFO - __main__ - Step 160 Global step 160 Train loss 4.632555 on epoch=79
03/12/2022 22:11:07 - INFO - __main__ - Step 170 Global step 170 Train loss 4.555925 on epoch=84
03/12/2022 22:11:12 - INFO - __main__ - Step 180 Global step 180 Train loss 4.330695 on epoch=89
03/12/2022 22:11:17 - INFO - __main__ - Step 190 Global step 190 Train loss 4.223512 on epoch=94
03/12/2022 22:11:22 - INFO - __main__ - Step 200 Global step 200 Train loss 3.806525 on epoch=99
03/12/2022 22:11:23 - INFO - __main__ - Global step 200 Train loss 4.309843 ACC 0.28125 on epoch=99
03/12/2022 22:11:29 - INFO - __main__ - Step 210 Global step 210 Train loss 3.914937 on epoch=104
03/12/2022 22:11:34 - INFO - __main__ - Step 220 Global step 220 Train loss 3.771779 on epoch=109
03/12/2022 22:11:39 - INFO - __main__ - Step 230 Global step 230 Train loss 3.419754 on epoch=114
03/12/2022 22:11:44 - INFO - __main__ - Step 240 Global step 240 Train loss 3.109151 on epoch=119
03/12/2022 22:11:50 - INFO - __main__ - Step 250 Global step 250 Train loss 2.998583 on epoch=124
03/12/2022 22:11:50 - INFO - __main__ - Global step 250 Train loss 3.442841 ACC 0.28125 on epoch=124
03/12/2022 22:11:55 - INFO - __main__ - Step 260 Global step 260 Train loss 2.712126 on epoch=129
03/12/2022 22:12:00 - INFO - __main__ - Step 270 Global step 270 Train loss 2.790170 on epoch=134
03/12/2022 22:12:06 - INFO - __main__ - Step 280 Global step 280 Train loss 2.255222 on epoch=139
03/12/2022 22:12:11 - INFO - __main__ - Step 290 Global step 290 Train loss 2.292427 on epoch=144
03/12/2022 22:12:16 - INFO - __main__ - Step 300 Global step 300 Train loss 2.087327 on epoch=149
03/12/2022 22:12:17 - INFO - __main__ - Global step 300 Train loss 2.427454 ACC 0.46875 on epoch=149
03/12/2022 22:12:22 - INFO - __main__ - Step 310 Global step 310 Train loss 1.967220 on epoch=154
03/12/2022 22:12:27 - INFO - __main__ - Step 320 Global step 320 Train loss 1.827039 on epoch=159
03/12/2022 22:12:33 - INFO - __main__ - Step 330 Global step 330 Train loss 1.548667 on epoch=164
03/12/2022 22:12:38 - INFO - __main__ - Step 340 Global step 340 Train loss 1.710471 on epoch=169
03/12/2022 22:12:43 - INFO - __main__ - Step 350 Global step 350 Train loss 1.290545 on epoch=174
03/12/2022 22:12:44 - INFO - __main__ - Global step 350 Train loss 1.668788 ACC 0.5 on epoch=174
03/12/2022 22:12:49 - INFO - __main__ - Step 360 Global step 360 Train loss 1.500126 on epoch=179
03/12/2022 22:12:54 - INFO - __main__ - Step 370 Global step 370 Train loss 1.395023 on epoch=184
03/12/2022 22:12:59 - INFO - __main__ - Step 380 Global step 380 Train loss 1.207832 on epoch=189
03/12/2022 22:13:05 - INFO - __main__ - Step 390 Global step 390 Train loss 1.201921 on epoch=194
03/12/2022 22:13:10 - INFO - __main__ - Step 400 Global step 400 Train loss 1.398432 on epoch=199
03/12/2022 22:13:10 - INFO - __main__ - Global step 400 Train loss 1.340667 ACC 0.5 on epoch=199
03/12/2022 22:13:15 - INFO - __main__ - Step 410 Global step 410 Train loss 1.229524 on epoch=204
03/12/2022 22:13:21 - INFO - __main__ - Step 420 Global step 420 Train loss 1.468298 on epoch=209
03/12/2022 22:13:26 - INFO - __main__ - Step 430 Global step 430 Train loss 1.126566 on epoch=214
03/12/2022 22:13:31 - INFO - __main__ - Step 440 Global step 440 Train loss 1.263479 on epoch=219
03/12/2022 22:13:36 - INFO - __main__ - Step 450 Global step 450 Train loss 1.178382 on epoch=224
03/12/2022 22:13:37 - INFO - __main__ - Global step 450 Train loss 1.253250 ACC 0.5 on epoch=224
03/12/2022 22:13:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.918634 on epoch=229
03/12/2022 22:13:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.797061 on epoch=234
03/12/2022 22:13:52 - INFO - __main__ - Step 480 Global step 480 Train loss 1.185680 on epoch=239
03/12/2022 22:13:57 - INFO - __main__ - Step 490 Global step 490 Train loss 1.059784 on epoch=244
03/12/2022 22:14:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.976546 on epoch=249
03/12/2022 22:14:03 - INFO - __main__ - Global step 500 Train loss 0.987541 ACC 0.5 on epoch=249
03/12/2022 22:14:08 - INFO - __main__ - Step 510 Global step 510 Train loss 1.183913 on epoch=254
03/12/2022 22:14:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.832477 on epoch=259
03/12/2022 22:14:18 - INFO - __main__ - Step 530 Global step 530 Train loss 1.197637 on epoch=264
03/12/2022 22:14:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.687228 on epoch=269
03/12/2022 22:14:28 - INFO - __main__ - Step 550 Global step 550 Train loss 1.034532 on epoch=274
03/12/2022 22:14:29 - INFO - __main__ - Global step 550 Train loss 0.987157 ACC 0.5 on epoch=274
03/12/2022 22:14:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.721401 on epoch=279
03/12/2022 22:14:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.753166 on epoch=284
03/12/2022 22:14:44 - INFO - __main__ - Step 580 Global step 580 Train loss 1.268327 on epoch=289
03/12/2022 22:14:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.951566 on epoch=294
03/12/2022 22:14:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.885939 on epoch=299
03/12/2022 22:14:55 - INFO - __main__ - Global step 600 Train loss 0.916080 ACC 0.5 on epoch=299
03/12/2022 22:14:55 - INFO - __main__ - save last model!
03/12/2022 22:14:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:14:55 - INFO - __main__ - Printing 3 examples
03/12/2022 22:14:55 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/12/2022 22:14:55 - INFO - __main__ - ['not_entailment']
03/12/2022 22:14:55 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/12/2022 22:14:55 - INFO - __main__ - ['not_entailment']
03/12/2022 22:14:55 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/12/2022 22:14:55 - INFO - __main__ - ['not_entailment']
03/12/2022 22:14:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 22:14:55 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:14:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:14:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:14:55 - INFO - __main__ - Printing 3 examples
03/12/2022 22:14:55 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/12/2022 22:14:55 - INFO - __main__ - ['not_entailment']
03/12/2022 22:14:55 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/12/2022 22:14:55 - INFO - __main__ - ['not_entailment']
03/12/2022 22:14:55 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/12/2022 22:14:55 - INFO - __main__ - ['not_entailment']
03/12/2022 22:14:55 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:14:55 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:14:55 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:15:01 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 22:15:02 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 22:15:02 - INFO - __main__ - Printing 3 examples
03/12/2022 22:15:02 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 22:15:02 - INFO - __main__ - ['entailment']
03/12/2022 22:15:02 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 22:15:02 - INFO - __main__ - ['not_entailment']
03/12/2022 22:15:02 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 22:15:02 - INFO - __main__ - ['not_entailment']
03/12/2022 22:15:02 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:15:05 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:15:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:15:06 - INFO - __main__ - Starting training!
03/12/2022 22:15:10 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 22:16:58 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_21_0.0001_8_predictions.txt
03/12/2022 22:16:58 - INFO - __main__ - ACC on test data: 0.4909
03/12/2022 22:16:58 - INFO - __main__ - prefix=glue-qnli_16_21, lr=0.0001, bsz=8, dev_performance=0.5, test_performance=0.49093904448105435
03/12/2022 22:16:58 - INFO - __main__ - Running ... prefix=glue-qnli_16_42, lr=0.0005, bsz=8 ...
03/12/2022 22:16:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:16:59 - INFO - __main__ - Printing 3 examples
03/12/2022 22:16:59 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/12/2022 22:16:59 - INFO - __main__ - ['not_entailment']
03/12/2022 22:16:59 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/12/2022 22:16:59 - INFO - __main__ - ['not_entailment']
03/12/2022 22:16:59 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/12/2022 22:16:59 - INFO - __main__ - ['not_entailment']
03/12/2022 22:16:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 22:16:59 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:16:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:16:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:16:59 - INFO - __main__ - Printing 3 examples
03/12/2022 22:16:59 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/12/2022 22:16:59 - INFO - __main__ - ['not_entailment']
03/12/2022 22:16:59 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/12/2022 22:16:59 - INFO - __main__ - ['not_entailment']
03/12/2022 22:16:59 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/12/2022 22:16:59 - INFO - __main__ - ['not_entailment']
03/12/2022 22:16:59 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:16:59 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:16:59 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:17:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:17:10 - INFO - __main__ - Starting training!
03/12/2022 22:17:14 - INFO - __main__ - Step 10 Global step 10 Train loss 21.599863 on epoch=4
03/12/2022 22:17:18 - INFO - __main__ - Step 20 Global step 20 Train loss 11.498017 on epoch=9
03/12/2022 22:17:23 - INFO - __main__ - Step 30 Global step 30 Train loss 5.551821 on epoch=14
03/12/2022 22:17:28 - INFO - __main__ - Step 40 Global step 40 Train loss 4.381860 on epoch=19
03/12/2022 22:17:33 - INFO - __main__ - Step 50 Global step 50 Train loss 3.994511 on epoch=24
03/12/2022 22:17:34 - INFO - __main__ - Global step 50 Train loss 9.405214 ACC 0.125 on epoch=24
03/12/2022 22:17:39 - INFO - __main__ - Step 60 Global step 60 Train loss 3.070858 on epoch=29
03/12/2022 22:17:44 - INFO - __main__ - Step 70 Global step 70 Train loss 1.827928 on epoch=34
03/12/2022 22:17:49 - INFO - __main__ - Step 80 Global step 80 Train loss 1.436056 on epoch=39
03/12/2022 22:17:54 - INFO - __main__ - Step 90 Global step 90 Train loss 1.104119 on epoch=44
03/12/2022 22:17:59 - INFO - __main__ - Step 100 Global step 100 Train loss 1.025909 on epoch=49
03/12/2022 22:17:59 - INFO - __main__ - Global step 100 Train loss 1.692974 ACC 0.5 on epoch=49
03/12/2022 22:18:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.571165 on epoch=54
03/12/2022 22:18:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.870031 on epoch=59
03/12/2022 22:18:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.653716 on epoch=64
03/12/2022 22:18:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.576579 on epoch=69
03/12/2022 22:18:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.533589 on epoch=74
03/12/2022 22:18:25 - INFO - __main__ - Global step 150 Train loss 0.641016 ACC 0.5 on epoch=74
03/12/2022 22:18:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.632619 on epoch=79
03/12/2022 22:18:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.635370 on epoch=84
03/12/2022 22:18:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.459550 on epoch=89
03/12/2022 22:18:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.483911 on epoch=94
03/12/2022 22:18:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.343494 on epoch=99
03/12/2022 22:18:51 - INFO - __main__ - Global step 200 Train loss 0.510989 ACC 0.53125 on epoch=99
03/12/2022 22:18:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.254276 on epoch=104
03/12/2022 22:19:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.447275 on epoch=109
03/12/2022 22:19:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.377850 on epoch=114
03/12/2022 22:19:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.287796 on epoch=119
03/12/2022 22:19:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.314048 on epoch=124
03/12/2022 22:19:16 - INFO - __main__ - Global step 250 Train loss 0.336249 ACC 0.53125 on epoch=124
03/12/2022 22:19:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.349218 on epoch=129
03/12/2022 22:19:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.235262 on epoch=134
03/12/2022 22:19:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.244114 on epoch=139
03/12/2022 22:19:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.219798 on epoch=144
03/12/2022 22:19:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.213673 on epoch=149
03/12/2022 22:19:42 - INFO - __main__ - Global step 300 Train loss 0.252413 ACC 0.5 on epoch=149
03/12/2022 22:19:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.267660 on epoch=154
03/12/2022 22:19:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.287335 on epoch=159
03/12/2022 22:19:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.210223 on epoch=164
03/12/2022 22:20:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.192278 on epoch=169
03/12/2022 22:20:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.192483 on epoch=174
03/12/2022 22:20:07 - INFO - __main__ - Global step 350 Train loss 0.229996 ACC 0.5 on epoch=174
03/12/2022 22:20:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.161874 on epoch=179
03/12/2022 22:20:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.165946 on epoch=184
03/12/2022 22:20:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.160907 on epoch=189
03/12/2022 22:20:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.146810 on epoch=194
03/12/2022 22:20:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.147229 on epoch=199
03/12/2022 22:20:32 - INFO - __main__ - Global step 400 Train loss 0.156553 ACC 0.46875 on epoch=199
03/12/2022 22:20:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.166754 on epoch=204
03/12/2022 22:20:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.563482 on epoch=209
03/12/2022 22:20:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.152444 on epoch=214
03/12/2022 22:20:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.133824 on epoch=219
03/12/2022 22:20:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.145454 on epoch=224
03/12/2022 22:20:57 - INFO - __main__ - Global step 450 Train loss 0.232392 ACC 0.46875 on epoch=224
03/12/2022 22:21:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.180895 on epoch=229
03/12/2022 22:21:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.147444 on epoch=234
03/12/2022 22:21:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.136294 on epoch=239
03/12/2022 22:21:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.113529 on epoch=244
03/12/2022 22:21:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.169250 on epoch=249
03/12/2022 22:21:21 - INFO - __main__ - Global step 500 Train loss 0.149482 ACC 0.4375 on epoch=249
03/12/2022 22:21:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.370479 on epoch=254
03/12/2022 22:21:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.151307 on epoch=259
03/12/2022 22:21:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.149160 on epoch=264
03/12/2022 22:21:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.176728 on epoch=269
03/12/2022 22:21:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.145281 on epoch=274
03/12/2022 22:21:46 - INFO - __main__ - Global step 550 Train loss 0.198591 ACC 0.5625 on epoch=274
03/12/2022 22:21:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.152715 on epoch=279
03/12/2022 22:21:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.529672 on epoch=284
03/12/2022 22:22:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.194310 on epoch=289
03/12/2022 22:22:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.151783 on epoch=294
03/12/2022 22:22:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.132420 on epoch=299
03/12/2022 22:22:12 - INFO - __main__ - Global step 600 Train loss 0.232180 ACC 0.5625 on epoch=299
03/12/2022 22:22:12 - INFO - __main__ - save last model!
03/12/2022 22:22:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:22:13 - INFO - __main__ - Printing 3 examples
03/12/2022 22:22:13 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/12/2022 22:22:13 - INFO - __main__ - ['not_entailment']
03/12/2022 22:22:13 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/12/2022 22:22:13 - INFO - __main__ - ['not_entailment']
03/12/2022 22:22:13 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/12/2022 22:22:13 - INFO - __main__ - ['not_entailment']
03/12/2022 22:22:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 22:22:13 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:22:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:22:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:22:13 - INFO - __main__ - Printing 3 examples
03/12/2022 22:22:13 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/12/2022 22:22:13 - INFO - __main__ - ['not_entailment']
03/12/2022 22:22:13 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/12/2022 22:22:13 - INFO - __main__ - ['not_entailment']
03/12/2022 22:22:13 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/12/2022 22:22:13 - INFO - __main__ - ['not_entailment']
03/12/2022 22:22:13 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:22:13 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:22:13 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:22:19 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 22:22:20 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 22:22:20 - INFO - __main__ - Printing 3 examples
03/12/2022 22:22:20 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 22:22:20 - INFO - __main__ - ['entailment']
03/12/2022 22:22:20 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 22:22:20 - INFO - __main__ - ['not_entailment']
03/12/2022 22:22:20 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 22:22:20 - INFO - __main__ - ['not_entailment']
03/12/2022 22:22:20 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:22:22 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:22:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:22:24 - INFO - __main__ - Starting training!
03/12/2022 22:22:28 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 22:27:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_42_0.0005_8_predictions.txt
03/12/2022 22:27:15 - INFO - __main__ - ACC on test data: 0.5092
03/12/2022 22:27:16 - INFO - __main__ - prefix=glue-qnli_16_42, lr=0.0005, bsz=8, dev_performance=0.5625, test_performance=0.509244005125389
03/12/2022 22:27:16 - INFO - __main__ - Running ... prefix=glue-qnli_16_42, lr=0.0003, bsz=8 ...
03/12/2022 22:27:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:27:17 - INFO - __main__ - Printing 3 examples
03/12/2022 22:27:17 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/12/2022 22:27:17 - INFO - __main__ - ['not_entailment']
03/12/2022 22:27:17 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/12/2022 22:27:17 - INFO - __main__ - ['not_entailment']
03/12/2022 22:27:17 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/12/2022 22:27:17 - INFO - __main__ - ['not_entailment']
03/12/2022 22:27:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 22:27:17 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:27:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:27:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:27:17 - INFO - __main__ - Printing 3 examples
03/12/2022 22:27:17 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/12/2022 22:27:17 - INFO - __main__ - ['not_entailment']
03/12/2022 22:27:17 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/12/2022 22:27:17 - INFO - __main__ - ['not_entailment']
03/12/2022 22:27:17 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/12/2022 22:27:17 - INFO - __main__ - ['not_entailment']
03/12/2022 22:27:17 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:27:17 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:27:17 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:27:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:27:30 - INFO - __main__ - Starting training!
03/12/2022 22:27:34 - INFO - __main__ - Step 10 Global step 10 Train loss 21.297859 on epoch=4
03/12/2022 22:27:38 - INFO - __main__ - Step 20 Global step 20 Train loss 12.639007 on epoch=9
03/12/2022 22:27:43 - INFO - __main__ - Step 30 Global step 30 Train loss 7.200208 on epoch=14
03/12/2022 22:27:48 - INFO - __main__ - Step 40 Global step 40 Train loss 6.121413 on epoch=19
03/12/2022 22:27:53 - INFO - __main__ - Step 50 Global step 50 Train loss 5.093534 on epoch=24
03/12/2022 22:27:54 - INFO - __main__ - Global step 50 Train loss 10.470404 ACC 0.0 on epoch=24
03/12/2022 22:28:00 - INFO - __main__ - Step 60 Global step 60 Train loss 4.518574 on epoch=29
03/12/2022 22:28:05 - INFO - __main__ - Step 70 Global step 70 Train loss 4.415362 on epoch=34
03/12/2022 22:28:10 - INFO - __main__ - Step 80 Global step 80 Train loss 3.496030 on epoch=39
03/12/2022 22:28:14 - INFO - __main__ - Step 90 Global step 90 Train loss 3.062530 on epoch=44
03/12/2022 22:28:19 - INFO - __main__ - Step 100 Global step 100 Train loss 1.872560 on epoch=49
03/12/2022 22:28:20 - INFO - __main__ - Global step 100 Train loss 3.473011 ACC 0.5 on epoch=49
03/12/2022 22:28:26 - INFO - __main__ - Step 110 Global step 110 Train loss 1.560394 on epoch=54
03/12/2022 22:28:31 - INFO - __main__ - Step 120 Global step 120 Train loss 1.585230 on epoch=59
03/12/2022 22:28:36 - INFO - __main__ - Step 130 Global step 130 Train loss 1.883166 on epoch=64
03/12/2022 22:28:41 - INFO - __main__ - Step 140 Global step 140 Train loss 1.264194 on epoch=69
03/12/2022 22:28:46 - INFO - __main__ - Step 150 Global step 150 Train loss 1.172074 on epoch=74
03/12/2022 22:28:47 - INFO - __main__ - Global step 150 Train loss 1.493011 ACC 0.5 on epoch=74
03/12/2022 22:28:52 - INFO - __main__ - Step 160 Global step 160 Train loss 1.109984 on epoch=79
03/12/2022 22:28:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.919402 on epoch=84
03/12/2022 22:29:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.812808 on epoch=89
03/12/2022 22:29:07 - INFO - __main__ - Step 190 Global step 190 Train loss 1.092076 on epoch=94
03/12/2022 22:29:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.845739 on epoch=99
03/12/2022 22:29:13 - INFO - __main__ - Global step 200 Train loss 0.956002 ACC 0.5 on epoch=99
03/12/2022 22:29:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.561473 on epoch=104
03/12/2022 22:29:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.783081 on epoch=109
03/12/2022 22:29:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.692063 on epoch=114
03/12/2022 22:29:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.716766 on epoch=119
03/12/2022 22:29:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.790924 on epoch=124
03/12/2022 22:29:39 - INFO - __main__ - Global step 250 Train loss 0.708861 ACC 0.5 on epoch=124
03/12/2022 22:29:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.765437 on epoch=129
03/12/2022 22:29:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.607153 on epoch=134
03/12/2022 22:29:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.638727 on epoch=139
03/12/2022 22:29:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.688258 on epoch=144
03/12/2022 22:30:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.810235 on epoch=149
03/12/2022 22:30:05 - INFO - __main__ - Global step 300 Train loss 0.701962 ACC 0.5 on epoch=149
03/12/2022 22:30:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.757070 on epoch=154
03/12/2022 22:30:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.658640 on epoch=159
03/12/2022 22:30:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.650914 on epoch=164
03/12/2022 22:30:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.528339 on epoch=169
03/12/2022 22:30:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.614330 on epoch=174
03/12/2022 22:30:31 - INFO - __main__ - Global step 350 Train loss 0.641859 ACC 0.5 on epoch=174
03/12/2022 22:30:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.578466 on epoch=179
03/12/2022 22:30:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.407577 on epoch=184
03/12/2022 22:30:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.594475 on epoch=189
03/12/2022 22:30:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.575979 on epoch=194
03/12/2022 22:30:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.368562 on epoch=199
03/12/2022 22:30:57 - INFO - __main__ - Global step 400 Train loss 0.505012 ACC 0.5 on epoch=199
03/12/2022 22:31:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.312348 on epoch=204
03/12/2022 22:31:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.378519 on epoch=209
03/12/2022 22:31:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.340992 on epoch=214
03/12/2022 22:31:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.322366 on epoch=219
03/12/2022 22:31:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.324708 on epoch=224
03/12/2022 22:31:23 - INFO - __main__ - Global step 450 Train loss 0.335787 ACC 0.59375 on epoch=224
03/12/2022 22:31:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.398747 on epoch=229
03/12/2022 22:31:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.356909 on epoch=234
03/12/2022 22:31:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.304514 on epoch=239
03/12/2022 22:31:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.275998 on epoch=244
03/12/2022 22:31:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.371869 on epoch=249
03/12/2022 22:31:49 - INFO - __main__ - Global step 500 Train loss 0.341608 ACC 0.5 on epoch=249
03/12/2022 22:31:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.211378 on epoch=254
03/12/2022 22:31:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.231310 on epoch=259
03/12/2022 22:32:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.217890 on epoch=264
03/12/2022 22:32:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.202623 on epoch=269
03/12/2022 22:32:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.206980 on epoch=274
03/12/2022 22:32:15 - INFO - __main__ - Global step 550 Train loss 0.214036 ACC 0.5 on epoch=274
03/12/2022 22:32:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.198984 on epoch=279
03/12/2022 22:32:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.141609 on epoch=284
03/12/2022 22:32:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.192099 on epoch=289
03/12/2022 22:32:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.249181 on epoch=294
03/12/2022 22:32:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.216307 on epoch=299
03/12/2022 22:32:41 - INFO - __main__ - Global step 600 Train loss 0.199636 ACC 0.46875 on epoch=299
03/12/2022 22:32:41 - INFO - __main__ - save last model!
03/12/2022 22:32:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:32:41 - INFO - __main__ - Printing 3 examples
03/12/2022 22:32:41 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/12/2022 22:32:41 - INFO - __main__ - ['not_entailment']
03/12/2022 22:32:41 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/12/2022 22:32:41 - INFO - __main__ - ['not_entailment']
03/12/2022 22:32:41 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/12/2022 22:32:41 - INFO - __main__ - ['not_entailment']
03/12/2022 22:32:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 22:32:41 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:32:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:32:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:32:41 - INFO - __main__ - Printing 3 examples
03/12/2022 22:32:41 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/12/2022 22:32:41 - INFO - __main__ - ['not_entailment']
03/12/2022 22:32:41 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/12/2022 22:32:41 - INFO - __main__ - ['not_entailment']
03/12/2022 22:32:41 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/12/2022 22:32:41 - INFO - __main__ - ['not_entailment']
03/12/2022 22:32:41 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:32:41 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:32:41 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:32:49 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 22:32:49 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 22:32:49 - INFO - __main__ - Printing 3 examples
03/12/2022 22:32:49 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 22:32:49 - INFO - __main__ - ['entailment']
03/12/2022 22:32:49 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 22:32:49 - INFO - __main__ - ['not_entailment']
03/12/2022 22:32:49 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 22:32:49 - INFO - __main__ - ['not_entailment']
03/12/2022 22:32:49 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:32:52 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:32:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:32:52 - INFO - __main__ - Starting training!
03/12/2022 22:32:57 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 22:34:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_42_0.0003_8_predictions.txt
03/12/2022 22:34:55 - INFO - __main__ - ACC on test data: 0.5594
03/12/2022 22:34:56 - INFO - __main__ - prefix=glue-qnli_16_42, lr=0.0003, bsz=8, dev_performance=0.59375, test_performance=0.5593995972908659
03/12/2022 22:34:56 - INFO - __main__ - Running ... prefix=glue-qnli_16_42, lr=0.0002, bsz=8 ...
03/12/2022 22:34:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:34:57 - INFO - __main__ - Printing 3 examples
03/12/2022 22:34:57 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/12/2022 22:34:57 - INFO - __main__ - ['not_entailment']
03/12/2022 22:34:57 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/12/2022 22:34:57 - INFO - __main__ - ['not_entailment']
03/12/2022 22:34:57 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/12/2022 22:34:57 - INFO - __main__ - ['not_entailment']
03/12/2022 22:34:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 22:34:57 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:34:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:34:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:34:57 - INFO - __main__ - Printing 3 examples
03/12/2022 22:34:57 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/12/2022 22:34:57 - INFO - __main__ - ['not_entailment']
03/12/2022 22:34:57 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/12/2022 22:34:57 - INFO - __main__ - ['not_entailment']
03/12/2022 22:34:57 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/12/2022 22:34:57 - INFO - __main__ - ['not_entailment']
03/12/2022 22:34:57 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:34:57 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:34:57 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:35:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:35:10 - INFO - __main__ - Starting training!
03/12/2022 22:35:14 - INFO - __main__ - Step 10 Global step 10 Train loss 21.872768 on epoch=4
03/12/2022 22:35:18 - INFO - __main__ - Step 20 Global step 20 Train loss 13.687366 on epoch=9
03/12/2022 22:35:23 - INFO - __main__ - Step 30 Global step 30 Train loss 7.332982 on epoch=14
03/12/2022 22:35:28 - INFO - __main__ - Step 40 Global step 40 Train loss 6.907430 on epoch=19
03/12/2022 22:35:33 - INFO - __main__ - Step 50 Global step 50 Train loss 6.049130 on epoch=24
03/12/2022 22:35:34 - INFO - __main__ - Global step 50 Train loss 11.169934 ACC 0.03125 on epoch=24
03/12/2022 22:35:39 - INFO - __main__ - Step 60 Global step 60 Train loss 5.889075 on epoch=29
03/12/2022 22:35:44 - INFO - __main__ - Step 70 Global step 70 Train loss 5.173577 on epoch=34
03/12/2022 22:35:49 - INFO - __main__ - Step 80 Global step 80 Train loss 4.779129 on epoch=39
03/12/2022 22:35:54 - INFO - __main__ - Step 90 Global step 90 Train loss 4.421164 on epoch=44
03/12/2022 22:35:59 - INFO - __main__ - Step 100 Global step 100 Train loss 4.451576 on epoch=49
03/12/2022 22:36:00 - INFO - __main__ - Global step 100 Train loss 4.942904 ACC 0.21875 on epoch=49
03/12/2022 22:36:05 - INFO - __main__ - Step 110 Global step 110 Train loss 4.157698 on epoch=54
03/12/2022 22:36:10 - INFO - __main__ - Step 120 Global step 120 Train loss 3.799674 on epoch=59
03/12/2022 22:36:15 - INFO - __main__ - Step 130 Global step 130 Train loss 3.564433 on epoch=64
03/12/2022 22:36:20 - INFO - __main__ - Step 140 Global step 140 Train loss 2.998310 on epoch=69
03/12/2022 22:36:25 - INFO - __main__ - Step 150 Global step 150 Train loss 2.214143 on epoch=74
03/12/2022 22:36:26 - INFO - __main__ - Global step 150 Train loss 3.346851 ACC 0.0625 on epoch=74
03/12/2022 22:36:31 - INFO - __main__ - Step 160 Global step 160 Train loss 2.061365 on epoch=79
03/12/2022 22:36:36 - INFO - __main__ - Step 170 Global step 170 Train loss 1.401911 on epoch=84
03/12/2022 22:36:41 - INFO - __main__ - Step 180 Global step 180 Train loss 1.401254 on epoch=89
03/12/2022 22:36:46 - INFO - __main__ - Step 190 Global step 190 Train loss 1.504519 on epoch=94
03/12/2022 22:36:51 - INFO - __main__ - Step 200 Global step 200 Train loss 1.538717 on epoch=99
03/12/2022 22:36:52 - INFO - __main__ - Global step 200 Train loss 1.581553 ACC 0.5 on epoch=99
03/12/2022 22:36:57 - INFO - __main__ - Step 210 Global step 210 Train loss 1.352486 on epoch=104
03/12/2022 22:37:02 - INFO - __main__ - Step 220 Global step 220 Train loss 1.023243 on epoch=109
03/12/2022 22:37:07 - INFO - __main__ - Step 230 Global step 230 Train loss 1.167619 on epoch=114
03/12/2022 22:37:12 - INFO - __main__ - Step 240 Global step 240 Train loss 1.063130 on epoch=119
03/12/2022 22:37:17 - INFO - __main__ - Step 250 Global step 250 Train loss 1.383191 on epoch=124
03/12/2022 22:37:18 - INFO - __main__ - Global step 250 Train loss 1.197934 ACC 0.5 on epoch=124
03/12/2022 22:37:23 - INFO - __main__ - Step 260 Global step 260 Train loss 1.097083 on epoch=129
03/12/2022 22:37:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.793133 on epoch=134
03/12/2022 22:37:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.833345 on epoch=139
03/12/2022 22:37:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.807875 on epoch=144
03/12/2022 22:37:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.646893 on epoch=149
03/12/2022 22:37:43 - INFO - __main__ - Global step 300 Train loss 0.835666 ACC 0.5 on epoch=149
03/12/2022 22:37:49 - INFO - __main__ - Step 310 Global step 310 Train loss 1.038768 on epoch=154
03/12/2022 22:37:53 - INFO - __main__ - Step 320 Global step 320 Train loss 1.163750 on epoch=159
03/12/2022 22:37:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.769247 on epoch=164
03/12/2022 22:38:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.706385 on epoch=169
03/12/2022 22:38:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.748987 on epoch=174
03/12/2022 22:38:09 - INFO - __main__ - Global step 350 Train loss 0.885428 ACC 0.5 on epoch=174
03/12/2022 22:38:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.748235 on epoch=179
03/12/2022 22:38:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.859423 on epoch=184
03/12/2022 22:38:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.714638 on epoch=189
03/12/2022 22:38:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.674131 on epoch=194
03/12/2022 22:38:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.503358 on epoch=199
03/12/2022 22:38:34 - INFO - __main__ - Global step 400 Train loss 0.699957 ACC 0.5 on epoch=199
03/12/2022 22:38:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.884135 on epoch=204
03/12/2022 22:38:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.652069 on epoch=209
03/12/2022 22:38:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.668132 on epoch=214
03/12/2022 22:38:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.777144 on epoch=219
03/12/2022 22:38:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.674072 on epoch=224
03/12/2022 22:39:00 - INFO - __main__ - Global step 450 Train loss 0.731110 ACC 0.5 on epoch=224
03/12/2022 22:39:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.420242 on epoch=229
03/12/2022 22:39:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.532236 on epoch=234
03/12/2022 22:39:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.630094 on epoch=239
03/12/2022 22:39:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.457565 on epoch=244
03/12/2022 22:39:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.481400 on epoch=249
03/12/2022 22:39:25 - INFO - __main__ - Global step 500 Train loss 0.504307 ACC 0.5 on epoch=249
03/12/2022 22:39:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.637957 on epoch=254
03/12/2022 22:39:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.425856 on epoch=259
03/12/2022 22:39:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.422026 on epoch=264
03/12/2022 22:39:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.654173 on epoch=269
03/12/2022 22:39:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.371090 on epoch=274
03/12/2022 22:39:51 - INFO - __main__ - Global step 550 Train loss 0.502220 ACC 0.5 on epoch=274
03/12/2022 22:39:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.464237 on epoch=279
03/12/2022 22:40:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.397048 on epoch=284
03/12/2022 22:40:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.410367 on epoch=289
03/12/2022 22:40:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.338797 on epoch=294
03/12/2022 22:40:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.380458 on epoch=299
03/12/2022 22:40:16 - INFO - __main__ - Global step 600 Train loss 0.398181 ACC 0.5 on epoch=299
03/12/2022 22:40:16 - INFO - __main__ - save last model!
03/12/2022 22:40:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:40:17 - INFO - __main__ - Printing 3 examples
03/12/2022 22:40:17 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/12/2022 22:40:17 - INFO - __main__ - ['not_entailment']
03/12/2022 22:40:17 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/12/2022 22:40:17 - INFO - __main__ - ['not_entailment']
03/12/2022 22:40:17 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/12/2022 22:40:17 - INFO - __main__ - ['not_entailment']
03/12/2022 22:40:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 22:40:17 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:40:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:40:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:40:17 - INFO - __main__ - Printing 3 examples
03/12/2022 22:40:17 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/12/2022 22:40:17 - INFO - __main__ - ['not_entailment']
03/12/2022 22:40:17 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/12/2022 22:40:17 - INFO - __main__ - ['not_entailment']
03/12/2022 22:40:17 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/12/2022 22:40:17 - INFO - __main__ - ['not_entailment']
03/12/2022 22:40:17 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:40:17 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:40:17 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:40:23 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 22:40:24 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 22:40:24 - INFO - __main__ - Printing 3 examples
03/12/2022 22:40:24 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 22:40:24 - INFO - __main__ - ['entailment']
03/12/2022 22:40:24 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 22:40:24 - INFO - __main__ - ['not_entailment']
03/12/2022 22:40:24 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 22:40:24 - INFO - __main__ - ['not_entailment']
03/12/2022 22:40:24 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:40:26 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:40:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:40:28 - INFO - __main__ - Starting training!
03/12/2022 22:40:32 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 22:42:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_42_0.0002_8_predictions.txt
03/12/2022 22:42:23 - INFO - __main__ - ACC on test data: 0.5052
03/12/2022 22:42:23 - INFO - __main__ - prefix=glue-qnli_16_42, lr=0.0002, bsz=8, dev_performance=0.5, test_performance=0.5052169137836353
03/12/2022 22:42:23 - INFO - __main__ - Running ... prefix=glue-qnli_16_42, lr=0.0001, bsz=8 ...
03/12/2022 22:42:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:42:24 - INFO - __main__ - Printing 3 examples
03/12/2022 22:42:24 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/12/2022 22:42:24 - INFO - __main__ - ['not_entailment']
03/12/2022 22:42:24 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/12/2022 22:42:24 - INFO - __main__ - ['not_entailment']
03/12/2022 22:42:24 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/12/2022 22:42:24 - INFO - __main__ - ['not_entailment']
03/12/2022 22:42:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 22:42:24 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:42:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:42:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:42:24 - INFO - __main__ - Printing 3 examples
03/12/2022 22:42:24 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/12/2022 22:42:24 - INFO - __main__ - ['not_entailment']
03/12/2022 22:42:24 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/12/2022 22:42:24 - INFO - __main__ - ['not_entailment']
03/12/2022 22:42:24 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/12/2022 22:42:24 - INFO - __main__ - ['not_entailment']
03/12/2022 22:42:24 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:42:24 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:42:24 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:42:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:42:37 - INFO - __main__ - Starting training!
03/12/2022 22:42:41 - INFO - __main__ - Step 10 Global step 10 Train loss 21.571280 on epoch=4
03/12/2022 22:42:46 - INFO - __main__ - Step 20 Global step 20 Train loss 18.801945 on epoch=9
03/12/2022 22:42:51 - INFO - __main__ - Step 30 Global step 30 Train loss 11.175662 on epoch=14
03/12/2022 22:42:56 - INFO - __main__ - Step 40 Global step 40 Train loss 8.095263 on epoch=19
03/12/2022 22:43:01 - INFO - __main__ - Step 50 Global step 50 Train loss 6.615928 on epoch=24
03/12/2022 22:43:03 - INFO - __main__ - Global step 50 Train loss 13.252015 ACC 0.0 on epoch=24
03/12/2022 22:43:09 - INFO - __main__ - Step 60 Global step 60 Train loss 6.427920 on epoch=29
03/12/2022 22:43:14 - INFO - __main__ - Step 70 Global step 70 Train loss 5.572381 on epoch=34
03/12/2022 22:43:18 - INFO - __main__ - Step 80 Global step 80 Train loss 5.972721 on epoch=39
03/12/2022 22:43:23 - INFO - __main__ - Step 90 Global step 90 Train loss 5.555741 on epoch=44
03/12/2022 22:43:28 - INFO - __main__ - Step 100 Global step 100 Train loss 5.217745 on epoch=49
03/12/2022 22:43:29 - INFO - __main__ - Global step 100 Train loss 5.749302 ACC 0.09375 on epoch=49
03/12/2022 22:43:35 - INFO - __main__ - Step 110 Global step 110 Train loss 5.252366 on epoch=54
03/12/2022 22:43:40 - INFO - __main__ - Step 120 Global step 120 Train loss 5.218877 on epoch=59
03/12/2022 22:43:45 - INFO - __main__ - Step 130 Global step 130 Train loss 4.905608 on epoch=64
03/12/2022 22:43:50 - INFO - __main__ - Step 140 Global step 140 Train loss 5.096938 on epoch=69
03/12/2022 22:43:55 - INFO - __main__ - Step 150 Global step 150 Train loss 4.591767 on epoch=74
03/12/2022 22:43:55 - INFO - __main__ - Global step 150 Train loss 5.013112 ACC 0.34375 on epoch=74
03/12/2022 22:44:01 - INFO - __main__ - Step 160 Global step 160 Train loss 4.225165 on epoch=79
03/12/2022 22:44:06 - INFO - __main__ - Step 170 Global step 170 Train loss 4.076833 on epoch=84
03/12/2022 22:44:11 - INFO - __main__ - Step 180 Global step 180 Train loss 4.093601 on epoch=89
03/12/2022 22:44:16 - INFO - __main__ - Step 190 Global step 190 Train loss 4.140102 on epoch=94
03/12/2022 22:44:21 - INFO - __main__ - Step 200 Global step 200 Train loss 3.743331 on epoch=99
03/12/2022 22:44:21 - INFO - __main__ - Global step 200 Train loss 4.055807 ACC 0.5 on epoch=99
03/12/2022 22:44:27 - INFO - __main__ - Step 210 Global step 210 Train loss 3.342527 on epoch=104
03/12/2022 22:44:32 - INFO - __main__ - Step 220 Global step 220 Train loss 3.216023 on epoch=109
03/12/2022 22:44:37 - INFO - __main__ - Step 230 Global step 230 Train loss 3.086860 on epoch=114
03/12/2022 22:44:42 - INFO - __main__ - Step 240 Global step 240 Train loss 2.613694 on epoch=119
03/12/2022 22:44:47 - INFO - __main__ - Step 250 Global step 250 Train loss 2.130822 on epoch=124
03/12/2022 22:44:47 - INFO - __main__ - Global step 250 Train loss 2.877985 ACC 0.5 on epoch=124
03/12/2022 22:44:52 - INFO - __main__ - Step 260 Global step 260 Train loss 2.253766 on epoch=129
03/12/2022 22:44:57 - INFO - __main__ - Step 270 Global step 270 Train loss 1.633258 on epoch=134
03/12/2022 22:45:02 - INFO - __main__ - Step 280 Global step 280 Train loss 1.863138 on epoch=139
03/12/2022 22:45:07 - INFO - __main__ - Step 290 Global step 290 Train loss 1.801479 on epoch=144
03/12/2022 22:45:12 - INFO - __main__ - Step 300 Global step 300 Train loss 1.155376 on epoch=149
03/12/2022 22:45:12 - INFO - __main__ - Global step 300 Train loss 1.741403 ACC 0.5 on epoch=149
03/12/2022 22:45:17 - INFO - __main__ - Step 310 Global step 310 Train loss 1.373203 on epoch=154
03/12/2022 22:45:22 - INFO - __main__ - Step 320 Global step 320 Train loss 1.198524 on epoch=159
03/12/2022 22:45:27 - INFO - __main__ - Step 330 Global step 330 Train loss 1.151820 on epoch=164
03/12/2022 22:45:32 - INFO - __main__ - Step 340 Global step 340 Train loss 1.165094 on epoch=169
03/12/2022 22:45:37 - INFO - __main__ - Step 350 Global step 350 Train loss 1.091400 on epoch=174
03/12/2022 22:45:37 - INFO - __main__ - Global step 350 Train loss 1.196008 ACC 0.5 on epoch=174
03/12/2022 22:45:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.432841 on epoch=179
03/12/2022 22:45:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.250981 on epoch=184
03/12/2022 22:45:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.198926 on epoch=189
03/12/2022 22:45:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.159746 on epoch=194
03/12/2022 22:46:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.157811 on epoch=199
03/12/2022 22:46:03 - INFO - __main__ - Global step 400 Train loss 0.240061 ACC 0.46875 on epoch=199
03/12/2022 22:46:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.177506 on epoch=204
03/12/2022 22:46:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.184531 on epoch=209
03/12/2022 22:46:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.194904 on epoch=214
03/12/2022 22:46:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.141547 on epoch=219
03/12/2022 22:46:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.145320 on epoch=224
03/12/2022 22:46:28 - INFO - __main__ - Global step 450 Train loss 0.168762 ACC 0.5 on epoch=224
03/12/2022 22:46:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.126016 on epoch=229
03/12/2022 22:46:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.142125 on epoch=234
03/12/2022 22:46:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.149018 on epoch=239
03/12/2022 22:46:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.110596 on epoch=244
03/12/2022 22:46:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.103884 on epoch=249
03/12/2022 22:46:53 - INFO - __main__ - Global step 500 Train loss 0.126328 ACC 0.5625 on epoch=249
03/12/2022 22:46:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.101907 on epoch=254
03/12/2022 22:47:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.094039 on epoch=259
03/12/2022 22:47:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.108224 on epoch=264
03/12/2022 22:47:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.099563 on epoch=269
03/12/2022 22:47:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.073635 on epoch=274
03/12/2022 22:47:19 - INFO - __main__ - Global step 550 Train loss 0.095474 ACC 0.53125 on epoch=274
03/12/2022 22:47:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.073502 on epoch=279
03/12/2022 22:47:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.058733 on epoch=284
03/12/2022 22:47:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.040590 on epoch=289
03/12/2022 22:47:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.054324 on epoch=294
03/12/2022 22:47:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.103869 on epoch=299
03/12/2022 22:47:44 - INFO - __main__ - Global step 600 Train loss 0.066204 ACC 0.59375 on epoch=299
03/12/2022 22:47:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:47:45 - INFO - __main__ - Printing 3 examples
03/12/2022 22:47:45 - INFO - __main__ -  [glue-qnli] question: Where does the Rhine originate? [SEP] sentence: The Rhine originates in a 30 square kilometre area in Switzerland and represents almost 60 percent of water exported from the country.
03/12/2022 22:47:45 - INFO - __main__ - ['entailment']
03/12/2022 22:47:45 - INFO - __main__ -  [glue-qnli] question: What is the children's agency of the United Nations Organization? [SEP] sentence: The United Nations Organization and its children's agency UNICEF withdrew their staff, saying that it wasn't sure the event would help its mission of raising awareness of conditions for children and amid concerns that the relay would be used as a propaganda stunt.
03/12/2022 22:47:45 - INFO - __main__ - ['entailment']
03/12/2022 22:47:45 - INFO - __main__ -  [glue-qnli] question: In what year did Paul VI die? [SEP] sentence: This became the last of Paul VI's consistories before his death in August 1978.
03/12/2022 22:47:45 - INFO - __main__ - ['entailment']
03/12/2022 22:47:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 22:47:45 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:47:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:47:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:47:45 - INFO - __main__ - Printing 3 examples
03/12/2022 22:47:45 - INFO - __main__ -  [glue-qnli] question: What movement came out of the French Revolution? [SEP] sentence: Nationalism during the 19th century threatened the old aristocratic regimes.
03/12/2022 22:47:45 - INFO - __main__ - ['entailment']
03/12/2022 22:47:45 - INFO - __main__ -  [glue-qnli] question: What is not possible unless the reactants surmount an energy barrier known as the activation energy? [SEP] sentence: Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy.
03/12/2022 22:47:45 - INFO - __main__ - ['entailment']
03/12/2022 22:47:45 - INFO - __main__ -  [glue-qnli] question: What percent of sales are contributed by hunters? [SEP] sentence: Although non-hunters buy a significant number of Duck Stamps, eighty-seven percent of their sales are contributed by hunters, which is logical, as hunters are required to purchase them.
03/12/2022 22:47:45 - INFO - __main__ - ['entailment']
03/12/2022 22:47:45 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:47:45 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:47:45 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:47:45 - INFO - __main__ - save last model!
03/12/2022 22:47:52 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 22:47:53 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 22:47:53 - INFO - __main__ - Printing 3 examples
03/12/2022 22:47:53 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 22:47:53 - INFO - __main__ - ['entailment']
03/12/2022 22:47:53 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 22:47:53 - INFO - __main__ - ['not_entailment']
03/12/2022 22:47:53 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 22:47:53 - INFO - __main__ - ['not_entailment']
03/12/2022 22:47:53 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:47:56 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:47:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:47:58 - INFO - __main__ - Starting training!
03/12/2022 22:48:01 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 22:49:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_42_0.0001_8_predictions.txt
03/12/2022 22:49:55 - INFO - __main__ - ACC on test data: 0.5682
03/12/2022 22:49:55 - INFO - __main__ - prefix=glue-qnli_16_42, lr=0.0001, bsz=8, dev_performance=0.59375, test_performance=0.5681859784001464
03/12/2022 22:49:55 - INFO - __main__ - Running ... prefix=glue-qnli_16_87, lr=0.0005, bsz=8 ...
03/12/2022 22:49:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:49:56 - INFO - __main__ - Printing 3 examples
03/12/2022 22:49:56 - INFO - __main__ -  [glue-qnli] question: Where does the Rhine originate? [SEP] sentence: The Rhine originates in a 30 square kilometre area in Switzerland and represents almost 60 percent of water exported from the country.
03/12/2022 22:49:56 - INFO - __main__ - ['entailment']
03/12/2022 22:49:56 - INFO - __main__ -  [glue-qnli] question: What is the children's agency of the United Nations Organization? [SEP] sentence: The United Nations Organization and its children's agency UNICEF withdrew their staff, saying that it wasn't sure the event would help its mission of raising awareness of conditions for children and amid concerns that the relay would be used as a propaganda stunt.
03/12/2022 22:49:56 - INFO - __main__ - ['entailment']
03/12/2022 22:49:56 - INFO - __main__ -  [glue-qnli] question: In what year did Paul VI die? [SEP] sentence: This became the last of Paul VI's consistories before his death in August 1978.
03/12/2022 22:49:56 - INFO - __main__ - ['entailment']
03/12/2022 22:49:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 22:49:56 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:49:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:49:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:49:56 - INFO - __main__ - Printing 3 examples
03/12/2022 22:49:56 - INFO - __main__ -  [glue-qnli] question: What movement came out of the French Revolution? [SEP] sentence: Nationalism during the 19th century threatened the old aristocratic regimes.
03/12/2022 22:49:56 - INFO - __main__ - ['entailment']
03/12/2022 22:49:56 - INFO - __main__ -  [glue-qnli] question: What is not possible unless the reactants surmount an energy barrier known as the activation energy? [SEP] sentence: Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy.
03/12/2022 22:49:56 - INFO - __main__ - ['entailment']
03/12/2022 22:49:56 - INFO - __main__ -  [glue-qnli] question: What percent of sales are contributed by hunters? [SEP] sentence: Although non-hunters buy a significant number of Duck Stamps, eighty-seven percent of their sales are contributed by hunters, which is logical, as hunters are required to purchase them.
03/12/2022 22:49:56 - INFO - __main__ - ['entailment']
03/12/2022 22:49:56 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:49:56 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:49:56 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:50:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:50:09 - INFO - __main__ - Starting training!
03/12/2022 22:50:13 - INFO - __main__ - Step 10 Global step 10 Train loss 23.323624 on epoch=4
03/12/2022 22:50:18 - INFO - __main__ - Step 20 Global step 20 Train loss 14.620420 on epoch=9
03/12/2022 22:50:23 - INFO - __main__ - Step 30 Global step 30 Train loss 6.778059 on epoch=14
03/12/2022 22:50:28 - INFO - __main__ - Step 40 Global step 40 Train loss 5.241807 on epoch=19
03/12/2022 22:50:33 - INFO - __main__ - Step 50 Global step 50 Train loss 4.085346 on epoch=24
03/12/2022 22:50:34 - INFO - __main__ - Global step 50 Train loss 10.809851 ACC 0.09375 on epoch=24
03/12/2022 22:50:39 - INFO - __main__ - Step 60 Global step 60 Train loss 3.547523 on epoch=29
03/12/2022 22:50:45 - INFO - __main__ - Step 70 Global step 70 Train loss 2.671822 on epoch=34
03/12/2022 22:50:50 - INFO - __main__ - Step 80 Global step 80 Train loss 1.559351 on epoch=39
03/12/2022 22:50:55 - INFO - __main__ - Step 90 Global step 90 Train loss 1.017663 on epoch=44
03/12/2022 22:51:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.785863 on epoch=49
03/12/2022 22:51:00 - INFO - __main__ - Global step 100 Train loss 1.916444 ACC 0.5 on epoch=49
03/12/2022 22:51:06 - INFO - __main__ - Step 110 Global step 110 Train loss 1.200384 on epoch=54
03/12/2022 22:51:11 - INFO - __main__ - Step 120 Global step 120 Train loss 1.042471 on epoch=59
03/12/2022 22:51:16 - INFO - __main__ - Step 130 Global step 130 Train loss 1.324782 on epoch=64
03/12/2022 22:51:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.817486 on epoch=69
03/12/2022 22:51:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.955105 on epoch=74
03/12/2022 22:51:27 - INFO - __main__ - Global step 150 Train loss 1.068045 ACC 0.5 on epoch=74
03/12/2022 22:51:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.940031 on epoch=79
03/12/2022 22:51:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.880234 on epoch=84
03/12/2022 22:51:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.666189 on epoch=89
03/12/2022 22:51:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.744994 on epoch=94
03/12/2022 22:51:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.614950 on epoch=99
03/12/2022 22:51:53 - INFO - __main__ - Global step 200 Train loss 0.769280 ACC 0.5625 on epoch=99
03/12/2022 22:51:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.546757 on epoch=104
03/12/2022 22:52:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.491415 on epoch=109
03/12/2022 22:52:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.531356 on epoch=114
03/12/2022 22:52:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.484484 on epoch=119
03/12/2022 22:52:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.354535 on epoch=124
03/12/2022 22:52:20 - INFO - __main__ - Global step 250 Train loss 0.481710 ACC 0.5 on epoch=124
03/12/2022 22:52:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.450573 on epoch=129
03/12/2022 22:52:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.364967 on epoch=134
03/12/2022 22:52:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.330905 on epoch=139
03/12/2022 22:52:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.310108 on epoch=144
03/12/2022 22:52:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.296313 on epoch=149
03/12/2022 22:52:46 - INFO - __main__ - Global step 300 Train loss 0.350573 ACC 0.5 on epoch=149
03/12/2022 22:52:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.242369 on epoch=154
03/12/2022 22:52:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.319028 on epoch=159
03/12/2022 22:53:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.235227 on epoch=164
03/12/2022 22:53:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.273852 on epoch=169
03/12/2022 22:53:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.246100 on epoch=174
03/12/2022 22:53:11 - INFO - __main__ - Global step 350 Train loss 0.263315 ACC 0.5 on epoch=174
03/12/2022 22:53:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.224681 on epoch=179
03/12/2022 22:53:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.324313 on epoch=184
03/12/2022 22:53:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.383415 on epoch=189
03/12/2022 22:53:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.186805 on epoch=194
03/12/2022 22:53:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.239585 on epoch=199
03/12/2022 22:53:37 - INFO - __main__ - Global step 400 Train loss 0.271760 ACC 0.53125 on epoch=199
03/12/2022 22:53:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.202666 on epoch=204
03/12/2022 22:53:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.211774 on epoch=209
03/12/2022 22:53:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.220656 on epoch=214
03/12/2022 22:53:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.168630 on epoch=219
03/12/2022 22:54:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.185617 on epoch=224
03/12/2022 22:54:03 - INFO - __main__ - Global step 450 Train loss 0.197868 ACC 0.5 on epoch=224
03/12/2022 22:54:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.201211 on epoch=229
03/12/2022 22:54:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.193813 on epoch=234
03/12/2022 22:54:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.138055 on epoch=239
03/12/2022 22:54:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.181123 on epoch=244
03/12/2022 22:54:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.149323 on epoch=249
03/12/2022 22:54:29 - INFO - __main__ - Global step 500 Train loss 0.172705 ACC 0.5 on epoch=249
03/12/2022 22:54:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.189746 on epoch=254
03/12/2022 22:54:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.168927 on epoch=259
03/12/2022 22:54:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.180509 on epoch=264
03/12/2022 22:54:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.162396 on epoch=269
03/12/2022 22:54:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.193122 on epoch=274
03/12/2022 22:54:54 - INFO - __main__ - Global step 550 Train loss 0.178940 ACC 0.5 on epoch=274
03/12/2022 22:55:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.164301 on epoch=279
03/12/2022 22:55:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.130548 on epoch=284
03/12/2022 22:55:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.161834 on epoch=289
03/12/2022 22:55:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.165567 on epoch=294
03/12/2022 22:55:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.144900 on epoch=299
03/12/2022 22:55:20 - INFO - __main__ - Global step 600 Train loss 0.153430 ACC 0.4375 on epoch=299
03/12/2022 22:55:20 - INFO - __main__ - save last model!
03/12/2022 22:55:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:55:22 - INFO - __main__ - Printing 3 examples
03/12/2022 22:55:22 - INFO - __main__ -  [glue-qnli] question: Where does the Rhine originate? [SEP] sentence: The Rhine originates in a 30 square kilometre area in Switzerland and represents almost 60 percent of water exported from the country.
03/12/2022 22:55:22 - INFO - __main__ - ['entailment']
03/12/2022 22:55:22 - INFO - __main__ -  [glue-qnli] question: What is the children's agency of the United Nations Organization? [SEP] sentence: The United Nations Organization and its children's agency UNICEF withdrew their staff, saying that it wasn't sure the event would help its mission of raising awareness of conditions for children and amid concerns that the relay would be used as a propaganda stunt.
03/12/2022 22:55:22 - INFO - __main__ - ['entailment']
03/12/2022 22:55:22 - INFO - __main__ -  [glue-qnli] question: In what year did Paul VI die? [SEP] sentence: This became the last of Paul VI's consistories before his death in August 1978.
03/12/2022 22:55:22 - INFO - __main__ - ['entailment']
03/12/2022 22:55:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 22:55:22 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:55:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:55:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:55:22 - INFO - __main__ - Printing 3 examples
03/12/2022 22:55:22 - INFO - __main__ -  [glue-qnli] question: What movement came out of the French Revolution? [SEP] sentence: Nationalism during the 19th century threatened the old aristocratic regimes.
03/12/2022 22:55:22 - INFO - __main__ - ['entailment']
03/12/2022 22:55:22 - INFO - __main__ -  [glue-qnli] question: What is not possible unless the reactants surmount an energy barrier known as the activation energy? [SEP] sentence: Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy.
03/12/2022 22:55:22 - INFO - __main__ - ['entailment']
03/12/2022 22:55:22 - INFO - __main__ -  [glue-qnli] question: What percent of sales are contributed by hunters? [SEP] sentence: Although non-hunters buy a significant number of Duck Stamps, eighty-seven percent of their sales are contributed by hunters, which is logical, as hunters are required to purchase them.
03/12/2022 22:55:22 - INFO - __main__ - ['entailment']
03/12/2022 22:55:22 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:55:22 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:55:22 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:55:28 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 22:55:29 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 22:55:29 - INFO - __main__ - Printing 3 examples
03/12/2022 22:55:29 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 22:55:29 - INFO - __main__ - ['entailment']
03/12/2022 22:55:29 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 22:55:29 - INFO - __main__ - ['not_entailment']
03/12/2022 22:55:29 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 22:55:29 - INFO - __main__ - ['not_entailment']
03/12/2022 22:55:29 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:55:31 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:55:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:55:33 - INFO - __main__ - Starting training!
03/12/2022 22:55:37 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 22:57:28 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_87_0.0005_8_predictions.txt
03/12/2022 22:57:28 - INFO - __main__ - ACC on test data: 0.5078
03/12/2022 22:57:28 - INFO - __main__ - prefix=glue-qnli_16_87, lr=0.0005, bsz=8, dev_performance=0.5625, test_performance=0.5077796082738422
03/12/2022 22:57:28 - INFO - __main__ - Running ... prefix=glue-qnli_16_87, lr=0.0003, bsz=8 ...
03/12/2022 22:57:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:57:29 - INFO - __main__ - Printing 3 examples
03/12/2022 22:57:29 - INFO - __main__ -  [glue-qnli] question: Where does the Rhine originate? [SEP] sentence: The Rhine originates in a 30 square kilometre area in Switzerland and represents almost 60 percent of water exported from the country.
03/12/2022 22:57:29 - INFO - __main__ - ['entailment']
03/12/2022 22:57:29 - INFO - __main__ -  [glue-qnli] question: What is the children's agency of the United Nations Organization? [SEP] sentence: The United Nations Organization and its children's agency UNICEF withdrew their staff, saying that it wasn't sure the event would help its mission of raising awareness of conditions for children and amid concerns that the relay would be used as a propaganda stunt.
03/12/2022 22:57:29 - INFO - __main__ - ['entailment']
03/12/2022 22:57:29 - INFO - __main__ -  [glue-qnli] question: In what year did Paul VI die? [SEP] sentence: This became the last of Paul VI's consistories before his death in August 1978.
03/12/2022 22:57:29 - INFO - __main__ - ['entailment']
03/12/2022 22:57:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 22:57:29 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:57:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 22:57:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 22:57:29 - INFO - __main__ - Printing 3 examples
03/12/2022 22:57:29 - INFO - __main__ -  [glue-qnli] question: What movement came out of the French Revolution? [SEP] sentence: Nationalism during the 19th century threatened the old aristocratic regimes.
03/12/2022 22:57:29 - INFO - __main__ - ['entailment']
03/12/2022 22:57:29 - INFO - __main__ -  [glue-qnli] question: What is not possible unless the reactants surmount an energy barrier known as the activation energy? [SEP] sentence: Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy.
03/12/2022 22:57:29 - INFO - __main__ - ['entailment']
03/12/2022 22:57:29 - INFO - __main__ -  [glue-qnli] question: What percent of sales are contributed by hunters? [SEP] sentence: Although non-hunters buy a significant number of Duck Stamps, eighty-seven percent of their sales are contributed by hunters, which is logical, as hunters are required to purchase them.
03/12/2022 22:57:29 - INFO - __main__ - ['entailment']
03/12/2022 22:57:29 - INFO - __main__ - Tokenizing Input ...
03/12/2022 22:57:29 - INFO - __main__ - Tokenizing Output ...
03/12/2022 22:57:29 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 22:57:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 22:57:42 - INFO - __main__ - Starting training!
03/12/2022 22:57:46 - INFO - __main__ - Step 10 Global step 10 Train loss 21.632389 on epoch=4
03/12/2022 22:57:51 - INFO - __main__ - Step 20 Global step 20 Train loss 17.101162 on epoch=9
03/12/2022 22:57:56 - INFO - __main__ - Step 30 Global step 30 Train loss 9.205278 on epoch=14
03/12/2022 22:58:00 - INFO - __main__ - Step 40 Global step 40 Train loss 7.003583 on epoch=19
03/12/2022 22:58:06 - INFO - __main__ - Step 50 Global step 50 Train loss 5.870696 on epoch=24
03/12/2022 22:58:06 - INFO - __main__ - Global step 50 Train loss 12.162621 ACC 0.0625 on epoch=24
03/12/2022 22:58:12 - INFO - __main__ - Step 60 Global step 60 Train loss 5.485781 on epoch=29
03/12/2022 22:58:17 - INFO - __main__ - Step 70 Global step 70 Train loss 4.503800 on epoch=34
03/12/2022 22:58:22 - INFO - __main__ - Step 80 Global step 80 Train loss 4.365075 on epoch=39
03/12/2022 22:58:27 - INFO - __main__ - Step 90 Global step 90 Train loss 3.677679 on epoch=44
03/12/2022 22:58:32 - INFO - __main__ - Step 100 Global step 100 Train loss 3.194104 on epoch=49
03/12/2022 22:58:33 - INFO - __main__ - Global step 100 Train loss 4.245288 ACC 0.5 on epoch=49
03/12/2022 22:58:39 - INFO - __main__ - Step 110 Global step 110 Train loss 2.575610 on epoch=54
03/12/2022 22:58:44 - INFO - __main__ - Step 120 Global step 120 Train loss 2.129915 on epoch=59
03/12/2022 22:58:49 - INFO - __main__ - Step 130 Global step 130 Train loss 1.808126 on epoch=64
03/12/2022 22:58:54 - INFO - __main__ - Step 140 Global step 140 Train loss 1.566492 on epoch=69
03/12/2022 22:58:59 - INFO - __main__ - Step 150 Global step 150 Train loss 1.277143 on epoch=74
03/12/2022 22:59:00 - INFO - __main__ - Global step 150 Train loss 1.871457 ACC 0.53125 on epoch=74
03/12/2022 22:59:05 - INFO - __main__ - Step 160 Global step 160 Train loss 1.279619 on epoch=79
03/12/2022 22:59:11 - INFO - __main__ - Step 170 Global step 170 Train loss 1.070997 on epoch=84
03/12/2022 22:59:16 - INFO - __main__ - Step 180 Global step 180 Train loss 1.137191 on epoch=89
03/12/2022 22:59:21 - INFO - __main__ - Step 190 Global step 190 Train loss 1.083990 on epoch=94
03/12/2022 22:59:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.908610 on epoch=99
03/12/2022 22:59:27 - INFO - __main__ - Global step 200 Train loss 1.096081 ACC 0.5 on epoch=99
03/12/2022 22:59:32 - INFO - __main__ - Step 210 Global step 210 Train loss 1.070354 on epoch=104
03/12/2022 22:59:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.842104 on epoch=109
03/12/2022 22:59:42 - INFO - __main__ - Step 230 Global step 230 Train loss 1.150175 on epoch=114
03/12/2022 22:59:47 - INFO - __main__ - Step 240 Global step 240 Train loss 1.013110 on epoch=119
03/12/2022 22:59:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.959585 on epoch=124
03/12/2022 22:59:53 - INFO - __main__ - Global step 250 Train loss 1.007066 ACC 0.5 on epoch=124
03/12/2022 22:59:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.848707 on epoch=129
03/12/2022 23:00:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.743642 on epoch=134
03/12/2022 23:00:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.657320 on epoch=139
03/12/2022 23:00:13 - INFO - __main__ - Step 290 Global step 290 Train loss 1.034831 on epoch=144
03/12/2022 23:00:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.551752 on epoch=149
03/12/2022 23:00:19 - INFO - __main__ - Global step 300 Train loss 0.767250 ACC 0.5 on epoch=149
03/12/2022 23:00:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.647660 on epoch=154
03/12/2022 23:00:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.550272 on epoch=159
03/12/2022 23:00:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.705527 on epoch=164
03/12/2022 23:00:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.571791 on epoch=169
03/12/2022 23:00:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.377709 on epoch=174
03/12/2022 23:00:45 - INFO - __main__ - Global step 350 Train loss 0.570592 ACC 0.5 on epoch=174
03/12/2022 23:00:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.584057 on epoch=179
03/12/2022 23:00:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.733893 on epoch=184
03/12/2022 23:01:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.676802 on epoch=189
03/12/2022 23:01:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.615986 on epoch=194
03/12/2022 23:01:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.459043 on epoch=199
03/12/2022 23:01:11 - INFO - __main__ - Global step 400 Train loss 0.613956 ACC 0.5 on epoch=199
03/12/2022 23:01:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.360817 on epoch=204
03/12/2022 23:01:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.560575 on epoch=209
03/12/2022 23:01:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.451889 on epoch=214
03/12/2022 23:01:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.397893 on epoch=219
03/12/2022 23:01:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.457989 on epoch=224
03/12/2022 23:01:38 - INFO - __main__ - Global step 450 Train loss 0.445833 ACC 0.5 on epoch=224
03/12/2022 23:01:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.451677 on epoch=229
03/12/2022 23:01:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.344552 on epoch=234
03/12/2022 23:01:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.405273 on epoch=239
03/12/2022 23:01:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.347533 on epoch=244
03/12/2022 23:02:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.433196 on epoch=249
03/12/2022 23:02:04 - INFO - __main__ - Global step 500 Train loss 0.396446 ACC 0.5 on epoch=249
03/12/2022 23:02:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.232300 on epoch=254
03/12/2022 23:02:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.337861 on epoch=259
03/12/2022 23:02:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.250678 on epoch=264
03/12/2022 23:02:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.238214 on epoch=269
03/12/2022 23:02:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.228965 on epoch=274
03/12/2022 23:02:30 - INFO - __main__ - Global step 550 Train loss 0.257604 ACC 0.5 on epoch=274
03/12/2022 23:02:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.323980 on epoch=279
03/12/2022 23:02:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.243745 on epoch=284
03/12/2022 23:02:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.202852 on epoch=289
03/12/2022 23:02:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.299926 on epoch=294
03/12/2022 23:02:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.220370 on epoch=299
03/12/2022 23:02:56 - INFO - __main__ - Global step 600 Train loss 0.258174 ACC 0.5 on epoch=299
03/12/2022 23:02:56 - INFO - __main__ - save last model!
03/12/2022 23:02:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:02:57 - INFO - __main__ - Printing 3 examples
03/12/2022 23:02:57 - INFO - __main__ -  [glue-qnli] question: Where does the Rhine originate? [SEP] sentence: The Rhine originates in a 30 square kilometre area in Switzerland and represents almost 60 percent of water exported from the country.
03/12/2022 23:02:57 - INFO - __main__ - ['entailment']
03/12/2022 23:02:57 - INFO - __main__ -  [glue-qnli] question: What is the children's agency of the United Nations Organization? [SEP] sentence: The United Nations Organization and its children's agency UNICEF withdrew their staff, saying that it wasn't sure the event would help its mission of raising awareness of conditions for children and amid concerns that the relay would be used as a propaganda stunt.
03/12/2022 23:02:57 - INFO - __main__ - ['entailment']
03/12/2022 23:02:57 - INFO - __main__ -  [glue-qnli] question: In what year did Paul VI die? [SEP] sentence: This became the last of Paul VI's consistories before his death in August 1978.
03/12/2022 23:02:57 - INFO - __main__ - ['entailment']
03/12/2022 23:02:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 23:02:57 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:02:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:02:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:02:57 - INFO - __main__ - Printing 3 examples
03/12/2022 23:02:57 - INFO - __main__ -  [glue-qnli] question: What movement came out of the French Revolution? [SEP] sentence: Nationalism during the 19th century threatened the old aristocratic regimes.
03/12/2022 23:02:57 - INFO - __main__ - ['entailment']
03/12/2022 23:02:57 - INFO - __main__ -  [glue-qnli] question: What is not possible unless the reactants surmount an energy barrier known as the activation energy? [SEP] sentence: Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy.
03/12/2022 23:02:57 - INFO - __main__ - ['entailment']
03/12/2022 23:02:57 - INFO - __main__ -  [glue-qnli] question: What percent of sales are contributed by hunters? [SEP] sentence: Although non-hunters buy a significant number of Duck Stamps, eighty-seven percent of their sales are contributed by hunters, which is logical, as hunters are required to purchase them.
03/12/2022 23:02:57 - INFO - __main__ - ['entailment']
03/12/2022 23:02:57 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:02:57 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:02:57 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:03:03 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 23:03:03 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 23:03:03 - INFO - __main__ - Printing 3 examples
03/12/2022 23:03:03 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 23:03:03 - INFO - __main__ - ['entailment']
03/12/2022 23:03:03 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 23:03:03 - INFO - __main__ - ['not_entailment']
03/12/2022 23:03:03 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 23:03:03 - INFO - __main__ - ['not_entailment']
03/12/2022 23:03:03 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:03:06 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:03:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:03:09 - INFO - __main__ - Starting training!
03/12/2022 23:03:11 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 23:05:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_87_0.0003_8_predictions.txt
03/12/2022 23:05:03 - INFO - __main__ - ACC on test data: 0.5074
03/12/2022 23:05:03 - INFO - __main__ - prefix=glue-qnli_16_87, lr=0.0003, bsz=8, dev_performance=0.53125, test_performance=0.5074135090609555
03/12/2022 23:05:03 - INFO - __main__ - Running ... prefix=glue-qnli_16_87, lr=0.0002, bsz=8 ...
03/12/2022 23:05:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:05:04 - INFO - __main__ - Printing 3 examples
03/12/2022 23:05:04 - INFO - __main__ -  [glue-qnli] question: Where does the Rhine originate? [SEP] sentence: The Rhine originates in a 30 square kilometre area in Switzerland and represents almost 60 percent of water exported from the country.
03/12/2022 23:05:04 - INFO - __main__ - ['entailment']
03/12/2022 23:05:04 - INFO - __main__ -  [glue-qnli] question: What is the children's agency of the United Nations Organization? [SEP] sentence: The United Nations Organization and its children's agency UNICEF withdrew their staff, saying that it wasn't sure the event would help its mission of raising awareness of conditions for children and amid concerns that the relay would be used as a propaganda stunt.
03/12/2022 23:05:04 - INFO - __main__ - ['entailment']
03/12/2022 23:05:04 - INFO - __main__ -  [glue-qnli] question: In what year did Paul VI die? [SEP] sentence: This became the last of Paul VI's consistories before his death in August 1978.
03/12/2022 23:05:04 - INFO - __main__ - ['entailment']
03/12/2022 23:05:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 23:05:04 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:05:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:05:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:05:04 - INFO - __main__ - Printing 3 examples
03/12/2022 23:05:04 - INFO - __main__ -  [glue-qnli] question: What movement came out of the French Revolution? [SEP] sentence: Nationalism during the 19th century threatened the old aristocratic regimes.
03/12/2022 23:05:04 - INFO - __main__ - ['entailment']
03/12/2022 23:05:04 - INFO - __main__ -  [glue-qnli] question: What is not possible unless the reactants surmount an energy barrier known as the activation energy? [SEP] sentence: Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy.
03/12/2022 23:05:04 - INFO - __main__ - ['entailment']
03/12/2022 23:05:04 - INFO - __main__ -  [glue-qnli] question: What percent of sales are contributed by hunters? [SEP] sentence: Although non-hunters buy a significant number of Duck Stamps, eighty-seven percent of their sales are contributed by hunters, which is logical, as hunters are required to purchase them.
03/12/2022 23:05:04 - INFO - __main__ - ['entailment']
03/12/2022 23:05:04 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:05:04 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:05:04 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:05:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:05:14 - INFO - __main__ - Starting training!
03/12/2022 23:05:18 - INFO - __main__ - Step 10 Global step 10 Train loss 22.479229 on epoch=4
03/12/2022 23:05:23 - INFO - __main__ - Step 20 Global step 20 Train loss 15.239314 on epoch=9
03/12/2022 23:05:28 - INFO - __main__ - Step 30 Global step 30 Train loss 7.640142 on epoch=14
03/12/2022 23:05:33 - INFO - __main__ - Step 40 Global step 40 Train loss 6.313544 on epoch=19
03/12/2022 23:05:38 - INFO - __main__ - Step 50 Global step 50 Train loss 5.705131 on epoch=24
03/12/2022 23:05:39 - INFO - __main__ - Global step 50 Train loss 11.475471 ACC 0.0 on epoch=24
03/12/2022 23:05:44 - INFO - __main__ - Step 60 Global step 60 Train loss 5.599736 on epoch=29
03/12/2022 23:05:49 - INFO - __main__ - Step 70 Global step 70 Train loss 4.832715 on epoch=34
03/12/2022 23:05:54 - INFO - __main__ - Step 80 Global step 80 Train loss 4.839236 on epoch=39
03/12/2022 23:05:59 - INFO - __main__ - Step 90 Global step 90 Train loss 4.427579 on epoch=44
03/12/2022 23:06:04 - INFO - __main__ - Step 100 Global step 100 Train loss 4.297036 on epoch=49
03/12/2022 23:06:05 - INFO - __main__ - Global step 100 Train loss 4.799261 ACC 0.375 on epoch=49
03/12/2022 23:06:11 - INFO - __main__ - Step 110 Global step 110 Train loss 4.224420 on epoch=54
03/12/2022 23:06:16 - INFO - __main__ - Step 120 Global step 120 Train loss 3.784446 on epoch=59
03/12/2022 23:06:21 - INFO - __main__ - Step 130 Global step 130 Train loss 3.332170 on epoch=64
03/12/2022 23:06:26 - INFO - __main__ - Step 140 Global step 140 Train loss 2.525684 on epoch=69
03/12/2022 23:06:31 - INFO - __main__ - Step 150 Global step 150 Train loss 2.554415 on epoch=74
03/12/2022 23:06:31 - INFO - __main__ - Global step 150 Train loss 3.284227 ACC 0.40625 on epoch=74
03/12/2022 23:06:37 - INFO - __main__ - Step 160 Global step 160 Train loss 2.216683 on epoch=79
03/12/2022 23:06:42 - INFO - __main__ - Step 170 Global step 170 Train loss 1.462823 on epoch=84
03/12/2022 23:06:47 - INFO - __main__ - Step 180 Global step 180 Train loss 1.733339 on epoch=89
03/12/2022 23:06:52 - INFO - __main__ - Step 190 Global step 190 Train loss 1.458819 on epoch=94
03/12/2022 23:06:57 - INFO - __main__ - Step 200 Global step 200 Train loss 1.449371 on epoch=99
03/12/2022 23:06:57 - INFO - __main__ - Global step 200 Train loss 1.664207 ACC 0.5 on epoch=99
03/12/2022 23:07:03 - INFO - __main__ - Step 210 Global step 210 Train loss 1.476780 on epoch=104
03/12/2022 23:07:08 - INFO - __main__ - Step 220 Global step 220 Train loss 1.294329 on epoch=109
03/12/2022 23:07:13 - INFO - __main__ - Step 230 Global step 230 Train loss 1.199437 on epoch=114
03/12/2022 23:07:18 - INFO - __main__ - Step 240 Global step 240 Train loss 1.351415 on epoch=119
03/12/2022 23:07:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.710714 on epoch=124
03/12/2022 23:07:24 - INFO - __main__ - Global step 250 Train loss 1.206535 ACC 0.5 on epoch=124
03/12/2022 23:07:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.835916 on epoch=129
03/12/2022 23:07:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.885427 on epoch=134
03/12/2022 23:07:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.865473 on epoch=139
03/12/2022 23:07:44 - INFO - __main__ - Step 290 Global step 290 Train loss 1.012123 on epoch=144
03/12/2022 23:07:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.924748 on epoch=149
03/12/2022 23:07:49 - INFO - __main__ - Global step 300 Train loss 0.904738 ACC 0.5 on epoch=149
03/12/2022 23:07:54 - INFO - __main__ - Step 310 Global step 310 Train loss 1.150884 on epoch=154
03/12/2022 23:07:59 - INFO - __main__ - Step 320 Global step 320 Train loss 1.057356 on epoch=159
03/12/2022 23:08:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.943184 on epoch=164
03/12/2022 23:08:09 - INFO - __main__ - Step 340 Global step 340 Train loss 1.043174 on epoch=169
03/12/2022 23:08:14 - INFO - __main__ - Step 350 Global step 350 Train loss 1.151471 on epoch=174
03/12/2022 23:08:15 - INFO - __main__ - Global step 350 Train loss 1.069214 ACC 0.5 on epoch=174
03/12/2022 23:08:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.849655 on epoch=179
03/12/2022 23:08:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.947154 on epoch=184
03/12/2022 23:08:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.892149 on epoch=189
03/12/2022 23:08:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.725762 on epoch=194
03/12/2022 23:08:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.676934 on epoch=199
03/12/2022 23:08:40 - INFO - __main__ - Global step 400 Train loss 0.818331 ACC 0.5 on epoch=199
03/12/2022 23:08:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.536606 on epoch=204
03/12/2022 23:08:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.745174 on epoch=209
03/12/2022 23:08:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.821296 on epoch=214
03/12/2022 23:09:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.827847 on epoch=219
03/12/2022 23:09:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.803393 on epoch=224
03/12/2022 23:09:06 - INFO - __main__ - Global step 450 Train loss 0.746863 ACC 0.5 on epoch=224
03/12/2022 23:09:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.723718 on epoch=229
03/12/2022 23:09:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.552466 on epoch=234
03/12/2022 23:09:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.730668 on epoch=239
03/12/2022 23:09:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.660247 on epoch=244
03/12/2022 23:09:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.695850 on epoch=249
03/12/2022 23:09:31 - INFO - __main__ - Global step 500 Train loss 0.672590 ACC 0.5 on epoch=249
03/12/2022 23:09:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.715318 on epoch=254
03/12/2022 23:09:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.650174 on epoch=259
03/12/2022 23:09:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.592026 on epoch=264
03/12/2022 23:09:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.613140 on epoch=269
03/12/2022 23:09:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.568047 on epoch=274
03/12/2022 23:09:56 - INFO - __main__ - Global step 550 Train loss 0.627741 ACC 0.5 on epoch=274
03/12/2022 23:10:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.711933 on epoch=279
03/12/2022 23:10:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.482036 on epoch=284
03/12/2022 23:10:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.501901 on epoch=289
03/12/2022 23:10:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.468851 on epoch=294
03/12/2022 23:10:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.497565 on epoch=299
03/12/2022 23:10:22 - INFO - __main__ - Global step 600 Train loss 0.532457 ACC 0.5 on epoch=299
03/12/2022 23:10:22 - INFO - __main__ - save last model!
03/12/2022 23:10:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:10:23 - INFO - __main__ - Printing 3 examples
03/12/2022 23:10:23 - INFO - __main__ -  [glue-qnli] question: Where does the Rhine originate? [SEP] sentence: The Rhine originates in a 30 square kilometre area in Switzerland and represents almost 60 percent of water exported from the country.
03/12/2022 23:10:23 - INFO - __main__ - ['entailment']
03/12/2022 23:10:23 - INFO - __main__ -  [glue-qnli] question: What is the children's agency of the United Nations Organization? [SEP] sentence: The United Nations Organization and its children's agency UNICEF withdrew their staff, saying that it wasn't sure the event would help its mission of raising awareness of conditions for children and amid concerns that the relay would be used as a propaganda stunt.
03/12/2022 23:10:23 - INFO - __main__ - ['entailment']
03/12/2022 23:10:23 - INFO - __main__ -  [glue-qnli] question: In what year did Paul VI die? [SEP] sentence: This became the last of Paul VI's consistories before his death in August 1978.
03/12/2022 23:10:23 - INFO - __main__ - ['entailment']
03/12/2022 23:10:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 23:10:23 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:10:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:10:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:10:23 - INFO - __main__ - Printing 3 examples
03/12/2022 23:10:23 - INFO - __main__ -  [glue-qnli] question: What movement came out of the French Revolution? [SEP] sentence: Nationalism during the 19th century threatened the old aristocratic regimes.
03/12/2022 23:10:23 - INFO - __main__ - ['entailment']
03/12/2022 23:10:23 - INFO - __main__ -  [glue-qnli] question: What is not possible unless the reactants surmount an energy barrier known as the activation energy? [SEP] sentence: Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy.
03/12/2022 23:10:23 - INFO - __main__ - ['entailment']
03/12/2022 23:10:23 - INFO - __main__ -  [glue-qnli] question: What percent of sales are contributed by hunters? [SEP] sentence: Although non-hunters buy a significant number of Duck Stamps, eighty-seven percent of their sales are contributed by hunters, which is logical, as hunters are required to purchase them.
03/12/2022 23:10:23 - INFO - __main__ - ['entailment']
03/12/2022 23:10:23 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:10:23 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:10:23 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:10:29 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 23:10:30 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 23:10:30 - INFO - __main__ - Printing 3 examples
03/12/2022 23:10:30 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 23:10:30 - INFO - __main__ - ['entailment']
03/12/2022 23:10:30 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 23:10:30 - INFO - __main__ - ['not_entailment']
03/12/2022 23:10:30 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 23:10:30 - INFO - __main__ - ['not_entailment']
03/12/2022 23:10:30 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:10:33 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:10:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:10:36 - INFO - __main__ - Starting training!
03/12/2022 23:10:38 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 23:12:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_87_0.0002_8_predictions.txt
03/12/2022 23:12:30 - INFO - __main__ - ACC on test data: 0.5045
03/12/2022 23:12:30 - INFO - __main__ - prefix=glue-qnli_16_87, lr=0.0002, bsz=8, dev_performance=0.5, test_performance=0.504484715357862
03/12/2022 23:12:31 - INFO - __main__ - Running ... prefix=glue-qnli_16_87, lr=0.0001, bsz=8 ...
03/12/2022 23:12:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:12:31 - INFO - __main__ - Printing 3 examples
03/12/2022 23:12:31 - INFO - __main__ -  [glue-qnli] question: Where does the Rhine originate? [SEP] sentence: The Rhine originates in a 30 square kilometre area in Switzerland and represents almost 60 percent of water exported from the country.
03/12/2022 23:12:31 - INFO - __main__ - ['entailment']
03/12/2022 23:12:31 - INFO - __main__ -  [glue-qnli] question: What is the children's agency of the United Nations Organization? [SEP] sentence: The United Nations Organization and its children's agency UNICEF withdrew their staff, saying that it wasn't sure the event would help its mission of raising awareness of conditions for children and amid concerns that the relay would be used as a propaganda stunt.
03/12/2022 23:12:31 - INFO - __main__ - ['entailment']
03/12/2022 23:12:31 - INFO - __main__ -  [glue-qnli] question: In what year did Paul VI die? [SEP] sentence: This became the last of Paul VI's consistories before his death in August 1978.
03/12/2022 23:12:31 - INFO - __main__ - ['entailment']
03/12/2022 23:12:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 23:12:31 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:12:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:12:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:12:31 - INFO - __main__ - Printing 3 examples
03/12/2022 23:12:31 - INFO - __main__ -  [glue-qnli] question: What movement came out of the French Revolution? [SEP] sentence: Nationalism during the 19th century threatened the old aristocratic regimes.
03/12/2022 23:12:31 - INFO - __main__ - ['entailment']
03/12/2022 23:12:31 - INFO - __main__ -  [glue-qnli] question: What is not possible unless the reactants surmount an energy barrier known as the activation energy? [SEP] sentence: Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy.
03/12/2022 23:12:31 - INFO - __main__ - ['entailment']
03/12/2022 23:12:31 - INFO - __main__ -  [glue-qnli] question: What percent of sales are contributed by hunters? [SEP] sentence: Although non-hunters buy a significant number of Duck Stamps, eighty-seven percent of their sales are contributed by hunters, which is logical, as hunters are required to purchase them.
03/12/2022 23:12:31 - INFO - __main__ - ['entailment']
03/12/2022 23:12:31 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:12:31 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:12:32 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:12:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:12:44 - INFO - __main__ - Starting training!
03/12/2022 23:12:48 - INFO - __main__ - Step 10 Global step 10 Train loss 21.643318 on epoch=4
03/12/2022 23:12:53 - INFO - __main__ - Step 20 Global step 20 Train loss 18.348133 on epoch=9
03/12/2022 23:12:58 - INFO - __main__ - Step 30 Global step 30 Train loss 13.994357 on epoch=14
03/12/2022 23:13:03 - INFO - __main__ - Step 40 Global step 40 Train loss 9.993745 on epoch=19
03/12/2022 23:13:08 - INFO - __main__ - Step 50 Global step 50 Train loss 7.995177 on epoch=24
03/12/2022 23:13:18 - INFO - __main__ - Global step 50 Train loss 14.394945 ACC 0.0 on epoch=24
03/12/2022 23:13:24 - INFO - __main__ - Step 60 Global step 60 Train loss 6.990483 on epoch=29
03/12/2022 23:13:29 - INFO - __main__ - Step 70 Global step 70 Train loss 7.073844 on epoch=34
03/12/2022 23:13:34 - INFO - __main__ - Step 80 Global step 80 Train loss 7.113516 on epoch=39
03/12/2022 23:13:39 - INFO - __main__ - Step 90 Global step 90 Train loss 5.924693 on epoch=44
03/12/2022 23:13:44 - INFO - __main__ - Step 100 Global step 100 Train loss 6.040525 on epoch=49
03/12/2022 23:13:45 - INFO - __main__ - Global step 100 Train loss 6.628613 ACC 0.0 on epoch=49
03/12/2022 23:13:50 - INFO - __main__ - Step 110 Global step 110 Train loss 5.669184 on epoch=54
03/12/2022 23:13:55 - INFO - __main__ - Step 120 Global step 120 Train loss 5.595884 on epoch=59
03/12/2022 23:14:00 - INFO - __main__ - Step 130 Global step 130 Train loss 4.896960 on epoch=64
03/12/2022 23:14:05 - INFO - __main__ - Step 140 Global step 140 Train loss 5.052491 on epoch=69
03/12/2022 23:14:10 - INFO - __main__ - Step 150 Global step 150 Train loss 5.018922 on epoch=74
03/12/2022 23:14:10 - INFO - __main__ - Global step 150 Train loss 5.246688 ACC 0.0625 on epoch=74
03/12/2022 23:14:16 - INFO - __main__ - Step 160 Global step 160 Train loss 4.761970 on epoch=79
03/12/2022 23:14:22 - INFO - __main__ - Step 170 Global step 170 Train loss 4.419102 on epoch=84
03/12/2022 23:14:27 - INFO - __main__ - Step 180 Global step 180 Train loss 4.795022 on epoch=89
03/12/2022 23:14:31 - INFO - __main__ - Step 190 Global step 190 Train loss 4.271342 on epoch=94
03/12/2022 23:14:37 - INFO - __main__ - Step 200 Global step 200 Train loss 4.036413 on epoch=99
03/12/2022 23:14:37 - INFO - __main__ - Global step 200 Train loss 4.456769 ACC 0.28125 on epoch=99
03/12/2022 23:14:43 - INFO - __main__ - Step 210 Global step 210 Train loss 4.241677 on epoch=104
03/12/2022 23:14:48 - INFO - __main__ - Step 220 Global step 220 Train loss 4.114524 on epoch=109
03/12/2022 23:14:53 - INFO - __main__ - Step 230 Global step 230 Train loss 4.006650 on epoch=114
03/12/2022 23:14:58 - INFO - __main__ - Step 240 Global step 240 Train loss 3.415734 on epoch=119
03/12/2022 23:15:03 - INFO - __main__ - Step 250 Global step 250 Train loss 3.364344 on epoch=124
03/12/2022 23:15:04 - INFO - __main__ - Global step 250 Train loss 3.828586 ACC 0.21875 on epoch=124
03/12/2022 23:15:09 - INFO - __main__ - Step 260 Global step 260 Train loss 3.459517 on epoch=129
03/12/2022 23:15:14 - INFO - __main__ - Step 270 Global step 270 Train loss 3.235468 on epoch=134
03/12/2022 23:15:19 - INFO - __main__ - Step 280 Global step 280 Train loss 3.116528 on epoch=139
03/12/2022 23:15:24 - INFO - __main__ - Step 290 Global step 290 Train loss 3.033173 on epoch=144
03/12/2022 23:15:29 - INFO - __main__ - Step 300 Global step 300 Train loss 2.957740 on epoch=149
03/12/2022 23:15:29 - INFO - __main__ - Global step 300 Train loss 3.160485 ACC 0.21875 on epoch=149
03/12/2022 23:15:34 - INFO - __main__ - Step 310 Global step 310 Train loss 2.655616 on epoch=154
03/12/2022 23:15:39 - INFO - __main__ - Step 320 Global step 320 Train loss 2.453618 on epoch=159
03/12/2022 23:15:44 - INFO - __main__ - Step 330 Global step 330 Train loss 2.060178 on epoch=164
03/12/2022 23:15:49 - INFO - __main__ - Step 340 Global step 340 Train loss 1.717580 on epoch=169
03/12/2022 23:15:54 - INFO - __main__ - Step 350 Global step 350 Train loss 1.823615 on epoch=174
03/12/2022 23:15:55 - INFO - __main__ - Global step 350 Train loss 2.142121 ACC 0.1875 on epoch=174
03/12/2022 23:16:00 - INFO - __main__ - Step 360 Global step 360 Train loss 1.307062 on epoch=179
03/12/2022 23:16:05 - INFO - __main__ - Step 370 Global step 370 Train loss 1.642122 on epoch=184
03/12/2022 23:16:10 - INFO - __main__ - Step 380 Global step 380 Train loss 1.463711 on epoch=189
03/12/2022 23:16:15 - INFO - __main__ - Step 390 Global step 390 Train loss 1.821189 on epoch=194
03/12/2022 23:16:20 - INFO - __main__ - Step 400 Global step 400 Train loss 1.082563 on epoch=199
03/12/2022 23:16:21 - INFO - __main__ - Global step 400 Train loss 1.463330 ACC 0.53125 on epoch=199
03/12/2022 23:16:27 - INFO - __main__ - Step 410 Global step 410 Train loss 1.146312 on epoch=204
03/12/2022 23:16:32 - INFO - __main__ - Step 420 Global step 420 Train loss 1.417862 on epoch=209
03/12/2022 23:16:37 - INFO - __main__ - Step 430 Global step 430 Train loss 1.144892 on epoch=214
03/12/2022 23:16:42 - INFO - __main__ - Step 440 Global step 440 Train loss 1.211997 on epoch=219
03/12/2022 23:16:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.936137 on epoch=224
03/12/2022 23:16:47 - INFO - __main__ - Global step 450 Train loss 1.171440 ACC 0.46875 on epoch=224
03/12/2022 23:16:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.990757 on epoch=229
03/12/2022 23:16:57 - INFO - __main__ - Step 470 Global step 470 Train loss 1.167578 on epoch=234
03/12/2022 23:17:03 - INFO - __main__ - Step 480 Global step 480 Train loss 1.281811 on epoch=239
03/12/2022 23:17:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.839375 on epoch=244
03/12/2022 23:17:13 - INFO - __main__ - Step 500 Global step 500 Train loss 1.345315 on epoch=249
03/12/2022 23:17:13 - INFO - __main__ - Global step 500 Train loss 1.124967 ACC 0.5 on epoch=249
03/12/2022 23:17:18 - INFO - __main__ - Step 510 Global step 510 Train loss 1.023044 on epoch=254
03/12/2022 23:17:23 - INFO - __main__ - Step 520 Global step 520 Train loss 1.137193 on epoch=259
03/12/2022 23:17:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.994891 on epoch=264
03/12/2022 23:17:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.702819 on epoch=269
03/12/2022 23:17:38 - INFO - __main__ - Step 550 Global step 550 Train loss 1.093724 on epoch=274
03/12/2022 23:17:39 - INFO - __main__ - Global step 550 Train loss 0.990334 ACC 0.5 on epoch=274
03/12/2022 23:17:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.861617 on epoch=279
03/12/2022 23:17:49 - INFO - __main__ - Step 570 Global step 570 Train loss 1.002839 on epoch=284
03/12/2022 23:17:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.921393 on epoch=289
03/12/2022 23:17:59 - INFO - __main__ - Step 590 Global step 590 Train loss 1.168765 on epoch=294
03/12/2022 23:18:04 - INFO - __main__ - Step 600 Global step 600 Train loss 1.253771 on epoch=299
03/12/2022 23:18:05 - INFO - __main__ - Global step 600 Train loss 1.041677 ACC 0.5 on epoch=299
03/12/2022 23:18:05 - INFO - __main__ - save last model!
03/12/2022 23:18:12 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 23:18:12 - INFO - __main__ - Start tokenizing ... 5463 instances
03/12/2022 23:18:12 - INFO - __main__ - Printing 3 examples
03/12/2022 23:18:12 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/12/2022 23:18:12 - INFO - __main__ - ['entailment']
03/12/2022 23:18:12 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/12/2022 23:18:12 - INFO - __main__ - ['not_entailment']
03/12/2022 23:18:12 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/12/2022 23:18:12 - INFO - __main__ - ['not_entailment']
03/12/2022 23:18:12 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:18:15 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:18:20 - INFO - __main__ - Loaded 5463 examples from test data
03/12/2022 23:20:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-glue-qnli/glue-qnli_16_87_0.0001_8_predictions.txt
03/12/2022 23:20:11 - INFO - __main__ - ACC on test data: 0.4856
03/12/2022 23:20:12 - INFO - __main__ - prefix=glue-qnli_16_87, lr=0.0001, bsz=8, dev_performance=0.53125, test_performance=0.4856306058941973
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (16662): No such process
Task: freebase_qa, Checkpoint: None, Identifier: T5-large-ft-random
Output directory () already exists and is not empty.
03/12/2022 23:20:17 - INFO - __main__ - Namespace(task_dir='data/freebase_qa/', task_name='freebase_qa', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-freebase_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/12/2022 23:20:17 - INFO - __main__ - models/T5-large-ft-random/singletask-freebase_qa
03/12/2022 23:20:17 - INFO - __main__ - Namespace(task_dir='data/freebase_qa/', task_name='freebase_qa', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-freebase_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/12/2022 23:20:17 - INFO - __main__ - models/T5-large-ft-random/singletask-freebase_qa
03/12/2022 23:20:19 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/12/2022 23:20:19 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/12/2022 23:20:19 - INFO - __main__ - args.device: cuda:0
03/12/2022 23:20:19 - INFO - __main__ - args.device: cuda:1
03/12/2022 23:20:19 - INFO - __main__ - Using 2 gpus
03/12/2022 23:20:19 - INFO - __main__ - Using 2 gpus
03/12/2022 23:20:19 - INFO - __main__ - Fine-tuning the following samples: ['freebase_qa_32_100', 'freebase_qa_32_13', 'freebase_qa_32_21', 'freebase_qa_32_42', 'freebase_qa_32_87']
03/12/2022 23:20:19 - INFO - __main__ - Fine-tuning the following samples: ['freebase_qa_32_100', 'freebase_qa_32_13', 'freebase_qa_32_21', 'freebase_qa_32_42', 'freebase_qa_32_87']
03/12/2022 23:20:26 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.0005, bsz=8 ...
03/12/2022 23:20:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:20:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:20:27 - INFO - __main__ - Printing 3 examples
03/12/2022 23:20:27 - INFO - __main__ - Printing 3 examples
03/12/2022 23:20:27 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/12/2022 23:20:27 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/12/2022 23:20:27 - INFO - __main__ - ['orange is the new black']
03/12/2022 23:20:27 - INFO - __main__ - ['orange is the new black']
03/12/2022 23:20:27 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/12/2022 23:20:27 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/12/2022 23:20:27 - INFO - __main__ - ['western australia']
03/12/2022 23:20:27 - INFO - __main__ - ['western australia']
03/12/2022 23:20:27 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/12/2022 23:20:27 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/12/2022 23:20:27 - INFO - __main__ - ['turkey']
03/12/2022 23:20:27 - INFO - __main__ - ['turkey']
03/12/2022 23:20:27 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:20:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 23:20:27 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:20:27 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:20:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:20:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:20:27 - INFO - __main__ - Printing 3 examples
03/12/2022 23:20:27 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/12/2022 23:20:27 - INFO - __main__ - ['benito mussolini']
03/12/2022 23:20:27 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/12/2022 23:20:27 - INFO - __main__ - ['the kinks']
03/12/2022 23:20:27 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/12/2022 23:20:27 - INFO - __main__ - ['saigon']
03/12/2022 23:20:27 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:20:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:20:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:20:27 - INFO - __main__ - Printing 3 examples
03/12/2022 23:20:27 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/12/2022 23:20:27 - INFO - __main__ - ['benito mussolini']
03/12/2022 23:20:27 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/12/2022 23:20:27 - INFO - __main__ - ['the kinks']
03/12/2022 23:20:27 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/12/2022 23:20:27 - INFO - __main__ - ['saigon']
03/12/2022 23:20:27 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:20:27 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:20:27 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:20:27 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:20:27 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:20:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:20:40 - INFO - __main__ - Starting training!
03/12/2022 23:20:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:20:40 - INFO - __main__ - Starting training!
03/12/2022 23:20:45 - INFO - __main__ - Step 10 Global step 10 Train loss 19.723627 on epoch=4
03/12/2022 23:20:49 - INFO - __main__ - Step 20 Global step 20 Train loss 14.612703 on epoch=9
03/12/2022 23:20:54 - INFO - __main__ - Step 30 Global step 30 Train loss 11.401545 on epoch=14
03/12/2022 23:20:59 - INFO - __main__ - Step 40 Global step 40 Train loss 9.777768 on epoch=19
03/12/2022 23:21:04 - INFO - __main__ - Step 50 Global step 50 Train loss 7.670809 on epoch=24
03/12/2022 23:21:04 - INFO - __main__ - Global step 50 Train loss 12.637290 EM 0.0 on epoch=24
03/12/2022 23:21:11 - INFO - __main__ - Step 60 Global step 60 Train loss 6.701577 on epoch=29
03/12/2022 23:21:16 - INFO - __main__ - Step 70 Global step 70 Train loss 5.367303 on epoch=34
03/12/2022 23:21:21 - INFO - __main__ - Step 80 Global step 80 Train loss 4.719977 on epoch=39
03/12/2022 23:21:26 - INFO - __main__ - Step 90 Global step 90 Train loss 4.283251 on epoch=44
03/12/2022 23:21:31 - INFO - __main__ - Step 100 Global step 100 Train loss 3.527660 on epoch=49
03/12/2022 23:21:32 - INFO - __main__ - Global step 100 Train loss 4.919954 EM 0.0 on epoch=49
03/12/2022 23:21:37 - INFO - __main__ - Step 110 Global step 110 Train loss 2.718527 on epoch=54
03/12/2022 23:21:42 - INFO - __main__ - Step 120 Global step 120 Train loss 2.765018 on epoch=59
03/12/2022 23:21:47 - INFO - __main__ - Step 130 Global step 130 Train loss 2.005637 on epoch=64
03/12/2022 23:21:52 - INFO - __main__ - Step 140 Global step 140 Train loss 1.941231 on epoch=69
03/12/2022 23:21:57 - INFO - __main__ - Step 150 Global step 150 Train loss 1.872529 on epoch=74
03/12/2022 23:21:58 - INFO - __main__ - Global step 150 Train loss 2.260589 EM 0.0 on epoch=74
03/12/2022 23:22:03 - INFO - __main__ - Step 160 Global step 160 Train loss 1.840739 on epoch=79
03/12/2022 23:22:08 - INFO - __main__ - Step 170 Global step 170 Train loss 1.360405 on epoch=84
03/12/2022 23:22:13 - INFO - __main__ - Step 180 Global step 180 Train loss 1.602959 on epoch=89
03/12/2022 23:22:19 - INFO - __main__ - Step 190 Global step 190 Train loss 1.540124 on epoch=94
03/12/2022 23:22:24 - INFO - __main__ - Step 200 Global step 200 Train loss 1.498271 on epoch=99
03/12/2022 23:22:24 - INFO - __main__ - Global step 200 Train loss 1.568500 EM 0.0 on epoch=99
03/12/2022 23:22:29 - INFO - __main__ - Step 210 Global step 210 Train loss 1.357319 on epoch=104
03/12/2022 23:22:34 - INFO - __main__ - Step 220 Global step 220 Train loss 1.370945 on epoch=109
03/12/2022 23:22:39 - INFO - __main__ - Step 230 Global step 230 Train loss 1.511523 on epoch=114
03/12/2022 23:22:44 - INFO - __main__ - Step 240 Global step 240 Train loss 1.162552 on epoch=119
03/12/2022 23:22:50 - INFO - __main__ - Step 250 Global step 250 Train loss 1.298892 on epoch=124
03/12/2022 23:22:50 - INFO - __main__ - Global step 250 Train loss 1.340246 EM 0.0 on epoch=124
03/12/2022 23:22:55 - INFO - __main__ - Step 260 Global step 260 Train loss 1.205673 on epoch=129
03/12/2022 23:23:00 - INFO - __main__ - Step 270 Global step 270 Train loss 1.103125 on epoch=134
03/12/2022 23:23:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.992836 on epoch=139
03/12/2022 23:23:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.928005 on epoch=144
03/12/2022 23:23:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.987711 on epoch=149
03/12/2022 23:23:17 - INFO - __main__ - Global step 300 Train loss 1.043470 EM 0.0 on epoch=149
03/12/2022 23:23:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.925884 on epoch=154
03/12/2022 23:23:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.787184 on epoch=159
03/12/2022 23:23:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.973426 on epoch=164
03/12/2022 23:23:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.954572 on epoch=169
03/12/2022 23:23:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.813104 on epoch=174
03/12/2022 23:23:43 - INFO - __main__ - Global step 350 Train loss 0.890834 EM 0.0 on epoch=174
03/12/2022 23:23:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.815179 on epoch=179
03/12/2022 23:23:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.800186 on epoch=184
03/12/2022 23:23:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.835337 on epoch=189
03/12/2022 23:24:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.782229 on epoch=194
03/12/2022 23:24:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.752004 on epoch=199
03/12/2022 23:24:09 - INFO - __main__ - Global step 400 Train loss 0.796987 EM 0.0 on epoch=199
03/12/2022 23:24:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.740615 on epoch=204
03/12/2022 23:24:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.716068 on epoch=209
03/12/2022 23:24:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.754824 on epoch=214
03/12/2022 23:24:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.627884 on epoch=219
03/12/2022 23:24:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.659657 on epoch=224
03/12/2022 23:24:35 - INFO - __main__ - Global step 450 Train loss 0.699810 EM 0.0 on epoch=224
03/12/2022 23:24:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.726472 on epoch=229
03/12/2022 23:24:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.666728 on epoch=234
03/12/2022 23:24:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.598745 on epoch=239
03/12/2022 23:24:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.721153 on epoch=244
03/12/2022 23:25:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.604231 on epoch=249
03/12/2022 23:25:00 - INFO - __main__ - Global step 500 Train loss 0.663466 EM 0.0 on epoch=249
03/12/2022 23:25:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.697153 on epoch=254
03/12/2022 23:25:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.621995 on epoch=259
03/12/2022 23:25:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.610826 on epoch=264
03/12/2022 23:25:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.715330 on epoch=269
03/12/2022 23:25:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.611735 on epoch=274
03/12/2022 23:25:26 - INFO - __main__ - Global step 550 Train loss 0.651408 EM 0.0 on epoch=274
03/12/2022 23:25:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.676281 on epoch=279
03/12/2022 23:25:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.609112 on epoch=284
03/12/2022 23:25:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.654443 on epoch=289
03/12/2022 23:25:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.568506 on epoch=294
03/12/2022 23:25:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.564150 on epoch=299
03/12/2022 23:25:52 - INFO - __main__ - Global step 600 Train loss 0.614498 EM 0.0 on epoch=299
03/12/2022 23:25:52 - INFO - __main__ - save last model!
03/12/2022 23:25:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:25:53 - INFO - __main__ - Printing 3 examples
03/12/2022 23:25:53 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/12/2022 23:25:53 - INFO - __main__ - ['orange is the new black']
03/12/2022 23:25:53 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/12/2022 23:25:53 - INFO - __main__ - ['western australia']
03/12/2022 23:25:53 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/12/2022 23:25:53 - INFO - __main__ - ['turkey']
03/12/2022 23:25:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 23:25:53 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:25:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:25:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:25:53 - INFO - __main__ - Printing 3 examples
03/12/2022 23:25:53 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/12/2022 23:25:53 - INFO - __main__ - ['benito mussolini']
03/12/2022 23:25:53 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/12/2022 23:25:53 - INFO - __main__ - ['the kinks']
03/12/2022 23:25:53 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/12/2022 23:25:53 - INFO - __main__ - ['saigon']
03/12/2022 23:25:53 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:25:53 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:25:53 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:26:02 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 23:26:03 - INFO - __main__ - Start tokenizing ... 3994 instances
03/12/2022 23:26:03 - INFO - __main__ - Printing 3 examples
03/12/2022 23:26:03 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/12/2022 23:26:03 - INFO - __main__ - ['taming of the shrew']
03/12/2022 23:26:03 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/12/2022 23:26:03 - INFO - __main__ - ['henry fonda']
03/12/2022 23:26:03 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/12/2022 23:26:03 - INFO - __main__ - ['tchaikovsky']
03/12/2022 23:26:03 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:26:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:26:04 - INFO - __main__ - Starting training!
03/12/2022 23:26:04 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:26:08 - INFO - __main__ - Loaded 3994 examples from test data
03/12/2022 23:28:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_100_0.0005_8_predictions.txt
03/12/2022 23:28:38 - INFO - __main__ - EM on test data: 0.0000
03/12/2022 23:28:38 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.0
03/12/2022 23:28:38 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.0003, bsz=8 ...
03/12/2022 23:28:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:28:39 - INFO - __main__ - Printing 3 examples
03/12/2022 23:28:39 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/12/2022 23:28:39 - INFO - __main__ - ['orange is the new black']
03/12/2022 23:28:39 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/12/2022 23:28:39 - INFO - __main__ - ['western australia']
03/12/2022 23:28:39 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/12/2022 23:28:39 - INFO - __main__ - ['turkey']
03/12/2022 23:28:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 23:28:39 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:28:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:28:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:28:39 - INFO - __main__ - Printing 3 examples
03/12/2022 23:28:39 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/12/2022 23:28:39 - INFO - __main__ - ['benito mussolini']
03/12/2022 23:28:39 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/12/2022 23:28:39 - INFO - __main__ - ['the kinks']
03/12/2022 23:28:39 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/12/2022 23:28:39 - INFO - __main__ - ['saigon']
03/12/2022 23:28:39 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:28:39 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:28:39 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:28:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:28:50 - INFO - __main__ - Starting training!
03/12/2022 23:28:55 - INFO - __main__ - Step 10 Global step 10 Train loss 19.057987 on epoch=4
03/12/2022 23:28:59 - INFO - __main__ - Step 20 Global step 20 Train loss 17.135296 on epoch=9
03/12/2022 23:29:04 - INFO - __main__ - Step 30 Global step 30 Train loss 15.033322 on epoch=14
03/12/2022 23:29:09 - INFO - __main__ - Step 40 Global step 40 Train loss 12.658286 on epoch=19
03/12/2022 23:29:14 - INFO - __main__ - Step 50 Global step 50 Train loss 11.214534 on epoch=24
03/12/2022 23:29:22 - INFO - __main__ - Global step 50 Train loss 15.019886 EM 0.0 on epoch=24
03/12/2022 23:29:28 - INFO - __main__ - Step 60 Global step 60 Train loss 9.192537 on epoch=29
03/12/2022 23:29:33 - INFO - __main__ - Step 70 Global step 70 Train loss 8.365095 on epoch=34
03/12/2022 23:29:37 - INFO - __main__ - Step 80 Global step 80 Train loss 8.108328 on epoch=39
03/12/2022 23:29:42 - INFO - __main__ - Step 90 Global step 90 Train loss 7.151807 on epoch=44
03/12/2022 23:29:47 - INFO - __main__ - Step 100 Global step 100 Train loss 6.479268 on epoch=49
03/12/2022 23:29:48 - INFO - __main__ - Global step 100 Train loss 7.859407 EM 0.0 on epoch=49
03/12/2022 23:29:53 - INFO - __main__ - Step 110 Global step 110 Train loss 5.654972 on epoch=54
03/12/2022 23:29:58 - INFO - __main__ - Step 120 Global step 120 Train loss 4.685846 on epoch=59
03/12/2022 23:30:03 - INFO - __main__ - Step 130 Global step 130 Train loss 4.382420 on epoch=64
03/12/2022 23:30:07 - INFO - __main__ - Step 140 Global step 140 Train loss 3.395922 on epoch=69
03/12/2022 23:30:12 - INFO - __main__ - Step 150 Global step 150 Train loss 3.800842 on epoch=74
03/12/2022 23:30:13 - INFO - __main__ - Global step 150 Train loss 4.384000 EM 0.0 on epoch=74
03/12/2022 23:30:18 - INFO - __main__ - Step 160 Global step 160 Train loss 3.082195 on epoch=79
03/12/2022 23:30:23 - INFO - __main__ - Step 170 Global step 170 Train loss 2.809532 on epoch=84
03/12/2022 23:30:28 - INFO - __main__ - Step 180 Global step 180 Train loss 2.784764 on epoch=89
03/12/2022 23:30:33 - INFO - __main__ - Step 190 Global step 190 Train loss 2.319645 on epoch=94
03/12/2022 23:30:38 - INFO - __main__ - Step 200 Global step 200 Train loss 2.091264 on epoch=99
03/12/2022 23:30:38 - INFO - __main__ - Global step 200 Train loss 2.617480 EM 0.0 on epoch=99
03/12/2022 23:30:43 - INFO - __main__ - Step 210 Global step 210 Train loss 2.189455 on epoch=104
03/12/2022 23:30:48 - INFO - __main__ - Step 220 Global step 220 Train loss 1.725870 on epoch=109
03/12/2022 23:30:53 - INFO - __main__ - Step 230 Global step 230 Train loss 1.568488 on epoch=114
03/12/2022 23:30:58 - INFO - __main__ - Step 240 Global step 240 Train loss 1.779311 on epoch=119
03/12/2022 23:31:03 - INFO - __main__ - Step 250 Global step 250 Train loss 1.479571 on epoch=124
03/12/2022 23:31:04 - INFO - __main__ - Global step 250 Train loss 1.748539 EM 0.0 on epoch=124
03/12/2022 23:31:09 - INFO - __main__ - Step 260 Global step 260 Train loss 1.376638 on epoch=129
03/12/2022 23:31:14 - INFO - __main__ - Step 270 Global step 270 Train loss 1.599639 on epoch=134
03/12/2022 23:31:19 - INFO - __main__ - Step 280 Global step 280 Train loss 1.317330 on epoch=139
03/12/2022 23:31:23 - INFO - __main__ - Step 290 Global step 290 Train loss 1.409771 on epoch=144
03/12/2022 23:31:28 - INFO - __main__ - Step 300 Global step 300 Train loss 1.159105 on epoch=149
03/12/2022 23:31:29 - INFO - __main__ - Global step 300 Train loss 1.372496 EM 0.0 on epoch=149
03/12/2022 23:31:34 - INFO - __main__ - Step 310 Global step 310 Train loss 1.253648 on epoch=154
03/12/2022 23:31:39 - INFO - __main__ - Step 320 Global step 320 Train loss 1.406760 on epoch=159
03/12/2022 23:31:44 - INFO - __main__ - Step 330 Global step 330 Train loss 1.365409 on epoch=164
03/12/2022 23:31:49 - INFO - __main__ - Step 340 Global step 340 Train loss 1.130057 on epoch=169
03/12/2022 23:31:54 - INFO - __main__ - Step 350 Global step 350 Train loss 1.275602 on epoch=174
03/12/2022 23:31:55 - INFO - __main__ - Global step 350 Train loss 1.286295 EM 0.0 on epoch=174
03/12/2022 23:32:00 - INFO - __main__ - Step 360 Global step 360 Train loss 1.279641 on epoch=179
03/12/2022 23:32:04 - INFO - __main__ - Step 370 Global step 370 Train loss 1.094018 on epoch=184
03/12/2022 23:32:09 - INFO - __main__ - Step 380 Global step 380 Train loss 1.289376 on epoch=189
03/12/2022 23:32:14 - INFO - __main__ - Step 390 Global step 390 Train loss 1.155033 on epoch=194
03/12/2022 23:32:19 - INFO - __main__ - Step 400 Global step 400 Train loss 1.186058 on epoch=199
03/12/2022 23:32:20 - INFO - __main__ - Global step 400 Train loss 1.200825 EM 0.0 on epoch=199
03/12/2022 23:32:25 - INFO - __main__ - Step 410 Global step 410 Train loss 1.295952 on epoch=204
03/12/2022 23:32:30 - INFO - __main__ - Step 420 Global step 420 Train loss 1.134191 on epoch=209
03/12/2022 23:32:35 - INFO - __main__ - Step 430 Global step 430 Train loss 1.014952 on epoch=214
03/12/2022 23:32:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.977614 on epoch=219
03/12/2022 23:32:45 - INFO - __main__ - Step 450 Global step 450 Train loss 1.005547 on epoch=224
03/12/2022 23:32:45 - INFO - __main__ - Global step 450 Train loss 1.085651 EM 0.0 on epoch=224
03/12/2022 23:32:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.957506 on epoch=229
03/12/2022 23:32:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.882511 on epoch=234
03/12/2022 23:33:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.924369 on epoch=239
03/12/2022 23:33:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.894329 on epoch=244
03/12/2022 23:33:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.898812 on epoch=249
03/12/2022 23:33:11 - INFO - __main__ - Global step 500 Train loss 0.911506 EM 0.0 on epoch=249
03/12/2022 23:33:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.980264 on epoch=254
03/12/2022 23:33:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.810252 on epoch=259
03/12/2022 23:33:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.951061 on epoch=264
03/12/2022 23:33:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.839400 on epoch=269
03/12/2022 23:33:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.818471 on epoch=274
03/12/2022 23:33:36 - INFO - __main__ - Global step 550 Train loss 0.879889 EM 0.0 on epoch=274
03/12/2022 23:33:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.798105 on epoch=279
03/12/2022 23:33:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.823258 on epoch=284
03/12/2022 23:33:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.735900 on epoch=289
03/12/2022 23:33:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.890197 on epoch=294
03/12/2022 23:34:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.806604 on epoch=299
03/12/2022 23:34:01 - INFO - __main__ - Global step 600 Train loss 0.810813 EM 0.0 on epoch=299
03/12/2022 23:34:01 - INFO - __main__ - save last model!
03/12/2022 23:34:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:34:01 - INFO - __main__ - Printing 3 examples
03/12/2022 23:34:01 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/12/2022 23:34:01 - INFO - __main__ - ['orange is the new black']
03/12/2022 23:34:01 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/12/2022 23:34:01 - INFO - __main__ - ['western australia']
03/12/2022 23:34:01 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/12/2022 23:34:01 - INFO - __main__ - ['turkey']
03/12/2022 23:34:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 23:34:01 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:34:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:34:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:34:01 - INFO - __main__ - Printing 3 examples
03/12/2022 23:34:01 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/12/2022 23:34:01 - INFO - __main__ - ['benito mussolini']
03/12/2022 23:34:01 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/12/2022 23:34:01 - INFO - __main__ - ['the kinks']
03/12/2022 23:34:01 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/12/2022 23:34:01 - INFO - __main__ - ['saigon']
03/12/2022 23:34:01 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:34:01 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:34:01 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:34:08 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 23:34:09 - INFO - __main__ - Start tokenizing ... 3994 instances
03/12/2022 23:34:09 - INFO - __main__ - Printing 3 examples
03/12/2022 23:34:09 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/12/2022 23:34:09 - INFO - __main__ - ['taming of the shrew']
03/12/2022 23:34:09 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/12/2022 23:34:09 - INFO - __main__ - ['henry fonda']
03/12/2022 23:34:09 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/12/2022 23:34:09 - INFO - __main__ - ['tchaikovsky']
03/12/2022 23:34:09 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:34:10 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:34:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:34:12 - INFO - __main__ - Starting training!
03/12/2022 23:34:14 - INFO - __main__ - Loaded 3994 examples from test data
03/12/2022 23:36:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_100_0.0003_8_predictions.txt
03/12/2022 23:36:54 - INFO - __main__ - EM on test data: 0.0003
03/12/2022 23:36:55 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.0002503755633450175
03/12/2022 23:36:55 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.0002, bsz=8 ...
03/12/2022 23:36:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:36:56 - INFO - __main__ - Printing 3 examples
03/12/2022 23:36:56 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/12/2022 23:36:56 - INFO - __main__ - ['orange is the new black']
03/12/2022 23:36:56 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/12/2022 23:36:56 - INFO - __main__ - ['western australia']
03/12/2022 23:36:56 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/12/2022 23:36:56 - INFO - __main__ - ['turkey']
03/12/2022 23:36:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 23:36:56 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:36:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:36:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:36:56 - INFO - __main__ - Printing 3 examples
03/12/2022 23:36:56 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/12/2022 23:36:56 - INFO - __main__ - ['benito mussolini']
03/12/2022 23:36:56 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/12/2022 23:36:56 - INFO - __main__ - ['the kinks']
03/12/2022 23:36:56 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/12/2022 23:36:56 - INFO - __main__ - ['saigon']
03/12/2022 23:36:56 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:36:56 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:36:56 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:37:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:37:08 - INFO - __main__ - Starting training!
03/12/2022 23:37:13 - INFO - __main__ - Step 10 Global step 10 Train loss 20.295103 on epoch=4
03/12/2022 23:37:17 - INFO - __main__ - Step 20 Global step 20 Train loss 18.296803 on epoch=9
03/12/2022 23:37:22 - INFO - __main__ - Step 30 Global step 30 Train loss 15.100385 on epoch=14
03/12/2022 23:37:27 - INFO - __main__ - Step 40 Global step 40 Train loss 12.268061 on epoch=19
03/12/2022 23:37:32 - INFO - __main__ - Step 50 Global step 50 Train loss 10.817968 on epoch=24
03/12/2022 23:37:41 - INFO - __main__ - Global step 50 Train loss 15.355663 EM 0.0 on epoch=24
03/12/2022 23:37:46 - INFO - __main__ - Step 60 Global step 60 Train loss 9.568928 on epoch=29
03/12/2022 23:37:51 - INFO - __main__ - Step 70 Global step 70 Train loss 8.371771 on epoch=34
03/12/2022 23:37:56 - INFO - __main__ - Step 80 Global step 80 Train loss 7.842072 on epoch=39
03/12/2022 23:38:01 - INFO - __main__ - Step 90 Global step 90 Train loss 6.830823 on epoch=44
03/12/2022 23:38:07 - INFO - __main__ - Step 100 Global step 100 Train loss 6.230772 on epoch=49
03/12/2022 23:38:07 - INFO - __main__ - Global step 100 Train loss 7.768874 EM 0.0 on epoch=49
03/12/2022 23:38:12 - INFO - __main__ - Step 110 Global step 110 Train loss 5.639005 on epoch=54
03/12/2022 23:38:17 - INFO - __main__ - Step 120 Global step 120 Train loss 5.323174 on epoch=59
03/12/2022 23:38:22 - INFO - __main__ - Step 130 Global step 130 Train loss 5.043777 on epoch=64
03/12/2022 23:38:27 - INFO - __main__ - Step 140 Global step 140 Train loss 4.611176 on epoch=69
03/12/2022 23:38:33 - INFO - __main__ - Step 150 Global step 150 Train loss 4.539546 on epoch=74
03/12/2022 23:38:34 - INFO - __main__ - Global step 150 Train loss 5.031335 EM 0.0 on epoch=74
03/12/2022 23:38:38 - INFO - __main__ - Step 160 Global step 160 Train loss 4.203378 on epoch=79
03/12/2022 23:38:44 - INFO - __main__ - Step 170 Global step 170 Train loss 3.819079 on epoch=84
03/12/2022 23:38:49 - INFO - __main__ - Step 180 Global step 180 Train loss 3.747610 on epoch=89
03/12/2022 23:38:54 - INFO - __main__ - Step 190 Global step 190 Train loss 3.417730 on epoch=94
03/12/2022 23:38:59 - INFO - __main__ - Step 200 Global step 200 Train loss 3.254574 on epoch=99
03/12/2022 23:39:00 - INFO - __main__ - Global step 200 Train loss 3.688474 EM 0.0 on epoch=99
03/12/2022 23:39:05 - INFO - __main__ - Step 210 Global step 210 Train loss 2.754676 on epoch=104
03/12/2022 23:39:10 - INFO - __main__ - Step 220 Global step 220 Train loss 2.733306 on epoch=109
03/12/2022 23:39:15 - INFO - __main__ - Step 230 Global step 230 Train loss 2.824021 on epoch=114
03/12/2022 23:39:20 - INFO - __main__ - Step 240 Global step 240 Train loss 2.496723 on epoch=119
03/12/2022 23:39:25 - INFO - __main__ - Step 250 Global step 250 Train loss 2.329699 on epoch=124
03/12/2022 23:39:26 - INFO - __main__ - Global step 250 Train loss 2.627685 EM 0.0 on epoch=124
03/12/2022 23:39:31 - INFO - __main__ - Step 260 Global step 260 Train loss 2.025935 on epoch=129
03/12/2022 23:39:36 - INFO - __main__ - Step 270 Global step 270 Train loss 1.967702 on epoch=134
03/12/2022 23:39:41 - INFO - __main__ - Step 280 Global step 280 Train loss 1.830106 on epoch=139
03/12/2022 23:39:46 - INFO - __main__ - Step 290 Global step 290 Train loss 1.850744 on epoch=144
03/12/2022 23:39:51 - INFO - __main__ - Step 300 Global step 300 Train loss 1.739413 on epoch=149
03/12/2022 23:39:52 - INFO - __main__ - Global step 300 Train loss 1.882780 EM 0.0 on epoch=149
03/12/2022 23:39:57 - INFO - __main__ - Step 310 Global step 310 Train loss 1.550287 on epoch=154
03/12/2022 23:40:02 - INFO - __main__ - Step 320 Global step 320 Train loss 1.648670 on epoch=159
03/12/2022 23:40:07 - INFO - __main__ - Step 330 Global step 330 Train loss 1.829911 on epoch=164
03/12/2022 23:40:12 - INFO - __main__ - Step 340 Global step 340 Train loss 1.735119 on epoch=169
03/12/2022 23:40:17 - INFO - __main__ - Step 350 Global step 350 Train loss 1.613115 on epoch=174
03/12/2022 23:40:18 - INFO - __main__ - Global step 350 Train loss 1.675420 EM 0.0 on epoch=174
03/12/2022 23:40:23 - INFO - __main__ - Step 360 Global step 360 Train loss 1.549056 on epoch=179
03/12/2022 23:40:28 - INFO - __main__ - Step 370 Global step 370 Train loss 1.812550 on epoch=184
03/12/2022 23:40:33 - INFO - __main__ - Step 380 Global step 380 Train loss 1.564440 on epoch=189
03/12/2022 23:40:38 - INFO - __main__ - Step 390 Global step 390 Train loss 1.596517 on epoch=194
03/12/2022 23:40:43 - INFO - __main__ - Step 400 Global step 400 Train loss 1.560383 on epoch=199
03/12/2022 23:40:44 - INFO - __main__ - Global step 400 Train loss 1.616589 EM 0.0 on epoch=199
03/12/2022 23:40:49 - INFO - __main__ - Step 410 Global step 410 Train loss 1.339828 on epoch=204
03/12/2022 23:40:54 - INFO - __main__ - Step 420 Global step 420 Train loss 1.446169 on epoch=209
03/12/2022 23:40:59 - INFO - __main__ - Step 430 Global step 430 Train loss 1.768797 on epoch=214
03/12/2022 23:41:04 - INFO - __main__ - Step 440 Global step 440 Train loss 1.404166 on epoch=219
03/12/2022 23:41:09 - INFO - __main__ - Step 450 Global step 450 Train loss 1.201140 on epoch=224
03/12/2022 23:41:10 - INFO - __main__ - Global step 450 Train loss 1.432020 EM 0.0 on epoch=224
03/12/2022 23:41:15 - INFO - __main__ - Step 460 Global step 460 Train loss 1.473465 on epoch=229
03/12/2022 23:41:20 - INFO - __main__ - Step 470 Global step 470 Train loss 1.384322 on epoch=234
03/12/2022 23:41:25 - INFO - __main__ - Step 480 Global step 480 Train loss 1.211902 on epoch=239
03/12/2022 23:41:30 - INFO - __main__ - Step 490 Global step 490 Train loss 1.275052 on epoch=244
03/12/2022 23:41:35 - INFO - __main__ - Step 500 Global step 500 Train loss 1.375960 on epoch=249
03/12/2022 23:41:36 - INFO - __main__ - Global step 500 Train loss 1.344141 EM 0.0 on epoch=249
03/12/2022 23:41:41 - INFO - __main__ - Step 510 Global step 510 Train loss 1.156485 on epoch=254
03/12/2022 23:41:46 - INFO - __main__ - Step 520 Global step 520 Train loss 1.214164 on epoch=259
03/12/2022 23:41:51 - INFO - __main__ - Step 530 Global step 530 Train loss 1.284337 on epoch=264
03/12/2022 23:41:56 - INFO - __main__ - Step 540 Global step 540 Train loss 1.171232 on epoch=269
03/12/2022 23:42:01 - INFO - __main__ - Step 550 Global step 550 Train loss 1.065514 on epoch=274
03/12/2022 23:42:02 - INFO - __main__ - Global step 550 Train loss 1.178346 EM 0.0 on epoch=274
03/12/2022 23:42:07 - INFO - __main__ - Step 560 Global step 560 Train loss 1.493858 on epoch=279
03/12/2022 23:42:12 - INFO - __main__ - Step 570 Global step 570 Train loss 1.128546 on epoch=284
03/12/2022 23:42:17 - INFO - __main__ - Step 580 Global step 580 Train loss 1.064618 on epoch=289
03/12/2022 23:42:22 - INFO - __main__ - Step 590 Global step 590 Train loss 1.021983 on epoch=294
03/12/2022 23:42:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.971940 on epoch=299
03/12/2022 23:42:28 - INFO - __main__ - Global step 600 Train loss 1.136189 EM 0.0 on epoch=299
03/12/2022 23:42:28 - INFO - __main__ - save last model!
03/12/2022 23:42:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:42:29 - INFO - __main__ - Printing 3 examples
03/12/2022 23:42:29 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/12/2022 23:42:29 - INFO - __main__ - ['orange is the new black']
03/12/2022 23:42:29 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/12/2022 23:42:29 - INFO - __main__ - ['western australia']
03/12/2022 23:42:29 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/12/2022 23:42:29 - INFO - __main__ - ['turkey']
03/12/2022 23:42:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 23:42:29 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:42:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:42:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:42:29 - INFO - __main__ - Printing 3 examples
03/12/2022 23:42:29 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/12/2022 23:42:29 - INFO - __main__ - ['benito mussolini']
03/12/2022 23:42:29 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/12/2022 23:42:29 - INFO - __main__ - ['the kinks']
03/12/2022 23:42:29 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/12/2022 23:42:29 - INFO - __main__ - ['saigon']
03/12/2022 23:42:29 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:42:29 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:42:29 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:42:35 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 23:42:35 - INFO - __main__ - Start tokenizing ... 3994 instances
03/12/2022 23:42:35 - INFO - __main__ - Printing 3 examples
03/12/2022 23:42:35 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/12/2022 23:42:35 - INFO - __main__ - ['taming of the shrew']
03/12/2022 23:42:35 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/12/2022 23:42:35 - INFO - __main__ - ['henry fonda']
03/12/2022 23:42:35 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/12/2022 23:42:35 - INFO - __main__ - ['tchaikovsky']
03/12/2022 23:42:35 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:42:37 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:42:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:42:39 - INFO - __main__ - Starting training!
03/12/2022 23:42:41 - INFO - __main__ - Loaded 3994 examples from test data
03/12/2022 23:48:36 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_100_0.0002_8_predictions.txt
03/12/2022 23:48:36 - INFO - __main__ - EM on test data: 0.0003
03/12/2022 23:48:37 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.0002503755633450175
03/12/2022 23:48:37 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.0001, bsz=8 ...
03/12/2022 23:48:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:48:38 - INFO - __main__ - Printing 3 examples
03/12/2022 23:48:38 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/12/2022 23:48:38 - INFO - __main__ - ['orange is the new black']
03/12/2022 23:48:38 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/12/2022 23:48:38 - INFO - __main__ - ['western australia']
03/12/2022 23:48:38 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/12/2022 23:48:38 - INFO - __main__ - ['turkey']
03/12/2022 23:48:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/12/2022 23:48:38 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:48:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:48:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:48:38 - INFO - __main__ - Printing 3 examples
03/12/2022 23:48:38 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/12/2022 23:48:38 - INFO - __main__ - ['benito mussolini']
03/12/2022 23:48:38 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/12/2022 23:48:38 - INFO - __main__ - ['the kinks']
03/12/2022 23:48:38 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/12/2022 23:48:38 - INFO - __main__ - ['saigon']
03/12/2022 23:48:38 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:48:38 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:48:38 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:48:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:48:51 - INFO - __main__ - Starting training!
03/12/2022 23:48:56 - INFO - __main__ - Step 10 Global step 10 Train loss 19.027750 on epoch=4
03/12/2022 23:49:01 - INFO - __main__ - Step 20 Global step 20 Train loss 19.357555 on epoch=9
03/12/2022 23:49:06 - INFO - __main__ - Step 30 Global step 30 Train loss 15.567294 on epoch=14
03/12/2022 23:49:10 - INFO - __main__ - Step 40 Global step 40 Train loss 13.975634 on epoch=19
03/12/2022 23:49:15 - INFO - __main__ - Step 50 Global step 50 Train loss 13.536389 on epoch=24
03/12/2022 23:49:26 - INFO - __main__ - Global step 50 Train loss 16.292923 EM 0.0 on epoch=24
03/12/2022 23:49:31 - INFO - __main__ - Step 60 Global step 60 Train loss 12.461739 on epoch=29
03/12/2022 23:49:36 - INFO - __main__ - Step 70 Global step 70 Train loss 11.788887 on epoch=34
03/12/2022 23:49:42 - INFO - __main__ - Step 80 Global step 80 Train loss 10.552436 on epoch=39
03/12/2022 23:49:47 - INFO - __main__ - Step 90 Global step 90 Train loss 10.054730 on epoch=44
03/12/2022 23:49:52 - INFO - __main__ - Step 100 Global step 100 Train loss 10.032635 on epoch=49
03/12/2022 23:49:53 - INFO - __main__ - Global step 100 Train loss 10.978086 EM 0.0 on epoch=49
03/12/2022 23:49:58 - INFO - __main__ - Step 110 Global step 110 Train loss 9.142342 on epoch=54
03/12/2022 23:50:03 - INFO - __main__ - Step 120 Global step 120 Train loss 8.581182 on epoch=59
03/12/2022 23:50:08 - INFO - __main__ - Step 130 Global step 130 Train loss 7.941063 on epoch=64
03/12/2022 23:50:13 - INFO - __main__ - Step 140 Global step 140 Train loss 7.538055 on epoch=69
03/12/2022 23:50:18 - INFO - __main__ - Step 150 Global step 150 Train loss 7.476145 on epoch=74
03/12/2022 23:50:19 - INFO - __main__ - Global step 150 Train loss 8.135756 EM 0.0 on epoch=74
03/12/2022 23:50:24 - INFO - __main__ - Step 160 Global step 160 Train loss 6.401768 on epoch=79
03/12/2022 23:50:29 - INFO - __main__ - Step 170 Global step 170 Train loss 6.389716 on epoch=84
03/12/2022 23:50:34 - INFO - __main__ - Step 180 Global step 180 Train loss 6.208212 on epoch=89
03/12/2022 23:50:39 - INFO - __main__ - Step 190 Global step 190 Train loss 6.151377 on epoch=94
03/12/2022 23:50:44 - INFO - __main__ - Step 200 Global step 200 Train loss 5.753003 on epoch=99
03/12/2022 23:50:45 - INFO - __main__ - Global step 200 Train loss 6.180815 EM 0.0 on epoch=99
03/12/2022 23:50:50 - INFO - __main__ - Step 210 Global step 210 Train loss 5.077855 on epoch=104
03/12/2022 23:50:55 - INFO - __main__ - Step 220 Global step 220 Train loss 5.498646 on epoch=109
03/12/2022 23:51:00 - INFO - __main__ - Step 230 Global step 230 Train loss 4.979362 on epoch=114
03/12/2022 23:51:05 - INFO - __main__ - Step 240 Global step 240 Train loss 4.640102 on epoch=119
03/12/2022 23:51:10 - INFO - __main__ - Step 250 Global step 250 Train loss 4.459491 on epoch=124
03/12/2022 23:51:11 - INFO - __main__ - Global step 250 Train loss 4.931091 EM 0.0 on epoch=124
03/12/2022 23:51:16 - INFO - __main__ - Step 260 Global step 260 Train loss 4.505036 on epoch=129
03/12/2022 23:51:21 - INFO - __main__ - Step 270 Global step 270 Train loss 4.292386 on epoch=134
03/12/2022 23:51:26 - INFO - __main__ - Step 280 Global step 280 Train loss 4.442796 on epoch=139
03/12/2022 23:51:31 - INFO - __main__ - Step 290 Global step 290 Train loss 4.186997 on epoch=144
03/12/2022 23:51:36 - INFO - __main__ - Step 300 Global step 300 Train loss 4.183673 on epoch=149
03/12/2022 23:51:37 - INFO - __main__ - Global step 300 Train loss 4.322178 EM 0.0 on epoch=149
03/12/2022 23:51:42 - INFO - __main__ - Step 310 Global step 310 Train loss 3.936415 on epoch=154
03/12/2022 23:51:47 - INFO - __main__ - Step 320 Global step 320 Train loss 4.113157 on epoch=159
03/12/2022 23:51:52 - INFO - __main__ - Step 330 Global step 330 Train loss 3.957535 on epoch=164
03/12/2022 23:51:57 - INFO - __main__ - Step 340 Global step 340 Train loss 3.653024 on epoch=169
03/12/2022 23:52:02 - INFO - __main__ - Step 350 Global step 350 Train loss 3.451979 on epoch=174
03/12/2022 23:52:03 - INFO - __main__ - Global step 350 Train loss 3.822422 EM 0.0 on epoch=174
03/12/2022 23:52:08 - INFO - __main__ - Step 360 Global step 360 Train loss 3.106309 on epoch=179
03/12/2022 23:52:13 - INFO - __main__ - Step 370 Global step 370 Train loss 3.376358 on epoch=184
03/12/2022 23:52:18 - INFO - __main__ - Step 380 Global step 380 Train loss 3.311705 on epoch=189
03/12/2022 23:52:23 - INFO - __main__ - Step 390 Global step 390 Train loss 2.893185 on epoch=194
03/12/2022 23:52:29 - INFO - __main__ - Step 400 Global step 400 Train loss 2.811042 on epoch=199
03/12/2022 23:52:29 - INFO - __main__ - Global step 400 Train loss 3.099720 EM 0.0 on epoch=199
03/12/2022 23:52:34 - INFO - __main__ - Step 410 Global step 410 Train loss 2.979361 on epoch=204
03/12/2022 23:52:40 - INFO - __main__ - Step 420 Global step 420 Train loss 2.767898 on epoch=209
03/12/2022 23:52:45 - INFO - __main__ - Step 430 Global step 430 Train loss 2.789943 on epoch=214
03/12/2022 23:52:50 - INFO - __main__ - Step 440 Global step 440 Train loss 2.649827 on epoch=219
03/12/2022 23:52:55 - INFO - __main__ - Step 450 Global step 450 Train loss 2.517746 on epoch=224
03/12/2022 23:52:56 - INFO - __main__ - Global step 450 Train loss 2.740955 EM 0.0 on epoch=224
03/12/2022 23:53:01 - INFO - __main__ - Step 460 Global step 460 Train loss 2.506381 on epoch=229
03/12/2022 23:53:06 - INFO - __main__ - Step 470 Global step 470 Train loss 2.089218 on epoch=234
03/12/2022 23:53:11 - INFO - __main__ - Step 480 Global step 480 Train loss 2.310827 on epoch=239
03/12/2022 23:53:16 - INFO - __main__ - Step 490 Global step 490 Train loss 2.207433 on epoch=244
03/12/2022 23:53:21 - INFO - __main__ - Step 500 Global step 500 Train loss 2.371124 on epoch=249
03/12/2022 23:53:22 - INFO - __main__ - Global step 500 Train loss 2.296996 EM 0.0 on epoch=249
03/12/2022 23:53:27 - INFO - __main__ - Step 510 Global step 510 Train loss 2.296086 on epoch=254
03/12/2022 23:53:32 - INFO - __main__ - Step 520 Global step 520 Train loss 2.072385 on epoch=259
03/12/2022 23:53:37 - INFO - __main__ - Step 530 Global step 530 Train loss 2.148956 on epoch=264
03/12/2022 23:53:42 - INFO - __main__ - Step 540 Global step 540 Train loss 1.984277 on epoch=269
03/12/2022 23:53:47 - INFO - __main__ - Step 550 Global step 550 Train loss 1.913383 on epoch=274
03/12/2022 23:53:48 - INFO - __main__ - Global step 550 Train loss 2.083017 EM 0.0 on epoch=274
03/12/2022 23:53:53 - INFO - __main__ - Step 560 Global step 560 Train loss 2.016610 on epoch=279
03/12/2022 23:53:58 - INFO - __main__ - Step 570 Global step 570 Train loss 1.757734 on epoch=284
03/12/2022 23:54:03 - INFO - __main__ - Step 580 Global step 580 Train loss 1.793972 on epoch=289
03/12/2022 23:54:08 - INFO - __main__ - Step 590 Global step 590 Train loss 1.464918 on epoch=294
03/12/2022 23:54:13 - INFO - __main__ - Step 600 Global step 600 Train loss 2.008346 on epoch=299
03/12/2022 23:54:14 - INFO - __main__ - Global step 600 Train loss 1.808316 EM 0.0 on epoch=299
03/12/2022 23:54:14 - INFO - __main__ - save last model!
03/12/2022 23:54:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:54:14 - INFO - __main__ - Printing 3 examples
03/12/2022 23:54:14 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/12/2022 23:54:14 - INFO - __main__ - ['kieren fallon']
03/12/2022 23:54:14 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/12/2022 23:54:14 - INFO - __main__ - ['uranus']
03/12/2022 23:54:14 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/12/2022 23:54:14 - INFO - __main__ - ['rob thomas']
03/12/2022 23:54:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/12/2022 23:54:14 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:54:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/12/2022 23:54:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/12/2022 23:54:14 - INFO - __main__ - Printing 3 examples
03/12/2022 23:54:14 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/12/2022 23:54:14 - INFO - __main__ - ['terence rattigan']
03/12/2022 23:54:14 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/12/2022 23:54:14 - INFO - __main__ - ['casino royale']
03/12/2022 23:54:14 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/12/2022 23:54:14 - INFO - __main__ - ['offenbach']
03/12/2022 23:54:14 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:54:14 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:54:14 - INFO - __main__ - Loaded 32 examples from dev data
03/12/2022 23:54:20 - INFO - __main__ - Loading checkpoint on the fly
03/12/2022 23:54:21 - INFO - __main__ - Start tokenizing ... 3994 instances
03/12/2022 23:54:21 - INFO - __main__ - Printing 3 examples
03/12/2022 23:54:21 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/12/2022 23:54:21 - INFO - __main__ - ['taming of the shrew']
03/12/2022 23:54:21 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/12/2022 23:54:21 - INFO - __main__ - ['henry fonda']
03/12/2022 23:54:21 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/12/2022 23:54:21 - INFO - __main__ - ['tchaikovsky']
03/12/2022 23:54:21 - INFO - __main__ - Tokenizing Input ...
03/12/2022 23:54:23 - INFO - __main__ - Tokenizing Output ...
03/12/2022 23:54:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/12/2022 23:54:25 - INFO - __main__ - Starting training!
03/12/2022 23:54:27 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 00:14:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_100_0.0001_8_predictions.txt
03/13/2022 00:14:33 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 00:14:34 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 00:14:34 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.0005, bsz=8 ...
03/13/2022 00:14:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:14:35 - INFO - __main__ - Printing 3 examples
03/13/2022 00:14:35 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/13/2022 00:14:35 - INFO - __main__ - ['kieren fallon']
03/13/2022 00:14:35 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/13/2022 00:14:35 - INFO - __main__ - ['uranus']
03/13/2022 00:14:35 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/13/2022 00:14:35 - INFO - __main__ - ['rob thomas']
03/13/2022 00:14:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 00:14:35 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:14:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 00:14:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:14:35 - INFO - __main__ - Printing 3 examples
03/13/2022 00:14:35 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/13/2022 00:14:35 - INFO - __main__ - ['terence rattigan']
03/13/2022 00:14:35 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/13/2022 00:14:35 - INFO - __main__ - ['casino royale']
03/13/2022 00:14:35 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/13/2022 00:14:35 - INFO - __main__ - ['offenbach']
03/13/2022 00:14:35 - INFO - __main__ - Tokenizing Input ...
03/13/2022 00:14:35 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:14:35 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 00:14:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 00:14:46 - INFO - __main__ - Starting training!
03/13/2022 00:14:50 - INFO - __main__ - Step 10 Global step 10 Train loss 20.250536 on epoch=4
03/13/2022 00:14:55 - INFO - __main__ - Step 20 Global step 20 Train loss 16.330112 on epoch=9
03/13/2022 00:15:00 - INFO - __main__ - Step 30 Global step 30 Train loss 10.751714 on epoch=14
03/13/2022 00:15:05 - INFO - __main__ - Step 40 Global step 40 Train loss 8.929488 on epoch=19
03/13/2022 00:15:10 - INFO - __main__ - Step 50 Global step 50 Train loss 7.866320 on epoch=24
03/13/2022 00:15:10 - INFO - __main__ - Global step 50 Train loss 12.825634 EM 0.0 on epoch=24
03/13/2022 00:15:16 - INFO - __main__ - Step 60 Global step 60 Train loss 6.324496 on epoch=29
03/13/2022 00:15:21 - INFO - __main__ - Step 70 Global step 70 Train loss 5.083985 on epoch=34
03/13/2022 00:15:26 - INFO - __main__ - Step 80 Global step 80 Train loss 3.899240 on epoch=39
03/13/2022 00:15:31 - INFO - __main__ - Step 90 Global step 90 Train loss 2.972076 on epoch=44
03/13/2022 00:15:36 - INFO - __main__ - Step 100 Global step 100 Train loss 2.044911 on epoch=49
03/13/2022 00:15:37 - INFO - __main__ - Global step 100 Train loss 4.064942 EM 0.0 on epoch=49
03/13/2022 00:15:41 - INFO - __main__ - Step 110 Global step 110 Train loss 2.054922 on epoch=54
03/13/2022 00:15:46 - INFO - __main__ - Step 120 Global step 120 Train loss 1.667290 on epoch=59
03/13/2022 00:15:51 - INFO - __main__ - Step 130 Global step 130 Train loss 1.814787 on epoch=64
03/13/2022 00:15:56 - INFO - __main__ - Step 140 Global step 140 Train loss 1.340275 on epoch=69
03/13/2022 00:16:01 - INFO - __main__ - Step 150 Global step 150 Train loss 1.421923 on epoch=74
03/13/2022 00:16:02 - INFO - __main__ - Global step 150 Train loss 1.659840 EM 0.0 on epoch=74
03/13/2022 00:16:07 - INFO - __main__ - Step 160 Global step 160 Train loss 1.303361 on epoch=79
03/13/2022 00:16:12 - INFO - __main__ - Step 170 Global step 170 Train loss 1.204662 on epoch=84
03/13/2022 00:16:17 - INFO - __main__ - Step 180 Global step 180 Train loss 1.198008 on epoch=89
03/13/2022 00:16:22 - INFO - __main__ - Step 190 Global step 190 Train loss 1.107295 on epoch=94
03/13/2022 00:16:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.935465 on epoch=99
03/13/2022 00:16:27 - INFO - __main__ - Global step 200 Train loss 1.149758 EM 0.0 on epoch=99
03/13/2022 00:16:32 - INFO - __main__ - Step 210 Global step 210 Train loss 1.148859 on epoch=104
03/13/2022 00:16:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.953072 on epoch=109
03/13/2022 00:16:42 - INFO - __main__ - Step 230 Global step 230 Train loss 1.032302 on epoch=114
03/13/2022 00:16:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.933401 on epoch=119
03/13/2022 00:16:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.881447 on epoch=124
03/13/2022 00:16:53 - INFO - __main__ - Global step 250 Train loss 0.989816 EM 0.0 on epoch=124
03/13/2022 00:16:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.746575 on epoch=129
03/13/2022 00:17:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.958431 on epoch=134
03/13/2022 00:17:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.812284 on epoch=139
03/13/2022 00:17:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.722762 on epoch=144
03/13/2022 00:17:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.756490 on epoch=149
03/13/2022 00:17:18 - INFO - __main__ - Global step 300 Train loss 0.799309 EM 0.0 on epoch=149
03/13/2022 00:17:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.633634 on epoch=154
03/13/2022 00:17:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.667513 on epoch=159
03/13/2022 00:17:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.641067 on epoch=164
03/13/2022 00:17:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.620375 on epoch=169
03/13/2022 00:17:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.533372 on epoch=174
03/13/2022 00:17:43 - INFO - __main__ - Global step 350 Train loss 0.619192 EM 0.0 on epoch=174
03/13/2022 00:17:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.543943 on epoch=179
03/13/2022 00:17:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.535199 on epoch=184
03/13/2022 00:17:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.480583 on epoch=189
03/13/2022 00:18:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.676861 on epoch=194
03/13/2022 00:18:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.583802 on epoch=199
03/13/2022 00:18:09 - INFO - __main__ - Global step 400 Train loss 0.564078 EM 0.0 on epoch=199
03/13/2022 00:18:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.519557 on epoch=204
03/13/2022 00:18:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.516618 on epoch=209
03/13/2022 00:18:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.561926 on epoch=214
03/13/2022 00:18:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.462441 on epoch=219
03/13/2022 00:18:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.560170 on epoch=224
03/13/2022 00:18:34 - INFO - __main__ - Global step 450 Train loss 0.524142 EM 0.0 on epoch=224
03/13/2022 00:18:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.501304 on epoch=229
03/13/2022 00:18:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.493528 on epoch=234
03/13/2022 00:18:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.499625 on epoch=239
03/13/2022 00:18:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.470985 on epoch=244
03/13/2022 00:18:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.462345 on epoch=249
03/13/2022 00:18:59 - INFO - __main__ - Global step 500 Train loss 0.485558 EM 0.0 on epoch=249
03/13/2022 00:19:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.481432 on epoch=254
03/13/2022 00:19:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.422655 on epoch=259
03/13/2022 00:19:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.459662 on epoch=264
03/13/2022 00:19:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.423994 on epoch=269
03/13/2022 00:19:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.431082 on epoch=274
03/13/2022 00:19:25 - INFO - __main__ - Global step 550 Train loss 0.443765 EM 0.0 on epoch=274
03/13/2022 00:19:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.421727 on epoch=279
03/13/2022 00:19:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.389924 on epoch=284
03/13/2022 00:19:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.419606 on epoch=289
03/13/2022 00:19:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.361092 on epoch=294
03/13/2022 00:19:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.383778 on epoch=299
03/13/2022 00:19:50 - INFO - __main__ - Global step 600 Train loss 0.395225 EM 0.0 on epoch=299
03/13/2022 00:19:50 - INFO - __main__ - save last model!
03/13/2022 00:19:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:19:51 - INFO - __main__ - Printing 3 examples
03/13/2022 00:19:51 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/13/2022 00:19:51 - INFO - __main__ - ['kieren fallon']
03/13/2022 00:19:51 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/13/2022 00:19:51 - INFO - __main__ - ['uranus']
03/13/2022 00:19:51 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/13/2022 00:19:51 - INFO - __main__ - ['rob thomas']
03/13/2022 00:19:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 00:19:51 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:19:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 00:19:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:19:51 - INFO - __main__ - Printing 3 examples
03/13/2022 00:19:51 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/13/2022 00:19:51 - INFO - __main__ - ['terence rattigan']
03/13/2022 00:19:51 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/13/2022 00:19:51 - INFO - __main__ - ['casino royale']
03/13/2022 00:19:51 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/13/2022 00:19:51 - INFO - __main__ - ['offenbach']
03/13/2022 00:19:51 - INFO - __main__ - Tokenizing Input ...
03/13/2022 00:19:51 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:19:51 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 00:19:57 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 00:19:58 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 00:19:58 - INFO - __main__ - Printing 3 examples
03/13/2022 00:19:58 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 00:19:58 - INFO - __main__ - ['taming of the shrew']
03/13/2022 00:19:58 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 00:19:58 - INFO - __main__ - ['henry fonda']
03/13/2022 00:19:58 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 00:19:58 - INFO - __main__ - ['tchaikovsky']
03/13/2022 00:19:58 - INFO - __main__ - Tokenizing Input ...
03/13/2022 00:19:59 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:20:03 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 00:20:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 00:20:04 - INFO - __main__ - Starting training!
03/13/2022 00:20:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_13_0.0005_8_predictions.txt
03/13/2022 00:20:53 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 00:20:53 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 00:20:53 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.0003, bsz=8 ...
03/13/2022 00:20:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:20:54 - INFO - __main__ - Printing 3 examples
03/13/2022 00:20:54 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/13/2022 00:20:54 - INFO - __main__ - ['kieren fallon']
03/13/2022 00:20:54 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/13/2022 00:20:54 - INFO - __main__ - ['uranus']
03/13/2022 00:20:54 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/13/2022 00:20:54 - INFO - __main__ - ['rob thomas']
03/13/2022 00:20:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 00:20:54 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:20:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 00:20:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:20:54 - INFO - __main__ - Printing 3 examples
03/13/2022 00:20:54 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/13/2022 00:20:54 - INFO - __main__ - ['terence rattigan']
03/13/2022 00:20:54 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/13/2022 00:20:54 - INFO - __main__ - ['casino royale']
03/13/2022 00:20:54 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/13/2022 00:20:54 - INFO - __main__ - ['offenbach']
03/13/2022 00:20:54 - INFO - __main__ - Tokenizing Input ...
03/13/2022 00:20:54 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:20:54 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 00:21:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 00:21:07 - INFO - __main__ - Starting training!
03/13/2022 00:21:11 - INFO - __main__ - Step 10 Global step 10 Train loss 18.836620 on epoch=4
03/13/2022 00:21:16 - INFO - __main__ - Step 20 Global step 20 Train loss 17.953547 on epoch=9
03/13/2022 00:21:21 - INFO - __main__ - Step 30 Global step 30 Train loss 14.086931 on epoch=14
03/13/2022 00:21:26 - INFO - __main__ - Step 40 Global step 40 Train loss 11.830763 on epoch=19
03/13/2022 00:21:31 - INFO - __main__ - Step 50 Global step 50 Train loss 10.334264 on epoch=24
03/13/2022 00:21:32 - INFO - __main__ - Global step 50 Train loss 14.608426 EM 0.0 on epoch=24
03/13/2022 00:21:37 - INFO - __main__ - Step 60 Global step 60 Train loss 9.138165 on epoch=29
03/13/2022 00:21:42 - INFO - __main__ - Step 70 Global step 70 Train loss 8.441557 on epoch=34
03/13/2022 00:21:47 - INFO - __main__ - Step 80 Global step 80 Train loss 7.787609 on epoch=39
03/13/2022 00:21:52 - INFO - __main__ - Step 90 Global step 90 Train loss 6.803780 on epoch=44
03/13/2022 00:21:57 - INFO - __main__ - Step 100 Global step 100 Train loss 6.041543 on epoch=49
03/13/2022 00:22:02 - INFO - __main__ - Global step 100 Train loss 7.642530 EM 0.0 on epoch=49
03/13/2022 00:22:07 - INFO - __main__ - Step 110 Global step 110 Train loss 5.430619 on epoch=54
03/13/2022 00:22:12 - INFO - __main__ - Step 120 Global step 120 Train loss 4.603088 on epoch=59
03/13/2022 00:22:17 - INFO - __main__ - Step 130 Global step 130 Train loss 3.782185 on epoch=64
03/13/2022 00:22:22 - INFO - __main__ - Step 140 Global step 140 Train loss 3.580584 on epoch=69
03/13/2022 00:22:27 - INFO - __main__ - Step 150 Global step 150 Train loss 2.906504 on epoch=74
03/13/2022 00:22:28 - INFO - __main__ - Global step 150 Train loss 4.060596 EM 0.0 on epoch=74
03/13/2022 00:22:33 - INFO - __main__ - Step 160 Global step 160 Train loss 2.951189 on epoch=79
03/13/2022 00:22:38 - INFO - __main__ - Step 170 Global step 170 Train loss 2.384788 on epoch=84
03/13/2022 00:22:43 - INFO - __main__ - Step 180 Global step 180 Train loss 2.182132 on epoch=89
03/13/2022 00:22:48 - INFO - __main__ - Step 190 Global step 190 Train loss 2.356191 on epoch=94
03/13/2022 00:22:53 - INFO - __main__ - Step 200 Global step 200 Train loss 1.942757 on epoch=99
03/13/2022 00:22:54 - INFO - __main__ - Global step 200 Train loss 2.363411 EM 0.0 on epoch=99
03/13/2022 00:22:59 - INFO - __main__ - Step 210 Global step 210 Train loss 1.774028 on epoch=104
03/13/2022 00:23:04 - INFO - __main__ - Step 220 Global step 220 Train loss 1.801622 on epoch=109
03/13/2022 00:23:09 - INFO - __main__ - Step 230 Global step 230 Train loss 1.592428 on epoch=114
03/13/2022 00:23:14 - INFO - __main__ - Step 240 Global step 240 Train loss 1.486695 on epoch=119
03/13/2022 00:23:19 - INFO - __main__ - Step 250 Global step 250 Train loss 1.330745 on epoch=124
03/13/2022 00:23:20 - INFO - __main__ - Global step 250 Train loss 1.597103 EM 0.0 on epoch=124
03/13/2022 00:23:25 - INFO - __main__ - Step 260 Global step 260 Train loss 1.395729 on epoch=129
03/13/2022 00:23:30 - INFO - __main__ - Step 270 Global step 270 Train loss 1.402604 on epoch=134
03/13/2022 00:23:35 - INFO - __main__ - Step 280 Global step 280 Train loss 1.734587 on epoch=139
03/13/2022 00:23:40 - INFO - __main__ - Step 290 Global step 290 Train loss 1.320740 on epoch=144
03/13/2022 00:23:45 - INFO - __main__ - Step 300 Global step 300 Train loss 1.100408 on epoch=149
03/13/2022 00:23:46 - INFO - __main__ - Global step 300 Train loss 1.390814 EM 0.0 on epoch=149
03/13/2022 00:23:51 - INFO - __main__ - Step 310 Global step 310 Train loss 1.323155 on epoch=154
03/13/2022 00:23:56 - INFO - __main__ - Step 320 Global step 320 Train loss 1.398601 on epoch=159
03/13/2022 00:24:01 - INFO - __main__ - Step 330 Global step 330 Train loss 1.070827 on epoch=164
03/13/2022 00:24:06 - INFO - __main__ - Step 340 Global step 340 Train loss 1.210262 on epoch=169
03/13/2022 00:24:11 - INFO - __main__ - Step 350 Global step 350 Train loss 1.100784 on epoch=174
03/13/2022 00:24:11 - INFO - __main__ - Global step 350 Train loss 1.220726 EM 0.0 on epoch=174
03/13/2022 00:24:16 - INFO - __main__ - Step 360 Global step 360 Train loss 1.176715 on epoch=179
03/13/2022 00:24:21 - INFO - __main__ - Step 370 Global step 370 Train loss 1.220779 on epoch=184
03/13/2022 00:24:26 - INFO - __main__ - Step 380 Global step 380 Train loss 1.011259 on epoch=189
03/13/2022 00:24:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.898219 on epoch=194
03/13/2022 00:24:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.950917 on epoch=199
03/13/2022 00:24:37 - INFO - __main__ - Global step 400 Train loss 1.051578 EM 0.0 on epoch=199
03/13/2022 00:24:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.858074 on epoch=204
03/13/2022 00:24:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.861687 on epoch=209
03/13/2022 00:24:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.811152 on epoch=214
03/13/2022 00:24:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.839018 on epoch=219
03/13/2022 00:25:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.777293 on epoch=224
03/13/2022 00:25:03 - INFO - __main__ - Global step 450 Train loss 0.829445 EM 0.0 on epoch=224
03/13/2022 00:25:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.964572 on epoch=229
03/13/2022 00:25:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.810398 on epoch=234
03/13/2022 00:25:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.996703 on epoch=239
03/13/2022 00:25:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.810195 on epoch=244
03/13/2022 00:25:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.700593 on epoch=249
03/13/2022 00:25:29 - INFO - __main__ - Global step 500 Train loss 0.856492 EM 0.0 on epoch=249
03/13/2022 00:25:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.687679 on epoch=254
03/13/2022 00:25:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.763579 on epoch=259
03/13/2022 00:25:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.911056 on epoch=264
03/13/2022 00:25:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.734359 on epoch=269
03/13/2022 00:25:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.722869 on epoch=274
03/13/2022 00:25:54 - INFO - __main__ - Global step 550 Train loss 0.763908 EM 0.0 on epoch=274
03/13/2022 00:25:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.741453 on epoch=279
03/13/2022 00:26:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.713229 on epoch=284
03/13/2022 00:26:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.598963 on epoch=289
03/13/2022 00:26:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.655305 on epoch=294
03/13/2022 00:26:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.613508 on epoch=299
03/13/2022 00:26:20 - INFO - __main__ - Global step 600 Train loss 0.664492 EM 0.0 on epoch=299
03/13/2022 00:26:20 - INFO - __main__ - save last model!
03/13/2022 00:26:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:26:21 - INFO - __main__ - Printing 3 examples
03/13/2022 00:26:21 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/13/2022 00:26:21 - INFO - __main__ - ['kieren fallon']
03/13/2022 00:26:21 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/13/2022 00:26:21 - INFO - __main__ - ['uranus']
03/13/2022 00:26:21 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/13/2022 00:26:21 - INFO - __main__ - ['rob thomas']
03/13/2022 00:26:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 00:26:21 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:26:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 00:26:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:26:21 - INFO - __main__ - Printing 3 examples
03/13/2022 00:26:21 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/13/2022 00:26:21 - INFO - __main__ - ['terence rattigan']
03/13/2022 00:26:21 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/13/2022 00:26:21 - INFO - __main__ - ['casino royale']
03/13/2022 00:26:21 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/13/2022 00:26:21 - INFO - __main__ - ['offenbach']
03/13/2022 00:26:21 - INFO - __main__ - Tokenizing Input ...
03/13/2022 00:26:21 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:26:21 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 00:26:27 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 00:26:28 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 00:26:28 - INFO - __main__ - Printing 3 examples
03/13/2022 00:26:28 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 00:26:28 - INFO - __main__ - ['taming of the shrew']
03/13/2022 00:26:28 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 00:26:28 - INFO - __main__ - ['henry fonda']
03/13/2022 00:26:28 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 00:26:28 - INFO - __main__ - ['tchaikovsky']
03/13/2022 00:26:28 - INFO - __main__ - Tokenizing Input ...
03/13/2022 00:26:29 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:26:33 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 00:26:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 00:26:33 - INFO - __main__ - Starting training!
03/13/2022 00:28:18 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_13_0.0003_8_predictions.txt
03/13/2022 00:28:18 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 00:28:19 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 00:28:19 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.0002, bsz=8 ...
03/13/2022 00:28:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:28:19 - INFO - __main__ - Printing 3 examples
03/13/2022 00:28:19 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/13/2022 00:28:19 - INFO - __main__ - ['kieren fallon']
03/13/2022 00:28:19 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/13/2022 00:28:19 - INFO - __main__ - ['uranus']
03/13/2022 00:28:19 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/13/2022 00:28:19 - INFO - __main__ - ['rob thomas']
03/13/2022 00:28:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 00:28:19 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:28:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 00:28:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:28:20 - INFO - __main__ - Printing 3 examples
03/13/2022 00:28:20 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/13/2022 00:28:20 - INFO - __main__ - ['terence rattigan']
03/13/2022 00:28:20 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/13/2022 00:28:20 - INFO - __main__ - ['casino royale']
03/13/2022 00:28:20 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/13/2022 00:28:20 - INFO - __main__ - ['offenbach']
03/13/2022 00:28:20 - INFO - __main__ - Tokenizing Input ...
03/13/2022 00:28:20 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:28:20 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 00:28:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 00:28:30 - INFO - __main__ - Starting training!
03/13/2022 00:28:34 - INFO - __main__ - Step 10 Global step 10 Train loss 20.414490 on epoch=4
03/13/2022 00:28:39 - INFO - __main__ - Step 20 Global step 20 Train loss 17.079016 on epoch=9
03/13/2022 00:28:44 - INFO - __main__ - Step 30 Global step 30 Train loss 12.089023 on epoch=14
03/13/2022 00:28:49 - INFO - __main__ - Step 40 Global step 40 Train loss 12.320376 on epoch=19
03/13/2022 00:28:54 - INFO - __main__ - Step 50 Global step 50 Train loss 10.454418 on epoch=24
03/13/2022 00:28:56 - INFO - __main__ - Global step 50 Train loss 14.471463 EM 0.0 on epoch=24
03/13/2022 00:29:02 - INFO - __main__ - Step 60 Global step 60 Train loss 9.103796 on epoch=29
03/13/2022 00:29:07 - INFO - __main__ - Step 70 Global step 70 Train loss 8.449147 on epoch=34
03/13/2022 00:29:12 - INFO - __main__ - Step 80 Global step 80 Train loss 6.878772 on epoch=39
03/13/2022 00:29:17 - INFO - __main__ - Step 90 Global step 90 Train loss 6.058568 on epoch=44
03/13/2022 00:29:22 - INFO - __main__ - Step 100 Global step 100 Train loss 5.589979 on epoch=49
03/13/2022 00:29:22 - INFO - __main__ - Global step 100 Train loss 7.216053 EM 0.0 on epoch=49
03/13/2022 00:29:27 - INFO - __main__ - Step 110 Global step 110 Train loss 5.130569 on epoch=54
03/13/2022 00:29:32 - INFO - __main__ - Step 120 Global step 120 Train loss 5.227522 on epoch=59
03/13/2022 00:29:37 - INFO - __main__ - Step 130 Global step 130 Train loss 4.346606 on epoch=64
03/13/2022 00:29:42 - INFO - __main__ - Step 140 Global step 140 Train loss 3.957483 on epoch=69
03/13/2022 00:29:47 - INFO - __main__ - Step 150 Global step 150 Train loss 3.868021 on epoch=74
03/13/2022 00:29:48 - INFO - __main__ - Global step 150 Train loss 4.506040 EM 0.0 on epoch=74
03/13/2022 00:29:53 - INFO - __main__ - Step 160 Global step 160 Train loss 3.647104 on epoch=79
03/13/2022 00:29:58 - INFO - __main__ - Step 170 Global step 170 Train loss 3.220709 on epoch=84
03/13/2022 00:30:02 - INFO - __main__ - Step 180 Global step 180 Train loss 3.388107 on epoch=89
03/13/2022 00:30:07 - INFO - __main__ - Step 190 Global step 190 Train loss 2.753988 on epoch=94
03/13/2022 00:30:12 - INFO - __main__ - Step 200 Global step 200 Train loss 2.495013 on epoch=99
03/13/2022 00:30:13 - INFO - __main__ - Global step 200 Train loss 3.100985 EM 0.0 on epoch=99
03/13/2022 00:30:18 - INFO - __main__ - Step 210 Global step 210 Train loss 2.219769 on epoch=104
03/13/2022 00:30:23 - INFO - __main__ - Step 220 Global step 220 Train loss 2.505224 on epoch=109
03/13/2022 00:30:28 - INFO - __main__ - Step 230 Global step 230 Train loss 2.075695 on epoch=114
03/13/2022 00:30:33 - INFO - __main__ - Step 240 Global step 240 Train loss 2.165919 on epoch=119
03/13/2022 00:30:38 - INFO - __main__ - Step 250 Global step 250 Train loss 2.245705 on epoch=124
03/13/2022 00:30:38 - INFO - __main__ - Global step 250 Train loss 2.242462 EM 0.0 on epoch=124
03/13/2022 00:30:43 - INFO - __main__ - Step 260 Global step 260 Train loss 1.958790 on epoch=129
03/13/2022 00:30:48 - INFO - __main__ - Step 270 Global step 270 Train loss 1.721898 on epoch=134
03/13/2022 00:30:53 - INFO - __main__ - Step 280 Global step 280 Train loss 1.623948 on epoch=139
03/13/2022 00:30:58 - INFO - __main__ - Step 290 Global step 290 Train loss 1.866730 on epoch=144
03/13/2022 00:31:03 - INFO - __main__ - Step 300 Global step 300 Train loss 1.587056 on epoch=149
03/13/2022 00:31:04 - INFO - __main__ - Global step 300 Train loss 1.751684 EM 0.0 on epoch=149
03/13/2022 00:31:09 - INFO - __main__ - Step 310 Global step 310 Train loss 1.613521 on epoch=154
03/13/2022 00:31:14 - INFO - __main__ - Step 320 Global step 320 Train loss 1.718831 on epoch=159
03/13/2022 00:31:19 - INFO - __main__ - Step 330 Global step 330 Train loss 1.404723 on epoch=164
03/13/2022 00:31:24 - INFO - __main__ - Step 340 Global step 340 Train loss 1.351472 on epoch=169
03/13/2022 00:31:29 - INFO - __main__ - Step 350 Global step 350 Train loss 1.160037 on epoch=174
03/13/2022 00:31:29 - INFO - __main__ - Global step 350 Train loss 1.449717 EM 0.0 on epoch=174
03/13/2022 00:31:34 - INFO - __main__ - Step 360 Global step 360 Train loss 1.382894 on epoch=179
03/13/2022 00:31:39 - INFO - __main__ - Step 370 Global step 370 Train loss 1.286462 on epoch=184
03/13/2022 00:31:44 - INFO - __main__ - Step 380 Global step 380 Train loss 1.325101 on epoch=189
03/13/2022 00:31:49 - INFO - __main__ - Step 390 Global step 390 Train loss 1.219042 on epoch=194
03/13/2022 00:31:54 - INFO - __main__ - Step 400 Global step 400 Train loss 1.105587 on epoch=199
03/13/2022 00:31:55 - INFO - __main__ - Global step 400 Train loss 1.263817 EM 0.0 on epoch=199
03/13/2022 00:32:00 - INFO - __main__ - Step 410 Global step 410 Train loss 1.377960 on epoch=204
03/13/2022 00:32:05 - INFO - __main__ - Step 420 Global step 420 Train loss 1.314555 on epoch=209
03/13/2022 00:32:10 - INFO - __main__ - Step 430 Global step 430 Train loss 1.128772 on epoch=214
03/13/2022 00:32:15 - INFO - __main__ - Step 440 Global step 440 Train loss 1.265045 on epoch=219
03/13/2022 00:32:19 - INFO - __main__ - Step 450 Global step 450 Train loss 1.152570 on epoch=224
03/13/2022 00:32:20 - INFO - __main__ - Global step 450 Train loss 1.247780 EM 0.0 on epoch=224
03/13/2022 00:32:25 - INFO - __main__ - Step 460 Global step 460 Train loss 1.221932 on epoch=229
03/13/2022 00:32:30 - INFO - __main__ - Step 470 Global step 470 Train loss 1.260016 on epoch=234
03/13/2022 00:32:35 - INFO - __main__ - Step 480 Global step 480 Train loss 1.128015 on epoch=239
03/13/2022 00:32:40 - INFO - __main__ - Step 490 Global step 490 Train loss 1.001962 on epoch=244
03/13/2022 00:32:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.942258 on epoch=249
03/13/2022 00:32:46 - INFO - __main__ - Global step 500 Train loss 1.110837 EM 0.0 on epoch=249
03/13/2022 00:32:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.982524 on epoch=254
03/13/2022 00:32:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.991893 on epoch=259
03/13/2022 00:33:01 - INFO - __main__ - Step 530 Global step 530 Train loss 1.059788 on epoch=264
03/13/2022 00:33:06 - INFO - __main__ - Step 540 Global step 540 Train loss 1.030799 on epoch=269
03/13/2022 00:33:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.833158 on epoch=274
03/13/2022 00:33:11 - INFO - __main__ - Global step 550 Train loss 0.979632 EM 0.0 on epoch=274
03/13/2022 00:33:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.761722 on epoch=279
03/13/2022 00:33:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.936360 on epoch=284
03/13/2022 00:33:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.952136 on epoch=289
03/13/2022 00:33:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.889407 on epoch=294
03/13/2022 00:33:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.837741 on epoch=299
03/13/2022 00:33:37 - INFO - __main__ - Global step 600 Train loss 0.875473 EM 0.0 on epoch=299
03/13/2022 00:33:37 - INFO - __main__ - save last model!
03/13/2022 00:33:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:33:37 - INFO - __main__ - Printing 3 examples
03/13/2022 00:33:37 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/13/2022 00:33:37 - INFO - __main__ - ['kieren fallon']
03/13/2022 00:33:37 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/13/2022 00:33:37 - INFO - __main__ - ['uranus']
03/13/2022 00:33:37 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/13/2022 00:33:37 - INFO - __main__ - ['rob thomas']
03/13/2022 00:33:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 00:33:37 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:33:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 00:33:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:33:37 - INFO - __main__ - Printing 3 examples
03/13/2022 00:33:37 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/13/2022 00:33:37 - INFO - __main__ - ['terence rattigan']
03/13/2022 00:33:37 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/13/2022 00:33:37 - INFO - __main__ - ['casino royale']
03/13/2022 00:33:37 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/13/2022 00:33:37 - INFO - __main__ - ['offenbach']
03/13/2022 00:33:37 - INFO - __main__ - Tokenizing Input ...
03/13/2022 00:33:37 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:33:37 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 00:33:44 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 00:33:44 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 00:33:44 - INFO - __main__ - Printing 3 examples
03/13/2022 00:33:44 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 00:33:44 - INFO - __main__ - ['taming of the shrew']
03/13/2022 00:33:44 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 00:33:44 - INFO - __main__ - ['henry fonda']
03/13/2022 00:33:44 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 00:33:44 - INFO - __main__ - ['tchaikovsky']
03/13/2022 00:33:44 - INFO - __main__ - Tokenizing Input ...
03/13/2022 00:33:46 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:33:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 00:33:48 - INFO - __main__ - Starting training!
03/13/2022 00:33:50 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 00:40:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_13_0.0002_8_predictions.txt
03/13/2022 00:40:42 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 00:40:43 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 00:40:43 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.0001, bsz=8 ...
03/13/2022 00:40:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:40:44 - INFO - __main__ - Printing 3 examples
03/13/2022 00:40:44 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/13/2022 00:40:44 - INFO - __main__ - ['kieren fallon']
03/13/2022 00:40:44 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/13/2022 00:40:44 - INFO - __main__ - ['uranus']
03/13/2022 00:40:44 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/13/2022 00:40:44 - INFO - __main__ - ['rob thomas']
03/13/2022 00:40:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 00:40:44 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:40:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 00:40:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:40:44 - INFO - __main__ - Printing 3 examples
03/13/2022 00:40:44 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/13/2022 00:40:44 - INFO - __main__ - ['terence rattigan']
03/13/2022 00:40:44 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/13/2022 00:40:44 - INFO - __main__ - ['casino royale']
03/13/2022 00:40:44 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/13/2022 00:40:44 - INFO - __main__ - ['offenbach']
03/13/2022 00:40:44 - INFO - __main__ - Tokenizing Input ...
03/13/2022 00:40:44 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:40:44 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 00:40:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 00:40:57 - INFO - __main__ - Starting training!
03/13/2022 00:41:01 - INFO - __main__ - Step 10 Global step 10 Train loss 19.269310 on epoch=4
03/13/2022 00:41:05 - INFO - __main__ - Step 20 Global step 20 Train loss 17.323755 on epoch=9
03/13/2022 00:41:10 - INFO - __main__ - Step 30 Global step 30 Train loss 14.236923 on epoch=14
03/13/2022 00:41:15 - INFO - __main__ - Step 40 Global step 40 Train loss 12.796759 on epoch=19
03/13/2022 00:41:20 - INFO - __main__ - Step 50 Global step 50 Train loss 12.135813 on epoch=24
03/13/2022 00:41:30 - INFO - __main__ - Global step 50 Train loss 15.152510 EM 0.0 on epoch=24
03/13/2022 00:41:36 - INFO - __main__ - Step 60 Global step 60 Train loss 11.079126 on epoch=29
03/13/2022 00:41:41 - INFO - __main__ - Step 70 Global step 70 Train loss 10.538333 on epoch=34
03/13/2022 00:41:46 - INFO - __main__ - Step 80 Global step 80 Train loss 9.897804 on epoch=39
03/13/2022 00:41:51 - INFO - __main__ - Step 90 Global step 90 Train loss 9.147839 on epoch=44
03/13/2022 00:41:56 - INFO - __main__ - Step 100 Global step 100 Train loss 8.963337 on epoch=49
03/13/2022 00:41:58 - INFO - __main__ - Global step 100 Train loss 9.925289 EM 0.0 on epoch=49
03/13/2022 00:42:03 - INFO - __main__ - Step 110 Global step 110 Train loss 8.627371 on epoch=54
03/13/2022 00:42:08 - INFO - __main__ - Step 120 Global step 120 Train loss 7.360074 on epoch=59
03/13/2022 00:42:13 - INFO - __main__ - Step 130 Global step 130 Train loss 7.171187 on epoch=64
03/13/2022 00:42:18 - INFO - __main__ - Step 140 Global step 140 Train loss 6.772900 on epoch=69
03/13/2022 00:42:23 - INFO - __main__ - Step 150 Global step 150 Train loss 6.514281 on epoch=74
03/13/2022 00:42:25 - INFO - __main__ - Global step 150 Train loss 7.289162 EM 0.0 on epoch=74
03/13/2022 00:42:30 - INFO - __main__ - Step 160 Global step 160 Train loss 6.193542 on epoch=79
03/13/2022 00:42:35 - INFO - __main__ - Step 170 Global step 170 Train loss 5.355539 on epoch=84
03/13/2022 00:42:40 - INFO - __main__ - Step 180 Global step 180 Train loss 5.507732 on epoch=89
03/13/2022 00:42:45 - INFO - __main__ - Step 190 Global step 190 Train loss 5.479001 on epoch=94
03/13/2022 00:42:50 - INFO - __main__ - Step 200 Global step 200 Train loss 5.192831 on epoch=99
03/13/2022 00:42:51 - INFO - __main__ - Global step 200 Train loss 5.545729 EM 0.0 on epoch=99
03/13/2022 00:42:56 - INFO - __main__ - Step 210 Global step 210 Train loss 4.910288 on epoch=104
03/13/2022 00:43:01 - INFO - __main__ - Step 220 Global step 220 Train loss 4.815755 on epoch=109
03/13/2022 00:43:06 - INFO - __main__ - Step 230 Global step 230 Train loss 4.318347 on epoch=114
03/13/2022 00:43:11 - INFO - __main__ - Step 240 Global step 240 Train loss 4.251761 on epoch=119
03/13/2022 00:43:16 - INFO - __main__ - Step 250 Global step 250 Train loss 3.933549 on epoch=124
03/13/2022 00:43:17 - INFO - __main__ - Global step 250 Train loss 4.445940 EM 0.0 on epoch=124
03/13/2022 00:43:22 - INFO - __main__ - Step 260 Global step 260 Train loss 3.775095 on epoch=129
03/13/2022 00:43:28 - INFO - __main__ - Step 270 Global step 270 Train loss 3.808733 on epoch=134
03/13/2022 00:43:33 - INFO - __main__ - Step 280 Global step 280 Train loss 3.536162 on epoch=139
03/13/2022 00:43:38 - INFO - __main__ - Step 290 Global step 290 Train loss 3.277447 on epoch=144
03/13/2022 00:43:43 - INFO - __main__ - Step 300 Global step 300 Train loss 3.466547 on epoch=149
03/13/2022 00:43:44 - INFO - __main__ - Global step 300 Train loss 3.572797 EM 0.0 on epoch=149
03/13/2022 00:43:49 - INFO - __main__ - Step 310 Global step 310 Train loss 3.478399 on epoch=154
03/13/2022 00:43:54 - INFO - __main__ - Step 320 Global step 320 Train loss 3.391630 on epoch=159
03/13/2022 00:43:59 - INFO - __main__ - Step 330 Global step 330 Train loss 3.169086 on epoch=164
03/13/2022 00:44:04 - INFO - __main__ - Step 340 Global step 340 Train loss 3.069670 on epoch=169
03/13/2022 00:44:09 - INFO - __main__ - Step 350 Global step 350 Train loss 2.949744 on epoch=174
03/13/2022 00:44:10 - INFO - __main__ - Global step 350 Train loss 3.211706 EM 0.0 on epoch=174
03/13/2022 00:44:15 - INFO - __main__ - Step 360 Global step 360 Train loss 2.924052 on epoch=179
03/13/2022 00:44:20 - INFO - __main__ - Step 370 Global step 370 Train loss 2.859942 on epoch=184
03/13/2022 00:44:25 - INFO - __main__ - Step 380 Global step 380 Train loss 2.495216 on epoch=189
03/13/2022 00:44:30 - INFO - __main__ - Step 390 Global step 390 Train loss 2.653051 on epoch=194
03/13/2022 00:44:35 - INFO - __main__ - Step 400 Global step 400 Train loss 2.391881 on epoch=199
03/13/2022 00:44:36 - INFO - __main__ - Global step 400 Train loss 2.664829 EM 0.0 on epoch=199
03/13/2022 00:44:41 - INFO - __main__ - Step 410 Global step 410 Train loss 2.255849 on epoch=204
03/13/2022 00:44:46 - INFO - __main__ - Step 420 Global step 420 Train loss 2.351448 on epoch=209
03/13/2022 00:44:51 - INFO - __main__ - Step 430 Global step 430 Train loss 2.198747 on epoch=214
03/13/2022 00:44:56 - INFO - __main__ - Step 440 Global step 440 Train loss 2.150790 on epoch=219
03/13/2022 00:45:01 - INFO - __main__ - Step 450 Global step 450 Train loss 2.141139 on epoch=224
03/13/2022 00:45:02 - INFO - __main__ - Global step 450 Train loss 2.219594 EM 0.0 on epoch=224
03/13/2022 00:45:07 - INFO - __main__ - Step 460 Global step 460 Train loss 1.977167 on epoch=229
03/13/2022 00:45:12 - INFO - __main__ - Step 470 Global step 470 Train loss 2.193000 on epoch=234
03/13/2022 00:45:17 - INFO - __main__ - Step 480 Global step 480 Train loss 1.858158 on epoch=239
03/13/2022 00:45:22 - INFO - __main__ - Step 490 Global step 490 Train loss 2.028031 on epoch=244
03/13/2022 00:45:28 - INFO - __main__ - Step 500 Global step 500 Train loss 2.228107 on epoch=249
03/13/2022 00:45:28 - INFO - __main__ - Global step 500 Train loss 2.056893 EM 0.0 on epoch=249
03/13/2022 00:45:33 - INFO - __main__ - Step 510 Global step 510 Train loss 1.716119 on epoch=254
03/13/2022 00:45:38 - INFO - __main__ - Step 520 Global step 520 Train loss 1.970831 on epoch=259
03/13/2022 00:45:43 - INFO - __main__ - Step 530 Global step 530 Train loss 1.765356 on epoch=264
03/13/2022 00:45:48 - INFO - __main__ - Step 540 Global step 540 Train loss 1.689953 on epoch=269
03/13/2022 00:45:53 - INFO - __main__ - Step 550 Global step 550 Train loss 1.499290 on epoch=274
03/13/2022 00:45:54 - INFO - __main__ - Global step 550 Train loss 1.728310 EM 0.0 on epoch=274
03/13/2022 00:45:59 - INFO - __main__ - Step 560 Global step 560 Train loss 1.622921 on epoch=279
03/13/2022 00:46:04 - INFO - __main__ - Step 570 Global step 570 Train loss 1.702657 on epoch=284
03/13/2022 00:46:09 - INFO - __main__ - Step 580 Global step 580 Train loss 1.897924 on epoch=289
03/13/2022 00:46:14 - INFO - __main__ - Step 590 Global step 590 Train loss 1.591749 on epoch=294
03/13/2022 00:46:19 - INFO - __main__ - Step 600 Global step 600 Train loss 1.492626 on epoch=299
03/13/2022 00:46:20 - INFO - __main__ - Global step 600 Train loss 1.661575 EM 0.0 on epoch=299
03/13/2022 00:46:20 - INFO - __main__ - save last model!
03/13/2022 00:46:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:46:22 - INFO - __main__ - Printing 3 examples
03/13/2022 00:46:22 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/13/2022 00:46:22 - INFO - __main__ - ['camille saint-saens']
03/13/2022 00:46:22 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/13/2022 00:46:22 - INFO - __main__ - ['madness']
03/13/2022 00:46:22 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/13/2022 00:46:22 - INFO - __main__ - ['genevieve']
03/13/2022 00:46:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 00:46:22 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:46:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 00:46:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 00:46:22 - INFO - __main__ - Printing 3 examples
03/13/2022 00:46:22 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/13/2022 00:46:22 - INFO - __main__ - ['will hay']
03/13/2022 00:46:22 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/13/2022 00:46:22 - INFO - __main__ - ['alan sugar']
03/13/2022 00:46:22 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/13/2022 00:46:22 - INFO - __main__ - ['cleopatra']
03/13/2022 00:46:22 - INFO - __main__ - Tokenizing Input ...
03/13/2022 00:46:22 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:46:22 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 00:46:27 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 00:46:28 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 00:46:28 - INFO - __main__ - Printing 3 examples
03/13/2022 00:46:28 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 00:46:28 - INFO - __main__ - ['taming of the shrew']
03/13/2022 00:46:28 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 00:46:28 - INFO - __main__ - ['henry fonda']
03/13/2022 00:46:28 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 00:46:28 - INFO - __main__ - ['tchaikovsky']
03/13/2022 00:46:28 - INFO - __main__ - Tokenizing Input ...
03/13/2022 00:46:29 - INFO - __main__ - Tokenizing Output ...
03/13/2022 00:46:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 00:46:33 - INFO - __main__ - Starting training!
03/13/2022 00:46:33 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 01:05:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_13_0.0001_8_predictions.txt
03/13/2022 01:05:41 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 01:05:41 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 01:05:41 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.0005, bsz=8 ...
03/13/2022 01:05:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:05:42 - INFO - __main__ - Printing 3 examples
03/13/2022 01:05:42 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/13/2022 01:05:42 - INFO - __main__ - ['camille saint-saens']
03/13/2022 01:05:42 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/13/2022 01:05:42 - INFO - __main__ - ['madness']
03/13/2022 01:05:42 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/13/2022 01:05:42 - INFO - __main__ - ['genevieve']
03/13/2022 01:05:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 01:05:42 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:05:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 01:05:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:05:42 - INFO - __main__ - Printing 3 examples
03/13/2022 01:05:42 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/13/2022 01:05:42 - INFO - __main__ - ['will hay']
03/13/2022 01:05:42 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/13/2022 01:05:42 - INFO - __main__ - ['alan sugar']
03/13/2022 01:05:42 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/13/2022 01:05:42 - INFO - __main__ - ['cleopatra']
03/13/2022 01:05:42 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:05:42 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:05:42 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 01:05:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 01:05:53 - INFO - __main__ - Starting training!
03/13/2022 01:05:57 - INFO - __main__ - Step 10 Global step 10 Train loss 20.391233 on epoch=4
03/13/2022 01:06:01 - INFO - __main__ - Step 20 Global step 20 Train loss 17.239750 on epoch=9
03/13/2022 01:06:06 - INFO - __main__ - Step 30 Global step 30 Train loss 14.528427 on epoch=14
03/13/2022 01:06:10 - INFO - __main__ - Step 40 Global step 40 Train loss 11.198919 on epoch=19
03/13/2022 01:06:15 - INFO - __main__ - Step 50 Global step 50 Train loss 9.129099 on epoch=24
03/13/2022 01:06:16 - INFO - __main__ - Global step 50 Train loss 14.497486 EM 0.0 on epoch=24
03/13/2022 01:06:21 - INFO - __main__ - Step 60 Global step 60 Train loss 7.514589 on epoch=29
03/13/2022 01:06:26 - INFO - __main__ - Step 70 Global step 70 Train loss 5.920504 on epoch=34
03/13/2022 01:06:31 - INFO - __main__ - Step 80 Global step 80 Train loss 5.162207 on epoch=39
03/13/2022 01:06:36 - INFO - __main__ - Step 90 Global step 90 Train loss 4.424291 on epoch=44
03/13/2022 01:06:41 - INFO - __main__ - Step 100 Global step 100 Train loss 3.621951 on epoch=49
03/13/2022 01:06:42 - INFO - __main__ - Global step 100 Train loss 5.328708 EM 0.0 on epoch=49
03/13/2022 01:06:47 - INFO - __main__ - Step 110 Global step 110 Train loss 3.235667 on epoch=54
03/13/2022 01:06:52 - INFO - __main__ - Step 120 Global step 120 Train loss 2.415406 on epoch=59
03/13/2022 01:06:57 - INFO - __main__ - Step 130 Global step 130 Train loss 2.397946 on epoch=64
03/13/2022 01:07:01 - INFO - __main__ - Step 140 Global step 140 Train loss 2.181601 on epoch=69
03/13/2022 01:07:06 - INFO - __main__ - Step 150 Global step 150 Train loss 1.983761 on epoch=74
03/13/2022 01:07:07 - INFO - __main__ - Global step 150 Train loss 2.442876 EM 0.0 on epoch=74
03/13/2022 01:07:12 - INFO - __main__ - Step 160 Global step 160 Train loss 1.906361 on epoch=79
03/13/2022 01:07:17 - INFO - __main__ - Step 170 Global step 170 Train loss 1.943767 on epoch=84
03/13/2022 01:07:22 - INFO - __main__ - Step 180 Global step 180 Train loss 2.120924 on epoch=89
03/13/2022 01:07:27 - INFO - __main__ - Step 190 Global step 190 Train loss 1.734363 on epoch=94
03/13/2022 01:07:32 - INFO - __main__ - Step 200 Global step 200 Train loss 1.860327 on epoch=99
03/13/2022 01:07:33 - INFO - __main__ - Global step 200 Train loss 1.913149 EM 0.0 on epoch=99
03/13/2022 01:07:39 - INFO - __main__ - Step 210 Global step 210 Train loss 1.638719 on epoch=104
03/13/2022 01:07:44 - INFO - __main__ - Step 220 Global step 220 Train loss 1.617538 on epoch=109
03/13/2022 01:07:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.589280 on epoch=114
03/13/2022 01:07:54 - INFO - __main__ - Step 240 Global step 240 Train loss 1.249929 on epoch=119
03/13/2022 01:07:59 - INFO - __main__ - Step 250 Global step 250 Train loss 1.479966 on epoch=124
03/13/2022 01:07:59 - INFO - __main__ - Global step 250 Train loss 1.515087 EM 0.0 on epoch=124
03/13/2022 01:08:05 - INFO - __main__ - Step 260 Global step 260 Train loss 1.432284 on epoch=129
03/13/2022 01:08:10 - INFO - __main__ - Step 270 Global step 270 Train loss 1.443131 on epoch=134
03/13/2022 01:08:15 - INFO - __main__ - Step 280 Global step 280 Train loss 1.147518 on epoch=139
03/13/2022 01:08:20 - INFO - __main__ - Step 290 Global step 290 Train loss 1.381662 on epoch=144
03/13/2022 01:08:25 - INFO - __main__ - Step 300 Global step 300 Train loss 1.166412 on epoch=149
03/13/2022 01:08:26 - INFO - __main__ - Global step 300 Train loss 1.314201 EM 0.0 on epoch=149
03/13/2022 01:08:31 - INFO - __main__ - Step 310 Global step 310 Train loss 1.275138 on epoch=154
03/13/2022 01:08:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.134515 on epoch=159
03/13/2022 01:08:41 - INFO - __main__ - Step 330 Global step 330 Train loss 1.088333 on epoch=164
03/13/2022 01:08:46 - INFO - __main__ - Step 340 Global step 340 Train loss 1.092754 on epoch=169
03/13/2022 01:08:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.179940 on epoch=174
03/13/2022 01:08:52 - INFO - __main__ - Global step 350 Train loss 1.154136 EM 0.0 on epoch=174
03/13/2022 01:08:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.986948 on epoch=179
03/13/2022 01:09:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.983099 on epoch=184
03/13/2022 01:09:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.942701 on epoch=189
03/13/2022 01:09:12 - INFO - __main__ - Step 390 Global step 390 Train loss 1.058988 on epoch=194
03/13/2022 01:09:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.874254 on epoch=199
03/13/2022 01:09:18 - INFO - __main__ - Global step 400 Train loss 0.969198 EM 0.0 on epoch=199
03/13/2022 01:09:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.921748 on epoch=204
03/13/2022 01:09:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.864411 on epoch=209
03/13/2022 01:09:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.833435 on epoch=214
03/13/2022 01:09:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.903896 on epoch=219
03/13/2022 01:09:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.861272 on epoch=224
03/13/2022 01:09:44 - INFO - __main__ - Global step 450 Train loss 0.876952 EM 0.0 on epoch=224
03/13/2022 01:09:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.802067 on epoch=229
03/13/2022 01:09:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.843258 on epoch=234
03/13/2022 01:09:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.823628 on epoch=239
03/13/2022 01:10:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.826383 on epoch=244
03/13/2022 01:10:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.748027 on epoch=249
03/13/2022 01:10:11 - INFO - __main__ - Global step 500 Train loss 0.808673 EM 0.0 on epoch=249
03/13/2022 01:10:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.742139 on epoch=254
03/13/2022 01:10:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.804473 on epoch=259
03/13/2022 01:10:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.685178 on epoch=264
03/13/2022 01:10:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.802336 on epoch=269
03/13/2022 01:10:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.826697 on epoch=274
03/13/2022 01:10:37 - INFO - __main__ - Global step 550 Train loss 0.772164 EM 0.0 on epoch=274
03/13/2022 01:10:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.716443 on epoch=279
03/13/2022 01:10:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.736378 on epoch=284
03/13/2022 01:10:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.770915 on epoch=289
03/13/2022 01:10:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.690166 on epoch=294
03/13/2022 01:11:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.692725 on epoch=299
03/13/2022 01:11:03 - INFO - __main__ - Global step 600 Train loss 0.721325 EM 0.0 on epoch=299
03/13/2022 01:11:03 - INFO - __main__ - save last model!
03/13/2022 01:11:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:11:03 - INFO - __main__ - Printing 3 examples
03/13/2022 01:11:03 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/13/2022 01:11:03 - INFO - __main__ - ['camille saint-saens']
03/13/2022 01:11:03 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/13/2022 01:11:03 - INFO - __main__ - ['madness']
03/13/2022 01:11:03 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/13/2022 01:11:03 - INFO - __main__ - ['genevieve']
03/13/2022 01:11:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 01:11:03 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:11:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 01:11:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:11:03 - INFO - __main__ - Printing 3 examples
03/13/2022 01:11:03 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/13/2022 01:11:03 - INFO - __main__ - ['will hay']
03/13/2022 01:11:03 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/13/2022 01:11:03 - INFO - __main__ - ['alan sugar']
03/13/2022 01:11:03 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/13/2022 01:11:03 - INFO - __main__ - ['cleopatra']
03/13/2022 01:11:03 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:11:03 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:11:03 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 01:11:10 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 01:11:10 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 01:11:10 - INFO - __main__ - Printing 3 examples
03/13/2022 01:11:10 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 01:11:10 - INFO - __main__ - ['taming of the shrew']
03/13/2022 01:11:10 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 01:11:10 - INFO - __main__ - ['henry fonda']
03/13/2022 01:11:10 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 01:11:10 - INFO - __main__ - ['tchaikovsky']
03/13/2022 01:11:10 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:11:12 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:11:16 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 01:11:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 01:11:16 - INFO - __main__ - Starting training!
03/13/2022 01:12:36 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_21_0.0005_8_predictions.txt
03/13/2022 01:12:36 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 01:12:36 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 01:12:36 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.0003, bsz=8 ...
03/13/2022 01:12:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:12:37 - INFO - __main__ - Printing 3 examples
03/13/2022 01:12:37 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/13/2022 01:12:37 - INFO - __main__ - ['camille saint-saens']
03/13/2022 01:12:37 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/13/2022 01:12:37 - INFO - __main__ - ['madness']
03/13/2022 01:12:37 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/13/2022 01:12:37 - INFO - __main__ - ['genevieve']
03/13/2022 01:12:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 01:12:37 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:12:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 01:12:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:12:37 - INFO - __main__ - Printing 3 examples
03/13/2022 01:12:37 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/13/2022 01:12:37 - INFO - __main__ - ['will hay']
03/13/2022 01:12:37 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/13/2022 01:12:37 - INFO - __main__ - ['alan sugar']
03/13/2022 01:12:37 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/13/2022 01:12:37 - INFO - __main__ - ['cleopatra']
03/13/2022 01:12:37 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:12:37 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:12:37 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 01:12:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 01:12:48 - INFO - __main__ - Starting training!
03/13/2022 01:12:53 - INFO - __main__ - Step 10 Global step 10 Train loss 19.655201 on epoch=4
03/13/2022 01:12:58 - INFO - __main__ - Step 20 Global step 20 Train loss 17.853943 on epoch=9
03/13/2022 01:13:03 - INFO - __main__ - Step 30 Global step 30 Train loss 14.718694 on epoch=14
03/13/2022 01:13:08 - INFO - __main__ - Step 40 Global step 40 Train loss 12.468868 on epoch=19
03/13/2022 01:13:13 - INFO - __main__ - Step 50 Global step 50 Train loss 11.270912 on epoch=24
03/13/2022 01:13:14 - INFO - __main__ - Global step 50 Train loss 15.193525 EM 0.0 on epoch=24
03/13/2022 01:13:20 - INFO - __main__ - Step 60 Global step 60 Train loss 9.972532 on epoch=29
03/13/2022 01:13:25 - INFO - __main__ - Step 70 Global step 70 Train loss 9.265184 on epoch=34
03/13/2022 01:13:30 - INFO - __main__ - Step 80 Global step 80 Train loss 8.462503 on epoch=39
03/13/2022 01:13:35 - INFO - __main__ - Step 90 Global step 90 Train loss 7.980350 on epoch=44
03/13/2022 01:13:40 - INFO - __main__ - Step 100 Global step 100 Train loss 7.272382 on epoch=49
03/13/2022 01:13:40 - INFO - __main__ - Global step 100 Train loss 8.590590 EM 0.0 on epoch=49
03/13/2022 01:13:45 - INFO - __main__ - Step 110 Global step 110 Train loss 6.348921 on epoch=54
03/13/2022 01:13:50 - INFO - __main__ - Step 120 Global step 120 Train loss 5.673235 on epoch=59
03/13/2022 01:13:55 - INFO - __main__ - Step 130 Global step 130 Train loss 5.159642 on epoch=64
03/13/2022 01:14:00 - INFO - __main__ - Step 140 Global step 140 Train loss 4.639626 on epoch=69
03/13/2022 01:14:05 - INFO - __main__ - Step 150 Global step 150 Train loss 3.816756 on epoch=74
03/13/2022 01:14:06 - INFO - __main__ - Global step 150 Train loss 5.127636 EM 0.0 on epoch=74
03/13/2022 01:14:11 - INFO - __main__ - Step 160 Global step 160 Train loss 3.641432 on epoch=79
03/13/2022 01:14:16 - INFO - __main__ - Step 170 Global step 170 Train loss 3.215319 on epoch=84
03/13/2022 01:14:21 - INFO - __main__ - Step 180 Global step 180 Train loss 3.152818 on epoch=89
03/13/2022 01:14:26 - INFO - __main__ - Step 190 Global step 190 Train loss 2.479953 on epoch=94
03/13/2022 01:14:31 - INFO - __main__ - Step 200 Global step 200 Train loss 2.320014 on epoch=99
03/13/2022 01:14:32 - INFO - __main__ - Global step 200 Train loss 2.961907 EM 0.0 on epoch=99
03/13/2022 01:14:37 - INFO - __main__ - Step 210 Global step 210 Train loss 2.128895 on epoch=104
03/13/2022 01:14:42 - INFO - __main__ - Step 220 Global step 220 Train loss 2.460812 on epoch=109
03/13/2022 01:14:47 - INFO - __main__ - Step 230 Global step 230 Train loss 2.027255 on epoch=114
03/13/2022 01:14:52 - INFO - __main__ - Step 240 Global step 240 Train loss 1.868671 on epoch=119
03/13/2022 01:14:57 - INFO - __main__ - Step 250 Global step 250 Train loss 1.920715 on epoch=124
03/13/2022 01:14:58 - INFO - __main__ - Global step 250 Train loss 2.081270 EM 0.0 on epoch=124
03/13/2022 01:15:03 - INFO - __main__ - Step 260 Global step 260 Train loss 1.849281 on epoch=129
03/13/2022 01:15:08 - INFO - __main__ - Step 270 Global step 270 Train loss 1.626132 on epoch=134
03/13/2022 01:15:13 - INFO - __main__ - Step 280 Global step 280 Train loss 1.743464 on epoch=139
03/13/2022 01:15:18 - INFO - __main__ - Step 290 Global step 290 Train loss 1.823004 on epoch=144
03/13/2022 01:15:23 - INFO - __main__ - Step 300 Global step 300 Train loss 1.636462 on epoch=149
03/13/2022 01:15:24 - INFO - __main__ - Global step 300 Train loss 1.735669 EM 0.0 on epoch=149
03/13/2022 01:15:29 - INFO - __main__ - Step 310 Global step 310 Train loss 1.459415 on epoch=154
03/13/2022 01:15:34 - INFO - __main__ - Step 320 Global step 320 Train loss 1.290205 on epoch=159
03/13/2022 01:15:39 - INFO - __main__ - Step 330 Global step 330 Train loss 1.429314 on epoch=164
03/13/2022 01:15:44 - INFO - __main__ - Step 340 Global step 340 Train loss 1.632338 on epoch=169
03/13/2022 01:15:49 - INFO - __main__ - Step 350 Global step 350 Train loss 1.513426 on epoch=174
03/13/2022 01:15:50 - INFO - __main__ - Global step 350 Train loss 1.464940 EM 0.0 on epoch=174
03/13/2022 01:15:55 - INFO - __main__ - Step 360 Global step 360 Train loss 1.439872 on epoch=179
03/13/2022 01:16:00 - INFO - __main__ - Step 370 Global step 370 Train loss 1.522362 on epoch=184
03/13/2022 01:16:05 - INFO - __main__ - Step 380 Global step 380 Train loss 1.199501 on epoch=189
03/13/2022 01:16:10 - INFO - __main__ - Step 390 Global step 390 Train loss 1.240397 on epoch=194
03/13/2022 01:16:15 - INFO - __main__ - Step 400 Global step 400 Train loss 1.416012 on epoch=199
03/13/2022 01:16:16 - INFO - __main__ - Global step 400 Train loss 1.363629 EM 0.0 on epoch=199
03/13/2022 01:16:21 - INFO - __main__ - Step 410 Global step 410 Train loss 1.373925 on epoch=204
03/13/2022 01:16:26 - INFO - __main__ - Step 420 Global step 420 Train loss 1.394631 on epoch=209
03/13/2022 01:16:31 - INFO - __main__ - Step 430 Global step 430 Train loss 1.202290 on epoch=214
03/13/2022 01:16:36 - INFO - __main__ - Step 440 Global step 440 Train loss 1.250742 on epoch=219
03/13/2022 01:16:41 - INFO - __main__ - Step 450 Global step 450 Train loss 1.179081 on epoch=224
03/13/2022 01:16:42 - INFO - __main__ - Global step 450 Train loss 1.280134 EM 0.0 on epoch=224
03/13/2022 01:16:47 - INFO - __main__ - Step 460 Global step 460 Train loss 1.092829 on epoch=229
03/13/2022 01:16:52 - INFO - __main__ - Step 470 Global step 470 Train loss 1.182397 on epoch=234
03/13/2022 01:16:57 - INFO - __main__ - Step 480 Global step 480 Train loss 1.115148 on epoch=239
03/13/2022 01:17:02 - INFO - __main__ - Step 490 Global step 490 Train loss 1.092874 on epoch=244
03/13/2022 01:17:07 - INFO - __main__ - Step 500 Global step 500 Train loss 1.269242 on epoch=249
03/13/2022 01:17:08 - INFO - __main__ - Global step 500 Train loss 1.150498 EM 0.0 on epoch=249
03/13/2022 01:17:13 - INFO - __main__ - Step 510 Global step 510 Train loss 1.222210 on epoch=254
03/13/2022 01:17:18 - INFO - __main__ - Step 520 Global step 520 Train loss 1.025208 on epoch=259
03/13/2022 01:17:23 - INFO - __main__ - Step 530 Global step 530 Train loss 1.054811 on epoch=264
03/13/2022 01:17:28 - INFO - __main__ - Step 540 Global step 540 Train loss 1.088992 on epoch=269
03/13/2022 01:17:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.944064 on epoch=274
03/13/2022 01:17:34 - INFO - __main__ - Global step 550 Train loss 1.067057 EM 0.0 on epoch=274
03/13/2022 01:17:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.981262 on epoch=279
03/13/2022 01:17:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.892213 on epoch=284
03/13/2022 01:17:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.899400 on epoch=289
03/13/2022 01:17:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.886871 on epoch=294
03/13/2022 01:17:59 - INFO - __main__ - Step 600 Global step 600 Train loss 1.011085 on epoch=299
03/13/2022 01:18:00 - INFO - __main__ - Global step 600 Train loss 0.934166 EM 0.0 on epoch=299
03/13/2022 01:18:00 - INFO - __main__ - save last model!
03/13/2022 01:18:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:18:00 - INFO - __main__ - Printing 3 examples
03/13/2022 01:18:00 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/13/2022 01:18:00 - INFO - __main__ - ['camille saint-saens']
03/13/2022 01:18:00 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/13/2022 01:18:00 - INFO - __main__ - ['madness']
03/13/2022 01:18:00 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/13/2022 01:18:00 - INFO - __main__ - ['genevieve']
03/13/2022 01:18:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 01:18:00 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:18:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 01:18:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:18:00 - INFO - __main__ - Printing 3 examples
03/13/2022 01:18:00 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/13/2022 01:18:00 - INFO - __main__ - ['will hay']
03/13/2022 01:18:00 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/13/2022 01:18:00 - INFO - __main__ - ['alan sugar']
03/13/2022 01:18:00 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/13/2022 01:18:00 - INFO - __main__ - ['cleopatra']
03/13/2022 01:18:00 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:18:00 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:18:00 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 01:18:07 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 01:18:07 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 01:18:07 - INFO - __main__ - Printing 3 examples
03/13/2022 01:18:07 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 01:18:07 - INFO - __main__ - ['taming of the shrew']
03/13/2022 01:18:07 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 01:18:07 - INFO - __main__ - ['henry fonda']
03/13/2022 01:18:07 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 01:18:07 - INFO - __main__ - ['tchaikovsky']
03/13/2022 01:18:07 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:18:09 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:18:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 01:18:13 - INFO - __main__ - Starting training!
03/13/2022 01:18:13 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 01:19:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_21_0.0003_8_predictions.txt
03/13/2022 01:19:38 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 01:19:39 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 01:19:39 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.0002, bsz=8 ...
03/13/2022 01:19:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:19:40 - INFO - __main__ - Printing 3 examples
03/13/2022 01:19:40 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/13/2022 01:19:40 - INFO - __main__ - ['camille saint-saens']
03/13/2022 01:19:40 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/13/2022 01:19:40 - INFO - __main__ - ['madness']
03/13/2022 01:19:40 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/13/2022 01:19:40 - INFO - __main__ - ['genevieve']
03/13/2022 01:19:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 01:19:40 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:19:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 01:19:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:19:40 - INFO - __main__ - Printing 3 examples
03/13/2022 01:19:40 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/13/2022 01:19:40 - INFO - __main__ - ['will hay']
03/13/2022 01:19:40 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/13/2022 01:19:40 - INFO - __main__ - ['alan sugar']
03/13/2022 01:19:40 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/13/2022 01:19:40 - INFO - __main__ - ['cleopatra']
03/13/2022 01:19:40 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:19:40 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:19:40 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 01:19:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 01:19:50 - INFO - __main__ - Starting training!
03/13/2022 01:19:55 - INFO - __main__ - Step 10 Global step 10 Train loss 19.526102 on epoch=4
03/13/2022 01:20:00 - INFO - __main__ - Step 20 Global step 20 Train loss 18.889338 on epoch=9
03/13/2022 01:20:05 - INFO - __main__ - Step 30 Global step 30 Train loss 15.641960 on epoch=14
03/13/2022 01:20:10 - INFO - __main__ - Step 40 Global step 40 Train loss 14.764400 on epoch=19
03/13/2022 01:20:15 - INFO - __main__ - Step 50 Global step 50 Train loss 13.711973 on epoch=24
03/13/2022 01:20:18 - INFO - __main__ - Global step 50 Train loss 16.506754 EM 0.0 on epoch=24
03/13/2022 01:20:24 - INFO - __main__ - Step 60 Global step 60 Train loss 11.876743 on epoch=29
03/13/2022 01:20:29 - INFO - __main__ - Step 70 Global step 70 Train loss 11.277519 on epoch=34
03/13/2022 01:20:34 - INFO - __main__ - Step 80 Global step 80 Train loss 10.478947 on epoch=39
03/13/2022 01:20:39 - INFO - __main__ - Step 90 Global step 90 Train loss 10.075795 on epoch=44
03/13/2022 01:20:44 - INFO - __main__ - Step 100 Global step 100 Train loss 9.002867 on epoch=49
03/13/2022 01:20:46 - INFO - __main__ - Global step 100 Train loss 10.542375 EM 0.0 on epoch=49
03/13/2022 01:20:51 - INFO - __main__ - Step 110 Global step 110 Train loss 8.144882 on epoch=54
03/13/2022 01:20:55 - INFO - __main__ - Step 120 Global step 120 Train loss 8.099684 on epoch=59
03/13/2022 01:21:00 - INFO - __main__ - Step 130 Global step 130 Train loss 7.274068 on epoch=64
03/13/2022 01:21:05 - INFO - __main__ - Step 140 Global step 140 Train loss 7.091673 on epoch=69
03/13/2022 01:21:10 - INFO - __main__ - Step 150 Global step 150 Train loss 6.269828 on epoch=74
03/13/2022 01:21:12 - INFO - __main__ - Global step 150 Train loss 7.376028 EM 0.0 on epoch=74
03/13/2022 01:21:17 - INFO - __main__ - Step 160 Global step 160 Train loss 6.058333 on epoch=79
03/13/2022 01:21:22 - INFO - __main__ - Step 170 Global step 170 Train loss 5.430897 on epoch=84
03/13/2022 01:21:27 - INFO - __main__ - Step 180 Global step 180 Train loss 4.945943 on epoch=89
03/13/2022 01:21:32 - INFO - __main__ - Step 190 Global step 190 Train loss 4.395987 on epoch=94
03/13/2022 01:21:37 - INFO - __main__ - Step 200 Global step 200 Train loss 4.543854 on epoch=99
03/13/2022 01:21:37 - INFO - __main__ - Global step 200 Train loss 5.075003 EM 0.0 on epoch=99
03/13/2022 01:21:42 - INFO - __main__ - Step 210 Global step 210 Train loss 4.058520 on epoch=104
03/13/2022 01:21:47 - INFO - __main__ - Step 220 Global step 220 Train loss 3.817075 on epoch=109
03/13/2022 01:21:52 - INFO - __main__ - Step 230 Global step 230 Train loss 3.416797 on epoch=114
03/13/2022 01:21:58 - INFO - __main__ - Step 240 Global step 240 Train loss 3.259975 on epoch=119
03/13/2022 01:22:03 - INFO - __main__ - Step 250 Global step 250 Train loss 2.930606 on epoch=124
03/13/2022 01:22:03 - INFO - __main__ - Global step 250 Train loss 3.496595 EM 0.0 on epoch=124
03/13/2022 01:22:08 - INFO - __main__ - Step 260 Global step 260 Train loss 2.841259 on epoch=129
03/13/2022 01:22:13 - INFO - __main__ - Step 270 Global step 270 Train loss 2.588202 on epoch=134
03/13/2022 01:22:18 - INFO - __main__ - Step 280 Global step 280 Train loss 2.547483 on epoch=139
03/13/2022 01:22:23 - INFO - __main__ - Step 290 Global step 290 Train loss 2.546774 on epoch=144
03/13/2022 01:22:28 - INFO - __main__ - Step 300 Global step 300 Train loss 2.121498 on epoch=149
03/13/2022 01:22:29 - INFO - __main__ - Global step 300 Train loss 2.529043 EM 0.0 on epoch=149
03/13/2022 01:22:34 - INFO - __main__ - Step 310 Global step 310 Train loss 2.260320 on epoch=154
03/13/2022 01:22:39 - INFO - __main__ - Step 320 Global step 320 Train loss 2.224599 on epoch=159
03/13/2022 01:22:44 - INFO - __main__ - Step 330 Global step 330 Train loss 2.138256 on epoch=164
03/13/2022 01:22:49 - INFO - __main__ - Step 340 Global step 340 Train loss 1.864120 on epoch=169
03/13/2022 01:22:54 - INFO - __main__ - Step 350 Global step 350 Train loss 1.965628 on epoch=174
03/13/2022 01:22:55 - INFO - __main__ - Global step 350 Train loss 2.090585 EM 0.0 on epoch=174
03/13/2022 01:23:00 - INFO - __main__ - Step 360 Global step 360 Train loss 2.012365 on epoch=179
03/13/2022 01:23:05 - INFO - __main__ - Step 370 Global step 370 Train loss 1.799160 on epoch=184
03/13/2022 01:23:10 - INFO - __main__ - Step 380 Global step 380 Train loss 2.007612 on epoch=189
03/13/2022 01:23:15 - INFO - __main__ - Step 390 Global step 390 Train loss 1.996671 on epoch=194
03/13/2022 01:23:20 - INFO - __main__ - Step 400 Global step 400 Train loss 1.911930 on epoch=199
03/13/2022 01:23:21 - INFO - __main__ - Global step 400 Train loss 1.945547 EM 0.0 on epoch=199
03/13/2022 01:23:26 - INFO - __main__ - Step 410 Global step 410 Train loss 1.884159 on epoch=204
03/13/2022 01:23:31 - INFO - __main__ - Step 420 Global step 420 Train loss 1.773238 on epoch=209
03/13/2022 01:23:36 - INFO - __main__ - Step 430 Global step 430 Train loss 1.513984 on epoch=214
03/13/2022 01:23:41 - INFO - __main__ - Step 440 Global step 440 Train loss 1.528648 on epoch=219
03/13/2022 01:23:46 - INFO - __main__ - Step 450 Global step 450 Train loss 1.839641 on epoch=224
03/13/2022 01:23:47 - INFO - __main__ - Global step 450 Train loss 1.707934 EM 0.0 on epoch=224
03/13/2022 01:23:52 - INFO - __main__ - Step 460 Global step 460 Train loss 1.483054 on epoch=229
03/13/2022 01:23:57 - INFO - __main__ - Step 470 Global step 470 Train loss 1.404210 on epoch=234
03/13/2022 01:24:02 - INFO - __main__ - Step 480 Global step 480 Train loss 1.695025 on epoch=239
03/13/2022 01:24:07 - INFO - __main__ - Step 490 Global step 490 Train loss 1.584065 on epoch=244
03/13/2022 01:24:12 - INFO - __main__ - Step 500 Global step 500 Train loss 1.617273 on epoch=249
03/13/2022 01:24:13 - INFO - __main__ - Global step 500 Train loss 1.556726 EM 0.0 on epoch=249
03/13/2022 01:24:18 - INFO - __main__ - Step 510 Global step 510 Train loss 1.483185 on epoch=254
03/13/2022 01:24:23 - INFO - __main__ - Step 520 Global step 520 Train loss 1.496403 on epoch=259
03/13/2022 01:24:28 - INFO - __main__ - Step 530 Global step 530 Train loss 1.540321 on epoch=264
03/13/2022 01:24:33 - INFO - __main__ - Step 540 Global step 540 Train loss 1.302649 on epoch=269
03/13/2022 01:24:38 - INFO - __main__ - Step 550 Global step 550 Train loss 1.380115 on epoch=274
03/13/2022 01:24:38 - INFO - __main__ - Global step 550 Train loss 1.440535 EM 0.0 on epoch=274
03/13/2022 01:24:44 - INFO - __main__ - Step 560 Global step 560 Train loss 1.190155 on epoch=279
03/13/2022 01:24:49 - INFO - __main__ - Step 570 Global step 570 Train loss 1.397454 on epoch=284
03/13/2022 01:24:54 - INFO - __main__ - Step 580 Global step 580 Train loss 1.402167 on epoch=289
03/13/2022 01:24:59 - INFO - __main__ - Step 590 Global step 590 Train loss 1.190253 on epoch=294
03/13/2022 01:25:04 - INFO - __main__ - Step 600 Global step 600 Train loss 1.364835 on epoch=299
03/13/2022 01:25:05 - INFO - __main__ - Global step 600 Train loss 1.308973 EM 0.0 on epoch=299
03/13/2022 01:25:05 - INFO - __main__ - save last model!
03/13/2022 01:25:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:25:05 - INFO - __main__ - Printing 3 examples
03/13/2022 01:25:05 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/13/2022 01:25:05 - INFO - __main__ - ['camille saint-saens']
03/13/2022 01:25:05 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/13/2022 01:25:05 - INFO - __main__ - ['madness']
03/13/2022 01:25:05 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/13/2022 01:25:05 - INFO - __main__ - ['genevieve']
03/13/2022 01:25:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 01:25:05 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:25:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 01:25:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:25:05 - INFO - __main__ - Printing 3 examples
03/13/2022 01:25:05 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/13/2022 01:25:05 - INFO - __main__ - ['will hay']
03/13/2022 01:25:05 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/13/2022 01:25:05 - INFO - __main__ - ['alan sugar']
03/13/2022 01:25:05 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/13/2022 01:25:05 - INFO - __main__ - ['cleopatra']
03/13/2022 01:25:05 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:25:05 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:25:05 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 01:25:12 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 01:25:12 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 01:25:12 - INFO - __main__ - Printing 3 examples
03/13/2022 01:25:12 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 01:25:12 - INFO - __main__ - ['taming of the shrew']
03/13/2022 01:25:12 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 01:25:12 - INFO - __main__ - ['henry fonda']
03/13/2022 01:25:12 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 01:25:12 - INFO - __main__ - ['tchaikovsky']
03/13/2022 01:25:12 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:25:14 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:25:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 01:25:16 - INFO - __main__ - Starting training!
03/13/2022 01:25:18 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 01:33:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_21_0.0002_8_predictions.txt
03/13/2022 01:33:32 - INFO - __main__ - EM on test data: 0.0003
03/13/2022 01:33:33 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.0002503755633450175
03/13/2022 01:33:33 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.0001, bsz=8 ...
03/13/2022 01:33:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:33:34 - INFO - __main__ - Printing 3 examples
03/13/2022 01:33:34 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/13/2022 01:33:34 - INFO - __main__ - ['camille saint-saens']
03/13/2022 01:33:34 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/13/2022 01:33:34 - INFO - __main__ - ['madness']
03/13/2022 01:33:34 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/13/2022 01:33:34 - INFO - __main__ - ['genevieve']
03/13/2022 01:33:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 01:33:34 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:33:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 01:33:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:33:34 - INFO - __main__ - Printing 3 examples
03/13/2022 01:33:34 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/13/2022 01:33:34 - INFO - __main__ - ['will hay']
03/13/2022 01:33:34 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/13/2022 01:33:34 - INFO - __main__ - ['alan sugar']
03/13/2022 01:33:34 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/13/2022 01:33:34 - INFO - __main__ - ['cleopatra']
03/13/2022 01:33:34 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:33:34 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:33:34 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 01:33:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 01:33:46 - INFO - __main__ - Starting training!
03/13/2022 01:33:51 - INFO - __main__ - Step 10 Global step 10 Train loss 20.266497 on epoch=4
03/13/2022 01:33:56 - INFO - __main__ - Step 20 Global step 20 Train loss 18.616344 on epoch=9
03/13/2022 01:34:01 - INFO - __main__ - Step 30 Global step 30 Train loss 17.245705 on epoch=14
03/13/2022 01:34:06 - INFO - __main__ - Step 40 Global step 40 Train loss 16.211716 on epoch=19
03/13/2022 01:34:11 - INFO - __main__ - Step 50 Global step 50 Train loss 14.868818 on epoch=24
03/13/2022 01:34:24 - INFO - __main__ - Global step 50 Train loss 17.441816 EM 0.0 on epoch=24
03/13/2022 01:34:30 - INFO - __main__ - Step 60 Global step 60 Train loss 14.030818 on epoch=29
03/13/2022 01:34:35 - INFO - __main__ - Step 70 Global step 70 Train loss 13.988269 on epoch=34
03/13/2022 01:34:40 - INFO - __main__ - Step 80 Global step 80 Train loss 13.414348 on epoch=39
03/13/2022 01:34:45 - INFO - __main__ - Step 90 Global step 90 Train loss 13.053638 on epoch=44
03/13/2022 01:34:50 - INFO - __main__ - Step 100 Global step 100 Train loss 12.191587 on epoch=49
03/13/2022 01:34:56 - INFO - __main__ - Global step 100 Train loss 13.335732 EM 0.0 on epoch=49
03/13/2022 01:35:01 - INFO - __main__ - Step 110 Global step 110 Train loss 11.972694 on epoch=54
03/13/2022 01:35:07 - INFO - __main__ - Step 120 Global step 120 Train loss 11.469126 on epoch=59
03/13/2022 01:35:12 - INFO - __main__ - Step 130 Global step 130 Train loss 10.782742 on epoch=64
03/13/2022 01:35:17 - INFO - __main__ - Step 140 Global step 140 Train loss 10.812144 on epoch=69
03/13/2022 01:35:22 - INFO - __main__ - Step 150 Global step 150 Train loss 10.022036 on epoch=74
03/13/2022 01:35:23 - INFO - __main__ - Global step 150 Train loss 11.011749 EM 0.0 on epoch=74
03/13/2022 01:35:28 - INFO - __main__ - Step 160 Global step 160 Train loss 10.007286 on epoch=79
03/13/2022 01:35:33 - INFO - __main__ - Step 170 Global step 170 Train loss 9.488407 on epoch=84
03/13/2022 01:35:38 - INFO - __main__ - Step 180 Global step 180 Train loss 9.185724 on epoch=89
03/13/2022 01:35:43 - INFO - __main__ - Step 190 Global step 190 Train loss 8.497015 on epoch=94
03/13/2022 01:35:49 - INFO - __main__ - Step 200 Global step 200 Train loss 8.175021 on epoch=99
03/13/2022 01:35:49 - INFO - __main__ - Global step 200 Train loss 9.070691 EM 0.0 on epoch=99
03/13/2022 01:35:54 - INFO - __main__ - Step 210 Global step 210 Train loss 8.010062 on epoch=104
03/13/2022 01:35:59 - INFO - __main__ - Step 220 Global step 220 Train loss 7.760756 on epoch=109
03/13/2022 01:36:04 - INFO - __main__ - Step 230 Global step 230 Train loss 7.467746 on epoch=114
03/13/2022 01:36:10 - INFO - __main__ - Step 240 Global step 240 Train loss 6.783430 on epoch=119
03/13/2022 01:36:15 - INFO - __main__ - Step 250 Global step 250 Train loss 6.676541 on epoch=124
03/13/2022 01:36:15 - INFO - __main__ - Global step 250 Train loss 7.339707 EM 0.0 on epoch=124
03/13/2022 01:36:20 - INFO - __main__ - Step 260 Global step 260 Train loss 6.671401 on epoch=129
03/13/2022 01:36:26 - INFO - __main__ - Step 270 Global step 270 Train loss 5.909028 on epoch=134
03/13/2022 01:36:31 - INFO - __main__ - Step 280 Global step 280 Train loss 6.109922 on epoch=139
03/13/2022 01:36:36 - INFO - __main__ - Step 290 Global step 290 Train loss 5.488276 on epoch=144
03/13/2022 01:36:41 - INFO - __main__ - Step 300 Global step 300 Train loss 5.318166 on epoch=149
03/13/2022 01:36:42 - INFO - __main__ - Global step 300 Train loss 5.899359 EM 0.0 on epoch=149
03/13/2022 01:36:47 - INFO - __main__ - Step 310 Global step 310 Train loss 5.132070 on epoch=154
03/13/2022 01:36:52 - INFO - __main__ - Step 320 Global step 320 Train loss 5.139691 on epoch=159
03/13/2022 01:36:57 - INFO - __main__ - Step 330 Global step 330 Train loss 4.925983 on epoch=164
03/13/2022 01:37:02 - INFO - __main__ - Step 340 Global step 340 Train loss 4.725871 on epoch=169
03/13/2022 01:37:08 - INFO - __main__ - Step 350 Global step 350 Train loss 4.678073 on epoch=174
03/13/2022 01:37:09 - INFO - __main__ - Global step 350 Train loss 4.920338 EM 0.0 on epoch=174
03/13/2022 01:37:14 - INFO - __main__ - Step 360 Global step 360 Train loss 4.346572 on epoch=179
03/13/2022 01:37:19 - INFO - __main__ - Step 370 Global step 370 Train loss 4.244716 on epoch=184
03/13/2022 01:37:24 - INFO - __main__ - Step 380 Global step 380 Train loss 4.041280 on epoch=189
03/13/2022 01:37:29 - INFO - __main__ - Step 390 Global step 390 Train loss 4.327994 on epoch=194
03/13/2022 01:37:34 - INFO - __main__ - Step 400 Global step 400 Train loss 3.682076 on epoch=199
03/13/2022 01:37:35 - INFO - __main__ - Global step 400 Train loss 4.128527 EM 0.0 on epoch=199
03/13/2022 01:37:40 - INFO - __main__ - Step 410 Global step 410 Train loss 3.761605 on epoch=204
03/13/2022 01:37:46 - INFO - __main__ - Step 420 Global step 420 Train loss 3.864433 on epoch=209
03/13/2022 01:37:51 - INFO - __main__ - Step 430 Global step 430 Train loss 3.438730 on epoch=214
03/13/2022 01:37:56 - INFO - __main__ - Step 440 Global step 440 Train loss 3.494233 on epoch=219
03/13/2022 01:38:01 - INFO - __main__ - Step 450 Global step 450 Train loss 2.935497 on epoch=224
03/13/2022 01:38:02 - INFO - __main__ - Global step 450 Train loss 3.498899 EM 0.0 on epoch=224
03/13/2022 01:38:07 - INFO - __main__ - Step 460 Global step 460 Train loss 3.193548 on epoch=229
03/13/2022 01:38:12 - INFO - __main__ - Step 470 Global step 470 Train loss 2.915692 on epoch=234
03/13/2022 01:38:17 - INFO - __main__ - Step 480 Global step 480 Train loss 3.142198 on epoch=239
03/13/2022 01:38:22 - INFO - __main__ - Step 490 Global step 490 Train loss 3.246109 on epoch=244
03/13/2022 01:38:28 - INFO - __main__ - Step 500 Global step 500 Train loss 2.769158 on epoch=249
03/13/2022 01:38:29 - INFO - __main__ - Global step 500 Train loss 3.053341 EM 0.0 on epoch=249
03/13/2022 01:38:34 - INFO - __main__ - Step 510 Global step 510 Train loss 2.977230 on epoch=254
03/13/2022 01:38:39 - INFO - __main__ - Step 520 Global step 520 Train loss 2.459240 on epoch=259
03/13/2022 01:38:44 - INFO - __main__ - Step 530 Global step 530 Train loss 2.668704 on epoch=264
03/13/2022 01:38:49 - INFO - __main__ - Step 540 Global step 540 Train loss 2.427141 on epoch=269
03/13/2022 01:38:54 - INFO - __main__ - Step 550 Global step 550 Train loss 2.268680 on epoch=274
03/13/2022 01:38:55 - INFO - __main__ - Global step 550 Train loss 2.560199 EM 0.0 on epoch=274
03/13/2022 01:39:00 - INFO - __main__ - Step 560 Global step 560 Train loss 2.558067 on epoch=279
03/13/2022 01:39:06 - INFO - __main__ - Step 570 Global step 570 Train loss 2.762139 on epoch=284
03/13/2022 01:39:11 - INFO - __main__ - Step 580 Global step 580 Train loss 2.281045 on epoch=289
03/13/2022 01:39:16 - INFO - __main__ - Step 590 Global step 590 Train loss 2.231143 on epoch=294
03/13/2022 01:39:21 - INFO - __main__ - Step 600 Global step 600 Train loss 2.357468 on epoch=299
03/13/2022 01:39:22 - INFO - __main__ - Global step 600 Train loss 2.437973 EM 0.0 on epoch=299
03/13/2022 01:39:22 - INFO - __main__ - save last model!
03/13/2022 01:39:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:39:22 - INFO - __main__ - Printing 3 examples
03/13/2022 01:39:22 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/13/2022 01:39:22 - INFO - __main__ - ['francois mitterrand']
03/13/2022 01:39:22 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/13/2022 01:39:22 - INFO - __main__ - ['james callaghan']
03/13/2022 01:39:22 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/13/2022 01:39:22 - INFO - __main__ - ['aberdeen']
03/13/2022 01:39:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 01:39:22 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:39:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 01:39:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:39:22 - INFO - __main__ - Printing 3 examples
03/13/2022 01:39:22 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/13/2022 01:39:22 - INFO - __main__ - ['tulisa']
03/13/2022 01:39:22 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/13/2022 01:39:22 - INFO - __main__ - ['calgary']
03/13/2022 01:39:22 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/13/2022 01:39:22 - INFO - __main__ - ['jeff bridges']
03/13/2022 01:39:22 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:39:22 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:39:22 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 01:39:28 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 01:39:29 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 01:39:29 - INFO - __main__ - Printing 3 examples
03/13/2022 01:39:29 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 01:39:29 - INFO - __main__ - ['taming of the shrew']
03/13/2022 01:39:29 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 01:39:29 - INFO - __main__ - ['henry fonda']
03/13/2022 01:39:29 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 01:39:29 - INFO - __main__ - ['tchaikovsky']
03/13/2022 01:39:29 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:39:31 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:39:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 01:39:33 - INFO - __main__ - Starting training!
03/13/2022 01:39:35 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 01:59:00 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_21_0.0001_8_predictions.txt
03/13/2022 01:59:00 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 01:59:01 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 01:59:01 - INFO - __main__ - Running ... prefix=freebase_qa_32_42, lr=0.0005, bsz=8 ...
03/13/2022 01:59:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:59:02 - INFO - __main__ - Printing 3 examples
03/13/2022 01:59:02 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/13/2022 01:59:02 - INFO - __main__ - ['francois mitterrand']
03/13/2022 01:59:02 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/13/2022 01:59:02 - INFO - __main__ - ['james callaghan']
03/13/2022 01:59:02 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/13/2022 01:59:02 - INFO - __main__ - ['aberdeen']
03/13/2022 01:59:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 01:59:02 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:59:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 01:59:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 01:59:02 - INFO - __main__ - Printing 3 examples
03/13/2022 01:59:02 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/13/2022 01:59:02 - INFO - __main__ - ['tulisa']
03/13/2022 01:59:02 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/13/2022 01:59:02 - INFO - __main__ - ['calgary']
03/13/2022 01:59:02 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/13/2022 01:59:02 - INFO - __main__ - ['jeff bridges']
03/13/2022 01:59:02 - INFO - __main__ - Tokenizing Input ...
03/13/2022 01:59:02 - INFO - __main__ - Tokenizing Output ...
03/13/2022 01:59:02 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 01:59:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 01:59:12 - INFO - __main__ - Starting training!
03/13/2022 01:59:17 - INFO - __main__ - Step 10 Global step 10 Train loss 20.202301 on epoch=4
03/13/2022 01:59:21 - INFO - __main__ - Step 20 Global step 20 Train loss 16.108776 on epoch=9
03/13/2022 01:59:26 - INFO - __main__ - Step 30 Global step 30 Train loss 11.919593 on epoch=14
03/13/2022 01:59:31 - INFO - __main__ - Step 40 Global step 40 Train loss 10.260761 on epoch=19
03/13/2022 01:59:36 - INFO - __main__ - Step 50 Global step 50 Train loss 9.072268 on epoch=24
03/13/2022 01:59:36 - INFO - __main__ - Global step 50 Train loss 13.512740 EM 0.0 on epoch=24
03/13/2022 01:59:42 - INFO - __main__ - Step 60 Global step 60 Train loss 7.868929 on epoch=29
03/13/2022 01:59:47 - INFO - __main__ - Step 70 Global step 70 Train loss 6.531641 on epoch=34
03/13/2022 01:59:52 - INFO - __main__ - Step 80 Global step 80 Train loss 5.681827 on epoch=39
03/13/2022 01:59:57 - INFO - __main__ - Step 90 Global step 90 Train loss 4.810251 on epoch=44
03/13/2022 02:00:02 - INFO - __main__ - Step 100 Global step 100 Train loss 3.614228 on epoch=49
03/13/2022 02:00:03 - INFO - __main__ - Global step 100 Train loss 5.701375 EM 0.0 on epoch=49
03/13/2022 02:00:08 - INFO - __main__ - Step 110 Global step 110 Train loss 3.030007 on epoch=54
03/13/2022 02:00:13 - INFO - __main__ - Step 120 Global step 120 Train loss 2.389507 on epoch=59
03/13/2022 02:00:18 - INFO - __main__ - Step 130 Global step 130 Train loss 1.998393 on epoch=64
03/13/2022 02:00:23 - INFO - __main__ - Step 140 Global step 140 Train loss 1.919999 on epoch=69
03/13/2022 02:00:29 - INFO - __main__ - Step 150 Global step 150 Train loss 1.965299 on epoch=74
03/13/2022 02:00:29 - INFO - __main__ - Global step 150 Train loss 2.260641 EM 0.0 on epoch=74
03/13/2022 02:00:34 - INFO - __main__ - Step 160 Global step 160 Train loss 1.943109 on epoch=79
03/13/2022 02:00:40 - INFO - __main__ - Step 170 Global step 170 Train loss 1.639971 on epoch=84
03/13/2022 02:00:45 - INFO - __main__ - Step 180 Global step 180 Train loss 1.311680 on epoch=89
03/13/2022 02:00:50 - INFO - __main__ - Step 190 Global step 190 Train loss 1.339939 on epoch=94
03/13/2022 02:00:55 - INFO - __main__ - Step 200 Global step 200 Train loss 1.312414 on epoch=99
03/13/2022 02:00:56 - INFO - __main__ - Global step 200 Train loss 1.509423 EM 0.0 on epoch=99
03/13/2022 02:01:01 - INFO - __main__ - Step 210 Global step 210 Train loss 1.163722 on epoch=104
03/13/2022 02:01:06 - INFO - __main__ - Step 220 Global step 220 Train loss 1.322392 on epoch=109
03/13/2022 02:01:11 - INFO - __main__ - Step 230 Global step 230 Train loss 1.000856 on epoch=114
03/13/2022 02:01:16 - INFO - __main__ - Step 240 Global step 240 Train loss 1.030453 on epoch=119
03/13/2022 02:01:21 - INFO - __main__ - Step 250 Global step 250 Train loss 2.524111 on epoch=124
03/13/2022 02:01:22 - INFO - __main__ - Global step 250 Train loss 1.408307 EM 0.0 on epoch=124
03/13/2022 02:01:27 - INFO - __main__ - Step 260 Global step 260 Train loss 1.302657 on epoch=129
03/13/2022 02:01:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.987525 on epoch=134
03/13/2022 02:01:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.888425 on epoch=139
03/13/2022 02:01:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.839193 on epoch=144
03/13/2022 02:01:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.928641 on epoch=149
03/13/2022 02:01:48 - INFO - __main__ - Global step 300 Train loss 0.989288 EM 0.0 on epoch=149
03/13/2022 02:01:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.769039 on epoch=154
03/13/2022 02:01:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.797141 on epoch=159
03/13/2022 02:02:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.816345 on epoch=164
03/13/2022 02:02:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.813819 on epoch=169
03/13/2022 02:02:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.869812 on epoch=174
03/13/2022 02:02:14 - INFO - __main__ - Global step 350 Train loss 0.813231 EM 0.0 on epoch=174
03/13/2022 02:02:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.799579 on epoch=179
03/13/2022 02:02:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.813298 on epoch=184
03/13/2022 02:02:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.726630 on epoch=189
03/13/2022 02:02:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.753712 on epoch=194
03/13/2022 02:02:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.718901 on epoch=199
03/13/2022 02:02:40 - INFO - __main__ - Global step 400 Train loss 0.762424 EM 0.0 on epoch=199
03/13/2022 02:02:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.788162 on epoch=204
03/13/2022 02:02:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.732628 on epoch=209
03/13/2022 02:02:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.763989 on epoch=214
03/13/2022 02:03:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.735523 on epoch=219
03/13/2022 02:03:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.606514 on epoch=224
03/13/2022 02:03:06 - INFO - __main__ - Global step 450 Train loss 0.725363 EM 0.0 on epoch=224
03/13/2022 02:03:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.640756 on epoch=229
03/13/2022 02:03:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.608702 on epoch=234
03/13/2022 02:03:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.588048 on epoch=239
03/13/2022 02:03:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.629669 on epoch=244
03/13/2022 02:03:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.683120 on epoch=249
03/13/2022 02:03:32 - INFO - __main__ - Global step 500 Train loss 0.630059 EM 0.0 on epoch=249
03/13/2022 02:03:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.609574 on epoch=254
03/13/2022 02:03:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.613554 on epoch=259
03/13/2022 02:03:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.604050 on epoch=264
03/13/2022 02:03:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.628212 on epoch=269
03/13/2022 02:03:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.612461 on epoch=274
03/13/2022 02:03:59 - INFO - __main__ - Global step 550 Train loss 0.613570 EM 0.0 on epoch=274
03/13/2022 02:04:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.569822 on epoch=279
03/13/2022 02:04:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.533785 on epoch=284
03/13/2022 02:04:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.551371 on epoch=289
03/13/2022 02:04:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.576424 on epoch=294
03/13/2022 02:04:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.576148 on epoch=299
03/13/2022 02:04:25 - INFO - __main__ - Global step 600 Train loss 0.561510 EM 0.0 on epoch=299
03/13/2022 02:04:25 - INFO - __main__ - save last model!
03/13/2022 02:04:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:04:25 - INFO - __main__ - Printing 3 examples
03/13/2022 02:04:25 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/13/2022 02:04:25 - INFO - __main__ - ['francois mitterrand']
03/13/2022 02:04:25 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/13/2022 02:04:25 - INFO - __main__ - ['james callaghan']
03/13/2022 02:04:25 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/13/2022 02:04:25 - INFO - __main__ - ['aberdeen']
03/13/2022 02:04:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 02:04:25 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:04:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:04:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:04:25 - INFO - __main__ - Printing 3 examples
03/13/2022 02:04:25 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/13/2022 02:04:25 - INFO - __main__ - ['tulisa']
03/13/2022 02:04:25 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/13/2022 02:04:25 - INFO - __main__ - ['calgary']
03/13/2022 02:04:25 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/13/2022 02:04:25 - INFO - __main__ - ['jeff bridges']
03/13/2022 02:04:25 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:04:25 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:04:25 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:04:31 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 02:04:32 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 02:04:32 - INFO - __main__ - Printing 3 examples
03/13/2022 02:04:32 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 02:04:32 - INFO - __main__ - ['taming of the shrew']
03/13/2022 02:04:32 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 02:04:32 - INFO - __main__ - ['henry fonda']
03/13/2022 02:04:32 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 02:04:32 - INFO - __main__ - ['tchaikovsky']
03/13/2022 02:04:32 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:04:34 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:04:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:04:36 - INFO - __main__ - Starting training!
03/13/2022 02:04:38 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 02:05:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_42_0.0005_8_predictions.txt
03/13/2022 02:05:46 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 02:05:46 - INFO - __main__ - prefix=freebase_qa_32_42, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 02:05:46 - INFO - __main__ - Running ... prefix=freebase_qa_32_42, lr=0.0003, bsz=8 ...
03/13/2022 02:05:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:05:47 - INFO - __main__ - Printing 3 examples
03/13/2022 02:05:47 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/13/2022 02:05:47 - INFO - __main__ - ['francois mitterrand']
03/13/2022 02:05:47 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/13/2022 02:05:47 - INFO - __main__ - ['james callaghan']
03/13/2022 02:05:47 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/13/2022 02:05:47 - INFO - __main__ - ['aberdeen']
03/13/2022 02:05:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 02:05:47 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:05:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:05:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:05:47 - INFO - __main__ - Printing 3 examples
03/13/2022 02:05:47 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/13/2022 02:05:47 - INFO - __main__ - ['tulisa']
03/13/2022 02:05:47 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/13/2022 02:05:47 - INFO - __main__ - ['calgary']
03/13/2022 02:05:47 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/13/2022 02:05:47 - INFO - __main__ - ['jeff bridges']
03/13/2022 02:05:47 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:05:47 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:05:47 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:06:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:06:00 - INFO - __main__ - Starting training!
03/13/2022 02:06:04 - INFO - __main__ - Step 10 Global step 10 Train loss 19.996861 on epoch=4
03/13/2022 02:06:09 - INFO - __main__ - Step 20 Global step 20 Train loss 16.629568 on epoch=9
03/13/2022 02:06:14 - INFO - __main__ - Step 30 Global step 30 Train loss 13.913488 on epoch=14
03/13/2022 02:06:19 - INFO - __main__ - Step 40 Global step 40 Train loss 12.001577 on epoch=19
03/13/2022 02:06:24 - INFO - __main__ - Step 50 Global step 50 Train loss 11.049895 on epoch=24
03/13/2022 02:06:25 - INFO - __main__ - Global step 50 Train loss 14.718278 EM 0.0 on epoch=24
03/13/2022 02:06:31 - INFO - __main__ - Step 60 Global step 60 Train loss 8.777529 on epoch=29
03/13/2022 02:06:36 - INFO - __main__ - Step 70 Global step 70 Train loss 7.830771 on epoch=34
03/13/2022 02:06:41 - INFO - __main__ - Step 80 Global step 80 Train loss 6.752294 on epoch=39
03/13/2022 02:06:46 - INFO - __main__ - Step 90 Global step 90 Train loss 5.513456 on epoch=44
03/13/2022 02:06:52 - INFO - __main__ - Step 100 Global step 100 Train loss 5.014773 on epoch=49
03/13/2022 02:06:52 - INFO - __main__ - Global step 100 Train loss 6.777764 EM 0.0 on epoch=49
03/13/2022 02:06:57 - INFO - __main__ - Step 110 Global step 110 Train loss 4.697238 on epoch=54
03/13/2022 02:07:03 - INFO - __main__ - Step 120 Global step 120 Train loss 3.895535 on epoch=59
03/13/2022 02:07:08 - INFO - __main__ - Step 130 Global step 130 Train loss 3.414906 on epoch=64
03/13/2022 02:07:13 - INFO - __main__ - Step 140 Global step 140 Train loss 3.520644 on epoch=69
03/13/2022 02:07:18 - INFO - __main__ - Step 150 Global step 150 Train loss 2.716619 on epoch=74
03/13/2022 02:07:19 - INFO - __main__ - Global step 150 Train loss 3.648988 EM 0.0 on epoch=74
03/13/2022 02:07:24 - INFO - __main__ - Step 160 Global step 160 Train loss 2.924039 on epoch=79
03/13/2022 02:07:29 - INFO - __main__ - Step 170 Global step 170 Train loss 2.567609 on epoch=84
03/13/2022 02:07:34 - INFO - __main__ - Step 180 Global step 180 Train loss 2.155737 on epoch=89
03/13/2022 02:07:40 - INFO - __main__ - Step 190 Global step 190 Train loss 2.161628 on epoch=94
03/13/2022 02:07:45 - INFO - __main__ - Step 200 Global step 200 Train loss 1.970512 on epoch=99
03/13/2022 02:07:45 - INFO - __main__ - Global step 200 Train loss 2.355905 EM 0.0 on epoch=99
03/13/2022 02:07:51 - INFO - __main__ - Step 210 Global step 210 Train loss 1.781763 on epoch=104
03/13/2022 02:07:56 - INFO - __main__ - Step 220 Global step 220 Train loss 1.657667 on epoch=109
03/13/2022 02:08:01 - INFO - __main__ - Step 230 Global step 230 Train loss 1.516011 on epoch=114
03/13/2022 02:08:06 - INFO - __main__ - Step 240 Global step 240 Train loss 1.496287 on epoch=119
03/13/2022 02:08:11 - INFO - __main__ - Step 250 Global step 250 Train loss 1.530906 on epoch=124
03/13/2022 02:08:12 - INFO - __main__ - Global step 250 Train loss 1.596527 EM 0.0 on epoch=124
03/13/2022 02:08:17 - INFO - __main__ - Step 260 Global step 260 Train loss 1.665787 on epoch=129
03/13/2022 02:08:22 - INFO - __main__ - Step 270 Global step 270 Train loss 1.507252 on epoch=134
03/13/2022 02:08:27 - INFO - __main__ - Step 280 Global step 280 Train loss 1.519662 on epoch=139
03/13/2022 02:08:32 - INFO - __main__ - Step 290 Global step 290 Train loss 1.406218 on epoch=144
03/13/2022 02:08:37 - INFO - __main__ - Step 300 Global step 300 Train loss 1.267688 on epoch=149
03/13/2022 02:08:38 - INFO - __main__ - Global step 300 Train loss 1.473322 EM 0.0 on epoch=149
03/13/2022 02:08:43 - INFO - __main__ - Step 310 Global step 310 Train loss 1.182528 on epoch=154
03/13/2022 02:08:48 - INFO - __main__ - Step 320 Global step 320 Train loss 1.289645 on epoch=159
03/13/2022 02:08:54 - INFO - __main__ - Step 330 Global step 330 Train loss 1.201045 on epoch=164
03/13/2022 02:08:59 - INFO - __main__ - Step 340 Global step 340 Train loss 1.142355 on epoch=169
03/13/2022 02:09:04 - INFO - __main__ - Step 350 Global step 350 Train loss 1.097285 on epoch=174
03/13/2022 02:09:05 - INFO - __main__ - Global step 350 Train loss 1.182571 EM 0.0 on epoch=174
03/13/2022 02:09:10 - INFO - __main__ - Step 360 Global step 360 Train loss 1.158675 on epoch=179
03/13/2022 02:09:15 - INFO - __main__ - Step 370 Global step 370 Train loss 1.102956 on epoch=184
03/13/2022 02:09:20 - INFO - __main__ - Step 380 Global step 380 Train loss 1.117229 on epoch=189
03/13/2022 02:09:25 - INFO - __main__ - Step 390 Global step 390 Train loss 1.170547 on epoch=194
03/13/2022 02:09:30 - INFO - __main__ - Step 400 Global step 400 Train loss 1.038520 on epoch=199
03/13/2022 02:09:31 - INFO - __main__ - Global step 400 Train loss 1.117585 EM 0.0 on epoch=199
03/13/2022 02:09:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.860874 on epoch=204
03/13/2022 02:09:41 - INFO - __main__ - Step 420 Global step 420 Train loss 1.128010 on epoch=209
03/13/2022 02:09:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.997325 on epoch=214
03/13/2022 02:09:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.986681 on epoch=219
03/13/2022 02:09:57 - INFO - __main__ - Step 450 Global step 450 Train loss 1.100482 on epoch=224
03/13/2022 02:09:58 - INFO - __main__ - Global step 450 Train loss 1.014674 EM 0.0 on epoch=224
03/13/2022 02:10:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.966114 on epoch=229
03/13/2022 02:10:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.980443 on epoch=234
03/13/2022 02:10:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.899579 on epoch=239
03/13/2022 02:10:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.871449 on epoch=244
03/13/2022 02:10:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.806836 on epoch=249
03/13/2022 02:10:24 - INFO - __main__ - Global step 500 Train loss 0.904884 EM 0.0 on epoch=249
03/13/2022 02:10:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.844473 on epoch=254
03/13/2022 02:10:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.802362 on epoch=259
03/13/2022 02:10:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.789862 on epoch=264
03/13/2022 02:10:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.794197 on epoch=269
03/13/2022 02:10:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.767879 on epoch=274
03/13/2022 02:10:50 - INFO - __main__ - Global step 550 Train loss 0.799754 EM 0.0 on epoch=274
03/13/2022 02:10:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.788822 on epoch=279
03/13/2022 02:11:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.725878 on epoch=284
03/13/2022 02:11:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.813351 on epoch=289
03/13/2022 02:11:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.789132 on epoch=294
03/13/2022 02:11:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.749820 on epoch=299
03/13/2022 02:11:17 - INFO - __main__ - Global step 600 Train loss 0.773401 EM 0.0 on epoch=299
03/13/2022 02:11:17 - INFO - __main__ - save last model!
03/13/2022 02:11:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:11:17 - INFO - __main__ - Printing 3 examples
03/13/2022 02:11:17 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/13/2022 02:11:17 - INFO - __main__ - ['francois mitterrand']
03/13/2022 02:11:17 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/13/2022 02:11:17 - INFO - __main__ - ['james callaghan']
03/13/2022 02:11:17 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/13/2022 02:11:17 - INFO - __main__ - ['aberdeen']
03/13/2022 02:11:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 02:11:17 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:11:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:11:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:11:17 - INFO - __main__ - Printing 3 examples
03/13/2022 02:11:17 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/13/2022 02:11:17 - INFO - __main__ - ['tulisa']
03/13/2022 02:11:17 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/13/2022 02:11:17 - INFO - __main__ - ['calgary']
03/13/2022 02:11:17 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/13/2022 02:11:17 - INFO - __main__ - ['jeff bridges']
03/13/2022 02:11:17 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:11:17 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:11:17 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:11:23 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 02:11:24 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 02:11:24 - INFO - __main__ - Printing 3 examples
03/13/2022 02:11:24 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 02:11:24 - INFO - __main__ - ['taming of the shrew']
03/13/2022 02:11:24 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 02:11:24 - INFO - __main__ - ['henry fonda']
03/13/2022 02:11:24 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 02:11:24 - INFO - __main__ - ['tchaikovsky']
03/13/2022 02:11:24 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:11:26 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:11:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:11:28 - INFO - __main__ - Starting training!
03/13/2022 02:11:30 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 02:15:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_42_0.0003_8_predictions.txt
03/13/2022 02:15:55 - INFO - __main__ - EM on test data: 0.0003
03/13/2022 02:15:55 - INFO - __main__ - prefix=freebase_qa_32_42, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.0002503755633450175
03/13/2022 02:15:55 - INFO - __main__ - Running ... prefix=freebase_qa_32_42, lr=0.0002, bsz=8 ...
03/13/2022 02:15:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:15:56 - INFO - __main__ - Printing 3 examples
03/13/2022 02:15:56 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/13/2022 02:15:56 - INFO - __main__ - ['francois mitterrand']
03/13/2022 02:15:56 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/13/2022 02:15:56 - INFO - __main__ - ['james callaghan']
03/13/2022 02:15:56 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/13/2022 02:15:56 - INFO - __main__ - ['aberdeen']
03/13/2022 02:15:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 02:15:56 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:15:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:15:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:15:56 - INFO - __main__ - Printing 3 examples
03/13/2022 02:15:56 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/13/2022 02:15:56 - INFO - __main__ - ['tulisa']
03/13/2022 02:15:56 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/13/2022 02:15:56 - INFO - __main__ - ['calgary']
03/13/2022 02:15:56 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/13/2022 02:15:56 - INFO - __main__ - ['jeff bridges']
03/13/2022 02:15:56 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:15:56 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:15:56 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:16:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:16:07 - INFO - __main__ - Starting training!
03/13/2022 02:16:11 - INFO - __main__ - Step 10 Global step 10 Train loss 20.151144 on epoch=4
03/13/2022 02:16:16 - INFO - __main__ - Step 20 Global step 20 Train loss 18.053598 on epoch=9
03/13/2022 02:16:21 - INFO - __main__ - Step 30 Global step 30 Train loss 14.116661 on epoch=14
03/13/2022 02:16:26 - INFO - __main__ - Step 40 Global step 40 Train loss 12.997294 on epoch=19
03/13/2022 02:16:31 - INFO - __main__ - Step 50 Global step 50 Train loss 11.794586 on epoch=24
03/13/2022 02:16:32 - INFO - __main__ - Global step 50 Train loss 15.422656 EM 0.0 on epoch=24
03/13/2022 02:16:38 - INFO - __main__ - Step 60 Global step 60 Train loss 10.992708 on epoch=29
03/13/2022 02:16:43 - INFO - __main__ - Step 70 Global step 70 Train loss 9.518502 on epoch=34
03/13/2022 02:16:48 - INFO - __main__ - Step 80 Global step 80 Train loss 9.090841 on epoch=39
03/13/2022 02:16:53 - INFO - __main__ - Step 90 Global step 90 Train loss 8.578634 on epoch=44
03/13/2022 02:16:58 - INFO - __main__ - Step 100 Global step 100 Train loss 8.240141 on epoch=49
03/13/2022 02:16:59 - INFO - __main__ - Global step 100 Train loss 9.284165 EM 0.0 on epoch=49
03/13/2022 02:17:04 - INFO - __main__ - Step 110 Global step 110 Train loss 7.329185 on epoch=54
03/13/2022 02:17:09 - INFO - __main__ - Step 120 Global step 120 Train loss 7.035658 on epoch=59
03/13/2022 02:17:14 - INFO - __main__ - Step 130 Global step 130 Train loss 6.424991 on epoch=64
03/13/2022 02:17:19 - INFO - __main__ - Step 140 Global step 140 Train loss 5.846254 on epoch=69
03/13/2022 02:17:24 - INFO - __main__ - Step 150 Global step 150 Train loss 4.693680 on epoch=74
03/13/2022 02:17:25 - INFO - __main__ - Global step 150 Train loss 6.265954 EM 0.0 on epoch=74
03/13/2022 02:17:30 - INFO - __main__ - Step 160 Global step 160 Train loss 4.476924 on epoch=79
03/13/2022 02:17:35 - INFO - __main__ - Step 170 Global step 170 Train loss 4.201994 on epoch=84
03/13/2022 02:17:40 - INFO - __main__ - Step 180 Global step 180 Train loss 3.548373 on epoch=89
03/13/2022 02:17:45 - INFO - __main__ - Step 190 Global step 190 Train loss 3.559992 on epoch=94
03/13/2022 02:17:50 - INFO - __main__ - Step 200 Global step 200 Train loss 3.103756 on epoch=99
03/13/2022 02:17:51 - INFO - __main__ - Global step 200 Train loss 3.778208 EM 0.0 on epoch=99
03/13/2022 02:17:56 - INFO - __main__ - Step 210 Global step 210 Train loss 2.916563 on epoch=104
03/13/2022 02:18:01 - INFO - __main__ - Step 220 Global step 220 Train loss 2.761400 on epoch=109
03/13/2022 02:18:07 - INFO - __main__ - Step 230 Global step 230 Train loss 2.685354 on epoch=114
03/13/2022 02:18:12 - INFO - __main__ - Step 240 Global step 240 Train loss 2.466880 on epoch=119
03/13/2022 02:18:17 - INFO - __main__ - Step 250 Global step 250 Train loss 2.105597 on epoch=124
03/13/2022 02:18:18 - INFO - __main__ - Global step 250 Train loss 2.587158 EM 0.0 on epoch=124
03/13/2022 02:18:23 - INFO - __main__ - Step 260 Global step 260 Train loss 2.202411 on epoch=129
03/13/2022 02:18:28 - INFO - __main__ - Step 270 Global step 270 Train loss 2.031473 on epoch=134
03/13/2022 02:18:33 - INFO - __main__ - Step 280 Global step 280 Train loss 1.976620 on epoch=139
03/13/2022 02:18:38 - INFO - __main__ - Step 290 Global step 290 Train loss 1.768205 on epoch=144
03/13/2022 02:18:43 - INFO - __main__ - Step 300 Global step 300 Train loss 1.741259 on epoch=149
03/13/2022 02:18:44 - INFO - __main__ - Global step 300 Train loss 1.943994 EM 0.0 on epoch=149
03/13/2022 02:18:49 - INFO - __main__ - Step 310 Global step 310 Train loss 1.672164 on epoch=154
03/13/2022 02:18:54 - INFO - __main__ - Step 320 Global step 320 Train loss 1.604352 on epoch=159
03/13/2022 02:18:59 - INFO - __main__ - Step 330 Global step 330 Train loss 1.452413 on epoch=164
03/13/2022 02:19:05 - INFO - __main__ - Step 340 Global step 340 Train loss 1.412709 on epoch=169
03/13/2022 02:19:10 - INFO - __main__ - Step 350 Global step 350 Train loss 1.577218 on epoch=174
03/13/2022 02:19:10 - INFO - __main__ - Global step 350 Train loss 1.543771 EM 0.0 on epoch=174
03/13/2022 02:19:16 - INFO - __main__ - Step 360 Global step 360 Train loss 1.279870 on epoch=179
03/13/2022 02:19:21 - INFO - __main__ - Step 370 Global step 370 Train loss 1.393365 on epoch=184
03/13/2022 02:19:26 - INFO - __main__ - Step 380 Global step 380 Train loss 1.289335 on epoch=189
03/13/2022 02:19:31 - INFO - __main__ - Step 390 Global step 390 Train loss 1.406915 on epoch=194
03/13/2022 02:19:36 - INFO - __main__ - Step 400 Global step 400 Train loss 1.358723 on epoch=199
03/13/2022 02:19:37 - INFO - __main__ - Global step 400 Train loss 1.345642 EM 0.0 on epoch=199
03/13/2022 02:19:42 - INFO - __main__ - Step 410 Global step 410 Train loss 1.419936 on epoch=204
03/13/2022 02:19:47 - INFO - __main__ - Step 420 Global step 420 Train loss 1.539566 on epoch=209
03/13/2022 02:19:52 - INFO - __main__ - Step 430 Global step 430 Train loss 1.411750 on epoch=214
03/13/2022 02:19:57 - INFO - __main__ - Step 440 Global step 440 Train loss 1.191888 on epoch=219
03/13/2022 02:20:02 - INFO - __main__ - Step 450 Global step 450 Train loss 1.319238 on epoch=224
03/13/2022 02:20:03 - INFO - __main__ - Global step 450 Train loss 1.376476 EM 0.0 on epoch=224
03/13/2022 02:20:08 - INFO - __main__ - Step 460 Global step 460 Train loss 1.158966 on epoch=229
03/13/2022 02:20:13 - INFO - __main__ - Step 470 Global step 470 Train loss 1.059341 on epoch=234
03/13/2022 02:20:18 - INFO - __main__ - Step 480 Global step 480 Train loss 1.185350 on epoch=239
03/13/2022 02:20:23 - INFO - __main__ - Step 490 Global step 490 Train loss 1.066876 on epoch=244
03/13/2022 02:20:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.964369 on epoch=249
03/13/2022 02:20:29 - INFO - __main__ - Global step 500 Train loss 1.086980 EM 0.0 on epoch=249
03/13/2022 02:20:34 - INFO - __main__ - Step 510 Global step 510 Train loss 1.046419 on epoch=254
03/13/2022 02:20:39 - INFO - __main__ - Step 520 Global step 520 Train loss 1.035260 on epoch=259
03/13/2022 02:20:45 - INFO - __main__ - Step 530 Global step 530 Train loss 1.244533 on epoch=264
03/13/2022 02:20:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.958767 on epoch=269
03/13/2022 02:20:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.983270 on epoch=274
03/13/2022 02:20:56 - INFO - __main__ - Global step 550 Train loss 1.053650 EM 0.0 on epoch=274
03/13/2022 02:21:01 - INFO - __main__ - Step 560 Global step 560 Train loss 1.132201 on epoch=279
03/13/2022 02:21:06 - INFO - __main__ - Step 570 Global step 570 Train loss 1.038381 on epoch=284
03/13/2022 02:21:11 - INFO - __main__ - Step 580 Global step 580 Train loss 1.082118 on epoch=289
03/13/2022 02:21:16 - INFO - __main__ - Step 590 Global step 590 Train loss 1.040005 on epoch=294
03/13/2022 02:21:21 - INFO - __main__ - Step 600 Global step 600 Train loss 1.013251 on epoch=299
03/13/2022 02:21:22 - INFO - __main__ - Global step 600 Train loss 1.061191 EM 0.0 on epoch=299
03/13/2022 02:21:22 - INFO - __main__ - save last model!
03/13/2022 02:21:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:21:22 - INFO - __main__ - Printing 3 examples
03/13/2022 02:21:22 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/13/2022 02:21:22 - INFO - __main__ - ['francois mitterrand']
03/13/2022 02:21:22 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/13/2022 02:21:22 - INFO - __main__ - ['james callaghan']
03/13/2022 02:21:22 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/13/2022 02:21:22 - INFO - __main__ - ['aberdeen']
03/13/2022 02:21:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 02:21:22 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:21:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:21:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:21:23 - INFO - __main__ - Printing 3 examples
03/13/2022 02:21:23 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/13/2022 02:21:23 - INFO - __main__ - ['tulisa']
03/13/2022 02:21:23 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/13/2022 02:21:23 - INFO - __main__ - ['calgary']
03/13/2022 02:21:23 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/13/2022 02:21:23 - INFO - __main__ - ['jeff bridges']
03/13/2022 02:21:23 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:21:23 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:21:23 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:21:29 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 02:21:29 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 02:21:29 - INFO - __main__ - Printing 3 examples
03/13/2022 02:21:29 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 02:21:29 - INFO - __main__ - ['taming of the shrew']
03/13/2022 02:21:29 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 02:21:29 - INFO - __main__ - ['henry fonda']
03/13/2022 02:21:29 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 02:21:29 - INFO - __main__ - ['tchaikovsky']
03/13/2022 02:21:29 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:21:31 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:21:35 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 02:21:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:21:35 - INFO - __main__ - Starting training!
03/13/2022 02:25:06 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_42_0.0002_8_predictions.txt
03/13/2022 02:25:06 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 02:25:06 - INFO - __main__ - prefix=freebase_qa_32_42, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 02:25:06 - INFO - __main__ - Running ... prefix=freebase_qa_32_42, lr=0.0001, bsz=8 ...
03/13/2022 02:25:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:25:07 - INFO - __main__ - Printing 3 examples
03/13/2022 02:25:07 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/13/2022 02:25:07 - INFO - __main__ - ['francois mitterrand']
03/13/2022 02:25:07 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/13/2022 02:25:07 - INFO - __main__ - ['james callaghan']
03/13/2022 02:25:07 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/13/2022 02:25:07 - INFO - __main__ - ['aberdeen']
03/13/2022 02:25:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 02:25:07 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:25:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:25:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:25:07 - INFO - __main__ - Printing 3 examples
03/13/2022 02:25:07 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/13/2022 02:25:07 - INFO - __main__ - ['tulisa']
03/13/2022 02:25:07 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/13/2022 02:25:07 - INFO - __main__ - ['calgary']
03/13/2022 02:25:07 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/13/2022 02:25:07 - INFO - __main__ - ['jeff bridges']
03/13/2022 02:25:07 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:25:07 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:25:07 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:25:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:25:20 - INFO - __main__ - Starting training!
03/13/2022 02:25:25 - INFO - __main__ - Step 10 Global step 10 Train loss 20.312607 on epoch=4
03/13/2022 02:25:30 - INFO - __main__ - Step 20 Global step 20 Train loss 19.173250 on epoch=9
03/13/2022 02:25:35 - INFO - __main__ - Step 30 Global step 30 Train loss 14.681717 on epoch=14
03/13/2022 02:25:40 - INFO - __main__ - Step 40 Global step 40 Train loss 12.527639 on epoch=19
03/13/2022 02:25:45 - INFO - __main__ - Step 50 Global step 50 Train loss 11.354466 on epoch=24
03/13/2022 02:25:47 - INFO - __main__ - Global step 50 Train loss 15.609936 EM 0.0 on epoch=24
03/13/2022 02:25:53 - INFO - __main__ - Step 60 Global step 60 Train loss 10.389153 on epoch=29
03/13/2022 02:25:58 - INFO - __main__ - Step 70 Global step 70 Train loss 9.378744 on epoch=34
03/13/2022 02:26:03 - INFO - __main__ - Step 80 Global step 80 Train loss 8.873373 on epoch=39
03/13/2022 02:26:08 - INFO - __main__ - Step 90 Global step 90 Train loss 8.188583 on epoch=44
03/13/2022 02:26:13 - INFO - __main__ - Step 100 Global step 100 Train loss 7.565304 on epoch=49
03/13/2022 02:26:14 - INFO - __main__ - Global step 100 Train loss 8.879031 EM 0.0 on epoch=49
03/13/2022 02:26:19 - INFO - __main__ - Step 110 Global step 110 Train loss 6.744957 on epoch=54
03/13/2022 02:26:24 - INFO - __main__ - Step 120 Global step 120 Train loss 6.277724 on epoch=59
03/13/2022 02:26:29 - INFO - __main__ - Step 130 Global step 130 Train loss 5.974738 on epoch=64
03/13/2022 02:26:34 - INFO - __main__ - Step 140 Global step 140 Train loss 5.525033 on epoch=69
03/13/2022 02:26:40 - INFO - __main__ - Step 150 Global step 150 Train loss 5.070276 on epoch=74
03/13/2022 02:26:41 - INFO - __main__ - Global step 150 Train loss 5.918546 EM 0.0 on epoch=74
03/13/2022 02:26:46 - INFO - __main__ - Step 160 Global step 160 Train loss 5.142338 on epoch=79
03/13/2022 02:26:51 - INFO - __main__ - Step 170 Global step 170 Train loss 5.068756 on epoch=84
03/13/2022 02:26:56 - INFO - __main__ - Step 180 Global step 180 Train loss 5.086846 on epoch=89
03/13/2022 02:27:01 - INFO - __main__ - Step 190 Global step 190 Train loss 4.853707 on epoch=94
03/13/2022 02:27:06 - INFO - __main__ - Step 200 Global step 200 Train loss 5.048813 on epoch=99
03/13/2022 02:27:07 - INFO - __main__ - Global step 200 Train loss 5.040092 EM 0.0 on epoch=99
03/13/2022 02:27:12 - INFO - __main__ - Step 210 Global step 210 Train loss 4.510486 on epoch=104
03/13/2022 02:27:17 - INFO - __main__ - Step 220 Global step 220 Train loss 4.242561 on epoch=109
03/13/2022 02:27:23 - INFO - __main__ - Step 230 Global step 230 Train loss 4.147705 on epoch=114
03/13/2022 02:27:28 - INFO - __main__ - Step 240 Global step 240 Train loss 4.458048 on epoch=119
03/13/2022 02:27:33 - INFO - __main__ - Step 250 Global step 250 Train loss 4.165033 on epoch=124
03/13/2022 02:27:34 - INFO - __main__ - Global step 250 Train loss 4.304767 EM 0.0 on epoch=124
03/13/2022 02:27:39 - INFO - __main__ - Step 260 Global step 260 Train loss 3.968007 on epoch=129
03/13/2022 02:27:44 - INFO - __main__ - Step 270 Global step 270 Train loss 3.979173 on epoch=134
03/13/2022 02:27:49 - INFO - __main__ - Step 280 Global step 280 Train loss 3.938221 on epoch=139
03/13/2022 02:27:54 - INFO - __main__ - Step 290 Global step 290 Train loss 3.864249 on epoch=144
03/13/2022 02:28:00 - INFO - __main__ - Step 300 Global step 300 Train loss 3.891999 on epoch=149
03/13/2022 02:28:01 - INFO - __main__ - Global step 300 Train loss 3.928330 EM 0.0 on epoch=149
03/13/2022 02:28:06 - INFO - __main__ - Step 310 Global step 310 Train loss 3.796554 on epoch=154
03/13/2022 02:28:11 - INFO - __main__ - Step 320 Global step 320 Train loss 3.311325 on epoch=159
03/13/2022 02:28:16 - INFO - __main__ - Step 330 Global step 330 Train loss 3.331394 on epoch=164
03/13/2022 02:28:21 - INFO - __main__ - Step 340 Global step 340 Train loss 3.241624 on epoch=169
03/13/2022 02:28:26 - INFO - __main__ - Step 350 Global step 350 Train loss 3.294084 on epoch=174
03/13/2022 02:28:27 - INFO - __main__ - Global step 350 Train loss 3.394996 EM 0.0 on epoch=174
03/13/2022 02:28:32 - INFO - __main__ - Step 360 Global step 360 Train loss 3.066355 on epoch=179
03/13/2022 02:28:37 - INFO - __main__ - Step 370 Global step 370 Train loss 2.801432 on epoch=184
03/13/2022 02:28:42 - INFO - __main__ - Step 380 Global step 380 Train loss 2.929871 on epoch=189
03/13/2022 02:28:47 - INFO - __main__ - Step 390 Global step 390 Train loss 2.926231 on epoch=194
03/13/2022 02:28:52 - INFO - __main__ - Step 400 Global step 400 Train loss 2.809701 on epoch=199
03/13/2022 02:28:53 - INFO - __main__ - Global step 400 Train loss 2.906718 EM 0.0 on epoch=199
03/13/2022 02:28:58 - INFO - __main__ - Step 410 Global step 410 Train loss 2.852390 on epoch=204
03/13/2022 02:29:04 - INFO - __main__ - Step 420 Global step 420 Train loss 2.395543 on epoch=209
03/13/2022 02:29:09 - INFO - __main__ - Step 430 Global step 430 Train loss 2.284448 on epoch=214
03/13/2022 02:29:14 - INFO - __main__ - Step 440 Global step 440 Train loss 2.302767 on epoch=219
03/13/2022 02:29:19 - INFO - __main__ - Step 450 Global step 450 Train loss 2.107393 on epoch=224
03/13/2022 02:29:20 - INFO - __main__ - Global step 450 Train loss 2.388508 EM 0.0 on epoch=224
03/13/2022 02:29:25 - INFO - __main__ - Step 460 Global step 460 Train loss 2.335943 on epoch=229
03/13/2022 02:29:30 - INFO - __main__ - Step 470 Global step 470 Train loss 2.145747 on epoch=234
03/13/2022 02:29:35 - INFO - __main__ - Step 480 Global step 480 Train loss 2.031107 on epoch=239
03/13/2022 02:29:40 - INFO - __main__ - Step 490 Global step 490 Train loss 1.681941 on epoch=244
03/13/2022 02:29:45 - INFO - __main__ - Step 500 Global step 500 Train loss 2.227254 on epoch=249
03/13/2022 02:29:46 - INFO - __main__ - Global step 500 Train loss 2.084399 EM 0.0 on epoch=249
03/13/2022 02:29:51 - INFO - __main__ - Step 510 Global step 510 Train loss 2.192461 on epoch=254
03/13/2022 02:29:56 - INFO - __main__ - Step 520 Global step 520 Train loss 2.154723 on epoch=259
03/13/2022 02:30:02 - INFO - __main__ - Step 530 Global step 530 Train loss 1.722979 on epoch=264
03/13/2022 02:30:07 - INFO - __main__ - Step 540 Global step 540 Train loss 1.874507 on epoch=269
03/13/2022 02:30:12 - INFO - __main__ - Step 550 Global step 550 Train loss 1.546242 on epoch=274
03/13/2022 02:30:13 - INFO - __main__ - Global step 550 Train loss 1.898183 EM 0.0 on epoch=274
03/13/2022 02:30:18 - INFO - __main__ - Step 560 Global step 560 Train loss 1.659152 on epoch=279
03/13/2022 02:30:23 - INFO - __main__ - Step 570 Global step 570 Train loss 1.887031 on epoch=284
03/13/2022 02:30:28 - INFO - __main__ - Step 580 Global step 580 Train loss 1.846057 on epoch=289
03/13/2022 02:30:33 - INFO - __main__ - Step 590 Global step 590 Train loss 1.762970 on epoch=294
03/13/2022 02:30:38 - INFO - __main__ - Step 600 Global step 600 Train loss 1.543129 on epoch=299
03/13/2022 02:30:39 - INFO - __main__ - Global step 600 Train loss 1.739668 EM 0.0 on epoch=299
03/13/2022 02:30:39 - INFO - __main__ - save last model!
03/13/2022 02:30:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:30:39 - INFO - __main__ - Printing 3 examples
03/13/2022 02:30:39 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/13/2022 02:30:39 - INFO - __main__ - ['ok computer']
03/13/2022 02:30:39 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/13/2022 02:30:39 - INFO - __main__ - ['thursday']
03/13/2022 02:30:39 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/13/2022 02:30:39 - INFO - __main__ - ['sigourney weaver']
03/13/2022 02:30:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 02:30:39 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:30:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:30:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:30:39 - INFO - __main__ - Printing 3 examples
03/13/2022 02:30:39 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/13/2022 02:30:39 - INFO - __main__ - ['pinta island tortoise']
03/13/2022 02:30:39 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/13/2022 02:30:39 - INFO - __main__ - ['daphne du maurier']
03/13/2022 02:30:39 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/13/2022 02:30:39 - INFO - __main__ - ['back to the future']
03/13/2022 02:30:39 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:30:39 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:30:39 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:30:46 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 02:30:47 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 02:30:47 - INFO - __main__ - Printing 3 examples
03/13/2022 02:30:47 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 02:30:47 - INFO - __main__ - ['taming of the shrew']
03/13/2022 02:30:47 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 02:30:47 - INFO - __main__ - ['henry fonda']
03/13/2022 02:30:47 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 02:30:47 - INFO - __main__ - ['tchaikovsky']
03/13/2022 02:30:47 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:30:48 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:30:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:30:51 - INFO - __main__ - Starting training!
03/13/2022 02:30:52 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 02:34:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_42_0.0001_8_predictions.txt
03/13/2022 02:34:45 - INFO - __main__ - EM on test data: 0.0005
03/13/2022 02:34:45 - INFO - __main__ - prefix=freebase_qa_32_42, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.000500751126690035
03/13/2022 02:34:45 - INFO - __main__ - Running ... prefix=freebase_qa_32_87, lr=0.0005, bsz=8 ...
03/13/2022 02:34:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:34:46 - INFO - __main__ - Printing 3 examples
03/13/2022 02:34:46 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/13/2022 02:34:46 - INFO - __main__ - ['ok computer']
03/13/2022 02:34:46 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/13/2022 02:34:46 - INFO - __main__ - ['thursday']
03/13/2022 02:34:46 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/13/2022 02:34:46 - INFO - __main__ - ['sigourney weaver']
03/13/2022 02:34:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 02:34:46 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:34:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:34:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:34:46 - INFO - __main__ - Printing 3 examples
03/13/2022 02:34:46 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/13/2022 02:34:46 - INFO - __main__ - ['pinta island tortoise']
03/13/2022 02:34:46 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/13/2022 02:34:46 - INFO - __main__ - ['daphne du maurier']
03/13/2022 02:34:46 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/13/2022 02:34:46 - INFO - __main__ - ['back to the future']
03/13/2022 02:34:46 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:34:46 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:34:46 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:34:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:34:59 - INFO - __main__ - Starting training!
03/13/2022 02:35:03 - INFO - __main__ - Step 10 Global step 10 Train loss 19.539822 on epoch=4
03/13/2022 02:35:08 - INFO - __main__ - Step 20 Global step 20 Train loss 17.327799 on epoch=9
03/13/2022 02:35:12 - INFO - __main__ - Step 30 Global step 30 Train loss 14.617910 on epoch=14
03/13/2022 02:35:17 - INFO - __main__ - Step 40 Global step 40 Train loss 11.675036 on epoch=19
03/13/2022 02:35:23 - INFO - __main__ - Step 50 Global step 50 Train loss 9.547266 on epoch=24
03/13/2022 02:35:23 - INFO - __main__ - Global step 50 Train loss 14.541568 EM 0.0 on epoch=24
03/13/2022 02:35:29 - INFO - __main__ - Step 60 Global step 60 Train loss 8.651091 on epoch=29
03/13/2022 02:35:34 - INFO - __main__ - Step 70 Global step 70 Train loss 7.532148 on epoch=34
03/13/2022 02:35:39 - INFO - __main__ - Step 80 Global step 80 Train loss 8.437346 on epoch=39
03/13/2022 02:35:44 - INFO - __main__ - Step 90 Global step 90 Train loss 6.365315 on epoch=44
03/13/2022 02:35:50 - INFO - __main__ - Step 100 Global step 100 Train loss 5.309904 on epoch=49
03/13/2022 02:35:50 - INFO - __main__ - Global step 100 Train loss 7.259160 EM 0.0 on epoch=49
03/13/2022 02:35:56 - INFO - __main__ - Step 110 Global step 110 Train loss 3.727590 on epoch=54
03/13/2022 02:36:01 - INFO - __main__ - Step 120 Global step 120 Train loss 3.101544 on epoch=59
03/13/2022 02:36:06 - INFO - __main__ - Step 130 Global step 130 Train loss 2.649367 on epoch=64
03/13/2022 02:36:11 - INFO - __main__ - Step 140 Global step 140 Train loss 2.241654 on epoch=69
03/13/2022 02:36:16 - INFO - __main__ - Step 150 Global step 150 Train loss 1.966734 on epoch=74
03/13/2022 02:36:17 - INFO - __main__ - Global step 150 Train loss 2.737378 EM 0.0 on epoch=74
03/13/2022 02:36:22 - INFO - __main__ - Step 160 Global step 160 Train loss 1.882545 on epoch=79
03/13/2022 02:36:27 - INFO - __main__ - Step 170 Global step 170 Train loss 1.696295 on epoch=84
03/13/2022 02:36:32 - INFO - __main__ - Step 180 Global step 180 Train loss 1.477637 on epoch=89
03/13/2022 02:36:37 - INFO - __main__ - Step 190 Global step 190 Train loss 1.617137 on epoch=94
03/13/2022 02:36:42 - INFO - __main__ - Step 200 Global step 200 Train loss 1.450253 on epoch=99
03/13/2022 02:36:43 - INFO - __main__ - Global step 200 Train loss 1.624773 EM 0.0 on epoch=99
03/13/2022 02:36:49 - INFO - __main__ - Step 210 Global step 210 Train loss 1.501925 on epoch=104
03/13/2022 02:36:54 - INFO - __main__ - Step 220 Global step 220 Train loss 1.360950 on epoch=109
03/13/2022 02:36:59 - INFO - __main__ - Step 230 Global step 230 Train loss 1.188668 on epoch=114
03/13/2022 02:37:04 - INFO - __main__ - Step 240 Global step 240 Train loss 1.245240 on epoch=119
03/13/2022 02:37:09 - INFO - __main__ - Step 250 Global step 250 Train loss 1.177361 on epoch=124
03/13/2022 02:37:10 - INFO - __main__ - Global step 250 Train loss 1.294829 EM 0.0 on epoch=124
03/13/2022 02:37:15 - INFO - __main__ - Step 260 Global step 260 Train loss 1.132733 on epoch=129
03/13/2022 02:37:20 - INFO - __main__ - Step 270 Global step 270 Train loss 1.020296 on epoch=134
03/13/2022 02:37:25 - INFO - __main__ - Step 280 Global step 280 Train loss 1.047568 on epoch=139
03/13/2022 02:37:30 - INFO - __main__ - Step 290 Global step 290 Train loss 1.024228 on epoch=144
03/13/2022 02:37:35 - INFO - __main__ - Step 300 Global step 300 Train loss 1.018821 on epoch=149
03/13/2022 02:37:36 - INFO - __main__ - Global step 300 Train loss 1.048729 EM 0.0 on epoch=149
03/13/2022 02:37:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.136373 on epoch=154
03/13/2022 02:37:46 - INFO - __main__ - Step 320 Global step 320 Train loss 1.012404 on epoch=159
03/13/2022 02:37:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.800257 on epoch=164
03/13/2022 02:37:57 - INFO - __main__ - Step 340 Global step 340 Train loss 1.082647 on epoch=169
03/13/2022 02:38:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.943470 on epoch=174
03/13/2022 02:38:03 - INFO - __main__ - Global step 350 Train loss 0.995030 EM 0.0 on epoch=174
03/13/2022 02:38:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.999138 on epoch=179
03/13/2022 02:38:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.998682 on epoch=184
03/13/2022 02:38:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.778549 on epoch=189
03/13/2022 02:38:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.812631 on epoch=194
03/13/2022 02:38:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.846762 on epoch=199
03/13/2022 02:38:29 - INFO - __main__ - Global step 400 Train loss 0.887152 EM 0.0 on epoch=199
03/13/2022 02:38:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.774085 on epoch=204
03/13/2022 02:38:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.829348 on epoch=209
03/13/2022 02:38:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.693207 on epoch=214
03/13/2022 02:38:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.707805 on epoch=219
03/13/2022 02:38:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.663121 on epoch=224
03/13/2022 02:38:55 - INFO - __main__ - Global step 450 Train loss 0.733513 EM 0.0 on epoch=224
03/13/2022 02:39:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.661614 on epoch=229
03/13/2022 02:39:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.674980 on epoch=234
03/13/2022 02:39:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.722212 on epoch=239
03/13/2022 02:39:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.675680 on epoch=244
03/13/2022 02:39:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.609254 on epoch=249
03/13/2022 02:39:22 - INFO - __main__ - Global step 500 Train loss 0.668748 EM 0.0 on epoch=249
03/13/2022 02:39:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.666432 on epoch=254
03/13/2022 02:39:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.617167 on epoch=259
03/13/2022 02:39:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.538935 on epoch=264
03/13/2022 02:39:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.525438 on epoch=269
03/13/2022 02:39:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.616428 on epoch=274
03/13/2022 02:39:48 - INFO - __main__ - Global step 550 Train loss 0.592880 EM 0.0 on epoch=274
03/13/2022 02:39:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.578956 on epoch=279
03/13/2022 02:39:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.597208 on epoch=284
03/13/2022 02:40:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.550454 on epoch=289
03/13/2022 02:40:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.530088 on epoch=294
03/13/2022 02:40:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.518769 on epoch=299
03/13/2022 02:40:15 - INFO - __main__ - Global step 600 Train loss 0.555095 EM 0.0 on epoch=299
03/13/2022 02:40:15 - INFO - __main__ - save last model!
03/13/2022 02:40:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:40:15 - INFO - __main__ - Printing 3 examples
03/13/2022 02:40:15 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/13/2022 02:40:15 - INFO - __main__ - ['ok computer']
03/13/2022 02:40:15 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/13/2022 02:40:15 - INFO - __main__ - ['thursday']
03/13/2022 02:40:15 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/13/2022 02:40:15 - INFO - __main__ - ['sigourney weaver']
03/13/2022 02:40:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 02:40:15 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:40:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:40:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:40:15 - INFO - __main__ - Printing 3 examples
03/13/2022 02:40:15 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/13/2022 02:40:15 - INFO - __main__ - ['pinta island tortoise']
03/13/2022 02:40:15 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/13/2022 02:40:15 - INFO - __main__ - ['daphne du maurier']
03/13/2022 02:40:15 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/13/2022 02:40:15 - INFO - __main__ - ['back to the future']
03/13/2022 02:40:15 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:40:15 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:40:15 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:40:21 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 02:40:22 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 02:40:22 - INFO - __main__ - Printing 3 examples
03/13/2022 02:40:22 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 02:40:22 - INFO - __main__ - ['taming of the shrew']
03/13/2022 02:40:22 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 02:40:22 - INFO - __main__ - ['henry fonda']
03/13/2022 02:40:22 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 02:40:22 - INFO - __main__ - ['tchaikovsky']
03/13/2022 02:40:22 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:40:24 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:40:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:40:26 - INFO - __main__ - Starting training!
03/13/2022 02:40:28 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 02:43:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_87_0.0005_8_predictions.txt
03/13/2022 02:43:15 - INFO - __main__ - EM on test data: 0.0005
03/13/2022 02:43:15 - INFO - __main__ - prefix=freebase_qa_32_87, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.000500751126690035
03/13/2022 02:43:15 - INFO - __main__ - Running ... prefix=freebase_qa_32_87, lr=0.0003, bsz=8 ...
03/13/2022 02:43:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:43:16 - INFO - __main__ - Printing 3 examples
03/13/2022 02:43:16 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/13/2022 02:43:16 - INFO - __main__ - ['ok computer']
03/13/2022 02:43:16 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/13/2022 02:43:16 - INFO - __main__ - ['thursday']
03/13/2022 02:43:16 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/13/2022 02:43:16 - INFO - __main__ - ['sigourney weaver']
03/13/2022 02:43:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 02:43:16 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:43:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:43:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:43:16 - INFO - __main__ - Printing 3 examples
03/13/2022 02:43:16 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/13/2022 02:43:16 - INFO - __main__ - ['pinta island tortoise']
03/13/2022 02:43:16 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/13/2022 02:43:16 - INFO - __main__ - ['daphne du maurier']
03/13/2022 02:43:16 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/13/2022 02:43:16 - INFO - __main__ - ['back to the future']
03/13/2022 02:43:16 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:43:16 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:43:16 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:43:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:43:29 - INFO - __main__ - Starting training!
03/13/2022 02:43:33 - INFO - __main__ - Step 10 Global step 10 Train loss 19.356178 on epoch=4
03/13/2022 02:43:37 - INFO - __main__ - Step 20 Global step 20 Train loss 17.910864 on epoch=9
03/13/2022 02:43:42 - INFO - __main__ - Step 30 Global step 30 Train loss 13.395816 on epoch=14
03/13/2022 02:43:47 - INFO - __main__ - Step 40 Global step 40 Train loss 11.307623 on epoch=19
03/13/2022 02:43:52 - INFO - __main__ - Step 50 Global step 50 Train loss 9.655619 on epoch=24
03/13/2022 02:43:53 - INFO - __main__ - Global step 50 Train loss 14.325220 EM 0.0 on epoch=24
03/13/2022 02:43:59 - INFO - __main__ - Step 60 Global step 60 Train loss 9.358699 on epoch=29
03/13/2022 02:44:04 - INFO - __main__ - Step 70 Global step 70 Train loss 8.014708 on epoch=34
03/13/2022 02:44:09 - INFO - __main__ - Step 80 Global step 80 Train loss 7.827475 on epoch=39
03/13/2022 02:44:14 - INFO - __main__ - Step 90 Global step 90 Train loss 6.722135 on epoch=44
03/13/2022 02:44:19 - INFO - __main__ - Step 100 Global step 100 Train loss 6.331596 on epoch=49
03/13/2022 02:44:20 - INFO - __main__ - Global step 100 Train loss 7.650922 EM 0.0 on epoch=49
03/13/2022 02:44:25 - INFO - __main__ - Step 110 Global step 110 Train loss 5.227160 on epoch=54
03/13/2022 02:44:30 - INFO - __main__ - Step 120 Global step 120 Train loss 4.915421 on epoch=59
03/13/2022 02:44:35 - INFO - __main__ - Step 130 Global step 130 Train loss 4.182195 on epoch=64
03/13/2022 02:44:40 - INFO - __main__ - Step 140 Global step 140 Train loss 3.782542 on epoch=69
03/13/2022 02:44:45 - INFO - __main__ - Step 150 Global step 150 Train loss 3.455909 on epoch=74
03/13/2022 02:44:46 - INFO - __main__ - Global step 150 Train loss 4.312645 EM 0.0 on epoch=74
03/13/2022 02:44:52 - INFO - __main__ - Step 160 Global step 160 Train loss 2.840530 on epoch=79
03/13/2022 02:44:57 - INFO - __main__ - Step 170 Global step 170 Train loss 2.709911 on epoch=84
03/13/2022 02:45:02 - INFO - __main__ - Step 180 Global step 180 Train loss 2.458667 on epoch=89
03/13/2022 02:45:07 - INFO - __main__ - Step 190 Global step 190 Train loss 2.120719 on epoch=94
03/13/2022 02:45:12 - INFO - __main__ - Step 200 Global step 200 Train loss 2.106222 on epoch=99
03/13/2022 02:45:13 - INFO - __main__ - Global step 200 Train loss 2.447210 EM 0.0 on epoch=99
03/13/2022 02:45:18 - INFO - __main__ - Step 210 Global step 210 Train loss 1.766495 on epoch=104
03/13/2022 02:45:23 - INFO - __main__ - Step 220 Global step 220 Train loss 1.833857 on epoch=109
03/13/2022 02:45:28 - INFO - __main__ - Step 230 Global step 230 Train loss 1.485302 on epoch=114
03/13/2022 02:45:33 - INFO - __main__ - Step 240 Global step 240 Train loss 1.622460 on epoch=119
03/13/2022 02:45:38 - INFO - __main__ - Step 250 Global step 250 Train loss 1.478621 on epoch=124
03/13/2022 02:45:39 - INFO - __main__ - Global step 250 Train loss 1.637347 EM 0.0 on epoch=124
03/13/2022 02:45:44 - INFO - __main__ - Step 260 Global step 260 Train loss 1.462389 on epoch=129
03/13/2022 02:45:49 - INFO - __main__ - Step 270 Global step 270 Train loss 1.249682 on epoch=134
03/13/2022 02:45:54 - INFO - __main__ - Step 280 Global step 280 Train loss 1.249830 on epoch=139
03/13/2022 02:45:59 - INFO - __main__ - Step 290 Global step 290 Train loss 1.476404 on epoch=144
03/13/2022 02:46:04 - INFO - __main__ - Step 300 Global step 300 Train loss 1.307307 on epoch=149
03/13/2022 02:46:05 - INFO - __main__ - Global step 300 Train loss 1.349122 EM 0.0 on epoch=149
03/13/2022 02:46:10 - INFO - __main__ - Step 310 Global step 310 Train loss 1.307741 on epoch=154
03/13/2022 02:46:15 - INFO - __main__ - Step 320 Global step 320 Train loss 1.144568 on epoch=159
03/13/2022 02:46:20 - INFO - __main__ - Step 330 Global step 330 Train loss 1.286835 on epoch=164
03/13/2022 02:46:25 - INFO - __main__ - Step 340 Global step 340 Train loss 1.152128 on epoch=169
03/13/2022 02:46:30 - INFO - __main__ - Step 350 Global step 350 Train loss 1.021990 on epoch=174
03/13/2022 02:46:31 - INFO - __main__ - Global step 350 Train loss 1.182652 EM 0.0 on epoch=174
03/13/2022 02:46:36 - INFO - __main__ - Step 360 Global step 360 Train loss 1.210896 on epoch=179
03/13/2022 02:46:41 - INFO - __main__ - Step 370 Global step 370 Train loss 1.322627 on epoch=184
03/13/2022 02:46:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.903645 on epoch=189
03/13/2022 02:46:51 - INFO - __main__ - Step 390 Global step 390 Train loss 1.058756 on epoch=194
03/13/2022 02:46:56 - INFO - __main__ - Step 400 Global step 400 Train loss 1.016738 on epoch=199
03/13/2022 02:46:57 - INFO - __main__ - Global step 400 Train loss 1.102533 EM 0.0 on epoch=199
03/13/2022 02:47:02 - INFO - __main__ - Step 410 Global step 410 Train loss 1.058577 on epoch=204
03/13/2022 02:47:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.936456 on epoch=209
03/13/2022 02:47:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.980409 on epoch=214
03/13/2022 02:47:17 - INFO - __main__ - Step 440 Global step 440 Train loss 1.069659 on epoch=219
03/13/2022 02:47:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.978602 on epoch=224
03/13/2022 02:47:23 - INFO - __main__ - Global step 450 Train loss 1.004740 EM 0.0 on epoch=224
03/13/2022 02:47:28 - INFO - __main__ - Step 460 Global step 460 Train loss 1.034138 on epoch=229
03/13/2022 02:47:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.910561 on epoch=234
03/13/2022 02:47:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.756482 on epoch=239
03/13/2022 02:47:43 - INFO - __main__ - Step 490 Global step 490 Train loss 1.060144 on epoch=244
03/13/2022 02:47:48 - INFO - __main__ - Step 500 Global step 500 Train loss 1.066674 on epoch=249
03/13/2022 02:47:49 - INFO - __main__ - Global step 500 Train loss 0.965600 EM 0.0 on epoch=249
03/13/2022 02:47:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.875734 on epoch=254
03/13/2022 02:47:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.835967 on epoch=259
03/13/2022 02:48:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.847706 on epoch=264
03/13/2022 02:48:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.925503 on epoch=269
03/13/2022 02:48:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.879827 on epoch=274
03/13/2022 02:48:15 - INFO - __main__ - Global step 550 Train loss 0.872948 EM 0.0 on epoch=274
03/13/2022 02:48:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.774561 on epoch=279
03/13/2022 02:48:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.725728 on epoch=284
03/13/2022 02:48:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.733965 on epoch=289
03/13/2022 02:48:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.789755 on epoch=294
03/13/2022 02:48:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.685665 on epoch=299
03/13/2022 02:48:41 - INFO - __main__ - Global step 600 Train loss 0.741935 EM 0.0 on epoch=299
03/13/2022 02:48:41 - INFO - __main__ - save last model!
03/13/2022 02:48:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:48:41 - INFO - __main__ - Printing 3 examples
03/13/2022 02:48:41 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/13/2022 02:48:41 - INFO - __main__ - ['ok computer']
03/13/2022 02:48:41 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/13/2022 02:48:41 - INFO - __main__ - ['thursday']
03/13/2022 02:48:41 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/13/2022 02:48:41 - INFO - __main__ - ['sigourney weaver']
03/13/2022 02:48:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 02:48:41 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:48:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:48:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:48:41 - INFO - __main__ - Printing 3 examples
03/13/2022 02:48:41 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/13/2022 02:48:41 - INFO - __main__ - ['pinta island tortoise']
03/13/2022 02:48:41 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/13/2022 02:48:41 - INFO - __main__ - ['daphne du maurier']
03/13/2022 02:48:41 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/13/2022 02:48:41 - INFO - __main__ - ['back to the future']
03/13/2022 02:48:41 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:48:41 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:48:41 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:48:47 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 02:48:48 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 02:48:48 - INFO - __main__ - Printing 3 examples
03/13/2022 02:48:48 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 02:48:48 - INFO - __main__ - ['taming of the shrew']
03/13/2022 02:48:48 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 02:48:48 - INFO - __main__ - ['henry fonda']
03/13/2022 02:48:48 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 02:48:48 - INFO - __main__ - ['tchaikovsky']
03/13/2022 02:48:48 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:48:50 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:48:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:48:52 - INFO - __main__ - Starting training!
03/13/2022 02:48:54 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 02:50:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_87_0.0003_8_predictions.txt
03/13/2022 02:50:40 - INFO - __main__ - EM on test data: 0.0003
03/13/2022 02:50:41 - INFO - __main__ - prefix=freebase_qa_32_87, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.0002503755633450175
03/13/2022 02:50:41 - INFO - __main__ - Running ... prefix=freebase_qa_32_87, lr=0.0002, bsz=8 ...
03/13/2022 02:50:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:50:42 - INFO - __main__ - Printing 3 examples
03/13/2022 02:50:42 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/13/2022 02:50:42 - INFO - __main__ - ['ok computer']
03/13/2022 02:50:42 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/13/2022 02:50:42 - INFO - __main__ - ['thursday']
03/13/2022 02:50:42 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/13/2022 02:50:42 - INFO - __main__ - ['sigourney weaver']
03/13/2022 02:50:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 02:50:42 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:50:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:50:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:50:42 - INFO - __main__ - Printing 3 examples
03/13/2022 02:50:42 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/13/2022 02:50:42 - INFO - __main__ - ['pinta island tortoise']
03/13/2022 02:50:42 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/13/2022 02:50:42 - INFO - __main__ - ['daphne du maurier']
03/13/2022 02:50:42 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/13/2022 02:50:42 - INFO - __main__ - ['back to the future']
03/13/2022 02:50:42 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:50:42 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:50:42 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:50:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:50:52 - INFO - __main__ - Starting training!
03/13/2022 02:50:56 - INFO - __main__ - Step 10 Global step 10 Train loss 19.280149 on epoch=4
03/13/2022 02:51:01 - INFO - __main__ - Step 20 Global step 20 Train loss 16.242132 on epoch=9
03/13/2022 02:51:06 - INFO - __main__ - Step 30 Global step 30 Train loss 12.899327 on epoch=14
03/13/2022 02:51:11 - INFO - __main__ - Step 40 Global step 40 Train loss 11.783308 on epoch=19
03/13/2022 02:51:16 - INFO - __main__ - Step 50 Global step 50 Train loss 9.724170 on epoch=24
03/13/2022 02:51:18 - INFO - __main__ - Global step 50 Train loss 13.985818 EM 0.0 on epoch=24
03/13/2022 02:51:24 - INFO - __main__ - Step 60 Global step 60 Train loss 8.643857 on epoch=29
03/13/2022 02:51:29 - INFO - __main__ - Step 70 Global step 70 Train loss 7.359014 on epoch=34
03/13/2022 02:51:34 - INFO - __main__ - Step 80 Global step 80 Train loss 6.417716 on epoch=39
03/13/2022 02:51:39 - INFO - __main__ - Step 90 Global step 90 Train loss 5.827487 on epoch=44
03/13/2022 02:51:44 - INFO - __main__ - Step 100 Global step 100 Train loss 5.595429 on epoch=49
03/13/2022 02:51:45 - INFO - __main__ - Global step 100 Train loss 6.768701 EM 0.0 on epoch=49
03/13/2022 02:51:50 - INFO - __main__ - Step 110 Global step 110 Train loss 4.908053 on epoch=54
03/13/2022 02:51:54 - INFO - __main__ - Step 120 Global step 120 Train loss 4.938562 on epoch=59
03/13/2022 02:51:59 - INFO - __main__ - Step 130 Global step 130 Train loss 4.410528 on epoch=64
03/13/2022 02:52:04 - INFO - __main__ - Step 140 Global step 140 Train loss 4.035619 on epoch=69
03/13/2022 02:52:09 - INFO - __main__ - Step 150 Global step 150 Train loss 3.444677 on epoch=74
03/13/2022 02:52:10 - INFO - __main__ - Global step 150 Train loss 4.347487 EM 0.0 on epoch=74
03/13/2022 02:52:15 - INFO - __main__ - Step 160 Global step 160 Train loss 3.747883 on epoch=79
03/13/2022 02:52:20 - INFO - __main__ - Step 170 Global step 170 Train loss 3.669141 on epoch=84
03/13/2022 02:52:25 - INFO - __main__ - Step 180 Global step 180 Train loss 3.230409 on epoch=89
03/13/2022 02:52:30 - INFO - __main__ - Step 190 Global step 190 Train loss 3.079982 on epoch=94
03/13/2022 02:52:35 - INFO - __main__ - Step 200 Global step 200 Train loss 2.540040 on epoch=99
03/13/2022 02:52:35 - INFO - __main__ - Global step 200 Train loss 3.253491 EM 0.0 on epoch=99
03/13/2022 02:52:40 - INFO - __main__ - Step 210 Global step 210 Train loss 2.545429 on epoch=104
03/13/2022 02:52:45 - INFO - __main__ - Step 220 Global step 220 Train loss 2.410427 on epoch=109
03/13/2022 02:52:50 - INFO - __main__ - Step 230 Global step 230 Train loss 2.381704 on epoch=114
03/13/2022 02:52:55 - INFO - __main__ - Step 240 Global step 240 Train loss 2.625529 on epoch=119
03/13/2022 02:53:00 - INFO - __main__ - Step 250 Global step 250 Train loss 2.533795 on epoch=124
03/13/2022 02:53:01 - INFO - __main__ - Global step 250 Train loss 2.499377 EM 0.0 on epoch=124
03/13/2022 02:53:05 - INFO - __main__ - Step 260 Global step 260 Train loss 2.152516 on epoch=129
03/13/2022 02:53:10 - INFO - __main__ - Step 270 Global step 270 Train loss 2.220917 on epoch=134
03/13/2022 02:53:15 - INFO - __main__ - Step 280 Global step 280 Train loss 1.988965 on epoch=139
03/13/2022 02:53:20 - INFO - __main__ - Step 290 Global step 290 Train loss 1.796339 on epoch=144
03/13/2022 02:53:25 - INFO - __main__ - Step 300 Global step 300 Train loss 1.680226 on epoch=149
03/13/2022 02:53:26 - INFO - __main__ - Global step 300 Train loss 1.967793 EM 0.0 on epoch=149
03/13/2022 02:53:31 - INFO - __main__ - Step 310 Global step 310 Train loss 1.837339 on epoch=154
03/13/2022 02:53:35 - INFO - __main__ - Step 320 Global step 320 Train loss 1.618006 on epoch=159
03/13/2022 02:53:40 - INFO - __main__ - Step 330 Global step 330 Train loss 1.692080 on epoch=164
03/13/2022 02:53:45 - INFO - __main__ - Step 340 Global step 340 Train loss 1.742896 on epoch=169
03/13/2022 02:53:50 - INFO - __main__ - Step 350 Global step 350 Train loss 1.468867 on epoch=174
03/13/2022 02:53:51 - INFO - __main__ - Global step 350 Train loss 1.671837 EM 0.0 on epoch=174
03/13/2022 02:53:56 - INFO - __main__ - Step 360 Global step 360 Train loss 1.547996 on epoch=179
03/13/2022 02:54:01 - INFO - __main__ - Step 370 Global step 370 Train loss 1.462571 on epoch=184
03/13/2022 02:54:05 - INFO - __main__ - Step 380 Global step 380 Train loss 1.420232 on epoch=189
03/13/2022 02:54:10 - INFO - __main__ - Step 390 Global step 390 Train loss 1.497290 on epoch=194
03/13/2022 02:54:15 - INFO - __main__ - Step 400 Global step 400 Train loss 1.379769 on epoch=199
03/13/2022 02:54:16 - INFO - __main__ - Global step 400 Train loss 1.461572 EM 0.0 on epoch=199
03/13/2022 02:54:21 - INFO - __main__ - Step 410 Global step 410 Train loss 1.372710 on epoch=204
03/13/2022 02:54:26 - INFO - __main__ - Step 420 Global step 420 Train loss 1.317984 on epoch=209
03/13/2022 02:54:31 - INFO - __main__ - Step 430 Global step 430 Train loss 1.450926 on epoch=214
03/13/2022 02:54:36 - INFO - __main__ - Step 440 Global step 440 Train loss 1.355923 on epoch=219
03/13/2022 02:54:41 - INFO - __main__ - Step 450 Global step 450 Train loss 1.286516 on epoch=224
03/13/2022 02:54:42 - INFO - __main__ - Global step 450 Train loss 1.356812 EM 0.0 on epoch=224
03/13/2022 02:54:47 - INFO - __main__ - Step 460 Global step 460 Train loss 1.539229 on epoch=229
03/13/2022 02:54:52 - INFO - __main__ - Step 470 Global step 470 Train loss 1.311210 on epoch=234
03/13/2022 02:54:57 - INFO - __main__ - Step 480 Global step 480 Train loss 1.190839 on epoch=239
03/13/2022 02:55:01 - INFO - __main__ - Step 490 Global step 490 Train loss 1.264855 on epoch=244
03/13/2022 02:55:06 - INFO - __main__ - Step 500 Global step 500 Train loss 1.177752 on epoch=249
03/13/2022 02:55:07 - INFO - __main__ - Global step 500 Train loss 1.296777 EM 0.0 on epoch=249
03/13/2022 02:55:12 - INFO - __main__ - Step 510 Global step 510 Train loss 1.134457 on epoch=254
03/13/2022 02:55:17 - INFO - __main__ - Step 520 Global step 520 Train loss 1.274125 on epoch=259
03/13/2022 02:55:22 - INFO - __main__ - Step 530 Global step 530 Train loss 1.246241 on epoch=264
03/13/2022 02:55:27 - INFO - __main__ - Step 540 Global step 540 Train loss 1.139944 on epoch=269
03/13/2022 02:55:32 - INFO - __main__ - Step 550 Global step 550 Train loss 1.116207 on epoch=274
03/13/2022 02:55:33 - INFO - __main__ - Global step 550 Train loss 1.182195 EM 0.0 on epoch=274
03/13/2022 02:55:37 - INFO - __main__ - Step 560 Global step 560 Train loss 1.071631 on epoch=279
03/13/2022 02:55:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.940196 on epoch=284
03/13/2022 02:55:47 - INFO - __main__ - Step 580 Global step 580 Train loss 1.054330 on epoch=289
03/13/2022 02:55:52 - INFO - __main__ - Step 590 Global step 590 Train loss 1.039939 on epoch=294
03/13/2022 02:55:57 - INFO - __main__ - Step 600 Global step 600 Train loss 1.055422 on epoch=299
03/13/2022 02:55:58 - INFO - __main__ - Global step 600 Train loss 1.032304 EM 0.0 on epoch=299
03/13/2022 02:55:58 - INFO - __main__ - save last model!
03/13/2022 02:55:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:55:58 - INFO - __main__ - Printing 3 examples
03/13/2022 02:55:58 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/13/2022 02:55:58 - INFO - __main__ - ['ok computer']
03/13/2022 02:55:58 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/13/2022 02:55:58 - INFO - __main__ - ['thursday']
03/13/2022 02:55:58 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/13/2022 02:55:58 - INFO - __main__ - ['sigourney weaver']
03/13/2022 02:55:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 02:55:58 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:55:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 02:55:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 02:55:58 - INFO - __main__ - Printing 3 examples
03/13/2022 02:55:58 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/13/2022 02:55:58 - INFO - __main__ - ['pinta island tortoise']
03/13/2022 02:55:58 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/13/2022 02:55:58 - INFO - __main__ - ['daphne du maurier']
03/13/2022 02:55:58 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/13/2022 02:55:58 - INFO - __main__ - ['back to the future']
03/13/2022 02:55:58 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:55:58 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:55:59 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 02:56:05 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 02:56:06 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 02:56:06 - INFO - __main__ - Printing 3 examples
03/13/2022 02:56:06 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 02:56:06 - INFO - __main__ - ['taming of the shrew']
03/13/2022 02:56:06 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 02:56:06 - INFO - __main__ - ['henry fonda']
03/13/2022 02:56:06 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 02:56:06 - INFO - __main__ - ['tchaikovsky']
03/13/2022 02:56:06 - INFO - __main__ - Tokenizing Input ...
03/13/2022 02:56:07 - INFO - __main__ - Tokenizing Output ...
03/13/2022 02:56:11 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 02:56:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 02:56:11 - INFO - __main__ - Starting training!
03/13/2022 03:02:58 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_87_0.0002_8_predictions.txt
03/13/2022 03:02:58 - INFO - __main__ - EM on test data: 0.0008
03/13/2022 03:02:58 - INFO - __main__ - prefix=freebase_qa_32_87, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.0007511266900350526
03/13/2022 03:02:58 - INFO - __main__ - Running ... prefix=freebase_qa_32_87, lr=0.0001, bsz=8 ...
03/13/2022 03:02:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 03:02:59 - INFO - __main__ - Printing 3 examples
03/13/2022 03:02:59 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/13/2022 03:02:59 - INFO - __main__ - ['ok computer']
03/13/2022 03:02:59 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/13/2022 03:02:59 - INFO - __main__ - ['thursday']
03/13/2022 03:02:59 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/13/2022 03:02:59 - INFO - __main__ - ['sigourney weaver']
03/13/2022 03:02:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 03:02:59 - INFO - __main__ - Tokenizing Output ...
03/13/2022 03:02:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 03:02:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 03:02:59 - INFO - __main__ - Printing 3 examples
03/13/2022 03:02:59 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/13/2022 03:02:59 - INFO - __main__ - ['pinta island tortoise']
03/13/2022 03:02:59 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/13/2022 03:02:59 - INFO - __main__ - ['daphne du maurier']
03/13/2022 03:02:59 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/13/2022 03:02:59 - INFO - __main__ - ['back to the future']
03/13/2022 03:02:59 - INFO - __main__ - Tokenizing Input ...
03/13/2022 03:02:59 - INFO - __main__ - Tokenizing Output ...
03/13/2022 03:02:59 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 03:03:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 03:03:12 - INFO - __main__ - Starting training!
03/13/2022 03:03:16 - INFO - __main__ - Step 10 Global step 10 Train loss 19.427010 on epoch=4
03/13/2022 03:03:22 - INFO - __main__ - Step 20 Global step 20 Train loss 17.765856 on epoch=9
03/13/2022 03:03:27 - INFO - __main__ - Step 30 Global step 30 Train loss 14.377902 on epoch=14
03/13/2022 03:03:32 - INFO - __main__ - Step 40 Global step 40 Train loss 12.004226 on epoch=19
03/13/2022 03:03:37 - INFO - __main__ - Step 50 Global step 50 Train loss 11.647676 on epoch=24
03/13/2022 03:03:45 - INFO - __main__ - Global step 50 Train loss 15.044535 EM 0.0 on epoch=24
03/13/2022 03:03:51 - INFO - __main__ - Step 60 Global step 60 Train loss 10.757505 on epoch=29
03/13/2022 03:03:56 - INFO - __main__ - Step 70 Global step 70 Train loss 9.803313 on epoch=34
03/13/2022 03:04:01 - INFO - __main__ - Step 80 Global step 80 Train loss 9.044086 on epoch=39
03/13/2022 03:04:06 - INFO - __main__ - Step 90 Global step 90 Train loss 8.485377 on epoch=44
03/13/2022 03:04:11 - INFO - __main__ - Step 100 Global step 100 Train loss 7.861635 on epoch=49
03/13/2022 03:04:13 - INFO - __main__ - Global step 100 Train loss 9.190383 EM 0.0 on epoch=49
03/13/2022 03:04:19 - INFO - __main__ - Step 110 Global step 110 Train loss 7.828353 on epoch=54
03/13/2022 03:04:24 - INFO - __main__ - Step 120 Global step 120 Train loss 7.437152 on epoch=59
03/13/2022 03:04:29 - INFO - __main__ - Step 130 Global step 130 Train loss 6.491985 on epoch=64
03/13/2022 03:04:34 - INFO - __main__ - Step 140 Global step 140 Train loss 6.559664 on epoch=69
03/13/2022 03:04:39 - INFO - __main__ - Step 150 Global step 150 Train loss 6.291701 on epoch=74
03/13/2022 03:04:41 - INFO - __main__ - Global step 150 Train loss 6.921771 EM 0.0 on epoch=74
03/13/2022 03:04:46 - INFO - __main__ - Step 160 Global step 160 Train loss 5.769382 on epoch=79
03/13/2022 03:04:51 - INFO - __main__ - Step 170 Global step 170 Train loss 5.624627 on epoch=84
03/13/2022 03:04:56 - INFO - __main__ - Step 180 Global step 180 Train loss 5.465472 on epoch=89
03/13/2022 03:05:01 - INFO - __main__ - Step 190 Global step 190 Train loss 5.198761 on epoch=94
03/13/2022 03:05:06 - INFO - __main__ - Step 200 Global step 200 Train loss 4.846466 on epoch=99
03/13/2022 03:05:08 - INFO - __main__ - Global step 200 Train loss 5.380941 EM 0.0 on epoch=99
03/13/2022 03:05:13 - INFO - __main__ - Step 210 Global step 210 Train loss 4.463162 on epoch=104
03/13/2022 03:05:18 - INFO - __main__ - Step 220 Global step 220 Train loss 4.270654 on epoch=109
03/13/2022 03:05:23 - INFO - __main__ - Step 230 Global step 230 Train loss 4.525218 on epoch=114
03/13/2022 03:05:28 - INFO - __main__ - Step 240 Global step 240 Train loss 4.043525 on epoch=119
03/13/2022 03:05:33 - INFO - __main__ - Step 250 Global step 250 Train loss 4.658610 on epoch=124
03/13/2022 03:05:35 - INFO - __main__ - Global step 250 Train loss 4.392234 EM 0.0 on epoch=124
03/13/2022 03:05:40 - INFO - __main__ - Step 260 Global step 260 Train loss 4.796770 on epoch=129
03/13/2022 03:05:45 - INFO - __main__ - Step 270 Global step 270 Train loss 4.086287 on epoch=134
03/13/2022 03:05:50 - INFO - __main__ - Step 280 Global step 280 Train loss 3.920797 on epoch=139
03/13/2022 03:05:55 - INFO - __main__ - Step 290 Global step 290 Train loss 3.849405 on epoch=144
03/13/2022 03:06:00 - INFO - __main__ - Step 300 Global step 300 Train loss 3.531738 on epoch=149
03/13/2022 03:06:02 - INFO - __main__ - Global step 300 Train loss 4.037000 EM 0.0 on epoch=149
03/13/2022 03:06:07 - INFO - __main__ - Step 310 Global step 310 Train loss 3.452266 on epoch=154
03/13/2022 03:06:12 - INFO - __main__ - Step 320 Global step 320 Train loss 3.797883 on epoch=159
03/13/2022 03:06:17 - INFO - __main__ - Step 330 Global step 330 Train loss 3.484015 on epoch=164
03/13/2022 03:06:22 - INFO - __main__ - Step 340 Global step 340 Train loss 3.574284 on epoch=169
03/13/2022 03:06:27 - INFO - __main__ - Step 350 Global step 350 Train loss 3.507544 on epoch=174
03/13/2022 03:06:28 - INFO - __main__ - Global step 350 Train loss 3.563198 EM 0.0 on epoch=174
03/13/2022 03:06:33 - INFO - __main__ - Step 360 Global step 360 Train loss 3.280455 on epoch=179
03/13/2022 03:06:38 - INFO - __main__ - Step 370 Global step 370 Train loss 3.425673 on epoch=184
03/13/2022 03:06:44 - INFO - __main__ - Step 380 Global step 380 Train loss 3.028971 on epoch=189
03/13/2022 03:06:49 - INFO - __main__ - Step 390 Global step 390 Train loss 3.486178 on epoch=194
03/13/2022 03:06:54 - INFO - __main__ - Step 400 Global step 400 Train loss 3.037928 on epoch=199
03/13/2022 03:06:55 - INFO - __main__ - Global step 400 Train loss 3.251841 EM 0.0 on epoch=199
03/13/2022 03:07:00 - INFO - __main__ - Step 410 Global step 410 Train loss 3.222436 on epoch=204
03/13/2022 03:07:05 - INFO - __main__ - Step 420 Global step 420 Train loss 3.018681 on epoch=209
03/13/2022 03:07:10 - INFO - __main__ - Step 430 Global step 430 Train loss 2.830933 on epoch=214
03/13/2022 03:07:15 - INFO - __main__ - Step 440 Global step 440 Train loss 2.669418 on epoch=219
03/13/2022 03:07:20 - INFO - __main__ - Step 450 Global step 450 Train loss 2.669140 on epoch=224
03/13/2022 03:07:21 - INFO - __main__ - Global step 450 Train loss 2.882121 EM 0.0 on epoch=224
03/13/2022 03:07:26 - INFO - __main__ - Step 460 Global step 460 Train loss 2.779222 on epoch=229
03/13/2022 03:07:32 - INFO - __main__ - Step 470 Global step 470 Train loss 2.426879 on epoch=234
03/13/2022 03:07:37 - INFO - __main__ - Step 480 Global step 480 Train loss 2.446698 on epoch=239
03/13/2022 03:07:42 - INFO - __main__ - Step 490 Global step 490 Train loss 2.361537 on epoch=244
03/13/2022 03:07:47 - INFO - __main__ - Step 500 Global step 500 Train loss 2.244475 on epoch=249
03/13/2022 03:07:48 - INFO - __main__ - Global step 500 Train loss 2.451762 EM 0.0 on epoch=249
03/13/2022 03:07:53 - INFO - __main__ - Step 510 Global step 510 Train loss 2.474133 on epoch=254
03/13/2022 03:07:58 - INFO - __main__ - Step 520 Global step 520 Train loss 2.222338 on epoch=259
03/13/2022 03:08:03 - INFO - __main__ - Step 530 Global step 530 Train loss 2.187944 on epoch=264
03/13/2022 03:08:08 - INFO - __main__ - Step 540 Global step 540 Train loss 2.062222 on epoch=269
03/13/2022 03:08:13 - INFO - __main__ - Step 550 Global step 550 Train loss 2.283453 on epoch=274
03/13/2022 03:08:14 - INFO - __main__ - Global step 550 Train loss 2.246018 EM 0.0 on epoch=274
03/13/2022 03:08:19 - INFO - __main__ - Step 560 Global step 560 Train loss 1.968182 on epoch=279
03/13/2022 03:08:24 - INFO - __main__ - Step 570 Global step 570 Train loss 1.804830 on epoch=284
03/13/2022 03:08:30 - INFO - __main__ - Step 580 Global step 580 Train loss 1.884342 on epoch=289
03/13/2022 03:08:35 - INFO - __main__ - Step 590 Global step 590 Train loss 1.862288 on epoch=294
03/13/2022 03:08:40 - INFO - __main__ - Step 600 Global step 600 Train loss 1.800812 on epoch=299
03/13/2022 03:08:41 - INFO - __main__ - Global step 600 Train loss 1.864091 EM 0.0 on epoch=299
03/13/2022 03:08:41 - INFO - __main__ - save last model!
03/13/2022 03:08:47 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 03:08:48 - INFO - __main__ - Start tokenizing ... 3994 instances
03/13/2022 03:08:48 - INFO - __main__ - Printing 3 examples
03/13/2022 03:08:48 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/13/2022 03:08:48 - INFO - __main__ - ['taming of the shrew']
03/13/2022 03:08:48 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/13/2022 03:08:48 - INFO - __main__ - ['henry fonda']
03/13/2022 03:08:48 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/13/2022 03:08:48 - INFO - __main__ - ['tchaikovsky']
03/13/2022 03:08:48 - INFO - __main__ - Tokenizing Input ...
03/13/2022 03:08:50 - INFO - __main__ - Tokenizing Output ...
03/13/2022 03:08:54 - INFO - __main__ - Loaded 3994 examples from test data
03/13/2022 03:25:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-freebase_qa/freebase_qa_32_87_0.0001_8_predictions.txt
03/13/2022 03:25:35 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 03:25:35 - INFO - __main__ - prefix=freebase_qa_32_87, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.0
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (16715): No such process
Task: break-QDMR, Checkpoint: None, Identifier: T5-large-ft-random
Output directory () already exists and is not empty.
03/13/2022 03:25:41 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/13/2022 03:25:41 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/13/2022 03:25:41 - INFO - __main__ - models/T5-large-ft-random/singletask-break-QDMR
03/13/2022 03:25:41 - INFO - __main__ - models/T5-large-ft-random/singletask-break-QDMR
03/13/2022 03:25:42 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/13/2022 03:25:42 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/13/2022 03:25:42 - INFO - __main__ - args.device: cuda:0
03/13/2022 03:25:42 - INFO - __main__ - Using 2 gpus
03/13/2022 03:25:42 - INFO - __main__ - args.device: cuda:1
03/13/2022 03:25:42 - INFO - __main__ - Using 2 gpus
03/13/2022 03:25:42 - INFO - __main__ - Fine-tuning the following samples: ['break-QDMR_32_100', 'break-QDMR_32_13', 'break-QDMR_32_21', 'break-QDMR_32_42', 'break-QDMR_32_87']
03/13/2022 03:25:42 - INFO - __main__ - Fine-tuning the following samples: ['break-QDMR_32_100', 'break-QDMR_32_13', 'break-QDMR_32_21', 'break-QDMR_32_42', 'break-QDMR_32_87']
03/13/2022 03:25:47 - INFO - __main__ - Running ... prefix=break-QDMR_32_100, lr=0.0005, bsz=8 ...
03/13/2022 03:25:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 03:25:48 - INFO - __main__ - Printing 3 examples
03/13/2022 03:25:48 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/13/2022 03:25:48 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/13/2022 03:25:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 03:25:48 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/13/2022 03:25:48 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/13/2022 03:25:48 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/13/2022 03:25:48 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/13/2022 03:25:48 - INFO - __main__ - Tokenizing Input ...
03/13/2022 03:25:48 - INFO - __main__ - Printing 3 examples
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 03:25:48 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/13/2022 03:25:48 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/13/2022 03:25:48 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/13/2022 03:25:48 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/13/2022 03:25:48 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/13/2022 03:25:48 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/13/2022 03:25:48 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 03:25:48 - INFO - __main__ - Tokenizing Output ...
03/13/2022 03:25:48 - INFO - __main__ - Tokenizing Output ...
03/13/2022 03:25:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 03:25:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 03:25:48 - INFO - __main__ - Printing 3 examples
03/13/2022 03:25:48 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/13/2022 03:25:48 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/13/2022 03:25:48 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/13/2022 03:25:48 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/13/2022 03:25:48 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/13/2022 03:25:48 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/13/2022 03:25:48 - INFO - __main__ - Tokenizing Input ...
03/13/2022 03:25:48 - INFO - __main__ - Tokenizing Output ...
03/13/2022 03:25:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 03:25:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 03:25:48 - INFO - __main__ - Printing 3 examples
03/13/2022 03:25:48 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/13/2022 03:25:48 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/13/2022 03:25:48 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/13/2022 03:25:48 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/13/2022 03:25:48 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/13/2022 03:25:48 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/13/2022 03:25:48 - INFO - __main__ - Tokenizing Input ...
03/13/2022 03:25:48 - INFO - __main__ - Tokenizing Output ...
03/13/2022 03:25:48 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 03:25:48 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 03:26:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 03:26:01 - INFO - __main__ - Starting training!
03/13/2022 03:26:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 03:26:01 - INFO - __main__ - Starting training!
03/13/2022 03:26:06 - INFO - __main__ - Step 10 Global step 10 Train loss 21.089163 on epoch=4
03/13/2022 03:26:11 - INFO - __main__ - Step 20 Global step 20 Train loss 11.124393 on epoch=9
03/13/2022 03:26:16 - INFO - __main__ - Step 30 Global step 30 Train loss 3.178520 on epoch=14
03/13/2022 03:26:21 - INFO - __main__ - Step 40 Global step 40 Train loss 2.043065 on epoch=19
03/13/2022 03:26:25 - INFO - __main__ - Step 50 Global step 50 Train loss 1.364674 on epoch=24
03/13/2022 03:26:37 - INFO - __main__ - Global step 50 Train loss 7.759963 EM 0.0 on epoch=24
03/13/2022 03:26:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.962048 on epoch=29
03/13/2022 03:26:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.700986 on epoch=34
03/13/2022 03:26:53 - INFO - __main__ - Step 80 Global step 80 Train loss 0.436522 on epoch=39
03/13/2022 03:26:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.329118 on epoch=44
03/13/2022 03:27:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.259774 on epoch=49
03/13/2022 03:27:13 - INFO - __main__ - Global step 100 Train loss 0.537690 EM 0.0 on epoch=49
03/13/2022 03:27:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.181323 on epoch=54
03/13/2022 03:27:23 - INFO - __main__ - Step 120 Global step 120 Train loss 0.149153 on epoch=59
03/13/2022 03:27:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.147353 on epoch=64
03/13/2022 03:27:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.119874 on epoch=69
03/13/2022 03:27:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.087445 on epoch=74
03/13/2022 03:27:48 - INFO - __main__ - Global step 150 Train loss 0.137030 EM 0.0 on epoch=74
03/13/2022 03:27:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.114851 on epoch=79
03/13/2022 03:27:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.093540 on epoch=84
03/13/2022 03:28:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.060072 on epoch=89
03/13/2022 03:28:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.074652 on epoch=94
03/13/2022 03:28:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.070348 on epoch=99
03/13/2022 03:28:23 - INFO - __main__ - Global step 200 Train loss 0.082693 EM 0.0 on epoch=99
03/13/2022 03:28:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.072845 on epoch=104
03/13/2022 03:28:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.045171 on epoch=109
03/13/2022 03:28:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.051875 on epoch=114
03/13/2022 03:28:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.056309 on epoch=119
03/13/2022 03:28:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.032974 on epoch=124
03/13/2022 03:28:58 - INFO - __main__ - Global step 250 Train loss 0.051835 EM 0.0 on epoch=124
03/13/2022 03:29:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.041545 on epoch=129
03/13/2022 03:29:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.036592 on epoch=134
03/13/2022 03:29:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.063548 on epoch=139
03/13/2022 03:29:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.046537 on epoch=144
03/13/2022 03:29:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.064147 on epoch=149
03/13/2022 03:29:32 - INFO - __main__ - Global step 300 Train loss 0.050474 EM 0.0 on epoch=149
03/13/2022 03:29:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.031460 on epoch=154
03/13/2022 03:29:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.015895 on epoch=159
03/13/2022 03:29:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.029972 on epoch=164
03/13/2022 03:29:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.029249 on epoch=169
03/13/2022 03:29:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.009843 on epoch=174
03/13/2022 03:30:07 - INFO - __main__ - Global step 350 Train loss 0.023284 EM 0.0 on epoch=174
03/13/2022 03:30:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.010786 on epoch=179
03/13/2022 03:30:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.006790 on epoch=184
03/13/2022 03:30:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.022091 on epoch=189
03/13/2022 03:30:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.012486 on epoch=194
03/13/2022 03:30:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.023716 on epoch=199
03/13/2022 03:30:42 - INFO - __main__ - Global step 400 Train loss 0.015174 EM 0.0 on epoch=199
03/13/2022 03:30:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.014186 on epoch=204
03/13/2022 03:30:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.007439 on epoch=209
03/13/2022 03:30:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.004814 on epoch=214
03/13/2022 03:31:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.005992 on epoch=219
03/13/2022 03:31:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.011421 on epoch=224
03/13/2022 03:31:17 - INFO - __main__ - Global step 450 Train loss 0.008770 EM 0.0 on epoch=224
03/13/2022 03:31:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.007666 on epoch=229
03/13/2022 03:31:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.004709 on epoch=234
03/13/2022 03:31:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004871 on epoch=239
03/13/2022 03:31:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001619 on epoch=244
03/13/2022 03:31:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.006175 on epoch=249
03/13/2022 03:31:51 - INFO - __main__ - Global step 500 Train loss 0.005008 EM 0.0 on epoch=249
03/13/2022 03:31:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.018571 on epoch=254
03/13/2022 03:32:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.006268 on epoch=259
03/13/2022 03:32:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.002446 on epoch=264
03/13/2022 03:32:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000718 on epoch=269
03/13/2022 03:32:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004485 on epoch=274
03/13/2022 03:32:26 - INFO - __main__ - Global step 550 Train loss 0.006498 EM 0.0 on epoch=274
03/13/2022 03:32:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000557 on epoch=279
03/13/2022 03:32:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004033 on epoch=284
03/13/2022 03:32:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001779 on epoch=289
03/13/2022 03:32:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002609 on epoch=294
03/13/2022 03:32:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000643 on epoch=299
03/13/2022 03:32:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 03:32:53 - INFO - __main__ - Printing 3 examples
03/13/2022 03:32:53 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/13/2022 03:32:53 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/13/2022 03:32:53 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/13/2022 03:32:53 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/13/2022 03:32:53 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/13/2022 03:32:53 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/13/2022 03:32:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 03:32:53 - INFO - __main__ - Tokenizing Output ...
03/13/2022 03:32:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 03:32:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 03:32:53 - INFO - __main__ - Printing 3 examples
03/13/2022 03:32:53 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/13/2022 03:32:53 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/13/2022 03:32:53 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/13/2022 03:32:53 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/13/2022 03:32:53 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/13/2022 03:32:53 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/13/2022 03:32:53 - INFO - __main__ - Tokenizing Input ...
03/13/2022 03:32:53 - INFO - __main__ - Tokenizing Output ...
03/13/2022 03:32:53 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 03:33:01 - INFO - __main__ - Global step 600 Train loss 0.001924 EM 0.0 on epoch=299
03/13/2022 03:33:01 - INFO - __main__ - save last model!
03/13/2022 03:33:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 03:33:04 - INFO - __main__ - Starting training!
03/13/2022 03:33:11 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 03:33:12 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 03:33:12 - INFO - __main__ - Printing 3 examples
03/13/2022 03:33:12 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 03:33:12 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 03:33:12 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 03:33:12 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 03:33:12 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 03:33:12 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 03:33:12 - INFO - __main__ - Tokenizing Input ...
03/13/2022 03:33:15 - INFO - __main__ - Tokenizing Output ...
03/13/2022 03:33:23 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 04:14:36 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_100_0.0005_8_predictions.txt
03/13/2022 04:14:36 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 04:14:37 - INFO - __main__ - prefix=break-QDMR_32_100, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 04:14:37 - INFO - __main__ - Running ... prefix=break-QDMR_32_100, lr=0.0003, bsz=8 ...
03/13/2022 04:14:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 04:14:38 - INFO - __main__ - Printing 3 examples
03/13/2022 04:14:38 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/13/2022 04:14:38 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/13/2022 04:14:38 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/13/2022 04:14:38 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/13/2022 04:14:38 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/13/2022 04:14:38 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/13/2022 04:14:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 04:14:38 - INFO - __main__ - Tokenizing Output ...
03/13/2022 04:14:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 04:14:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 04:14:38 - INFO - __main__ - Printing 3 examples
03/13/2022 04:14:38 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/13/2022 04:14:38 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/13/2022 04:14:38 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/13/2022 04:14:38 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/13/2022 04:14:38 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/13/2022 04:14:38 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/13/2022 04:14:38 - INFO - __main__ - Tokenizing Input ...
03/13/2022 04:14:38 - INFO - __main__ - Tokenizing Output ...
03/13/2022 04:14:38 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 04:14:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 04:14:49 - INFO - __main__ - Starting training!
03/13/2022 04:14:54 - INFO - __main__ - Step 10 Global step 10 Train loss 20.942789 on epoch=4
03/13/2022 04:14:59 - INFO - __main__ - Step 20 Global step 20 Train loss 16.328938 on epoch=9
03/13/2022 04:15:04 - INFO - __main__ - Step 30 Global step 30 Train loss 8.426278 on epoch=14
03/13/2022 04:15:09 - INFO - __main__ - Step 40 Global step 40 Train loss 6.256832 on epoch=19
03/13/2022 04:15:14 - INFO - __main__ - Step 50 Global step 50 Train loss 5.582806 on epoch=24
03/13/2022 04:15:27 - INFO - __main__ - Global step 50 Train loss 11.507528 EM 0.0 on epoch=24
03/13/2022 04:15:33 - INFO - __main__ - Step 60 Global step 60 Train loss 5.102548 on epoch=29
03/13/2022 04:15:38 - INFO - __main__ - Step 70 Global step 70 Train loss 4.517020 on epoch=34
03/13/2022 04:15:43 - INFO - __main__ - Step 80 Global step 80 Train loss 3.265490 on epoch=39
03/13/2022 04:15:48 - INFO - __main__ - Step 90 Global step 90 Train loss 2.604666 on epoch=44
03/13/2022 04:15:53 - INFO - __main__ - Step 100 Global step 100 Train loss 1.921681 on epoch=49
03/13/2022 04:16:03 - INFO - __main__ - Global step 100 Train loss 3.482281 EM 0.0 on epoch=49
03/13/2022 04:16:08 - INFO - __main__ - Step 110 Global step 110 Train loss 1.571337 on epoch=54
03/13/2022 04:16:13 - INFO - __main__ - Step 120 Global step 120 Train loss 1.179308 on epoch=59
03/13/2022 04:16:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.942125 on epoch=64
03/13/2022 04:16:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.777559 on epoch=69
03/13/2022 04:16:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.659139 on epoch=74
03/13/2022 04:16:39 - INFO - __main__ - Global step 150 Train loss 1.025893 EM 0.0 on epoch=74
03/13/2022 04:16:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.555738 on epoch=79
03/13/2022 04:16:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.503270 on epoch=84
03/13/2022 04:16:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.409521 on epoch=89
03/13/2022 04:16:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.395804 on epoch=94
03/13/2022 04:17:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.277246 on epoch=99
03/13/2022 04:17:14 - INFO - __main__ - Global step 200 Train loss 0.428316 EM 0.0 on epoch=99
03/13/2022 04:17:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.263250 on epoch=104
03/13/2022 04:17:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.187220 on epoch=109
03/13/2022 04:17:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.195252 on epoch=114
03/13/2022 04:17:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.154578 on epoch=119
03/13/2022 04:17:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.152739 on epoch=124
03/13/2022 04:17:48 - INFO - __main__ - Global step 250 Train loss 0.190608 EM 0.0 on epoch=124
03/13/2022 04:17:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.175208 on epoch=129
03/13/2022 04:17:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.138067 on epoch=134
03/13/2022 04:18:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.196662 on epoch=139
03/13/2022 04:18:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.156872 on epoch=144
03/13/2022 04:18:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.137217 on epoch=149
03/13/2022 04:18:23 - INFO - __main__ - Global step 300 Train loss 0.160805 EM 0.0 on epoch=149
03/13/2022 04:18:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.135183 on epoch=154
03/13/2022 04:18:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.143224 on epoch=159
03/13/2022 04:18:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.082044 on epoch=164
03/13/2022 04:18:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.072852 on epoch=169
03/13/2022 04:18:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.095034 on epoch=174
03/13/2022 04:18:57 - INFO - __main__ - Global step 350 Train loss 0.105667 EM 0.0 on epoch=174
03/13/2022 04:19:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.080441 on epoch=179
03/13/2022 04:19:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.065486 on epoch=184
03/13/2022 04:19:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.070018 on epoch=189
03/13/2022 04:19:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.055250 on epoch=194
03/13/2022 04:19:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.035731 on epoch=199
03/13/2022 04:19:32 - INFO - __main__ - Global step 400 Train loss 0.061385 EM 0.0 on epoch=199
03/13/2022 04:19:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.043118 on epoch=204
03/13/2022 04:19:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.065473 on epoch=209
03/13/2022 04:19:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.057489 on epoch=214
03/13/2022 04:19:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.068138 on epoch=219
03/13/2022 04:19:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.036412 on epoch=224
03/13/2022 04:20:06 - INFO - __main__ - Global step 450 Train loss 0.054126 EM 0.0 on epoch=224
03/13/2022 04:20:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.038897 on epoch=229
03/13/2022 04:20:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.042377 on epoch=234
03/13/2022 04:20:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.034640 on epoch=239
03/13/2022 04:20:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.022093 on epoch=244
03/13/2022 04:20:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.038508 on epoch=249
03/13/2022 04:20:41 - INFO - __main__ - Global step 500 Train loss 0.035303 EM 0.0 on epoch=249
03/13/2022 04:20:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.024572 on epoch=254
03/13/2022 04:20:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.026992 on epoch=259
03/13/2022 04:20:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.012858 on epoch=264
03/13/2022 04:21:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.022841 on epoch=269
03/13/2022 04:21:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.022056 on epoch=274
03/13/2022 04:21:15 - INFO - __main__ - Global step 550 Train loss 0.021864 EM 0.0 on epoch=274
03/13/2022 04:21:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.021664 on epoch=279
03/13/2022 04:21:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.014333 on epoch=284
03/13/2022 04:21:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.020133 on epoch=289
03/13/2022 04:21:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.024239 on epoch=294
03/13/2022 04:21:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.024808 on epoch=299
03/13/2022 04:21:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 04:21:41 - INFO - __main__ - Printing 3 examples
03/13/2022 04:21:41 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/13/2022 04:21:41 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/13/2022 04:21:41 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/13/2022 04:21:41 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/13/2022 04:21:41 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/13/2022 04:21:41 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/13/2022 04:21:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 04:21:41 - INFO - __main__ - Tokenizing Output ...
03/13/2022 04:21:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 04:21:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 04:21:41 - INFO - __main__ - Printing 3 examples
03/13/2022 04:21:41 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/13/2022 04:21:41 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/13/2022 04:21:41 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/13/2022 04:21:41 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/13/2022 04:21:41 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/13/2022 04:21:41 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/13/2022 04:21:41 - INFO - __main__ - Tokenizing Input ...
03/13/2022 04:21:41 - INFO - __main__ - Tokenizing Output ...
03/13/2022 04:21:41 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 04:21:50 - INFO - __main__ - Global step 600 Train loss 0.021035 EM 0.0 on epoch=299
03/13/2022 04:21:50 - INFO - __main__ - save last model!
03/13/2022 04:21:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 04:21:54 - INFO - __main__ - Starting training!
03/13/2022 04:21:59 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 04:22:00 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 04:22:00 - INFO - __main__ - Printing 3 examples
03/13/2022 04:22:00 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 04:22:00 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 04:22:00 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 04:22:00 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 04:22:00 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 04:22:00 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 04:22:00 - INFO - __main__ - Tokenizing Input ...
03/13/2022 04:22:03 - INFO - __main__ - Tokenizing Output ...
03/13/2022 04:22:11 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 05:09:21 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_100_0.0003_8_predictions.txt
03/13/2022 05:09:22 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 05:09:22 - INFO - __main__ - prefix=break-QDMR_32_100, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 05:09:22 - INFO - __main__ - Running ... prefix=break-QDMR_32_100, lr=0.0002, bsz=8 ...
03/13/2022 05:09:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 05:09:23 - INFO - __main__ - Printing 3 examples
03/13/2022 05:09:23 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/13/2022 05:09:23 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/13/2022 05:09:23 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/13/2022 05:09:23 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/13/2022 05:09:23 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/13/2022 05:09:23 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/13/2022 05:09:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 05:09:23 - INFO - __main__ - Tokenizing Output ...
03/13/2022 05:09:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 05:09:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 05:09:23 - INFO - __main__ - Printing 3 examples
03/13/2022 05:09:23 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/13/2022 05:09:23 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/13/2022 05:09:23 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/13/2022 05:09:23 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/13/2022 05:09:23 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/13/2022 05:09:23 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/13/2022 05:09:23 - INFO - __main__ - Tokenizing Input ...
03/13/2022 05:09:23 - INFO - __main__ - Tokenizing Output ...
03/13/2022 05:09:23 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 05:09:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 05:09:34 - INFO - __main__ - Starting training!
03/13/2022 05:09:39 - INFO - __main__ - Step 10 Global step 10 Train loss 20.819563 on epoch=4
03/13/2022 05:09:43 - INFO - __main__ - Step 20 Global step 20 Train loss 14.455811 on epoch=9
03/13/2022 05:09:48 - INFO - __main__ - Step 30 Global step 30 Train loss 8.420411 on epoch=14
03/13/2022 05:09:53 - INFO - __main__ - Step 40 Global step 40 Train loss 5.614488 on epoch=19
03/13/2022 05:09:58 - INFO - __main__ - Step 50 Global step 50 Train loss 4.725338 on epoch=24
03/13/2022 05:10:10 - INFO - __main__ - Global step 50 Train loss 10.807123 EM 0.0 on epoch=24
03/13/2022 05:10:16 - INFO - __main__ - Step 60 Global step 60 Train loss 3.688461 on epoch=29
03/13/2022 05:10:21 - INFO - __main__ - Step 70 Global step 70 Train loss 3.881512 on epoch=34
03/13/2022 05:10:26 - INFO - __main__ - Step 80 Global step 80 Train loss 3.511349 on epoch=39
03/13/2022 05:10:31 - INFO - __main__ - Step 90 Global step 90 Train loss 2.823583 on epoch=44
03/13/2022 05:10:36 - INFO - __main__ - Step 100 Global step 100 Train loss 2.273460 on epoch=49
03/13/2022 05:10:47 - INFO - __main__ - Global step 100 Train loss 3.235673 EM 0.0 on epoch=49
03/13/2022 05:10:52 - INFO - __main__ - Step 110 Global step 110 Train loss 2.250789 on epoch=54
03/13/2022 05:10:57 - INFO - __main__ - Step 120 Global step 120 Train loss 1.729947 on epoch=59
03/13/2022 05:11:02 - INFO - __main__ - Step 130 Global step 130 Train loss 1.682586 on epoch=64
03/13/2022 05:11:07 - INFO - __main__ - Step 140 Global step 140 Train loss 1.320084 on epoch=69
03/13/2022 05:11:12 - INFO - __main__ - Step 150 Global step 150 Train loss 1.124108 on epoch=74
03/13/2022 05:11:22 - INFO - __main__ - Global step 150 Train loss 1.621503 EM 0.0 on epoch=74
03/13/2022 05:11:27 - INFO - __main__ - Step 160 Global step 160 Train loss 1.088097 on epoch=79
03/13/2022 05:11:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.856298 on epoch=84
03/13/2022 05:11:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.704230 on epoch=89
03/13/2022 05:11:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.605222 on epoch=94
03/13/2022 05:11:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.522264 on epoch=99
03/13/2022 05:11:57 - INFO - __main__ - Global step 200 Train loss 0.755222 EM 0.0 on epoch=99
03/13/2022 05:12:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.439090 on epoch=104
03/13/2022 05:12:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.375876 on epoch=109
03/13/2022 05:12:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.368471 on epoch=114
03/13/2022 05:12:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.334120 on epoch=119
03/13/2022 05:12:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.320354 on epoch=124
03/13/2022 05:12:31 - INFO - __main__ - Global step 250 Train loss 0.367582 EM 0.0 on epoch=124
03/13/2022 05:12:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.257975 on epoch=129
03/13/2022 05:12:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.199585 on epoch=134
03/13/2022 05:12:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.184711 on epoch=139
03/13/2022 05:12:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.160363 on epoch=144
03/13/2022 05:12:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.159287 on epoch=149
03/13/2022 05:13:06 - INFO - __main__ - Global step 300 Train loss 0.192384 EM 0.0 on epoch=149
03/13/2022 05:13:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.116515 on epoch=154
03/13/2022 05:13:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.149056 on epoch=159
03/13/2022 05:13:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.122667 on epoch=164
03/13/2022 05:13:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.127965 on epoch=169
03/13/2022 05:13:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.115374 on epoch=174
03/13/2022 05:13:41 - INFO - __main__ - Global step 350 Train loss 0.126315 EM 0.0 on epoch=174
03/13/2022 05:13:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.091591 on epoch=179
03/13/2022 05:13:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.099824 on epoch=184
03/13/2022 05:13:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.101611 on epoch=189
03/13/2022 05:14:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.136983 on epoch=194
03/13/2022 05:14:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.088278 on epoch=199
03/13/2022 05:14:16 - INFO - __main__ - Global step 400 Train loss 0.103657 EM 0.0 on epoch=199
03/13/2022 05:14:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.089392 on epoch=204
03/13/2022 05:14:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.084713 on epoch=209
03/13/2022 05:14:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.057241 on epoch=214
03/13/2022 05:14:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.090933 on epoch=219
03/13/2022 05:14:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.077587 on epoch=224
03/13/2022 05:14:51 - INFO - __main__ - Global step 450 Train loss 0.079973 EM 0.0 on epoch=224
03/13/2022 05:14:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.090846 on epoch=229
03/13/2022 05:15:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.087194 on epoch=234
03/13/2022 05:15:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.065220 on epoch=239
03/13/2022 05:15:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.079363 on epoch=244
03/13/2022 05:15:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.040165 on epoch=249
03/13/2022 05:15:26 - INFO - __main__ - Global step 500 Train loss 0.072558 EM 0.0 on epoch=249
03/13/2022 05:15:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.040381 on epoch=254
03/13/2022 05:15:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.029650 on epoch=259
03/13/2022 05:15:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.046723 on epoch=264
03/13/2022 05:15:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.040577 on epoch=269
03/13/2022 05:15:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.043414 on epoch=274
03/13/2022 05:16:01 - INFO - __main__ - Global step 550 Train loss 0.040149 EM 0.0 on epoch=274
03/13/2022 05:16:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.032664 on epoch=279
03/13/2022 05:16:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.051522 on epoch=284
03/13/2022 05:16:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.046644 on epoch=289
03/13/2022 05:16:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.047877 on epoch=294
03/13/2022 05:16:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.045219 on epoch=299
03/13/2022 05:16:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 05:16:27 - INFO - __main__ - Printing 3 examples
03/13/2022 05:16:27 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/13/2022 05:16:27 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/13/2022 05:16:27 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/13/2022 05:16:27 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/13/2022 05:16:27 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/13/2022 05:16:27 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/13/2022 05:16:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 05:16:27 - INFO - __main__ - Tokenizing Output ...
03/13/2022 05:16:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 05:16:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 05:16:27 - INFO - __main__ - Printing 3 examples
03/13/2022 05:16:27 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/13/2022 05:16:27 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/13/2022 05:16:27 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/13/2022 05:16:27 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/13/2022 05:16:27 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/13/2022 05:16:27 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/13/2022 05:16:27 - INFO - __main__ - Tokenizing Input ...
03/13/2022 05:16:27 - INFO - __main__ - Tokenizing Output ...
03/13/2022 05:16:27 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 05:16:36 - INFO - __main__ - Global step 600 Train loss 0.044785 EM 0.0 on epoch=299
03/13/2022 05:16:36 - INFO - __main__ - save last model!
03/13/2022 05:16:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 05:16:38 - INFO - __main__ - Starting training!
03/13/2022 05:16:43 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 05:16:44 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 05:16:44 - INFO - __main__ - Printing 3 examples
03/13/2022 05:16:44 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 05:16:44 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 05:16:44 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 05:16:44 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 05:16:44 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 05:16:44 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 05:16:44 - INFO - __main__ - Tokenizing Input ...
03/13/2022 05:16:47 - INFO - __main__ - Tokenizing Output ...
03/13/2022 05:16:55 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 06:04:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_100_0.0002_8_predictions.txt
03/13/2022 06:04:56 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 06:04:57 - INFO - __main__ - prefix=break-QDMR_32_100, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 06:04:57 - INFO - __main__ - Running ... prefix=break-QDMR_32_100, lr=0.0001, bsz=8 ...
03/13/2022 06:04:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 06:04:58 - INFO - __main__ - Printing 3 examples
03/13/2022 06:04:58 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/13/2022 06:04:58 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/13/2022 06:04:58 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/13/2022 06:04:58 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/13/2022 06:04:58 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/13/2022 06:04:58 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/13/2022 06:04:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 06:04:58 - INFO - __main__ - Tokenizing Output ...
03/13/2022 06:04:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 06:04:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 06:04:58 - INFO - __main__ - Printing 3 examples
03/13/2022 06:04:58 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/13/2022 06:04:58 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/13/2022 06:04:58 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/13/2022 06:04:58 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/13/2022 06:04:58 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/13/2022 06:04:58 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/13/2022 06:04:58 - INFO - __main__ - Tokenizing Input ...
03/13/2022 06:04:58 - INFO - __main__ - Tokenizing Output ...
03/13/2022 06:04:58 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 06:05:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 06:05:09 - INFO - __main__ - Starting training!
03/13/2022 06:05:13 - INFO - __main__ - Step 10 Global step 10 Train loss 20.940763 on epoch=4
03/13/2022 06:05:18 - INFO - __main__ - Step 20 Global step 20 Train loss 16.661369 on epoch=9
03/13/2022 06:05:23 - INFO - __main__ - Step 30 Global step 30 Train loss 11.086750 on epoch=14
03/13/2022 06:05:28 - INFO - __main__ - Step 40 Global step 40 Train loss 9.462389 on epoch=19
03/13/2022 06:05:32 - INFO - __main__ - Step 50 Global step 50 Train loss 8.923197 on epoch=24
03/13/2022 06:05:44 - INFO - __main__ - Global step 50 Train loss 13.414895 EM 0.0 on epoch=24
03/13/2022 06:05:50 - INFO - __main__ - Step 60 Global step 60 Train loss 7.524674 on epoch=29
03/13/2022 06:05:55 - INFO - __main__ - Step 70 Global step 70 Train loss 6.832935 on epoch=34
03/13/2022 06:06:00 - INFO - __main__ - Step 80 Global step 80 Train loss 6.068121 on epoch=39
03/13/2022 06:06:05 - INFO - __main__ - Step 90 Global step 90 Train loss 5.674467 on epoch=44
03/13/2022 06:06:10 - INFO - __main__ - Step 100 Global step 100 Train loss 5.333120 on epoch=49
03/13/2022 06:06:20 - INFO - __main__ - Global step 100 Train loss 6.286663 EM 0.0 on epoch=49
03/13/2022 06:06:25 - INFO - __main__ - Step 110 Global step 110 Train loss 5.111243 on epoch=54
03/13/2022 06:06:30 - INFO - __main__ - Step 120 Global step 120 Train loss 4.800393 on epoch=59
03/13/2022 06:06:35 - INFO - __main__ - Step 130 Global step 130 Train loss 4.647439 on epoch=64
03/13/2022 06:06:40 - INFO - __main__ - Step 140 Global step 140 Train loss 4.451574 on epoch=69
03/13/2022 06:06:45 - INFO - __main__ - Step 150 Global step 150 Train loss 4.228400 on epoch=74
03/13/2022 06:06:54 - INFO - __main__ - Global step 150 Train loss 4.647810 EM 0.0 on epoch=74
03/13/2022 06:06:59 - INFO - __main__ - Step 160 Global step 160 Train loss 4.126074 on epoch=79
03/13/2022 06:07:04 - INFO - __main__ - Step 170 Global step 170 Train loss 3.978408 on epoch=84
03/13/2022 06:07:09 - INFO - __main__ - Step 180 Global step 180 Train loss 3.846165 on epoch=89
03/13/2022 06:07:14 - INFO - __main__ - Step 190 Global step 190 Train loss 3.630032 on epoch=94
03/13/2022 06:07:19 - INFO - __main__ - Step 200 Global step 200 Train loss 3.465145 on epoch=99
03/13/2022 06:07:22 - INFO - __main__ - Global step 200 Train loss 3.809165 EM 0.0 on epoch=99
03/13/2022 06:07:27 - INFO - __main__ - Step 210 Global step 210 Train loss 3.397083 on epoch=104
03/13/2022 06:07:32 - INFO - __main__ - Step 220 Global step 220 Train loss 3.318368 on epoch=109
03/13/2022 06:07:37 - INFO - __main__ - Step 230 Global step 230 Train loss 2.992046 on epoch=114
03/13/2022 06:07:42 - INFO - __main__ - Step 240 Global step 240 Train loss 2.957101 on epoch=119
03/13/2022 06:07:47 - INFO - __main__ - Step 250 Global step 250 Train loss 2.622157 on epoch=124
03/13/2022 06:07:56 - INFO - __main__ - Global step 250 Train loss 3.057351 EM 0.0 on epoch=124
03/13/2022 06:08:01 - INFO - __main__ - Step 260 Global step 260 Train loss 2.481953 on epoch=129
03/13/2022 06:08:06 - INFO - __main__ - Step 270 Global step 270 Train loss 2.340779 on epoch=134
03/13/2022 06:08:11 - INFO - __main__ - Step 280 Global step 280 Train loss 2.097832 on epoch=139
03/13/2022 06:08:16 - INFO - __main__ - Step 290 Global step 290 Train loss 1.942108 on epoch=144
03/13/2022 06:08:21 - INFO - __main__ - Step 300 Global step 300 Train loss 1.850144 on epoch=149
03/13/2022 06:08:29 - INFO - __main__ - Global step 300 Train loss 2.142563 EM 0.0 on epoch=149
03/13/2022 06:08:34 - INFO - __main__ - Step 310 Global step 310 Train loss 1.650620 on epoch=154
03/13/2022 06:08:39 - INFO - __main__ - Step 320 Global step 320 Train loss 1.619506 on epoch=159
03/13/2022 06:08:44 - INFO - __main__ - Step 330 Global step 330 Train loss 1.520903 on epoch=164
03/13/2022 06:08:49 - INFO - __main__ - Step 340 Global step 340 Train loss 1.324591 on epoch=169
03/13/2022 06:08:54 - INFO - __main__ - Step 350 Global step 350 Train loss 1.212765 on epoch=174
03/13/2022 06:09:03 - INFO - __main__ - Global step 350 Train loss 1.465677 EM 0.0 on epoch=174
03/13/2022 06:09:09 - INFO - __main__ - Step 360 Global step 360 Train loss 1.119744 on epoch=179
03/13/2022 06:09:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.972397 on epoch=184
03/13/2022 06:09:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.958748 on epoch=189
03/13/2022 06:09:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.797436 on epoch=194
03/13/2022 06:09:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.819346 on epoch=199
03/13/2022 06:09:39 - INFO - __main__ - Global step 400 Train loss 0.933535 EM 0.0 on epoch=199
03/13/2022 06:09:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.740231 on epoch=204
03/13/2022 06:09:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.643381 on epoch=209
03/13/2022 06:09:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.599010 on epoch=214
03/13/2022 06:09:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.535447 on epoch=219
03/13/2022 06:10:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.526618 on epoch=224
03/13/2022 06:10:14 - INFO - __main__ - Global step 450 Train loss 0.608938 EM 0.0 on epoch=224
03/13/2022 06:10:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.442784 on epoch=229
03/13/2022 06:10:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.453668 on epoch=234
03/13/2022 06:10:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.386679 on epoch=239
03/13/2022 06:10:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.333132 on epoch=244
03/13/2022 06:10:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.306180 on epoch=249
03/13/2022 06:10:47 - INFO - __main__ - Global step 500 Train loss 0.384489 EM 0.0 on epoch=249
03/13/2022 06:10:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.322740 on epoch=254
03/13/2022 06:10:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.280964 on epoch=259
03/13/2022 06:11:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.290810 on epoch=264
03/13/2022 06:11:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.271699 on epoch=269
03/13/2022 06:11:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.271568 on epoch=274
03/13/2022 06:11:22 - INFO - __main__ - Global step 550 Train loss 0.287556 EM 0.0 on epoch=274
03/13/2022 06:11:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.221425 on epoch=279
03/13/2022 06:11:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.219002 on epoch=284
03/13/2022 06:11:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.196632 on epoch=289
03/13/2022 06:11:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.192612 on epoch=294
03/13/2022 06:11:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.164901 on epoch=299
03/13/2022 06:11:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 06:11:49 - INFO - __main__ - Printing 3 examples
03/13/2022 06:11:49 - INFO - __main__ -  [break-QDMR] question: How many field goals did Janikowski kick during the first half?
03/13/2022 06:11:49 - INFO - __main__ - ['return Janikowski ;return field goals of #1 ;return #2 during the  first half ;return number of  #3']
03/13/2022 06:11:49 - INFO - __main__ -  [break-QDMR] question: what was the first james bond movie to come out?
03/13/2022 06:11:49 - INFO - __main__ - ['return james bond movies ;return the  first of #1']
03/13/2022 06:11:49 - INFO - __main__ -  [break-QDMR] question: If the right photo shows an adult gorilla interacting with a human being
03/13/2022 06:11:49 - INFO - __main__ - ['return gorilla ;return #1 that is an adult ;return human being ;return #2 interacting with #3 ;return if  #4 is in  the  right photo']
03/13/2022 06:11:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 06:11:49 - INFO - __main__ - Tokenizing Output ...
03/13/2022 06:11:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 06:11:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 06:11:49 - INFO - __main__ - Printing 3 examples
03/13/2022 06:11:49 - INFO - __main__ -  [break-QDMR] question: What Jane Austen books and movies are in French?
03/13/2022 06:11:49 - INFO - __main__ - ['return Jane Austen ;return books of #1 ;return movies of #1 ;return #2 that are in  French ;return #3 that are in  French ;return #4 ,  #5']
03/13/2022 06:11:49 - INFO - __main__ -  [break-QDMR] question: What shape are the only shiny objects?
03/13/2022 06:11:49 - INFO - __main__ - ['return objects ;return #1 that  is  shiny ;return shape of #2']
03/13/2022 06:11:49 - INFO - __main__ -  [break-QDMR] question: What shape is the large blue object on the very right?
03/13/2022 06:11:49 - INFO - __main__ - ['return blue object ;return #1 that  is  large ;return #2 on  the  very right ;return shape of #3']
03/13/2022 06:11:49 - INFO - __main__ - Tokenizing Input ...
03/13/2022 06:11:49 - INFO - __main__ - Tokenizing Output ...
03/13/2022 06:11:49 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 06:11:53 - INFO - __main__ - Global step 600 Train loss 0.198914 EM 0.0 on epoch=299
03/13/2022 06:11:53 - INFO - __main__ - save last model!
03/13/2022 06:12:00 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 06:12:01 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 06:12:01 - INFO - __main__ - Printing 3 examples
03/13/2022 06:12:01 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 06:12:01 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 06:12:01 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 06:12:01 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 06:12:01 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 06:12:01 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 06:12:01 - INFO - __main__ - Tokenizing Input ...
03/13/2022 06:12:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 06:12:02 - INFO - __main__ - Starting training!
03/13/2022 06:12:04 - INFO - __main__ - Tokenizing Output ...
03/13/2022 06:12:12 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 07:00:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_100_0.0001_8_predictions.txt
03/13/2022 07:00:04 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 07:00:05 - INFO - __main__ - prefix=break-QDMR_32_100, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 07:00:05 - INFO - __main__ - Running ... prefix=break-QDMR_32_13, lr=0.0005, bsz=8 ...
03/13/2022 07:00:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 07:00:06 - INFO - __main__ - Printing 3 examples
03/13/2022 07:00:06 - INFO - __main__ -  [break-QDMR] question: How many field goals did Janikowski kick during the first half?
03/13/2022 07:00:06 - INFO - __main__ - ['return Janikowski ;return field goals of #1 ;return #2 during the  first half ;return number of  #3']
03/13/2022 07:00:06 - INFO - __main__ -  [break-QDMR] question: what was the first james bond movie to come out?
03/13/2022 07:00:06 - INFO - __main__ - ['return james bond movies ;return the  first of #1']
03/13/2022 07:00:06 - INFO - __main__ -  [break-QDMR] question: If the right photo shows an adult gorilla interacting with a human being
03/13/2022 07:00:06 - INFO - __main__ - ['return gorilla ;return #1 that is an adult ;return human being ;return #2 interacting with #3 ;return if  #4 is in  the  right photo']
03/13/2022 07:00:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 07:00:06 - INFO - __main__ - Tokenizing Output ...
03/13/2022 07:00:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 07:00:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 07:00:06 - INFO - __main__ - Printing 3 examples
03/13/2022 07:00:06 - INFO - __main__ -  [break-QDMR] question: What Jane Austen books and movies are in French?
03/13/2022 07:00:06 - INFO - __main__ - ['return Jane Austen ;return books of #1 ;return movies of #1 ;return #2 that are in  French ;return #3 that are in  French ;return #4 ,  #5']
03/13/2022 07:00:06 - INFO - __main__ -  [break-QDMR] question: What shape are the only shiny objects?
03/13/2022 07:00:06 - INFO - __main__ - ['return objects ;return #1 that  is  shiny ;return shape of #2']
03/13/2022 07:00:06 - INFO - __main__ -  [break-QDMR] question: What shape is the large blue object on the very right?
03/13/2022 07:00:06 - INFO - __main__ - ['return blue object ;return #1 that  is  large ;return #2 on  the  very right ;return shape of #3']
03/13/2022 07:00:06 - INFO - __main__ - Tokenizing Input ...
03/13/2022 07:00:06 - INFO - __main__ - Tokenizing Output ...
03/13/2022 07:00:06 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 07:00:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 07:00:17 - INFO - __main__ - Starting training!
03/13/2022 07:00:22 - INFO - __main__ - Step 10 Global step 10 Train loss 21.581095 on epoch=4
03/13/2022 07:00:27 - INFO - __main__ - Step 20 Global step 20 Train loss 14.068362 on epoch=9
03/13/2022 07:00:32 - INFO - __main__ - Step 30 Global step 30 Train loss 5.853137 on epoch=14
03/13/2022 07:00:37 - INFO - __main__ - Step 40 Global step 40 Train loss 4.419248 on epoch=19
03/13/2022 07:00:42 - INFO - __main__ - Step 50 Global step 50 Train loss 3.690553 on epoch=24
03/13/2022 07:00:55 - INFO - __main__ - Global step 50 Train loss 9.922480 EM 0.0 on epoch=24
03/13/2022 07:01:00 - INFO - __main__ - Step 60 Global step 60 Train loss 2.747082 on epoch=29
03/13/2022 07:01:05 - INFO - __main__ - Step 70 Global step 70 Train loss 2.010749 on epoch=34
03/13/2022 07:01:10 - INFO - __main__ - Step 80 Global step 80 Train loss 2.191675 on epoch=39
03/13/2022 07:01:15 - INFO - __main__ - Step 90 Global step 90 Train loss 2.114138 on epoch=44
03/13/2022 07:01:20 - INFO - __main__ - Step 100 Global step 100 Train loss 1.325158 on epoch=49
03/13/2022 07:01:23 - INFO - __main__ - Global step 100 Train loss 2.077761 EM 0.0 on epoch=49
03/13/2022 07:01:28 - INFO - __main__ - Step 110 Global step 110 Train loss 1.264415 on epoch=54
03/13/2022 07:01:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.858672 on epoch=59
03/13/2022 07:01:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.586837 on epoch=64
03/13/2022 07:01:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.377959 on epoch=69
03/13/2022 07:01:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.324756 on epoch=74
03/13/2022 07:01:53 - INFO - __main__ - Global step 150 Train loss 0.682528 EM 0.0 on epoch=74
03/13/2022 07:01:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.236471 on epoch=79
03/13/2022 07:02:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.167057 on epoch=84
03/13/2022 07:02:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.139831 on epoch=89
03/13/2022 07:02:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.094042 on epoch=94
03/13/2022 07:02:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.082877 on epoch=99
03/13/2022 07:02:27 - INFO - __main__ - Global step 200 Train loss 0.144056 EM 0.0 on epoch=99
03/13/2022 07:02:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.111314 on epoch=104
03/13/2022 07:02:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.080459 on epoch=109
03/13/2022 07:02:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.080809 on epoch=114
03/13/2022 07:02:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.050147 on epoch=119
03/13/2022 07:02:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.061086 on epoch=124
03/13/2022 07:02:58 - INFO - __main__ - Global step 250 Train loss 0.076763 EM 0.0 on epoch=124
03/13/2022 07:03:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.044737 on epoch=129
03/13/2022 07:03:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.048695 on epoch=134
03/13/2022 07:03:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.049314 on epoch=139
03/13/2022 07:03:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.070001 on epoch=144
03/13/2022 07:03:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.056576 on epoch=149
03/13/2022 07:03:30 - INFO - __main__ - Global step 300 Train loss 0.053865 EM 0.0 on epoch=149
03/13/2022 07:03:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.036359 on epoch=154
03/13/2022 07:03:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.029636 on epoch=159
03/13/2022 07:03:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.017316 on epoch=164
03/13/2022 07:03:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.043615 on epoch=169
03/13/2022 07:03:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.030189 on epoch=174
03/13/2022 07:04:00 - INFO - __main__ - Global step 350 Train loss 0.031423 EM 0.0 on epoch=174
03/13/2022 07:04:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.029242 on epoch=179
03/13/2022 07:04:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.032664 on epoch=184
03/13/2022 07:04:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.019264 on epoch=189
03/13/2022 07:04:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.023642 on epoch=194
03/13/2022 07:04:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.005363 on epoch=199
03/13/2022 07:04:30 - INFO - __main__ - Global step 400 Train loss 0.022035 EM 0.0 on epoch=199
03/13/2022 07:04:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.010287 on epoch=204
03/13/2022 07:04:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.002698 on epoch=209
03/13/2022 07:04:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.004309 on epoch=214
03/13/2022 07:04:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.016066 on epoch=219
03/13/2022 07:04:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.005436 on epoch=224
03/13/2022 07:05:02 - INFO - __main__ - Global step 450 Train loss 0.007759 EM 0.0 on epoch=224
03/13/2022 07:05:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.003826 on epoch=229
03/13/2022 07:05:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.004584 on epoch=234
03/13/2022 07:05:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.006497 on epoch=239
03/13/2022 07:05:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002442 on epoch=244
03/13/2022 07:05:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.004648 on epoch=249
03/13/2022 07:05:36 - INFO - __main__ - Global step 500 Train loss 0.004399 EM 0.03125 on epoch=249
03/13/2022 07:05:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002154 on epoch=254
03/13/2022 07:05:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.024182 on epoch=259
03/13/2022 07:05:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.012919 on epoch=264
03/13/2022 07:05:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002239 on epoch=269
03/13/2022 07:06:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.003558 on epoch=274
03/13/2022 07:06:08 - INFO - __main__ - Global step 550 Train loss 0.009010 EM 0.0 on epoch=274
03/13/2022 07:06:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.013657 on epoch=279
03/13/2022 07:06:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.012320 on epoch=284
03/13/2022 07:06:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.016403 on epoch=289
03/13/2022 07:06:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.012287 on epoch=294
03/13/2022 07:06:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.002355 on epoch=299
03/13/2022 07:06:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 07:06:35 - INFO - __main__ - Printing 3 examples
03/13/2022 07:06:35 - INFO - __main__ -  [break-QDMR] question: How many field goals did Janikowski kick during the first half?
03/13/2022 07:06:35 - INFO - __main__ - ['return Janikowski ;return field goals of #1 ;return #2 during the  first half ;return number of  #3']
03/13/2022 07:06:35 - INFO - __main__ -  [break-QDMR] question: what was the first james bond movie to come out?
03/13/2022 07:06:35 - INFO - __main__ - ['return james bond movies ;return the  first of #1']
03/13/2022 07:06:35 - INFO - __main__ -  [break-QDMR] question: If the right photo shows an adult gorilla interacting with a human being
03/13/2022 07:06:35 - INFO - __main__ - ['return gorilla ;return #1 that is an adult ;return human being ;return #2 interacting with #3 ;return if  #4 is in  the  right photo']
03/13/2022 07:06:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 07:06:35 - INFO - __main__ - Tokenizing Output ...
03/13/2022 07:06:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 07:06:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 07:06:35 - INFO - __main__ - Printing 3 examples
03/13/2022 07:06:35 - INFO - __main__ -  [break-QDMR] question: What Jane Austen books and movies are in French?
03/13/2022 07:06:35 - INFO - __main__ - ['return Jane Austen ;return books of #1 ;return movies of #1 ;return #2 that are in  French ;return #3 that are in  French ;return #4 ,  #5']
03/13/2022 07:06:35 - INFO - __main__ -  [break-QDMR] question: What shape are the only shiny objects?
03/13/2022 07:06:35 - INFO - __main__ - ['return objects ;return #1 that  is  shiny ;return shape of #2']
03/13/2022 07:06:35 - INFO - __main__ -  [break-QDMR] question: What shape is the large blue object on the very right?
03/13/2022 07:06:35 - INFO - __main__ - ['return blue object ;return #1 that  is  large ;return #2 on  the  very right ;return shape of #3']
03/13/2022 07:06:35 - INFO - __main__ - Tokenizing Input ...
03/13/2022 07:06:35 - INFO - __main__ - Tokenizing Output ...
03/13/2022 07:06:35 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 07:06:39 - INFO - __main__ - Global step 600 Train loss 0.011404 EM 0.0 on epoch=299
03/13/2022 07:06:39 - INFO - __main__ - save last model!
03/13/2022 07:06:46 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 07:06:47 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 07:06:47 - INFO - __main__ - Printing 3 examples
03/13/2022 07:06:47 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 07:06:47 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 07:06:47 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 07:06:47 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 07:06:47 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 07:06:47 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 07:06:47 - INFO - __main__ - Tokenizing Input ...
03/13/2022 07:06:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 07:06:48 - INFO - __main__ - Starting training!
03/13/2022 07:06:50 - INFO - __main__ - Tokenizing Output ...
03/13/2022 07:06:58 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 07:35:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_13_0.0005_8_predictions.txt
03/13/2022 07:35:12 - INFO - __main__ - EM on test data: 0.0006
03/13/2022 07:35:13 - INFO - __main__ - prefix=break-QDMR_32_13, lr=0.0005, bsz=8, dev_performance=0.03125, test_performance=0.0006443298969072165
03/13/2022 07:35:13 - INFO - __main__ - Running ... prefix=break-QDMR_32_13, lr=0.0003, bsz=8 ...
03/13/2022 07:35:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 07:35:14 - INFO - __main__ - Printing 3 examples
03/13/2022 07:35:14 - INFO - __main__ -  [break-QDMR] question: How many field goals did Janikowski kick during the first half?
03/13/2022 07:35:14 - INFO - __main__ - ['return Janikowski ;return field goals of #1 ;return #2 during the  first half ;return number of  #3']
03/13/2022 07:35:14 - INFO - __main__ -  [break-QDMR] question: what was the first james bond movie to come out?
03/13/2022 07:35:14 - INFO - __main__ - ['return james bond movies ;return the  first of #1']
03/13/2022 07:35:14 - INFO - __main__ -  [break-QDMR] question: If the right photo shows an adult gorilla interacting with a human being
03/13/2022 07:35:14 - INFO - __main__ - ['return gorilla ;return #1 that is an adult ;return human being ;return #2 interacting with #3 ;return if  #4 is in  the  right photo']
03/13/2022 07:35:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 07:35:14 - INFO - __main__ - Tokenizing Output ...
03/13/2022 07:35:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 07:35:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 07:35:14 - INFO - __main__ - Printing 3 examples
03/13/2022 07:35:14 - INFO - __main__ -  [break-QDMR] question: What Jane Austen books and movies are in French?
03/13/2022 07:35:14 - INFO - __main__ - ['return Jane Austen ;return books of #1 ;return movies of #1 ;return #2 that are in  French ;return #3 that are in  French ;return #4 ,  #5']
03/13/2022 07:35:14 - INFO - __main__ -  [break-QDMR] question: What shape are the only shiny objects?
03/13/2022 07:35:14 - INFO - __main__ - ['return objects ;return #1 that  is  shiny ;return shape of #2']
03/13/2022 07:35:14 - INFO - __main__ -  [break-QDMR] question: What shape is the large blue object on the very right?
03/13/2022 07:35:14 - INFO - __main__ - ['return blue object ;return #1 that  is  large ;return #2 on  the  very right ;return shape of #3']
03/13/2022 07:35:14 - INFO - __main__ - Tokenizing Input ...
03/13/2022 07:35:14 - INFO - __main__ - Tokenizing Output ...
03/13/2022 07:35:14 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 07:35:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 07:35:25 - INFO - __main__ - Starting training!
03/13/2022 07:35:29 - INFO - __main__ - Step 10 Global step 10 Train loss 20.572653 on epoch=4
03/13/2022 07:35:33 - INFO - __main__ - Step 20 Global step 20 Train loss 14.511398 on epoch=9
03/13/2022 07:35:38 - INFO - __main__ - Step 30 Global step 30 Train loss 7.487261 on epoch=14
03/13/2022 07:35:43 - INFO - __main__ - Step 40 Global step 40 Train loss 5.261787 on epoch=19
03/13/2022 07:35:48 - INFO - __main__ - Step 50 Global step 50 Train loss 4.270722 on epoch=24
03/13/2022 07:36:00 - INFO - __main__ - Global step 50 Train loss 10.420764 EM 0.0 on epoch=24
03/13/2022 07:36:06 - INFO - __main__ - Step 60 Global step 60 Train loss 3.706598 on epoch=29
03/13/2022 07:36:11 - INFO - __main__ - Step 70 Global step 70 Train loss 3.043337 on epoch=34
03/13/2022 07:36:16 - INFO - __main__ - Step 80 Global step 80 Train loss 2.365226 on epoch=39
03/13/2022 07:36:21 - INFO - __main__ - Step 90 Global step 90 Train loss 1.890749 on epoch=44
03/13/2022 07:36:26 - INFO - __main__ - Step 100 Global step 100 Train loss 1.512138 on epoch=49
03/13/2022 07:36:36 - INFO - __main__ - Global step 100 Train loss 2.503609 EM 0.0 on epoch=49
03/13/2022 07:36:41 - INFO - __main__ - Step 110 Global step 110 Train loss 1.107383 on epoch=54
03/13/2022 07:36:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.930655 on epoch=59
03/13/2022 07:36:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.662192 on epoch=64
03/13/2022 07:36:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.443587 on epoch=69
03/13/2022 07:37:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.434071 on epoch=74
03/13/2022 07:37:10 - INFO - __main__ - Global step 150 Train loss 0.715578 EM 0.0 on epoch=74
03/13/2022 07:37:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.325866 on epoch=79
03/13/2022 07:37:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.283857 on epoch=84
03/13/2022 07:37:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.254875 on epoch=89
03/13/2022 07:37:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.199875 on epoch=94
03/13/2022 07:37:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.191554 on epoch=99
03/13/2022 07:37:40 - INFO - __main__ - Global step 200 Train loss 0.251205 EM 0.03125 on epoch=99
03/13/2022 07:37:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.145701 on epoch=104
03/13/2022 07:37:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.151482 on epoch=109
03/13/2022 07:37:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.106508 on epoch=114
03/13/2022 07:38:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.127167 on epoch=119
03/13/2022 07:38:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.115225 on epoch=124
03/13/2022 07:38:10 - INFO - __main__ - Global step 250 Train loss 0.129217 EM 0.0 on epoch=124
03/13/2022 07:38:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.074985 on epoch=129
03/13/2022 07:38:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.075695 on epoch=134
03/13/2022 07:38:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.078005 on epoch=139
03/13/2022 07:38:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.078429 on epoch=144
03/13/2022 07:38:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.071556 on epoch=149
03/13/2022 07:38:40 - INFO - __main__ - Global step 300 Train loss 0.075734 EM 0.03125 on epoch=149
03/13/2022 07:38:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.062921 on epoch=154
03/13/2022 07:38:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.056587 on epoch=159
03/13/2022 07:38:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.043198 on epoch=164
03/13/2022 07:39:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.057261 on epoch=169
03/13/2022 07:39:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.032259 on epoch=174
03/13/2022 07:39:09 - INFO - __main__ - Global step 350 Train loss 0.050445 EM 0.0 on epoch=174
03/13/2022 07:39:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.046011 on epoch=179
03/13/2022 07:39:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.043608 on epoch=184
03/13/2022 07:39:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.036956 on epoch=189
03/13/2022 07:39:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.029593 on epoch=194
03/13/2022 07:39:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.018123 on epoch=199
03/13/2022 07:39:40 - INFO - __main__ - Global step 400 Train loss 0.034858 EM 0.0 on epoch=199
03/13/2022 07:39:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.031762 on epoch=204
03/13/2022 07:39:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.042253 on epoch=209
03/13/2022 07:39:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.036508 on epoch=214
03/13/2022 07:40:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.016761 on epoch=219
03/13/2022 07:40:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.031793 on epoch=224
03/13/2022 07:40:10 - INFO - __main__ - Global step 450 Train loss 0.031815 EM 0.03125 on epoch=224
03/13/2022 07:40:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.016130 on epoch=229
03/13/2022 07:40:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.026266 on epoch=234
03/13/2022 07:40:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.032888 on epoch=239
03/13/2022 07:40:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.261633 on epoch=244
03/13/2022 07:40:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.201971 on epoch=249
03/13/2022 07:40:40 - INFO - __main__ - Global step 500 Train loss 0.107778 EM 0.0 on epoch=249
03/13/2022 07:40:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.095427 on epoch=254
03/13/2022 07:40:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.075729 on epoch=259
03/13/2022 07:40:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.053280 on epoch=264
03/13/2022 07:41:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.065770 on epoch=269
03/13/2022 07:41:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.054885 on epoch=274
03/13/2022 07:41:11 - INFO - __main__ - Global step 550 Train loss 0.069018 EM 0.0 on epoch=274
03/13/2022 07:41:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.042218 on epoch=279
03/13/2022 07:41:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.035748 on epoch=284
03/13/2022 07:41:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.052247 on epoch=289
03/13/2022 07:41:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.049174 on epoch=294
03/13/2022 07:41:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.026134 on epoch=299
03/13/2022 07:41:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 07:41:36 - INFO - __main__ - Printing 3 examples
03/13/2022 07:41:36 - INFO - __main__ -  [break-QDMR] question: How many field goals did Janikowski kick during the first half?
03/13/2022 07:41:36 - INFO - __main__ - ['return Janikowski ;return field goals of #1 ;return #2 during the  first half ;return number of  #3']
03/13/2022 07:41:36 - INFO - __main__ -  [break-QDMR] question: what was the first james bond movie to come out?
03/13/2022 07:41:36 - INFO - __main__ - ['return james bond movies ;return the  first of #1']
03/13/2022 07:41:36 - INFO - __main__ -  [break-QDMR] question: If the right photo shows an adult gorilla interacting with a human being
03/13/2022 07:41:36 - INFO - __main__ - ['return gorilla ;return #1 that is an adult ;return human being ;return #2 interacting with #3 ;return if  #4 is in  the  right photo']
03/13/2022 07:41:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 07:41:36 - INFO - __main__ - Tokenizing Output ...
03/13/2022 07:41:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 07:41:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 07:41:37 - INFO - __main__ - Printing 3 examples
03/13/2022 07:41:37 - INFO - __main__ -  [break-QDMR] question: What Jane Austen books and movies are in French?
03/13/2022 07:41:37 - INFO - __main__ - ['return Jane Austen ;return books of #1 ;return movies of #1 ;return #2 that are in  French ;return #3 that are in  French ;return #4 ,  #5']
03/13/2022 07:41:37 - INFO - __main__ -  [break-QDMR] question: What shape are the only shiny objects?
03/13/2022 07:41:37 - INFO - __main__ - ['return objects ;return #1 that  is  shiny ;return shape of #2']
03/13/2022 07:41:37 - INFO - __main__ -  [break-QDMR] question: What shape is the large blue object on the very right?
03/13/2022 07:41:37 - INFO - __main__ - ['return blue object ;return #1 that  is  large ;return #2 on  the  very right ;return shape of #3']
03/13/2022 07:41:37 - INFO - __main__ - Tokenizing Input ...
03/13/2022 07:41:37 - INFO - __main__ - Tokenizing Output ...
03/13/2022 07:41:37 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 07:41:41 - INFO - __main__ - Global step 600 Train loss 0.041104 EM 0.0 on epoch=299
03/13/2022 07:41:41 - INFO - __main__ - save last model!
03/13/2022 07:41:48 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 07:41:49 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 07:41:49 - INFO - __main__ - Printing 3 examples
03/13/2022 07:41:49 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 07:41:49 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 07:41:49 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 07:41:49 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 07:41:49 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 07:41:49 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 07:41:49 - INFO - __main__ - Tokenizing Input ...
03/13/2022 07:41:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 07:41:50 - INFO - __main__ - Starting training!
03/13/2022 07:41:52 - INFO - __main__ - Tokenizing Output ...
03/13/2022 07:42:00 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 08:08:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_13_0.0003_8_predictions.txt
03/13/2022 08:08:33 - INFO - __main__ - EM on test data: 0.0027
03/13/2022 08:08:34 - INFO - __main__ - prefix=break-QDMR_32_13, lr=0.0003, bsz=8, dev_performance=0.03125, test_performance=0.002706185567010309
03/13/2022 08:08:34 - INFO - __main__ - Running ... prefix=break-QDMR_32_13, lr=0.0002, bsz=8 ...
03/13/2022 08:08:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 08:08:35 - INFO - __main__ - Printing 3 examples
03/13/2022 08:08:35 - INFO - __main__ -  [break-QDMR] question: How many field goals did Janikowski kick during the first half?
03/13/2022 08:08:35 - INFO - __main__ - ['return Janikowski ;return field goals of #1 ;return #2 during the  first half ;return number of  #3']
03/13/2022 08:08:35 - INFO - __main__ -  [break-QDMR] question: what was the first james bond movie to come out?
03/13/2022 08:08:35 - INFO - __main__ - ['return james bond movies ;return the  first of #1']
03/13/2022 08:08:35 - INFO - __main__ -  [break-QDMR] question: If the right photo shows an adult gorilla interacting with a human being
03/13/2022 08:08:35 - INFO - __main__ - ['return gorilla ;return #1 that is an adult ;return human being ;return #2 interacting with #3 ;return if  #4 is in  the  right photo']
03/13/2022 08:08:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 08:08:35 - INFO - __main__ - Tokenizing Output ...
03/13/2022 08:08:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 08:08:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 08:08:35 - INFO - __main__ - Printing 3 examples
03/13/2022 08:08:35 - INFO - __main__ -  [break-QDMR] question: What Jane Austen books and movies are in French?
03/13/2022 08:08:35 - INFO - __main__ - ['return Jane Austen ;return books of #1 ;return movies of #1 ;return #2 that are in  French ;return #3 that are in  French ;return #4 ,  #5']
03/13/2022 08:08:35 - INFO - __main__ -  [break-QDMR] question: What shape are the only shiny objects?
03/13/2022 08:08:35 - INFO - __main__ - ['return objects ;return #1 that  is  shiny ;return shape of #2']
03/13/2022 08:08:35 - INFO - __main__ -  [break-QDMR] question: What shape is the large blue object on the very right?
03/13/2022 08:08:35 - INFO - __main__ - ['return blue object ;return #1 that  is  large ;return #2 on  the  very right ;return shape of #3']
03/13/2022 08:08:35 - INFO - __main__ - Tokenizing Input ...
03/13/2022 08:08:35 - INFO - __main__ - Tokenizing Output ...
03/13/2022 08:08:35 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 08:08:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 08:08:47 - INFO - __main__ - Starting training!
03/13/2022 08:08:52 - INFO - __main__ - Step 10 Global step 10 Train loss 20.325939 on epoch=4
03/13/2022 08:08:57 - INFO - __main__ - Step 20 Global step 20 Train loss 13.685745 on epoch=9
03/13/2022 08:09:02 - INFO - __main__ - Step 30 Global step 30 Train loss 7.823003 on epoch=14
03/13/2022 08:09:07 - INFO - __main__ - Step 40 Global step 40 Train loss 6.330777 on epoch=19
03/13/2022 08:09:12 - INFO - __main__ - Step 50 Global step 50 Train loss 4.437376 on epoch=24
03/13/2022 08:09:27 - INFO - __main__ - Global step 50 Train loss 10.520568 EM 0.0 on epoch=24
03/13/2022 08:09:32 - INFO - __main__ - Step 60 Global step 60 Train loss 3.484222 on epoch=29
03/13/2022 08:09:38 - INFO - __main__ - Step 70 Global step 70 Train loss 2.857602 on epoch=34
03/13/2022 08:09:43 - INFO - __main__ - Step 80 Global step 80 Train loss 2.468167 on epoch=39
03/13/2022 08:09:48 - INFO - __main__ - Step 90 Global step 90 Train loss 2.125697 on epoch=44
03/13/2022 08:09:53 - INFO - __main__ - Step 100 Global step 100 Train loss 1.838473 on epoch=49
03/13/2022 08:10:02 - INFO - __main__ - Global step 100 Train loss 2.554832 EM 0.0 on epoch=49
03/13/2022 08:10:07 - INFO - __main__ - Step 110 Global step 110 Train loss 1.616599 on epoch=54
03/13/2022 08:10:12 - INFO - __main__ - Step 120 Global step 120 Train loss 1.348006 on epoch=59
03/13/2022 08:10:17 - INFO - __main__ - Step 130 Global step 130 Train loss 1.099586 on epoch=64
03/13/2022 08:10:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.957475 on epoch=69
03/13/2022 08:10:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.900169 on epoch=74
03/13/2022 08:10:37 - INFO - __main__ - Global step 150 Train loss 1.184367 EM 0.0 on epoch=74
03/13/2022 08:10:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.698991 on epoch=79
03/13/2022 08:10:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.595619 on epoch=84
03/13/2022 08:10:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.500473 on epoch=89
03/13/2022 08:10:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.527624 on epoch=94
03/13/2022 08:11:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.434068 on epoch=99
03/13/2022 08:11:07 - INFO - __main__ - Global step 200 Train loss 0.551355 EM 0.0 on epoch=99
03/13/2022 08:11:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.466166 on epoch=104
03/13/2022 08:11:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.408480 on epoch=109
03/13/2022 08:11:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.276705 on epoch=114
03/13/2022 08:11:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.253774 on epoch=119
03/13/2022 08:11:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.227821 on epoch=124
03/13/2022 08:11:38 - INFO - __main__ - Global step 250 Train loss 0.326589 EM 0.0 on epoch=124
03/13/2022 08:11:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.246945 on epoch=129
03/13/2022 08:11:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.154384 on epoch=134
03/13/2022 08:11:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.151421 on epoch=139
03/13/2022 08:11:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.139008 on epoch=144
03/13/2022 08:12:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.148280 on epoch=149
03/13/2022 08:12:09 - INFO - __main__ - Global step 300 Train loss 0.168008 EM 0.0 on epoch=149
03/13/2022 08:12:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.143845 on epoch=154
03/13/2022 08:12:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.137028 on epoch=159
03/13/2022 08:12:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.132460 on epoch=164
03/13/2022 08:12:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.118833 on epoch=169
03/13/2022 08:12:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.100336 on epoch=174
03/13/2022 08:12:40 - INFO - __main__ - Global step 350 Train loss 0.126500 EM 0.0 on epoch=174
03/13/2022 08:12:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.097395 on epoch=179
03/13/2022 08:12:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.104093 on epoch=184
03/13/2022 08:12:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.076734 on epoch=189
03/13/2022 08:13:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.074512 on epoch=194
03/13/2022 08:13:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.123879 on epoch=199
03/13/2022 08:13:12 - INFO - __main__ - Global step 400 Train loss 0.095323 EM 0.0 on epoch=199
03/13/2022 08:13:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.105492 on epoch=204
03/13/2022 08:13:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.108160 on epoch=209
03/13/2022 08:13:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.101830 on epoch=214
03/13/2022 08:13:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.156351 on epoch=219
03/13/2022 08:13:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.092683 on epoch=224
03/13/2022 08:13:41 - INFO - __main__ - Global step 450 Train loss 0.112903 EM 0.03125 on epoch=224
03/13/2022 08:13:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.077810 on epoch=229
03/13/2022 08:13:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.059973 on epoch=234
03/13/2022 08:13:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.060167 on epoch=239
03/13/2022 08:14:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.110173 on epoch=244
03/13/2022 08:14:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.053403 on epoch=249
03/13/2022 08:14:12 - INFO - __main__ - Global step 500 Train loss 0.072305 EM 0.0 on epoch=249
03/13/2022 08:14:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.044608 on epoch=254
03/13/2022 08:14:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.031812 on epoch=259
03/13/2022 08:14:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.055921 on epoch=264
03/13/2022 08:14:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.046130 on epoch=269
03/13/2022 08:14:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.039372 on epoch=274
03/13/2022 08:14:42 - INFO - __main__ - Global step 550 Train loss 0.043568 EM 0.0 on epoch=274
03/13/2022 08:14:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.051349 on epoch=279
03/13/2022 08:14:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.068755 on epoch=284
03/13/2022 08:14:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.037345 on epoch=289
03/13/2022 08:15:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.052435 on epoch=294
03/13/2022 08:15:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.067353 on epoch=299
03/13/2022 08:15:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 08:15:08 - INFO - __main__ - Printing 3 examples
03/13/2022 08:15:08 - INFO - __main__ -  [break-QDMR] question: How many field goals did Janikowski kick during the first half?
03/13/2022 08:15:08 - INFO - __main__ - ['return Janikowski ;return field goals of #1 ;return #2 during the  first half ;return number of  #3']
03/13/2022 08:15:08 - INFO - __main__ -  [break-QDMR] question: what was the first james bond movie to come out?
03/13/2022 08:15:08 - INFO - __main__ - ['return james bond movies ;return the  first of #1']
03/13/2022 08:15:08 - INFO - __main__ -  [break-QDMR] question: If the right photo shows an adult gorilla interacting with a human being
03/13/2022 08:15:08 - INFO - __main__ - ['return gorilla ;return #1 that is an adult ;return human being ;return #2 interacting with #3 ;return if  #4 is in  the  right photo']
03/13/2022 08:15:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 08:15:08 - INFO - __main__ - Tokenizing Output ...
03/13/2022 08:15:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 08:15:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 08:15:08 - INFO - __main__ - Printing 3 examples
03/13/2022 08:15:08 - INFO - __main__ -  [break-QDMR] question: What Jane Austen books and movies are in French?
03/13/2022 08:15:08 - INFO - __main__ - ['return Jane Austen ;return books of #1 ;return movies of #1 ;return #2 that are in  French ;return #3 that are in  French ;return #4 ,  #5']
03/13/2022 08:15:08 - INFO - __main__ -  [break-QDMR] question: What shape are the only shiny objects?
03/13/2022 08:15:08 - INFO - __main__ - ['return objects ;return #1 that  is  shiny ;return shape of #2']
03/13/2022 08:15:08 - INFO - __main__ -  [break-QDMR] question: What shape is the large blue object on the very right?
03/13/2022 08:15:08 - INFO - __main__ - ['return blue object ;return #1 that  is  large ;return #2 on  the  very right ;return shape of #3']
03/13/2022 08:15:08 - INFO - __main__ - Tokenizing Input ...
03/13/2022 08:15:08 - INFO - __main__ - Tokenizing Output ...
03/13/2022 08:15:08 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 08:15:13 - INFO - __main__ - Global step 600 Train loss 0.055447 EM 0.0 on epoch=299
03/13/2022 08:15:13 - INFO - __main__ - save last model!
03/13/2022 08:15:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 08:15:19 - INFO - __main__ - Starting training!
03/13/2022 08:15:20 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 08:15:21 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 08:15:21 - INFO - __main__ - Printing 3 examples
03/13/2022 08:15:21 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 08:15:21 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 08:15:21 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 08:15:21 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 08:15:21 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 08:15:21 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 08:15:21 - INFO - __main__ - Tokenizing Input ...
03/13/2022 08:15:24 - INFO - __main__ - Tokenizing Output ...
03/13/2022 08:15:32 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 08:38:21 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_13_0.0002_8_predictions.txt
03/13/2022 08:38:21 - INFO - __main__ - EM on test data: 0.0004
03/13/2022 08:38:22 - INFO - __main__ - prefix=break-QDMR_32_13, lr=0.0002, bsz=8, dev_performance=0.03125, test_performance=0.0003865979381443299
03/13/2022 08:38:22 - INFO - __main__ - Running ... prefix=break-QDMR_32_13, lr=0.0001, bsz=8 ...
03/13/2022 08:38:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 08:38:23 - INFO - __main__ - Printing 3 examples
03/13/2022 08:38:23 - INFO - __main__ -  [break-QDMR] question: How many field goals did Janikowski kick during the first half?
03/13/2022 08:38:23 - INFO - __main__ - ['return Janikowski ;return field goals of #1 ;return #2 during the  first half ;return number of  #3']
03/13/2022 08:38:23 - INFO - __main__ -  [break-QDMR] question: what was the first james bond movie to come out?
03/13/2022 08:38:23 - INFO - __main__ - ['return james bond movies ;return the  first of #1']
03/13/2022 08:38:23 - INFO - __main__ -  [break-QDMR] question: If the right photo shows an adult gorilla interacting with a human being
03/13/2022 08:38:23 - INFO - __main__ - ['return gorilla ;return #1 that is an adult ;return human being ;return #2 interacting with #3 ;return if  #4 is in  the  right photo']
03/13/2022 08:38:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 08:38:23 - INFO - __main__ - Tokenizing Output ...
03/13/2022 08:38:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 08:38:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 08:38:23 - INFO - __main__ - Printing 3 examples
03/13/2022 08:38:23 - INFO - __main__ -  [break-QDMR] question: What Jane Austen books and movies are in French?
03/13/2022 08:38:23 - INFO - __main__ - ['return Jane Austen ;return books of #1 ;return movies of #1 ;return #2 that are in  French ;return #3 that are in  French ;return #4 ,  #5']
03/13/2022 08:38:23 - INFO - __main__ -  [break-QDMR] question: What shape are the only shiny objects?
03/13/2022 08:38:23 - INFO - __main__ - ['return objects ;return #1 that  is  shiny ;return shape of #2']
03/13/2022 08:38:23 - INFO - __main__ -  [break-QDMR] question: What shape is the large blue object on the very right?
03/13/2022 08:38:23 - INFO - __main__ - ['return blue object ;return #1 that  is  large ;return #2 on  the  very right ;return shape of #3']
03/13/2022 08:38:23 - INFO - __main__ - Tokenizing Input ...
03/13/2022 08:38:23 - INFO - __main__ - Tokenizing Output ...
03/13/2022 08:38:23 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 08:38:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 08:38:34 - INFO - __main__ - Starting training!
03/13/2022 08:38:38 - INFO - __main__ - Step 10 Global step 10 Train loss 20.845366 on epoch=4
03/13/2022 08:38:42 - INFO - __main__ - Step 20 Global step 20 Train loss 17.515545 on epoch=9
03/13/2022 08:38:47 - INFO - __main__ - Step 30 Global step 30 Train loss 10.331816 on epoch=14
03/13/2022 08:38:52 - INFO - __main__ - Step 40 Global step 40 Train loss 7.943976 on epoch=19
03/13/2022 08:38:57 - INFO - __main__ - Step 50 Global step 50 Train loss 6.511230 on epoch=24
03/13/2022 08:39:09 - INFO - __main__ - Global step 50 Train loss 12.629586 EM 0.0 on epoch=24
03/13/2022 08:39:14 - INFO - __main__ - Step 60 Global step 60 Train loss 6.037297 on epoch=29
03/13/2022 08:39:19 - INFO - __main__ - Step 70 Global step 70 Train loss 5.487615 on epoch=34
03/13/2022 08:39:24 - INFO - __main__ - Step 80 Global step 80 Train loss 4.960143 on epoch=39
03/13/2022 08:39:29 - INFO - __main__ - Step 90 Global step 90 Train loss 4.519241 on epoch=44
03/13/2022 08:39:34 - INFO - __main__ - Step 100 Global step 100 Train loss 3.988388 on epoch=49
03/13/2022 08:39:46 - INFO - __main__ - Global step 100 Train loss 4.998537 EM 0.0 on epoch=49
03/13/2022 08:39:51 - INFO - __main__ - Step 110 Global step 110 Train loss 3.746336 on epoch=54
03/13/2022 08:39:55 - INFO - __main__ - Step 120 Global step 120 Train loss 3.350446 on epoch=59
03/13/2022 08:40:00 - INFO - __main__ - Step 130 Global step 130 Train loss 3.083101 on epoch=64
03/13/2022 08:40:05 - INFO - __main__ - Step 140 Global step 140 Train loss 2.936326 on epoch=69
03/13/2022 08:40:10 - INFO - __main__ - Step 150 Global step 150 Train loss 2.613515 on epoch=74
03/13/2022 08:40:20 - INFO - __main__ - Global step 150 Train loss 3.145945 EM 0.0 on epoch=74
03/13/2022 08:40:25 - INFO - __main__ - Step 160 Global step 160 Train loss 2.403795 on epoch=79
03/13/2022 08:40:30 - INFO - __main__ - Step 170 Global step 170 Train loss 2.322436 on epoch=84
03/13/2022 08:40:35 - INFO - __main__ - Step 180 Global step 180 Train loss 2.156148 on epoch=89
03/13/2022 08:40:40 - INFO - __main__ - Step 190 Global step 190 Train loss 2.038837 on epoch=94
03/13/2022 08:40:45 - INFO - __main__ - Step 200 Global step 200 Train loss 1.879247 on epoch=99
03/13/2022 08:40:54 - INFO - __main__ - Global step 200 Train loss 2.160093 EM 0.0 on epoch=99
03/13/2022 08:40:59 - INFO - __main__ - Step 210 Global step 210 Train loss 1.725699 on epoch=104
03/13/2022 08:41:04 - INFO - __main__ - Step 220 Global step 220 Train loss 1.573686 on epoch=109
03/13/2022 08:41:09 - INFO - __main__ - Step 230 Global step 230 Train loss 1.532587 on epoch=114
03/13/2022 08:41:14 - INFO - __main__ - Step 240 Global step 240 Train loss 1.299705 on epoch=119
03/13/2022 08:41:19 - INFO - __main__ - Step 250 Global step 250 Train loss 1.244918 on epoch=124
03/13/2022 08:41:27 - INFO - __main__ - Global step 250 Train loss 1.475319 EM 0.0 on epoch=124
03/13/2022 08:41:32 - INFO - __main__ - Step 260 Global step 260 Train loss 1.226350 on epoch=129
03/13/2022 08:41:37 - INFO - __main__ - Step 270 Global step 270 Train loss 1.076103 on epoch=134
03/13/2022 08:41:42 - INFO - __main__ - Step 280 Global step 280 Train loss 1.045756 on epoch=139
03/13/2022 08:41:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.949351 on epoch=144
03/13/2022 08:41:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.918968 on epoch=149
03/13/2022 08:42:01 - INFO - __main__ - Global step 300 Train loss 1.043305 EM 0.0 on epoch=149
03/13/2022 08:42:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.809120 on epoch=154
03/13/2022 08:42:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.733116 on epoch=159
03/13/2022 08:42:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.687834 on epoch=164
03/13/2022 08:42:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.671279 on epoch=169
03/13/2022 08:42:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.686874 on epoch=174
03/13/2022 08:42:34 - INFO - __main__ - Global step 350 Train loss 0.717645 EM 0.0 on epoch=174
03/13/2022 08:42:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.558587 on epoch=179
03/13/2022 08:42:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.560000 on epoch=184
03/13/2022 08:42:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.486853 on epoch=189
03/13/2022 08:42:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.461024 on epoch=194
03/13/2022 08:42:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.396093 on epoch=199
03/13/2022 08:43:07 - INFO - __main__ - Global step 400 Train loss 0.492511 EM 0.0 on epoch=199
03/13/2022 08:43:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.407225 on epoch=204
03/13/2022 08:43:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.379253 on epoch=209
03/13/2022 08:43:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.287794 on epoch=214
03/13/2022 08:43:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.365569 on epoch=219
03/13/2022 08:43:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.320382 on epoch=224
03/13/2022 08:43:37 - INFO - __main__ - Global step 450 Train loss 0.352045 EM 0.03125 on epoch=224
03/13/2022 08:43:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.253458 on epoch=229
03/13/2022 08:43:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.274065 on epoch=234
03/13/2022 08:43:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.292449 on epoch=239
03/13/2022 08:43:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.249115 on epoch=244
03/13/2022 08:44:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.242587 on epoch=249
03/13/2022 08:44:07 - INFO - __main__ - Global step 500 Train loss 0.262335 EM 0.03125 on epoch=249
03/13/2022 08:44:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.208255 on epoch=254
03/13/2022 08:44:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.158531 on epoch=259
03/13/2022 08:44:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.191486 on epoch=264
03/13/2022 08:44:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.173523 on epoch=269
03/13/2022 08:44:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.146943 on epoch=274
03/13/2022 08:44:36 - INFO - __main__ - Global step 550 Train loss 0.175748 EM 0.0 on epoch=274
03/13/2022 08:44:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.178013 on epoch=279
03/13/2022 08:44:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.109313 on epoch=284
03/13/2022 08:44:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.121829 on epoch=289
03/13/2022 08:44:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.137131 on epoch=294
03/13/2022 08:45:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.153684 on epoch=299
03/13/2022 08:45:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 08:45:02 - INFO - __main__ - Printing 3 examples
03/13/2022 08:45:02 - INFO - __main__ -  [break-QDMR] question: If in the left image there is a wold walking on snow covered ground with twigs sticking up through the snow
03/13/2022 08:45:02 - INFO - __main__ - ['return left image ;return wold in  #1 ;return ground in  #1 ;return snow  in  #1 ;return twigs in  #1 ;return #4 that #5 are sticking up through ;return #3 that is covered with #6 ;return #2 that is walking on #7 ;return number of  #8 ;return if  #9 is at least one']
03/13/2022 08:45:02 - INFO - __main__ -  [break-QDMR] question: If both images shows a perfume box with a human being on it.
03/13/2022 08:45:02 - INFO - __main__ - ['return perfume box ;return human being ;return #1 with #2 on it ;return images of #3 ;return number of  #4 ;return if  #5 is equal to  two']
03/13/2022 08:45:02 - INFO - __main__ -  [break-QDMR] question: How many touchdowns did the Browns score in the first quarter?
03/13/2022 08:45:02 - INFO - __main__ - ['return Browns ;return touchdowns of #1 ;return #2 in  the  first quarter ;return number of  #3']
03/13/2022 08:45:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 08:45:02 - INFO - __main__ - Tokenizing Output ...
03/13/2022 08:45:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 08:45:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 08:45:02 - INFO - __main__ - Printing 3 examples
03/13/2022 08:45:02 - INFO - __main__ -  [break-QDMR] question: How many years did Konbaung control the upper Tenasserim peninsula?
03/13/2022 08:45:02 - INFO - __main__ - ['return Konbaung did control the  upper Tenasserim peninsula ;return years of #1 ;return number of  #2']
03/13/2022 08:45:02 - INFO - __main__ -  [break-QDMR] question: What Daytona popular attraction venue has 167785 as its capacity?
03/13/2022 08:45:02 - INFO - __main__ - ['return Daytona ;return popular attractions of #1 ;return venues of #2 ;return capacities of #3 ;return #2 where #4 is 167785']
03/13/2022 08:45:02 - INFO - __main__ -  [break-QDMR] question: How many touchdown runs were made for the same yardage?
03/13/2022 08:45:02 - INFO - __main__ - ['return touchdown runs ;return yardages of #1 ;return #1 where #2 is the  same ;return number of  #3']
03/13/2022 08:45:02 - INFO - __main__ - Tokenizing Input ...
03/13/2022 08:45:02 - INFO - __main__ - Tokenizing Output ...
03/13/2022 08:45:02 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 08:45:06 - INFO - __main__ - Global step 600 Train loss 0.139994 EM 0.0 on epoch=299
03/13/2022 08:45:06 - INFO - __main__ - save last model!
03/13/2022 08:45:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 08:45:13 - INFO - __main__ - Starting training!
03/13/2022 08:45:13 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 08:45:14 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 08:45:14 - INFO - __main__ - Printing 3 examples
03/13/2022 08:45:14 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 08:45:14 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 08:45:14 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 08:45:14 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 08:45:14 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 08:45:14 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 08:45:14 - INFO - __main__ - Tokenizing Input ...
03/13/2022 08:45:17 - INFO - __main__ - Tokenizing Output ...
03/13/2022 08:45:25 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 09:08:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_13_0.0001_8_predictions.txt
03/13/2022 09:08:41 - INFO - __main__ - EM on test data: 0.0034
03/13/2022 09:08:41 - INFO - __main__ - prefix=break-QDMR_32_13, lr=0.0001, bsz=8, dev_performance=0.03125, test_performance=0.0033505154639175256
03/13/2022 09:08:41 - INFO - __main__ - Running ... prefix=break-QDMR_32_21, lr=0.0005, bsz=8 ...
03/13/2022 09:08:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 09:08:42 - INFO - __main__ - Printing 3 examples
03/13/2022 09:08:42 - INFO - __main__ -  [break-QDMR] question: If in the left image there is a wold walking on snow covered ground with twigs sticking up through the snow
03/13/2022 09:08:42 - INFO - __main__ - ['return left image ;return wold in  #1 ;return ground in  #1 ;return snow  in  #1 ;return twigs in  #1 ;return #4 that #5 are sticking up through ;return #3 that is covered with #6 ;return #2 that is walking on #7 ;return number of  #8 ;return if  #9 is at least one']
03/13/2022 09:08:42 - INFO - __main__ -  [break-QDMR] question: If both images shows a perfume box with a human being on it.
03/13/2022 09:08:42 - INFO - __main__ - ['return perfume box ;return human being ;return #1 with #2 on it ;return images of #3 ;return number of  #4 ;return if  #5 is equal to  two']
03/13/2022 09:08:42 - INFO - __main__ -  [break-QDMR] question: How many touchdowns did the Browns score in the first quarter?
03/13/2022 09:08:42 - INFO - __main__ - ['return Browns ;return touchdowns of #1 ;return #2 in  the  first quarter ;return number of  #3']
03/13/2022 09:08:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 09:08:42 - INFO - __main__ - Tokenizing Output ...
03/13/2022 09:08:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 09:08:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 09:08:42 - INFO - __main__ - Printing 3 examples
03/13/2022 09:08:42 - INFO - __main__ -  [break-QDMR] question: How many years did Konbaung control the upper Tenasserim peninsula?
03/13/2022 09:08:42 - INFO - __main__ - ['return Konbaung did control the  upper Tenasserim peninsula ;return years of #1 ;return number of  #2']
03/13/2022 09:08:42 - INFO - __main__ -  [break-QDMR] question: What Daytona popular attraction venue has 167785 as its capacity?
03/13/2022 09:08:42 - INFO - __main__ - ['return Daytona ;return popular attractions of #1 ;return venues of #2 ;return capacities of #3 ;return #2 where #4 is 167785']
03/13/2022 09:08:42 - INFO - __main__ -  [break-QDMR] question: How many touchdown runs were made for the same yardage?
03/13/2022 09:08:42 - INFO - __main__ - ['return touchdown runs ;return yardages of #1 ;return #1 where #2 is the  same ;return number of  #3']
03/13/2022 09:08:42 - INFO - __main__ - Tokenizing Input ...
03/13/2022 09:08:42 - INFO - __main__ - Tokenizing Output ...
03/13/2022 09:08:42 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 09:08:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 09:08:53 - INFO - __main__ - Starting training!
03/13/2022 09:08:58 - INFO - __main__ - Step 10 Global step 10 Train loss 21.403440 on epoch=4
03/13/2022 09:09:02 - INFO - __main__ - Step 20 Global step 20 Train loss 12.548373 on epoch=9
03/13/2022 09:09:07 - INFO - __main__ - Step 30 Global step 30 Train loss 5.248944 on epoch=14
03/13/2022 09:09:12 - INFO - __main__ - Step 40 Global step 40 Train loss 2.945282 on epoch=19
03/13/2022 09:09:17 - INFO - __main__ - Step 50 Global step 50 Train loss 1.921242 on epoch=24
03/13/2022 09:09:27 - INFO - __main__ - Global step 50 Train loss 8.813457 EM 0.0 on epoch=24
03/13/2022 09:09:33 - INFO - __main__ - Step 60 Global step 60 Train loss 1.336384 on epoch=29
03/13/2022 09:09:38 - INFO - __main__ - Step 70 Global step 70 Train loss 1.020993 on epoch=34
03/13/2022 09:09:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.792266 on epoch=39
03/13/2022 09:09:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.573621 on epoch=44
03/13/2022 09:09:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.456425 on epoch=49
03/13/2022 09:10:03 - INFO - __main__ - Global step 100 Train loss 0.835938 EM 0.0 on epoch=49
03/13/2022 09:10:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.335882 on epoch=54
03/13/2022 09:10:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.260442 on epoch=59
03/13/2022 09:10:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.263382 on epoch=64
03/13/2022 09:10:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.252775 on epoch=69
03/13/2022 09:10:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.185341 on epoch=74
03/13/2022 09:10:33 - INFO - __main__ - Global step 150 Train loss 0.259564 EM 0.0 on epoch=74
03/13/2022 09:10:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.236084 on epoch=79
03/13/2022 09:10:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.225561 on epoch=84
03/13/2022 09:10:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.195740 on epoch=89
03/13/2022 09:10:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.155538 on epoch=94
03/13/2022 09:10:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.123966 on epoch=99
03/13/2022 09:11:04 - INFO - __main__ - Global step 200 Train loss 0.187378 EM 0.0 on epoch=99
03/13/2022 09:11:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.102226 on epoch=104
03/13/2022 09:11:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.078461 on epoch=109
03/13/2022 09:11:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.091367 on epoch=114
03/13/2022 09:11:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.043900 on epoch=119
03/13/2022 09:11:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.066267 on epoch=124
03/13/2022 09:11:35 - INFO - __main__ - Global step 250 Train loss 0.076444 EM 0.0 on epoch=124
03/13/2022 09:11:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.039646 on epoch=129
03/13/2022 09:11:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.036212 on epoch=134
03/13/2022 09:11:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.039374 on epoch=139
03/13/2022 09:11:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.032106 on epoch=144
03/13/2022 09:12:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.044841 on epoch=149
03/13/2022 09:12:05 - INFO - __main__ - Global step 300 Train loss 0.038436 EM 0.0 on epoch=149
03/13/2022 09:12:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.031712 on epoch=154
03/13/2022 09:12:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.026448 on epoch=159
03/13/2022 09:12:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.018843 on epoch=164
03/13/2022 09:12:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.023493 on epoch=169
03/13/2022 09:12:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.021037 on epoch=174
03/13/2022 09:12:35 - INFO - __main__ - Global step 350 Train loss 0.024307 EM 0.0 on epoch=174
03/13/2022 09:12:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.012353 on epoch=179
03/13/2022 09:12:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.013010 on epoch=184
03/13/2022 09:12:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.016812 on epoch=189
03/13/2022 09:12:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.025055 on epoch=194
03/13/2022 09:13:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.015747 on epoch=199
03/13/2022 09:13:06 - INFO - __main__ - Global step 400 Train loss 0.016595 EM 0.0 on epoch=199
03/13/2022 09:13:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.026561 on epoch=204
03/13/2022 09:13:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.014245 on epoch=209
03/13/2022 09:13:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.012868 on epoch=214
03/13/2022 09:13:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.007650 on epoch=219
03/13/2022 09:13:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.020332 on epoch=224
03/13/2022 09:13:38 - INFO - __main__ - Global step 450 Train loss 0.016331 EM 0.0 on epoch=224
03/13/2022 09:13:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.012302 on epoch=229
03/13/2022 09:13:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.012511 on epoch=234
03/13/2022 09:13:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.006605 on epoch=239
03/13/2022 09:13:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.019497 on epoch=244
03/13/2022 09:14:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.014509 on epoch=249
03/13/2022 09:14:09 - INFO - __main__ - Global step 500 Train loss 0.013085 EM 0.0 on epoch=249
03/13/2022 09:14:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.011970 on epoch=254
03/13/2022 09:14:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.004452 on epoch=259
03/13/2022 09:14:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001694 on epoch=264
03/13/2022 09:14:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.003178 on epoch=269
03/13/2022 09:14:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.019334 on epoch=274
03/13/2022 09:14:40 - INFO - __main__ - Global step 550 Train loss 0.008126 EM 0.0 on epoch=274
03/13/2022 09:14:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.009005 on epoch=279
03/13/2022 09:14:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.028949 on epoch=284
03/13/2022 09:14:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.008008 on epoch=289
03/13/2022 09:15:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.006143 on epoch=294
03/13/2022 09:15:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.012220 on epoch=299
03/13/2022 09:15:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 09:15:06 - INFO - __main__ - Printing 3 examples
03/13/2022 09:15:06 - INFO - __main__ -  [break-QDMR] question: If in the left image there is a wold walking on snow covered ground with twigs sticking up through the snow
03/13/2022 09:15:06 - INFO - __main__ - ['return left image ;return wold in  #1 ;return ground in  #1 ;return snow  in  #1 ;return twigs in  #1 ;return #4 that #5 are sticking up through ;return #3 that is covered with #6 ;return #2 that is walking on #7 ;return number of  #8 ;return if  #9 is at least one']
03/13/2022 09:15:06 - INFO - __main__ -  [break-QDMR] question: If both images shows a perfume box with a human being on it.
03/13/2022 09:15:06 - INFO - __main__ - ['return perfume box ;return human being ;return #1 with #2 on it ;return images of #3 ;return number of  #4 ;return if  #5 is equal to  two']
03/13/2022 09:15:06 - INFO - __main__ -  [break-QDMR] question: How many touchdowns did the Browns score in the first quarter?
03/13/2022 09:15:06 - INFO - __main__ - ['return Browns ;return touchdowns of #1 ;return #2 in  the  first quarter ;return number of  #3']
03/13/2022 09:15:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 09:15:06 - INFO - __main__ - Tokenizing Output ...
03/13/2022 09:15:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 09:15:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 09:15:06 - INFO - __main__ - Printing 3 examples
03/13/2022 09:15:06 - INFO - __main__ -  [break-QDMR] question: How many years did Konbaung control the upper Tenasserim peninsula?
03/13/2022 09:15:06 - INFO - __main__ - ['return Konbaung did control the  upper Tenasserim peninsula ;return years of #1 ;return number of  #2']
03/13/2022 09:15:06 - INFO - __main__ -  [break-QDMR] question: What Daytona popular attraction venue has 167785 as its capacity?
03/13/2022 09:15:06 - INFO - __main__ - ['return Daytona ;return popular attractions of #1 ;return venues of #2 ;return capacities of #3 ;return #2 where #4 is 167785']
03/13/2022 09:15:06 - INFO - __main__ -  [break-QDMR] question: How many touchdown runs were made for the same yardage?
03/13/2022 09:15:06 - INFO - __main__ - ['return touchdown runs ;return yardages of #1 ;return #1 where #2 is the  same ;return number of  #3']
03/13/2022 09:15:06 - INFO - __main__ - Tokenizing Input ...
03/13/2022 09:15:06 - INFO - __main__ - Tokenizing Output ...
03/13/2022 09:15:06 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 09:15:10 - INFO - __main__ - Global step 600 Train loss 0.012865 EM 0.0 on epoch=299
03/13/2022 09:15:10 - INFO - __main__ - save last model!
03/13/2022 09:15:19 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 09:15:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 09:15:19 - INFO - __main__ - Starting training!
03/13/2022 09:15:19 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 09:15:19 - INFO - __main__ - Printing 3 examples
03/13/2022 09:15:19 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 09:15:19 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 09:15:19 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 09:15:19 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 09:15:19 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 09:15:19 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 09:15:19 - INFO - __main__ - Tokenizing Input ...
03/13/2022 09:15:22 - INFO - __main__ - Tokenizing Output ...
03/13/2022 09:15:30 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 09:54:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_21_0.0005_8_predictions.txt
03/13/2022 09:54:31 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 09:54:31 - INFO - __main__ - prefix=break-QDMR_32_21, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 09:54:31 - INFO - __main__ - Running ... prefix=break-QDMR_32_21, lr=0.0003, bsz=8 ...
03/13/2022 09:54:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 09:54:32 - INFO - __main__ - Printing 3 examples
03/13/2022 09:54:32 - INFO - __main__ -  [break-QDMR] question: If in the left image there is a wold walking on snow covered ground with twigs sticking up through the snow
03/13/2022 09:54:32 - INFO - __main__ - ['return left image ;return wold in  #1 ;return ground in  #1 ;return snow  in  #1 ;return twigs in  #1 ;return #4 that #5 are sticking up through ;return #3 that is covered with #6 ;return #2 that is walking on #7 ;return number of  #8 ;return if  #9 is at least one']
03/13/2022 09:54:32 - INFO - __main__ -  [break-QDMR] question: If both images shows a perfume box with a human being on it.
03/13/2022 09:54:32 - INFO - __main__ - ['return perfume box ;return human being ;return #1 with #2 on it ;return images of #3 ;return number of  #4 ;return if  #5 is equal to  two']
03/13/2022 09:54:32 - INFO - __main__ -  [break-QDMR] question: How many touchdowns did the Browns score in the first quarter?
03/13/2022 09:54:32 - INFO - __main__ - ['return Browns ;return touchdowns of #1 ;return #2 in  the  first quarter ;return number of  #3']
03/13/2022 09:54:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 09:54:32 - INFO - __main__ - Tokenizing Output ...
03/13/2022 09:54:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 09:54:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 09:54:32 - INFO - __main__ - Printing 3 examples
03/13/2022 09:54:32 - INFO - __main__ -  [break-QDMR] question: How many years did Konbaung control the upper Tenasserim peninsula?
03/13/2022 09:54:32 - INFO - __main__ - ['return Konbaung did control the  upper Tenasserim peninsula ;return years of #1 ;return number of  #2']
03/13/2022 09:54:32 - INFO - __main__ -  [break-QDMR] question: What Daytona popular attraction venue has 167785 as its capacity?
03/13/2022 09:54:32 - INFO - __main__ - ['return Daytona ;return popular attractions of #1 ;return venues of #2 ;return capacities of #3 ;return #2 where #4 is 167785']
03/13/2022 09:54:32 - INFO - __main__ -  [break-QDMR] question: How many touchdown runs were made for the same yardage?
03/13/2022 09:54:32 - INFO - __main__ - ['return touchdown runs ;return yardages of #1 ;return #1 where #2 is the  same ;return number of  #3']
03/13/2022 09:54:32 - INFO - __main__ - Tokenizing Input ...
03/13/2022 09:54:32 - INFO - __main__ - Tokenizing Output ...
03/13/2022 09:54:32 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 09:54:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 09:54:45 - INFO - __main__ - Starting training!
03/13/2022 09:54:49 - INFO - __main__ - Step 10 Global step 10 Train loss 21.432095 on epoch=4
03/13/2022 09:54:54 - INFO - __main__ - Step 20 Global step 20 Train loss 13.113287 on epoch=9
03/13/2022 09:54:58 - INFO - __main__ - Step 30 Global step 30 Train loss 7.058741 on epoch=14
03/13/2022 09:55:03 - INFO - __main__ - Step 40 Global step 40 Train loss 5.880805 on epoch=19
03/13/2022 09:55:07 - INFO - __main__ - Step 50 Global step 50 Train loss 5.461066 on epoch=24
03/13/2022 09:55:19 - INFO - __main__ - Global step 50 Train loss 10.589198 EM 0.0 on epoch=24
03/13/2022 09:55:25 - INFO - __main__ - Step 60 Global step 60 Train loss 5.227458 on epoch=29
03/13/2022 09:55:30 - INFO - __main__ - Step 70 Global step 70 Train loss 4.953238 on epoch=34
03/13/2022 09:55:35 - INFO - __main__ - Step 80 Global step 80 Train loss 4.493644 on epoch=39
03/13/2022 09:55:40 - INFO - __main__ - Step 90 Global step 90 Train loss 3.890266 on epoch=44
03/13/2022 09:55:44 - INFO - __main__ - Step 100 Global step 100 Train loss 2.999939 on epoch=49
03/13/2022 09:55:54 - INFO - __main__ - Global step 100 Train loss 4.312909 EM 0.0 on epoch=49
03/13/2022 09:55:59 - INFO - __main__ - Step 110 Global step 110 Train loss 2.397483 on epoch=54
03/13/2022 09:56:04 - INFO - __main__ - Step 120 Global step 120 Train loss 1.889733 on epoch=59
03/13/2022 09:56:09 - INFO - __main__ - Step 130 Global step 130 Train loss 1.463882 on epoch=64
03/13/2022 09:56:14 - INFO - __main__ - Step 140 Global step 140 Train loss 1.249532 on epoch=69
03/13/2022 09:56:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.996005 on epoch=74
03/13/2022 09:56:29 - INFO - __main__ - Global step 150 Train loss 1.599327 EM 0.0 on epoch=74
03/13/2022 09:56:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.802671 on epoch=79
03/13/2022 09:56:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.688181 on epoch=84
03/13/2022 09:56:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.543801 on epoch=89
03/13/2022 09:56:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.485005 on epoch=94
03/13/2022 09:56:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.442718 on epoch=99
03/13/2022 09:57:02 - INFO - __main__ - Global step 200 Train loss 0.592475 EM 0.0 on epoch=99
03/13/2022 09:57:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.347521 on epoch=104
03/13/2022 09:57:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.316833 on epoch=109
03/13/2022 09:57:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.266417 on epoch=114
03/13/2022 09:57:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.231903 on epoch=119
03/13/2022 09:57:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.260398 on epoch=124
03/13/2022 09:57:33 - INFO - __main__ - Global step 250 Train loss 0.284615 EM 0.0 on epoch=124
03/13/2022 09:57:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.191881 on epoch=129
03/13/2022 09:57:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.179534 on epoch=134
03/13/2022 09:57:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.145933 on epoch=139
03/13/2022 09:57:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.149339 on epoch=144
03/13/2022 09:57:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.133452 on epoch=149
03/13/2022 09:58:05 - INFO - __main__ - Global step 300 Train loss 0.160028 EM 0.0 on epoch=149
03/13/2022 09:58:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.132103 on epoch=154
03/13/2022 09:58:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.116109 on epoch=159
03/13/2022 09:58:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.101288 on epoch=164
03/13/2022 09:58:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.110072 on epoch=169
03/13/2022 09:58:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.088836 on epoch=174
03/13/2022 09:58:36 - INFO - __main__ - Global step 350 Train loss 0.109682 EM 0.0 on epoch=174
03/13/2022 09:58:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.084471 on epoch=179
03/13/2022 09:58:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.074479 on epoch=184
03/13/2022 09:58:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.073726 on epoch=189
03/13/2022 09:58:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.073233 on epoch=194
03/13/2022 09:59:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.070296 on epoch=199
03/13/2022 09:59:06 - INFO - __main__ - Global step 400 Train loss 0.075241 EM 0.0 on epoch=199
03/13/2022 09:59:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.071118 on epoch=204
03/13/2022 09:59:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.058737 on epoch=209
03/13/2022 09:59:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.046825 on epoch=214
03/13/2022 09:59:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.053005 on epoch=219
03/13/2022 09:59:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.048708 on epoch=224
03/13/2022 09:59:37 - INFO - __main__ - Global step 450 Train loss 0.055679 EM 0.0 on epoch=224
03/13/2022 09:59:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.063245 on epoch=229
03/13/2022 09:59:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.052302 on epoch=234
03/13/2022 09:59:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.042151 on epoch=239
03/13/2022 09:59:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.050433 on epoch=244
03/13/2022 10:00:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.028733 on epoch=249
03/13/2022 10:00:08 - INFO - __main__ - Global step 500 Train loss 0.047373 EM 0.0 on epoch=249
03/13/2022 10:00:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.031321 on epoch=254
03/13/2022 10:00:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.040115 on epoch=259
03/13/2022 10:00:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.043957 on epoch=264
03/13/2022 10:00:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.029088 on epoch=269
03/13/2022 10:00:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.025749 on epoch=274
03/13/2022 10:00:41 - INFO - __main__ - Global step 550 Train loss 0.034046 EM 0.0 on epoch=274
03/13/2022 10:00:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.026557 on epoch=279
03/13/2022 10:00:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.031908 on epoch=284
03/13/2022 10:00:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.033888 on epoch=289
03/13/2022 10:01:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.018150 on epoch=294
03/13/2022 10:01:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.027416 on epoch=299
03/13/2022 10:01:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 10:01:07 - INFO - __main__ - Printing 3 examples
03/13/2022 10:01:07 - INFO - __main__ -  [break-QDMR] question: If in the left image there is a wold walking on snow covered ground with twigs sticking up through the snow
03/13/2022 10:01:07 - INFO - __main__ - ['return left image ;return wold in  #1 ;return ground in  #1 ;return snow  in  #1 ;return twigs in  #1 ;return #4 that #5 are sticking up through ;return #3 that is covered with #6 ;return #2 that is walking on #7 ;return number of  #8 ;return if  #9 is at least one']
03/13/2022 10:01:07 - INFO - __main__ -  [break-QDMR] question: If both images shows a perfume box with a human being on it.
03/13/2022 10:01:07 - INFO - __main__ - ['return perfume box ;return human being ;return #1 with #2 on it ;return images of #3 ;return number of  #4 ;return if  #5 is equal to  two']
03/13/2022 10:01:07 - INFO - __main__ -  [break-QDMR] question: How many touchdowns did the Browns score in the first quarter?
03/13/2022 10:01:07 - INFO - __main__ - ['return Browns ;return touchdowns of #1 ;return #2 in  the  first quarter ;return number of  #3']
03/13/2022 10:01:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 10:01:07 - INFO - __main__ - Tokenizing Output ...
03/13/2022 10:01:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 10:01:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 10:01:07 - INFO - __main__ - Printing 3 examples
03/13/2022 10:01:07 - INFO - __main__ -  [break-QDMR] question: How many years did Konbaung control the upper Tenasserim peninsula?
03/13/2022 10:01:07 - INFO - __main__ - ['return Konbaung did control the  upper Tenasserim peninsula ;return years of #1 ;return number of  #2']
03/13/2022 10:01:07 - INFO - __main__ -  [break-QDMR] question: What Daytona popular attraction venue has 167785 as its capacity?
03/13/2022 10:01:07 - INFO - __main__ - ['return Daytona ;return popular attractions of #1 ;return venues of #2 ;return capacities of #3 ;return #2 where #4 is 167785']
03/13/2022 10:01:07 - INFO - __main__ -  [break-QDMR] question: How many touchdown runs were made for the same yardage?
03/13/2022 10:01:07 - INFO - __main__ - ['return touchdown runs ;return yardages of #1 ;return #1 where #2 is the  same ;return number of  #3']
03/13/2022 10:01:07 - INFO - __main__ - Tokenizing Input ...
03/13/2022 10:01:07 - INFO - __main__ - Tokenizing Output ...
03/13/2022 10:01:07 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 10:01:13 - INFO - __main__ - Global step 600 Train loss 0.027584 EM 0.0 on epoch=299
03/13/2022 10:01:13 - INFO - __main__ - save last model!
03/13/2022 10:01:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 10:01:20 - INFO - __main__ - Starting training!
03/13/2022 10:01:20 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 10:01:21 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 10:01:21 - INFO - __main__ - Printing 3 examples
03/13/2022 10:01:21 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 10:01:21 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 10:01:21 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 10:01:21 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 10:01:21 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 10:01:21 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 10:01:21 - INFO - __main__ - Tokenizing Input ...
03/13/2022 10:01:24 - INFO - __main__ - Tokenizing Output ...
03/13/2022 10:01:32 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 10:45:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_21_0.0003_8_predictions.txt
03/13/2022 10:45:20 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 10:45:22 - INFO - __main__ - prefix=break-QDMR_32_21, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 10:45:22 - INFO - __main__ - Running ... prefix=break-QDMR_32_21, lr=0.0002, bsz=8 ...
03/13/2022 10:45:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 10:45:22 - INFO - __main__ - Printing 3 examples
03/13/2022 10:45:22 - INFO - __main__ -  [break-QDMR] question: If in the left image there is a wold walking on snow covered ground with twigs sticking up through the snow
03/13/2022 10:45:22 - INFO - __main__ - ['return left image ;return wold in  #1 ;return ground in  #1 ;return snow  in  #1 ;return twigs in  #1 ;return #4 that #5 are sticking up through ;return #3 that is covered with #6 ;return #2 that is walking on #7 ;return number of  #8 ;return if  #9 is at least one']
03/13/2022 10:45:22 - INFO - __main__ -  [break-QDMR] question: If both images shows a perfume box with a human being on it.
03/13/2022 10:45:22 - INFO - __main__ - ['return perfume box ;return human being ;return #1 with #2 on it ;return images of #3 ;return number of  #4 ;return if  #5 is equal to  two']
03/13/2022 10:45:22 - INFO - __main__ -  [break-QDMR] question: How many touchdowns did the Browns score in the first quarter?
03/13/2022 10:45:22 - INFO - __main__ - ['return Browns ;return touchdowns of #1 ;return #2 in  the  first quarter ;return number of  #3']
03/13/2022 10:45:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 10:45:22 - INFO - __main__ - Tokenizing Output ...
03/13/2022 10:45:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 10:45:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 10:45:22 - INFO - __main__ - Printing 3 examples
03/13/2022 10:45:22 - INFO - __main__ -  [break-QDMR] question: How many years did Konbaung control the upper Tenasserim peninsula?
03/13/2022 10:45:22 - INFO - __main__ - ['return Konbaung did control the  upper Tenasserim peninsula ;return years of #1 ;return number of  #2']
03/13/2022 10:45:22 - INFO - __main__ -  [break-QDMR] question: What Daytona popular attraction venue has 167785 as its capacity?
03/13/2022 10:45:22 - INFO - __main__ - ['return Daytona ;return popular attractions of #1 ;return venues of #2 ;return capacities of #3 ;return #2 where #4 is 167785']
03/13/2022 10:45:22 - INFO - __main__ -  [break-QDMR] question: How many touchdown runs were made for the same yardage?
03/13/2022 10:45:22 - INFO - __main__ - ['return touchdown runs ;return yardages of #1 ;return #1 where #2 is the  same ;return number of  #3']
03/13/2022 10:45:22 - INFO - __main__ - Tokenizing Input ...
03/13/2022 10:45:22 - INFO - __main__ - Tokenizing Output ...
03/13/2022 10:45:23 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 10:45:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 10:45:34 - INFO - __main__ - Starting training!
03/13/2022 10:45:38 - INFO - __main__ - Step 10 Global step 10 Train loss 20.958347 on epoch=4
03/13/2022 10:45:43 - INFO - __main__ - Step 20 Global step 20 Train loss 14.682065 on epoch=9
03/13/2022 10:45:48 - INFO - __main__ - Step 30 Global step 30 Train loss 8.865798 on epoch=14
03/13/2022 10:45:53 - INFO - __main__ - Step 40 Global step 40 Train loss 6.437499 on epoch=19
03/13/2022 10:45:58 - INFO - __main__ - Step 50 Global step 50 Train loss 5.680892 on epoch=24
03/13/2022 10:46:10 - INFO - __main__ - Global step 50 Train loss 11.324921 EM 0.0 on epoch=24
03/13/2022 10:46:15 - INFO - __main__ - Step 60 Global step 60 Train loss 5.124084 on epoch=29
03/13/2022 10:46:20 - INFO - __main__ - Step 70 Global step 70 Train loss 4.718488 on epoch=34
03/13/2022 10:46:25 - INFO - __main__ - Step 80 Global step 80 Train loss 4.451524 on epoch=39
03/13/2022 10:46:30 - INFO - __main__ - Step 90 Global step 90 Train loss 4.322318 on epoch=44
03/13/2022 10:46:36 - INFO - __main__ - Step 100 Global step 100 Train loss 4.027993 on epoch=49
03/13/2022 10:46:47 - INFO - __main__ - Global step 100 Train loss 4.528882 EM 0.0 on epoch=49
03/13/2022 10:46:51 - INFO - __main__ - Step 110 Global step 110 Train loss 3.886819 on epoch=54
03/13/2022 10:46:56 - INFO - __main__ - Step 120 Global step 120 Train loss 3.573881 on epoch=59
03/13/2022 10:47:01 - INFO - __main__ - Step 130 Global step 130 Train loss 3.270325 on epoch=64
03/13/2022 10:47:06 - INFO - __main__ - Step 140 Global step 140 Train loss 3.015305 on epoch=69
03/13/2022 10:47:11 - INFO - __main__ - Step 150 Global step 150 Train loss 2.798186 on epoch=74
03/13/2022 10:47:22 - INFO - __main__ - Global step 150 Train loss 3.308903 EM 0.0 on epoch=74
03/13/2022 10:47:27 - INFO - __main__ - Step 160 Global step 160 Train loss 2.777477 on epoch=79
03/13/2022 10:47:32 - INFO - __main__ - Step 170 Global step 170 Train loss 2.351976 on epoch=84
03/13/2022 10:47:37 - INFO - __main__ - Step 180 Global step 180 Train loss 2.104378 on epoch=89
03/13/2022 10:47:42 - INFO - __main__ - Step 190 Global step 190 Train loss 1.888702 on epoch=94
03/13/2022 10:47:47 - INFO - __main__ - Step 200 Global step 200 Train loss 1.647758 on epoch=99
03/13/2022 10:47:58 - INFO - __main__ - Global step 200 Train loss 2.154058 EM 0.0 on epoch=99
03/13/2022 10:48:03 - INFO - __main__ - Step 210 Global step 210 Train loss 1.495540 on epoch=104
03/13/2022 10:48:08 - INFO - __main__ - Step 220 Global step 220 Train loss 1.352256 on epoch=109
03/13/2022 10:48:13 - INFO - __main__ - Step 230 Global step 230 Train loss 1.204933 on epoch=114
03/13/2022 10:48:18 - INFO - __main__ - Step 240 Global step 240 Train loss 1.038115 on epoch=119
03/13/2022 10:48:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.863867 on epoch=124
03/13/2022 10:48:31 - INFO - __main__ - Global step 250 Train loss 1.190942 EM 0.0 on epoch=124
03/13/2022 10:48:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.709671 on epoch=129
03/13/2022 10:48:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.592841 on epoch=134
03/13/2022 10:48:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.527455 on epoch=139
03/13/2022 10:48:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.453527 on epoch=144
03/13/2022 10:48:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.375468 on epoch=149
03/13/2022 10:49:05 - INFO - __main__ - Global step 300 Train loss 0.531792 EM 0.03125 on epoch=149
03/13/2022 10:49:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.325847 on epoch=154
03/13/2022 10:49:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.267989 on epoch=159
03/13/2022 10:49:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.262733 on epoch=164
03/13/2022 10:49:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.243388 on epoch=169
03/13/2022 10:49:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.229759 on epoch=174
03/13/2022 10:49:38 - INFO - __main__ - Global step 350 Train loss 0.265943 EM 0.0 on epoch=174
03/13/2022 10:49:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.322809 on epoch=179
03/13/2022 10:49:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.222913 on epoch=184
03/13/2022 10:49:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.212415 on epoch=189
03/13/2022 10:49:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.304405 on epoch=194
03/13/2022 10:50:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.361284 on epoch=199
03/13/2022 10:50:08 - INFO - __main__ - Global step 400 Train loss 0.284765 EM 0.0 on epoch=199
03/13/2022 10:50:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.161529 on epoch=204
03/13/2022 10:50:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.142320 on epoch=209
03/13/2022 10:50:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.086525 on epoch=214
03/13/2022 10:50:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.109270 on epoch=219
03/13/2022 10:50:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.100415 on epoch=224
03/13/2022 10:50:39 - INFO - __main__ - Global step 450 Train loss 0.120012 EM 0.0 on epoch=224
03/13/2022 10:50:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.172780 on epoch=229
03/13/2022 10:50:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.143043 on epoch=234
03/13/2022 10:50:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.111681 on epoch=239
03/13/2022 10:50:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.084608 on epoch=244
03/13/2022 10:51:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.100546 on epoch=249
03/13/2022 10:51:09 - INFO - __main__ - Global step 500 Train loss 0.122532 EM 0.0 on epoch=249
03/13/2022 10:51:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.132794 on epoch=254
03/13/2022 10:51:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.125637 on epoch=259
03/13/2022 10:51:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.125782 on epoch=264
03/13/2022 10:51:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.108645 on epoch=269
03/13/2022 10:51:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.100549 on epoch=274
03/13/2022 10:51:40 - INFO - __main__ - Global step 550 Train loss 0.118681 EM 0.0 on epoch=274
03/13/2022 10:51:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.083520 on epoch=279
03/13/2022 10:51:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.072253 on epoch=284
03/13/2022 10:51:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.082357 on epoch=289
03/13/2022 10:52:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.073781 on epoch=294
03/13/2022 10:52:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.067574 on epoch=299
03/13/2022 10:52:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 10:52:06 - INFO - __main__ - Printing 3 examples
03/13/2022 10:52:06 - INFO - __main__ -  [break-QDMR] question: If in the left image there is a wold walking on snow covered ground with twigs sticking up through the snow
03/13/2022 10:52:06 - INFO - __main__ - ['return left image ;return wold in  #1 ;return ground in  #1 ;return snow  in  #1 ;return twigs in  #1 ;return #4 that #5 are sticking up through ;return #3 that is covered with #6 ;return #2 that is walking on #7 ;return number of  #8 ;return if  #9 is at least one']
03/13/2022 10:52:06 - INFO - __main__ -  [break-QDMR] question: If both images shows a perfume box with a human being on it.
03/13/2022 10:52:06 - INFO - __main__ - ['return perfume box ;return human being ;return #1 with #2 on it ;return images of #3 ;return number of  #4 ;return if  #5 is equal to  two']
03/13/2022 10:52:06 - INFO - __main__ -  [break-QDMR] question: How many touchdowns did the Browns score in the first quarter?
03/13/2022 10:52:06 - INFO - __main__ - ['return Browns ;return touchdowns of #1 ;return #2 in  the  first quarter ;return number of  #3']
03/13/2022 10:52:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 10:52:06 - INFO - __main__ - Tokenizing Output ...
03/13/2022 10:52:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 10:52:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 10:52:07 - INFO - __main__ - Printing 3 examples
03/13/2022 10:52:07 - INFO - __main__ -  [break-QDMR] question: How many years did Konbaung control the upper Tenasserim peninsula?
03/13/2022 10:52:07 - INFO - __main__ - ['return Konbaung did control the  upper Tenasserim peninsula ;return years of #1 ;return number of  #2']
03/13/2022 10:52:07 - INFO - __main__ -  [break-QDMR] question: What Daytona popular attraction venue has 167785 as its capacity?
03/13/2022 10:52:07 - INFO - __main__ - ['return Daytona ;return popular attractions of #1 ;return venues of #2 ;return capacities of #3 ;return #2 where #4 is 167785']
03/13/2022 10:52:07 - INFO - __main__ -  [break-QDMR] question: How many touchdown runs were made for the same yardage?
03/13/2022 10:52:07 - INFO - __main__ - ['return touchdown runs ;return yardages of #1 ;return #1 where #2 is the  same ;return number of  #3']
03/13/2022 10:52:07 - INFO - __main__ - Tokenizing Input ...
03/13/2022 10:52:07 - INFO - __main__ - Tokenizing Output ...
03/13/2022 10:52:07 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 10:52:11 - INFO - __main__ - Global step 600 Train loss 0.075897 EM 0.0 on epoch=299
03/13/2022 10:52:11 - INFO - __main__ - save last model!
03/13/2022 10:52:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 10:52:17 - INFO - __main__ - Starting training!
03/13/2022 10:52:18 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 10:52:19 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 10:52:19 - INFO - __main__ - Printing 3 examples
03/13/2022 10:52:19 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 10:52:19 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 10:52:19 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 10:52:19 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 10:52:19 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 10:52:19 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 10:52:19 - INFO - __main__ - Tokenizing Input ...
03/13/2022 10:52:22 - INFO - __main__ - Tokenizing Output ...
03/13/2022 10:52:30 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 11:12:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_21_0.0002_8_predictions.txt
03/13/2022 11:12:36 - INFO - __main__ - EM on test data: 0.0024
03/13/2022 11:12:36 - INFO - __main__ - prefix=break-QDMR_32_21, lr=0.0002, bsz=8, dev_performance=0.03125, test_performance=0.0024484536082474227
03/13/2022 11:12:36 - INFO - __main__ - Running ... prefix=break-QDMR_32_21, lr=0.0001, bsz=8 ...
03/13/2022 11:12:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 11:12:37 - INFO - __main__ - Printing 3 examples
03/13/2022 11:12:37 - INFO - __main__ -  [break-QDMR] question: If in the left image there is a wold walking on snow covered ground with twigs sticking up through the snow
03/13/2022 11:12:37 - INFO - __main__ - ['return left image ;return wold in  #1 ;return ground in  #1 ;return snow  in  #1 ;return twigs in  #1 ;return #4 that #5 are sticking up through ;return #3 that is covered with #6 ;return #2 that is walking on #7 ;return number of  #8 ;return if  #9 is at least one']
03/13/2022 11:12:37 - INFO - __main__ -  [break-QDMR] question: If both images shows a perfume box with a human being on it.
03/13/2022 11:12:37 - INFO - __main__ - ['return perfume box ;return human being ;return #1 with #2 on it ;return images of #3 ;return number of  #4 ;return if  #5 is equal to  two']
03/13/2022 11:12:37 - INFO - __main__ -  [break-QDMR] question: How many touchdowns did the Browns score in the first quarter?
03/13/2022 11:12:37 - INFO - __main__ - ['return Browns ;return touchdowns of #1 ;return #2 in  the  first quarter ;return number of  #3']
03/13/2022 11:12:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 11:12:37 - INFO - __main__ - Tokenizing Output ...
03/13/2022 11:12:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 11:12:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 11:12:37 - INFO - __main__ - Printing 3 examples
03/13/2022 11:12:37 - INFO - __main__ -  [break-QDMR] question: How many years did Konbaung control the upper Tenasserim peninsula?
03/13/2022 11:12:37 - INFO - __main__ - ['return Konbaung did control the  upper Tenasserim peninsula ;return years of #1 ;return number of  #2']
03/13/2022 11:12:37 - INFO - __main__ -  [break-QDMR] question: What Daytona popular attraction venue has 167785 as its capacity?
03/13/2022 11:12:37 - INFO - __main__ - ['return Daytona ;return popular attractions of #1 ;return venues of #2 ;return capacities of #3 ;return #2 where #4 is 167785']
03/13/2022 11:12:37 - INFO - __main__ -  [break-QDMR] question: How many touchdown runs were made for the same yardage?
03/13/2022 11:12:37 - INFO - __main__ - ['return touchdown runs ;return yardages of #1 ;return #1 where #2 is the  same ;return number of  #3']
03/13/2022 11:12:37 - INFO - __main__ - Tokenizing Input ...
03/13/2022 11:12:37 - INFO - __main__ - Tokenizing Output ...
03/13/2022 11:12:37 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 11:12:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 11:12:48 - INFO - __main__ - Starting training!
03/13/2022 11:12:54 - INFO - __main__ - Step 10 Global step 10 Train loss 20.925819 on epoch=4
03/13/2022 11:12:58 - INFO - __main__ - Step 20 Global step 20 Train loss 18.558720 on epoch=9
03/13/2022 11:13:03 - INFO - __main__ - Step 30 Global step 30 Train loss 13.030545 on epoch=14
03/13/2022 11:13:08 - INFO - __main__ - Step 40 Global step 40 Train loss 10.107366 on epoch=19
03/13/2022 11:13:13 - INFO - __main__ - Step 50 Global step 50 Train loss 8.846495 on epoch=24
03/13/2022 11:13:27 - INFO - __main__ - Global step 50 Train loss 14.293790 EM 0.0 on epoch=24
03/13/2022 11:13:33 - INFO - __main__ - Step 60 Global step 60 Train loss 7.664223 on epoch=29
03/13/2022 11:13:38 - INFO - __main__ - Step 70 Global step 70 Train loss 7.106213 on epoch=34
03/13/2022 11:13:43 - INFO - __main__ - Step 80 Global step 80 Train loss 6.970191 on epoch=39
03/13/2022 11:13:48 - INFO - __main__ - Step 90 Global step 90 Train loss 6.586261 on epoch=44
03/13/2022 11:13:53 - INFO - __main__ - Step 100 Global step 100 Train loss 6.121999 on epoch=49
03/13/2022 11:14:04 - INFO - __main__ - Global step 100 Train loss 6.889776 EM 0.0 on epoch=49
03/13/2022 11:14:09 - INFO - __main__ - Step 110 Global step 110 Train loss 5.876488 on epoch=54
03/13/2022 11:14:14 - INFO - __main__ - Step 120 Global step 120 Train loss 5.491704 on epoch=59
03/13/2022 11:14:19 - INFO - __main__ - Step 130 Global step 130 Train loss 5.138078 on epoch=64
03/13/2022 11:14:24 - INFO - __main__ - Step 140 Global step 140 Train loss 4.788557 on epoch=69
03/13/2022 11:14:29 - INFO - __main__ - Step 150 Global step 150 Train loss 4.376722 on epoch=74
03/13/2022 11:14:41 - INFO - __main__ - Global step 150 Train loss 5.134310 EM 0.0 on epoch=74
03/13/2022 11:14:46 - INFO - __main__ - Step 160 Global step 160 Train loss 3.940379 on epoch=79
03/13/2022 11:14:51 - INFO - __main__ - Step 170 Global step 170 Train loss 3.674522 on epoch=84
03/13/2022 11:14:56 - INFO - __main__ - Step 180 Global step 180 Train loss 3.479312 on epoch=89
03/13/2022 11:15:01 - INFO - __main__ - Step 190 Global step 190 Train loss 3.115145 on epoch=94
03/13/2022 11:15:06 - INFO - __main__ - Step 200 Global step 200 Train loss 2.894218 on epoch=99
03/13/2022 11:15:17 - INFO - __main__ - Global step 200 Train loss 3.420715 EM 0.0 on epoch=99
03/13/2022 11:15:22 - INFO - __main__ - Step 210 Global step 210 Train loss 2.660455 on epoch=104
03/13/2022 11:15:27 - INFO - __main__ - Step 220 Global step 220 Train loss 2.527209 on epoch=109
03/13/2022 11:15:32 - INFO - __main__ - Step 230 Global step 230 Train loss 2.242340 on epoch=114
03/13/2022 11:15:37 - INFO - __main__ - Step 240 Global step 240 Train loss 2.166832 on epoch=119
03/13/2022 11:15:42 - INFO - __main__ - Step 250 Global step 250 Train loss 2.022883 on epoch=124
03/13/2022 11:15:52 - INFO - __main__ - Global step 250 Train loss 2.323944 EM 0.0 on epoch=124
03/13/2022 11:15:57 - INFO - __main__ - Step 260 Global step 260 Train loss 1.981853 on epoch=129
03/13/2022 11:16:02 - INFO - __main__ - Step 270 Global step 270 Train loss 1.776211 on epoch=134
03/13/2022 11:16:07 - INFO - __main__ - Step 280 Global step 280 Train loss 1.772768 on epoch=139
03/13/2022 11:16:12 - INFO - __main__ - Step 290 Global step 290 Train loss 1.593222 on epoch=144
03/13/2022 11:16:17 - INFO - __main__ - Step 300 Global step 300 Train loss 1.502709 on epoch=149
03/13/2022 11:16:26 - INFO - __main__ - Global step 300 Train loss 1.725352 EM 0.0 on epoch=149
03/13/2022 11:16:31 - INFO - __main__ - Step 310 Global step 310 Train loss 1.407684 on epoch=154
03/13/2022 11:16:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.327814 on epoch=159
03/13/2022 11:16:41 - INFO - __main__ - Step 330 Global step 330 Train loss 1.231485 on epoch=164
03/13/2022 11:16:46 - INFO - __main__ - Step 340 Global step 340 Train loss 1.209543 on epoch=169
03/13/2022 11:16:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.153108 on epoch=174
03/13/2022 11:16:56 - INFO - __main__ - Global step 350 Train loss 1.265927 EM 0.0 on epoch=174
03/13/2022 11:17:01 - INFO - __main__ - Step 360 Global step 360 Train loss 1.038836 on epoch=179
03/13/2022 11:17:06 - INFO - __main__ - Step 370 Global step 370 Train loss 1.007968 on epoch=184
03/13/2022 11:17:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.939747 on epoch=189
03/13/2022 11:17:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.868912 on epoch=194
03/13/2022 11:17:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.818177 on epoch=199
03/13/2022 11:17:26 - INFO - __main__ - Global step 400 Train loss 0.934728 EM 0.0 on epoch=199
03/13/2022 11:17:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.822423 on epoch=204
03/13/2022 11:17:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.748629 on epoch=209
03/13/2022 11:17:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.682467 on epoch=214
03/13/2022 11:17:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.660000 on epoch=219
03/13/2022 11:17:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.643043 on epoch=224
03/13/2022 11:17:56 - INFO - __main__ - Global step 450 Train loss 0.711312 EM 0.0 on epoch=224
03/13/2022 11:18:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.584190 on epoch=229
03/13/2022 11:18:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.530750 on epoch=234
03/13/2022 11:18:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.562796 on epoch=239
03/13/2022 11:18:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.501907 on epoch=244
03/13/2022 11:18:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.482612 on epoch=249
03/13/2022 11:18:26 - INFO - __main__ - Global step 500 Train loss 0.532451 EM 0.0 on epoch=249
03/13/2022 11:18:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.452174 on epoch=254
03/13/2022 11:18:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.440058 on epoch=259
03/13/2022 11:18:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.443788 on epoch=264
03/13/2022 11:18:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.398580 on epoch=269
03/13/2022 11:18:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.387354 on epoch=274
03/13/2022 11:18:56 - INFO - __main__ - Global step 550 Train loss 0.424391 EM 0.0 on epoch=274
03/13/2022 11:19:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.351908 on epoch=279
03/13/2022 11:19:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.397861 on epoch=284
03/13/2022 11:19:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.335010 on epoch=289
03/13/2022 11:19:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.334649 on epoch=294
03/13/2022 11:19:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.333099 on epoch=299
03/13/2022 11:19:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 11:19:22 - INFO - __main__ - Printing 3 examples
03/13/2022 11:19:22 - INFO - __main__ -  [break-QDMR] question: when did vincent von gogh die?
03/13/2022 11:19:22 - INFO - __main__ - ['return vincent von gogh ;return when  did  #1 die']
03/13/2022 11:19:22 - INFO - __main__ -  [break-QDMR] question: Does the Bronx have more non-Hispanic whites, or Hispanic whites?
03/13/2022 11:19:22 - INFO - __main__ - ['return the  Bronx ;return non-Hispanic whites in  #1 ;return Hispanic whites in  #1 ;return number of  #2 ;return number of  #3 ;return which is highest of #4 ,  #5']
03/13/2022 11:19:22 - INFO - __main__ -  [break-QDMR] question: What is the oldest log id and its corresponding problem id?
03/13/2022 11:19:22 - INFO - __main__ - ['return log ids ;return #1 that  is  the  oldest ;return corresponding problem id  of #2 ;return #2 ,  #3']
03/13/2022 11:19:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 11:19:22 - INFO - __main__ - Tokenizing Output ...
03/13/2022 11:19:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 11:19:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 11:19:23 - INFO - __main__ - Printing 3 examples
03/13/2022 11:19:23 - INFO - __main__ -  [break-QDMR] question: If the dog in the left photo has a star shapped tag hanging from its collar.
03/13/2022 11:19:23 - INFO - __main__ - ['return dog ;return collar of #1 ;return tag ;return #3 that is star shapped ;return #1 where  #2 has #4 hanging ;return if  #5 is in  the  left photo']
03/13/2022 11:19:23 - INFO - __main__ -  [break-QDMR] question: Who died from asphyxia and was the actor who played Alan Parrish in Jumanji?
03/13/2022 11:19:23 - INFO - __main__ - ['return Alan Parrish in  Jumanji ;return actor of #1 ;return #2 that died from  asphyxia']
03/13/2022 11:19:23 - INFO - __main__ -  [break-QDMR] question: show me flights from milwaukee to orlando on a thursday before noon 
03/13/2022 11:19:23 - INFO - __main__ - ['return flights ;return #1 from  milwaukee ;return #2 to orlando ;return #3 on  a thursday ;return #4 before noon']
03/13/2022 11:19:23 - INFO - __main__ - Tokenizing Input ...
03/13/2022 11:19:23 - INFO - __main__ - Tokenizing Output ...
03/13/2022 11:19:23 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 11:19:28 - INFO - __main__ - Global step 600 Train loss 0.350506 EM 0.0 on epoch=299
03/13/2022 11:19:28 - INFO - __main__ - save last model!
03/13/2022 11:19:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 11:19:35 - INFO - __main__ - Starting training!
03/13/2022 11:19:36 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 11:19:36 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 11:19:36 - INFO - __main__ - Printing 3 examples
03/13/2022 11:19:36 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 11:19:36 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 11:19:36 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 11:19:36 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 11:19:36 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 11:19:36 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 11:19:36 - INFO - __main__ - Tokenizing Input ...
03/13/2022 11:19:39 - INFO - __main__ - Tokenizing Output ...
03/13/2022 11:19:48 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 12:05:28 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_21_0.0001_8_predictions.txt
03/13/2022 12:05:28 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 12:05:29 - INFO - __main__ - prefix=break-QDMR_32_21, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 12:05:29 - INFO - __main__ - Running ... prefix=break-QDMR_32_42, lr=0.0005, bsz=8 ...
03/13/2022 12:05:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 12:05:30 - INFO - __main__ - Printing 3 examples
03/13/2022 12:05:30 - INFO - __main__ -  [break-QDMR] question: when did vincent von gogh die?
03/13/2022 12:05:30 - INFO - __main__ - ['return vincent von gogh ;return when  did  #1 die']
03/13/2022 12:05:30 - INFO - __main__ -  [break-QDMR] question: Does the Bronx have more non-Hispanic whites, or Hispanic whites?
03/13/2022 12:05:30 - INFO - __main__ - ['return the  Bronx ;return non-Hispanic whites in  #1 ;return Hispanic whites in  #1 ;return number of  #2 ;return number of  #3 ;return which is highest of #4 ,  #5']
03/13/2022 12:05:30 - INFO - __main__ -  [break-QDMR] question: What is the oldest log id and its corresponding problem id?
03/13/2022 12:05:30 - INFO - __main__ - ['return log ids ;return #1 that  is  the  oldest ;return corresponding problem id  of #2 ;return #2 ,  #3']
03/13/2022 12:05:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 12:05:30 - INFO - __main__ - Tokenizing Output ...
03/13/2022 12:05:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 12:05:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 12:05:30 - INFO - __main__ - Printing 3 examples
03/13/2022 12:05:30 - INFO - __main__ -  [break-QDMR] question: If the dog in the left photo has a star shapped tag hanging from its collar.
03/13/2022 12:05:30 - INFO - __main__ - ['return dog ;return collar of #1 ;return tag ;return #3 that is star shapped ;return #1 where  #2 has #4 hanging ;return if  #5 is in  the  left photo']
03/13/2022 12:05:30 - INFO - __main__ -  [break-QDMR] question: Who died from asphyxia and was the actor who played Alan Parrish in Jumanji?
03/13/2022 12:05:30 - INFO - __main__ - ['return Alan Parrish in  Jumanji ;return actor of #1 ;return #2 that died from  asphyxia']
03/13/2022 12:05:30 - INFO - __main__ -  [break-QDMR] question: show me flights from milwaukee to orlando on a thursday before noon 
03/13/2022 12:05:30 - INFO - __main__ - ['return flights ;return #1 from  milwaukee ;return #2 to orlando ;return #3 on  a thursday ;return #4 before noon']
03/13/2022 12:05:30 - INFO - __main__ - Tokenizing Input ...
03/13/2022 12:05:30 - INFO - __main__ - Tokenizing Output ...
03/13/2022 12:05:30 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 12:05:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 12:05:43 - INFO - __main__ - Starting training!
03/13/2022 12:05:47 - INFO - __main__ - Step 10 Global step 10 Train loss 21.805109 on epoch=4
03/13/2022 12:05:51 - INFO - __main__ - Step 20 Global step 20 Train loss 12.962845 on epoch=9
03/13/2022 12:05:56 - INFO - __main__ - Step 30 Global step 30 Train loss 8.382372 on epoch=14
03/13/2022 12:06:01 - INFO - __main__ - Step 40 Global step 40 Train loss 6.062182 on epoch=19
03/13/2022 12:06:06 - INFO - __main__ - Step 50 Global step 50 Train loss 5.170661 on epoch=24
03/13/2022 12:06:17 - INFO - __main__ - Global step 50 Train loss 10.876634 EM 0.0 on epoch=24
03/13/2022 12:06:23 - INFO - __main__ - Step 60 Global step 60 Train loss 4.810098 on epoch=29
03/13/2022 12:06:28 - INFO - __main__ - Step 70 Global step 70 Train loss 4.013345 on epoch=34
03/13/2022 12:06:33 - INFO - __main__ - Step 80 Global step 80 Train loss 3.306384 on epoch=39
03/13/2022 12:06:38 - INFO - __main__ - Step 90 Global step 90 Train loss 2.553175 on epoch=44
03/13/2022 12:06:43 - INFO - __main__ - Step 100 Global step 100 Train loss 1.932357 on epoch=49
03/13/2022 12:06:53 - INFO - __main__ - Global step 100 Train loss 3.323071 EM 0.0 on epoch=49
03/13/2022 12:06:58 - INFO - __main__ - Step 110 Global step 110 Train loss 1.453486 on epoch=54
03/13/2022 12:07:03 - INFO - __main__ - Step 120 Global step 120 Train loss 1.171985 on epoch=59
03/13/2022 12:07:08 - INFO - __main__ - Step 130 Global step 130 Train loss 1.006989 on epoch=64
03/13/2022 12:07:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.767041 on epoch=69
03/13/2022 12:07:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.639519 on epoch=74
03/13/2022 12:07:24 - INFO - __main__ - Global step 150 Train loss 1.007804 EM 0.0 on epoch=74
03/13/2022 12:07:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.490981 on epoch=79
03/13/2022 12:07:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.432444 on epoch=84
03/13/2022 12:07:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.317200 on epoch=89
03/13/2022 12:07:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.302706 on epoch=94
03/13/2022 12:07:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.268828 on epoch=99
03/13/2022 12:07:58 - INFO - __main__ - Global step 200 Train loss 0.362432 EM 0.0 on epoch=99
03/13/2022 12:08:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.211784 on epoch=104
03/13/2022 12:08:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.198113 on epoch=109
03/13/2022 12:08:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.169112 on epoch=114
03/13/2022 12:08:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.146088 on epoch=119
03/13/2022 12:08:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.149972 on epoch=124
03/13/2022 12:08:29 - INFO - __main__ - Global step 250 Train loss 0.175014 EM 0.0 on epoch=124
03/13/2022 12:08:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.098691 on epoch=129
03/13/2022 12:08:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.075584 on epoch=134
03/13/2022 12:08:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.092888 on epoch=139
03/13/2022 12:08:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.088132 on epoch=144
03/13/2022 12:08:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.070643 on epoch=149
03/13/2022 12:08:59 - INFO - __main__ - Global step 300 Train loss 0.085188 EM 0.0 on epoch=149
03/13/2022 12:09:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.071416 on epoch=154
03/13/2022 12:09:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.081354 on epoch=159
03/13/2022 12:09:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.055413 on epoch=164
03/13/2022 12:09:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.070623 on epoch=169
03/13/2022 12:09:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.124376 on epoch=174
03/13/2022 12:09:28 - INFO - __main__ - Global step 350 Train loss 0.080636 EM 0.0 on epoch=174
03/13/2022 12:09:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.046189 on epoch=179
03/13/2022 12:09:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.036533 on epoch=184
03/13/2022 12:09:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.041843 on epoch=189
03/13/2022 12:09:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.028363 on epoch=194
03/13/2022 12:09:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.036851 on epoch=199
03/13/2022 12:09:58 - INFO - __main__ - Global step 400 Train loss 0.037956 EM 0.0 on epoch=199
03/13/2022 12:10:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.036188 on epoch=204
03/13/2022 12:10:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.027965 on epoch=209
03/13/2022 12:10:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.046757 on epoch=214
03/13/2022 12:10:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.031903 on epoch=219
03/13/2022 12:10:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.037178 on epoch=224
03/13/2022 12:10:29 - INFO - __main__ - Global step 450 Train loss 0.035998 EM 0.0 on epoch=224
03/13/2022 12:10:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.022849 on epoch=229
03/13/2022 12:10:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.030074 on epoch=234
03/13/2022 12:10:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.017859 on epoch=239
03/13/2022 12:10:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.026533 on epoch=244
03/13/2022 12:10:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.019153 on epoch=249
03/13/2022 12:10:59 - INFO - __main__ - Global step 500 Train loss 0.023294 EM 0.0 on epoch=249
03/13/2022 12:11:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.026911 on epoch=254
03/13/2022 12:11:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.022091 on epoch=259
03/13/2022 12:11:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.017258 on epoch=264
03/13/2022 12:11:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.015846 on epoch=269
03/13/2022 12:11:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.015526 on epoch=274
03/13/2022 12:11:29 - INFO - __main__ - Global step 550 Train loss 0.019526 EM 0.0 on epoch=274
03/13/2022 12:11:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.023611 on epoch=279
03/13/2022 12:11:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.010088 on epoch=284
03/13/2022 12:11:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.014837 on epoch=289
03/13/2022 12:11:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.020280 on epoch=294
03/13/2022 12:11:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.009407 on epoch=299
03/13/2022 12:11:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 12:11:56 - INFO - __main__ - Printing 3 examples
03/13/2022 12:11:56 - INFO - __main__ -  [break-QDMR] question: when did vincent von gogh die?
03/13/2022 12:11:56 - INFO - __main__ - ['return vincent von gogh ;return when  did  #1 die']
03/13/2022 12:11:56 - INFO - __main__ -  [break-QDMR] question: Does the Bronx have more non-Hispanic whites, or Hispanic whites?
03/13/2022 12:11:56 - INFO - __main__ - ['return the  Bronx ;return non-Hispanic whites in  #1 ;return Hispanic whites in  #1 ;return number of  #2 ;return number of  #3 ;return which is highest of #4 ,  #5']
03/13/2022 12:11:56 - INFO - __main__ -  [break-QDMR] question: What is the oldest log id and its corresponding problem id?
03/13/2022 12:11:56 - INFO - __main__ - ['return log ids ;return #1 that  is  the  oldest ;return corresponding problem id  of #2 ;return #2 ,  #3']
03/13/2022 12:11:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 12:11:56 - INFO - __main__ - Tokenizing Output ...
03/13/2022 12:11:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 12:11:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 12:11:56 - INFO - __main__ - Printing 3 examples
03/13/2022 12:11:56 - INFO - __main__ -  [break-QDMR] question: If the dog in the left photo has a star shapped tag hanging from its collar.
03/13/2022 12:11:56 - INFO - __main__ - ['return dog ;return collar of #1 ;return tag ;return #3 that is star shapped ;return #1 where  #2 has #4 hanging ;return if  #5 is in  the  left photo']
03/13/2022 12:11:56 - INFO - __main__ -  [break-QDMR] question: Who died from asphyxia and was the actor who played Alan Parrish in Jumanji?
03/13/2022 12:11:56 - INFO - __main__ - ['return Alan Parrish in  Jumanji ;return actor of #1 ;return #2 that died from  asphyxia']
03/13/2022 12:11:56 - INFO - __main__ -  [break-QDMR] question: show me flights from milwaukee to orlando on a thursday before noon 
03/13/2022 12:11:56 - INFO - __main__ - ['return flights ;return #1 from  milwaukee ;return #2 to orlando ;return #3 on  a thursday ;return #4 before noon']
03/13/2022 12:11:56 - INFO - __main__ - Tokenizing Input ...
03/13/2022 12:11:56 - INFO - __main__ - Tokenizing Output ...
03/13/2022 12:11:56 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 12:12:00 - INFO - __main__ - Global step 600 Train loss 0.015645 EM 0.0 on epoch=299
03/13/2022 12:12:00 - INFO - __main__ - save last model!
03/13/2022 12:12:07 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 12:12:08 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 12:12:08 - INFO - __main__ - Printing 3 examples
03/13/2022 12:12:08 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 12:12:08 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 12:12:08 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 12:12:08 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 12:12:08 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 12:12:08 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 12:12:08 - INFO - __main__ - Tokenizing Input ...
03/13/2022 12:12:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 12:12:09 - INFO - __main__ - Starting training!
03/13/2022 12:12:11 - INFO - __main__ - Tokenizing Output ...
03/13/2022 12:12:19 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 12:58:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_42_0.0005_8_predictions.txt
03/13/2022 12:58:31 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 12:58:32 - INFO - __main__ - prefix=break-QDMR_32_42, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 12:58:32 - INFO - __main__ - Running ... prefix=break-QDMR_32_42, lr=0.0003, bsz=8 ...
03/13/2022 12:58:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 12:58:33 - INFO - __main__ - Printing 3 examples
03/13/2022 12:58:33 - INFO - __main__ -  [break-QDMR] question: when did vincent von gogh die?
03/13/2022 12:58:33 - INFO - __main__ - ['return vincent von gogh ;return when  did  #1 die']
03/13/2022 12:58:33 - INFO - __main__ -  [break-QDMR] question: Does the Bronx have more non-Hispanic whites, or Hispanic whites?
03/13/2022 12:58:33 - INFO - __main__ - ['return the  Bronx ;return non-Hispanic whites in  #1 ;return Hispanic whites in  #1 ;return number of  #2 ;return number of  #3 ;return which is highest of #4 ,  #5']
03/13/2022 12:58:33 - INFO - __main__ -  [break-QDMR] question: What is the oldest log id and its corresponding problem id?
03/13/2022 12:58:33 - INFO - __main__ - ['return log ids ;return #1 that  is  the  oldest ;return corresponding problem id  of #2 ;return #2 ,  #3']
03/13/2022 12:58:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 12:58:33 - INFO - __main__ - Tokenizing Output ...
03/13/2022 12:58:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 12:58:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 12:58:33 - INFO - __main__ - Printing 3 examples
03/13/2022 12:58:33 - INFO - __main__ -  [break-QDMR] question: If the dog in the left photo has a star shapped tag hanging from its collar.
03/13/2022 12:58:33 - INFO - __main__ - ['return dog ;return collar of #1 ;return tag ;return #3 that is star shapped ;return #1 where  #2 has #4 hanging ;return if  #5 is in  the  left photo']
03/13/2022 12:58:33 - INFO - __main__ -  [break-QDMR] question: Who died from asphyxia and was the actor who played Alan Parrish in Jumanji?
03/13/2022 12:58:33 - INFO - __main__ - ['return Alan Parrish in  Jumanji ;return actor of #1 ;return #2 that died from  asphyxia']
03/13/2022 12:58:33 - INFO - __main__ -  [break-QDMR] question: show me flights from milwaukee to orlando on a thursday before noon 
03/13/2022 12:58:33 - INFO - __main__ - ['return flights ;return #1 from  milwaukee ;return #2 to orlando ;return #3 on  a thursday ;return #4 before noon']
03/13/2022 12:58:33 - INFO - __main__ - Tokenizing Input ...
03/13/2022 12:58:33 - INFO - __main__ - Tokenizing Output ...
03/13/2022 12:58:33 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 12:58:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 12:58:45 - INFO - __main__ - Starting training!
03/13/2022 12:58:50 - INFO - __main__ - Step 10 Global step 10 Train loss 20.769550 on epoch=4
03/13/2022 12:58:55 - INFO - __main__ - Step 20 Global step 20 Train loss 13.567011 on epoch=9
03/13/2022 12:59:00 - INFO - __main__ - Step 30 Global step 30 Train loss 5.914211 on epoch=14
03/13/2022 12:59:05 - INFO - __main__ - Step 40 Global step 40 Train loss 3.992385 on epoch=19
03/13/2022 12:59:10 - INFO - __main__ - Step 50 Global step 50 Train loss 2.806207 on epoch=24
03/13/2022 12:59:23 - INFO - __main__ - Global step 50 Train loss 9.409874 EM 0.0 on epoch=24
03/13/2022 12:59:29 - INFO - __main__ - Step 60 Global step 60 Train loss 2.310688 on epoch=29
03/13/2022 12:59:34 - INFO - __main__ - Step 70 Global step 70 Train loss 1.867672 on epoch=34
03/13/2022 12:59:39 - INFO - __main__ - Step 80 Global step 80 Train loss 1.586210 on epoch=39
03/13/2022 12:59:44 - INFO - __main__ - Step 90 Global step 90 Train loss 1.281965 on epoch=44
03/13/2022 12:59:49 - INFO - __main__ - Step 100 Global step 100 Train loss 1.033760 on epoch=49
03/13/2022 12:59:57 - INFO - __main__ - Global step 100 Train loss 1.616059 EM 0.0 on epoch=49
03/13/2022 13:00:02 - INFO - __main__ - Step 110 Global step 110 Train loss 0.888418 on epoch=54
03/13/2022 13:00:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.715922 on epoch=59
03/13/2022 13:00:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.610289 on epoch=64
03/13/2022 13:00:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.463308 on epoch=69
03/13/2022 13:00:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.423562 on epoch=74
03/13/2022 13:00:28 - INFO - __main__ - Global step 150 Train loss 0.620299 EM 0.0 on epoch=74
03/13/2022 13:00:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.361586 on epoch=79
03/13/2022 13:00:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.302537 on epoch=84
03/13/2022 13:00:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.282372 on epoch=89
03/13/2022 13:00:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.208103 on epoch=94
03/13/2022 13:00:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.184761 on epoch=99
03/13/2022 13:00:58 - INFO - __main__ - Global step 200 Train loss 0.267872 EM 0.0 on epoch=99
03/13/2022 13:01:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.163073 on epoch=104
03/13/2022 13:01:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.163098 on epoch=109
03/13/2022 13:01:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.137686 on epoch=114
03/13/2022 13:01:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.091899 on epoch=119
03/13/2022 13:01:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.106378 on epoch=124
03/13/2022 13:01:29 - INFO - __main__ - Global step 250 Train loss 0.132427 EM 0.0 on epoch=124
03/13/2022 13:01:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.084464 on epoch=129
03/13/2022 13:01:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.082287 on epoch=134
03/13/2022 13:01:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.070853 on epoch=139
03/13/2022 13:01:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.098220 on epoch=144
03/13/2022 13:01:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.076683 on epoch=149
03/13/2022 13:01:59 - INFO - __main__ - Global step 300 Train loss 0.082502 EM 0.0 on epoch=149
03/13/2022 13:02:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.064640 on epoch=154
03/13/2022 13:02:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.111692 on epoch=159
03/13/2022 13:02:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.067664 on epoch=164
03/13/2022 13:02:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.085364 on epoch=169
03/13/2022 13:02:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.061234 on epoch=174
03/13/2022 13:02:30 - INFO - __main__ - Global step 350 Train loss 0.078119 EM 0.0 on epoch=174
03/13/2022 13:02:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.051483 on epoch=179
03/13/2022 13:02:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.052876 on epoch=184
03/13/2022 13:02:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.047048 on epoch=189
03/13/2022 13:02:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.039481 on epoch=194
03/13/2022 13:02:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.072122 on epoch=199
03/13/2022 13:03:00 - INFO - __main__ - Global step 400 Train loss 0.052602 EM 0.0 on epoch=199
03/13/2022 13:03:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.026473 on epoch=204
03/13/2022 13:03:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.042772 on epoch=209
03/13/2022 13:03:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.028425 on epoch=214
03/13/2022 13:03:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.056516 on epoch=219
03/13/2022 13:03:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.033678 on epoch=224
03/13/2022 13:03:30 - INFO - __main__ - Global step 450 Train loss 0.037573 EM 0.0 on epoch=224
03/13/2022 13:03:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.043860 on epoch=229
03/13/2022 13:03:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.034814 on epoch=234
03/13/2022 13:03:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.050307 on epoch=239
03/13/2022 13:03:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.029197 on epoch=244
03/13/2022 13:03:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.023678 on epoch=249
03/13/2022 13:04:01 - INFO - __main__ - Global step 500 Train loss 0.036371 EM 0.0 on epoch=249
03/13/2022 13:04:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.024071 on epoch=254
03/13/2022 13:04:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.024428 on epoch=259
03/13/2022 13:04:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.035479 on epoch=264
03/13/2022 13:04:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.026988 on epoch=269
03/13/2022 13:04:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.030603 on epoch=274
03/13/2022 13:04:31 - INFO - __main__ - Global step 550 Train loss 0.028314 EM 0.0 on epoch=274
03/13/2022 13:04:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.019886 on epoch=279
03/13/2022 13:04:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.018865 on epoch=284
03/13/2022 13:04:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.017479 on epoch=289
03/13/2022 13:04:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.015737 on epoch=294
03/13/2022 13:04:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.019961 on epoch=299
03/13/2022 13:04:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 13:04:58 - INFO - __main__ - Printing 3 examples
03/13/2022 13:04:58 - INFO - __main__ -  [break-QDMR] question: when did vincent von gogh die?
03/13/2022 13:04:58 - INFO - __main__ - ['return vincent von gogh ;return when  did  #1 die']
03/13/2022 13:04:58 - INFO - __main__ -  [break-QDMR] question: Does the Bronx have more non-Hispanic whites, or Hispanic whites?
03/13/2022 13:04:58 - INFO - __main__ - ['return the  Bronx ;return non-Hispanic whites in  #1 ;return Hispanic whites in  #1 ;return number of  #2 ;return number of  #3 ;return which is highest of #4 ,  #5']
03/13/2022 13:04:58 - INFO - __main__ -  [break-QDMR] question: What is the oldest log id and its corresponding problem id?
03/13/2022 13:04:58 - INFO - __main__ - ['return log ids ;return #1 that  is  the  oldest ;return corresponding problem id  of #2 ;return #2 ,  #3']
03/13/2022 13:04:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 13:04:58 - INFO - __main__ - Tokenizing Output ...
03/13/2022 13:04:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 13:04:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 13:04:58 - INFO - __main__ - Printing 3 examples
03/13/2022 13:04:58 - INFO - __main__ -  [break-QDMR] question: If the dog in the left photo has a star shapped tag hanging from its collar.
03/13/2022 13:04:58 - INFO - __main__ - ['return dog ;return collar of #1 ;return tag ;return #3 that is star shapped ;return #1 where  #2 has #4 hanging ;return if  #5 is in  the  left photo']
03/13/2022 13:04:58 - INFO - __main__ -  [break-QDMR] question: Who died from asphyxia and was the actor who played Alan Parrish in Jumanji?
03/13/2022 13:04:58 - INFO - __main__ - ['return Alan Parrish in  Jumanji ;return actor of #1 ;return #2 that died from  asphyxia']
03/13/2022 13:04:58 - INFO - __main__ -  [break-QDMR] question: show me flights from milwaukee to orlando on a thursday before noon 
03/13/2022 13:04:58 - INFO - __main__ - ['return flights ;return #1 from  milwaukee ;return #2 to orlando ;return #3 on  a thursday ;return #4 before noon']
03/13/2022 13:04:58 - INFO - __main__ - Tokenizing Input ...
03/13/2022 13:04:58 - INFO - __main__ - Tokenizing Output ...
03/13/2022 13:04:58 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 13:05:03 - INFO - __main__ - Global step 600 Train loss 0.018385 EM 0.0 on epoch=299
03/13/2022 13:05:03 - INFO - __main__ - save last model!
03/13/2022 13:05:10 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 13:05:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 13:05:11 - INFO - __main__ - Starting training!
03/13/2022 13:05:11 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 13:05:11 - INFO - __main__ - Printing 3 examples
03/13/2022 13:05:11 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 13:05:11 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 13:05:11 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 13:05:11 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 13:05:11 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 13:05:11 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 13:05:11 - INFO - __main__ - Tokenizing Input ...
03/13/2022 13:05:14 - INFO - __main__ - Tokenizing Output ...
03/13/2022 13:05:22 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 13:44:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_42_0.0003_8_predictions.txt
03/13/2022 13:44:25 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 13:44:25 - INFO - __main__ - prefix=break-QDMR_32_42, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 13:44:25 - INFO - __main__ - Running ... prefix=break-QDMR_32_42, lr=0.0002, bsz=8 ...
03/13/2022 13:44:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 13:44:26 - INFO - __main__ - Printing 3 examples
03/13/2022 13:44:26 - INFO - __main__ -  [break-QDMR] question: when did vincent von gogh die?
03/13/2022 13:44:26 - INFO - __main__ - ['return vincent von gogh ;return when  did  #1 die']
03/13/2022 13:44:26 - INFO - __main__ -  [break-QDMR] question: Does the Bronx have more non-Hispanic whites, or Hispanic whites?
03/13/2022 13:44:26 - INFO - __main__ - ['return the  Bronx ;return non-Hispanic whites in  #1 ;return Hispanic whites in  #1 ;return number of  #2 ;return number of  #3 ;return which is highest of #4 ,  #5']
03/13/2022 13:44:26 - INFO - __main__ -  [break-QDMR] question: What is the oldest log id and its corresponding problem id?
03/13/2022 13:44:26 - INFO - __main__ - ['return log ids ;return #1 that  is  the  oldest ;return corresponding problem id  of #2 ;return #2 ,  #3']
03/13/2022 13:44:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 13:44:26 - INFO - __main__ - Tokenizing Output ...
03/13/2022 13:44:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 13:44:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 13:44:26 - INFO - __main__ - Printing 3 examples
03/13/2022 13:44:26 - INFO - __main__ -  [break-QDMR] question: If the dog in the left photo has a star shapped tag hanging from its collar.
03/13/2022 13:44:26 - INFO - __main__ - ['return dog ;return collar of #1 ;return tag ;return #3 that is star shapped ;return #1 where  #2 has #4 hanging ;return if  #5 is in  the  left photo']
03/13/2022 13:44:26 - INFO - __main__ -  [break-QDMR] question: Who died from asphyxia and was the actor who played Alan Parrish in Jumanji?
03/13/2022 13:44:26 - INFO - __main__ - ['return Alan Parrish in  Jumanji ;return actor of #1 ;return #2 that died from  asphyxia']
03/13/2022 13:44:26 - INFO - __main__ -  [break-QDMR] question: show me flights from milwaukee to orlando on a thursday before noon 
03/13/2022 13:44:26 - INFO - __main__ - ['return flights ;return #1 from  milwaukee ;return #2 to orlando ;return #3 on  a thursday ;return #4 before noon']
03/13/2022 13:44:26 - INFO - __main__ - Tokenizing Input ...
03/13/2022 13:44:26 - INFO - __main__ - Tokenizing Output ...
03/13/2022 13:44:27 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 13:44:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 13:44:37 - INFO - __main__ - Starting training!
03/13/2022 13:44:42 - INFO - __main__ - Step 10 Global step 10 Train loss 21.690575 on epoch=4
03/13/2022 13:44:46 - INFO - __main__ - Step 20 Global step 20 Train loss 16.183226 on epoch=9
03/13/2022 13:44:51 - INFO - __main__ - Step 30 Global step 30 Train loss 9.077143 on epoch=14
03/13/2022 13:44:57 - INFO - __main__ - Step 40 Global step 40 Train loss 6.452624 on epoch=19
03/13/2022 13:45:02 - INFO - __main__ - Step 50 Global step 50 Train loss 5.335573 on epoch=24
03/13/2022 13:45:13 - INFO - __main__ - Global step 50 Train loss 11.747828 EM 0.0 on epoch=24
03/13/2022 13:45:19 - INFO - __main__ - Step 60 Global step 60 Train loss 4.847156 on epoch=29
03/13/2022 13:45:24 - INFO - __main__ - Step 70 Global step 70 Train loss 4.446666 on epoch=34
03/13/2022 13:45:29 - INFO - __main__ - Step 80 Global step 80 Train loss 3.772481 on epoch=39
03/13/2022 13:45:34 - INFO - __main__ - Step 90 Global step 90 Train loss 3.337489 on epoch=44
03/13/2022 13:45:39 - INFO - __main__ - Step 100 Global step 100 Train loss 3.030082 on epoch=49
03/13/2022 13:45:50 - INFO - __main__ - Global step 100 Train loss 3.886775 EM 0.0 on epoch=49
03/13/2022 13:45:55 - INFO - __main__ - Step 110 Global step 110 Train loss 2.537208 on epoch=54
03/13/2022 13:46:00 - INFO - __main__ - Step 120 Global step 120 Train loss 2.384771 on epoch=59
03/13/2022 13:46:05 - INFO - __main__ - Step 130 Global step 130 Train loss 2.061496 on epoch=64
03/13/2022 13:46:10 - INFO - __main__ - Step 140 Global step 140 Train loss 1.850169 on epoch=69
03/13/2022 13:46:15 - INFO - __main__ - Step 150 Global step 150 Train loss 1.704983 on epoch=74
03/13/2022 13:46:25 - INFO - __main__ - Global step 150 Train loss 2.107726 EM 0.0 on epoch=74
03/13/2022 13:46:30 - INFO - __main__ - Step 160 Global step 160 Train loss 1.381226 on epoch=79
03/13/2022 13:46:35 - INFO - __main__ - Step 170 Global step 170 Train loss 1.305304 on epoch=84
03/13/2022 13:46:40 - INFO - __main__ - Step 180 Global step 180 Train loss 1.156399 on epoch=89
03/13/2022 13:46:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.990456 on epoch=94
03/13/2022 13:46:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.942576 on epoch=99
03/13/2022 13:46:55 - INFO - __main__ - Global step 200 Train loss 1.155192 EM 0.0 on epoch=99
03/13/2022 13:47:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.862137 on epoch=104
03/13/2022 13:47:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.751581 on epoch=109
03/13/2022 13:47:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.789439 on epoch=114
03/13/2022 13:47:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.759976 on epoch=119
03/13/2022 13:47:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.771099 on epoch=124
03/13/2022 13:47:25 - INFO - __main__ - Global step 250 Train loss 0.786847 EM 0.0 on epoch=124
03/13/2022 13:47:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.555460 on epoch=129
03/13/2022 13:47:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.574346 on epoch=134
03/13/2022 13:47:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.490778 on epoch=139
03/13/2022 13:47:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.440627 on epoch=144
03/13/2022 13:47:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.402687 on epoch=149
03/13/2022 13:47:55 - INFO - __main__ - Global step 300 Train loss 0.492779 EM 0.0 on epoch=149
03/13/2022 13:48:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.349338 on epoch=154
03/13/2022 13:48:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.323411 on epoch=159
03/13/2022 13:48:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.287815 on epoch=164
03/13/2022 13:48:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.234703 on epoch=169
03/13/2022 13:48:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.222606 on epoch=174
03/13/2022 13:48:26 - INFO - __main__ - Global step 350 Train loss 0.283575 EM 0.0 on epoch=174
03/13/2022 13:48:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.210130 on epoch=179
03/13/2022 13:48:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.179369 on epoch=184
03/13/2022 13:48:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.205615 on epoch=189
03/13/2022 13:48:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.162653 on epoch=194
03/13/2022 13:48:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.172782 on epoch=199
03/13/2022 13:48:58 - INFO - __main__ - Global step 400 Train loss 0.186110 EM 0.0 on epoch=199
03/13/2022 13:49:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.127809 on epoch=204
03/13/2022 13:49:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.103018 on epoch=209
03/13/2022 13:49:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.130421 on epoch=214
03/13/2022 13:49:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.147354 on epoch=219
03/13/2022 13:49:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.128288 on epoch=224
03/13/2022 13:49:28 - INFO - __main__ - Global step 450 Train loss 0.127378 EM 0.0 on epoch=224
03/13/2022 13:49:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.079652 on epoch=229
03/13/2022 13:49:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.102589 on epoch=234
03/13/2022 13:49:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.103480 on epoch=239
03/13/2022 13:49:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.106955 on epoch=244
03/13/2022 13:49:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.067549 on epoch=249
03/13/2022 13:49:58 - INFO - __main__ - Global step 500 Train loss 0.092045 EM 0.0 on epoch=249
03/13/2022 13:50:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.076821 on epoch=254
03/13/2022 13:50:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.085073 on epoch=259
03/13/2022 13:50:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.088457 on epoch=264
03/13/2022 13:50:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.071270 on epoch=269
03/13/2022 13:50:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.059825 on epoch=274
03/13/2022 13:50:28 - INFO - __main__ - Global step 550 Train loss 0.076289 EM 0.0 on epoch=274
03/13/2022 13:50:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.052215 on epoch=279
03/13/2022 13:50:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.053131 on epoch=284
03/13/2022 13:50:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.075730 on epoch=289
03/13/2022 13:50:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.054522 on epoch=294
03/13/2022 13:50:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.065879 on epoch=299
03/13/2022 13:50:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 13:50:54 - INFO - __main__ - Printing 3 examples
03/13/2022 13:50:54 - INFO - __main__ -  [break-QDMR] question: when did vincent von gogh die?
03/13/2022 13:50:54 - INFO - __main__ - ['return vincent von gogh ;return when  did  #1 die']
03/13/2022 13:50:54 - INFO - __main__ -  [break-QDMR] question: Does the Bronx have more non-Hispanic whites, or Hispanic whites?
03/13/2022 13:50:54 - INFO - __main__ - ['return the  Bronx ;return non-Hispanic whites in  #1 ;return Hispanic whites in  #1 ;return number of  #2 ;return number of  #3 ;return which is highest of #4 ,  #5']
03/13/2022 13:50:54 - INFO - __main__ -  [break-QDMR] question: What is the oldest log id and its corresponding problem id?
03/13/2022 13:50:54 - INFO - __main__ - ['return log ids ;return #1 that  is  the  oldest ;return corresponding problem id  of #2 ;return #2 ,  #3']
03/13/2022 13:50:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 13:50:54 - INFO - __main__ - Tokenizing Output ...
03/13/2022 13:50:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 13:50:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 13:50:55 - INFO - __main__ - Printing 3 examples
03/13/2022 13:50:55 - INFO - __main__ -  [break-QDMR] question: If the dog in the left photo has a star shapped tag hanging from its collar.
03/13/2022 13:50:55 - INFO - __main__ - ['return dog ;return collar of #1 ;return tag ;return #3 that is star shapped ;return #1 where  #2 has #4 hanging ;return if  #5 is in  the  left photo']
03/13/2022 13:50:55 - INFO - __main__ -  [break-QDMR] question: Who died from asphyxia and was the actor who played Alan Parrish in Jumanji?
03/13/2022 13:50:55 - INFO - __main__ - ['return Alan Parrish in  Jumanji ;return actor of #1 ;return #2 that died from  asphyxia']
03/13/2022 13:50:55 - INFO - __main__ -  [break-QDMR] question: show me flights from milwaukee to orlando on a thursday before noon 
03/13/2022 13:50:55 - INFO - __main__ - ['return flights ;return #1 from  milwaukee ;return #2 to orlando ;return #3 on  a thursday ;return #4 before noon']
03/13/2022 13:50:55 - INFO - __main__ - Tokenizing Input ...
03/13/2022 13:50:55 - INFO - __main__ - Tokenizing Output ...
03/13/2022 13:50:55 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 13:50:58 - INFO - __main__ - Global step 600 Train loss 0.060296 EM 0.0 on epoch=299
03/13/2022 13:50:58 - INFO - __main__ - save last model!
03/13/2022 13:51:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 13:51:05 - INFO - __main__ - Starting training!
03/13/2022 13:51:05 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 13:51:06 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 13:51:06 - INFO - __main__ - Printing 3 examples
03/13/2022 13:51:06 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 13:51:06 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 13:51:06 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 13:51:06 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 13:51:06 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 13:51:06 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 13:51:06 - INFO - __main__ - Tokenizing Input ...
03/13/2022 13:51:09 - INFO - __main__ - Tokenizing Output ...
03/13/2022 13:51:17 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 14:38:09 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_42_0.0002_8_predictions.txt
03/13/2022 14:38:10 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 14:38:10 - INFO - __main__ - prefix=break-QDMR_32_42, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 14:38:10 - INFO - __main__ - Running ... prefix=break-QDMR_32_42, lr=0.0001, bsz=8 ...
03/13/2022 14:38:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 14:38:11 - INFO - __main__ - Printing 3 examples
03/13/2022 14:38:11 - INFO - __main__ -  [break-QDMR] question: when did vincent von gogh die?
03/13/2022 14:38:11 - INFO - __main__ - ['return vincent von gogh ;return when  did  #1 die']
03/13/2022 14:38:11 - INFO - __main__ -  [break-QDMR] question: Does the Bronx have more non-Hispanic whites, or Hispanic whites?
03/13/2022 14:38:11 - INFO - __main__ - ['return the  Bronx ;return non-Hispanic whites in  #1 ;return Hispanic whites in  #1 ;return number of  #2 ;return number of  #3 ;return which is highest of #4 ,  #5']
03/13/2022 14:38:11 - INFO - __main__ -  [break-QDMR] question: What is the oldest log id and its corresponding problem id?
03/13/2022 14:38:11 - INFO - __main__ - ['return log ids ;return #1 that  is  the  oldest ;return corresponding problem id  of #2 ;return #2 ,  #3']
03/13/2022 14:38:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 14:38:11 - INFO - __main__ - Tokenizing Output ...
03/13/2022 14:38:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 14:38:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 14:38:11 - INFO - __main__ - Printing 3 examples
03/13/2022 14:38:11 - INFO - __main__ -  [break-QDMR] question: If the dog in the left photo has a star shapped tag hanging from its collar.
03/13/2022 14:38:11 - INFO - __main__ - ['return dog ;return collar of #1 ;return tag ;return #3 that is star shapped ;return #1 where  #2 has #4 hanging ;return if  #5 is in  the  left photo']
03/13/2022 14:38:11 - INFO - __main__ -  [break-QDMR] question: Who died from asphyxia and was the actor who played Alan Parrish in Jumanji?
03/13/2022 14:38:11 - INFO - __main__ - ['return Alan Parrish in  Jumanji ;return actor of #1 ;return #2 that died from  asphyxia']
03/13/2022 14:38:11 - INFO - __main__ -  [break-QDMR] question: show me flights from milwaukee to orlando on a thursday before noon 
03/13/2022 14:38:11 - INFO - __main__ - ['return flights ;return #1 from  milwaukee ;return #2 to orlando ;return #3 on  a thursday ;return #4 before noon']
03/13/2022 14:38:11 - INFO - __main__ - Tokenizing Input ...
03/13/2022 14:38:11 - INFO - __main__ - Tokenizing Output ...
03/13/2022 14:38:11 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 14:38:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 14:38:22 - INFO - __main__ - Starting training!
03/13/2022 14:38:27 - INFO - __main__ - Step 10 Global step 10 Train loss 21.487179 on epoch=4
03/13/2022 14:38:31 - INFO - __main__ - Step 20 Global step 20 Train loss 17.508755 on epoch=9
03/13/2022 14:38:36 - INFO - __main__ - Step 30 Global step 30 Train loss 11.787848 on epoch=14
03/13/2022 14:38:41 - INFO - __main__ - Step 40 Global step 40 Train loss 8.871302 on epoch=19
03/13/2022 14:38:47 - INFO - __main__ - Step 50 Global step 50 Train loss 7.242504 on epoch=24
03/13/2022 14:38:58 - INFO - __main__ - Global step 50 Train loss 13.379518 EM 0.0 on epoch=24
03/13/2022 14:39:04 - INFO - __main__ - Step 60 Global step 60 Train loss 6.573911 on epoch=29
03/13/2022 14:39:09 - INFO - __main__ - Step 70 Global step 70 Train loss 5.682603 on epoch=34
03/13/2022 14:39:14 - INFO - __main__ - Step 80 Global step 80 Train loss 5.131415 on epoch=39
03/13/2022 14:39:19 - INFO - __main__ - Step 90 Global step 90 Train loss 4.722340 on epoch=44
03/13/2022 14:39:24 - INFO - __main__ - Step 100 Global step 100 Train loss 4.281151 on epoch=49
03/13/2022 14:39:35 - INFO - __main__ - Global step 100 Train loss 5.278284 EM 0.0 on epoch=49
03/13/2022 14:39:40 - INFO - __main__ - Step 110 Global step 110 Train loss 3.835303 on epoch=54
03/13/2022 14:39:45 - INFO - __main__ - Step 120 Global step 120 Train loss 3.441545 on epoch=59
03/13/2022 14:39:50 - INFO - __main__ - Step 130 Global step 130 Train loss 3.154231 on epoch=64
03/13/2022 14:39:55 - INFO - __main__ - Step 140 Global step 140 Train loss 2.718731 on epoch=69
03/13/2022 14:40:00 - INFO - __main__ - Step 150 Global step 150 Train loss 2.562695 on epoch=74
03/13/2022 14:40:11 - INFO - __main__ - Global step 150 Train loss 3.142501 EM 0.0 on epoch=74
03/13/2022 14:40:16 - INFO - __main__ - Step 160 Global step 160 Train loss 2.315766 on epoch=79
03/13/2022 14:40:21 - INFO - __main__ - Step 170 Global step 170 Train loss 2.162185 on epoch=84
03/13/2022 14:40:26 - INFO - __main__ - Step 180 Global step 180 Train loss 1.984101 on epoch=89
03/13/2022 14:40:31 - INFO - __main__ - Step 190 Global step 190 Train loss 1.849656 on epoch=94
03/13/2022 14:40:36 - INFO - __main__ - Step 200 Global step 200 Train loss 1.687761 on epoch=99
03/13/2022 14:40:44 - INFO - __main__ - Global step 200 Train loss 1.999894 EM 0.0 on epoch=99
03/13/2022 14:40:49 - INFO - __main__ - Step 210 Global step 210 Train loss 1.503590 on epoch=104
03/13/2022 14:40:54 - INFO - __main__ - Step 220 Global step 220 Train loss 1.393982 on epoch=109
03/13/2022 14:41:00 - INFO - __main__ - Step 230 Global step 230 Train loss 1.332331 on epoch=114
03/13/2022 14:41:05 - INFO - __main__ - Step 240 Global step 240 Train loss 1.202533 on epoch=119
03/13/2022 14:41:10 - INFO - __main__ - Step 250 Global step 250 Train loss 1.119614 on epoch=124
03/13/2022 14:41:19 - INFO - __main__ - Global step 250 Train loss 1.310410 EM 0.0 on epoch=124
03/13/2022 14:41:24 - INFO - __main__ - Step 260 Global step 260 Train loss 1.055356 on epoch=129
03/13/2022 14:41:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.935465 on epoch=134
03/13/2022 14:41:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.849765 on epoch=139
03/13/2022 14:41:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.835393 on epoch=144
03/13/2022 14:41:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.755175 on epoch=149
03/13/2022 14:41:49 - INFO - __main__ - Global step 300 Train loss 0.886231 EM 0.0 on epoch=149
03/13/2022 14:41:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.767560 on epoch=154
03/13/2022 14:41:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.700516 on epoch=159
03/13/2022 14:42:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.625342 on epoch=164
03/13/2022 14:42:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.556362 on epoch=169
03/13/2022 14:42:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.541163 on epoch=174
03/13/2022 14:42:20 - INFO - __main__ - Global step 350 Train loss 0.638188 EM 0.0 on epoch=174
03/13/2022 14:42:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.486423 on epoch=179
03/13/2022 14:42:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.491968 on epoch=184
03/13/2022 14:42:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.421704 on epoch=189
03/13/2022 14:42:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.399881 on epoch=194
03/13/2022 14:42:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.340000 on epoch=199
03/13/2022 14:42:50 - INFO - __main__ - Global step 400 Train loss 0.427995 EM 0.0 on epoch=199
03/13/2022 14:42:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.342587 on epoch=204
03/13/2022 14:43:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.292216 on epoch=209
03/13/2022 14:43:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.340266 on epoch=214
03/13/2022 14:43:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.244275 on epoch=219
03/13/2022 14:43:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.229920 on epoch=224
03/13/2022 14:43:20 - INFO - __main__ - Global step 450 Train loss 0.289853 EM 0.0 on epoch=224
03/13/2022 14:43:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.227609 on epoch=229
03/13/2022 14:43:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.212849 on epoch=234
03/13/2022 14:43:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.216797 on epoch=239
03/13/2022 14:43:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.199589 on epoch=244
03/13/2022 14:43:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.176296 on epoch=249
03/13/2022 14:43:50 - INFO - __main__ - Global step 500 Train loss 0.206628 EM 0.0 on epoch=249
03/13/2022 14:43:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.182328 on epoch=254
03/13/2022 14:44:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.193838 on epoch=259
03/13/2022 14:44:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.156068 on epoch=264
03/13/2022 14:44:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.147791 on epoch=269
03/13/2022 14:44:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.204915 on epoch=274
03/13/2022 14:44:20 - INFO - __main__ - Global step 550 Train loss 0.176988 EM 0.0 on epoch=274
03/13/2022 14:44:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.119780 on epoch=279
03/13/2022 14:44:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.144561 on epoch=284
03/13/2022 14:44:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.123606 on epoch=289
03/13/2022 14:44:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.095142 on epoch=294
03/13/2022 14:44:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.130555 on epoch=299
03/13/2022 14:44:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 14:44:46 - INFO - __main__ - Printing 3 examples
03/13/2022 14:44:46 - INFO - __main__ -  [break-QDMR] question: How many people are apart of the police department?
03/13/2022 14:44:46 - INFO - __main__ - ['return the  police department ;return people that are apart of #1 ;return number of  #2']
03/13/2022 14:44:46 - INFO - __main__ -  [break-QDMR] question: If the rectangular dispenser on the left is taller than the white dispenser on the right.
03/13/2022 14:44:46 - INFO - __main__ - ['return dispensers ;return #1 that is rectangular ;return #2 that is on the  left ;return #1 that is white ;return #4 that is on the  right ;return tallness of #3 ;return tallness of #5 ;return if  #6 is higher than #7']
03/13/2022 14:44:46 - INFO - __main__ -  [break-QDMR] question: which airline has more business class flights than any other airline 
03/13/2022 14:44:46 - INFO - __main__ - ['return airlines ;return flights of #1 ;return #2 that are business class flights ;return number of  #3 for each #1 ;return #1 where #4 is the  highest']
03/13/2022 14:44:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 14:44:46 - INFO - __main__ - Tokenizing Output ...
03/13/2022 14:44:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 14:44:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 14:44:47 - INFO - __main__ - Printing 3 examples
03/13/2022 14:44:47 - INFO - __main__ -  [break-QDMR] question: What is the shape of the large cyan object?
03/13/2022 14:44:47 - INFO - __main__ - ['return objects ;return #1 that  are large ;return #2 that  are cyan ;return the  shape of #3']
03/13/2022 14:44:47 - INFO - __main__ -  [break-QDMR] question: Is the large purple thing made out of the same material as the large sphere? 
03/13/2022 14:44:47 - INFO - __main__ - ['return large thing ;return #1 that  is  purple ;return large sphere ;return material of #2 ;return material of #3 ;return is  #4 the   same as #5']
03/13/2022 14:44:47 - INFO - __main__ -  [break-QDMR] question: what object shape is there only two in matte?
03/13/2022 14:44:47 - INFO - __main__ - ['return shapes ;return #1 that  are matte ;return number of #2 for each #1 ;return #1 where #3 is  two']
03/13/2022 14:44:47 - INFO - __main__ - Tokenizing Input ...
03/13/2022 14:44:47 - INFO - __main__ - Tokenizing Output ...
03/13/2022 14:44:47 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 14:44:50 - INFO - __main__ - Global step 600 Train loss 0.122729 EM 0.0 on epoch=299
03/13/2022 14:44:50 - INFO - __main__ - save last model!
03/13/2022 14:44:58 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 14:44:58 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 14:44:58 - INFO - __main__ - Printing 3 examples
03/13/2022 14:44:58 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 14:44:58 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 14:44:58 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 14:44:58 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 14:44:58 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 14:44:58 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 14:44:58 - INFO - __main__ - Tokenizing Input ...
03/13/2022 14:44:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 14:44:59 - INFO - __main__ - Starting training!
03/13/2022 14:45:01 - INFO - __main__ - Tokenizing Output ...
03/13/2022 14:45:10 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 15:29:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_42_0.0001_8_predictions.txt
03/13/2022 15:29:44 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 15:29:45 - INFO - __main__ - prefix=break-QDMR_32_42, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 15:29:45 - INFO - __main__ - Running ... prefix=break-QDMR_32_87, lr=0.0005, bsz=8 ...
03/13/2022 15:29:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 15:29:46 - INFO - __main__ - Printing 3 examples
03/13/2022 15:29:46 - INFO - __main__ -  [break-QDMR] question: How many people are apart of the police department?
03/13/2022 15:29:46 - INFO - __main__ - ['return the  police department ;return people that are apart of #1 ;return number of  #2']
03/13/2022 15:29:46 - INFO - __main__ -  [break-QDMR] question: If the rectangular dispenser on the left is taller than the white dispenser on the right.
03/13/2022 15:29:46 - INFO - __main__ - ['return dispensers ;return #1 that is rectangular ;return #2 that is on the  left ;return #1 that is white ;return #4 that is on the  right ;return tallness of #3 ;return tallness of #5 ;return if  #6 is higher than #7']
03/13/2022 15:29:46 - INFO - __main__ -  [break-QDMR] question: which airline has more business class flights than any other airline 
03/13/2022 15:29:46 - INFO - __main__ - ['return airlines ;return flights of #1 ;return #2 that are business class flights ;return number of  #3 for each #1 ;return #1 where #4 is the  highest']
03/13/2022 15:29:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 15:29:46 - INFO - __main__ - Tokenizing Output ...
03/13/2022 15:29:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 15:29:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 15:29:46 - INFO - __main__ - Printing 3 examples
03/13/2022 15:29:46 - INFO - __main__ -  [break-QDMR] question: What is the shape of the large cyan object?
03/13/2022 15:29:46 - INFO - __main__ - ['return objects ;return #1 that  are large ;return #2 that  are cyan ;return the  shape of #3']
03/13/2022 15:29:46 - INFO - __main__ -  [break-QDMR] question: Is the large purple thing made out of the same material as the large sphere? 
03/13/2022 15:29:46 - INFO - __main__ - ['return large thing ;return #1 that  is  purple ;return large sphere ;return material of #2 ;return material of #3 ;return is  #4 the   same as #5']
03/13/2022 15:29:46 - INFO - __main__ -  [break-QDMR] question: what object shape is there only two in matte?
03/13/2022 15:29:46 - INFO - __main__ - ['return shapes ;return #1 that  are matte ;return number of #2 for each #1 ;return #1 where #3 is  two']
03/13/2022 15:29:46 - INFO - __main__ - Tokenizing Input ...
03/13/2022 15:29:46 - INFO - __main__ - Tokenizing Output ...
03/13/2022 15:29:46 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 15:29:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 15:29:59 - INFO - __main__ - Starting training!
03/13/2022 15:30:03 - INFO - __main__ - Step 10 Global step 10 Train loss 22.527386 on epoch=4
03/13/2022 15:30:08 - INFO - __main__ - Step 20 Global step 20 Train loss 13.841191 on epoch=9
03/13/2022 15:30:13 - INFO - __main__ - Step 30 Global step 30 Train loss 6.250714 on epoch=14
03/13/2022 15:30:18 - INFO - __main__ - Step 40 Global step 40 Train loss 5.178321 on epoch=19
03/13/2022 15:30:23 - INFO - __main__ - Step 50 Global step 50 Train loss 4.248099 on epoch=24
03/13/2022 15:30:34 - INFO - __main__ - Global step 50 Train loss 10.409143 EM 0.0 on epoch=24
03/13/2022 15:30:40 - INFO - __main__ - Step 60 Global step 60 Train loss 3.228686 on epoch=29
03/13/2022 15:30:45 - INFO - __main__ - Step 70 Global step 70 Train loss 2.331606 on epoch=34
03/13/2022 15:30:50 - INFO - __main__ - Step 80 Global step 80 Train loss 1.366931 on epoch=39
03/13/2022 15:30:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.978773 on epoch=44
03/13/2022 15:31:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.679463 on epoch=49
03/13/2022 15:31:11 - INFO - __main__ - Global step 100 Train loss 1.717092 EM 0.0 on epoch=49
03/13/2022 15:31:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.460362 on epoch=54
03/13/2022 15:31:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.365407 on epoch=59
03/13/2022 15:31:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.262270 on epoch=64
03/13/2022 15:31:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.229469 on epoch=69
03/13/2022 15:31:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.167721 on epoch=74
03/13/2022 15:31:43 - INFO - __main__ - Global step 150 Train loss 0.297046 EM 0.0 on epoch=74
03/13/2022 15:31:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.124122 on epoch=79
03/13/2022 15:31:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.097832 on epoch=84
03/13/2022 15:31:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.084091 on epoch=89
03/13/2022 15:32:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.081390 on epoch=94
03/13/2022 15:32:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.067374 on epoch=99
03/13/2022 15:32:17 - INFO - __main__ - Global step 200 Train loss 0.090962 EM 0.0 on epoch=99
03/13/2022 15:32:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.072496 on epoch=104
03/13/2022 15:32:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.058690 on epoch=109
03/13/2022 15:32:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.056971 on epoch=114
03/13/2022 15:32:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.057828 on epoch=119
03/13/2022 15:32:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.038553 on epoch=124
03/13/2022 15:32:50 - INFO - __main__ - Global step 250 Train loss 0.056908 EM 0.0 on epoch=124
03/13/2022 15:32:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.056722 on epoch=129
03/13/2022 15:33:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.055345 on epoch=134
03/13/2022 15:33:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.048650 on epoch=139
03/13/2022 15:33:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.038279 on epoch=144
03/13/2022 15:33:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.025137 on epoch=149
03/13/2022 15:33:21 - INFO - __main__ - Global step 300 Train loss 0.044827 EM 0.0 on epoch=149
03/13/2022 15:33:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.022613 on epoch=154
03/13/2022 15:33:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.015097 on epoch=159
03/13/2022 15:33:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.019915 on epoch=164
03/13/2022 15:33:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.028358 on epoch=169
03/13/2022 15:33:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.019803 on epoch=174
03/13/2022 15:33:53 - INFO - __main__ - Global step 350 Train loss 0.021157 EM 0.0 on epoch=174
03/13/2022 15:33:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.018202 on epoch=179
03/13/2022 15:34:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.014335 on epoch=184
03/13/2022 15:34:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.009377 on epoch=189
03/13/2022 15:34:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.014873 on epoch=194
03/13/2022 15:34:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.021561 on epoch=199
03/13/2022 15:34:27 - INFO - __main__ - Global step 400 Train loss 0.015669 EM 0.0 on epoch=199
03/13/2022 15:34:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.010810 on epoch=204
03/13/2022 15:34:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.008710 on epoch=209
03/13/2022 15:34:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.012748 on epoch=214
03/13/2022 15:34:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.014242 on epoch=219
03/13/2022 15:34:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.014616 on epoch=224
03/13/2022 15:34:59 - INFO - __main__ - Global step 450 Train loss 0.012225 EM 0.0 on epoch=224
03/13/2022 15:35:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.017361 on epoch=229
03/13/2022 15:35:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.009878 on epoch=234
03/13/2022 15:35:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.005846 on epoch=239
03/13/2022 15:35:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.004212 on epoch=244
03/13/2022 15:35:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013175 on epoch=249
03/13/2022 15:35:32 - INFO - __main__ - Global step 500 Train loss 0.010095 EM 0.0 on epoch=249
03/13/2022 15:35:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.020063 on epoch=254
03/13/2022 15:35:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.005221 on epoch=259
03/13/2022 15:35:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.005582 on epoch=264
03/13/2022 15:35:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.010408 on epoch=269
03/13/2022 15:35:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.003673 on epoch=274
03/13/2022 15:36:06 - INFO - __main__ - Global step 550 Train loss 0.008989 EM 0.0 on epoch=274
03/13/2022 15:36:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.005470 on epoch=279
03/13/2022 15:36:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.002721 on epoch=284
03/13/2022 15:36:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.006074 on epoch=289
03/13/2022 15:36:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.059560 on epoch=294
03/13/2022 15:36:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.029779 on epoch=299
03/13/2022 15:36:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 15:36:32 - INFO - __main__ - Printing 3 examples
03/13/2022 15:36:32 - INFO - __main__ -  [break-QDMR] question: How many people are apart of the police department?
03/13/2022 15:36:32 - INFO - __main__ - ['return the  police department ;return people that are apart of #1 ;return number of  #2']
03/13/2022 15:36:32 - INFO - __main__ -  [break-QDMR] question: If the rectangular dispenser on the left is taller than the white dispenser on the right.
03/13/2022 15:36:32 - INFO - __main__ - ['return dispensers ;return #1 that is rectangular ;return #2 that is on the  left ;return #1 that is white ;return #4 that is on the  right ;return tallness of #3 ;return tallness of #5 ;return if  #6 is higher than #7']
03/13/2022 15:36:32 - INFO - __main__ -  [break-QDMR] question: which airline has more business class flights than any other airline 
03/13/2022 15:36:32 - INFO - __main__ - ['return airlines ;return flights of #1 ;return #2 that are business class flights ;return number of  #3 for each #1 ;return #1 where #4 is the  highest']
03/13/2022 15:36:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 15:36:32 - INFO - __main__ - Tokenizing Output ...
03/13/2022 15:36:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 15:36:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 15:36:32 - INFO - __main__ - Printing 3 examples
03/13/2022 15:36:32 - INFO - __main__ -  [break-QDMR] question: What is the shape of the large cyan object?
03/13/2022 15:36:32 - INFO - __main__ - ['return objects ;return #1 that  are large ;return #2 that  are cyan ;return the  shape of #3']
03/13/2022 15:36:32 - INFO - __main__ -  [break-QDMR] question: Is the large purple thing made out of the same material as the large sphere? 
03/13/2022 15:36:32 - INFO - __main__ - ['return large thing ;return #1 that  is  purple ;return large sphere ;return material of #2 ;return material of #3 ;return is  #4 the   same as #5']
03/13/2022 15:36:32 - INFO - __main__ -  [break-QDMR] question: what object shape is there only two in matte?
03/13/2022 15:36:32 - INFO - __main__ - ['return shapes ;return #1 that  are matte ;return number of #2 for each #1 ;return #1 where #3 is  two']
03/13/2022 15:36:32 - INFO - __main__ - Tokenizing Input ...
03/13/2022 15:36:32 - INFO - __main__ - Tokenizing Output ...
03/13/2022 15:36:32 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 15:36:40 - INFO - __main__ - Global step 600 Train loss 0.020721 EM 0.0 on epoch=299
03/13/2022 15:36:40 - INFO - __main__ - save last model!
03/13/2022 15:36:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 15:36:45 - INFO - __main__ - Starting training!
03/13/2022 15:36:47 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 15:36:47 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 15:36:47 - INFO - __main__ - Printing 3 examples
03/13/2022 15:36:47 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 15:36:47 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 15:36:47 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 15:36:47 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 15:36:47 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 15:36:47 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 15:36:47 - INFO - __main__ - Tokenizing Input ...
03/13/2022 15:36:50 - INFO - __main__ - Tokenizing Output ...
03/13/2022 15:36:59 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 16:17:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_87_0.0005_8_predictions.txt
03/13/2022 16:17:27 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 16:17:28 - INFO - __main__ - prefix=break-QDMR_32_87, lr=0.0005, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 16:17:28 - INFO - __main__ - Running ... prefix=break-QDMR_32_87, lr=0.0003, bsz=8 ...
03/13/2022 16:17:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 16:17:29 - INFO - __main__ - Printing 3 examples
03/13/2022 16:17:29 - INFO - __main__ -  [break-QDMR] question: How many people are apart of the police department?
03/13/2022 16:17:29 - INFO - __main__ - ['return the  police department ;return people that are apart of #1 ;return number of  #2']
03/13/2022 16:17:29 - INFO - __main__ -  [break-QDMR] question: If the rectangular dispenser on the left is taller than the white dispenser on the right.
03/13/2022 16:17:29 - INFO - __main__ - ['return dispensers ;return #1 that is rectangular ;return #2 that is on the  left ;return #1 that is white ;return #4 that is on the  right ;return tallness of #3 ;return tallness of #5 ;return if  #6 is higher than #7']
03/13/2022 16:17:29 - INFO - __main__ -  [break-QDMR] question: which airline has more business class flights than any other airline 
03/13/2022 16:17:29 - INFO - __main__ - ['return airlines ;return flights of #1 ;return #2 that are business class flights ;return number of  #3 for each #1 ;return #1 where #4 is the  highest']
03/13/2022 16:17:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 16:17:29 - INFO - __main__ - Tokenizing Output ...
03/13/2022 16:17:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 16:17:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 16:17:29 - INFO - __main__ - Printing 3 examples
03/13/2022 16:17:29 - INFO - __main__ -  [break-QDMR] question: What is the shape of the large cyan object?
03/13/2022 16:17:29 - INFO - __main__ - ['return objects ;return #1 that  are large ;return #2 that  are cyan ;return the  shape of #3']
03/13/2022 16:17:29 - INFO - __main__ -  [break-QDMR] question: Is the large purple thing made out of the same material as the large sphere? 
03/13/2022 16:17:29 - INFO - __main__ - ['return large thing ;return #1 that  is  purple ;return large sphere ;return material of #2 ;return material of #3 ;return is  #4 the   same as #5']
03/13/2022 16:17:29 - INFO - __main__ -  [break-QDMR] question: what object shape is there only two in matte?
03/13/2022 16:17:29 - INFO - __main__ - ['return shapes ;return #1 that  are matte ;return number of #2 for each #1 ;return #1 where #3 is  two']
03/13/2022 16:17:29 - INFO - __main__ - Tokenizing Input ...
03/13/2022 16:17:29 - INFO - __main__ - Tokenizing Output ...
03/13/2022 16:17:29 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 16:17:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 16:17:40 - INFO - __main__ - Starting training!
03/13/2022 16:17:44 - INFO - __main__ - Step 10 Global step 10 Train loss 21.997122 on epoch=4
03/13/2022 16:17:49 - INFO - __main__ - Step 20 Global step 20 Train loss 15.479917 on epoch=9
03/13/2022 16:17:54 - INFO - __main__ - Step 30 Global step 30 Train loss 8.516282 on epoch=14
03/13/2022 16:17:59 - INFO - __main__ - Step 40 Global step 40 Train loss 6.596780 on epoch=19
03/13/2022 16:18:04 - INFO - __main__ - Step 50 Global step 50 Train loss 5.840090 on epoch=24
03/13/2022 16:18:15 - INFO - __main__ - Global step 50 Train loss 11.686038 EM 0.0 on epoch=24
03/13/2022 16:18:20 - INFO - __main__ - Step 60 Global step 60 Train loss 4.876555 on epoch=29
03/13/2022 16:18:25 - INFO - __main__ - Step 70 Global step 70 Train loss 4.001629 on epoch=34
03/13/2022 16:18:30 - INFO - __main__ - Step 80 Global step 80 Train loss 3.226854 on epoch=39
03/13/2022 16:18:35 - INFO - __main__ - Step 90 Global step 90 Train loss 2.550079 on epoch=44
03/13/2022 16:18:40 - INFO - __main__ - Step 100 Global step 100 Train loss 2.118057 on epoch=49
03/13/2022 16:18:51 - INFO - __main__ - Global step 100 Train loss 3.354635 EM 0.0 on epoch=49
03/13/2022 16:18:56 - INFO - __main__ - Step 110 Global step 110 Train loss 1.715574 on epoch=54
03/13/2022 16:19:01 - INFO - __main__ - Step 120 Global step 120 Train loss 1.413780 on epoch=59
03/13/2022 16:19:06 - INFO - __main__ - Step 130 Global step 130 Train loss 1.089656 on epoch=64
03/13/2022 16:19:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.891278 on epoch=69
03/13/2022 16:19:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.694445 on epoch=74
03/13/2022 16:19:24 - INFO - __main__ - Global step 150 Train loss 1.160947 EM 0.0 on epoch=74
03/13/2022 16:19:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.658342 on epoch=79
03/13/2022 16:19:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.479749 on epoch=84
03/13/2022 16:19:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.422056 on epoch=89
03/13/2022 16:19:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.337562 on epoch=94
03/13/2022 16:19:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.296484 on epoch=99
03/13/2022 16:19:58 - INFO - __main__ - Global step 200 Train loss 0.438839 EM 0.0 on epoch=99
03/13/2022 16:20:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.237150 on epoch=104
03/13/2022 16:20:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.236954 on epoch=109
03/13/2022 16:20:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.207607 on epoch=114
03/13/2022 16:20:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.186834 on epoch=119
03/13/2022 16:20:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.156719 on epoch=124
03/13/2022 16:20:31 - INFO - __main__ - Global step 250 Train loss 0.205053 EM 0.0 on epoch=124
03/13/2022 16:20:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.134313 on epoch=129
03/13/2022 16:20:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.135523 on epoch=134
03/13/2022 16:20:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.118378 on epoch=139
03/13/2022 16:20:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.104000 on epoch=144
03/13/2022 16:20:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.096534 on epoch=149
03/13/2022 16:21:04 - INFO - __main__ - Global step 300 Train loss 0.117750 EM 0.0 on epoch=149
03/13/2022 16:21:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.102623 on epoch=154
03/13/2022 16:21:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.088883 on epoch=159
03/13/2022 16:21:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.067333 on epoch=164
03/13/2022 16:21:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.089713 on epoch=169
03/13/2022 16:21:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.065210 on epoch=174
03/13/2022 16:21:37 - INFO - __main__ - Global step 350 Train loss 0.082752 EM 0.0 on epoch=174
03/13/2022 16:21:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.067526 on epoch=179
03/13/2022 16:21:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.080436 on epoch=184
03/13/2022 16:21:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.057722 on epoch=189
03/13/2022 16:21:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.057817 on epoch=194
03/13/2022 16:22:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.047460 on epoch=199
03/13/2022 16:22:08 - INFO - __main__ - Global step 400 Train loss 0.062192 EM 0.0 on epoch=199
03/13/2022 16:22:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.044932 on epoch=204
03/13/2022 16:22:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.036423 on epoch=209
03/13/2022 16:22:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.038731 on epoch=214
03/13/2022 16:22:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.045397 on epoch=219
03/13/2022 16:22:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.042649 on epoch=224
03/13/2022 16:22:41 - INFO - __main__ - Global step 450 Train loss 0.041627 EM 0.0 on epoch=224
03/13/2022 16:22:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.051646 on epoch=229
03/13/2022 16:22:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.048454 on epoch=234
03/13/2022 16:22:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.070756 on epoch=239
03/13/2022 16:23:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.056780 on epoch=244
03/13/2022 16:23:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.051632 on epoch=249
03/13/2022 16:23:14 - INFO - __main__ - Global step 500 Train loss 0.055854 EM 0.0 on epoch=249
03/13/2022 16:23:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.057689 on epoch=254
03/13/2022 16:23:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.055747 on epoch=259
03/13/2022 16:23:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.048688 on epoch=264
03/13/2022 16:23:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.038599 on epoch=269
03/13/2022 16:23:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.044305 on epoch=274
03/13/2022 16:23:48 - INFO - __main__ - Global step 550 Train loss 0.049006 EM 0.0 on epoch=274
03/13/2022 16:23:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.032519 on epoch=279
03/13/2022 16:23:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.036387 on epoch=284
03/13/2022 16:24:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.037235 on epoch=289
03/13/2022 16:24:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.046429 on epoch=294
03/13/2022 16:24:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.046574 on epoch=299
03/13/2022 16:24:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 16:24:14 - INFO - __main__ - Printing 3 examples
03/13/2022 16:24:14 - INFO - __main__ -  [break-QDMR] question: How many people are apart of the police department?
03/13/2022 16:24:14 - INFO - __main__ - ['return the  police department ;return people that are apart of #1 ;return number of  #2']
03/13/2022 16:24:14 - INFO - __main__ -  [break-QDMR] question: If the rectangular dispenser on the left is taller than the white dispenser on the right.
03/13/2022 16:24:14 - INFO - __main__ - ['return dispensers ;return #1 that is rectangular ;return #2 that is on the  left ;return #1 that is white ;return #4 that is on the  right ;return tallness of #3 ;return tallness of #5 ;return if  #6 is higher than #7']
03/13/2022 16:24:14 - INFO - __main__ -  [break-QDMR] question: which airline has more business class flights than any other airline 
03/13/2022 16:24:14 - INFO - __main__ - ['return airlines ;return flights of #1 ;return #2 that are business class flights ;return number of  #3 for each #1 ;return #1 where #4 is the  highest']
03/13/2022 16:24:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 16:24:14 - INFO - __main__ - Tokenizing Output ...
03/13/2022 16:24:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 16:24:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 16:24:14 - INFO - __main__ - Printing 3 examples
03/13/2022 16:24:14 - INFO - __main__ -  [break-QDMR] question: What is the shape of the large cyan object?
03/13/2022 16:24:14 - INFO - __main__ - ['return objects ;return #1 that  are large ;return #2 that  are cyan ;return the  shape of #3']
03/13/2022 16:24:14 - INFO - __main__ -  [break-QDMR] question: Is the large purple thing made out of the same material as the large sphere? 
03/13/2022 16:24:14 - INFO - __main__ - ['return large thing ;return #1 that  is  purple ;return large sphere ;return material of #2 ;return material of #3 ;return is  #4 the   same as #5']
03/13/2022 16:24:14 - INFO - __main__ -  [break-QDMR] question: what object shape is there only two in matte?
03/13/2022 16:24:14 - INFO - __main__ - ['return shapes ;return #1 that  are matte ;return number of #2 for each #1 ;return #1 where #3 is  two']
03/13/2022 16:24:14 - INFO - __main__ - Tokenizing Input ...
03/13/2022 16:24:14 - INFO - __main__ - Tokenizing Output ...
03/13/2022 16:24:14 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 16:24:18 - INFO - __main__ - Global step 600 Train loss 0.039829 EM 0.0 on epoch=299
03/13/2022 16:24:18 - INFO - __main__ - save last model!
03/13/2022 16:24:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 16:24:25 - INFO - __main__ - Starting training!
03/13/2022 16:24:25 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 16:24:26 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 16:24:26 - INFO - __main__ - Printing 3 examples
03/13/2022 16:24:26 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 16:24:26 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 16:24:26 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 16:24:26 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 16:24:26 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 16:24:26 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 16:24:26 - INFO - __main__ - Tokenizing Input ...
03/13/2022 16:24:29 - INFO - __main__ - Tokenizing Output ...
03/13/2022 16:24:37 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 17:08:49 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_87_0.0003_8_predictions.txt
03/13/2022 17:08:50 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 17:08:51 - INFO - __main__ - prefix=break-QDMR_32_87, lr=0.0003, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 17:08:51 - INFO - __main__ - Running ... prefix=break-QDMR_32_87, lr=0.0002, bsz=8 ...
03/13/2022 17:08:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 17:08:52 - INFO - __main__ - Printing 3 examples
03/13/2022 17:08:52 - INFO - __main__ -  [break-QDMR] question: How many people are apart of the police department?
03/13/2022 17:08:52 - INFO - __main__ - ['return the  police department ;return people that are apart of #1 ;return number of  #2']
03/13/2022 17:08:52 - INFO - __main__ -  [break-QDMR] question: If the rectangular dispenser on the left is taller than the white dispenser on the right.
03/13/2022 17:08:52 - INFO - __main__ - ['return dispensers ;return #1 that is rectangular ;return #2 that is on the  left ;return #1 that is white ;return #4 that is on the  right ;return tallness of #3 ;return tallness of #5 ;return if  #6 is higher than #7']
03/13/2022 17:08:52 - INFO - __main__ -  [break-QDMR] question: which airline has more business class flights than any other airline 
03/13/2022 17:08:52 - INFO - __main__ - ['return airlines ;return flights of #1 ;return #2 that are business class flights ;return number of  #3 for each #1 ;return #1 where #4 is the  highest']
03/13/2022 17:08:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 17:08:52 - INFO - __main__ - Tokenizing Output ...
03/13/2022 17:08:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 17:08:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 17:08:52 - INFO - __main__ - Printing 3 examples
03/13/2022 17:08:52 - INFO - __main__ -  [break-QDMR] question: What is the shape of the large cyan object?
03/13/2022 17:08:52 - INFO - __main__ - ['return objects ;return #1 that  are large ;return #2 that  are cyan ;return the  shape of #3']
03/13/2022 17:08:52 - INFO - __main__ -  [break-QDMR] question: Is the large purple thing made out of the same material as the large sphere? 
03/13/2022 17:08:52 - INFO - __main__ - ['return large thing ;return #1 that  is  purple ;return large sphere ;return material of #2 ;return material of #3 ;return is  #4 the   same as #5']
03/13/2022 17:08:52 - INFO - __main__ -  [break-QDMR] question: what object shape is there only two in matte?
03/13/2022 17:08:52 - INFO - __main__ - ['return shapes ;return #1 that  are matte ;return number of #2 for each #1 ;return #1 where #3 is  two']
03/13/2022 17:08:52 - INFO - __main__ - Tokenizing Input ...
03/13/2022 17:08:52 - INFO - __main__ - Tokenizing Output ...
03/13/2022 17:08:52 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 17:09:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 17:09:03 - INFO - __main__ - Starting training!
03/13/2022 17:09:07 - INFO - __main__ - Step 10 Global step 10 Train loss 21.060802 on epoch=4
03/13/2022 17:09:12 - INFO - __main__ - Step 20 Global step 20 Train loss 16.452175 on epoch=9
03/13/2022 17:09:17 - INFO - __main__ - Step 30 Global step 30 Train loss 7.573934 on epoch=14
03/13/2022 17:09:22 - INFO - __main__ - Step 40 Global step 40 Train loss 5.075614 on epoch=19
03/13/2022 17:09:27 - INFO - __main__ - Step 50 Global step 50 Train loss 3.916099 on epoch=24
03/13/2022 17:09:38 - INFO - __main__ - Global step 50 Train loss 10.815725 EM 0.0 on epoch=24
03/13/2022 17:09:44 - INFO - __main__ - Step 60 Global step 60 Train loss 3.157478 on epoch=29
03/13/2022 17:09:49 - INFO - __main__ - Step 70 Global step 70 Train loss 2.659449 on epoch=34
03/13/2022 17:09:54 - INFO - __main__ - Step 80 Global step 80 Train loss 2.286922 on epoch=39
03/13/2022 17:09:59 - INFO - __main__ - Step 90 Global step 90 Train loss 2.094838 on epoch=44
03/13/2022 17:10:04 - INFO - __main__ - Step 100 Global step 100 Train loss 1.732866 on epoch=49
03/13/2022 17:10:14 - INFO - __main__ - Global step 100 Train loss 2.386311 EM 0.0 on epoch=49
03/13/2022 17:10:19 - INFO - __main__ - Step 110 Global step 110 Train loss 1.462382 on epoch=54
03/13/2022 17:10:24 - INFO - __main__ - Step 120 Global step 120 Train loss 1.298153 on epoch=59
03/13/2022 17:10:29 - INFO - __main__ - Step 130 Global step 130 Train loss 1.115890 on epoch=64
03/13/2022 17:10:34 - INFO - __main__ - Step 140 Global step 140 Train loss 1.011937 on epoch=69
03/13/2022 17:10:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.853942 on epoch=74
03/13/2022 17:10:49 - INFO - __main__ - Global step 150 Train loss 1.148461 EM 0.0 on epoch=74
03/13/2022 17:10:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.754272 on epoch=79
03/13/2022 17:10:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.590059 on epoch=84
03/13/2022 17:11:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.520617 on epoch=89
03/13/2022 17:11:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.513196 on epoch=94
03/13/2022 17:11:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.416471 on epoch=99
03/13/2022 17:11:23 - INFO - __main__ - Global step 200 Train loss 0.558923 EM 0.0 on epoch=99
03/13/2022 17:11:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.375814 on epoch=104
03/13/2022 17:11:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.369765 on epoch=109
03/13/2022 17:11:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.266960 on epoch=114
03/13/2022 17:11:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.214851 on epoch=119
03/13/2022 17:11:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.214833 on epoch=124
03/13/2022 17:11:56 - INFO - __main__ - Global step 250 Train loss 0.288445 EM 0.0 on epoch=124
03/13/2022 17:12:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.174531 on epoch=129
03/13/2022 17:12:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.201417 on epoch=134
03/13/2022 17:12:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.449507 on epoch=139
03/13/2022 17:12:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.641827 on epoch=144
03/13/2022 17:12:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.403179 on epoch=149
03/13/2022 17:12:28 - INFO - __main__ - Global step 300 Train loss 0.374092 EM 0.0 on epoch=149
03/13/2022 17:12:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.239940 on epoch=154
03/13/2022 17:12:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.216209 on epoch=159
03/13/2022 17:12:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.164549 on epoch=164
03/13/2022 17:12:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.165438 on epoch=169
03/13/2022 17:12:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.145633 on epoch=174
03/13/2022 17:12:59 - INFO - __main__ - Global step 350 Train loss 0.186354 EM 0.0 on epoch=174
03/13/2022 17:13:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.137533 on epoch=179
03/13/2022 17:13:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.130958 on epoch=184
03/13/2022 17:13:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.192999 on epoch=189
03/13/2022 17:13:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.133939 on epoch=194
03/13/2022 17:13:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.124957 on epoch=199
03/13/2022 17:13:34 - INFO - __main__ - Global step 400 Train loss 0.144077 EM 0.0 on epoch=199
03/13/2022 17:13:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.119138 on epoch=204
03/13/2022 17:13:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.103708 on epoch=209
03/13/2022 17:13:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.127582 on epoch=214
03/13/2022 17:13:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.128262 on epoch=219
03/13/2022 17:13:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.122090 on epoch=224
03/13/2022 17:14:07 - INFO - __main__ - Global step 450 Train loss 0.120156 EM 0.0 on epoch=224
03/13/2022 17:14:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.125369 on epoch=229
03/13/2022 17:14:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.098681 on epoch=234
03/13/2022 17:14:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.091571 on epoch=239
03/13/2022 17:14:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.117464 on epoch=244
03/13/2022 17:14:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.103617 on epoch=249
03/13/2022 17:14:40 - INFO - __main__ - Global step 500 Train loss 0.107340 EM 0.0 on epoch=249
03/13/2022 17:14:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.060390 on epoch=254
03/13/2022 17:14:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.060322 on epoch=259
03/13/2022 17:14:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.078377 on epoch=264
03/13/2022 17:15:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.061073 on epoch=269
03/13/2022 17:15:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.054192 on epoch=274
03/13/2022 17:15:13 - INFO - __main__ - Global step 550 Train loss 0.062871 EM 0.0 on epoch=274
03/13/2022 17:15:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.053324 on epoch=279
03/13/2022 17:15:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.045072 on epoch=284
03/13/2022 17:15:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.050981 on epoch=289
03/13/2022 17:15:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.060878 on epoch=294
03/13/2022 17:15:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.044164 on epoch=299
03/13/2022 17:15:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 17:15:39 - INFO - __main__ - Printing 3 examples
03/13/2022 17:15:39 - INFO - __main__ -  [break-QDMR] question: How many people are apart of the police department?
03/13/2022 17:15:39 - INFO - __main__ - ['return the  police department ;return people that are apart of #1 ;return number of  #2']
03/13/2022 17:15:39 - INFO - __main__ -  [break-QDMR] question: If the rectangular dispenser on the left is taller than the white dispenser on the right.
03/13/2022 17:15:39 - INFO - __main__ - ['return dispensers ;return #1 that is rectangular ;return #2 that is on the  left ;return #1 that is white ;return #4 that is on the  right ;return tallness of #3 ;return tallness of #5 ;return if  #6 is higher than #7']
03/13/2022 17:15:39 - INFO - __main__ -  [break-QDMR] question: which airline has more business class flights than any other airline 
03/13/2022 17:15:39 - INFO - __main__ - ['return airlines ;return flights of #1 ;return #2 that are business class flights ;return number of  #3 for each #1 ;return #1 where #4 is the  highest']
03/13/2022 17:15:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 17:15:39 - INFO - __main__ - Tokenizing Output ...
03/13/2022 17:15:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 17:15:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 17:15:39 - INFO - __main__ - Printing 3 examples
03/13/2022 17:15:39 - INFO - __main__ -  [break-QDMR] question: What is the shape of the large cyan object?
03/13/2022 17:15:39 - INFO - __main__ - ['return objects ;return #1 that  are large ;return #2 that  are cyan ;return the  shape of #3']
03/13/2022 17:15:39 - INFO - __main__ -  [break-QDMR] question: Is the large purple thing made out of the same material as the large sphere? 
03/13/2022 17:15:39 - INFO - __main__ - ['return large thing ;return #1 that  is  purple ;return large sphere ;return material of #2 ;return material of #3 ;return is  #4 the   same as #5']
03/13/2022 17:15:39 - INFO - __main__ -  [break-QDMR] question: what object shape is there only two in matte?
03/13/2022 17:15:39 - INFO - __main__ - ['return shapes ;return #1 that  are matte ;return number of #2 for each #1 ;return #1 where #3 is  two']
03/13/2022 17:15:39 - INFO - __main__ - Tokenizing Input ...
03/13/2022 17:15:39 - INFO - __main__ - Tokenizing Output ...
03/13/2022 17:15:39 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 17:15:46 - INFO - __main__ - Global step 600 Train loss 0.050884 EM 0.0 on epoch=299
03/13/2022 17:15:46 - INFO - __main__ - save last model!
03/13/2022 17:15:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 17:15:52 - INFO - __main__ - Starting training!
03/13/2022 17:15:53 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 17:15:54 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 17:15:54 - INFO - __main__ - Printing 3 examples
03/13/2022 17:15:54 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 17:15:54 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 17:15:54 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 17:15:54 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 17:15:54 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 17:15:54 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 17:15:54 - INFO - __main__ - Tokenizing Input ...
03/13/2022 17:15:57 - INFO - __main__ - Tokenizing Output ...
03/13/2022 17:16:05 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 18:01:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_87_0.0002_8_predictions.txt
03/13/2022 18:01:13 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 18:01:14 - INFO - __main__ - prefix=break-QDMR_32_87, lr=0.0002, bsz=8, dev_performance=0.0, test_performance=0.0
03/13/2022 18:01:14 - INFO - __main__ - Running ... prefix=break-QDMR_32_87, lr=0.0001, bsz=8 ...
03/13/2022 18:01:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 18:01:15 - INFO - __main__ - Printing 3 examples
03/13/2022 18:01:15 - INFO - __main__ -  [break-QDMR] question: How many people are apart of the police department?
03/13/2022 18:01:15 - INFO - __main__ - ['return the  police department ;return people that are apart of #1 ;return number of  #2']
03/13/2022 18:01:15 - INFO - __main__ -  [break-QDMR] question: If the rectangular dispenser on the left is taller than the white dispenser on the right.
03/13/2022 18:01:15 - INFO - __main__ - ['return dispensers ;return #1 that is rectangular ;return #2 that is on the  left ;return #1 that is white ;return #4 that is on the  right ;return tallness of #3 ;return tallness of #5 ;return if  #6 is higher than #7']
03/13/2022 18:01:15 - INFO - __main__ -  [break-QDMR] question: which airline has more business class flights than any other airline 
03/13/2022 18:01:15 - INFO - __main__ - ['return airlines ;return flights of #1 ;return #2 that are business class flights ;return number of  #3 for each #1 ;return #1 where #4 is the  highest']
03/13/2022 18:01:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 18:01:15 - INFO - __main__ - Tokenizing Output ...
03/13/2022 18:01:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/13/2022 18:01:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/13/2022 18:01:15 - INFO - __main__ - Printing 3 examples
03/13/2022 18:01:15 - INFO - __main__ -  [break-QDMR] question: What is the shape of the large cyan object?
03/13/2022 18:01:15 - INFO - __main__ - ['return objects ;return #1 that  are large ;return #2 that  are cyan ;return the  shape of #3']
03/13/2022 18:01:15 - INFO - __main__ -  [break-QDMR] question: Is the large purple thing made out of the same material as the large sphere? 
03/13/2022 18:01:15 - INFO - __main__ - ['return large thing ;return #1 that  is  purple ;return large sphere ;return material of #2 ;return material of #3 ;return is  #4 the   same as #5']
03/13/2022 18:01:15 - INFO - __main__ -  [break-QDMR] question: what object shape is there only two in matte?
03/13/2022 18:01:15 - INFO - __main__ - ['return shapes ;return #1 that  are matte ;return number of #2 for each #1 ;return #1 where #3 is  two']
03/13/2022 18:01:15 - INFO - __main__ - Tokenizing Input ...
03/13/2022 18:01:15 - INFO - __main__ - Tokenizing Output ...
03/13/2022 18:01:15 - INFO - __main__ - Loaded 32 examples from dev data
03/13/2022 18:01:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 18:01:26 - INFO - __main__ - Starting training!
03/13/2022 18:01:30 - INFO - __main__ - Step 10 Global step 10 Train loss 21.837605 on epoch=4
03/13/2022 18:01:35 - INFO - __main__ - Step 20 Global step 20 Train loss 17.143139 on epoch=9
03/13/2022 18:01:40 - INFO - __main__ - Step 30 Global step 30 Train loss 12.374871 on epoch=14
03/13/2022 18:01:45 - INFO - __main__ - Step 40 Global step 40 Train loss 9.561899 on epoch=19
03/13/2022 18:01:50 - INFO - __main__ - Step 50 Global step 50 Train loss 8.228202 on epoch=24
03/13/2022 18:02:01 - INFO - __main__ - Global step 50 Train loss 13.829144 EM 0.0 on epoch=24
03/13/2022 18:02:07 - INFO - __main__ - Step 60 Global step 60 Train loss 7.981092 on epoch=29
03/13/2022 18:02:12 - INFO - __main__ - Step 70 Global step 70 Train loss 6.938435 on epoch=34
03/13/2022 18:02:17 - INFO - __main__ - Step 80 Global step 80 Train loss 6.251736 on epoch=39
03/13/2022 18:02:22 - INFO - __main__ - Step 90 Global step 90 Train loss 5.805457 on epoch=44
03/13/2022 18:02:27 - INFO - __main__ - Step 100 Global step 100 Train loss 5.742110 on epoch=49
03/13/2022 18:02:38 - INFO - __main__ - Global step 100 Train loss 6.543765 EM 0.0 on epoch=49
03/13/2022 18:02:43 - INFO - __main__ - Step 110 Global step 110 Train loss 5.441484 on epoch=54
03/13/2022 18:02:48 - INFO - __main__ - Step 120 Global step 120 Train loss 5.087247 on epoch=59
03/13/2022 18:02:53 - INFO - __main__ - Step 130 Global step 130 Train loss 4.831286 on epoch=64
03/13/2022 18:02:59 - INFO - __main__ - Step 140 Global step 140 Train loss 4.461567 on epoch=69
03/13/2022 18:03:04 - INFO - __main__ - Step 150 Global step 150 Train loss 4.146869 on epoch=74
03/13/2022 18:03:15 - INFO - __main__ - Global step 150 Train loss 4.793691 EM 0.0 on epoch=74
03/13/2022 18:03:20 - INFO - __main__ - Step 160 Global step 160 Train loss 3.647945 on epoch=79
03/13/2022 18:03:25 - INFO - __main__ - Step 170 Global step 170 Train loss 3.257483 on epoch=84
03/13/2022 18:03:30 - INFO - __main__ - Step 180 Global step 180 Train loss 3.058453 on epoch=89
03/13/2022 18:03:35 - INFO - __main__ - Step 190 Global step 190 Train loss 2.820066 on epoch=94
03/13/2022 18:03:40 - INFO - __main__ - Step 200 Global step 200 Train loss 2.663238 on epoch=99
03/13/2022 18:03:51 - INFO - __main__ - Global step 200 Train loss 3.089437 EM 0.0 on epoch=99
03/13/2022 18:03:56 - INFO - __main__ - Step 210 Global step 210 Train loss 2.351491 on epoch=104
03/13/2022 18:04:01 - INFO - __main__ - Step 220 Global step 220 Train loss 2.147367 on epoch=109
03/13/2022 18:04:06 - INFO - __main__ - Step 230 Global step 230 Train loss 1.936219 on epoch=114
03/13/2022 18:04:11 - INFO - __main__ - Step 240 Global step 240 Train loss 1.859858 on epoch=119
03/13/2022 18:04:16 - INFO - __main__ - Step 250 Global step 250 Train loss 1.737173 on epoch=124
03/13/2022 18:04:27 - INFO - __main__ - Global step 250 Train loss 2.006422 EM 0.0 on epoch=124
03/13/2022 18:04:32 - INFO - __main__ - Step 260 Global step 260 Train loss 1.574830 on epoch=129
03/13/2022 18:04:37 - INFO - __main__ - Step 270 Global step 270 Train loss 1.438766 on epoch=134
03/13/2022 18:04:42 - INFO - __main__ - Step 280 Global step 280 Train loss 1.360844 on epoch=139
03/13/2022 18:04:47 - INFO - __main__ - Step 290 Global step 290 Train loss 1.210558 on epoch=144
03/13/2022 18:04:52 - INFO - __main__ - Step 300 Global step 300 Train loss 1.148216 on epoch=149
03/13/2022 18:05:02 - INFO - __main__ - Global step 300 Train loss 1.346643 EM 0.0 on epoch=149
03/13/2022 18:05:07 - INFO - __main__ - Step 310 Global step 310 Train loss 1.060922 on epoch=154
03/13/2022 18:05:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.997314 on epoch=159
03/13/2022 18:05:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.898692 on epoch=164
03/13/2022 18:05:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.844931 on epoch=169
03/13/2022 18:05:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.771545 on epoch=174
03/13/2022 18:05:37 - INFO - __main__ - Global step 350 Train loss 0.914681 EM 0.0 on epoch=174
03/13/2022 18:05:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.706868 on epoch=179
03/13/2022 18:05:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.658859 on epoch=184
03/13/2022 18:05:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.604093 on epoch=189
03/13/2022 18:05:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.575146 on epoch=194
03/13/2022 18:06:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.496886 on epoch=199
03/13/2022 18:06:11 - INFO - __main__ - Global step 400 Train loss 0.608371 EM 0.0 on epoch=199
03/13/2022 18:06:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.426561 on epoch=204
03/13/2022 18:06:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.445605 on epoch=209
03/13/2022 18:06:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.432435 on epoch=214
03/13/2022 18:06:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.360028 on epoch=219
03/13/2022 18:06:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.369878 on epoch=224
03/13/2022 18:06:44 - INFO - __main__ - Global step 450 Train loss 0.406901 EM 0.0 on epoch=224
03/13/2022 18:06:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.390681 on epoch=229
03/13/2022 18:06:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.332126 on epoch=234
03/13/2022 18:06:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.302317 on epoch=239
03/13/2022 18:07:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.229445 on epoch=244
03/13/2022 18:07:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.190036 on epoch=249
03/13/2022 18:07:17 - INFO - __main__ - Global step 500 Train loss 0.288921 EM 0.0 on epoch=249
03/13/2022 18:07:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.224786 on epoch=254
03/13/2022 18:07:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.216795 on epoch=259
03/13/2022 18:07:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.201070 on epoch=264
03/13/2022 18:07:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.202237 on epoch=269
03/13/2022 18:07:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.157305 on epoch=274
03/13/2022 18:07:50 - INFO - __main__ - Global step 550 Train loss 0.200439 EM 0.0 on epoch=274
03/13/2022 18:07:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.168654 on epoch=279
03/13/2022 18:07:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.203286 on epoch=284
03/13/2022 18:08:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.151679 on epoch=289
03/13/2022 18:08:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.130523 on epoch=294
03/13/2022 18:08:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.139355 on epoch=299
03/13/2022 18:08:22 - INFO - __main__ - Global step 600 Train loss 0.158700 EM 0.0 on epoch=299
03/13/2022 18:08:22 - INFO - __main__ - save last model!
03/13/2022 18:08:29 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 18:08:29 - INFO - __main__ - Start tokenizing ... 7760 instances
03/13/2022 18:08:29 - INFO - __main__ - Printing 3 examples
03/13/2022 18:08:29 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/13/2022 18:08:29 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/13/2022 18:08:29 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/13/2022 18:08:29 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/13/2022 18:08:29 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/13/2022 18:08:29 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/13/2022 18:08:29 - INFO - __main__ - Tokenizing Input ...
03/13/2022 18:08:32 - INFO - __main__ - Tokenizing Output ...
03/13/2022 18:08:41 - INFO - __main__ - Loaded 7760 examples from test data
03/13/2022 18:51:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-break-QDMR/break-QDMR_32_87_0.0001_8_predictions.txt
03/13/2022 18:51:47 - INFO - __main__ - EM on test data: 0.0000
03/13/2022 18:51:49 - INFO - __main__ - prefix=break-QDMR_32_87, lr=0.0001, bsz=8, dev_performance=0.0, test_performance=0.0
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (16822): No such process
Task: hatexplain, Checkpoint: None, Identifier: T5-large-ft-random
Output directory () already exists and is not empty.
03/13/2022 18:51:55 - INFO - __main__ - Namespace(task_dir='data/hatexplain/', task_name='hatexplain', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-hatexplain', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/13/2022 18:51:55 - INFO - __main__ - models/T5-large-ft-random/singletask-hatexplain
03/13/2022 18:51:55 - INFO - __main__ - Namespace(task_dir='data/hatexplain/', task_name='hatexplain', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-hatexplain', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/13/2022 18:51:55 - INFO - __main__ - models/T5-large-ft-random/singletask-hatexplain
03/13/2022 18:51:56 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/13/2022 18:51:56 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/13/2022 18:51:56 - INFO - __main__ - args.device: cuda:0
03/13/2022 18:51:56 - INFO - __main__ - Using 2 gpus
03/13/2022 18:51:56 - INFO - __main__ - args.device: cuda:1
03/13/2022 18:51:56 - INFO - __main__ - Using 2 gpus
03/13/2022 18:51:56 - INFO - __main__ - Fine-tuning the following samples: ['hatexplain_16_100', 'hatexplain_16_13', 'hatexplain_16_21', 'hatexplain_16_42', 'hatexplain_16_87']
03/13/2022 18:51:56 - INFO - __main__ - Fine-tuning the following samples: ['hatexplain_16_100', 'hatexplain_16_13', 'hatexplain_16_21', 'hatexplain_16_42', 'hatexplain_16_87']
03/13/2022 18:52:00 - INFO - __main__ - Running ... prefix=hatexplain_16_100, lr=0.0005, bsz=8 ...
03/13/2022 18:52:01 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 18:52:01 - INFO - __main__ - Printing 3 examples
03/13/2022 18:52:01 - INFO - __main__ -  [hatexplain]     germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/13/2022 18:52:01 - INFO - __main__ - ['offensive']
03/13/2022 18:52:01 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/13/2022 18:52:01 - INFO - __main__ - ['offensive']
03/13/2022 18:52:01 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/13/2022 18:52:01 - INFO - __main__ - ['offensive']
03/13/2022 18:52:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 18:52:01 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 18:52:01 - INFO - __main__ - Printing 3 examples
03/13/2022 18:52:01 - INFO - __main__ -  [hatexplain]     germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/13/2022 18:52:01 - INFO - __main__ - ['offensive']
03/13/2022 18:52:01 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/13/2022 18:52:01 - INFO - __main__ - ['offensive']
03/13/2022 18:52:01 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/13/2022 18:52:01 - INFO - __main__ - ['offensive']
03/13/2022 18:52:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 18:52:01 - INFO - __main__ - Tokenizing Output ...
03/13/2022 18:52:01 - INFO - __main__ - Tokenizing Output ...
03/13/2022 18:52:01 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 18:52:01 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 18:52:01 - INFO - __main__ - Printing 3 examples
03/13/2022 18:52:01 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/13/2022 18:52:01 - INFO - __main__ - ['offensive']
03/13/2022 18:52:01 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/13/2022 18:52:01 - INFO - __main__ - ['offensive']
03/13/2022 18:52:01 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 
03/13/2022 18:52:01 - INFO - __main__ - ['offensive']
03/13/2022 18:52:01 - INFO - __main__ - Tokenizing Input ...
03/13/2022 18:52:01 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 18:52:01 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 18:52:01 - INFO - __main__ - Printing 3 examples
03/13/2022 18:52:01 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/13/2022 18:52:01 - INFO - __main__ - ['offensive']
03/13/2022 18:52:01 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/13/2022 18:52:01 - INFO - __main__ - ['offensive']
03/13/2022 18:52:01 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 
03/13/2022 18:52:01 - INFO - __main__ - ['offensive']
03/13/2022 18:52:01 - INFO - __main__ - Tokenizing Input ...
03/13/2022 18:52:01 - INFO - __main__ - Tokenizing Output ...
03/13/2022 18:52:01 - INFO - __main__ - Tokenizing Output ...
03/13/2022 18:52:01 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 18:52:01 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 18:52:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 18:52:14 - INFO - __main__ - Starting training!
03/13/2022 18:52:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 18:52:14 - INFO - __main__ - Starting training!
03/13/2022 18:52:19 - INFO - __main__ - Step 10 Global step 10 Train loss 21.017229 on epoch=3
03/13/2022 18:52:24 - INFO - __main__ - Step 20 Global step 20 Train loss 17.429760 on epoch=6
03/13/2022 18:52:29 - INFO - __main__ - Step 30 Global step 30 Train loss 12.168169 on epoch=9
03/13/2022 18:52:34 - INFO - __main__ - Step 40 Global step 40 Train loss 10.304108 on epoch=13
03/13/2022 18:52:39 - INFO - __main__ - Step 50 Global step 50 Train loss 9.581705 on epoch=16
03/13/2022 18:52:41 - INFO - __main__ - Global step 50 Train loss 14.100194 Classification-F1 0.0 on epoch=16
03/13/2022 18:52:48 - INFO - __main__ - Step 60 Global step 60 Train loss 7.440858 on epoch=19
03/13/2022 18:52:53 - INFO - __main__ - Step 70 Global step 70 Train loss 5.499893 on epoch=23
03/13/2022 18:52:58 - INFO - __main__ - Step 80 Global step 80 Train loss 3.351662 on epoch=26
03/13/2022 18:53:03 - INFO - __main__ - Step 90 Global step 90 Train loss 2.673774 on epoch=29
03/13/2022 18:53:08 - INFO - __main__ - Step 100 Global step 100 Train loss 1.998204 on epoch=33
03/13/2022 18:53:09 - INFO - __main__ - Global step 100 Train loss 4.192878 Classification-F1 0.16666666666666666 on epoch=33
03/13/2022 18:53:16 - INFO - __main__ - Step 110 Global step 110 Train loss 2.007862 on epoch=36
03/13/2022 18:53:21 - INFO - __main__ - Step 120 Global step 120 Train loss 1.839201 on epoch=39
03/13/2022 18:53:26 - INFO - __main__ - Step 130 Global step 130 Train loss 2.372794 on epoch=43
03/13/2022 18:53:31 - INFO - __main__ - Step 140 Global step 140 Train loss 1.914384 on epoch=46
03/13/2022 18:53:36 - INFO - __main__ - Step 150 Global step 150 Train loss 1.833459 on epoch=49
03/13/2022 18:53:37 - INFO - __main__ - Global step 150 Train loss 1.993540 Classification-F1 0.16666666666666666 on epoch=49
03/13/2022 18:53:42 - INFO - __main__ - Step 160 Global step 160 Train loss 1.860986 on epoch=53
03/13/2022 18:53:47 - INFO - __main__ - Step 170 Global step 170 Train loss 1.731154 on epoch=56
03/13/2022 18:53:52 - INFO - __main__ - Step 180 Global step 180 Train loss 1.436525 on epoch=59
03/13/2022 18:53:57 - INFO - __main__ - Step 190 Global step 190 Train loss 1.513515 on epoch=63
03/13/2022 18:54:02 - INFO - __main__ - Step 200 Global step 200 Train loss 1.329725 on epoch=66
03/13/2022 18:54:03 - INFO - __main__ - Global step 200 Train loss 1.574381 Classification-F1 0.21001779811848462 on epoch=66
03/13/2022 18:54:09 - INFO - __main__ - Step 210 Global step 210 Train loss 1.123140 on epoch=69
03/13/2022 18:54:14 - INFO - __main__ - Step 220 Global step 220 Train loss 1.105427 on epoch=73
03/13/2022 18:54:19 - INFO - __main__ - Step 230 Global step 230 Train loss 1.018878 on epoch=76
03/13/2022 18:54:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.732703 on epoch=79
03/13/2022 18:54:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.844853 on epoch=83
03/13/2022 18:54:30 - INFO - __main__ - Global step 250 Train loss 0.965000 Classification-F1 0.2647296206618241 on epoch=83
03/13/2022 18:54:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.719795 on epoch=86
03/13/2022 18:54:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.566550 on epoch=89
03/13/2022 18:54:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.671687 on epoch=93
03/13/2022 18:54:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.675827 on epoch=96
03/13/2022 18:54:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.357587 on epoch=99
03/13/2022 18:54:56 - INFO - __main__ - Global step 300 Train loss 0.598289 Classification-F1 0.28275448110278595 on epoch=99
03/13/2022 18:55:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.322470 on epoch=103
03/13/2022 18:55:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.291828 on epoch=106
03/13/2022 18:55:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.200474 on epoch=109
03/13/2022 18:55:17 - INFO - __main__ - Step 340 Global step 340 Train loss 1.331959 on epoch=113
03/13/2022 18:55:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.470861 on epoch=116
03/13/2022 18:55:23 - INFO - __main__ - Global step 350 Train loss 0.523519 Classification-F1 0.36903562479010055 on epoch=116
03/13/2022 18:55:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.301121 on epoch=119
03/13/2022 18:55:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.270560 on epoch=123
03/13/2022 18:55:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.299105 on epoch=126
03/13/2022 18:55:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.240578 on epoch=129
03/13/2022 18:55:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.215455 on epoch=133
03/13/2022 18:55:50 - INFO - __main__ - Global step 400 Train loss 0.265364 Classification-F1 0.38803418803418804 on epoch=133
03/13/2022 18:55:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.188486 on epoch=136
03/13/2022 18:56:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.152204 on epoch=139
03/13/2022 18:56:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.148897 on epoch=143
03/13/2022 18:56:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.135594 on epoch=146
03/13/2022 18:56:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.038381 on epoch=149
03/13/2022 18:56:17 - INFO - __main__ - Global step 450 Train loss 0.132712 Classification-F1 0.37527685492801766 on epoch=149
03/13/2022 18:56:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.076943 on epoch=153
03/13/2022 18:56:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.037946 on epoch=156
03/13/2022 18:56:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.008148 on epoch=159
03/13/2022 18:56:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.025328 on epoch=163
03/13/2022 18:56:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.011159 on epoch=166
03/13/2022 18:56:42 - INFO - __main__ - Global step 500 Train loss 0.031905 Classification-F1 0.432967032967033 on epoch=166
03/13/2022 18:56:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.016616 on epoch=169
03/13/2022 18:56:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.009880 on epoch=173
03/13/2022 18:56:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.019512 on epoch=176
03/13/2022 18:57:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.057145 on epoch=179
03/13/2022 18:57:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.017429 on epoch=183
03/13/2022 18:57:09 - INFO - __main__ - Global step 550 Train loss 0.024116 Classification-F1 0.45529953917050686 on epoch=183
03/13/2022 18:57:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.008124 on epoch=186
03/13/2022 18:57:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000902 on epoch=189
03/13/2022 18:57:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001662 on epoch=193
03/13/2022 18:57:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.023010 on epoch=196
03/13/2022 18:57:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003288 on epoch=199
03/13/2022 18:57:35 - INFO - __main__ - Global step 600 Train loss 0.007397 Classification-F1 0.36724240375355993 on epoch=199
03/13/2022 18:57:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.003780 on epoch=203
03/13/2022 18:57:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.023482 on epoch=206
03/13/2022 18:57:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.004499 on epoch=209
03/13/2022 18:57:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001292 on epoch=213
03/13/2022 18:58:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000876 on epoch=216
03/13/2022 18:58:01 - INFO - __main__ - Global step 650 Train loss 0.006786 Classification-F1 0.4378066378066378 on epoch=216
03/13/2022 18:58:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.008613 on epoch=219
03/13/2022 18:58:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.007294 on epoch=223
03/13/2022 18:58:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000731 on epoch=226
03/13/2022 18:58:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.014631 on epoch=229
03/13/2022 18:58:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.101463 on epoch=233
03/13/2022 18:58:27 - INFO - __main__ - Global step 700 Train loss 0.026546 Classification-F1 0.3207070707070707 on epoch=233
03/13/2022 18:58:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.181807 on epoch=236
03/13/2022 18:58:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.002477 on epoch=239
03/13/2022 18:58:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001208 on epoch=243
03/13/2022 18:58:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.003014 on epoch=246
03/13/2022 18:58:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000144 on epoch=249
03/13/2022 18:58:52 - INFO - __main__ - Global step 750 Train loss 0.037730 Classification-F1 0.39720430107526883 on epoch=249
03/13/2022 18:58:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001927 on epoch=253
03/13/2022 18:59:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.001234 on epoch=256
03/13/2022 18:59:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000832 on epoch=259
03/13/2022 18:59:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.008417 on epoch=263
03/13/2022 18:59:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.001653 on epoch=266
03/13/2022 18:59:18 - INFO - __main__ - Global step 800 Train loss 0.002813 Classification-F1 0.35275244849712933 on epoch=266
03/13/2022 18:59:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000438 on epoch=269
03/13/2022 18:59:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001000 on epoch=273
03/13/2022 18:59:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000408 on epoch=276
03/13/2022 18:59:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000329 on epoch=279
03/13/2022 18:59:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000127 on epoch=283
03/13/2022 18:59:44 - INFO - __main__ - Global step 850 Train loss 0.000460 Classification-F1 0.36605182257356167 on epoch=283
03/13/2022 18:59:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000507 on epoch=286
03/13/2022 18:59:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000034 on epoch=289
03/13/2022 18:59:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000049 on epoch=293
03/13/2022 19:00:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000131 on epoch=296
03/13/2022 19:00:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000149 on epoch=299
03/13/2022 19:00:09 - INFO - __main__ - Global step 900 Train loss 0.000174 Classification-F1 0.3477633477633478 on epoch=299
03/13/2022 19:00:09 - INFO - __main__ - save last model!
03/13/2022 19:00:10 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:00:10 - INFO - __main__ - Printing 3 examples
03/13/2022 19:00:10 - INFO - __main__ -  [hatexplain]     germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/13/2022 19:00:10 - INFO - __main__ - ['offensive']
03/13/2022 19:00:10 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/13/2022 19:00:10 - INFO - __main__ - ['offensive']
03/13/2022 19:00:10 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/13/2022 19:00:10 - INFO - __main__ - ['offensive']
03/13/2022 19:00:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 19:00:10 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:00:10 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:00:10 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:00:10 - INFO - __main__ - Printing 3 examples
03/13/2022 19:00:10 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/13/2022 19:00:10 - INFO - __main__ - ['offensive']
03/13/2022 19:00:10 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/13/2022 19:00:10 - INFO - __main__ - ['offensive']
03/13/2022 19:00:10 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 
03/13/2022 19:00:10 - INFO - __main__ - ['offensive']
03/13/2022 19:00:10 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:00:10 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:00:10 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:00:16 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 19:00:16 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 19:00:16 - INFO - __main__ - Printing 3 examples
03/13/2022 19:00:16 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 19:00:16 - INFO - __main__ - ['normal']
03/13/2022 19:00:16 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 19:00:16 - INFO - __main__ - ['normal']
03/13/2022 19:00:16 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 19:00:16 - INFO - __main__ - ['normal']
03/13/2022 19:00:16 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:00:17 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:00:19 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 19:00:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:00:21 - INFO - __main__ - Starting training!
03/13/2022 19:00:50 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_100_0.0005_8_predictions.txt
03/13/2022 19:00:50 - INFO - __main__ - Classification-F1 on test data: 0.4668
03/13/2022 19:00:51 - INFO - __main__ - prefix=hatexplain_16_100, lr=0.0005, bsz=8, dev_performance=0.45529953917050686, test_performance=0.466827236913927
03/13/2022 19:00:51 - INFO - __main__ - Running ... prefix=hatexplain_16_100, lr=0.0003, bsz=8 ...
03/13/2022 19:00:52 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:00:52 - INFO - __main__ - Printing 3 examples
03/13/2022 19:00:52 - INFO - __main__ -  [hatexplain]     germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/13/2022 19:00:52 - INFO - __main__ - ['offensive']
03/13/2022 19:00:52 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/13/2022 19:00:52 - INFO - __main__ - ['offensive']
03/13/2022 19:00:52 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/13/2022 19:00:52 - INFO - __main__ - ['offensive']
03/13/2022 19:00:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:00:52 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:00:52 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:00:52 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:00:52 - INFO - __main__ - Printing 3 examples
03/13/2022 19:00:52 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/13/2022 19:00:52 - INFO - __main__ - ['offensive']
03/13/2022 19:00:52 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/13/2022 19:00:52 - INFO - __main__ - ['offensive']
03/13/2022 19:00:52 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 
03/13/2022 19:00:52 - INFO - __main__ - ['offensive']
03/13/2022 19:00:52 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:00:52 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:00:52 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:01:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:01:03 - INFO - __main__ - Starting training!
03/13/2022 19:01:07 - INFO - __main__ - Step 10 Global step 10 Train loss 21.087626 on epoch=3
03/13/2022 19:01:12 - INFO - __main__ - Step 20 Global step 20 Train loss 17.607637 on epoch=6
03/13/2022 19:01:17 - INFO - __main__ - Step 30 Global step 30 Train loss 12.484542 on epoch=9
03/13/2022 19:01:22 - INFO - __main__ - Step 40 Global step 40 Train loss 11.572233 on epoch=13
03/13/2022 19:01:27 - INFO - __main__ - Step 50 Global step 50 Train loss 10.192852 on epoch=16
03/13/2022 19:01:29 - INFO - __main__ - Global step 50 Train loss 14.588978 Classification-F1 0.0 on epoch=16
03/13/2022 19:01:35 - INFO - __main__ - Step 60 Global step 60 Train loss 10.069716 on epoch=19
03/13/2022 19:01:40 - INFO - __main__ - Step 70 Global step 70 Train loss 9.099693 on epoch=23
03/13/2022 19:01:45 - INFO - __main__ - Step 80 Global step 80 Train loss 7.863889 on epoch=26
03/13/2022 19:01:50 - INFO - __main__ - Step 90 Global step 90 Train loss 7.328546 on epoch=29
03/13/2022 19:01:55 - INFO - __main__ - Step 100 Global step 100 Train loss 5.041904 on epoch=33
03/13/2022 19:01:56 - INFO - __main__ - Global step 100 Train loss 7.880750 Classification-F1 0.037037037037037035 on epoch=33
03/13/2022 19:02:02 - INFO - __main__ - Step 110 Global step 110 Train loss 2.977713 on epoch=36
03/13/2022 19:02:07 - INFO - __main__ - Step 120 Global step 120 Train loss 2.359624 on epoch=39
03/13/2022 19:02:12 - INFO - __main__ - Step 130 Global step 130 Train loss 2.609776 on epoch=43
03/13/2022 19:02:17 - INFO - __main__ - Step 140 Global step 140 Train loss 3.298322 on epoch=46
03/13/2022 19:02:22 - INFO - __main__ - Step 150 Global step 150 Train loss 2.053990 on epoch=49
03/13/2022 19:02:23 - INFO - __main__ - Global step 150 Train loss 2.659885 Classification-F1 0.16666666666666666 on epoch=49
03/13/2022 19:02:28 - INFO - __main__ - Step 160 Global step 160 Train loss 2.316216 on epoch=53
03/13/2022 19:02:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.720803 on epoch=56
03/13/2022 19:02:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.505657 on epoch=59
03/13/2022 19:02:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.479609 on epoch=63
03/13/2022 19:02:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.374027 on epoch=66
03/13/2022 19:02:49 - INFO - __main__ - Global step 200 Train loss 0.879262 Classification-F1 0.16666666666666666 on epoch=66
03/13/2022 19:02:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.397463 on epoch=69
03/13/2022 19:02:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.360273 on epoch=73
03/13/2022 19:03:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.365798 on epoch=76
03/13/2022 19:03:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.298347 on epoch=79
03/13/2022 19:03:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.302950 on epoch=83
03/13/2022 19:03:14 - INFO - __main__ - Global step 250 Train loss 0.344966 Classification-F1 0.16666666666666666 on epoch=83
03/13/2022 19:03:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.337439 on epoch=86
03/13/2022 19:03:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.290719 on epoch=89
03/13/2022 19:03:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.262550 on epoch=93
03/13/2022 19:03:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.255716 on epoch=96
03/13/2022 19:03:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.172569 on epoch=99
03/13/2022 19:03:40 - INFO - __main__ - Global step 300 Train loss 0.263798 Classification-F1 0.2940516273849607 on epoch=99
03/13/2022 19:03:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.130486 on epoch=103
03/13/2022 19:03:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.114443 on epoch=106
03/13/2022 19:03:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.114905 on epoch=109
03/13/2022 19:04:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.042684 on epoch=113
03/13/2022 19:04:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.021378 on epoch=116
03/13/2022 19:04:07 - INFO - __main__ - Global step 350 Train loss 0.084779 Classification-F1 0.31685273790536944 on epoch=116
03/13/2022 19:04:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.054965 on epoch=119
03/13/2022 19:04:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.051188 on epoch=123
03/13/2022 19:04:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.020113 on epoch=126
03/13/2022 19:04:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.006505 on epoch=129
03/13/2022 19:04:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.003619 on epoch=133
03/13/2022 19:04:33 - INFO - __main__ - Global step 400 Train loss 0.027278 Classification-F1 0.3189935064935065 on epoch=133
03/13/2022 19:04:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.010908 on epoch=136
03/13/2022 19:04:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.004674 on epoch=139
03/13/2022 19:04:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.011002 on epoch=143
03/13/2022 19:04:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.006671 on epoch=146
03/13/2022 19:04:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.004576 on epoch=149
03/13/2022 19:05:00 - INFO - __main__ - Global step 450 Train loss 0.007566 Classification-F1 0.3724053724053724 on epoch=149
03/13/2022 19:05:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002842 on epoch=153
03/13/2022 19:05:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.002585 on epoch=156
03/13/2022 19:05:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001393 on epoch=159
03/13/2022 19:05:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.006317 on epoch=163
03/13/2022 19:05:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.002298 on epoch=166
03/13/2022 19:05:26 - INFO - __main__ - Global step 500 Train loss 0.003087 Classification-F1 0.3044871794871795 on epoch=166
03/13/2022 19:05:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.003567 on epoch=169
03/13/2022 19:05:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000651 on epoch=173
03/13/2022 19:05:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.033170 on epoch=176
03/13/2022 19:05:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001458 on epoch=179
03/13/2022 19:05:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000511 on epoch=183
03/13/2022 19:05:52 - INFO - __main__ - Global step 550 Train loss 0.007871 Classification-F1 0.296767355590885 on epoch=183
03/13/2022 19:05:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000416 on epoch=186
03/13/2022 19:06:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000232 on epoch=189
03/13/2022 19:06:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.008760 on epoch=193
03/13/2022 19:06:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001442 on epoch=196
03/13/2022 19:06:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000206 on epoch=199
03/13/2022 19:06:18 - INFO - __main__ - Global step 600 Train loss 0.002211 Classification-F1 0.27928913192071086 on epoch=199
03/13/2022 19:06:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.003304 on epoch=203
03/13/2022 19:06:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000416 on epoch=206
03/13/2022 19:06:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000351 on epoch=209
03/13/2022 19:06:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000238 on epoch=213
03/13/2022 19:06:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001081 on epoch=216
03/13/2022 19:06:43 - INFO - __main__ - Global step 650 Train loss 0.001078 Classification-F1 0.29238160603980784 on epoch=216
03/13/2022 19:06:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000146 on epoch=219
03/13/2022 19:06:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.022241 on epoch=223
03/13/2022 19:06:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.016560 on epoch=226
03/13/2022 19:07:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000465 on epoch=229
03/13/2022 19:07:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.001130 on epoch=233
03/13/2022 19:07:09 - INFO - __main__ - Global step 700 Train loss 0.008108 Classification-F1 0.34553418803418806 on epoch=233
03/13/2022 19:07:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000300 on epoch=236
03/13/2022 19:07:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000266 on epoch=239
03/13/2022 19:07:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000063 on epoch=243
03/13/2022 19:07:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000051 on epoch=246
03/13/2022 19:07:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000040 on epoch=249
03/13/2022 19:07:35 - INFO - __main__ - Global step 750 Train loss 0.000144 Classification-F1 0.41253561253561255 on epoch=249
03/13/2022 19:07:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000034 on epoch=253
03/13/2022 19:07:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000287 on epoch=256
03/13/2022 19:07:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000052 on epoch=259
03/13/2022 19:07:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000066 on epoch=263
03/13/2022 19:08:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000040 on epoch=266
03/13/2022 19:08:01 - INFO - __main__ - Global step 800 Train loss 0.000096 Classification-F1 0.19109133282012888 on epoch=266
03/13/2022 19:08:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000053 on epoch=269
03/13/2022 19:08:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000027 on epoch=273
03/13/2022 19:08:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000043 on epoch=276
03/13/2022 19:08:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000024 on epoch=279
03/13/2022 19:08:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000070 on epoch=283
03/13/2022 19:08:27 - INFO - __main__ - Global step 850 Train loss 0.000043 Classification-F1 0.25594405594405595 on epoch=283
03/13/2022 19:08:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000024 on epoch=286
03/13/2022 19:08:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003963 on epoch=289
03/13/2022 19:08:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.069391 on epoch=293
03/13/2022 19:08:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001612 on epoch=296
03/13/2022 19:08:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000075 on epoch=299
03/13/2022 19:08:53 - INFO - __main__ - Global step 900 Train loss 0.015013 Classification-F1 0.2789290298918027 on epoch=299
03/13/2022 19:08:53 - INFO - __main__ - save last model!
03/13/2022 19:08:53 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:08:53 - INFO - __main__ - Printing 3 examples
03/13/2022 19:08:53 - INFO - __main__ -  [hatexplain]     germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/13/2022 19:08:53 - INFO - __main__ - ['offensive']
03/13/2022 19:08:53 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/13/2022 19:08:53 - INFO - __main__ - ['offensive']
03/13/2022 19:08:53 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/13/2022 19:08:53 - INFO - __main__ - ['offensive']
03/13/2022 19:08:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 19:08:53 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:08:53 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:08:53 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:08:53 - INFO - __main__ - Printing 3 examples
03/13/2022 19:08:53 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/13/2022 19:08:53 - INFO - __main__ - ['offensive']
03/13/2022 19:08:53 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/13/2022 19:08:53 - INFO - __main__ - ['offensive']
03/13/2022 19:08:53 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 
03/13/2022 19:08:53 - INFO - __main__ - ['offensive']
03/13/2022 19:08:53 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:08:53 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:08:53 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:09:00 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 19:09:00 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 19:09:00 - INFO - __main__ - Printing 3 examples
03/13/2022 19:09:00 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 19:09:00 - INFO - __main__ - ['normal']
03/13/2022 19:09:00 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 19:09:00 - INFO - __main__ - ['normal']
03/13/2022 19:09:00 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 19:09:00 - INFO - __main__ - ['normal']
03/13/2022 19:09:00 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:09:01 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:09:03 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 19:09:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:09:04 - INFO - __main__ - Starting training!
03/13/2022 19:09:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_100_0.0003_8_predictions.txt
03/13/2022 19:09:35 - INFO - __main__ - Classification-F1 on test data: 0.1061
03/13/2022 19:09:36 - INFO - __main__ - prefix=hatexplain_16_100, lr=0.0003, bsz=8, dev_performance=0.41253561253561255, test_performance=0.1060701675419775
03/13/2022 19:09:36 - INFO - __main__ - Running ... prefix=hatexplain_16_100, lr=0.0002, bsz=8 ...
03/13/2022 19:09:37 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:09:37 - INFO - __main__ - Printing 3 examples
03/13/2022 19:09:37 - INFO - __main__ -  [hatexplain]     germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/13/2022 19:09:37 - INFO - __main__ - ['offensive']
03/13/2022 19:09:37 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/13/2022 19:09:37 - INFO - __main__ - ['offensive']
03/13/2022 19:09:37 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/13/2022 19:09:37 - INFO - __main__ - ['offensive']
03/13/2022 19:09:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:09:37 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:09:37 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:09:37 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:09:37 - INFO - __main__ - Printing 3 examples
03/13/2022 19:09:37 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/13/2022 19:09:37 - INFO - __main__ - ['offensive']
03/13/2022 19:09:37 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/13/2022 19:09:37 - INFO - __main__ - ['offensive']
03/13/2022 19:09:37 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 
03/13/2022 19:09:37 - INFO - __main__ - ['offensive']
03/13/2022 19:09:37 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:09:37 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:09:37 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:09:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:09:47 - INFO - __main__ - Starting training!
03/13/2022 19:09:52 - INFO - __main__ - Step 10 Global step 10 Train loss 21.580975 on epoch=3
03/13/2022 19:09:57 - INFO - __main__ - Step 20 Global step 20 Train loss 17.037640 on epoch=6
03/13/2022 19:10:01 - INFO - __main__ - Step 30 Global step 30 Train loss 15.392708 on epoch=9
03/13/2022 19:10:06 - INFO - __main__ - Step 40 Global step 40 Train loss 14.293363 on epoch=13
03/13/2022 19:10:11 - INFO - __main__ - Step 50 Global step 50 Train loss 12.540731 on epoch=16
03/13/2022 19:10:12 - INFO - __main__ - Global step 50 Train loss 16.169083 Classification-F1 0.0 on epoch=16
03/13/2022 19:10:18 - INFO - __main__ - Step 60 Global step 60 Train loss 11.589516 on epoch=19
03/13/2022 19:10:23 - INFO - __main__ - Step 70 Global step 70 Train loss 11.150962 on epoch=23
03/13/2022 19:10:28 - INFO - __main__ - Step 80 Global step 80 Train loss 10.570818 on epoch=26
03/13/2022 19:10:33 - INFO - __main__ - Step 90 Global step 90 Train loss 9.963995 on epoch=29
03/13/2022 19:10:38 - INFO - __main__ - Step 100 Global step 100 Train loss 9.049536 on epoch=33
03/13/2022 19:10:39 - INFO - __main__ - Global step 100 Train loss 10.464965 Classification-F1 0.0 on epoch=33
03/13/2022 19:10:44 - INFO - __main__ - Step 110 Global step 110 Train loss 8.767810 on epoch=36
03/13/2022 19:10:49 - INFO - __main__ - Step 120 Global step 120 Train loss 8.409823 on epoch=39
03/13/2022 19:10:54 - INFO - __main__ - Step 130 Global step 130 Train loss 7.768689 on epoch=43
03/13/2022 19:10:59 - INFO - __main__ - Step 140 Global step 140 Train loss 6.230284 on epoch=46
03/13/2022 19:11:04 - INFO - __main__ - Step 150 Global step 150 Train loss 6.278208 on epoch=49
03/13/2022 19:11:05 - INFO - __main__ - Global step 150 Train loss 7.490963 Classification-F1 0.0 on epoch=49
03/13/2022 19:11:10 - INFO - __main__ - Step 160 Global step 160 Train loss 5.882268 on epoch=53
03/13/2022 19:11:15 - INFO - __main__ - Step 170 Global step 170 Train loss 4.202134 on epoch=56
03/13/2022 19:11:20 - INFO - __main__ - Step 180 Global step 180 Train loss 3.512141 on epoch=59
03/13/2022 19:11:25 - INFO - __main__ - Step 190 Global step 190 Train loss 2.786873 on epoch=63
03/13/2022 19:11:30 - INFO - __main__ - Step 200 Global step 200 Train loss 2.756058 on epoch=66
03/13/2022 19:11:31 - INFO - __main__ - Global step 200 Train loss 3.827895 Classification-F1 0.14207650273224043 on epoch=66
03/13/2022 19:11:36 - INFO - __main__ - Step 210 Global step 210 Train loss 1.983998 on epoch=69
03/13/2022 19:11:41 - INFO - __main__ - Step 220 Global step 220 Train loss 2.706369 on epoch=73
03/13/2022 19:11:46 - INFO - __main__ - Step 230 Global step 230 Train loss 2.165416 on epoch=76
03/13/2022 19:11:51 - INFO - __main__ - Step 240 Global step 240 Train loss 2.166903 on epoch=79
03/13/2022 19:11:56 - INFO - __main__ - Step 250 Global step 250 Train loss 2.240813 on epoch=83
03/13/2022 19:11:57 - INFO - __main__ - Global step 250 Train loss 2.252700 Classification-F1 0.16933638443935925 on epoch=83
03/13/2022 19:12:03 - INFO - __main__ - Step 260 Global step 260 Train loss 2.757240 on epoch=86
03/13/2022 19:12:08 - INFO - __main__ - Step 270 Global step 270 Train loss 1.690930 on epoch=89
03/13/2022 19:12:13 - INFO - __main__ - Step 280 Global step 280 Train loss 1.615852 on epoch=93
03/13/2022 19:12:18 - INFO - __main__ - Step 290 Global step 290 Train loss 1.965391 on epoch=96
03/13/2022 19:12:23 - INFO - __main__ - Step 300 Global step 300 Train loss 2.253091 on epoch=99
03/13/2022 19:12:24 - INFO - __main__ - Global step 300 Train loss 2.056501 Classification-F1 0.13095238095238096 on epoch=99
03/13/2022 19:12:28 - INFO - __main__ - Step 310 Global step 310 Train loss 1.589295 on epoch=103
03/13/2022 19:12:33 - INFO - __main__ - Step 320 Global step 320 Train loss 1.921963 on epoch=106
03/13/2022 19:12:38 - INFO - __main__ - Step 330 Global step 330 Train loss 2.316967 on epoch=109
03/13/2022 19:12:43 - INFO - __main__ - Step 340 Global step 340 Train loss 1.944608 on epoch=113
03/13/2022 19:12:48 - INFO - __main__ - Step 350 Global step 350 Train loss 1.796801 on epoch=116
03/13/2022 19:12:49 - INFO - __main__ - Global step 350 Train loss 1.913927 Classification-F1 0.2647296206618241 on epoch=116
03/13/2022 19:12:55 - INFO - __main__ - Step 360 Global step 360 Train loss 1.786053 on epoch=119
03/13/2022 19:13:00 - INFO - __main__ - Step 370 Global step 370 Train loss 1.468630 on epoch=123
03/13/2022 19:13:05 - INFO - __main__ - Step 380 Global step 380 Train loss 1.489228 on epoch=126
03/13/2022 19:13:10 - INFO - __main__ - Step 390 Global step 390 Train loss 2.025909 on epoch=129
03/13/2022 19:13:15 - INFO - __main__ - Step 400 Global step 400 Train loss 1.714980 on epoch=133
03/13/2022 19:13:16 - INFO - __main__ - Global step 400 Train loss 1.696960 Classification-F1 0.22207592714505156 on epoch=133
03/13/2022 19:13:21 - INFO - __main__ - Step 410 Global step 410 Train loss 1.881180 on epoch=136
03/13/2022 19:13:26 - INFO - __main__ - Step 420 Global step 420 Train loss 1.463067 on epoch=139
03/13/2022 19:13:31 - INFO - __main__ - Step 430 Global step 430 Train loss 1.730677 on epoch=143
03/13/2022 19:13:36 - INFO - __main__ - Step 440 Global step 440 Train loss 1.207038 on epoch=146
03/13/2022 19:13:41 - INFO - __main__ - Step 450 Global step 450 Train loss 1.373866 on epoch=149
03/13/2022 19:13:42 - INFO - __main__ - Global step 450 Train loss 1.531166 Classification-F1 0.21003134796238246 on epoch=149
03/13/2022 19:13:46 - INFO - __main__ - Step 460 Global step 460 Train loss 1.320495 on epoch=153
03/13/2022 19:13:51 - INFO - __main__ - Step 470 Global step 470 Train loss 1.455413 on epoch=156
03/13/2022 19:13:56 - INFO - __main__ - Step 480 Global step 480 Train loss 1.545071 on epoch=159
03/13/2022 19:14:01 - INFO - __main__ - Step 490 Global step 490 Train loss 1.448909 on epoch=163
03/13/2022 19:14:06 - INFO - __main__ - Step 500 Global step 500 Train loss 1.258764 on epoch=166
03/13/2022 19:14:07 - INFO - __main__ - Global step 500 Train loss 1.405730 Classification-F1 0.19999999999999998 on epoch=166
03/13/2022 19:14:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.896064 on epoch=169
03/13/2022 19:14:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.854328 on epoch=173
03/13/2022 19:14:22 - INFO - __main__ - Step 530 Global step 530 Train loss 1.022005 on epoch=176
03/13/2022 19:14:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.963845 on epoch=179
03/13/2022 19:14:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.908077 on epoch=183
03/13/2022 19:14:33 - INFO - __main__ - Global step 550 Train loss 0.928864 Classification-F1 0.2754036087369421 on epoch=183
03/13/2022 19:14:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.919474 on epoch=186
03/13/2022 19:14:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.984545 on epoch=189
03/13/2022 19:14:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.864137 on epoch=193
03/13/2022 19:14:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.704219 on epoch=196
03/13/2022 19:14:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.726617 on epoch=199
03/13/2022 19:14:59 - INFO - __main__ - Global step 600 Train loss 0.839799 Classification-F1 0.2985347985347986 on epoch=199
03/13/2022 19:15:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.772529 on epoch=203
03/13/2022 19:15:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.724420 on epoch=206
03/13/2022 19:15:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.685320 on epoch=209
03/13/2022 19:15:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.621532 on epoch=213
03/13/2022 19:15:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.643163 on epoch=216
03/13/2022 19:15:26 - INFO - __main__ - Global step 650 Train loss 0.689393 Classification-F1 0.26385336743393006 on epoch=216
03/13/2022 19:15:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.508611 on epoch=219
03/13/2022 19:15:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.671170 on epoch=223
03/13/2022 19:15:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.523629 on epoch=226
03/13/2022 19:15:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.899590 on epoch=229
03/13/2022 19:15:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.794020 on epoch=233
03/13/2022 19:15:52 - INFO - __main__ - Global step 700 Train loss 0.679404 Classification-F1 0.2142857142857143 on epoch=233
03/13/2022 19:15:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.505698 on epoch=236
03/13/2022 19:16:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.470852 on epoch=239
03/13/2022 19:16:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.390252 on epoch=243
03/13/2022 19:16:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.434112 on epoch=246
03/13/2022 19:16:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.366369 on epoch=249
03/13/2022 19:16:17 - INFO - __main__ - Global step 750 Train loss 0.433457 Classification-F1 0.39900426742532 on epoch=249
03/13/2022 19:16:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.364341 on epoch=253
03/13/2022 19:16:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.347936 on epoch=256
03/13/2022 19:16:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.352064 on epoch=259
03/13/2022 19:16:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.350424 on epoch=263
03/13/2022 19:16:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.332931 on epoch=266
03/13/2022 19:16:43 - INFO - __main__ - Global step 800 Train loss 0.349539 Classification-F1 0.35573122529644263 on epoch=266
03/13/2022 19:16:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.282631 on epoch=269
03/13/2022 19:16:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.246428 on epoch=273
03/13/2022 19:16:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.274093 on epoch=276
03/13/2022 19:17:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.275643 on epoch=279
03/13/2022 19:17:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.219960 on epoch=283
03/13/2022 19:17:09 - INFO - __main__ - Global step 850 Train loss 0.259751 Classification-F1 0.24951267056530213 on epoch=283
03/13/2022 19:17:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.199913 on epoch=286
03/13/2022 19:17:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.160519 on epoch=289
03/13/2022 19:17:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.178247 on epoch=293
03/13/2022 19:17:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.130755 on epoch=296
03/13/2022 19:17:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.145962 on epoch=299
03/13/2022 19:17:34 - INFO - __main__ - Global step 900 Train loss 0.163079 Classification-F1 0.38390804597701145 on epoch=299
03/13/2022 19:17:34 - INFO - __main__ - save last model!
03/13/2022 19:17:34 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:17:34 - INFO - __main__ - Printing 3 examples
03/13/2022 19:17:34 - INFO - __main__ -  [hatexplain]     germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/13/2022 19:17:34 - INFO - __main__ - ['offensive']
03/13/2022 19:17:34 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/13/2022 19:17:34 - INFO - __main__ - ['offensive']
03/13/2022 19:17:34 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/13/2022 19:17:34 - INFO - __main__ - ['offensive']
03/13/2022 19:17:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 19:17:34 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:17:34 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:17:34 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:17:34 - INFO - __main__ - Printing 3 examples
03/13/2022 19:17:34 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/13/2022 19:17:34 - INFO - __main__ - ['offensive']
03/13/2022 19:17:34 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/13/2022 19:17:34 - INFO - __main__ - ['offensive']
03/13/2022 19:17:34 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 
03/13/2022 19:17:34 - INFO - __main__ - ['offensive']
03/13/2022 19:17:34 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:17:34 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:17:34 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:17:41 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 19:17:42 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 19:17:42 - INFO - __main__ - Printing 3 examples
03/13/2022 19:17:42 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 19:17:42 - INFO - __main__ - ['normal']
03/13/2022 19:17:42 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 19:17:42 - INFO - __main__ - ['normal']
03/13/2022 19:17:42 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 19:17:42 - INFO - __main__ - ['normal']
03/13/2022 19:17:42 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:17:43 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:17:45 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 19:17:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:17:47 - INFO - __main__ - Starting training!
03/13/2022 19:20:10 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_100_0.0002_8_predictions.txt
03/13/2022 19:20:10 - INFO - __main__ - Classification-F1 on test data: 0.0556
03/13/2022 19:20:10 - INFO - __main__ - prefix=hatexplain_16_100, lr=0.0002, bsz=8, dev_performance=0.39900426742532, test_performance=0.05564628384182439
03/13/2022 19:20:10 - INFO - __main__ - Running ... prefix=hatexplain_16_100, lr=0.0001, bsz=8 ...
03/13/2022 19:20:11 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:20:11 - INFO - __main__ - Printing 3 examples
03/13/2022 19:20:11 - INFO - __main__ -  [hatexplain]     germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/13/2022 19:20:11 - INFO - __main__ - ['offensive']
03/13/2022 19:20:11 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/13/2022 19:20:11 - INFO - __main__ - ['offensive']
03/13/2022 19:20:11 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/13/2022 19:20:11 - INFO - __main__ - ['offensive']
03/13/2022 19:20:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:20:11 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:20:11 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:20:11 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:20:11 - INFO - __main__ - Printing 3 examples
03/13/2022 19:20:11 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/13/2022 19:20:11 - INFO - __main__ - ['offensive']
03/13/2022 19:20:11 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/13/2022 19:20:11 - INFO - __main__ - ['offensive']
03/13/2022 19:20:11 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 
03/13/2022 19:20:11 - INFO - __main__ - ['offensive']
03/13/2022 19:20:11 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:20:11 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:20:11 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:20:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:20:24 - INFO - __main__ - Starting training!
03/13/2022 19:20:29 - INFO - __main__ - Step 10 Global step 10 Train loss 21.303284 on epoch=3
03/13/2022 19:20:33 - INFO - __main__ - Step 20 Global step 20 Train loss 19.009893 on epoch=6
03/13/2022 19:20:38 - INFO - __main__ - Step 30 Global step 30 Train loss 15.785113 on epoch=9
03/13/2022 19:20:43 - INFO - __main__ - Step 40 Global step 40 Train loss 13.705650 on epoch=13
03/13/2022 19:20:48 - INFO - __main__ - Step 50 Global step 50 Train loss 13.052701 on epoch=16
03/13/2022 19:21:02 - INFO - __main__ - Global step 50 Train loss 16.571327 Classification-F1 0.0 on epoch=16
03/13/2022 19:21:07 - INFO - __main__ - Step 60 Global step 60 Train loss 12.734560 on epoch=19
03/13/2022 19:21:12 - INFO - __main__ - Step 70 Global step 70 Train loss 11.994937 on epoch=23
03/13/2022 19:21:17 - INFO - __main__ - Step 80 Global step 80 Train loss 11.596296 on epoch=26
03/13/2022 19:21:22 - INFO - __main__ - Step 90 Global step 90 Train loss 11.481025 on epoch=29
03/13/2022 19:21:27 - INFO - __main__ - Step 100 Global step 100 Train loss 11.573483 on epoch=33
03/13/2022 19:21:33 - INFO - __main__ - Global step 100 Train loss 11.876060 Classification-F1 0.0 on epoch=33
03/13/2022 19:21:38 - INFO - __main__ - Step 110 Global step 110 Train loss 11.239363 on epoch=36
03/13/2022 19:21:43 - INFO - __main__ - Step 120 Global step 120 Train loss 10.235968 on epoch=39
03/13/2022 19:21:48 - INFO - __main__ - Step 130 Global step 130 Train loss 10.281214 on epoch=43
03/13/2022 19:21:53 - INFO - __main__ - Step 140 Global step 140 Train loss 10.378849 on epoch=46
03/13/2022 19:21:58 - INFO - __main__ - Step 150 Global step 150 Train loss 9.607717 on epoch=49
03/13/2022 19:22:02 - INFO - __main__ - Global step 150 Train loss 10.348621 Classification-F1 0.0 on epoch=49
03/13/2022 19:22:07 - INFO - __main__ - Step 160 Global step 160 Train loss 9.966012 on epoch=53
03/13/2022 19:22:12 - INFO - __main__ - Step 170 Global step 170 Train loss 9.660518 on epoch=56
03/13/2022 19:22:17 - INFO - __main__ - Step 180 Global step 180 Train loss 9.752003 on epoch=59
03/13/2022 19:22:22 - INFO - __main__ - Step 190 Global step 190 Train loss 9.058794 on epoch=63
03/13/2022 19:22:27 - INFO - __main__ - Step 200 Global step 200 Train loss 9.017029 on epoch=66
03/13/2022 19:22:29 - INFO - __main__ - Global step 200 Train loss 9.490871 Classification-F1 0.0 on epoch=66
03/13/2022 19:22:34 - INFO - __main__ - Step 210 Global step 210 Train loss 8.577325 on epoch=69
03/13/2022 19:22:39 - INFO - __main__ - Step 220 Global step 220 Train loss 8.691366 on epoch=73
03/13/2022 19:22:44 - INFO - __main__ - Step 230 Global step 230 Train loss 8.280038 on epoch=76
03/13/2022 19:22:49 - INFO - __main__ - Step 240 Global step 240 Train loss 7.538398 on epoch=79
03/13/2022 19:22:54 - INFO - __main__ - Step 250 Global step 250 Train loss 7.152421 on epoch=83
03/13/2022 19:22:56 - INFO - __main__ - Global step 250 Train loss 8.047910 Classification-F1 0.0 on epoch=83
03/13/2022 19:23:01 - INFO - __main__ - Step 260 Global step 260 Train loss 6.086773 on epoch=86
03/13/2022 19:23:06 - INFO - __main__ - Step 270 Global step 270 Train loss 4.986634 on epoch=89
03/13/2022 19:23:11 - INFO - __main__ - Step 280 Global step 280 Train loss 3.760651 on epoch=93
03/13/2022 19:23:15 - INFO - __main__ - Step 290 Global step 290 Train loss 2.651705 on epoch=96
03/13/2022 19:23:20 - INFO - __main__ - Step 300 Global step 300 Train loss 2.602998 on epoch=99
03/13/2022 19:23:21 - INFO - __main__ - Global step 300 Train loss 4.017752 Classification-F1 0.3308541807527609 on epoch=99
03/13/2022 19:23:27 - INFO - __main__ - Step 310 Global step 310 Train loss 3.002449 on epoch=103
03/13/2022 19:23:32 - INFO - __main__ - Step 320 Global step 320 Train loss 2.812084 on epoch=106
03/13/2022 19:23:36 - INFO - __main__ - Step 330 Global step 330 Train loss 2.564074 on epoch=109
03/13/2022 19:23:41 - INFO - __main__ - Step 340 Global step 340 Train loss 2.349197 on epoch=113
03/13/2022 19:23:46 - INFO - __main__ - Step 350 Global step 350 Train loss 1.838189 on epoch=116
03/13/2022 19:23:47 - INFO - __main__ - Global step 350 Train loss 2.513198 Classification-F1 0.3258113045347088 on epoch=116
03/13/2022 19:23:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.935588 on epoch=119
03/13/2022 19:23:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.545116 on epoch=123
03/13/2022 19:24:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.481759 on epoch=126
03/13/2022 19:24:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.418052 on epoch=129
03/13/2022 19:24:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.374961 on epoch=133
03/13/2022 19:24:12 - INFO - __main__ - Global step 400 Train loss 0.551095 Classification-F1 0.20908004778972522 on epoch=133
03/13/2022 19:24:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.398593 on epoch=136
03/13/2022 19:24:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.404331 on epoch=139
03/13/2022 19:24:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.381460 on epoch=143
03/13/2022 19:24:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.274533 on epoch=146
03/13/2022 19:24:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.296046 on epoch=149
03/13/2022 19:24:38 - INFO - __main__ - Global step 450 Train loss 0.350993 Classification-F1 0.30952380952380953 on epoch=149
03/13/2022 19:24:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.379287 on epoch=153
03/13/2022 19:24:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.279078 on epoch=156
03/13/2022 19:24:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.284736 on epoch=159
03/13/2022 19:24:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.277085 on epoch=163
03/13/2022 19:25:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.220899 on epoch=166
03/13/2022 19:25:03 - INFO - __main__ - Global step 500 Train loss 0.288217 Classification-F1 0.2621082621082621 on epoch=166
03/13/2022 19:25:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.250708 on epoch=169
03/13/2022 19:25:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.187660 on epoch=173
03/13/2022 19:25:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.233953 on epoch=176
03/13/2022 19:25:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.194911 on epoch=179
03/13/2022 19:25:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.122696 on epoch=183
03/13/2022 19:25:28 - INFO - __main__ - Global step 550 Train loss 0.197986 Classification-F1 0.4105832076420312 on epoch=183
03/13/2022 19:25:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.110675 on epoch=186
03/13/2022 19:25:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.084070 on epoch=189
03/13/2022 19:25:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.125105 on epoch=193
03/13/2022 19:25:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.093441 on epoch=196
03/13/2022 19:25:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.129486 on epoch=199
03/13/2022 19:25:54 - INFO - __main__ - Global step 600 Train loss 0.108556 Classification-F1 0.3803997421018697 on epoch=199
03/13/2022 19:25:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.104876 on epoch=203
03/13/2022 19:26:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.073472 on epoch=206
03/13/2022 19:26:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.070038 on epoch=209
03/13/2022 19:26:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.074672 on epoch=213
03/13/2022 19:26:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.027247 on epoch=216
03/13/2022 19:26:20 - INFO - __main__ - Global step 650 Train loss 0.070061 Classification-F1 0.3922323743361273 on epoch=216
03/13/2022 19:26:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.040667 on epoch=219
03/13/2022 19:26:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.022518 on epoch=223
03/13/2022 19:26:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.021521 on epoch=226
03/13/2022 19:26:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.044767 on epoch=229
03/13/2022 19:26:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.029276 on epoch=233
03/13/2022 19:26:46 - INFO - __main__ - Global step 700 Train loss 0.031750 Classification-F1 0.4076616080734132 on epoch=233
03/13/2022 19:26:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.104220 on epoch=236
03/13/2022 19:26:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.023715 on epoch=239
03/13/2022 19:27:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.017471 on epoch=243
03/13/2022 19:27:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.007807 on epoch=246
03/13/2022 19:27:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.044839 on epoch=249
03/13/2022 19:27:12 - INFO - __main__ - Global step 750 Train loss 0.039611 Classification-F1 0.34091300602928504 on epoch=249
03/13/2022 19:27:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.008826 on epoch=253
03/13/2022 19:27:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.012768 on epoch=256
03/13/2022 19:27:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.006524 on epoch=259
03/13/2022 19:27:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.004603 on epoch=263
03/13/2022 19:27:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.005784 on epoch=266
03/13/2022 19:27:37 - INFO - __main__ - Global step 800 Train loss 0.007701 Classification-F1 0.430085165379283 on epoch=266
03/13/2022 19:27:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.082783 on epoch=269
03/13/2022 19:27:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.014299 on epoch=273
03/13/2022 19:27:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.026605 on epoch=276
03/13/2022 19:27:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.003898 on epoch=279
03/13/2022 19:28:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.010836 on epoch=283
03/13/2022 19:28:04 - INFO - __main__ - Global step 850 Train loss 0.027684 Classification-F1 0.4485279290179472 on epoch=283
03/13/2022 19:28:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.008202 on epoch=286
03/13/2022 19:28:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.011212 on epoch=289
03/13/2022 19:28:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.036531 on epoch=293
03/13/2022 19:28:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.006770 on epoch=296
03/13/2022 19:28:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.002824 on epoch=299
03/13/2022 19:28:30 - INFO - __main__ - Global step 900 Train loss 0.013108 Classification-F1 0.3361111111111111 on epoch=299
03/13/2022 19:28:30 - INFO - __main__ - save last model!
03/13/2022 19:28:30 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:28:30 - INFO - __main__ - Printing 3 examples
03/13/2022 19:28:30 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/13/2022 19:28:30 - INFO - __main__ - ['hatespeech']
03/13/2022 19:28:30 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/13/2022 19:28:30 - INFO - __main__ - ['hatespeech']
03/13/2022 19:28:30 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/13/2022 19:28:30 - INFO - __main__ - ['hatespeech']
03/13/2022 19:28:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 19:28:30 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:28:30 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:28:30 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:28:30 - INFO - __main__ - Printing 3 examples
03/13/2022 19:28:30 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/13/2022 19:28:30 - INFO - __main__ - ['hatespeech']
03/13/2022 19:28:30 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/13/2022 19:28:30 - INFO - __main__ - ['hatespeech']
03/13/2022 19:28:30 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/13/2022 19:28:30 - INFO - __main__ - ['hatespeech']
03/13/2022 19:28:30 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:28:30 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:28:30 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:28:37 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 19:28:37 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 19:28:37 - INFO - __main__ - Printing 3 examples
03/13/2022 19:28:37 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 19:28:37 - INFO - __main__ - ['normal']
03/13/2022 19:28:37 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 19:28:37 - INFO - __main__ - ['normal']
03/13/2022 19:28:37 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 19:28:37 - INFO - __main__ - ['normal']
03/13/2022 19:28:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:28:38 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:28:40 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 19:28:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:28:43 - INFO - __main__ - Starting training!
03/13/2022 19:29:08 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_100_0.0001_8_predictions.txt
03/13/2022 19:29:08 - INFO - __main__ - Classification-F1 on test data: 0.1322
03/13/2022 19:29:09 - INFO - __main__ - prefix=hatexplain_16_100, lr=0.0001, bsz=8, dev_performance=0.4485279290179472, test_performance=0.13215947680672865
03/13/2022 19:29:09 - INFO - __main__ - Running ... prefix=hatexplain_16_13, lr=0.0005, bsz=8 ...
03/13/2022 19:29:10 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:29:10 - INFO - __main__ - Printing 3 examples
03/13/2022 19:29:10 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/13/2022 19:29:10 - INFO - __main__ - ['hatespeech']
03/13/2022 19:29:10 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/13/2022 19:29:10 - INFO - __main__ - ['hatespeech']
03/13/2022 19:29:10 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/13/2022 19:29:10 - INFO - __main__ - ['hatespeech']
03/13/2022 19:29:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:29:10 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:29:10 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:29:10 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:29:10 - INFO - __main__ - Printing 3 examples
03/13/2022 19:29:10 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/13/2022 19:29:10 - INFO - __main__ - ['hatespeech']
03/13/2022 19:29:10 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/13/2022 19:29:10 - INFO - __main__ - ['hatespeech']
03/13/2022 19:29:10 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/13/2022 19:29:10 - INFO - __main__ - ['hatespeech']
03/13/2022 19:29:10 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:29:10 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:29:10 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:29:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:29:23 - INFO - __main__ - Starting training!
03/13/2022 19:29:27 - INFO - __main__ - Step 10 Global step 10 Train loss 21.533550 on epoch=3
03/13/2022 19:29:32 - INFO - __main__ - Step 20 Global step 20 Train loss 17.085430 on epoch=6
03/13/2022 19:29:36 - INFO - __main__ - Step 30 Global step 30 Train loss 11.882076 on epoch=9
03/13/2022 19:29:41 - INFO - __main__ - Step 40 Global step 40 Train loss 11.219011 on epoch=13
03/13/2022 19:29:46 - INFO - __main__ - Step 50 Global step 50 Train loss 10.467864 on epoch=16
03/13/2022 19:29:58 - INFO - __main__ - Global step 50 Train loss 14.437587 Classification-F1 0.0 on epoch=16
03/13/2022 19:30:03 - INFO - __main__ - Step 60 Global step 60 Train loss 8.537171 on epoch=19
03/13/2022 19:30:08 - INFO - __main__ - Step 70 Global step 70 Train loss 7.309466 on epoch=23
03/13/2022 19:30:13 - INFO - __main__ - Step 80 Global step 80 Train loss 4.459693 on epoch=26
03/13/2022 19:30:17 - INFO - __main__ - Step 90 Global step 90 Train loss 2.856814 on epoch=29
03/13/2022 19:30:22 - INFO - __main__ - Step 100 Global step 100 Train loss 2.477633 on epoch=33
03/13/2022 19:30:23 - INFO - __main__ - Global step 100 Train loss 5.128156 Classification-F1 0.17204301075268816 on epoch=33
03/13/2022 19:30:28 - INFO - __main__ - Step 110 Global step 110 Train loss 2.358895 on epoch=36
03/13/2022 19:30:33 - INFO - __main__ - Step 120 Global step 120 Train loss 2.151570 on epoch=39
03/13/2022 19:30:38 - INFO - __main__ - Step 130 Global step 130 Train loss 1.986573 on epoch=43
03/13/2022 19:30:43 - INFO - __main__ - Step 140 Global step 140 Train loss 2.306874 on epoch=46
03/13/2022 19:30:48 - INFO - __main__ - Step 150 Global step 150 Train loss 1.888665 on epoch=49
03/13/2022 19:30:49 - INFO - __main__ - Global step 150 Train loss 2.138515 Classification-F1 0.17204301075268816 on epoch=49
03/13/2022 19:30:54 - INFO - __main__ - Step 160 Global step 160 Train loss 1.754200 on epoch=53
03/13/2022 19:30:59 - INFO - __main__ - Step 170 Global step 170 Train loss 1.642759 on epoch=56
03/13/2022 19:31:04 - INFO - __main__ - Step 180 Global step 180 Train loss 1.748823 on epoch=59
03/13/2022 19:31:09 - INFO - __main__ - Step 190 Global step 190 Train loss 1.754821 on epoch=63
03/13/2022 19:31:14 - INFO - __main__ - Step 200 Global step 200 Train loss 1.821846 on epoch=66
03/13/2022 19:31:15 - INFO - __main__ - Global step 200 Train loss 1.744490 Classification-F1 0.15256916996047432 on epoch=66
03/13/2022 19:31:20 - INFO - __main__ - Step 210 Global step 210 Train loss 1.332699 on epoch=69
03/13/2022 19:31:25 - INFO - __main__ - Step 220 Global step 220 Train loss 1.646062 on epoch=73
03/13/2022 19:31:30 - INFO - __main__ - Step 230 Global step 230 Train loss 1.255867 on epoch=76
03/13/2022 19:31:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.993802 on epoch=79
03/13/2022 19:31:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.975157 on epoch=83
03/13/2022 19:31:42 - INFO - __main__ - Global step 250 Train loss 1.240717 Classification-F1 0.1611111111111111 on epoch=83
03/13/2022 19:31:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.772277 on epoch=86
03/13/2022 19:31:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.750088 on epoch=89
03/13/2022 19:31:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.594970 on epoch=93
03/13/2022 19:32:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.615575 on epoch=96
03/13/2022 19:32:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.773262 on epoch=99
03/13/2022 19:32:07 - INFO - __main__ - Global step 300 Train loss 0.701234 Classification-F1 0.19098883572567782 on epoch=99
03/13/2022 19:32:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.775645 on epoch=103
03/13/2022 19:32:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.727747 on epoch=106
03/13/2022 19:32:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.515991 on epoch=109
03/13/2022 19:32:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.649721 on epoch=113
03/13/2022 19:32:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.570958 on epoch=116
03/13/2022 19:32:34 - INFO - __main__ - Global step 350 Train loss 0.648012 Classification-F1 0.17204301075268816 on epoch=116
03/13/2022 19:32:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.462884 on epoch=119
03/13/2022 19:32:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.560119 on epoch=123
03/13/2022 19:32:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.490316 on epoch=126
03/13/2022 19:32:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.494318 on epoch=129
03/13/2022 19:32:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.521800 on epoch=133
03/13/2022 19:33:00 - INFO - __main__ - Global step 400 Train loss 0.505887 Classification-F1 0.24611708482676223 on epoch=133
03/13/2022 19:33:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.556316 on epoch=136
03/13/2022 19:33:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.567894 on epoch=139
03/13/2022 19:33:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.430925 on epoch=143
03/13/2022 19:33:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.444518 on epoch=146
03/13/2022 19:33:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.461654 on epoch=149
03/13/2022 19:33:26 - INFO - __main__ - Global step 450 Train loss 0.492261 Classification-F1 0.16666666666666666 on epoch=149
03/13/2022 19:33:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.559643 on epoch=153
03/13/2022 19:33:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.443968 on epoch=156
03/13/2022 19:33:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.421972 on epoch=159
03/13/2022 19:33:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.446016 on epoch=163
03/13/2022 19:33:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.441594 on epoch=166
03/13/2022 19:33:52 - INFO - __main__ - Global step 500 Train loss 0.462639 Classification-F1 0.5167847503373819 on epoch=166
03/13/2022 19:33:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.454230 on epoch=169
03/13/2022 19:34:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.434294 on epoch=173
03/13/2022 19:34:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.463359 on epoch=176
03/13/2022 19:34:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.378637 on epoch=179
03/13/2022 19:34:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.394895 on epoch=183
03/13/2022 19:34:18 - INFO - __main__ - Global step 550 Train loss 0.425083 Classification-F1 0.3111111111111111 on epoch=183
03/13/2022 19:34:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.356452 on epoch=186
03/13/2022 19:34:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.367749 on epoch=189
03/13/2022 19:34:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.374724 on epoch=193
03/13/2022 19:34:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.405802 on epoch=196
03/13/2022 19:34:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.350227 on epoch=199
03/13/2022 19:34:43 - INFO - __main__ - Global step 600 Train loss 0.370991 Classification-F1 0.32668566001899335 on epoch=199
03/13/2022 19:34:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.367975 on epoch=203
03/13/2022 19:34:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.399878 on epoch=206
03/13/2022 19:34:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.360527 on epoch=209
03/13/2022 19:35:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.354313 on epoch=213
03/13/2022 19:35:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.350886 on epoch=216
03/13/2022 19:35:09 - INFO - __main__ - Global step 650 Train loss 0.366716 Classification-F1 0.4227533193570929 on epoch=216
03/13/2022 19:35:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.288411 on epoch=219
03/13/2022 19:35:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.280604 on epoch=223
03/13/2022 19:35:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.318058 on epoch=226
03/13/2022 19:35:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.252814 on epoch=229
03/13/2022 19:35:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.332323 on epoch=233
03/13/2022 19:35:35 - INFO - __main__ - Global step 700 Train loss 0.294442 Classification-F1 0.3077750874361044 on epoch=233
03/13/2022 19:35:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.240255 on epoch=236
03/13/2022 19:35:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.183650 on epoch=239
03/13/2022 19:35:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.206881 on epoch=243
03/13/2022 19:35:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.208728 on epoch=246
03/13/2022 19:36:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.386398 on epoch=249
03/13/2022 19:36:01 - INFO - __main__ - Global step 750 Train loss 0.245182 Classification-F1 0.5850439882697948 on epoch=249
03/13/2022 19:36:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.093797 on epoch=253
03/13/2022 19:36:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.071232 on epoch=256
03/13/2022 19:36:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.094592 on epoch=259
03/13/2022 19:36:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.062514 on epoch=263
03/13/2022 19:36:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.012092 on epoch=266
03/13/2022 19:36:27 - INFO - __main__ - Global step 800 Train loss 0.066845 Classification-F1 0.6676691729323307 on epoch=266
03/13/2022 19:36:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.011289 on epoch=269
03/13/2022 19:36:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.011245 on epoch=273
03/13/2022 19:36:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.004288 on epoch=276
03/13/2022 19:36:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.007151 on epoch=279
03/13/2022 19:36:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.020239 on epoch=283
03/13/2022 19:36:53 - INFO - __main__ - Global step 850 Train loss 0.010843 Classification-F1 0.6863415139277208 on epoch=283
03/13/2022 19:36:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.007751 on epoch=286
03/13/2022 19:37:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.018634 on epoch=289
03/13/2022 19:37:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.047945 on epoch=293
03/13/2022 19:37:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002290 on epoch=296
03/13/2022 19:37:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.002936 on epoch=299
03/13/2022 19:37:20 - INFO - __main__ - Global step 900 Train loss 0.015911 Classification-F1 0.669089779434607 on epoch=299
03/13/2022 19:37:20 - INFO - __main__ - save last model!
03/13/2022 19:37:20 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:37:20 - INFO - __main__ - Printing 3 examples
03/13/2022 19:37:20 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/13/2022 19:37:20 - INFO - __main__ - ['hatespeech']
03/13/2022 19:37:20 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/13/2022 19:37:20 - INFO - __main__ - ['hatespeech']
03/13/2022 19:37:20 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/13/2022 19:37:20 - INFO - __main__ - ['hatespeech']
03/13/2022 19:37:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 19:37:20 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:37:21 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:37:21 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:37:21 - INFO - __main__ - Printing 3 examples
03/13/2022 19:37:21 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/13/2022 19:37:21 - INFO - __main__ - ['hatespeech']
03/13/2022 19:37:21 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/13/2022 19:37:21 - INFO - __main__ - ['hatespeech']
03/13/2022 19:37:21 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/13/2022 19:37:21 - INFO - __main__ - ['hatespeech']
03/13/2022 19:37:21 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:37:21 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:37:21 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:37:26 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 19:37:27 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 19:37:27 - INFO - __main__ - Printing 3 examples
03/13/2022 19:37:27 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 19:37:27 - INFO - __main__ - ['normal']
03/13/2022 19:37:27 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 19:37:27 - INFO - __main__ - ['normal']
03/13/2022 19:37:27 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 19:37:27 - INFO - __main__ - ['normal']
03/13/2022 19:37:27 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:37:28 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:37:30 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 19:37:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:37:31 - INFO - __main__ - Starting training!
03/13/2022 19:38:00 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_13_0.0005_8_predictions.txt
03/13/2022 19:38:00 - INFO - __main__ - Classification-F1 on test data: 0.3951
03/13/2022 19:38:00 - INFO - __main__ - prefix=hatexplain_16_13, lr=0.0005, bsz=8, dev_performance=0.6863415139277208, test_performance=0.39514817896172727
03/13/2022 19:38:00 - INFO - __main__ - Running ... prefix=hatexplain_16_13, lr=0.0003, bsz=8 ...
03/13/2022 19:38:01 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:38:01 - INFO - __main__ - Printing 3 examples
03/13/2022 19:38:01 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/13/2022 19:38:01 - INFO - __main__ - ['hatespeech']
03/13/2022 19:38:01 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/13/2022 19:38:01 - INFO - __main__ - ['hatespeech']
03/13/2022 19:38:01 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/13/2022 19:38:01 - INFO - __main__ - ['hatespeech']
03/13/2022 19:38:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:38:01 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:38:01 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:38:01 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:38:01 - INFO - __main__ - Printing 3 examples
03/13/2022 19:38:01 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/13/2022 19:38:01 - INFO - __main__ - ['hatespeech']
03/13/2022 19:38:01 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/13/2022 19:38:01 - INFO - __main__ - ['hatespeech']
03/13/2022 19:38:01 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/13/2022 19:38:01 - INFO - __main__ - ['hatespeech']
03/13/2022 19:38:01 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:38:01 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:38:01 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:38:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:38:14 - INFO - __main__ - Starting training!
03/13/2022 19:38:18 - INFO - __main__ - Step 10 Global step 10 Train loss 21.223370 on epoch=3
03/13/2022 19:38:23 - INFO - __main__ - Step 20 Global step 20 Train loss 16.745855 on epoch=6
03/13/2022 19:38:28 - INFO - __main__ - Step 30 Global step 30 Train loss 13.492159 on epoch=9
03/13/2022 19:38:33 - INFO - __main__ - Step 40 Global step 40 Train loss 12.653894 on epoch=13
03/13/2022 19:38:38 - INFO - __main__ - Step 50 Global step 50 Train loss 10.959620 on epoch=16
03/13/2022 19:38:41 - INFO - __main__ - Global step 50 Train loss 15.014978 Classification-F1 0.0 on epoch=16
03/13/2022 19:38:46 - INFO - __main__ - Step 60 Global step 60 Train loss 10.220237 on epoch=19
03/13/2022 19:38:51 - INFO - __main__ - Step 70 Global step 70 Train loss 9.592682 on epoch=23
03/13/2022 19:38:56 - INFO - __main__ - Step 80 Global step 80 Train loss 9.232901 on epoch=26
03/13/2022 19:39:01 - INFO - __main__ - Step 90 Global step 90 Train loss 7.355954 on epoch=29
03/13/2022 19:39:06 - INFO - __main__ - Step 100 Global step 100 Train loss 6.081744 on epoch=33
03/13/2022 19:39:07 - INFO - __main__ - Global step 100 Train loss 8.496703 Classification-F1 0.007352941176470588 on epoch=33
03/13/2022 19:39:12 - INFO - __main__ - Step 110 Global step 110 Train loss 4.425601 on epoch=36
03/13/2022 19:39:17 - INFO - __main__ - Step 120 Global step 120 Train loss 3.315253 on epoch=39
03/13/2022 19:39:22 - INFO - __main__ - Step 130 Global step 130 Train loss 2.940093 on epoch=43
03/13/2022 19:39:27 - INFO - __main__ - Step 140 Global step 140 Train loss 2.469557 on epoch=46
03/13/2022 19:39:32 - INFO - __main__ - Step 150 Global step 150 Train loss 2.620544 on epoch=49
03/13/2022 19:39:33 - INFO - __main__ - Global step 150 Train loss 3.154210 Classification-F1 0.16666666666666666 on epoch=49
03/13/2022 19:39:38 - INFO - __main__ - Step 160 Global step 160 Train loss 2.633135 on epoch=53
03/13/2022 19:39:43 - INFO - __main__ - Step 170 Global step 170 Train loss 2.301742 on epoch=56
03/13/2022 19:39:48 - INFO - __main__ - Step 180 Global step 180 Train loss 1.734173 on epoch=59
03/13/2022 19:39:53 - INFO - __main__ - Step 190 Global step 190 Train loss 2.040167 on epoch=63
03/13/2022 19:39:57 - INFO - __main__ - Step 200 Global step 200 Train loss 2.150940 on epoch=66
03/13/2022 19:39:58 - INFO - __main__ - Global step 200 Train loss 2.172031 Classification-F1 0.384498480243161 on epoch=66
03/13/2022 19:40:04 - INFO - __main__ - Step 210 Global step 210 Train loss 2.173374 on epoch=69
03/13/2022 19:40:09 - INFO - __main__ - Step 220 Global step 220 Train loss 2.164958 on epoch=73
03/13/2022 19:40:14 - INFO - __main__ - Step 230 Global step 230 Train loss 1.864992 on epoch=76
03/13/2022 19:40:18 - INFO - __main__ - Step 240 Global step 240 Train loss 1.674523 on epoch=79
03/13/2022 19:40:23 - INFO - __main__ - Step 250 Global step 250 Train loss 1.931466 on epoch=83
03/13/2022 19:40:24 - INFO - __main__ - Global step 250 Train loss 1.961863 Classification-F1 0.27636363636363637 on epoch=83
03/13/2022 19:40:29 - INFO - __main__ - Step 260 Global step 260 Train loss 1.386324 on epoch=86
03/13/2022 19:40:34 - INFO - __main__ - Step 270 Global step 270 Train loss 1.466527 on epoch=89
03/13/2022 19:40:39 - INFO - __main__ - Step 280 Global step 280 Train loss 1.442030 on epoch=93
03/13/2022 19:40:44 - INFO - __main__ - Step 290 Global step 290 Train loss 1.430800 on epoch=96
03/13/2022 19:40:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.032204 on epoch=99
03/13/2022 19:40:50 - INFO - __main__ - Global step 300 Train loss 1.351577 Classification-F1 0.31746031746031744 on epoch=99
03/13/2022 19:40:55 - INFO - __main__ - Step 310 Global step 310 Train loss 1.373400 on epoch=103
03/13/2022 19:41:00 - INFO - __main__ - Step 320 Global step 320 Train loss 1.218646 on epoch=106
03/13/2022 19:41:05 - INFO - __main__ - Step 330 Global step 330 Train loss 1.039410 on epoch=109
03/13/2022 19:41:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.865904 on epoch=113
03/13/2022 19:41:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.769172 on epoch=116
03/13/2022 19:41:16 - INFO - __main__ - Global step 350 Train loss 1.053307 Classification-F1 0.3881927039821777 on epoch=116
03/13/2022 19:41:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.370734 on epoch=119
03/13/2022 19:41:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.201157 on epoch=123
03/13/2022 19:41:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.158174 on epoch=126
03/13/2022 19:41:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.180553 on epoch=129
03/13/2022 19:41:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.098649 on epoch=133
03/13/2022 19:41:43 - INFO - __main__ - Global step 400 Train loss 0.201853 Classification-F1 0.5672268907563026 on epoch=133
03/13/2022 19:41:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.057607 on epoch=136
03/13/2022 19:41:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.087145 on epoch=139
03/13/2022 19:41:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.042212 on epoch=143
03/13/2022 19:42:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.056928 on epoch=146
03/13/2022 19:42:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.025723 on epoch=149
03/13/2022 19:42:09 - INFO - __main__ - Global step 450 Train loss 0.053923 Classification-F1 0.6240079365079364 on epoch=149
03/13/2022 19:42:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.019692 on epoch=153
03/13/2022 19:42:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.019223 on epoch=156
03/13/2022 19:42:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.020736 on epoch=159
03/13/2022 19:42:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.026584 on epoch=163
03/13/2022 19:42:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.007092 on epoch=166
03/13/2022 19:42:36 - INFO - __main__ - Global step 500 Train loss 0.018665 Classification-F1 0.5834586466165415 on epoch=166
03/13/2022 19:42:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.047980 on epoch=169
03/13/2022 19:42:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.021290 on epoch=173
03/13/2022 19:42:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.036461 on epoch=176
03/13/2022 19:42:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004508 on epoch=179
03/13/2022 19:43:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002001 on epoch=183
03/13/2022 19:43:02 - INFO - __main__ - Global step 550 Train loss 0.022448 Classification-F1 0.584039408866995 on epoch=183
03/13/2022 19:43:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.004277 on epoch=186
03/13/2022 19:43:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.013162 on epoch=189
03/13/2022 19:43:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.002577 on epoch=193
03/13/2022 19:43:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001210 on epoch=196
03/13/2022 19:43:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.093028 on epoch=199
03/13/2022 19:43:28 - INFO - __main__ - Global step 600 Train loss 0.022851 Classification-F1 0.5993891684948597 on epoch=199
03/13/2022 19:43:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001763 on epoch=203
03/13/2022 19:43:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001470 on epoch=206
03/13/2022 19:43:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.006062 on epoch=209
03/13/2022 19:43:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000907 on epoch=213
03/13/2022 19:43:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001192 on epoch=216
03/13/2022 19:43:54 - INFO - __main__ - Global step 650 Train loss 0.002279 Classification-F1 0.6240805265195509 on epoch=216
03/13/2022 19:43:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.006768 on epoch=219
03/13/2022 19:44:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001741 on epoch=223
03/13/2022 19:44:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000677 on epoch=226
03/13/2022 19:44:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000542 on epoch=229
03/13/2022 19:44:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.002355 on epoch=233
03/13/2022 19:44:20 - INFO - __main__ - Global step 700 Train loss 0.002417 Classification-F1 0.5235294117647059 on epoch=233
03/13/2022 19:44:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001480 on epoch=236
03/13/2022 19:44:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.002397 on epoch=239
03/13/2022 19:44:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000476 on epoch=243
03/13/2022 19:44:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000828 on epoch=246
03/13/2022 19:44:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000288 on epoch=249
03/13/2022 19:44:45 - INFO - __main__ - Global step 750 Train loss 0.001094 Classification-F1 0.5815056824308296 on epoch=249
03/13/2022 19:44:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000198 on epoch=253
03/13/2022 19:44:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000479 on epoch=256
03/13/2022 19:45:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.009916 on epoch=259
03/13/2022 19:45:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000412 on epoch=263
03/13/2022 19:45:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.001179 on epoch=266
03/13/2022 19:45:11 - INFO - __main__ - Global step 800 Train loss 0.002437 Classification-F1 0.5573727422003284 on epoch=266
03/13/2022 19:45:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000605 on epoch=269
03/13/2022 19:45:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000146 on epoch=273
03/13/2022 19:45:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000167 on epoch=276
03/13/2022 19:45:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000548 on epoch=279
03/13/2022 19:45:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000297 on epoch=283
03/13/2022 19:45:36 - INFO - __main__ - Global step 850 Train loss 0.000352 Classification-F1 0.3416988416988417 on epoch=283
03/13/2022 19:45:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000767 on epoch=286
03/13/2022 19:45:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.029966 on epoch=289
03/13/2022 19:45:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.003930 on epoch=293
03/13/2022 19:45:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000377 on epoch=296
03/13/2022 19:46:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000364 on epoch=299
03/13/2022 19:46:01 - INFO - __main__ - Global step 900 Train loss 0.007081 Classification-F1 0.5571428571428572 on epoch=299
03/13/2022 19:46:01 - INFO - __main__ - save last model!
03/13/2022 19:46:01 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:46:01 - INFO - __main__ - Printing 3 examples
03/13/2022 19:46:01 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/13/2022 19:46:01 - INFO - __main__ - ['hatespeech']
03/13/2022 19:46:01 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/13/2022 19:46:01 - INFO - __main__ - ['hatespeech']
03/13/2022 19:46:01 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/13/2022 19:46:01 - INFO - __main__ - ['hatespeech']
03/13/2022 19:46:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 19:46:01 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:46:02 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:46:02 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:46:02 - INFO - __main__ - Printing 3 examples
03/13/2022 19:46:02 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/13/2022 19:46:02 - INFO - __main__ - ['hatespeech']
03/13/2022 19:46:02 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/13/2022 19:46:02 - INFO - __main__ - ['hatespeech']
03/13/2022 19:46:02 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/13/2022 19:46:02 - INFO - __main__ - ['hatespeech']
03/13/2022 19:46:02 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:46:02 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:46:02 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:46:08 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 19:46:08 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 19:46:08 - INFO - __main__ - Printing 3 examples
03/13/2022 19:46:08 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 19:46:08 - INFO - __main__ - ['normal']
03/13/2022 19:46:08 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 19:46:08 - INFO - __main__ - ['normal']
03/13/2022 19:46:08 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 19:46:08 - INFO - __main__ - ['normal']
03/13/2022 19:46:08 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:46:09 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:46:11 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 19:46:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:46:12 - INFO - __main__ - Starting training!
03/13/2022 19:46:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_13_0.0003_8_predictions.txt
03/13/2022 19:46:42 - INFO - __main__ - Classification-F1 on test data: 0.4836
03/13/2022 19:46:42 - INFO - __main__ - prefix=hatexplain_16_13, lr=0.0003, bsz=8, dev_performance=0.6240805265195509, test_performance=0.4835987228850395
03/13/2022 19:46:42 - INFO - __main__ - Running ... prefix=hatexplain_16_13, lr=0.0002, bsz=8 ...
03/13/2022 19:46:43 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:46:43 - INFO - __main__ - Printing 3 examples
03/13/2022 19:46:43 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/13/2022 19:46:43 - INFO - __main__ - ['hatespeech']
03/13/2022 19:46:43 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/13/2022 19:46:43 - INFO - __main__ - ['hatespeech']
03/13/2022 19:46:43 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/13/2022 19:46:43 - INFO - __main__ - ['hatespeech']
03/13/2022 19:46:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:46:43 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:46:43 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:46:43 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:46:43 - INFO - __main__ - Printing 3 examples
03/13/2022 19:46:43 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/13/2022 19:46:43 - INFO - __main__ - ['hatespeech']
03/13/2022 19:46:43 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/13/2022 19:46:43 - INFO - __main__ - ['hatespeech']
03/13/2022 19:46:43 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/13/2022 19:46:43 - INFO - __main__ - ['hatespeech']
03/13/2022 19:46:43 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:46:43 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:46:43 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:46:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:46:53 - INFO - __main__ - Starting training!
03/13/2022 19:46:57 - INFO - __main__ - Step 10 Global step 10 Train loss 22.588755 on epoch=3
03/13/2022 19:47:02 - INFO - __main__ - Step 20 Global step 20 Train loss 16.396811 on epoch=6
03/13/2022 19:47:07 - INFO - __main__ - Step 30 Global step 30 Train loss 12.892909 on epoch=9
03/13/2022 19:47:12 - INFO - __main__ - Step 40 Global step 40 Train loss 12.929541 on epoch=13
03/13/2022 19:47:17 - INFO - __main__ - Step 50 Global step 50 Train loss 12.338463 on epoch=16
03/13/2022 19:47:27 - INFO - __main__ - Global step 50 Train loss 15.429296 Classification-F1 0.0 on epoch=16
03/13/2022 19:47:33 - INFO - __main__ - Step 60 Global step 60 Train loss 11.596699 on epoch=19
03/13/2022 19:47:38 - INFO - __main__ - Step 70 Global step 70 Train loss 11.163732 on epoch=23
03/13/2022 19:47:43 - INFO - __main__ - Step 80 Global step 80 Train loss 10.130156 on epoch=26
03/13/2022 19:47:48 - INFO - __main__ - Step 90 Global step 90 Train loss 9.912639 on epoch=29
03/13/2022 19:47:53 - INFO - __main__ - Step 100 Global step 100 Train loss 9.351948 on epoch=33
03/13/2022 19:47:59 - INFO - __main__ - Global step 100 Train loss 10.431035 Classification-F1 0.0 on epoch=33
03/13/2022 19:48:04 - INFO - __main__ - Step 110 Global step 110 Train loss 9.027982 on epoch=36
03/13/2022 19:48:09 - INFO - __main__ - Step 120 Global step 120 Train loss 8.665695 on epoch=39
03/13/2022 19:48:14 - INFO - __main__ - Step 130 Global step 130 Train loss 7.450057 on epoch=43
03/13/2022 19:48:19 - INFO - __main__ - Step 140 Global step 140 Train loss 4.834466 on epoch=46
03/13/2022 19:48:24 - INFO - __main__ - Step 150 Global step 150 Train loss 4.101116 on epoch=49
03/13/2022 19:48:25 - INFO - __main__ - Global step 150 Train loss 6.815863 Classification-F1 0.10666666666666667 on epoch=49
03/13/2022 19:48:30 - INFO - __main__ - Step 160 Global step 160 Train loss 2.909971 on epoch=53
03/13/2022 19:48:35 - INFO - __main__ - Step 170 Global step 170 Train loss 2.490271 on epoch=56
03/13/2022 19:48:40 - INFO - __main__ - Step 180 Global step 180 Train loss 2.766316 on epoch=59
03/13/2022 19:48:45 - INFO - __main__ - Step 190 Global step 190 Train loss 3.248089 on epoch=63
03/13/2022 19:48:50 - INFO - __main__ - Step 200 Global step 200 Train loss 2.391539 on epoch=66
03/13/2022 19:48:51 - INFO - __main__ - Global step 200 Train loss 2.761237 Classification-F1 0.2638888888888889 on epoch=66
03/13/2022 19:48:57 - INFO - __main__ - Step 210 Global step 210 Train loss 2.893976 on epoch=69
03/13/2022 19:49:02 - INFO - __main__ - Step 220 Global step 220 Train loss 2.317166 on epoch=73
03/13/2022 19:49:07 - INFO - __main__ - Step 230 Global step 230 Train loss 2.154258 on epoch=76
03/13/2022 19:49:12 - INFO - __main__ - Step 240 Global step 240 Train loss 2.779430 on epoch=79
03/13/2022 19:49:17 - INFO - __main__ - Step 250 Global step 250 Train loss 2.155865 on epoch=83
03/13/2022 19:49:18 - INFO - __main__ - Global step 250 Train loss 2.460139 Classification-F1 0.23081882656350736 on epoch=83
03/13/2022 19:49:23 - INFO - __main__ - Step 260 Global step 260 Train loss 2.654802 on epoch=86
03/13/2022 19:49:27 - INFO - __main__ - Step 270 Global step 270 Train loss 2.154003 on epoch=89
03/13/2022 19:49:32 - INFO - __main__ - Step 280 Global step 280 Train loss 2.270875 on epoch=93
03/13/2022 19:49:37 - INFO - __main__ - Step 290 Global step 290 Train loss 1.701866 on epoch=96
03/13/2022 19:49:42 - INFO - __main__ - Step 300 Global step 300 Train loss 2.321273 on epoch=99
03/13/2022 19:49:43 - INFO - __main__ - Global step 300 Train loss 2.220564 Classification-F1 0.3571428571428572 on epoch=99
03/13/2022 19:49:49 - INFO - __main__ - Step 310 Global step 310 Train loss 1.960187 on epoch=103
03/13/2022 19:49:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.590908 on epoch=106
03/13/2022 19:49:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.380666 on epoch=109
03/13/2022 19:50:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.224248 on epoch=113
03/13/2022 19:50:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.192105 on epoch=116
03/13/2022 19:50:10 - INFO - __main__ - Global step 350 Train loss 0.669623 Classification-F1 0.47707070707070703 on epoch=116
03/13/2022 19:50:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.116787 on epoch=119
03/13/2022 19:50:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.071793 on epoch=123
03/13/2022 19:50:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.079897 on epoch=126
03/13/2022 19:50:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.072457 on epoch=129
03/13/2022 19:50:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.049764 on epoch=133
03/13/2022 19:50:36 - INFO - __main__ - Global step 400 Train loss 0.078140 Classification-F1 0.3817663817663817 on epoch=133
03/13/2022 19:50:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.031990 on epoch=136
03/13/2022 19:50:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.034851 on epoch=139
03/13/2022 19:50:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.018571 on epoch=143
03/13/2022 19:50:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.097198 on epoch=146
03/13/2022 19:51:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.027293 on epoch=149
03/13/2022 19:51:02 - INFO - __main__ - Global step 450 Train loss 0.041981 Classification-F1 0.5529914529914529 on epoch=149
03/13/2022 19:51:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.004348 on epoch=153
03/13/2022 19:51:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.003466 on epoch=156
03/13/2022 19:51:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.006921 on epoch=159
03/13/2022 19:51:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.007649 on epoch=163
03/13/2022 19:51:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.004015 on epoch=166
03/13/2022 19:51:28 - INFO - __main__ - Global step 500 Train loss 0.005280 Classification-F1 0.5028984518856987 on epoch=166
03/13/2022 19:51:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.012281 on epoch=169
03/13/2022 19:51:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.016367 on epoch=173
03/13/2022 19:51:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001562 on epoch=176
03/13/2022 19:51:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001041 on epoch=179
03/13/2022 19:51:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.003479 on epoch=183
03/13/2022 19:51:54 - INFO - __main__ - Global step 550 Train loss 0.006946 Classification-F1 0.5014992503748126 on epoch=183
03/13/2022 19:51:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.012734 on epoch=186
03/13/2022 19:52:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.017753 on epoch=189
03/13/2022 19:52:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001825 on epoch=193
03/13/2022 19:52:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001578 on epoch=196
03/13/2022 19:52:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000883 on epoch=199
03/13/2022 19:52:20 - INFO - __main__ - Global step 600 Train loss 0.006955 Classification-F1 0.5429292929292929 on epoch=199
03/13/2022 19:52:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.008024 on epoch=203
03/13/2022 19:52:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001866 on epoch=206
03/13/2022 19:52:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000881 on epoch=209
03/13/2022 19:52:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.002084 on epoch=213
03/13/2022 19:52:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001664 on epoch=216
03/13/2022 19:52:46 - INFO - __main__ - Global step 650 Train loss 0.002904 Classification-F1 0.5301068376068376 on epoch=216
03/13/2022 19:52:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001338 on epoch=219
03/13/2022 19:52:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.006616 on epoch=223
03/13/2022 19:53:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.046307 on epoch=226
03/13/2022 19:53:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.568359 on epoch=229
03/13/2022 19:53:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.222584 on epoch=233
03/13/2022 19:53:12 - INFO - __main__ - Global step 700 Train loss 0.169041 Classification-F1 0.5075837048693067 on epoch=233
03/13/2022 19:53:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.005675 on epoch=236
03/13/2022 19:53:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.567513 on epoch=239
03/13/2022 19:53:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.754708 on epoch=243
03/13/2022 19:53:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.516538 on epoch=246
03/13/2022 19:53:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.284134 on epoch=249
03/13/2022 19:53:38 - INFO - __main__ - Global step 750 Train loss 0.425714 Classification-F1 0.3239289446185998 on epoch=249
03/13/2022 19:53:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.267523 on epoch=253
03/13/2022 19:53:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.270276 on epoch=256
03/13/2022 19:53:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.301058 on epoch=259
03/13/2022 19:53:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.189190 on epoch=263
03/13/2022 19:54:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.115746 on epoch=266
03/13/2022 19:54:04 - INFO - __main__ - Global step 800 Train loss 0.228759 Classification-F1 0.5084347309411248 on epoch=266
03/13/2022 19:54:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.061365 on epoch=269
03/13/2022 19:54:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.112235 on epoch=273
03/13/2022 19:54:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.160747 on epoch=276
03/13/2022 19:54:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.209122 on epoch=279
03/13/2022 19:54:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.153677 on epoch=283
03/13/2022 19:54:30 - INFO - __main__ - Global step 850 Train loss 0.139429 Classification-F1 0.41668033907574514 on epoch=283
03/13/2022 19:54:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.069704 on epoch=286
03/13/2022 19:54:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.113932 on epoch=289
03/13/2022 19:54:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.150217 on epoch=293
03/13/2022 19:54:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.046844 on epoch=296
03/13/2022 19:54:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.060115 on epoch=299
03/13/2022 19:54:56 - INFO - __main__ - Global step 900 Train loss 0.088162 Classification-F1 0.5553246753246753 on epoch=299
03/13/2022 19:54:56 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:54:56 - INFO - __main__ - Printing 3 examples
03/13/2022 19:54:56 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/13/2022 19:54:56 - INFO - __main__ - ['hatespeech']
03/13/2022 19:54:56 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/13/2022 19:54:56 - INFO - __main__ - ['hatespeech']
03/13/2022 19:54:56 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/13/2022 19:54:56 - INFO - __main__ - ['hatespeech']
03/13/2022 19:54:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 19:54:56 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:54:56 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:54:56 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:54:56 - INFO - __main__ - Printing 3 examples
03/13/2022 19:54:56 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/13/2022 19:54:56 - INFO - __main__ - ['hatespeech']
03/13/2022 19:54:56 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/13/2022 19:54:56 - INFO - __main__ - ['hatespeech']
03/13/2022 19:54:56 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/13/2022 19:54:56 - INFO - __main__ - ['hatespeech']
03/13/2022 19:54:56 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:54:56 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:54:56 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:54:57 - INFO - __main__ - save last model!
03/13/2022 19:55:03 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 19:55:04 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 19:55:04 - INFO - __main__ - Printing 3 examples
03/13/2022 19:55:04 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 19:55:04 - INFO - __main__ - ['normal']
03/13/2022 19:55:04 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 19:55:04 - INFO - __main__ - ['normal']
03/13/2022 19:55:04 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 19:55:04 - INFO - __main__ - ['normal']
03/13/2022 19:55:04 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:55:05 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:55:07 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 19:55:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:55:09 - INFO - __main__ - Starting training!
03/13/2022 19:55:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_13_0.0002_8_predictions.txt
03/13/2022 19:55:38 - INFO - __main__ - Classification-F1 on test data: 0.3505
03/13/2022 19:55:38 - INFO - __main__ - prefix=hatexplain_16_13, lr=0.0002, bsz=8, dev_performance=0.5553246753246753, test_performance=0.35047131875937165
03/13/2022 19:55:38 - INFO - __main__ - Running ... prefix=hatexplain_16_13, lr=0.0001, bsz=8 ...
03/13/2022 19:55:39 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:55:39 - INFO - __main__ - Printing 3 examples
03/13/2022 19:55:39 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/13/2022 19:55:39 - INFO - __main__ - ['hatespeech']
03/13/2022 19:55:39 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/13/2022 19:55:39 - INFO - __main__ - ['hatespeech']
03/13/2022 19:55:39 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/13/2022 19:55:39 - INFO - __main__ - ['hatespeech']
03/13/2022 19:55:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 19:55:39 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:55:39 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 19:55:39 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 19:55:39 - INFO - __main__ - Printing 3 examples
03/13/2022 19:55:39 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/13/2022 19:55:39 - INFO - __main__ - ['hatespeech']
03/13/2022 19:55:39 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/13/2022 19:55:39 - INFO - __main__ - ['hatespeech']
03/13/2022 19:55:39 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/13/2022 19:55:39 - INFO - __main__ - ['hatespeech']
03/13/2022 19:55:39 - INFO - __main__ - Tokenizing Input ...
03/13/2022 19:55:39 - INFO - __main__ - Tokenizing Output ...
03/13/2022 19:55:39 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 19:55:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 19:55:50 - INFO - __main__ - Starting training!
03/13/2022 19:55:55 - INFO - __main__ - Step 10 Global step 10 Train loss 22.550255 on epoch=3
03/13/2022 19:55:59 - INFO - __main__ - Step 20 Global step 20 Train loss 20.055050 on epoch=6
03/13/2022 19:56:04 - INFO - __main__ - Step 30 Global step 30 Train loss 17.954037 on epoch=9
03/13/2022 19:56:09 - INFO - __main__ - Step 40 Global step 40 Train loss 15.931355 on epoch=13
03/13/2022 19:56:14 - INFO - __main__ - Step 50 Global step 50 Train loss 13.849345 on epoch=16
03/13/2022 19:56:33 - INFO - __main__ - Global step 50 Train loss 18.068007 Classification-F1 0.0 on epoch=16
03/13/2022 19:56:39 - INFO - __main__ - Step 60 Global step 60 Train loss 12.873093 on epoch=19
03/13/2022 19:56:44 - INFO - __main__ - Step 70 Global step 70 Train loss 12.328465 on epoch=23
03/13/2022 19:56:49 - INFO - __main__ - Step 80 Global step 80 Train loss 12.673904 on epoch=26
03/13/2022 19:56:54 - INFO - __main__ - Step 90 Global step 90 Train loss 11.916022 on epoch=29
03/13/2022 19:56:59 - INFO - __main__ - Step 100 Global step 100 Train loss 12.095675 on epoch=33
03/13/2022 19:57:04 - INFO - __main__ - Global step 100 Train loss 12.377432 Classification-F1 0.0 on epoch=33
03/13/2022 19:57:09 - INFO - __main__ - Step 110 Global step 110 Train loss 11.559312 on epoch=36
03/13/2022 19:57:14 - INFO - __main__ - Step 120 Global step 120 Train loss 11.122303 on epoch=39
03/13/2022 19:57:19 - INFO - __main__ - Step 130 Global step 130 Train loss 10.900710 on epoch=43
03/13/2022 19:57:24 - INFO - __main__ - Step 140 Global step 140 Train loss 10.968815 on epoch=46
03/13/2022 19:57:29 - INFO - __main__ - Step 150 Global step 150 Train loss 10.174188 on epoch=49
03/13/2022 19:57:32 - INFO - __main__ - Global step 150 Train loss 10.945065 Classification-F1 0.0 on epoch=49
03/13/2022 19:57:37 - INFO - __main__ - Step 160 Global step 160 Train loss 9.937740 on epoch=53
03/13/2022 19:57:42 - INFO - __main__ - Step 170 Global step 170 Train loss 9.588833 on epoch=56
03/13/2022 19:57:47 - INFO - __main__ - Step 180 Global step 180 Train loss 9.638487 on epoch=59
03/13/2022 19:57:52 - INFO - __main__ - Step 190 Global step 190 Train loss 9.081449 on epoch=63
03/13/2022 19:57:57 - INFO - __main__ - Step 200 Global step 200 Train loss 9.111473 on epoch=66
03/13/2022 19:58:00 - INFO - __main__ - Global step 200 Train loss 9.471598 Classification-F1 0.0 on epoch=66
03/13/2022 19:58:05 - INFO - __main__ - Step 210 Global step 210 Train loss 8.725302 on epoch=69
03/13/2022 19:58:10 - INFO - __main__ - Step 220 Global step 220 Train loss 8.100193 on epoch=73
03/13/2022 19:58:15 - INFO - __main__ - Step 230 Global step 230 Train loss 7.876835 on epoch=76
03/13/2022 19:58:20 - INFO - __main__ - Step 240 Global step 240 Train loss 7.079817 on epoch=79
03/13/2022 19:58:25 - INFO - __main__ - Step 250 Global step 250 Train loss 7.153145 on epoch=83
03/13/2022 19:58:27 - INFO - __main__ - Global step 250 Train loss 7.787058 Classification-F1 0.0 on epoch=83
03/13/2022 19:58:32 - INFO - __main__ - Step 260 Global step 260 Train loss 6.571101 on epoch=86
03/13/2022 19:58:37 - INFO - __main__ - Step 270 Global step 270 Train loss 5.369191 on epoch=89
03/13/2022 19:58:42 - INFO - __main__ - Step 280 Global step 280 Train loss 5.455820 on epoch=93
03/13/2022 19:58:47 - INFO - __main__ - Step 290 Global step 290 Train loss 3.784561 on epoch=96
03/13/2022 19:58:52 - INFO - __main__ - Step 300 Global step 300 Train loss 3.067622 on epoch=99
03/13/2022 19:58:53 - INFO - __main__ - Global step 300 Train loss 4.849659 Classification-F1 0.14060606060606062 on epoch=99
03/13/2022 19:58:59 - INFO - __main__ - Step 310 Global step 310 Train loss 3.202832 on epoch=103
03/13/2022 19:59:04 - INFO - __main__ - Step 320 Global step 320 Train loss 3.123855 on epoch=106
03/13/2022 19:59:09 - INFO - __main__ - Step 330 Global step 330 Train loss 2.500751 on epoch=109
03/13/2022 19:59:14 - INFO - __main__ - Step 340 Global step 340 Train loss 2.572582 on epoch=113
03/13/2022 19:59:19 - INFO - __main__ - Step 350 Global step 350 Train loss 2.497999 on epoch=116
03/13/2022 19:59:20 - INFO - __main__ - Global step 350 Train loss 2.779604 Classification-F1 0.21253699219800914 on epoch=116
03/13/2022 19:59:26 - INFO - __main__ - Step 360 Global step 360 Train loss 2.387589 on epoch=119
03/13/2022 19:59:31 - INFO - __main__ - Step 370 Global step 370 Train loss 3.233325 on epoch=123
03/13/2022 19:59:36 - INFO - __main__ - Step 380 Global step 380 Train loss 2.777408 on epoch=126
03/13/2022 19:59:41 - INFO - __main__ - Step 390 Global step 390 Train loss 2.587664 on epoch=129
03/13/2022 19:59:46 - INFO - __main__ - Step 400 Global step 400 Train loss 2.575723 on epoch=133
03/13/2022 19:59:47 - INFO - __main__ - Global step 400 Train loss 2.712342 Classification-F1 0.19272819730485635 on epoch=133
03/13/2022 19:59:52 - INFO - __main__ - Step 410 Global step 410 Train loss 2.748005 on epoch=136
03/13/2022 19:59:57 - INFO - __main__ - Step 420 Global step 420 Train loss 2.603519 on epoch=139
03/13/2022 20:00:02 - INFO - __main__ - Step 430 Global step 430 Train loss 2.962955 on epoch=143
03/13/2022 20:00:07 - INFO - __main__ - Step 440 Global step 440 Train loss 2.187032 on epoch=146
03/13/2022 20:00:12 - INFO - __main__ - Step 450 Global step 450 Train loss 2.266629 on epoch=149
03/13/2022 20:00:13 - INFO - __main__ - Global step 450 Train loss 2.553628 Classification-F1 0.1693121693121693 on epoch=149
03/13/2022 20:00:17 - INFO - __main__ - Step 460 Global step 460 Train loss 2.572051 on epoch=153
03/13/2022 20:00:22 - INFO - __main__ - Step 470 Global step 470 Train loss 2.085147 on epoch=156
03/13/2022 20:00:27 - INFO - __main__ - Step 480 Global step 480 Train loss 2.463162 on epoch=159
03/13/2022 20:00:32 - INFO - __main__ - Step 490 Global step 490 Train loss 1.978836 on epoch=163
03/13/2022 20:00:37 - INFO - __main__ - Step 500 Global step 500 Train loss 2.270683 on epoch=166
03/13/2022 20:00:38 - INFO - __main__ - Global step 500 Train loss 2.273976 Classification-F1 0.18898648648648647 on epoch=166
03/13/2022 20:00:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.877215 on epoch=169
03/13/2022 20:00:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.484316 on epoch=173
03/13/2022 20:00:53 - INFO - __main__ - Step 530 Global step 530 Train loss 1.371436 on epoch=176
03/13/2022 20:00:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.722756 on epoch=179
03/13/2022 20:01:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.795740 on epoch=183
03/13/2022 20:01:04 - INFO - __main__ - Global step 550 Train loss 0.850293 Classification-F1 0.23757575757575758 on epoch=183
03/13/2022 20:01:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.394427 on epoch=186
03/13/2022 20:01:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.215928 on epoch=189
03/13/2022 20:01:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.262933 on epoch=193
03/13/2022 20:01:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.221026 on epoch=196
03/13/2022 20:01:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.127858 on epoch=199
03/13/2022 20:01:30 - INFO - __main__ - Global step 600 Train loss 0.244434 Classification-F1 0.3148148148148148 on epoch=199
03/13/2022 20:01:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.168714 on epoch=203
03/13/2022 20:01:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.085302 on epoch=206
03/13/2022 20:01:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.099802 on epoch=209
03/13/2022 20:01:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.095874 on epoch=213
03/13/2022 20:01:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.062449 on epoch=216
03/13/2022 20:01:56 - INFO - __main__ - Global step 650 Train loss 0.102428 Classification-F1 0.32580709051297285 on epoch=216
03/13/2022 20:02:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.072629 on epoch=219
03/13/2022 20:02:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.033115 on epoch=223
03/13/2022 20:02:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.022160 on epoch=226
03/13/2022 20:02:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.028878 on epoch=229
03/13/2022 20:02:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.020167 on epoch=233
03/13/2022 20:02:22 - INFO - __main__ - Global step 700 Train loss 0.035390 Classification-F1 0.34493177387914226 on epoch=233
03/13/2022 20:02:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.018280 on epoch=236
03/13/2022 20:02:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.020197 on epoch=239
03/13/2022 20:02:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.014037 on epoch=243
03/13/2022 20:02:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.043051 on epoch=246
03/13/2022 20:02:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.015326 on epoch=249
03/13/2022 20:02:48 - INFO - __main__ - Global step 750 Train loss 0.022178 Classification-F1 0.28619528619528617 on epoch=249
03/13/2022 20:02:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.006090 on epoch=253
03/13/2022 20:02:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.014186 on epoch=256
03/13/2022 20:03:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.005085 on epoch=259
03/13/2022 20:03:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.008542 on epoch=263
03/13/2022 20:03:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.130937 on epoch=266
03/13/2022 20:03:14 - INFO - __main__ - Global step 800 Train loss 0.032968 Classification-F1 0.3246031746031746 on epoch=266
03/13/2022 20:03:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.016586 on epoch=269
03/13/2022 20:03:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.006639 on epoch=273
03/13/2022 20:03:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.010227 on epoch=276
03/13/2022 20:03:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.002846 on epoch=279
03/13/2022 20:03:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.003019 on epoch=283
03/13/2022 20:03:40 - INFO - __main__ - Global step 850 Train loss 0.007863 Classification-F1 0.3226298565488053 on epoch=283
03/13/2022 20:03:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.004409 on epoch=286
03/13/2022 20:03:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003637 on epoch=289
03/13/2022 20:03:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.006601 on epoch=293
03/13/2022 20:04:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.006375 on epoch=296
03/13/2022 20:04:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.004372 on epoch=299
03/13/2022 20:04:05 - INFO - __main__ - Global step 900 Train loss 0.005079 Classification-F1 0.3181818181818182 on epoch=299
03/13/2022 20:04:05 - INFO - __main__ - save last model!
03/13/2022 20:04:06 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:04:06 - INFO - __main__ - Printing 3 examples
03/13/2022 20:04:06 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass  
03/13/2022 20:04:06 - INFO - __main__ - ['offensive']
03/13/2022 20:04:06 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/13/2022 20:04:06 - INFO - __main__ - ['offensive']
03/13/2022 20:04:06 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/13/2022 20:04:06 - INFO - __main__ - ['offensive']
03/13/2022 20:04:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 20:04:06 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:04:06 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 20:04:06 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:04:06 - INFO - __main__ - Printing 3 examples
03/13/2022 20:04:06 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/13/2022 20:04:06 - INFO - __main__ - ['offensive']
03/13/2022 20:04:06 - INFO - __main__ -  [hatexplain] <user> it  genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/13/2022 20:04:06 - INFO - __main__ - ['offensive']
03/13/2022 20:04:06 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/13/2022 20:04:06 - INFO - __main__ - ['offensive']
03/13/2022 20:04:06 - INFO - __main__ - Tokenizing Input ...
03/13/2022 20:04:06 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:04:06 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 20:04:13 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 20:04:14 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 20:04:14 - INFO - __main__ - Printing 3 examples
03/13/2022 20:04:14 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 20:04:14 - INFO - __main__ - ['normal']
03/13/2022 20:04:14 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 20:04:14 - INFO - __main__ - ['normal']
03/13/2022 20:04:14 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 20:04:14 - INFO - __main__ - ['normal']
03/13/2022 20:04:14 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 20:04:14 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:04:16 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 20:04:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 20:04:19 - INFO - __main__ - Starting training!
03/13/2022 20:04:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_13_0.0001_8_predictions.txt
03/13/2022 20:04:44 - INFO - __main__ - Classification-F1 on test data: 0.1531
03/13/2022 20:04:44 - INFO - __main__ - prefix=hatexplain_16_13, lr=0.0001, bsz=8, dev_performance=0.34493177387914226, test_performance=0.15313159047025057
03/13/2022 20:04:44 - INFO - __main__ - Running ... prefix=hatexplain_16_21, lr=0.0005, bsz=8 ...
03/13/2022 20:04:45 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:04:45 - INFO - __main__ - Printing 3 examples
03/13/2022 20:04:45 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass  
03/13/2022 20:04:45 - INFO - __main__ - ['offensive']
03/13/2022 20:04:45 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/13/2022 20:04:45 - INFO - __main__ - ['offensive']
03/13/2022 20:04:45 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/13/2022 20:04:45 - INFO - __main__ - ['offensive']
03/13/2022 20:04:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 20:04:45 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:04:45 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 20:04:45 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:04:45 - INFO - __main__ - Printing 3 examples
03/13/2022 20:04:45 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/13/2022 20:04:45 - INFO - __main__ - ['offensive']
03/13/2022 20:04:45 - INFO - __main__ -  [hatexplain] <user> it  genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/13/2022 20:04:45 - INFO - __main__ - ['offensive']
03/13/2022 20:04:45 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/13/2022 20:04:45 - INFO - __main__ - ['offensive']
03/13/2022 20:04:45 - INFO - __main__ - Tokenizing Input ...
03/13/2022 20:04:45 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:04:45 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 20:04:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 20:04:59 - INFO - __main__ - Starting training!
03/13/2022 20:05:03 - INFO - __main__ - Step 10 Global step 10 Train loss 21.693624 on epoch=3
03/13/2022 20:05:07 - INFO - __main__ - Step 20 Global step 20 Train loss 14.649393 on epoch=6
03/13/2022 20:05:12 - INFO - __main__ - Step 30 Global step 30 Train loss 11.686747 on epoch=9
03/13/2022 20:05:17 - INFO - __main__ - Step 40 Global step 40 Train loss 10.158603 on epoch=13
03/13/2022 20:05:22 - INFO - __main__ - Step 50 Global step 50 Train loss 8.923606 on epoch=16
03/13/2022 20:05:24 - INFO - __main__ - Global step 50 Train loss 13.422395 Classification-F1 0.0 on epoch=16
03/13/2022 20:05:30 - INFO - __main__ - Step 60 Global step 60 Train loss 7.110892 on epoch=19
03/13/2022 20:05:35 - INFO - __main__ - Step 70 Global step 70 Train loss 3.512460 on epoch=23
03/13/2022 20:05:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.753728 on epoch=26
03/13/2022 20:05:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.492545 on epoch=29
03/13/2022 20:05:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.448347 on epoch=33
03/13/2022 20:05:50 - INFO - __main__ - Global step 100 Train loss 2.463595 Classification-F1 0.18451300665456746 on epoch=33
03/13/2022 20:05:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.295019 on epoch=36
03/13/2022 20:06:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.223064 on epoch=39
03/13/2022 20:06:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.201373 on epoch=43
03/13/2022 20:06:11 - INFO - __main__ - Step 140 Global step 140 Train loss 2.246935 on epoch=46
03/13/2022 20:06:16 - INFO - __main__ - Step 150 Global step 150 Train loss 1.146634 on epoch=49
03/13/2022 20:06:16 - INFO - __main__ - Global step 150 Train loss 0.822605 Classification-F1 0.2341269841269841 on epoch=49
03/13/2022 20:06:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.286262 on epoch=53
03/13/2022 20:06:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.201733 on epoch=56
03/13/2022 20:06:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.138381 on epoch=59
03/13/2022 20:06:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.082748 on epoch=63
03/13/2022 20:06:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.063242 on epoch=66
03/13/2022 20:06:42 - INFO - __main__ - Global step 200 Train loss 0.154473 Classification-F1 0.4246246246246246 on epoch=66
03/13/2022 20:06:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.040675 on epoch=69
03/13/2022 20:06:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.051080 on epoch=73
03/13/2022 20:06:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.028640 on epoch=76
03/13/2022 20:07:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.041214 on epoch=79
03/13/2022 20:07:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.099308 on epoch=83
03/13/2022 20:07:08 - INFO - __main__ - Global step 250 Train loss 0.052183 Classification-F1 0.4193899782135076 on epoch=83
03/13/2022 20:07:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.023472 on epoch=86
03/13/2022 20:07:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.080617 on epoch=89
03/13/2022 20:07:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.711331 on epoch=93
03/13/2022 20:07:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.362099 on epoch=96
03/13/2022 20:07:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.264317 on epoch=99
03/13/2022 20:07:34 - INFO - __main__ - Global step 300 Train loss 0.288367 Classification-F1 0.2875457875457876 on epoch=99
03/13/2022 20:07:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.312645 on epoch=103
03/13/2022 20:07:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.118179 on epoch=106
03/13/2022 20:07:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.067010 on epoch=109
03/13/2022 20:07:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.053410 on epoch=113
03/13/2022 20:07:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.079166 on epoch=116
03/13/2022 20:07:59 - INFO - __main__ - Global step 350 Train loss 0.126082 Classification-F1 0.40502586844050265 on epoch=116
03/13/2022 20:08:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.094086 on epoch=119
03/13/2022 20:08:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.015852 on epoch=123
03/13/2022 20:08:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.013336 on epoch=126
03/13/2022 20:08:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.049784 on epoch=129
03/13/2022 20:08:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.040596 on epoch=133
03/13/2022 20:08:32 - INFO - __main__ - Global step 400 Train loss 0.042731 Classification-F1 0.3204684035476718 on epoch=133
03/13/2022 20:08:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.096562 on epoch=136
03/13/2022 20:08:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.008389 on epoch=139
03/13/2022 20:08:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.007708 on epoch=143
03/13/2022 20:08:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.148994 on epoch=146
03/13/2022 20:08:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.482862 on epoch=149
03/13/2022 20:08:58 - INFO - __main__ - Global step 450 Train loss 0.148903 Classification-F1 0.47565681444991786 on epoch=149
03/13/2022 20:09:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.019591 on epoch=153
03/13/2022 20:09:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.015516 on epoch=156
03/13/2022 20:09:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.010866 on epoch=159
03/13/2022 20:09:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.005445 on epoch=163
03/13/2022 20:09:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.005869 on epoch=166
03/13/2022 20:09:24 - INFO - __main__ - Global step 500 Train loss 0.011457 Classification-F1 0.4781444403246405 on epoch=166
03/13/2022 20:09:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.004145 on epoch=169
03/13/2022 20:09:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000708 on epoch=173
03/13/2022 20:09:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.006964 on epoch=176
03/13/2022 20:09:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.012873 on epoch=179
03/13/2022 20:09:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.005248 on epoch=183
03/13/2022 20:09:57 - INFO - __main__ - Global step 550 Train loss 0.005988 Classification-F1 0.37564778216952127 on epoch=183
03/13/2022 20:10:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001393 on epoch=186
03/13/2022 20:10:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001868 on epoch=189
03/13/2022 20:10:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.020359 on epoch=193
03/13/2022 20:10:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.013777 on epoch=196
03/13/2022 20:10:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001371 on epoch=199
03/13/2022 20:10:22 - INFO - __main__ - Global step 600 Train loss 0.007754 Classification-F1 0.4915966386554622 on epoch=199
03/13/2022 20:10:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000929 on epoch=203
03/13/2022 20:10:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.008113 on epoch=206
03/13/2022 20:10:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.004104 on epoch=209
03/13/2022 20:10:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.042894 on epoch=213
03/13/2022 20:10:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.012537 on epoch=216
03/13/2022 20:10:48 - INFO - __main__ - Global step 650 Train loss 0.013716 Classification-F1 0.417004048582996 on epoch=216
03/13/2022 20:10:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.005924 on epoch=219
03/13/2022 20:10:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001599 on epoch=223
03/13/2022 20:11:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.003320 on epoch=226
03/13/2022 20:11:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.002687 on epoch=229
03/13/2022 20:11:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.032788 on epoch=233
03/13/2022 20:11:14 - INFO - __main__ - Global step 700 Train loss 0.009264 Classification-F1 0.5 on epoch=233
03/13/2022 20:11:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000854 on epoch=236
03/13/2022 20:11:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000353 on epoch=239
03/13/2022 20:11:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.036622 on epoch=243
03/13/2022 20:11:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.006801 on epoch=246
03/13/2022 20:11:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.002683 on epoch=249
03/13/2022 20:11:40 - INFO - __main__ - Global step 750 Train loss 0.009463 Classification-F1 0.4228675136116153 on epoch=249
03/13/2022 20:11:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.005558 on epoch=253
03/13/2022 20:11:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.064055 on epoch=256
03/13/2022 20:11:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001567 on epoch=259
03/13/2022 20:12:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000834 on epoch=263
03/13/2022 20:12:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.003580 on epoch=266
03/13/2022 20:12:06 - INFO - __main__ - Global step 800 Train loss 0.015119 Classification-F1 0.4131364997860505 on epoch=266
03/13/2022 20:12:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.004147 on epoch=269
03/13/2022 20:12:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.003195 on epoch=273
03/13/2022 20:12:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.004558 on epoch=276
03/13/2022 20:12:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001298 on epoch=279
03/13/2022 20:12:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000551 on epoch=283
03/13/2022 20:12:31 - INFO - __main__ - Global step 850 Train loss 0.002750 Classification-F1 0.41125541125541126 on epoch=283
03/13/2022 20:12:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000435 on epoch=286
03/13/2022 20:12:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000108 on epoch=289
03/13/2022 20:12:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000771 on epoch=293
03/13/2022 20:12:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000048 on epoch=296
03/13/2022 20:12:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001579 on epoch=299
03/13/2022 20:12:57 - INFO - __main__ - Global step 900 Train loss 0.000588 Classification-F1 0.5309941520467837 on epoch=299
03/13/2022 20:12:57 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:12:57 - INFO - __main__ - Printing 3 examples
03/13/2022 20:12:57 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass  
03/13/2022 20:12:57 - INFO - __main__ - ['offensive']
03/13/2022 20:12:57 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/13/2022 20:12:57 - INFO - __main__ - ['offensive']
03/13/2022 20:12:57 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/13/2022 20:12:57 - INFO - __main__ - ['offensive']
03/13/2022 20:12:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 20:12:57 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:12:57 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 20:12:57 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:12:57 - INFO - __main__ - Printing 3 examples
03/13/2022 20:12:57 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/13/2022 20:12:57 - INFO - __main__ - ['offensive']
03/13/2022 20:12:57 - INFO - __main__ -  [hatexplain] <user> it  genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/13/2022 20:12:57 - INFO - __main__ - ['offensive']
03/13/2022 20:12:57 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/13/2022 20:12:57 - INFO - __main__ - ['offensive']
03/13/2022 20:12:57 - INFO - __main__ - Tokenizing Input ...
03/13/2022 20:12:57 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:12:58 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 20:12:58 - INFO - __main__ - save last model!
03/13/2022 20:13:05 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 20:13:06 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 20:13:06 - INFO - __main__ - Printing 3 examples
03/13/2022 20:13:06 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 20:13:06 - INFO - __main__ - ['normal']
03/13/2022 20:13:06 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 20:13:06 - INFO - __main__ - ['normal']
03/13/2022 20:13:06 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 20:13:06 - INFO - __main__ - ['normal']
03/13/2022 20:13:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 20:13:07 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:13:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 20:13:08 - INFO - __main__ - Starting training!
03/13/2022 20:13:08 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 20:16:10 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_21_0.0005_8_predictions.txt
03/13/2022 20:16:10 - INFO - __main__ - Classification-F1 on test data: 0.0697
03/13/2022 20:16:10 - INFO - __main__ - prefix=hatexplain_16_21, lr=0.0005, bsz=8, dev_performance=0.5309941520467837, test_performance=0.06974213327687313
03/13/2022 20:16:10 - INFO - __main__ - Running ... prefix=hatexplain_16_21, lr=0.0003, bsz=8 ...
03/13/2022 20:16:11 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:16:11 - INFO - __main__ - Printing 3 examples
03/13/2022 20:16:11 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass  
03/13/2022 20:16:11 - INFO - __main__ - ['offensive']
03/13/2022 20:16:11 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/13/2022 20:16:11 - INFO - __main__ - ['offensive']
03/13/2022 20:16:11 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/13/2022 20:16:11 - INFO - __main__ - ['offensive']
03/13/2022 20:16:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 20:16:11 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:16:11 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 20:16:11 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:16:11 - INFO - __main__ - Printing 3 examples
03/13/2022 20:16:11 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/13/2022 20:16:11 - INFO - __main__ - ['offensive']
03/13/2022 20:16:11 - INFO - __main__ -  [hatexplain] <user> it  genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/13/2022 20:16:11 - INFO - __main__ - ['offensive']
03/13/2022 20:16:11 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/13/2022 20:16:11 - INFO - __main__ - ['offensive']
03/13/2022 20:16:11 - INFO - __main__ - Tokenizing Input ...
03/13/2022 20:16:11 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:16:11 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 20:16:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 20:16:22 - INFO - __main__ - Starting training!
03/13/2022 20:16:26 - INFO - __main__ - Step 10 Global step 10 Train loss 22.054956 on epoch=3
03/13/2022 20:16:31 - INFO - __main__ - Step 20 Global step 20 Train loss 18.633999 on epoch=6
03/13/2022 20:16:36 - INFO - __main__ - Step 30 Global step 30 Train loss 12.835515 on epoch=9
03/13/2022 20:16:41 - INFO - __main__ - Step 40 Global step 40 Train loss 10.670999 on epoch=13
03/13/2022 20:16:46 - INFO - __main__ - Step 50 Global step 50 Train loss 10.116638 on epoch=16
03/13/2022 20:16:51 - INFO - __main__ - Global step 50 Train loss 14.862421 Classification-F1 0.0 on epoch=16
03/13/2022 20:16:57 - INFO - __main__ - Step 60 Global step 60 Train loss 9.755873 on epoch=19
03/13/2022 20:17:02 - INFO - __main__ - Step 70 Global step 70 Train loss 8.759227 on epoch=23
03/13/2022 20:17:07 - INFO - __main__ - Step 80 Global step 80 Train loss 7.834673 on epoch=26
03/13/2022 20:17:12 - INFO - __main__ - Step 90 Global step 90 Train loss 7.398389 on epoch=29
03/13/2022 20:17:17 - INFO - __main__ - Step 100 Global step 100 Train loss 5.797489 on epoch=33
03/13/2022 20:17:19 - INFO - __main__ - Global step 100 Train loss 7.909130 Classification-F1 0.0 on epoch=33
03/13/2022 20:17:24 - INFO - __main__ - Step 110 Global step 110 Train loss 4.027099 on epoch=36
03/13/2022 20:17:29 - INFO - __main__ - Step 120 Global step 120 Train loss 3.629290 on epoch=39
03/13/2022 20:17:34 - INFO - __main__ - Step 130 Global step 130 Train loss 2.362642 on epoch=43
03/13/2022 20:17:39 - INFO - __main__ - Step 140 Global step 140 Train loss 2.131939 on epoch=46
03/13/2022 20:17:44 - INFO - __main__ - Step 150 Global step 150 Train loss 2.187305 on epoch=49
03/13/2022 20:17:45 - INFO - __main__ - Global step 150 Train loss 2.867655 Classification-F1 0.32002376708259056 on epoch=49
03/13/2022 20:17:50 - INFO - __main__ - Step 160 Global step 160 Train loss 2.057946 on epoch=53
03/13/2022 20:17:55 - INFO - __main__ - Step 170 Global step 170 Train loss 2.374049 on epoch=56
03/13/2022 20:18:00 - INFO - __main__ - Step 180 Global step 180 Train loss 2.338321 on epoch=59
03/13/2022 20:18:05 - INFO - __main__ - Step 190 Global step 190 Train loss 2.026107 on epoch=63
03/13/2022 20:18:10 - INFO - __main__ - Step 200 Global step 200 Train loss 1.961627 on epoch=66
03/13/2022 20:18:11 - INFO - __main__ - Global step 200 Train loss 2.151610 Classification-F1 0.16666666666666666 on epoch=66
03/13/2022 20:18:16 - INFO - __main__ - Step 210 Global step 210 Train loss 2.204322 on epoch=69
03/13/2022 20:18:21 - INFO - __main__ - Step 220 Global step 220 Train loss 1.748313 on epoch=73
03/13/2022 20:18:26 - INFO - __main__ - Step 230 Global step 230 Train loss 2.027739 on epoch=76
03/13/2022 20:18:31 - INFO - __main__ - Step 240 Global step 240 Train loss 1.987167 on epoch=79
03/13/2022 20:18:36 - INFO - __main__ - Step 250 Global step 250 Train loss 1.504788 on epoch=83
03/13/2022 20:18:37 - INFO - __main__ - Global step 250 Train loss 1.894466 Classification-F1 0.1983273596176822 on epoch=83
03/13/2022 20:18:42 - INFO - __main__ - Step 260 Global step 260 Train loss 1.297701 on epoch=86
03/13/2022 20:18:47 - INFO - __main__ - Step 270 Global step 270 Train loss 1.464931 on epoch=89
03/13/2022 20:18:52 - INFO - __main__ - Step 280 Global step 280 Train loss 1.373139 on epoch=93
03/13/2022 20:18:57 - INFO - __main__ - Step 290 Global step 290 Train loss 1.186712 on epoch=96
03/13/2022 20:19:02 - INFO - __main__ - Step 300 Global step 300 Train loss 1.350050 on epoch=99
03/13/2022 20:19:03 - INFO - __main__ - Global step 300 Train loss 1.334506 Classification-F1 0.38315613925370023 on epoch=99
03/13/2022 20:19:09 - INFO - __main__ - Step 310 Global step 310 Train loss 1.429883 on epoch=103
03/13/2022 20:19:14 - INFO - __main__ - Step 320 Global step 320 Train loss 1.583482 on epoch=106
03/13/2022 20:19:19 - INFO - __main__ - Step 330 Global step 330 Train loss 1.076992 on epoch=109
03/13/2022 20:19:24 - INFO - __main__ - Step 340 Global step 340 Train loss 1.172029 on epoch=113
03/13/2022 20:19:29 - INFO - __main__ - Step 350 Global step 350 Train loss 1.109808 on epoch=116
03/13/2022 20:19:38 - INFO - __main__ - Global step 350 Train loss 1.274439 Classification-F1 0.12698412698412698 on epoch=116
03/13/2022 20:19:43 - INFO - __main__ - Step 360 Global step 360 Train loss 1.037686 on epoch=119
03/13/2022 20:19:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.857890 on epoch=123
03/13/2022 20:19:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.752260 on epoch=126
03/13/2022 20:19:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.777735 on epoch=129
03/13/2022 20:20:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.669963 on epoch=133
03/13/2022 20:20:12 - INFO - __main__ - Global step 400 Train loss 0.819107 Classification-F1 0.19514627659574468 on epoch=133
03/13/2022 20:20:17 - INFO - __main__ - Step 410 Global step 410 Train loss 1.003064 on epoch=136
03/13/2022 20:20:22 - INFO - __main__ - Step 420 Global step 420 Train loss 1.469809 on epoch=139
03/13/2022 20:20:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.661473 on epoch=143
03/13/2022 20:20:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.603280 on epoch=146
03/13/2022 20:20:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.473701 on epoch=149
03/13/2022 20:20:38 - INFO - __main__ - Global step 450 Train loss 0.842265 Classification-F1 0.2916666666666667 on epoch=149
03/13/2022 20:20:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.402087 on epoch=153
03/13/2022 20:20:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.439392 on epoch=156
03/13/2022 20:20:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.393045 on epoch=159
03/13/2022 20:20:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.322006 on epoch=163
03/13/2022 20:21:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.405356 on epoch=166
03/13/2022 20:21:04 - INFO - __main__ - Global step 500 Train loss 0.392377 Classification-F1 0.3561000597626569 on epoch=166
03/13/2022 20:21:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.222973 on epoch=169
03/13/2022 20:21:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.228795 on epoch=173
03/13/2022 20:21:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.122350 on epoch=176
03/13/2022 20:21:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.054148 on epoch=179
03/13/2022 20:21:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.043581 on epoch=183
03/13/2022 20:21:29 - INFO - __main__ - Global step 550 Train loss 0.134369 Classification-F1 0.3819418129762957 on epoch=183
03/13/2022 20:21:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.033950 on epoch=186
03/13/2022 20:21:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.023351 on epoch=189
03/13/2022 20:21:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.007016 on epoch=193
03/13/2022 20:21:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.011326 on epoch=196
03/13/2022 20:21:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.006859 on epoch=199
03/13/2022 20:21:55 - INFO - __main__ - Global step 600 Train loss 0.016500 Classification-F1 0.39080158261743936 on epoch=199
03/13/2022 20:22:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.036184 on epoch=203
03/13/2022 20:22:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.004006 on epoch=206
03/13/2022 20:22:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.004970 on epoch=209
03/13/2022 20:22:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.003525 on epoch=213
03/13/2022 20:22:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001966 on epoch=216
03/13/2022 20:22:22 - INFO - __main__ - Global step 650 Train loss 0.010130 Classification-F1 0.4500561167227834 on epoch=216
03/13/2022 20:22:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001155 on epoch=219
03/13/2022 20:22:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001212 on epoch=223
03/13/2022 20:22:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.007793 on epoch=226
03/13/2022 20:22:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.001164 on epoch=229
03/13/2022 20:22:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.004456 on epoch=233
03/13/2022 20:22:49 - INFO - __main__ - Global step 700 Train loss 0.003156 Classification-F1 0.3541398541398541 on epoch=233
03/13/2022 20:22:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.002192 on epoch=236
03/13/2022 20:22:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.027716 on epoch=239
03/13/2022 20:23:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.003024 on epoch=243
03/13/2022 20:23:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.001129 on epoch=246
03/13/2022 20:23:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000581 on epoch=249
03/13/2022 20:23:15 - INFO - __main__ - Global step 750 Train loss 0.006928 Classification-F1 0.3794708994708995 on epoch=249
03/13/2022 20:23:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000206 on epoch=253
03/13/2022 20:23:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000257 on epoch=256
03/13/2022 20:23:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000406 on epoch=259
03/13/2022 20:23:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000409 on epoch=263
03/13/2022 20:23:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000579 on epoch=266
03/13/2022 20:23:41 - INFO - __main__ - Global step 800 Train loss 0.000371 Classification-F1 0.33375661375661375 on epoch=266
03/13/2022 20:23:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.003496 on epoch=269
03/13/2022 20:23:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001277 on epoch=273
03/13/2022 20:23:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000260 on epoch=276
03/13/2022 20:24:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000671 on epoch=279
03/13/2022 20:24:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000558 on epoch=283
03/13/2022 20:24:07 - INFO - __main__ - Global step 850 Train loss 0.001252 Classification-F1 0.40765459564124745 on epoch=283
03/13/2022 20:24:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000170 on epoch=286
03/13/2022 20:24:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000294 on epoch=289
03/13/2022 20:24:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000228 on epoch=293
03/13/2022 20:24:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000444 on epoch=296
03/13/2022 20:24:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000143 on epoch=299
03/13/2022 20:24:33 - INFO - __main__ - Global step 900 Train loss 0.000256 Classification-F1 0.39812517231872074 on epoch=299
03/13/2022 20:24:33 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:24:33 - INFO - __main__ - Printing 3 examples
03/13/2022 20:24:33 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass  
03/13/2022 20:24:33 - INFO - __main__ - ['offensive']
03/13/2022 20:24:33 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/13/2022 20:24:33 - INFO - __main__ - ['offensive']
03/13/2022 20:24:33 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/13/2022 20:24:33 - INFO - __main__ - ['offensive']
03/13/2022 20:24:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 20:24:33 - INFO - __main__ - save last model!
03/13/2022 20:24:33 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:24:33 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 20:24:33 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:24:33 - INFO - __main__ - Printing 3 examples
03/13/2022 20:24:33 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/13/2022 20:24:33 - INFO - __main__ - ['offensive']
03/13/2022 20:24:33 - INFO - __main__ -  [hatexplain] <user> it  genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/13/2022 20:24:33 - INFO - __main__ - ['offensive']
03/13/2022 20:24:33 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/13/2022 20:24:33 - INFO - __main__ - ['offensive']
03/13/2022 20:24:33 - INFO - __main__ - Tokenizing Input ...
03/13/2022 20:24:33 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:24:33 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 20:24:40 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 20:24:41 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 20:24:41 - INFO - __main__ - Printing 3 examples
03/13/2022 20:24:41 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 20:24:41 - INFO - __main__ - ['normal']
03/13/2022 20:24:41 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 20:24:41 - INFO - __main__ - ['normal']
03/13/2022 20:24:41 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 20:24:41 - INFO - __main__ - ['normal']
03/13/2022 20:24:41 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 20:24:42 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:24:44 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 20:24:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 20:24:46 - INFO - __main__ - Starting training!
03/13/2022 20:28:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_21_0.0003_8_predictions.txt
03/13/2022 20:28:31 - INFO - __main__ - Classification-F1 on test data: 0.0422
03/13/2022 20:28:31 - INFO - __main__ - prefix=hatexplain_16_21, lr=0.0003, bsz=8, dev_performance=0.4500561167227834, test_performance=0.04216748620406051
03/13/2022 20:28:31 - INFO - __main__ - Running ... prefix=hatexplain_16_21, lr=0.0002, bsz=8 ...
03/13/2022 20:28:32 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:28:32 - INFO - __main__ - Printing 3 examples
03/13/2022 20:28:32 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass  
03/13/2022 20:28:32 - INFO - __main__ - ['offensive']
03/13/2022 20:28:32 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/13/2022 20:28:32 - INFO - __main__ - ['offensive']
03/13/2022 20:28:32 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/13/2022 20:28:32 - INFO - __main__ - ['offensive']
03/13/2022 20:28:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 20:28:32 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:28:32 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 20:28:32 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:28:32 - INFO - __main__ - Printing 3 examples
03/13/2022 20:28:32 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/13/2022 20:28:32 - INFO - __main__ - ['offensive']
03/13/2022 20:28:32 - INFO - __main__ -  [hatexplain] <user> it  genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/13/2022 20:28:32 - INFO - __main__ - ['offensive']
03/13/2022 20:28:32 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/13/2022 20:28:32 - INFO - __main__ - ['offensive']
03/13/2022 20:28:32 - INFO - __main__ - Tokenizing Input ...
03/13/2022 20:28:32 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:28:32 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 20:28:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 20:28:43 - INFO - __main__ - Starting training!
03/13/2022 20:28:47 - INFO - __main__ - Step 10 Global step 10 Train loss 20.049547 on epoch=3
03/13/2022 20:28:52 - INFO - __main__ - Step 20 Global step 20 Train loss 17.312901 on epoch=6
03/13/2022 20:28:57 - INFO - __main__ - Step 30 Global step 30 Train loss 12.752293 on epoch=9
03/13/2022 20:29:01 - INFO - __main__ - Step 40 Global step 40 Train loss 10.867311 on epoch=13
03/13/2022 20:29:06 - INFO - __main__ - Step 50 Global step 50 Train loss 10.897753 on epoch=16
03/13/2022 20:29:11 - INFO - __main__ - Global step 50 Train loss 14.375960 Classification-F1 0.0 on epoch=16
03/13/2022 20:29:17 - INFO - __main__ - Step 60 Global step 60 Train loss 10.420955 on epoch=19
03/13/2022 20:29:22 - INFO - __main__ - Step 70 Global step 70 Train loss 9.720479 on epoch=23
03/13/2022 20:29:27 - INFO - __main__ - Step 80 Global step 80 Train loss 9.867679 on epoch=26
03/13/2022 20:29:32 - INFO - __main__ - Step 90 Global step 90 Train loss 9.299641 on epoch=29
03/13/2022 20:29:37 - INFO - __main__ - Step 100 Global step 100 Train loss 8.295353 on epoch=33
03/13/2022 20:29:39 - INFO - __main__ - Global step 100 Train loss 9.520821 Classification-F1 0.0 on epoch=33
03/13/2022 20:29:44 - INFO - __main__ - Step 110 Global step 110 Train loss 7.925229 on epoch=36
03/13/2022 20:29:49 - INFO - __main__ - Step 120 Global step 120 Train loss 7.607925 on epoch=39
03/13/2022 20:29:54 - INFO - __main__ - Step 130 Global step 130 Train loss 6.573853 on epoch=43
03/13/2022 20:29:59 - INFO - __main__ - Step 140 Global step 140 Train loss 4.793708 on epoch=46
03/13/2022 20:30:04 - INFO - __main__ - Step 150 Global step 150 Train loss 4.012813 on epoch=49
03/13/2022 20:30:05 - INFO - __main__ - Global step 150 Train loss 6.182706 Classification-F1 0.30510752688172044 on epoch=49
03/13/2022 20:30:11 - INFO - __main__ - Step 160 Global step 160 Train loss 2.752888 on epoch=53
03/13/2022 20:30:16 - INFO - __main__ - Step 170 Global step 170 Train loss 2.502196 on epoch=56
03/13/2022 20:30:21 - INFO - __main__ - Step 180 Global step 180 Train loss 2.462783 on epoch=59
03/13/2022 20:30:26 - INFO - __main__ - Step 190 Global step 190 Train loss 2.760015 on epoch=63
03/13/2022 20:30:31 - INFO - __main__ - Step 200 Global step 200 Train loss 2.662649 on epoch=66
03/13/2022 20:30:32 - INFO - __main__ - Global step 200 Train loss 2.628106 Classification-F1 0.2647296206618241 on epoch=66
03/13/2022 20:30:37 - INFO - __main__ - Step 210 Global step 210 Train loss 2.140177 on epoch=69
03/13/2022 20:30:42 - INFO - __main__ - Step 220 Global step 220 Train loss 1.707314 on epoch=73
03/13/2022 20:30:47 - INFO - __main__ - Step 230 Global step 230 Train loss 1.747875 on epoch=76
03/13/2022 20:30:52 - INFO - __main__ - Step 240 Global step 240 Train loss 2.003146 on epoch=79
03/13/2022 20:30:57 - INFO - __main__ - Step 250 Global step 250 Train loss 2.207464 on epoch=83
03/13/2022 20:30:58 - INFO - __main__ - Global step 250 Train loss 1.961195 Classification-F1 0.24611708482676223 on epoch=83
03/13/2022 20:31:03 - INFO - __main__ - Step 260 Global step 260 Train loss 2.097547 on epoch=86
03/13/2022 20:31:08 - INFO - __main__ - Step 270 Global step 270 Train loss 2.353380 on epoch=89
03/13/2022 20:31:13 - INFO - __main__ - Step 280 Global step 280 Train loss 1.986220 on epoch=93
03/13/2022 20:31:18 - INFO - __main__ - Step 290 Global step 290 Train loss 1.775092 on epoch=96
03/13/2022 20:31:23 - INFO - __main__ - Step 300 Global step 300 Train loss 1.857376 on epoch=99
03/13/2022 20:31:24 - INFO - __main__ - Global step 300 Train loss 2.013923 Classification-F1 0.28869895536562207 on epoch=99
03/13/2022 20:31:29 - INFO - __main__ - Step 310 Global step 310 Train loss 2.111214 on epoch=103
03/13/2022 20:31:34 - INFO - __main__ - Step 320 Global step 320 Train loss 1.863803 on epoch=106
03/13/2022 20:31:39 - INFO - __main__ - Step 330 Global step 330 Train loss 2.526437 on epoch=109
03/13/2022 20:31:43 - INFO - __main__ - Step 340 Global step 340 Train loss 1.914959 on epoch=113
03/13/2022 20:31:48 - INFO - __main__ - Step 350 Global step 350 Train loss 1.942731 on epoch=116
03/13/2022 20:31:49 - INFO - __main__ - Global step 350 Train loss 2.071829 Classification-F1 0.2085278555866791 on epoch=116
03/13/2022 20:31:55 - INFO - __main__ - Step 360 Global step 360 Train loss 1.942416 on epoch=119
03/13/2022 20:32:00 - INFO - __main__ - Step 370 Global step 370 Train loss 1.535455 on epoch=123
03/13/2022 20:32:04 - INFO - __main__ - Step 380 Global step 380 Train loss 1.668316 on epoch=126
03/13/2022 20:32:09 - INFO - __main__ - Step 390 Global step 390 Train loss 1.481815 on epoch=129
03/13/2022 20:32:14 - INFO - __main__ - Step 400 Global step 400 Train loss 1.704028 on epoch=133
03/13/2022 20:32:16 - INFO - __main__ - Global step 400 Train loss 1.666406 Classification-F1 0.22222222222222224 on epoch=133
03/13/2022 20:32:21 - INFO - __main__ - Step 410 Global step 410 Train loss 1.163822 on epoch=136
03/13/2022 20:32:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.528584 on epoch=139
03/13/2022 20:32:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.408649 on epoch=143
03/13/2022 20:32:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.337370 on epoch=146
03/13/2022 20:32:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.290354 on epoch=149
03/13/2022 20:32:41 - INFO - __main__ - Global step 450 Train loss 0.545756 Classification-F1 0.43528693528693524 on epoch=149
03/13/2022 20:32:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.188376 on epoch=153
03/13/2022 20:32:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.116525 on epoch=156
03/13/2022 20:32:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.106311 on epoch=159
03/13/2022 20:33:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.064785 on epoch=163
03/13/2022 20:33:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.052245 on epoch=166
03/13/2022 20:33:08 - INFO - __main__ - Global step 500 Train loss 0.105648 Classification-F1 0.4701628623530152 on epoch=166
03/13/2022 20:33:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.034092 on epoch=169
03/13/2022 20:33:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.049299 on epoch=173
03/13/2022 20:33:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.030597 on epoch=176
03/13/2022 20:33:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.039369 on epoch=179
03/13/2022 20:33:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.021120 on epoch=183
03/13/2022 20:33:34 - INFO - __main__ - Global step 550 Train loss 0.034895 Classification-F1 0.3537103628636809 on epoch=183
03/13/2022 20:33:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.009880 on epoch=186
03/13/2022 20:33:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.008662 on epoch=189
03/13/2022 20:33:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.007659 on epoch=193
03/13/2022 20:33:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.012127 on epoch=196
03/13/2022 20:33:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.002197 on epoch=199
03/13/2022 20:34:00 - INFO - __main__ - Global step 600 Train loss 0.008105 Classification-F1 0.45286195286195285 on epoch=199
03/13/2022 20:34:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.006189 on epoch=203
03/13/2022 20:34:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.011890 on epoch=206
03/13/2022 20:34:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.002796 on epoch=209
03/13/2022 20:34:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001948 on epoch=213
03/13/2022 20:34:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001140 on epoch=216
03/13/2022 20:34:25 - INFO - __main__ - Global step 650 Train loss 0.004793 Classification-F1 0.38493038493038495 on epoch=216
03/13/2022 20:34:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001231 on epoch=219
03/13/2022 20:34:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000448 on epoch=223
03/13/2022 20:34:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000908 on epoch=226
03/13/2022 20:34:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.001976 on epoch=229
03/13/2022 20:34:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.011522 on epoch=233
03/13/2022 20:34:51 - INFO - __main__ - Global step 700 Train loss 0.003217 Classification-F1 0.43496367180577705 on epoch=233
03/13/2022 20:34:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001204 on epoch=236
03/13/2022 20:35:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.001105 on epoch=239
03/13/2022 20:35:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.003490 on epoch=243
03/13/2022 20:35:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000592 on epoch=246
03/13/2022 20:35:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000548 on epoch=249
03/13/2022 20:35:17 - INFO - __main__ - Global step 750 Train loss 0.001388 Classification-F1 0.49775533108866443 on epoch=249
03/13/2022 20:35:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001166 on epoch=253
03/13/2022 20:35:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.001237 on epoch=256
03/13/2022 20:35:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001421 on epoch=259
03/13/2022 20:35:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000937 on epoch=263
03/13/2022 20:35:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000361 on epoch=266
03/13/2022 20:35:43 - INFO - __main__ - Global step 800 Train loss 0.001025 Classification-F1 0.39500265816055286 on epoch=266
03/13/2022 20:35:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000243 on epoch=269
03/13/2022 20:35:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000567 on epoch=273
03/13/2022 20:35:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000437 on epoch=276
03/13/2022 20:36:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001426 on epoch=279
03/13/2022 20:36:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000245 on epoch=283
03/13/2022 20:36:09 - INFO - __main__ - Global step 850 Train loss 0.000584 Classification-F1 0.23313090418353577 on epoch=283
03/13/2022 20:36:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000254 on epoch=286
03/13/2022 20:36:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000466 on epoch=289
03/13/2022 20:36:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000365 on epoch=293
03/13/2022 20:36:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000333 on epoch=296
03/13/2022 20:36:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000947 on epoch=299
03/13/2022 20:36:34 - INFO - __main__ - Global step 900 Train loss 0.000473 Classification-F1 0.21522742254449573 on epoch=299
03/13/2022 20:36:34 - INFO - __main__ - save last model!
03/13/2022 20:36:34 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:36:34 - INFO - __main__ - Printing 3 examples
03/13/2022 20:36:34 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass  
03/13/2022 20:36:34 - INFO - __main__ - ['offensive']
03/13/2022 20:36:34 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/13/2022 20:36:34 - INFO - __main__ - ['offensive']
03/13/2022 20:36:34 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/13/2022 20:36:34 - INFO - __main__ - ['offensive']
03/13/2022 20:36:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 20:36:34 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:36:35 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 20:36:35 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:36:35 - INFO - __main__ - Printing 3 examples
03/13/2022 20:36:35 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/13/2022 20:36:35 - INFO - __main__ - ['offensive']
03/13/2022 20:36:35 - INFO - __main__ -  [hatexplain] <user> it  genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/13/2022 20:36:35 - INFO - __main__ - ['offensive']
03/13/2022 20:36:35 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/13/2022 20:36:35 - INFO - __main__ - ['offensive']
03/13/2022 20:36:35 - INFO - __main__ - Tokenizing Input ...
03/13/2022 20:36:35 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:36:35 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 20:36:41 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 20:36:42 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 20:36:42 - INFO - __main__ - Printing 3 examples
03/13/2022 20:36:42 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 20:36:42 - INFO - __main__ - ['normal']
03/13/2022 20:36:42 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 20:36:42 - INFO - __main__ - ['normal']
03/13/2022 20:36:42 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 20:36:42 - INFO - __main__ - ['normal']
03/13/2022 20:36:42 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 20:36:43 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:36:45 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 20:36:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 20:36:45 - INFO - __main__ - Starting training!
03/13/2022 20:37:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_21_0.0002_8_predictions.txt
03/13/2022 20:37:34 - INFO - __main__ - Classification-F1 on test data: 0.0621
03/13/2022 20:37:34 - INFO - __main__ - prefix=hatexplain_16_21, lr=0.0002, bsz=8, dev_performance=0.49775533108866443, test_performance=0.0621007085412561
03/13/2022 20:37:34 - INFO - __main__ - Running ... prefix=hatexplain_16_21, lr=0.0001, bsz=8 ...
03/13/2022 20:37:35 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:37:35 - INFO - __main__ - Printing 3 examples
03/13/2022 20:37:35 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass  
03/13/2022 20:37:35 - INFO - __main__ - ['offensive']
03/13/2022 20:37:35 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/13/2022 20:37:35 - INFO - __main__ - ['offensive']
03/13/2022 20:37:35 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/13/2022 20:37:35 - INFO - __main__ - ['offensive']
03/13/2022 20:37:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 20:37:35 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:37:35 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 20:37:35 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:37:35 - INFO - __main__ - Printing 3 examples
03/13/2022 20:37:35 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/13/2022 20:37:35 - INFO - __main__ - ['offensive']
03/13/2022 20:37:35 - INFO - __main__ -  [hatexplain] <user> it  genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/13/2022 20:37:35 - INFO - __main__ - ['offensive']
03/13/2022 20:37:35 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/13/2022 20:37:35 - INFO - __main__ - ['offensive']
03/13/2022 20:37:35 - INFO - __main__ - Tokenizing Input ...
03/13/2022 20:37:35 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:37:35 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 20:37:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 20:37:49 - INFO - __main__ - Starting training!
03/13/2022 20:37:54 - INFO - __main__ - Step 10 Global step 10 Train loss 20.942852 on epoch=3
03/13/2022 20:37:58 - INFO - __main__ - Step 20 Global step 20 Train loss 18.819721 on epoch=6
03/13/2022 20:38:03 - INFO - __main__ - Step 30 Global step 30 Train loss 14.816938 on epoch=9
03/13/2022 20:38:08 - INFO - __main__ - Step 40 Global step 40 Train loss 12.610403 on epoch=13
03/13/2022 20:38:13 - INFO - __main__ - Step 50 Global step 50 Train loss 11.850084 on epoch=16
03/13/2022 20:38:25 - INFO - __main__ - Global step 50 Train loss 15.808001 Classification-F1 0.0 on epoch=16
03/13/2022 20:38:31 - INFO - __main__ - Step 60 Global step 60 Train loss 11.516070 on epoch=19
03/13/2022 20:38:36 - INFO - __main__ - Step 70 Global step 70 Train loss 10.822906 on epoch=23
03/13/2022 20:38:41 - INFO - __main__ - Step 80 Global step 80 Train loss 10.734095 on epoch=26
03/13/2022 20:38:46 - INFO - __main__ - Step 90 Global step 90 Train loss 10.515871 on epoch=29
03/13/2022 20:38:51 - INFO - __main__ - Step 100 Global step 100 Train loss 10.333982 on epoch=33
03/13/2022 20:38:55 - INFO - __main__ - Global step 100 Train loss 10.784585 Classification-F1 0.0 on epoch=33
03/13/2022 20:39:00 - INFO - __main__ - Step 110 Global step 110 Train loss 10.305426 on epoch=36
03/13/2022 20:39:05 - INFO - __main__ - Step 120 Global step 120 Train loss 9.944492 on epoch=39
03/13/2022 20:39:10 - INFO - __main__ - Step 130 Global step 130 Train loss 9.961385 on epoch=43
03/13/2022 20:39:15 - INFO - __main__ - Step 140 Global step 140 Train loss 9.878102 on epoch=46
03/13/2022 20:39:20 - INFO - __main__ - Step 150 Global step 150 Train loss 9.462103 on epoch=49
03/13/2022 20:39:24 - INFO - __main__ - Global step 150 Train loss 9.910301 Classification-F1 0.0 on epoch=49
03/13/2022 20:39:29 - INFO - __main__ - Step 160 Global step 160 Train loss 8.536931 on epoch=53
03/13/2022 20:39:34 - INFO - __main__ - Step 170 Global step 170 Train loss 8.420218 on epoch=56
03/13/2022 20:39:39 - INFO - __main__ - Step 180 Global step 180 Train loss 8.884075 on epoch=59
03/13/2022 20:39:44 - INFO - __main__ - Step 190 Global step 190 Train loss 7.794564 on epoch=63
03/13/2022 20:39:49 - INFO - __main__ - Step 200 Global step 200 Train loss 7.698637 on epoch=66
03/13/2022 20:39:51 - INFO - __main__ - Global step 200 Train loss 8.266886 Classification-F1 0.0 on epoch=66
03/13/2022 20:39:56 - INFO - __main__ - Step 210 Global step 210 Train loss 7.463573 on epoch=69
03/13/2022 20:40:01 - INFO - __main__ - Step 220 Global step 220 Train loss 5.554535 on epoch=73
03/13/2022 20:40:06 - INFO - __main__ - Step 230 Global step 230 Train loss 6.109230 on epoch=76
03/13/2022 20:40:11 - INFO - __main__ - Step 240 Global step 240 Train loss 4.374839 on epoch=79
03/13/2022 20:40:16 - INFO - __main__ - Step 250 Global step 250 Train loss 4.118287 on epoch=83
03/13/2022 20:40:17 - INFO - __main__ - Global step 250 Train loss 5.524092 Classification-F1 0.07281996755680967 on epoch=83
03/13/2022 20:40:22 - INFO - __main__ - Step 260 Global step 260 Train loss 1.882182 on epoch=86
03/13/2022 20:40:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.737892 on epoch=89
03/13/2022 20:40:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.606660 on epoch=93
03/13/2022 20:40:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.497735 on epoch=96
03/13/2022 20:40:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.566880 on epoch=99
03/13/2022 20:40:43 - INFO - __main__ - Global step 300 Train loss 0.858270 Classification-F1 0.29794906468334464 on epoch=99
03/13/2022 20:40:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.559810 on epoch=103
03/13/2022 20:40:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.426013 on epoch=106
03/13/2022 20:40:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.374722 on epoch=109
03/13/2022 20:41:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.440381 on epoch=113
03/13/2022 20:41:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.416168 on epoch=116
03/13/2022 20:41:09 - INFO - __main__ - Global step 350 Train loss 0.443419 Classification-F1 0.2333333333333333 on epoch=116
03/13/2022 20:41:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.479872 on epoch=119
03/13/2022 20:41:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.368638 on epoch=123
03/13/2022 20:41:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.305202 on epoch=126
03/13/2022 20:41:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.289537 on epoch=129
03/13/2022 20:41:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.323402 on epoch=133
03/13/2022 20:41:34 - INFO - __main__ - Global step 400 Train loss 0.353330 Classification-F1 0.3060528559249787 on epoch=133
03/13/2022 20:41:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.257148 on epoch=136
03/13/2022 20:41:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.240515 on epoch=139
03/13/2022 20:41:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.300602 on epoch=143
03/13/2022 20:41:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.281506 on epoch=146
03/13/2022 20:42:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.640119 on epoch=149
03/13/2022 20:42:01 - INFO - __main__ - Global step 450 Train loss 0.343978 Classification-F1 0.3343408509648919 on epoch=149
03/13/2022 20:42:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.307738 on epoch=153
03/13/2022 20:42:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.173709 on epoch=156
03/13/2022 20:42:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.144126 on epoch=159
03/13/2022 20:42:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.246637 on epoch=163
03/13/2022 20:42:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.219894 on epoch=166
03/13/2022 20:42:28 - INFO - __main__ - Global step 500 Train loss 0.218421 Classification-F1 0.3343915343915344 on epoch=166
03/13/2022 20:42:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.306663 on epoch=169
03/13/2022 20:42:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.106462 on epoch=173
03/13/2022 20:42:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.133449 on epoch=176
03/13/2022 20:42:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.136471 on epoch=179
03/13/2022 20:42:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.081909 on epoch=183
03/13/2022 20:42:54 - INFO - __main__ - Global step 550 Train loss 0.152991 Classification-F1 0.31764453041048785 on epoch=183
03/13/2022 20:42:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.081352 on epoch=186
03/13/2022 20:43:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.099454 on epoch=189
03/13/2022 20:43:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.076045 on epoch=193
03/13/2022 20:43:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.094024 on epoch=196
03/13/2022 20:43:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.167867 on epoch=199
03/13/2022 20:43:20 - INFO - __main__ - Global step 600 Train loss 0.103749 Classification-F1 0.291593567251462 on epoch=199
03/13/2022 20:43:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.058414 on epoch=203
03/13/2022 20:43:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.049163 on epoch=206
03/13/2022 20:43:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.082885 on epoch=209
03/13/2022 20:43:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.057170 on epoch=213
03/13/2022 20:43:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.021857 on epoch=216
03/13/2022 20:43:47 - INFO - __main__ - Global step 650 Train loss 0.053898 Classification-F1 0.3586309523809524 on epoch=216
03/13/2022 20:43:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.029764 on epoch=219
03/13/2022 20:43:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.033845 on epoch=223
03/13/2022 20:44:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.071250 on epoch=226
03/13/2022 20:44:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.056232 on epoch=229
03/13/2022 20:44:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.042364 on epoch=233
03/13/2022 20:44:13 - INFO - __main__ - Global step 700 Train loss 0.046691 Classification-F1 0.3128654970760234 on epoch=233
03/13/2022 20:44:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.022918 on epoch=236
03/13/2022 20:44:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.019956 on epoch=239
03/13/2022 20:44:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.026199 on epoch=243
03/13/2022 20:44:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.010166 on epoch=246
03/13/2022 20:44:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.013657 on epoch=249
03/13/2022 20:44:40 - INFO - __main__ - Global step 750 Train loss 0.018579 Classification-F1 0.30565998329156224 on epoch=249
03/13/2022 20:44:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.016679 on epoch=253
03/13/2022 20:44:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.007808 on epoch=256
03/13/2022 20:44:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.016428 on epoch=259
03/13/2022 20:45:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.006852 on epoch=263
03/13/2022 20:45:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.033597 on epoch=266
03/13/2022 20:45:06 - INFO - __main__ - Global step 800 Train loss 0.016273 Classification-F1 0.3696236559139785 on epoch=266
03/13/2022 20:45:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.006942 on epoch=269
03/13/2022 20:45:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.002705 on epoch=273
03/13/2022 20:45:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.017489 on epoch=276
03/13/2022 20:45:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.004845 on epoch=279
03/13/2022 20:45:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001679 on epoch=283
03/13/2022 20:45:32 - INFO - __main__ - Global step 850 Train loss 0.006732 Classification-F1 0.2783216783216783 on epoch=283
03/13/2022 20:45:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002292 on epoch=286
03/13/2022 20:45:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.006486 on epoch=289
03/13/2022 20:45:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.023353 on epoch=293
03/13/2022 20:45:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.003844 on epoch=296
03/13/2022 20:45:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.005353 on epoch=299
03/13/2022 20:45:58 - INFO - __main__ - Global step 900 Train loss 0.008266 Classification-F1 0.2158823529411765 on epoch=299
03/13/2022 20:45:58 - INFO - __main__ - save last model!
03/13/2022 20:45:58 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:45:58 - INFO - __main__ - Printing 3 examples
03/13/2022 20:45:58 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/13/2022 20:45:58 - INFO - __main__ - ['hatespeech']
03/13/2022 20:45:58 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/13/2022 20:45:58 - INFO - __main__ - ['hatespeech']
03/13/2022 20:45:58 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/13/2022 20:45:58 - INFO - __main__ - ['hatespeech']
03/13/2022 20:45:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 20:45:58 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:45:58 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 20:45:58 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:45:58 - INFO - __main__ - Printing 3 examples
03/13/2022 20:45:58 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/13/2022 20:45:58 - INFO - __main__ - ['hatespeech']
03/13/2022 20:45:58 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/13/2022 20:45:58 - INFO - __main__ - ['hatespeech']
03/13/2022 20:45:58 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/13/2022 20:45:58 - INFO - __main__ - ['hatespeech']
03/13/2022 20:45:58 - INFO - __main__ - Tokenizing Input ...
03/13/2022 20:45:58 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:45:59 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 20:46:05 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 20:46:06 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 20:46:06 - INFO - __main__ - Printing 3 examples
03/13/2022 20:46:06 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 20:46:06 - INFO - __main__ - ['normal']
03/13/2022 20:46:06 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 20:46:06 - INFO - __main__ - ['normal']
03/13/2022 20:46:06 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 20:46:06 - INFO - __main__ - ['normal']
03/13/2022 20:46:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 20:46:07 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:46:09 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 20:46:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 20:46:11 - INFO - __main__ - Starting training!
03/13/2022 20:47:00 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_21_0.0001_8_predictions.txt
03/13/2022 20:47:00 - INFO - __main__ - Classification-F1 on test data: 0.0572
03/13/2022 20:47:00 - INFO - __main__ - prefix=hatexplain_16_21, lr=0.0001, bsz=8, dev_performance=0.3696236559139785, test_performance=0.05716425381573742
03/13/2022 20:47:00 - INFO - __main__ - Running ... prefix=hatexplain_16_42, lr=0.0005, bsz=8 ...
03/13/2022 20:47:01 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:47:01 - INFO - __main__ - Printing 3 examples
03/13/2022 20:47:01 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/13/2022 20:47:01 - INFO - __main__ - ['hatespeech']
03/13/2022 20:47:01 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/13/2022 20:47:01 - INFO - __main__ - ['hatespeech']
03/13/2022 20:47:01 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/13/2022 20:47:01 - INFO - __main__ - ['hatespeech']
03/13/2022 20:47:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 20:47:01 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:47:01 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 20:47:01 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:47:01 - INFO - __main__ - Printing 3 examples
03/13/2022 20:47:01 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/13/2022 20:47:01 - INFO - __main__ - ['hatespeech']
03/13/2022 20:47:01 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/13/2022 20:47:01 - INFO - __main__ - ['hatespeech']
03/13/2022 20:47:01 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/13/2022 20:47:01 - INFO - __main__ - ['hatespeech']
03/13/2022 20:47:01 - INFO - __main__ - Tokenizing Input ...
03/13/2022 20:47:01 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:47:01 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 20:47:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 20:47:15 - INFO - __main__ - Starting training!
03/13/2022 20:47:19 - INFO - __main__ - Step 10 Global step 10 Train loss 21.260029 on epoch=3
03/13/2022 20:47:24 - INFO - __main__ - Step 20 Global step 20 Train loss 15.944504 on epoch=6
03/13/2022 20:47:29 - INFO - __main__ - Step 30 Global step 30 Train loss 12.076797 on epoch=9
03/13/2022 20:47:34 - INFO - __main__ - Step 40 Global step 40 Train loss 11.549568 on epoch=13
03/13/2022 20:47:39 - INFO - __main__ - Step 50 Global step 50 Train loss 10.251993 on epoch=16
03/13/2022 20:47:46 - INFO - __main__ - Global step 50 Train loss 14.216578 Classification-F1 0.0 on epoch=16
03/13/2022 20:47:52 - INFO - __main__ - Step 60 Global step 60 Train loss 8.493125 on epoch=19
03/13/2022 20:47:57 - INFO - __main__ - Step 70 Global step 70 Train loss 6.065610 on epoch=23
03/13/2022 20:48:02 - INFO - __main__ - Step 80 Global step 80 Train loss 3.683010 on epoch=26
03/13/2022 20:48:07 - INFO - __main__ - Step 90 Global step 90 Train loss 2.518175 on epoch=29
03/13/2022 20:48:12 - INFO - __main__ - Step 100 Global step 100 Train loss 2.276120 on epoch=33
03/13/2022 20:48:13 - INFO - __main__ - Global step 100 Train loss 4.607208 Classification-F1 0.20908004778972522 on epoch=33
03/13/2022 20:48:18 - INFO - __main__ - Step 110 Global step 110 Train loss 2.266688 on epoch=36
03/13/2022 20:48:23 - INFO - __main__ - Step 120 Global step 120 Train loss 2.464396 on epoch=39
03/13/2022 20:48:28 - INFO - __main__ - Step 130 Global step 130 Train loss 2.220261 on epoch=43
03/13/2022 20:48:33 - INFO - __main__ - Step 140 Global step 140 Train loss 1.505165 on epoch=46
03/13/2022 20:48:38 - INFO - __main__ - Step 150 Global step 150 Train loss 1.828241 on epoch=49
03/13/2022 20:48:39 - INFO - __main__ - Global step 150 Train loss 2.056950 Classification-F1 0.23224728487886384 on epoch=49
03/13/2022 20:48:44 - INFO - __main__ - Step 160 Global step 160 Train loss 1.479183 on epoch=53
03/13/2022 20:48:49 - INFO - __main__ - Step 170 Global step 170 Train loss 1.831160 on epoch=56
03/13/2022 20:48:54 - INFO - __main__ - Step 180 Global step 180 Train loss 1.717667 on epoch=59
03/13/2022 20:48:59 - INFO - __main__ - Step 190 Global step 190 Train loss 1.887467 on epoch=63
03/13/2022 20:49:04 - INFO - __main__ - Step 200 Global step 200 Train loss 1.267593 on epoch=66
03/13/2022 20:49:05 - INFO - __main__ - Global step 200 Train loss 1.636614 Classification-F1 0.19272819730485635 on epoch=66
03/13/2022 20:49:10 - INFO - __main__ - Step 210 Global step 210 Train loss 1.210947 on epoch=69
03/13/2022 20:49:15 - INFO - __main__ - Step 220 Global step 220 Train loss 1.216171 on epoch=73
03/13/2022 20:49:20 - INFO - __main__ - Step 230 Global step 230 Train loss 1.087737 on epoch=76
03/13/2022 20:49:25 - INFO - __main__ - Step 240 Global step 240 Train loss 1.101538 on epoch=79
03/13/2022 20:49:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.854476 on epoch=83
03/13/2022 20:49:31 - INFO - __main__ - Global step 250 Train loss 1.094174 Classification-F1 0.2917797888386124 on epoch=83
03/13/2022 20:49:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.729295 on epoch=86
03/13/2022 20:49:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.796620 on epoch=89
03/13/2022 20:49:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.790499 on epoch=93
03/13/2022 20:49:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.686664 on epoch=96
03/13/2022 20:49:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.575629 on epoch=99
03/13/2022 20:49:57 - INFO - __main__ - Global step 300 Train loss 0.715742 Classification-F1 0.21527777777777776 on epoch=99
03/13/2022 20:50:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.669077 on epoch=103
03/13/2022 20:50:07 - INFO - __main__ - Step 320 Global step 320 Train loss 1.031449 on epoch=106
03/13/2022 20:50:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.482989 on epoch=109
03/13/2022 20:50:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.568138 on epoch=113
03/13/2022 20:50:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.496671 on epoch=116
03/13/2022 20:50:23 - INFO - __main__ - Global step 350 Train loss 0.649665 Classification-F1 0.28697880270207315 on epoch=116
03/13/2022 20:50:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.467059 on epoch=119
03/13/2022 20:50:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.445253 on epoch=123
03/13/2022 20:50:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.745101 on epoch=126
03/13/2022 20:50:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.364223 on epoch=129
03/13/2022 20:50:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.325896 on epoch=133
03/13/2022 20:50:49 - INFO - __main__ - Global step 400 Train loss 0.469506 Classification-F1 0.5476190476190476 on epoch=133
03/13/2022 20:50:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.409192 on epoch=136
03/13/2022 20:50:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.321026 on epoch=139
03/13/2022 20:51:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.280061 on epoch=143
03/13/2022 20:51:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.295709 on epoch=146
03/13/2022 20:51:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.306446 on epoch=149
03/13/2022 20:51:23 - INFO - __main__ - Global step 450 Train loss 0.322487 Classification-F1 0.31392388099705176 on epoch=149
03/13/2022 20:51:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.335014 on epoch=153
03/13/2022 20:51:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.273340 on epoch=156
03/13/2022 20:51:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.309155 on epoch=159
03/13/2022 20:51:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.351892 on epoch=163
03/13/2022 20:51:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.210282 on epoch=166
03/13/2022 20:51:57 - INFO - __main__ - Global step 500 Train loss 0.295937 Classification-F1 0.4245358090185677 on epoch=166
03/13/2022 20:52:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.177608 on epoch=169
03/13/2022 20:52:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.180903 on epoch=173
03/13/2022 20:52:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.142004 on epoch=176
03/13/2022 20:52:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.154915 on epoch=179
03/13/2022 20:52:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.125224 on epoch=183
03/13/2022 20:52:39 - INFO - __main__ - Global step 550 Train loss 0.156131 Classification-F1 0.3060483870967742 on epoch=183
03/13/2022 20:52:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.134220 on epoch=186
03/13/2022 20:52:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.237531 on epoch=189
03/13/2022 20:52:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.155955 on epoch=193
03/13/2022 20:52:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.218898 on epoch=196
03/13/2022 20:53:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.195523 on epoch=199
03/13/2022 20:53:22 - INFO - __main__ - Global step 600 Train loss 0.188425 Classification-F1 0.31800766283524906 on epoch=199
03/13/2022 20:53:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.151704 on epoch=203
03/13/2022 20:53:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.180898 on epoch=206
03/13/2022 20:53:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.123785 on epoch=209
03/13/2022 20:53:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.109677 on epoch=213
03/13/2022 20:53:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.136825 on epoch=216
03/13/2022 20:54:04 - INFO - __main__ - Global step 650 Train loss 0.140578 Classification-F1 0.3136248948696384 on epoch=216
03/13/2022 20:54:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.168562 on epoch=219
03/13/2022 20:54:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.108935 on epoch=223
03/13/2022 20:54:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.112388 on epoch=226
03/13/2022 20:54:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.104495 on epoch=229
03/13/2022 20:54:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.109750 on epoch=233
03/13/2022 20:54:47 - INFO - __main__ - Global step 700 Train loss 0.120826 Classification-F1 0.3218637992831542 on epoch=233
03/13/2022 20:54:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.106027 on epoch=236
03/13/2022 20:54:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.156298 on epoch=239
03/13/2022 20:55:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.163145 on epoch=243
03/13/2022 20:55:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.117587 on epoch=246
03/13/2022 20:55:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.050822 on epoch=249
03/13/2022 20:55:30 - INFO - __main__ - Global step 750 Train loss 0.118776 Classification-F1 0.3006073153899241 on epoch=249
03/13/2022 20:55:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.171929 on epoch=253
03/13/2022 20:55:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.123715 on epoch=256
03/13/2022 20:55:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.052777 on epoch=259
03/13/2022 20:55:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.085915 on epoch=263
03/13/2022 20:55:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.063886 on epoch=266
03/13/2022 20:56:13 - INFO - __main__ - Global step 800 Train loss 0.099644 Classification-F1 0.32670807453416145 on epoch=266
03/13/2022 20:56:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.057158 on epoch=269
03/13/2022 20:56:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.104534 on epoch=273
03/13/2022 20:56:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.027448 on epoch=276
03/13/2022 20:56:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.034103 on epoch=279
03/13/2022 20:56:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.035181 on epoch=283
03/13/2022 20:56:55 - INFO - __main__ - Global step 850 Train loss 0.051685 Classification-F1 0.36363636363636365 on epoch=283
03/13/2022 20:57:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.072218 on epoch=286
03/13/2022 20:57:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.023320 on epoch=289
03/13/2022 20:57:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.021174 on epoch=293
03/13/2022 20:57:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.010222 on epoch=296
03/13/2022 20:57:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.013909 on epoch=299
03/13/2022 20:57:22 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:57:22 - INFO - __main__ - Printing 3 examples
03/13/2022 20:57:22 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/13/2022 20:57:22 - INFO - __main__ - ['hatespeech']
03/13/2022 20:57:22 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/13/2022 20:57:22 - INFO - __main__ - ['hatespeech']
03/13/2022 20:57:22 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/13/2022 20:57:22 - INFO - __main__ - ['hatespeech']
03/13/2022 20:57:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 20:57:22 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:57:22 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 20:57:22 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:57:22 - INFO - __main__ - Printing 3 examples
03/13/2022 20:57:22 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/13/2022 20:57:22 - INFO - __main__ - ['hatespeech']
03/13/2022 20:57:22 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/13/2022 20:57:22 - INFO - __main__ - ['hatespeech']
03/13/2022 20:57:22 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/13/2022 20:57:22 - INFO - __main__ - ['hatespeech']
03/13/2022 20:57:22 - INFO - __main__ - Tokenizing Input ...
03/13/2022 20:57:22 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:57:22 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 20:57:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 20:57:33 - INFO - __main__ - Starting training!
03/13/2022 20:57:38 - INFO - __main__ - Global step 900 Train loss 0.028168 Classification-F1 0.45920329670329674 on epoch=299
03/13/2022 20:57:38 - INFO - __main__ - save last model!
03/13/2022 20:57:45 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 20:57:46 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 20:57:46 - INFO - __main__ - Printing 3 examples
03/13/2022 20:57:46 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 20:57:46 - INFO - __main__ - ['normal']
03/13/2022 20:57:46 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 20:57:46 - INFO - __main__ - ['normal']
03/13/2022 20:57:46 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 20:57:46 - INFO - __main__ - ['normal']
03/13/2022 20:57:46 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 20:57:46 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:57:48 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 20:59:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_42_0.0005_8_predictions.txt
03/13/2022 20:59:20 - INFO - __main__ - Classification-F1 on test data: 0.0944
03/13/2022 20:59:22 - INFO - __main__ - prefix=hatexplain_16_42, lr=0.0005, bsz=8, dev_performance=0.5476190476190476, test_performance=0.09438661430476218
03/13/2022 20:59:22 - INFO - __main__ - Running ... prefix=hatexplain_16_42, lr=0.0003, bsz=8 ...
03/13/2022 20:59:23 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:59:23 - INFO - __main__ - Printing 3 examples
03/13/2022 20:59:23 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/13/2022 20:59:23 - INFO - __main__ - ['hatespeech']
03/13/2022 20:59:23 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/13/2022 20:59:23 - INFO - __main__ - ['hatespeech']
03/13/2022 20:59:23 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/13/2022 20:59:23 - INFO - __main__ - ['hatespeech']
03/13/2022 20:59:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 20:59:23 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:59:23 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 20:59:23 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 20:59:23 - INFO - __main__ - Printing 3 examples
03/13/2022 20:59:23 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/13/2022 20:59:23 - INFO - __main__ - ['hatespeech']
03/13/2022 20:59:23 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/13/2022 20:59:23 - INFO - __main__ - ['hatespeech']
03/13/2022 20:59:23 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/13/2022 20:59:23 - INFO - __main__ - ['hatespeech']
03/13/2022 20:59:23 - INFO - __main__ - Tokenizing Input ...
03/13/2022 20:59:23 - INFO - __main__ - Tokenizing Output ...
03/13/2022 20:59:23 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 20:59:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 20:59:36 - INFO - __main__ - Starting training!
03/13/2022 20:59:40 - INFO - __main__ - Step 10 Global step 10 Train loss 21.628637 on epoch=3
03/13/2022 20:59:45 - INFO - __main__ - Step 20 Global step 20 Train loss 13.991156 on epoch=6
03/13/2022 20:59:50 - INFO - __main__ - Step 30 Global step 30 Train loss 12.464751 on epoch=9
03/13/2022 20:59:55 - INFO - __main__ - Step 40 Global step 40 Train loss 11.628385 on epoch=13
03/13/2022 21:00:00 - INFO - __main__ - Step 50 Global step 50 Train loss 10.167814 on epoch=16
03/13/2022 21:00:10 - INFO - __main__ - Global step 50 Train loss 13.976148 Classification-F1 0.0 on epoch=16
03/13/2022 21:00:15 - INFO - __main__ - Step 60 Global step 60 Train loss 9.979187 on epoch=19
03/13/2022 21:00:20 - INFO - __main__ - Step 70 Global step 70 Train loss 9.471861 on epoch=23
03/13/2022 21:00:25 - INFO - __main__ - Step 80 Global step 80 Train loss 7.789921 on epoch=26
03/13/2022 21:00:30 - INFO - __main__ - Step 90 Global step 90 Train loss 1.728558 on epoch=29
03/13/2022 21:00:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.592889 on epoch=33
03/13/2022 21:00:36 - INFO - __main__ - Global step 100 Train loss 5.912484 Classification-F1 0.09836065573770493 on epoch=33
03/13/2022 21:00:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.508655 on epoch=36
03/13/2022 21:00:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.443283 on epoch=39
03/13/2022 21:00:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.399095 on epoch=43
03/13/2022 21:00:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.246572 on epoch=46
03/13/2022 21:01:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.317367 on epoch=49
03/13/2022 21:01:03 - INFO - __main__ - Global step 150 Train loss 0.382994 Classification-F1 0.20295182400445558 on epoch=49
03/13/2022 21:01:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.110602 on epoch=53
03/13/2022 21:01:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.072959 on epoch=56
03/13/2022 21:01:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.066458 on epoch=59
03/13/2022 21:01:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.059673 on epoch=63
03/13/2022 21:01:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.044466 on epoch=66
03/13/2022 21:01:30 - INFO - __main__ - Global step 200 Train loss 0.070831 Classification-F1 0.4272941970310391 on epoch=66
03/13/2022 21:01:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.015718 on epoch=69
03/13/2022 21:01:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.039855 on epoch=73
03/13/2022 21:01:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.011687 on epoch=76
03/13/2022 21:01:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.013937 on epoch=79
03/13/2022 21:01:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.017504 on epoch=83
03/13/2022 21:01:56 - INFO - __main__ - Global step 250 Train loss 0.019740 Classification-F1 0.42992942852690397 on epoch=83
03/13/2022 21:02:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.004002 on epoch=86
03/13/2022 21:02:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.051540 on epoch=89
03/13/2022 21:02:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.015391 on epoch=93
03/13/2022 21:02:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.005104 on epoch=96
03/13/2022 21:02:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.003125 on epoch=99
03/13/2022 21:02:23 - INFO - __main__ - Global step 300 Train loss 0.015833 Classification-F1 0.28205128205128205 on epoch=99
03/13/2022 21:02:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.002139 on epoch=103
03/13/2022 21:02:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.001348 on epoch=106
03/13/2022 21:02:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001053 on epoch=109
03/13/2022 21:02:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000829 on epoch=113
03/13/2022 21:02:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000574 on epoch=116
03/13/2022 21:02:48 - INFO - __main__ - Global step 350 Train loss 0.001188 Classification-F1 0.421115368241805 on epoch=116
03/13/2022 21:02:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000350 on epoch=119
03/13/2022 21:02:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000579 on epoch=123
03/13/2022 21:03:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000785 on epoch=126
03/13/2022 21:03:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000497 on epoch=129
03/13/2022 21:03:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000270 on epoch=133
03/13/2022 21:03:14 - INFO - __main__ - Global step 400 Train loss 0.000496 Classification-F1 0.2913558720010333 on epoch=133
03/13/2022 21:03:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000241 on epoch=136
03/13/2022 21:03:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000113 on epoch=139
03/13/2022 21:03:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001307 on epoch=143
03/13/2022 21:03:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000640 on epoch=146
03/13/2022 21:03:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.090073 on epoch=149
03/13/2022 21:03:40 - INFO - __main__ - Global step 450 Train loss 0.018475 Classification-F1 0.480835001668335 on epoch=149
03/13/2022 21:03:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.012418 on epoch=153
03/13/2022 21:03:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.010089 on epoch=156
03/13/2022 21:03:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000306 on epoch=159
03/13/2022 21:04:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000238 on epoch=163
03/13/2022 21:04:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000354 on epoch=166
03/13/2022 21:04:07 - INFO - __main__ - Global step 500 Train loss 0.004681 Classification-F1 0.40317460317460313 on epoch=166
03/13/2022 21:04:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000198 on epoch=169
03/13/2022 21:04:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000227 on epoch=173
03/13/2022 21:04:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000653 on epoch=176
03/13/2022 21:04:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001448 on epoch=179
03/13/2022 21:04:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000334 on epoch=183
03/13/2022 21:04:32 - INFO - __main__ - Global step 550 Train loss 0.000572 Classification-F1 0.4384571521668296 on epoch=183
03/13/2022 21:04:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000244 on epoch=186
03/13/2022 21:04:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000085 on epoch=189
03/13/2022 21:04:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000063 on epoch=193
03/13/2022 21:04:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000195 on epoch=196
03/13/2022 21:04:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000084 on epoch=199
03/13/2022 21:04:58 - INFO - __main__ - Global step 600 Train loss 0.000134 Classification-F1 0.4880194880194881 on epoch=199
03/13/2022 21:05:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000084 on epoch=203
03/13/2022 21:05:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000050 on epoch=206
03/13/2022 21:05:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000626 on epoch=209
03/13/2022 21:05:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000028 on epoch=213
03/13/2022 21:05:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000048 on epoch=216
03/13/2022 21:05:25 - INFO - __main__ - Global step 650 Train loss 0.000167 Classification-F1 0.4402597402597402 on epoch=216
03/13/2022 21:05:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000085 on epoch=219
03/13/2022 21:05:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000096 on epoch=223
03/13/2022 21:05:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000114 on epoch=226
03/13/2022 21:05:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000075 on epoch=229
03/13/2022 21:05:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000246 on epoch=233
03/13/2022 21:05:51 - INFO - __main__ - Global step 700 Train loss 0.000123 Classification-F1 0.4211503169836503 on epoch=233
03/13/2022 21:05:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000042 on epoch=236
03/13/2022 21:06:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000186 on epoch=239
03/13/2022 21:06:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.006625 on epoch=243
03/13/2022 21:06:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000232 on epoch=246
03/13/2022 21:06:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000078 on epoch=249
03/13/2022 21:06:17 - INFO - __main__ - Global step 750 Train loss 0.001433 Classification-F1 0.34419642857142857 on epoch=249
03/13/2022 21:06:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001470 on epoch=253
03/13/2022 21:06:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.001951 on epoch=256
03/13/2022 21:06:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000312 on epoch=259
03/13/2022 21:06:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000038 on epoch=263
03/13/2022 21:06:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000071 on epoch=266
03/13/2022 21:06:44 - INFO - __main__ - Global step 800 Train loss 0.000768 Classification-F1 0.3306513409961686 on epoch=266
03/13/2022 21:06:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000093 on epoch=269
03/13/2022 21:06:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000146 on epoch=273
03/13/2022 21:06:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000034 on epoch=276
03/13/2022 21:07:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.005240 on epoch=279
03/13/2022 21:07:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000141 on epoch=283
03/13/2022 21:07:10 - INFO - __main__ - Global step 850 Train loss 0.001131 Classification-F1 0.42425183804494154 on epoch=283
03/13/2022 21:07:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000207 on epoch=286
03/13/2022 21:07:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000202 on epoch=289
03/13/2022 21:07:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000019 on epoch=293
03/13/2022 21:07:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000047 on epoch=296
03/13/2022 21:07:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000012 on epoch=299
03/13/2022 21:07:36 - INFO - __main__ - Global step 900 Train loss 0.000098 Classification-F1 0.4477513227513228 on epoch=299
03/13/2022 21:07:36 - INFO - __main__ - save last model!
03/13/2022 21:07:36 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:07:36 - INFO - __main__ - Printing 3 examples
03/13/2022 21:07:36 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/13/2022 21:07:36 - INFO - __main__ - ['hatespeech']
03/13/2022 21:07:36 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/13/2022 21:07:36 - INFO - __main__ - ['hatespeech']
03/13/2022 21:07:36 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/13/2022 21:07:36 - INFO - __main__ - ['hatespeech']
03/13/2022 21:07:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 21:07:36 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:07:36 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 21:07:36 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:07:36 - INFO - __main__ - Printing 3 examples
03/13/2022 21:07:36 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/13/2022 21:07:36 - INFO - __main__ - ['hatespeech']
03/13/2022 21:07:36 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/13/2022 21:07:36 - INFO - __main__ - ['hatespeech']
03/13/2022 21:07:36 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/13/2022 21:07:36 - INFO - __main__ - ['hatespeech']
03/13/2022 21:07:36 - INFO - __main__ - Tokenizing Input ...
03/13/2022 21:07:36 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:07:36 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 21:07:43 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 21:07:44 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 21:07:44 - INFO - __main__ - Printing 3 examples
03/13/2022 21:07:44 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 21:07:44 - INFO - __main__ - ['normal']
03/13/2022 21:07:44 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 21:07:44 - INFO - __main__ - ['normal']
03/13/2022 21:07:44 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 21:07:44 - INFO - __main__ - ['normal']
03/13/2022 21:07:44 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 21:07:45 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:07:47 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 21:07:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 21:07:49 - INFO - __main__ - Starting training!
03/13/2022 21:08:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_42_0.0003_8_predictions.txt
03/13/2022 21:08:29 - INFO - __main__ - Classification-F1 on test data: 0.0924
03/13/2022 21:08:30 - INFO - __main__ - prefix=hatexplain_16_42, lr=0.0003, bsz=8, dev_performance=0.4880194880194881, test_performance=0.09238227967301929
03/13/2022 21:08:30 - INFO - __main__ - Running ... prefix=hatexplain_16_42, lr=0.0002, bsz=8 ...
03/13/2022 21:08:30 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:08:30 - INFO - __main__ - Printing 3 examples
03/13/2022 21:08:30 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/13/2022 21:08:30 - INFO - __main__ - ['hatespeech']
03/13/2022 21:08:30 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/13/2022 21:08:30 - INFO - __main__ - ['hatespeech']
03/13/2022 21:08:30 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/13/2022 21:08:30 - INFO - __main__ - ['hatespeech']
03/13/2022 21:08:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 21:08:30 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:08:31 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 21:08:31 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:08:31 - INFO - __main__ - Printing 3 examples
03/13/2022 21:08:31 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/13/2022 21:08:31 - INFO - __main__ - ['hatespeech']
03/13/2022 21:08:31 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/13/2022 21:08:31 - INFO - __main__ - ['hatespeech']
03/13/2022 21:08:31 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/13/2022 21:08:31 - INFO - __main__ - ['hatespeech']
03/13/2022 21:08:31 - INFO - __main__ - Tokenizing Input ...
03/13/2022 21:08:31 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:08:31 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 21:08:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 21:08:41 - INFO - __main__ - Starting training!
03/13/2022 21:08:45 - INFO - __main__ - Step 10 Global step 10 Train loss 22.019314 on epoch=3
03/13/2022 21:08:51 - INFO - __main__ - Step 20 Global step 20 Train loss 19.328650 on epoch=6
03/13/2022 21:08:56 - INFO - __main__ - Step 30 Global step 30 Train loss 16.626770 on epoch=9
03/13/2022 21:09:01 - INFO - __main__ - Step 40 Global step 40 Train loss 16.045195 on epoch=13
03/13/2022 21:09:06 - INFO - __main__ - Step 50 Global step 50 Train loss 12.589307 on epoch=16
03/13/2022 21:09:16 - INFO - __main__ - Global step 50 Train loss 17.321846 Classification-F1 0.0 on epoch=16
03/13/2022 21:09:22 - INFO - __main__ - Step 60 Global step 60 Train loss 11.459547 on epoch=19
03/13/2022 21:09:27 - INFO - __main__ - Step 70 Global step 70 Train loss 10.992337 on epoch=23
03/13/2022 21:09:32 - INFO - __main__ - Step 80 Global step 80 Train loss 10.267084 on epoch=26
03/13/2022 21:09:37 - INFO - __main__ - Step 90 Global step 90 Train loss 10.082750 on epoch=29
03/13/2022 21:09:42 - INFO - __main__ - Step 100 Global step 100 Train loss 9.446356 on epoch=33
03/13/2022 21:09:44 - INFO - __main__ - Global step 100 Train loss 10.449615 Classification-F1 0.0 on epoch=33
03/13/2022 21:09:49 - INFO - __main__ - Step 110 Global step 110 Train loss 9.077116 on epoch=36
03/13/2022 21:09:54 - INFO - __main__ - Step 120 Global step 120 Train loss 8.359447 on epoch=39
03/13/2022 21:09:59 - INFO - __main__ - Step 130 Global step 130 Train loss 7.434390 on epoch=43
03/13/2022 21:10:04 - INFO - __main__ - Step 140 Global step 140 Train loss 6.431546 on epoch=46
03/13/2022 21:10:10 - INFO - __main__ - Step 150 Global step 150 Train loss 5.150754 on epoch=49
03/13/2022 21:10:11 - INFO - __main__ - Global step 150 Train loss 7.290650 Classification-F1 0.0 on epoch=49
03/13/2022 21:10:16 - INFO - __main__ - Step 160 Global step 160 Train loss 3.726555 on epoch=53
03/13/2022 21:10:21 - INFO - __main__ - Step 170 Global step 170 Train loss 3.056206 on epoch=56
03/13/2022 21:10:26 - INFO - __main__ - Step 180 Global step 180 Train loss 3.220949 on epoch=59
03/13/2022 21:10:31 - INFO - __main__ - Step 190 Global step 190 Train loss 2.269006 on epoch=63
03/13/2022 21:10:36 - INFO - __main__ - Step 200 Global step 200 Train loss 2.451390 on epoch=66
03/13/2022 21:10:37 - INFO - __main__ - Global step 200 Train loss 2.944821 Classification-F1 0.18809318377911996 on epoch=66
03/13/2022 21:10:43 - INFO - __main__ - Step 210 Global step 210 Train loss 2.605864 on epoch=69
03/13/2022 21:10:48 - INFO - __main__ - Step 220 Global step 220 Train loss 2.233434 on epoch=73
03/13/2022 21:10:53 - INFO - __main__ - Step 230 Global step 230 Train loss 1.958382 on epoch=76
03/13/2022 21:10:58 - INFO - __main__ - Step 240 Global step 240 Train loss 2.530587 on epoch=79
03/13/2022 21:11:03 - INFO - __main__ - Step 250 Global step 250 Train loss 2.601770 on epoch=83
03/13/2022 21:11:04 - INFO - __main__ - Global step 250 Train loss 2.386007 Classification-F1 0.30205415499533145 on epoch=83
03/13/2022 21:11:09 - INFO - __main__ - Step 260 Global step 260 Train loss 1.332399 on epoch=86
03/13/2022 21:11:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.706196 on epoch=89
03/13/2022 21:11:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.465323 on epoch=93
03/13/2022 21:11:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.453509 on epoch=96
03/13/2022 21:11:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.353503 on epoch=99
03/13/2022 21:11:30 - INFO - __main__ - Global step 300 Train loss 0.662186 Classification-F1 0.22266139657444003 on epoch=99
03/13/2022 21:11:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.301827 on epoch=103
03/13/2022 21:11:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.209551 on epoch=106
03/13/2022 21:11:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.210882 on epoch=109
03/13/2022 21:11:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.122406 on epoch=113
03/13/2022 21:11:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.156017 on epoch=116
03/13/2022 21:11:56 - INFO - __main__ - Global step 350 Train loss 0.200137 Classification-F1 0.2878787878787879 on epoch=116
03/13/2022 21:12:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.120039 on epoch=119
03/13/2022 21:12:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.103802 on epoch=123
03/13/2022 21:12:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.079146 on epoch=126
03/13/2022 21:12:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.057805 on epoch=129
03/13/2022 21:12:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.065192 on epoch=133
03/13/2022 21:12:22 - INFO - __main__ - Global step 400 Train loss 0.085197 Classification-F1 0.36226575357010143 on epoch=133
03/13/2022 21:12:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.035857 on epoch=136
03/13/2022 21:12:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.022371 on epoch=139
03/13/2022 21:12:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.039461 on epoch=143
03/13/2022 21:12:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.015202 on epoch=146
03/13/2022 21:12:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.004829 on epoch=149
03/13/2022 21:12:48 - INFO - __main__ - Global step 450 Train loss 0.023544 Classification-F1 0.1906766917293233 on epoch=149
03/13/2022 21:12:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.014257 on epoch=153
03/13/2022 21:12:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.010213 on epoch=156
03/13/2022 21:13:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.012585 on epoch=159
03/13/2022 21:13:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002777 on epoch=163
03/13/2022 21:13:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.010163 on epoch=166
03/13/2022 21:13:14 - INFO - __main__ - Global step 500 Train loss 0.009999 Classification-F1 0.25700483091787435 on epoch=166
03/13/2022 21:13:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002387 on epoch=169
03/13/2022 21:13:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.010163 on epoch=173
03/13/2022 21:13:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.007092 on epoch=176
03/13/2022 21:13:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.007533 on epoch=179
03/13/2022 21:13:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.011372 on epoch=183
03/13/2022 21:13:40 - INFO - __main__ - Global step 550 Train loss 0.007709 Classification-F1 0.2393699302018487 on epoch=183
03/13/2022 21:13:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001962 on epoch=186
03/13/2022 21:13:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.052355 on epoch=189
03/13/2022 21:13:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.013660 on epoch=193
03/13/2022 21:14:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.004274 on epoch=196
03/13/2022 21:14:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001257 on epoch=199
03/13/2022 21:14:06 - INFO - __main__ - Global step 600 Train loss 0.014701 Classification-F1 0.22217868338557994 on epoch=199
03/13/2022 21:14:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001343 on epoch=203
03/13/2022 21:14:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000590 on epoch=206
03/13/2022 21:14:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.008129 on epoch=209
03/13/2022 21:14:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.052665 on epoch=213
03/13/2022 21:14:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.095599 on epoch=216
03/13/2022 21:14:32 - INFO - __main__ - Global step 650 Train loss 0.031665 Classification-F1 0.3138211382113821 on epoch=216
03/13/2022 21:14:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.037030 on epoch=219
03/13/2022 21:14:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.039364 on epoch=223
03/13/2022 21:14:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.027566 on epoch=226
03/13/2022 21:14:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.006403 on epoch=229
03/13/2022 21:14:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.001068 on epoch=233
03/13/2022 21:14:57 - INFO - __main__ - Global step 700 Train loss 0.022286 Classification-F1 0.44035087719298244 on epoch=233
03/13/2022 21:15:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000468 on epoch=236
03/13/2022 21:15:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.002520 on epoch=239
03/13/2022 21:15:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.003829 on epoch=243
03/13/2022 21:15:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.003151 on epoch=246
03/13/2022 21:15:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001674 on epoch=249
03/13/2022 21:15:24 - INFO - __main__ - Global step 750 Train loss 0.002328 Classification-F1 0.310806334062148 on epoch=249
03/13/2022 21:15:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000250 on epoch=253
03/13/2022 21:15:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000799 on epoch=256
03/13/2022 21:15:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001734 on epoch=259
03/13/2022 21:15:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000356 on epoch=263
03/13/2022 21:15:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000424 on epoch=266
03/13/2022 21:15:50 - INFO - __main__ - Global step 800 Train loss 0.000713 Classification-F1 0.2886002886002886 on epoch=266
03/13/2022 21:15:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000372 on epoch=269
03/13/2022 21:16:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000349 on epoch=273
03/13/2022 21:16:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000254 on epoch=276
03/13/2022 21:16:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000959 on epoch=279
03/13/2022 21:16:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000859 on epoch=283
03/13/2022 21:16:16 - INFO - __main__ - Global step 850 Train loss 0.000559 Classification-F1 0.3333333333333333 on epoch=283
03/13/2022 21:16:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000205 on epoch=286
03/13/2022 21:16:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000120 on epoch=289
03/13/2022 21:16:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000209 on epoch=293
03/13/2022 21:16:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000312 on epoch=296
03/13/2022 21:16:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000136 on epoch=299
03/13/2022 21:16:42 - INFO - __main__ - Global step 900 Train loss 0.000196 Classification-F1 0.3670940170940171 on epoch=299
03/13/2022 21:16:42 - INFO - __main__ - save last model!
03/13/2022 21:16:42 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:16:42 - INFO - __main__ - Printing 3 examples
03/13/2022 21:16:42 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/13/2022 21:16:42 - INFO - __main__ - ['hatespeech']
03/13/2022 21:16:42 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/13/2022 21:16:42 - INFO - __main__ - ['hatespeech']
03/13/2022 21:16:42 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/13/2022 21:16:42 - INFO - __main__ - ['hatespeech']
03/13/2022 21:16:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 21:16:42 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:16:42 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 21:16:42 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:16:42 - INFO - __main__ - Printing 3 examples
03/13/2022 21:16:42 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/13/2022 21:16:42 - INFO - __main__ - ['hatespeech']
03/13/2022 21:16:42 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/13/2022 21:16:42 - INFO - __main__ - ['hatespeech']
03/13/2022 21:16:42 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/13/2022 21:16:42 - INFO - __main__ - ['hatespeech']
03/13/2022 21:16:42 - INFO - __main__ - Tokenizing Input ...
03/13/2022 21:16:42 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:16:42 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 21:16:49 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 21:16:50 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 21:16:50 - INFO - __main__ - Printing 3 examples
03/13/2022 21:16:50 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 21:16:50 - INFO - __main__ - ['normal']
03/13/2022 21:16:50 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 21:16:50 - INFO - __main__ - ['normal']
03/13/2022 21:16:50 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 21:16:50 - INFO - __main__ - ['normal']
03/13/2022 21:16:50 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 21:16:51 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:16:53 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 21:16:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 21:16:55 - INFO - __main__ - Starting training!
03/13/2022 21:17:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_42_0.0002_8_predictions.txt
03/13/2022 21:17:23 - INFO - __main__ - Classification-F1 on test data: 0.1150
03/13/2022 21:17:24 - INFO - __main__ - prefix=hatexplain_16_42, lr=0.0002, bsz=8, dev_performance=0.44035087719298244, test_performance=0.11495436967844577
03/13/2022 21:17:24 - INFO - __main__ - Running ... prefix=hatexplain_16_42, lr=0.0001, bsz=8 ...
03/13/2022 21:17:24 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:17:24 - INFO - __main__ - Printing 3 examples
03/13/2022 21:17:24 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/13/2022 21:17:24 - INFO - __main__ - ['hatespeech']
03/13/2022 21:17:24 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/13/2022 21:17:24 - INFO - __main__ - ['hatespeech']
03/13/2022 21:17:24 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/13/2022 21:17:24 - INFO - __main__ - ['hatespeech']
03/13/2022 21:17:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 21:17:25 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:17:25 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 21:17:25 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:17:25 - INFO - __main__ - Printing 3 examples
03/13/2022 21:17:25 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/13/2022 21:17:25 - INFO - __main__ - ['hatespeech']
03/13/2022 21:17:25 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/13/2022 21:17:25 - INFO - __main__ - ['hatespeech']
03/13/2022 21:17:25 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/13/2022 21:17:25 - INFO - __main__ - ['hatespeech']
03/13/2022 21:17:25 - INFO - __main__ - Tokenizing Input ...
03/13/2022 21:17:25 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:17:25 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 21:17:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 21:17:35 - INFO - __main__ - Starting training!
03/13/2022 21:17:39 - INFO - __main__ - Step 10 Global step 10 Train loss 21.550594 on epoch=3
03/13/2022 21:17:44 - INFO - __main__ - Step 20 Global step 20 Train loss 20.518337 on epoch=6
03/13/2022 21:17:49 - INFO - __main__ - Step 30 Global step 30 Train loss 16.698238 on epoch=9
03/13/2022 21:17:54 - INFO - __main__ - Step 40 Global step 40 Train loss 13.935153 on epoch=13
03/13/2022 21:17:59 - INFO - __main__ - Step 50 Global step 50 Train loss 13.054075 on epoch=16
03/13/2022 21:18:13 - INFO - __main__ - Global step 50 Train loss 17.151281 Classification-F1 0.0 on epoch=16
03/13/2022 21:18:19 - INFO - __main__ - Step 60 Global step 60 Train loss 13.034933 on epoch=19
03/13/2022 21:18:24 - INFO - __main__ - Step 70 Global step 70 Train loss 12.675761 on epoch=23
03/13/2022 21:18:29 - INFO - __main__ - Step 80 Global step 80 Train loss 11.678720 on epoch=26
03/13/2022 21:18:34 - INFO - __main__ - Step 90 Global step 90 Train loss 11.657497 on epoch=29
03/13/2022 21:18:39 - INFO - __main__ - Step 100 Global step 100 Train loss 11.257556 on epoch=33
03/13/2022 21:18:52 - INFO - __main__ - Global step 100 Train loss 12.060894 Classification-F1 0.0 on epoch=33
03/13/2022 21:18:57 - INFO - __main__ - Step 110 Global step 110 Train loss 10.885130 on epoch=36
03/13/2022 21:19:02 - INFO - __main__ - Step 120 Global step 120 Train loss 10.625172 on epoch=39
03/13/2022 21:19:07 - INFO - __main__ - Step 130 Global step 130 Train loss 10.382820 on epoch=43
03/13/2022 21:19:12 - INFO - __main__ - Step 140 Global step 140 Train loss 10.392998 on epoch=46
03/13/2022 21:19:17 - INFO - __main__ - Step 150 Global step 150 Train loss 10.084262 on epoch=49
03/13/2022 21:19:20 - INFO - __main__ - Global step 150 Train loss 10.474076 Classification-F1 0.0 on epoch=49
03/13/2022 21:19:25 - INFO - __main__ - Step 160 Global step 160 Train loss 10.051940 on epoch=53
03/13/2022 21:19:30 - INFO - __main__ - Step 170 Global step 170 Train loss 9.705655 on epoch=56
03/13/2022 21:19:35 - INFO - __main__ - Step 180 Global step 180 Train loss 9.111604 on epoch=59
03/13/2022 21:19:40 - INFO - __main__ - Step 190 Global step 190 Train loss 9.160499 on epoch=63
03/13/2022 21:19:45 - INFO - __main__ - Step 200 Global step 200 Train loss 8.468416 on epoch=66
03/13/2022 21:19:47 - INFO - __main__ - Global step 200 Train loss 9.299623 Classification-F1 0.0 on epoch=66
03/13/2022 21:19:52 - INFO - __main__ - Step 210 Global step 210 Train loss 8.116040 on epoch=69
03/13/2022 21:19:56 - INFO - __main__ - Step 220 Global step 220 Train loss 7.979961 on epoch=73
03/13/2022 21:20:02 - INFO - __main__ - Step 230 Global step 230 Train loss 7.391511 on epoch=76
03/13/2022 21:20:06 - INFO - __main__ - Step 240 Global step 240 Train loss 6.635457 on epoch=79
03/13/2022 21:20:11 - INFO - __main__ - Step 250 Global step 250 Train loss 6.689532 on epoch=83
03/13/2022 21:20:13 - INFO - __main__ - Global step 250 Train loss 7.362500 Classification-F1 0.0 on epoch=83
03/13/2022 21:20:18 - INFO - __main__ - Step 260 Global step 260 Train loss 5.820060 on epoch=86
03/13/2022 21:20:23 - INFO - __main__ - Step 270 Global step 270 Train loss 4.248547 on epoch=89
03/13/2022 21:20:28 - INFO - __main__ - Step 280 Global step 280 Train loss 4.384259 on epoch=93
03/13/2022 21:20:33 - INFO - __main__ - Step 290 Global step 290 Train loss 3.547988 on epoch=96
03/13/2022 21:20:38 - INFO - __main__ - Step 300 Global step 300 Train loss 2.672313 on epoch=99
03/13/2022 21:20:46 - INFO - __main__ - Global step 300 Train loss 4.134633 Classification-F1 0.17984330484330485 on epoch=99
03/13/2022 21:20:52 - INFO - __main__ - Step 310 Global step 310 Train loss 2.644780 on epoch=103
03/13/2022 21:20:57 - INFO - __main__ - Step 320 Global step 320 Train loss 2.954321 on epoch=106
03/13/2022 21:21:02 - INFO - __main__ - Step 330 Global step 330 Train loss 3.217994 on epoch=109
03/13/2022 21:21:07 - INFO - __main__ - Step 340 Global step 340 Train loss 2.666269 on epoch=113
03/13/2022 21:21:12 - INFO - __main__ - Step 350 Global step 350 Train loss 2.795246 on epoch=116
03/13/2022 21:21:13 - INFO - __main__ - Global step 350 Train loss 2.855722 Classification-F1 0.2832080200501253 on epoch=116
03/13/2022 21:21:19 - INFO - __main__ - Step 360 Global step 360 Train loss 2.522575 on epoch=119
03/13/2022 21:21:24 - INFO - __main__ - Step 370 Global step 370 Train loss 2.990366 on epoch=123
03/13/2022 21:21:29 - INFO - __main__ - Step 380 Global step 380 Train loss 2.279944 on epoch=126
03/13/2022 21:21:34 - INFO - __main__ - Step 390 Global step 390 Train loss 2.397040 on epoch=129
03/13/2022 21:21:39 - INFO - __main__ - Step 400 Global step 400 Train loss 2.182289 on epoch=133
03/13/2022 21:21:40 - INFO - __main__ - Global step 400 Train loss 2.474442 Classification-F1 0.19707602339181285 on epoch=133
03/13/2022 21:21:45 - INFO - __main__ - Step 410 Global step 410 Train loss 2.646623 on epoch=136
03/13/2022 21:21:50 - INFO - __main__ - Step 420 Global step 420 Train loss 2.516292 on epoch=139
03/13/2022 21:21:55 - INFO - __main__ - Step 430 Global step 430 Train loss 2.539910 on epoch=143
03/13/2022 21:22:00 - INFO - __main__ - Step 440 Global step 440 Train loss 2.238711 on epoch=146
03/13/2022 21:22:05 - INFO - __main__ - Step 450 Global step 450 Train loss 2.548794 on epoch=149
03/13/2022 21:22:06 - INFO - __main__ - Global step 450 Train loss 2.498066 Classification-F1 0.30307653190948386 on epoch=149
03/13/2022 21:22:12 - INFO - __main__ - Step 460 Global step 460 Train loss 1.956997 on epoch=153
03/13/2022 21:22:17 - INFO - __main__ - Step 470 Global step 470 Train loss 2.923124 on epoch=156
03/13/2022 21:22:22 - INFO - __main__ - Step 480 Global step 480 Train loss 2.524955 on epoch=159
03/13/2022 21:22:27 - INFO - __main__ - Step 490 Global step 490 Train loss 1.921654 on epoch=163
03/13/2022 21:22:32 - INFO - __main__ - Step 500 Global step 500 Train loss 2.036661 on epoch=166
03/13/2022 21:22:33 - INFO - __main__ - Global step 500 Train loss 2.272678 Classification-F1 0.20908004778972522 on epoch=166
03/13/2022 21:22:38 - INFO - __main__ - Step 510 Global step 510 Train loss 2.069386 on epoch=169
03/13/2022 21:22:43 - INFO - __main__ - Step 520 Global step 520 Train loss 1.919736 on epoch=173
03/13/2022 21:22:48 - INFO - __main__ - Step 530 Global step 530 Train loss 2.071003 on epoch=176
03/13/2022 21:22:53 - INFO - __main__ - Step 540 Global step 540 Train loss 1.763210 on epoch=179
03/13/2022 21:22:58 - INFO - __main__ - Step 550 Global step 550 Train loss 1.943820 on epoch=183
03/13/2022 21:22:59 - INFO - __main__ - Global step 550 Train loss 1.953431 Classification-F1 0.22303030303030302 on epoch=183
03/13/2022 21:23:04 - INFO - __main__ - Step 560 Global step 560 Train loss 1.980852 on epoch=186
03/13/2022 21:23:09 - INFO - __main__ - Step 570 Global step 570 Train loss 1.599797 on epoch=189
03/13/2022 21:23:14 - INFO - __main__ - Step 580 Global step 580 Train loss 2.288161 on epoch=193
03/13/2022 21:23:19 - INFO - __main__ - Step 590 Global step 590 Train loss 2.441813 on epoch=196
03/13/2022 21:23:24 - INFO - __main__ - Step 600 Global step 600 Train loss 1.774840 on epoch=199
03/13/2022 21:23:25 - INFO - __main__ - Global step 600 Train loss 2.017093 Classification-F1 0.2748171368861024 on epoch=199
03/13/2022 21:23:30 - INFO - __main__ - Step 610 Global step 610 Train loss 2.685961 on epoch=203
03/13/2022 21:23:35 - INFO - __main__ - Step 620 Global step 620 Train loss 2.337285 on epoch=206
03/13/2022 21:23:40 - INFO - __main__ - Step 630 Global step 630 Train loss 1.848014 on epoch=209
03/13/2022 21:23:45 - INFO - __main__ - Step 640 Global step 640 Train loss 1.748090 on epoch=213
03/13/2022 21:23:50 - INFO - __main__ - Step 650 Global step 650 Train loss 2.068822 on epoch=216
03/13/2022 21:24:00 - INFO - __main__ - Global step 650 Train loss 2.137635 Classification-F1 0.18555008210180624 on epoch=216
03/13/2022 21:24:05 - INFO - __main__ - Step 660 Global step 660 Train loss 1.751177 on epoch=219
03/13/2022 21:24:10 - INFO - __main__ - Step 670 Global step 670 Train loss 1.606142 on epoch=223
03/13/2022 21:24:15 - INFO - __main__ - Step 680 Global step 680 Train loss 1.739004 on epoch=226
03/13/2022 21:24:20 - INFO - __main__ - Step 690 Global step 690 Train loss 1.721042 on epoch=229
03/13/2022 21:24:25 - INFO - __main__ - Step 700 Global step 700 Train loss 1.647527 on epoch=233
03/13/2022 21:24:35 - INFO - __main__ - Global step 700 Train loss 1.692979 Classification-F1 0.22822931785195938 on epoch=233
03/13/2022 21:24:40 - INFO - __main__ - Step 710 Global step 710 Train loss 1.921083 on epoch=236
03/13/2022 21:24:45 - INFO - __main__ - Step 720 Global step 720 Train loss 1.624617 on epoch=239
03/13/2022 21:24:50 - INFO - __main__ - Step 730 Global step 730 Train loss 1.445124 on epoch=243
03/13/2022 21:24:55 - INFO - __main__ - Step 740 Global step 740 Train loss 1.782364 on epoch=246
03/13/2022 21:25:00 - INFO - __main__ - Step 750 Global step 750 Train loss 1.872408 on epoch=249
03/13/2022 21:25:10 - INFO - __main__ - Global step 750 Train loss 1.729119 Classification-F1 0.27629870129870127 on epoch=249
03/13/2022 21:25:15 - INFO - __main__ - Step 760 Global step 760 Train loss 1.547168 on epoch=253
03/13/2022 21:25:20 - INFO - __main__ - Step 770 Global step 770 Train loss 1.861517 on epoch=256
03/13/2022 21:25:25 - INFO - __main__ - Step 780 Global step 780 Train loss 1.514983 on epoch=259
03/13/2022 21:25:30 - INFO - __main__ - Step 790 Global step 790 Train loss 1.206264 on epoch=263
03/13/2022 21:25:35 - INFO - __main__ - Step 800 Global step 800 Train loss 1.249413 on epoch=266
03/13/2022 21:25:45 - INFO - __main__ - Global step 800 Train loss 1.475869 Classification-F1 0.19062901155327344 on epoch=266
03/13/2022 21:25:49 - INFO - __main__ - Step 810 Global step 810 Train loss 1.149627 on epoch=269
03/13/2022 21:25:54 - INFO - __main__ - Step 820 Global step 820 Train loss 1.246961 on epoch=273
03/13/2022 21:25:59 - INFO - __main__ - Step 830 Global step 830 Train loss 1.229878 on epoch=276
03/13/2022 21:26:04 - INFO - __main__ - Step 840 Global step 840 Train loss 1.148152 on epoch=279
03/13/2022 21:26:09 - INFO - __main__ - Step 850 Global step 850 Train loss 1.435407 on epoch=283
03/13/2022 21:26:19 - INFO - __main__ - Global step 850 Train loss 1.242005 Classification-F1 0.21598202824133503 on epoch=283
03/13/2022 21:26:24 - INFO - __main__ - Step 860 Global step 860 Train loss 1.403697 on epoch=286
03/13/2022 21:26:29 - INFO - __main__ - Step 870 Global step 870 Train loss 1.261598 on epoch=289
03/13/2022 21:26:34 - INFO - __main__ - Step 880 Global step 880 Train loss 1.176746 on epoch=293
03/13/2022 21:26:39 - INFO - __main__ - Step 890 Global step 890 Train loss 1.369427 on epoch=296
03/13/2022 21:26:44 - INFO - __main__ - Step 900 Global step 900 Train loss 1.155382 on epoch=299
03/13/2022 21:26:45 - INFO - __main__ - Global step 900 Train loss 1.273370 Classification-F1 0.3292847503373819 on epoch=299
03/13/2022 21:26:45 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:26:45 - INFO - __main__ - Printing 3 examples
03/13/2022 21:26:45 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/13/2022 21:26:45 - INFO - __main__ - ['offensive']
03/13/2022 21:26:45 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/13/2022 21:26:45 - INFO - __main__ - ['offensive']
03/13/2022 21:26:45 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/13/2022 21:26:45 - INFO - __main__ - ['offensive']
03/13/2022 21:26:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 21:26:45 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:26:45 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 21:26:45 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:26:45 - INFO - __main__ - Printing 3 examples
03/13/2022 21:26:45 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/13/2022 21:26:45 - INFO - __main__ - ['offensive']
03/13/2022 21:26:45 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/13/2022 21:26:45 - INFO - __main__ - ['offensive']
03/13/2022 21:26:45 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 
03/13/2022 21:26:45 - INFO - __main__ - ['offensive']
03/13/2022 21:26:45 - INFO - __main__ - Tokenizing Input ...
03/13/2022 21:26:45 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:26:45 - INFO - __main__ - save last model!
03/13/2022 21:26:45 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 21:26:52 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 21:26:53 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 21:26:53 - INFO - __main__ - Printing 3 examples
03/13/2022 21:26:53 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 21:26:53 - INFO - __main__ - ['normal']
03/13/2022 21:26:53 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 21:26:53 - INFO - __main__ - ['normal']
03/13/2022 21:26:53 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 21:26:53 - INFO - __main__ - ['normal']
03/13/2022 21:26:53 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 21:26:54 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:26:56 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 21:26:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 21:26:58 - INFO - __main__ - Starting training!
03/13/2022 21:27:58 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_42_0.0001_8_predictions.txt
03/13/2022 21:27:59 - INFO - __main__ - Classification-F1 on test data: 0.1318
03/13/2022 21:27:59 - INFO - __main__ - prefix=hatexplain_16_42, lr=0.0001, bsz=8, dev_performance=0.3292847503373819, test_performance=0.13184765991552608
03/13/2022 21:27:59 - INFO - __main__ - Running ... prefix=hatexplain_16_87, lr=0.0005, bsz=8 ...
03/13/2022 21:28:00 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:28:00 - INFO - __main__ - Printing 3 examples
03/13/2022 21:28:00 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/13/2022 21:28:00 - INFO - __main__ - ['offensive']
03/13/2022 21:28:00 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/13/2022 21:28:00 - INFO - __main__ - ['offensive']
03/13/2022 21:28:00 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/13/2022 21:28:00 - INFO - __main__ - ['offensive']
03/13/2022 21:28:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 21:28:00 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:28:00 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 21:28:00 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:28:00 - INFO - __main__ - Printing 3 examples
03/13/2022 21:28:00 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/13/2022 21:28:00 - INFO - __main__ - ['offensive']
03/13/2022 21:28:00 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/13/2022 21:28:00 - INFO - __main__ - ['offensive']
03/13/2022 21:28:00 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 
03/13/2022 21:28:00 - INFO - __main__ - ['offensive']
03/13/2022 21:28:00 - INFO - __main__ - Tokenizing Input ...
03/13/2022 21:28:00 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:28:00 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 21:28:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 21:28:13 - INFO - __main__ - Starting training!
03/13/2022 21:28:17 - INFO - __main__ - Step 10 Global step 10 Train loss 20.958618 on epoch=3
03/13/2022 21:28:22 - INFO - __main__ - Step 20 Global step 20 Train loss 14.714088 on epoch=6
03/13/2022 21:28:26 - INFO - __main__ - Step 30 Global step 30 Train loss 12.637248 on epoch=9
03/13/2022 21:28:31 - INFO - __main__ - Step 40 Global step 40 Train loss 11.604495 on epoch=13
03/13/2022 21:28:36 - INFO - __main__ - Step 50 Global step 50 Train loss 9.745857 on epoch=16
03/13/2022 21:28:37 - INFO - __main__ - Global step 50 Train loss 13.932062 Classification-F1 0.0 on epoch=16
03/13/2022 21:28:43 - INFO - __main__ - Step 60 Global step 60 Train loss 8.353910 on epoch=19
03/13/2022 21:28:47 - INFO - __main__ - Step 70 Global step 70 Train loss 6.524805 on epoch=23
03/13/2022 21:28:52 - INFO - __main__ - Step 80 Global step 80 Train loss 3.989790 on epoch=26
03/13/2022 21:28:57 - INFO - __main__ - Step 90 Global step 90 Train loss 2.372810 on epoch=29
03/13/2022 21:29:02 - INFO - __main__ - Step 100 Global step 100 Train loss 2.564869 on epoch=33
03/13/2022 21:29:03 - INFO - __main__ - Global step 100 Train loss 4.761237 Classification-F1 0.16666666666666666 on epoch=33
03/13/2022 21:29:08 - INFO - __main__ - Step 110 Global step 110 Train loss 2.204303 on epoch=36
03/13/2022 21:29:13 - INFO - __main__ - Step 120 Global step 120 Train loss 2.178687 on epoch=39
03/13/2022 21:29:18 - INFO - __main__ - Step 130 Global step 130 Train loss 2.246317 on epoch=43
03/13/2022 21:29:23 - INFO - __main__ - Step 140 Global step 140 Train loss 1.814223 on epoch=46
03/13/2022 21:29:28 - INFO - __main__ - Step 150 Global step 150 Train loss 1.668816 on epoch=49
03/13/2022 21:29:29 - INFO - __main__ - Global step 150 Train loss 2.022469 Classification-F1 0.20634920634920637 on epoch=49
03/13/2022 21:29:34 - INFO - __main__ - Step 160 Global step 160 Train loss 1.956548 on epoch=53
03/13/2022 21:29:39 - INFO - __main__ - Step 170 Global step 170 Train loss 1.451104 on epoch=56
03/13/2022 21:29:44 - INFO - __main__ - Step 180 Global step 180 Train loss 1.638358 on epoch=59
03/13/2022 21:29:49 - INFO - __main__ - Step 190 Global step 190 Train loss 1.553120 on epoch=63
03/13/2022 21:29:53 - INFO - __main__ - Step 200 Global step 200 Train loss 1.355969 on epoch=66
03/13/2022 21:29:54 - INFO - __main__ - Global step 200 Train loss 1.591020 Classification-F1 0.16666666666666666 on epoch=66
03/13/2022 21:29:59 - INFO - __main__ - Step 210 Global step 210 Train loss 1.480815 on epoch=69
03/13/2022 21:30:04 - INFO - __main__ - Step 220 Global step 220 Train loss 1.169874 on epoch=73
03/13/2022 21:30:09 - INFO - __main__ - Step 230 Global step 230 Train loss 1.079170 on epoch=76
03/13/2022 21:30:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.853438 on epoch=79
03/13/2022 21:30:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.960915 on epoch=83
03/13/2022 21:30:19 - INFO - __main__ - Global step 250 Train loss 1.108842 Classification-F1 0.16666666666666666 on epoch=83
03/13/2022 21:30:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.769566 on epoch=86
03/13/2022 21:30:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.758165 on epoch=89
03/13/2022 21:30:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.702140 on epoch=93
03/13/2022 21:30:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.507567 on epoch=96
03/13/2022 21:30:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.629901 on epoch=99
03/13/2022 21:30:44 - INFO - __main__ - Global step 300 Train loss 0.673468 Classification-F1 0.23347925786950177 on epoch=99
03/13/2022 21:30:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.658726 on epoch=103
03/13/2022 21:30:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.538849 on epoch=106
03/13/2022 21:30:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.579911 on epoch=109
03/13/2022 21:31:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.508221 on epoch=113
03/13/2022 21:31:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.475777 on epoch=116
03/13/2022 21:31:10 - INFO - __main__ - Global step 350 Train loss 0.552297 Classification-F1 0.16666666666666666 on epoch=116
03/13/2022 21:31:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.681524 on epoch=119
03/13/2022 21:31:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.522996 on epoch=123
03/13/2022 21:31:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.514481 on epoch=126
03/13/2022 21:31:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.441564 on epoch=129
03/13/2022 21:31:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.693691 on epoch=133
03/13/2022 21:31:35 - INFO - __main__ - Global step 400 Train loss 0.570851 Classification-F1 0.16666666666666666 on epoch=133
03/13/2022 21:31:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.494175 on epoch=136
03/13/2022 21:31:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.486717 on epoch=139
03/13/2022 21:31:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.574816 on epoch=143
03/13/2022 21:31:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.476714 on epoch=146
03/13/2022 21:31:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.504350 on epoch=149
03/13/2022 21:32:00 - INFO - __main__ - Global step 450 Train loss 0.507354 Classification-F1 0.16666666666666666 on epoch=149
03/13/2022 21:32:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.728935 on epoch=153
03/13/2022 21:32:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.461809 on epoch=156
03/13/2022 21:32:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.414351 on epoch=159
03/13/2022 21:32:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.440987 on epoch=163
03/13/2022 21:32:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.453539 on epoch=166
03/13/2022 21:32:25 - INFO - __main__ - Global step 500 Train loss 0.499924 Classification-F1 0.16666666666666666 on epoch=166
03/13/2022 21:32:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.438761 on epoch=169
03/13/2022 21:32:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.443672 on epoch=173
03/13/2022 21:32:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.477604 on epoch=176
03/13/2022 21:32:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.466079 on epoch=179
03/13/2022 21:32:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.461015 on epoch=183
03/13/2022 21:32:50 - INFO - __main__ - Global step 550 Train loss 0.457426 Classification-F1 0.16666666666666666 on epoch=183
03/13/2022 21:32:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.471657 on epoch=186
03/13/2022 21:33:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.447943 on epoch=189
03/13/2022 21:33:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.476523 on epoch=193
03/13/2022 21:33:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.421699 on epoch=196
03/13/2022 21:33:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.464436 on epoch=199
03/13/2022 21:33:15 - INFO - __main__ - Global step 600 Train loss 0.456452 Classification-F1 0.16666666666666666 on epoch=199
03/13/2022 21:33:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.483231 on epoch=203
03/13/2022 21:33:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.437126 on epoch=206
03/13/2022 21:33:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.409583 on epoch=209
03/13/2022 21:33:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.443458 on epoch=213
03/13/2022 21:33:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.405195 on epoch=216
03/13/2022 21:33:40 - INFO - __main__ - Global step 650 Train loss 0.435718 Classification-F1 0.16666666666666666 on epoch=216
03/13/2022 21:33:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.464058 on epoch=219
03/13/2022 21:33:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.436838 on epoch=223
03/13/2022 21:33:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.431811 on epoch=226
03/13/2022 21:33:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.398301 on epoch=229
03/13/2022 21:34:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.453098 on epoch=233
03/13/2022 21:34:05 - INFO - __main__ - Global step 700 Train loss 0.436822 Classification-F1 0.16666666666666666 on epoch=233
03/13/2022 21:34:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.409705 on epoch=236
03/13/2022 21:34:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.429340 on epoch=239
03/13/2022 21:34:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.421153 on epoch=243
03/13/2022 21:34:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.384436 on epoch=246
03/13/2022 21:34:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.415285 on epoch=249
03/13/2022 21:34:29 - INFO - __main__ - Global step 750 Train loss 0.411984 Classification-F1 0.16666666666666666 on epoch=249
03/13/2022 21:34:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.416267 on epoch=253
03/13/2022 21:34:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.401957 on epoch=256
03/13/2022 21:34:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.386098 on epoch=259
03/13/2022 21:34:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.454242 on epoch=263
03/13/2022 21:34:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.397130 on epoch=266
03/13/2022 21:34:55 - INFO - __main__ - Global step 800 Train loss 0.411139 Classification-F1 0.16666666666666666 on epoch=266
03/13/2022 21:34:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.402185 on epoch=269
03/13/2022 21:35:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.438123 on epoch=273
03/13/2022 21:35:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.362036 on epoch=276
03/13/2022 21:35:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.403921 on epoch=279
03/13/2022 21:35:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.383472 on epoch=283
03/13/2022 21:35:20 - INFO - __main__ - Global step 850 Train loss 0.397947 Classification-F1 0.1693121693121693 on epoch=283
03/13/2022 21:35:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.434807 on epoch=286
03/13/2022 21:35:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.369796 on epoch=289
03/13/2022 21:35:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.412550 on epoch=293
03/13/2022 21:35:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.393789 on epoch=296
03/13/2022 21:35:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.403962 on epoch=299
03/13/2022 21:35:46 - INFO - __main__ - Global step 900 Train loss 0.402981 Classification-F1 0.2877551539185033 on epoch=299
03/13/2022 21:35:47 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:35:47 - INFO - __main__ - Printing 3 examples
03/13/2022 21:35:47 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/13/2022 21:35:47 - INFO - __main__ - ['offensive']
03/13/2022 21:35:47 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/13/2022 21:35:47 - INFO - __main__ - ['offensive']
03/13/2022 21:35:47 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/13/2022 21:35:47 - INFO - __main__ - ['offensive']
03/13/2022 21:35:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 21:35:47 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:35:47 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 21:35:47 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:35:47 - INFO - __main__ - Printing 3 examples
03/13/2022 21:35:47 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/13/2022 21:35:47 - INFO - __main__ - ['offensive']
03/13/2022 21:35:47 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/13/2022 21:35:47 - INFO - __main__ - ['offensive']
03/13/2022 21:35:47 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 
03/13/2022 21:35:47 - INFO - __main__ - ['offensive']
03/13/2022 21:35:47 - INFO - __main__ - Tokenizing Input ...
03/13/2022 21:35:47 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:35:47 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 21:35:47 - INFO - __main__ - save last model!
03/13/2022 21:35:53 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 21:35:54 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 21:35:54 - INFO - __main__ - Printing 3 examples
03/13/2022 21:35:54 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 21:35:54 - INFO - __main__ - ['normal']
03/13/2022 21:35:54 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 21:35:54 - INFO - __main__ - ['normal']
03/13/2022 21:35:54 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 21:35:54 - INFO - __main__ - ['normal']
03/13/2022 21:35:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 21:35:54 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:35:56 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 21:35:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 21:35:57 - INFO - __main__ - Starting training!
03/13/2022 21:36:26 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_87_0.0005_8_predictions.txt
03/13/2022 21:36:26 - INFO - __main__ - Classification-F1 on test data: 0.2812
03/13/2022 21:36:27 - INFO - __main__ - prefix=hatexplain_16_87, lr=0.0005, bsz=8, dev_performance=0.2877551539185033, test_performance=0.28115083961723286
03/13/2022 21:36:27 - INFO - __main__ - Running ... prefix=hatexplain_16_87, lr=0.0003, bsz=8 ...
03/13/2022 21:36:28 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:36:28 - INFO - __main__ - Printing 3 examples
03/13/2022 21:36:28 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/13/2022 21:36:28 - INFO - __main__ - ['offensive']
03/13/2022 21:36:28 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/13/2022 21:36:28 - INFO - __main__ - ['offensive']
03/13/2022 21:36:28 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/13/2022 21:36:28 - INFO - __main__ - ['offensive']
03/13/2022 21:36:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 21:36:28 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:36:28 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 21:36:28 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:36:28 - INFO - __main__ - Printing 3 examples
03/13/2022 21:36:28 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/13/2022 21:36:28 - INFO - __main__ - ['offensive']
03/13/2022 21:36:28 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/13/2022 21:36:28 - INFO - __main__ - ['offensive']
03/13/2022 21:36:28 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 
03/13/2022 21:36:28 - INFO - __main__ - ['offensive']
03/13/2022 21:36:28 - INFO - __main__ - Tokenizing Input ...
03/13/2022 21:36:28 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:36:28 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 21:36:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 21:36:38 - INFO - __main__ - Starting training!
03/13/2022 21:36:42 - INFO - __main__ - Step 10 Global step 10 Train loss 22.012388 on epoch=3
03/13/2022 21:36:47 - INFO - __main__ - Step 20 Global step 20 Train loss 16.479025 on epoch=6
03/13/2022 21:36:52 - INFO - __main__ - Step 30 Global step 30 Train loss 12.229788 on epoch=9
03/13/2022 21:36:57 - INFO - __main__ - Step 40 Global step 40 Train loss 11.586967 on epoch=13
03/13/2022 21:37:02 - INFO - __main__ - Step 50 Global step 50 Train loss 10.538363 on epoch=16
03/13/2022 21:37:10 - INFO - __main__ - Global step 50 Train loss 14.569306 Classification-F1 0.0 on epoch=16
03/13/2022 21:37:15 - INFO - __main__ - Step 60 Global step 60 Train loss 9.945102 on epoch=19
03/13/2022 21:37:20 - INFO - __main__ - Step 70 Global step 70 Train loss 9.225048 on epoch=23
03/13/2022 21:37:25 - INFO - __main__ - Step 80 Global step 80 Train loss 8.539811 on epoch=26
03/13/2022 21:37:30 - INFO - __main__ - Step 90 Global step 90 Train loss 7.297805 on epoch=29
03/13/2022 21:37:35 - INFO - __main__ - Step 100 Global step 100 Train loss 4.638174 on epoch=33
03/13/2022 21:37:36 - INFO - __main__ - Global step 100 Train loss 7.929188 Classification-F1 0.11090909090909092 on epoch=33
03/13/2022 21:37:42 - INFO - __main__ - Step 110 Global step 110 Train loss 3.109079 on epoch=36
03/13/2022 21:37:47 - INFO - __main__ - Step 120 Global step 120 Train loss 3.313699 on epoch=39
03/13/2022 21:37:52 - INFO - __main__ - Step 130 Global step 130 Train loss 2.378465 on epoch=43
03/13/2022 21:37:57 - INFO - __main__ - Step 140 Global step 140 Train loss 2.661977 on epoch=46
03/13/2022 21:38:02 - INFO - __main__ - Step 150 Global step 150 Train loss 3.145203 on epoch=49
03/13/2022 21:38:03 - INFO - __main__ - Global step 150 Train loss 2.921685 Classification-F1 0.2929292929292929 on epoch=49
03/13/2022 21:38:08 - INFO - __main__ - Step 160 Global step 160 Train loss 2.559170 on epoch=53
03/13/2022 21:38:13 - INFO - __main__ - Step 170 Global step 170 Train loss 2.717654 on epoch=56
03/13/2022 21:38:18 - INFO - __main__ - Step 180 Global step 180 Train loss 2.002474 on epoch=59
03/13/2022 21:38:23 - INFO - __main__ - Step 190 Global step 190 Train loss 2.416712 on epoch=63
03/13/2022 21:38:28 - INFO - __main__ - Step 200 Global step 200 Train loss 2.227336 on epoch=66
03/13/2022 21:38:29 - INFO - __main__ - Global step 200 Train loss 2.384669 Classification-F1 0.1983273596176822 on epoch=66
03/13/2022 21:38:34 - INFO - __main__ - Step 210 Global step 210 Train loss 2.151353 on epoch=69
03/13/2022 21:38:39 - INFO - __main__ - Step 220 Global step 220 Train loss 2.246650 on epoch=73
03/13/2022 21:38:44 - INFO - __main__ - Step 230 Global step 230 Train loss 2.509145 on epoch=76
03/13/2022 21:38:49 - INFO - __main__ - Step 240 Global step 240 Train loss 2.275213 on epoch=79
03/13/2022 21:38:54 - INFO - __main__ - Step 250 Global step 250 Train loss 2.076817 on epoch=83
03/13/2022 21:38:55 - INFO - __main__ - Global step 250 Train loss 2.251836 Classification-F1 0.125 on epoch=83
03/13/2022 21:39:00 - INFO - __main__ - Step 260 Global step 260 Train loss 1.744872 on epoch=86
03/13/2022 21:39:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.644947 on epoch=89
03/13/2022 21:39:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.768764 on epoch=93
03/13/2022 21:39:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.484768 on epoch=96
03/13/2022 21:39:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.492832 on epoch=99
03/13/2022 21:39:20 - INFO - __main__ - Global step 300 Train loss 0.827237 Classification-F1 0.16666666666666666 on epoch=99
03/13/2022 21:39:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.643064 on epoch=103
03/13/2022 21:39:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.451466 on epoch=106
03/13/2022 21:39:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.458396 on epoch=109
03/13/2022 21:39:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.442801 on epoch=113
03/13/2022 21:39:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.379999 on epoch=116
03/13/2022 21:39:46 - INFO - __main__ - Global step 350 Train loss 0.475145 Classification-F1 0.3055555555555555 on epoch=116
03/13/2022 21:39:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.353271 on epoch=119
03/13/2022 21:39:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.353407 on epoch=123
03/13/2022 21:40:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.325393 on epoch=126
03/13/2022 21:40:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.320538 on epoch=129
03/13/2022 21:40:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.277038 on epoch=133
03/13/2022 21:40:12 - INFO - __main__ - Global step 400 Train loss 0.325929 Classification-F1 0.28116488581604854 on epoch=133
03/13/2022 21:40:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.370562 on epoch=136
03/13/2022 21:40:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.205691 on epoch=139
03/13/2022 21:40:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.158392 on epoch=143
03/13/2022 21:40:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.109312 on epoch=146
03/13/2022 21:40:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.046952 on epoch=149
03/13/2022 21:40:37 - INFO - __main__ - Global step 450 Train loss 0.178182 Classification-F1 0.4582299421009099 on epoch=149
03/13/2022 21:40:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.073660 on epoch=153
03/13/2022 21:40:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.025232 on epoch=156
03/13/2022 21:40:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.027576 on epoch=159
03/13/2022 21:40:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.019800 on epoch=163
03/13/2022 21:41:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013546 on epoch=166
03/13/2022 21:41:04 - INFO - __main__ - Global step 500 Train loss 0.031963 Classification-F1 0.3113678505206104 on epoch=166
03/13/2022 21:41:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.007422 on epoch=169
03/13/2022 21:41:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003378 on epoch=173
03/13/2022 21:41:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.004082 on epoch=176
03/13/2022 21:41:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.011011 on epoch=179
03/13/2022 21:41:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.021054 on epoch=183
03/13/2022 21:41:29 - INFO - __main__ - Global step 550 Train loss 0.009389 Classification-F1 0.4258165516259873 on epoch=183
03/13/2022 21:41:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.011497 on epoch=186
03/13/2022 21:41:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.002143 on epoch=189
03/13/2022 21:41:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.002979 on epoch=193
03/13/2022 21:41:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.006920 on epoch=196
03/13/2022 21:41:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.065324 on epoch=199
03/13/2022 21:41:55 - INFO - __main__ - Global step 600 Train loss 0.017773 Classification-F1 0.4081168257334747 on epoch=199
03/13/2022 21:42:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.005007 on epoch=203
03/13/2022 21:42:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001787 on epoch=206
03/13/2022 21:42:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.001486 on epoch=209
03/13/2022 21:42:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.002524 on epoch=213
03/13/2022 21:42:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.016982 on epoch=216
03/13/2022 21:42:21 - INFO - __main__ - Global step 650 Train loss 0.005557 Classification-F1 0.22822930341727332 on epoch=216
03/13/2022 21:42:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.011261 on epoch=219
03/13/2022 21:42:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001069 on epoch=223
03/13/2022 21:42:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.009214 on epoch=226
03/13/2022 21:42:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.001926 on epoch=229
03/13/2022 21:42:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.001194 on epoch=233
03/13/2022 21:42:46 - INFO - __main__ - Global step 700 Train loss 0.004933 Classification-F1 0.3172839506172839 on epoch=233
03/13/2022 21:42:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001467 on epoch=236
03/13/2022 21:42:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000826 on epoch=239
03/13/2022 21:43:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.003772 on epoch=243
03/13/2022 21:43:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000632 on epoch=246
03/13/2022 21:43:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000483 on epoch=249
03/13/2022 21:43:12 - INFO - __main__ - Global step 750 Train loss 0.001436 Classification-F1 0.3977718360071301 on epoch=249
03/13/2022 21:43:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000565 on epoch=253
03/13/2022 21:43:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000292 on epoch=256
03/13/2022 21:43:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000908 on epoch=259
03/13/2022 21:43:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001071 on epoch=263
03/13/2022 21:43:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.001402 on epoch=266
03/13/2022 21:43:37 - INFO - __main__ - Global step 800 Train loss 0.000847 Classification-F1 0.3853744159017054 on epoch=266
03/13/2022 21:43:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000304 on epoch=269
03/13/2022 21:43:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001325 on epoch=273
03/13/2022 21:43:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000435 on epoch=276
03/13/2022 21:43:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000305 on epoch=279
03/13/2022 21:44:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000472 on epoch=283
03/13/2022 21:44:03 - INFO - __main__ - Global step 850 Train loss 0.000568 Classification-F1 0.36554484941581716 on epoch=283
03/13/2022 21:44:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002463 on epoch=286
03/13/2022 21:44:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000777 on epoch=289
03/13/2022 21:44:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.034699 on epoch=293
03/13/2022 21:44:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000930 on epoch=296
03/13/2022 21:44:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000147 on epoch=299
03/13/2022 21:44:28 - INFO - __main__ - Global step 900 Train loss 0.007803 Classification-F1 0.36645015906680806 on epoch=299
03/13/2022 21:44:28 - INFO - __main__ - save last model!
03/13/2022 21:44:28 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:44:28 - INFO - __main__ - Printing 3 examples
03/13/2022 21:44:28 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/13/2022 21:44:28 - INFO - __main__ - ['offensive']
03/13/2022 21:44:28 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/13/2022 21:44:28 - INFO - __main__ - ['offensive']
03/13/2022 21:44:28 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/13/2022 21:44:28 - INFO - __main__ - ['offensive']
03/13/2022 21:44:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 21:44:29 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:44:29 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 21:44:29 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:44:29 - INFO - __main__ - Printing 3 examples
03/13/2022 21:44:29 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/13/2022 21:44:29 - INFO - __main__ - ['offensive']
03/13/2022 21:44:29 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/13/2022 21:44:29 - INFO - __main__ - ['offensive']
03/13/2022 21:44:29 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 
03/13/2022 21:44:29 - INFO - __main__ - ['offensive']
03/13/2022 21:44:29 - INFO - __main__ - Tokenizing Input ...
03/13/2022 21:44:29 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:44:29 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 21:44:35 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 21:44:36 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 21:44:36 - INFO - __main__ - Printing 3 examples
03/13/2022 21:44:36 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 21:44:36 - INFO - __main__ - ['normal']
03/13/2022 21:44:36 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 21:44:36 - INFO - __main__ - ['normal']
03/13/2022 21:44:36 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 21:44:36 - INFO - __main__ - ['normal']
03/13/2022 21:44:36 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 21:44:37 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:44:39 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 21:44:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 21:44:41 - INFO - __main__ - Starting training!
03/13/2022 21:45:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_87_0.0003_8_predictions.txt
03/13/2022 21:45:11 - INFO - __main__ - Classification-F1 on test data: 0.4210
03/13/2022 21:45:11 - INFO - __main__ - prefix=hatexplain_16_87, lr=0.0003, bsz=8, dev_performance=0.4582299421009099, test_performance=0.4210433991019465
03/13/2022 21:45:11 - INFO - __main__ - Running ... prefix=hatexplain_16_87, lr=0.0002, bsz=8 ...
03/13/2022 21:45:12 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:45:12 - INFO - __main__ - Printing 3 examples
03/13/2022 21:45:12 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/13/2022 21:45:12 - INFO - __main__ - ['offensive']
03/13/2022 21:45:12 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/13/2022 21:45:12 - INFO - __main__ - ['offensive']
03/13/2022 21:45:12 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/13/2022 21:45:12 - INFO - __main__ - ['offensive']
03/13/2022 21:45:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 21:45:12 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:45:12 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 21:45:12 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:45:12 - INFO - __main__ - Printing 3 examples
03/13/2022 21:45:12 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/13/2022 21:45:12 - INFO - __main__ - ['offensive']
03/13/2022 21:45:12 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/13/2022 21:45:12 - INFO - __main__ - ['offensive']
03/13/2022 21:45:12 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 
03/13/2022 21:45:12 - INFO - __main__ - ['offensive']
03/13/2022 21:45:12 - INFO - __main__ - Tokenizing Input ...
03/13/2022 21:45:12 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:45:12 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 21:45:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 21:45:23 - INFO - __main__ - Starting training!
03/13/2022 21:45:27 - INFO - __main__ - Step 10 Global step 10 Train loss 21.451544 on epoch=3
03/13/2022 21:45:32 - INFO - __main__ - Step 20 Global step 20 Train loss 18.263533 on epoch=6
03/13/2022 21:45:37 - INFO - __main__ - Step 30 Global step 30 Train loss 15.209242 on epoch=9
03/13/2022 21:45:42 - INFO - __main__ - Step 40 Global step 40 Train loss 13.192087 on epoch=13
03/13/2022 21:45:47 - INFO - __main__ - Step 50 Global step 50 Train loss 11.619866 on epoch=16
03/13/2022 21:45:49 - INFO - __main__ - Global step 50 Train loss 15.947253 Classification-F1 0.0 on epoch=16
03/13/2022 21:45:55 - INFO - __main__ - Step 60 Global step 60 Train loss 11.006351 on epoch=19
03/13/2022 21:46:00 - INFO - __main__ - Step 70 Global step 70 Train loss 11.190933 on epoch=23
03/13/2022 21:46:05 - INFO - __main__ - Step 80 Global step 80 Train loss 9.687448 on epoch=26
03/13/2022 21:46:10 - INFO - __main__ - Step 90 Global step 90 Train loss 9.784358 on epoch=29
03/13/2022 21:46:15 - INFO - __main__ - Step 100 Global step 100 Train loss 9.371084 on epoch=33
03/13/2022 21:46:16 - INFO - __main__ - Global step 100 Train loss 10.208035 Classification-F1 0.0 on epoch=33
03/13/2022 21:46:21 - INFO - __main__ - Step 110 Global step 110 Train loss 8.800120 on epoch=36
03/13/2022 21:46:26 - INFO - __main__ - Step 120 Global step 120 Train loss 7.355493 on epoch=39
03/13/2022 21:46:31 - INFO - __main__ - Step 130 Global step 130 Train loss 6.595419 on epoch=43
03/13/2022 21:46:36 - INFO - __main__ - Step 140 Global step 140 Train loss 5.690400 on epoch=46
03/13/2022 21:46:41 - INFO - __main__ - Step 150 Global step 150 Train loss 3.976475 on epoch=49
03/13/2022 21:46:42 - INFO - __main__ - Global step 150 Train loss 6.483582 Classification-F1 0.1454311454311454 on epoch=49
03/13/2022 21:46:47 - INFO - __main__ - Step 160 Global step 160 Train loss 3.035707 on epoch=53
03/13/2022 21:46:52 - INFO - __main__ - Step 170 Global step 170 Train loss 2.475735 on epoch=56
03/13/2022 21:46:57 - INFO - __main__ - Step 180 Global step 180 Train loss 1.899459 on epoch=59
03/13/2022 21:47:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.755454 on epoch=63
03/13/2022 21:47:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.460401 on epoch=66
03/13/2022 21:47:08 - INFO - __main__ - Global step 200 Train loss 1.725351 Classification-F1 0.28213166144200624 on epoch=66
03/13/2022 21:47:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.684108 on epoch=69
03/13/2022 21:47:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.367107 on epoch=73
03/13/2022 21:47:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.359790 on epoch=76
03/13/2022 21:47:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.298704 on epoch=79
03/13/2022 21:47:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.246444 on epoch=83
03/13/2022 21:47:34 - INFO - __main__ - Global step 250 Train loss 0.391230 Classification-F1 0.22617283950617284 on epoch=83
03/13/2022 21:47:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.216184 on epoch=86
03/13/2022 21:47:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.170322 on epoch=89
03/13/2022 21:47:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.139514 on epoch=93
03/13/2022 21:47:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.082805 on epoch=96
03/13/2022 21:47:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.095093 on epoch=99
03/13/2022 21:48:00 - INFO - __main__ - Global step 300 Train loss 0.140783 Classification-F1 0.4515960230245944 on epoch=99
03/13/2022 21:48:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.099698 on epoch=103
03/13/2022 21:48:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.055369 on epoch=106
03/13/2022 21:48:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.053395 on epoch=109
03/13/2022 21:48:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.023791 on epoch=113
03/13/2022 21:48:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.021917 on epoch=116
03/13/2022 21:48:27 - INFO - __main__ - Global step 350 Train loss 0.050834 Classification-F1 0.2796607757762031 on epoch=116
03/13/2022 21:48:31 - INFO - __main__ - Step 360 Global step 360 Train loss 1.125137 on epoch=119
03/13/2022 21:48:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.496564 on epoch=123
03/13/2022 21:48:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.274780 on epoch=126
03/13/2022 21:48:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.385585 on epoch=129
03/13/2022 21:48:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.499252 on epoch=133
03/13/2022 21:48:52 - INFO - __main__ - Global step 400 Train loss 0.556264 Classification-F1 0.2962962962962963 on epoch=133
03/13/2022 21:48:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.316738 on epoch=136
03/13/2022 21:49:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.381775 on epoch=139
03/13/2022 21:49:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.285584 on epoch=143
03/13/2022 21:49:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.350460 on epoch=146
03/13/2022 21:49:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.146223 on epoch=149
03/13/2022 21:49:17 - INFO - __main__ - Global step 450 Train loss 0.296156 Classification-F1 0.2450314465408805 on epoch=149
03/13/2022 21:49:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.132625 on epoch=153
03/13/2022 21:49:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.059913 on epoch=156
03/13/2022 21:49:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.031474 on epoch=159
03/13/2022 21:49:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.033348 on epoch=163
03/13/2022 21:49:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.022984 on epoch=166
03/13/2022 21:49:43 - INFO - __main__ - Global step 500 Train loss 0.056069 Classification-F1 0.2705128205128205 on epoch=166
03/13/2022 21:49:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.023621 on epoch=169
03/13/2022 21:49:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.049214 on epoch=173
03/13/2022 21:49:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.021198 on epoch=176
03/13/2022 21:50:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.005493 on epoch=179
03/13/2022 21:50:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.023895 on epoch=183
03/13/2022 21:50:09 - INFO - __main__ - Global step 550 Train loss 0.024684 Classification-F1 0.34369708053918585 on epoch=183
03/13/2022 21:50:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.006650 on epoch=186
03/13/2022 21:50:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.007615 on epoch=189
03/13/2022 21:50:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.012041 on epoch=193
03/13/2022 21:50:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.048491 on epoch=196
03/13/2022 21:50:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.013012 on epoch=199
03/13/2022 21:50:35 - INFO - __main__ - Global step 600 Train loss 0.017562 Classification-F1 0.4137079608343976 on epoch=199
03/13/2022 21:50:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.020696 on epoch=203
03/13/2022 21:50:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.002451 on epoch=206
03/13/2022 21:50:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.001323 on epoch=209
03/13/2022 21:50:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.002417 on epoch=213
03/13/2022 21:51:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001200 on epoch=216
03/13/2022 21:51:01 - INFO - __main__ - Global step 650 Train loss 0.005617 Classification-F1 0.37335937335937336 on epoch=216
03/13/2022 21:51:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.004588 on epoch=219
03/13/2022 21:51:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.004552 on epoch=223
03/13/2022 21:51:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.015264 on epoch=226
03/13/2022 21:51:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.010935 on epoch=229
03/13/2022 21:51:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.011194 on epoch=233
03/13/2022 21:51:27 - INFO - __main__ - Global step 700 Train loss 0.009307 Classification-F1 0.39086021505376345 on epoch=233
03/13/2022 21:51:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.005654 on epoch=236
03/13/2022 21:51:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.002778 on epoch=239
03/13/2022 21:51:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001854 on epoch=243
03/13/2022 21:51:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000845 on epoch=246
03/13/2022 21:51:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.011022 on epoch=249
03/13/2022 21:51:53 - INFO - __main__ - Global step 750 Train loss 0.004431 Classification-F1 0.3705379587732529 on epoch=249
03/13/2022 21:51:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001460 on epoch=253
03/13/2022 21:52:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.007307 on epoch=256
03/13/2022 21:52:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000515 on epoch=259
03/13/2022 21:52:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.005250 on epoch=263
03/13/2022 21:52:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.011403 on epoch=266
03/13/2022 21:52:19 - INFO - __main__ - Global step 800 Train loss 0.005187 Classification-F1 0.35166154296589075 on epoch=266
03/13/2022 21:52:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.023769 on epoch=269
03/13/2022 21:52:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.225896 on epoch=273
03/13/2022 21:52:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.224342 on epoch=276
03/13/2022 21:52:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.304974 on epoch=279
03/13/2022 21:52:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.197123 on epoch=283
03/13/2022 21:52:44 - INFO - __main__ - Global step 850 Train loss 0.195221 Classification-F1 0.24611708482676223 on epoch=283
03/13/2022 21:52:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.256765 on epoch=286
03/13/2022 21:52:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.309310 on epoch=289
03/13/2022 21:52:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.309528 on epoch=293
03/13/2022 21:53:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.281068 on epoch=296
03/13/2022 21:53:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.310137 on epoch=299
03/13/2022 21:53:10 - INFO - __main__ - Global step 900 Train loss 0.293362 Classification-F1 0.3417957351290685 on epoch=299
03/13/2022 21:53:10 - INFO - __main__ - save last model!
03/13/2022 21:53:10 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:53:10 - INFO - __main__ - Printing 3 examples
03/13/2022 21:53:10 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/13/2022 21:53:10 - INFO - __main__ - ['offensive']
03/13/2022 21:53:10 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/13/2022 21:53:10 - INFO - __main__ - ['offensive']
03/13/2022 21:53:10 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/13/2022 21:53:10 - INFO - __main__ - ['offensive']
03/13/2022 21:53:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 21:53:10 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:53:10 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 21:53:10 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:53:10 - INFO - __main__ - Printing 3 examples
03/13/2022 21:53:10 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/13/2022 21:53:10 - INFO - __main__ - ['offensive']
03/13/2022 21:53:10 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/13/2022 21:53:10 - INFO - __main__ - ['offensive']
03/13/2022 21:53:10 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 
03/13/2022 21:53:10 - INFO - __main__ - ['offensive']
03/13/2022 21:53:10 - INFO - __main__ - Tokenizing Input ...
03/13/2022 21:53:10 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:53:11 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 21:53:17 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 21:53:18 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 21:53:18 - INFO - __main__ - Printing 3 examples
03/13/2022 21:53:18 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 21:53:18 - INFO - __main__ - ['normal']
03/13/2022 21:53:18 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 21:53:18 - INFO - __main__ - ['normal']
03/13/2022 21:53:18 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 21:53:18 - INFO - __main__ - ['normal']
03/13/2022 21:53:18 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 21:53:19 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:53:21 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 21:53:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 21:53:23 - INFO - __main__ - Starting training!
03/13/2022 21:53:49 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_87_0.0002_8_predictions.txt
03/13/2022 21:53:49 - INFO - __main__ - Classification-F1 on test data: 0.3512
03/13/2022 21:53:50 - INFO - __main__ - prefix=hatexplain_16_87, lr=0.0002, bsz=8, dev_performance=0.4515960230245944, test_performance=0.3511747388936956
03/13/2022 21:53:50 - INFO - __main__ - Running ... prefix=hatexplain_16_87, lr=0.0001, bsz=8 ...
03/13/2022 21:53:51 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:53:51 - INFO - __main__ - Printing 3 examples
03/13/2022 21:53:51 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/13/2022 21:53:51 - INFO - __main__ - ['offensive']
03/13/2022 21:53:51 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/13/2022 21:53:51 - INFO - __main__ - ['offensive']
03/13/2022 21:53:51 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/13/2022 21:53:51 - INFO - __main__ - ['offensive']
03/13/2022 21:53:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 21:53:51 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:53:51 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/13/2022 21:53:51 - INFO - __main__ - Start tokenizing ... 48 instances
03/13/2022 21:53:51 - INFO - __main__ - Printing 3 examples
03/13/2022 21:53:51 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/13/2022 21:53:51 - INFO - __main__ - ['offensive']
03/13/2022 21:53:51 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/13/2022 21:53:51 - INFO - __main__ - ['offensive']
03/13/2022 21:53:51 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 
03/13/2022 21:53:51 - INFO - __main__ - ['offensive']
03/13/2022 21:53:51 - INFO - __main__ - Tokenizing Input ...
03/13/2022 21:53:51 - INFO - __main__ - Tokenizing Output ...
03/13/2022 21:53:51 - INFO - __main__ - Loaded 48 examples from dev data
03/13/2022 21:54:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 21:54:04 - INFO - __main__ - Starting training!
03/13/2022 21:54:08 - INFO - __main__ - Step 10 Global step 10 Train loss 21.965752 on epoch=3
03/13/2022 21:54:12 - INFO - __main__ - Step 20 Global step 20 Train loss 19.445461 on epoch=6
03/13/2022 21:54:17 - INFO - __main__ - Step 30 Global step 30 Train loss 16.009878 on epoch=9
03/13/2022 21:54:22 - INFO - __main__ - Step 40 Global step 40 Train loss 13.718562 on epoch=13
03/13/2022 21:54:27 - INFO - __main__ - Step 50 Global step 50 Train loss 14.457474 on epoch=16
03/13/2022 21:54:42 - INFO - __main__ - Global step 50 Train loss 17.119425 Classification-F1 0.0 on epoch=16
03/13/2022 21:54:48 - INFO - __main__ - Step 60 Global step 60 Train loss 11.931456 on epoch=19
03/13/2022 21:54:52 - INFO - __main__ - Step 70 Global step 70 Train loss 11.985227 on epoch=23
03/13/2022 21:54:57 - INFO - __main__ - Step 80 Global step 80 Train loss 11.435492 on epoch=26
03/13/2022 21:55:02 - INFO - __main__ - Step 90 Global step 90 Train loss 10.964897 on epoch=29
03/13/2022 21:55:07 - INFO - __main__ - Step 100 Global step 100 Train loss 11.812413 on epoch=33
03/13/2022 21:55:12 - INFO - __main__ - Global step 100 Train loss 11.625897 Classification-F1 0.0 on epoch=33
03/13/2022 21:55:17 - INFO - __main__ - Step 110 Global step 110 Train loss 10.643916 on epoch=36
03/13/2022 21:55:22 - INFO - __main__ - Step 120 Global step 120 Train loss 10.568440 on epoch=39
03/13/2022 21:55:27 - INFO - __main__ - Step 130 Global step 130 Train loss 10.385080 on epoch=43
03/13/2022 21:55:32 - INFO - __main__ - Step 140 Global step 140 Train loss 9.886113 on epoch=46
03/13/2022 21:55:37 - INFO - __main__ - Step 150 Global step 150 Train loss 9.479166 on epoch=49
03/13/2022 21:55:41 - INFO - __main__ - Global step 150 Train loss 10.192545 Classification-F1 0.0 on epoch=49
03/13/2022 21:55:46 - INFO - __main__ - Step 160 Global step 160 Train loss 9.352296 on epoch=53
03/13/2022 21:55:51 - INFO - __main__ - Step 170 Global step 170 Train loss 9.235785 on epoch=56
03/13/2022 21:55:56 - INFO - __main__ - Step 180 Global step 180 Train loss 9.252764 on epoch=59
03/13/2022 21:56:01 - INFO - __main__ - Step 190 Global step 190 Train loss 9.186338 on epoch=63
03/13/2022 21:56:06 - INFO - __main__ - Step 200 Global step 200 Train loss 8.585858 on epoch=66
03/13/2022 21:56:08 - INFO - __main__ - Global step 200 Train loss 9.122607 Classification-F1 0.0 on epoch=66
03/13/2022 21:56:13 - INFO - __main__ - Step 210 Global step 210 Train loss 8.567053 on epoch=69
03/13/2022 21:56:18 - INFO - __main__ - Step 220 Global step 220 Train loss 7.854992 on epoch=73
03/13/2022 21:56:23 - INFO - __main__ - Step 230 Global step 230 Train loss 7.274162 on epoch=76
03/13/2022 21:56:28 - INFO - __main__ - Step 240 Global step 240 Train loss 7.121827 on epoch=79
03/13/2022 21:56:33 - INFO - __main__ - Step 250 Global step 250 Train loss 6.743196 on epoch=83
03/13/2022 21:56:34 - INFO - __main__ - Global step 250 Train loss 7.512246 Classification-F1 0.0 on epoch=83
03/13/2022 21:56:39 - INFO - __main__ - Step 260 Global step 260 Train loss 5.377069 on epoch=86
03/13/2022 21:56:44 - INFO - __main__ - Step 270 Global step 270 Train loss 4.823828 on epoch=89
03/13/2022 21:56:49 - INFO - __main__ - Step 280 Global step 280 Train loss 4.057120 on epoch=93
03/13/2022 21:56:54 - INFO - __main__ - Step 290 Global step 290 Train loss 2.589792 on epoch=96
03/13/2022 21:56:59 - INFO - __main__ - Step 300 Global step 300 Train loss 2.445481 on epoch=99
03/13/2022 21:57:00 - INFO - __main__ - Global step 300 Train loss 3.858658 Classification-F1 0.31660130718954255 on epoch=99
03/13/2022 21:57:05 - INFO - __main__ - Step 310 Global step 310 Train loss 3.586262 on epoch=103
03/13/2022 21:57:10 - INFO - __main__ - Step 320 Global step 320 Train loss 1.366624 on epoch=106
03/13/2022 21:57:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.767062 on epoch=109
03/13/2022 21:57:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.647569 on epoch=113
03/13/2022 21:57:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.660088 on epoch=116
03/13/2022 21:57:26 - INFO - __main__ - Global step 350 Train loss 1.405521 Classification-F1 0.2795933188090051 on epoch=116
03/13/2022 21:57:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.548987 on epoch=119
03/13/2022 21:57:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.419389 on epoch=123
03/13/2022 21:57:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.442523 on epoch=126
03/13/2022 21:57:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.377845 on epoch=129
03/13/2022 21:57:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.407803 on epoch=133
03/13/2022 21:57:51 - INFO - __main__ - Global step 400 Train loss 0.439309 Classification-F1 0.15873015873015875 on epoch=133
03/13/2022 21:57:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.316175 on epoch=136
03/13/2022 21:58:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.331823 on epoch=139
03/13/2022 21:58:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.444851 on epoch=143
03/13/2022 21:58:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.382639 on epoch=146
03/13/2022 21:58:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.441291 on epoch=149
03/13/2022 21:58:17 - INFO - __main__ - Global step 450 Train loss 0.383356 Classification-F1 0.3159420289855073 on epoch=149
03/13/2022 21:58:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.428320 on epoch=153
03/13/2022 21:58:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.381171 on epoch=156
03/13/2022 21:58:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.353512 on epoch=159
03/13/2022 21:58:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.313726 on epoch=163
03/13/2022 21:58:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.295190 on epoch=166
03/13/2022 21:58:42 - INFO - __main__ - Global step 500 Train loss 0.354384 Classification-F1 0.3481781376518218 on epoch=166
03/13/2022 21:58:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.294332 on epoch=169
03/13/2022 21:58:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.183315 on epoch=173
03/13/2022 21:58:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.197187 on epoch=176
03/13/2022 21:59:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.275702 on epoch=179
03/13/2022 21:59:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.218423 on epoch=183
03/13/2022 21:59:08 - INFO - __main__ - Global step 550 Train loss 0.233792 Classification-F1 0.4518518518518519 on epoch=183
03/13/2022 21:59:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.255328 on epoch=186
03/13/2022 21:59:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.214121 on epoch=189
03/13/2022 21:59:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.188783 on epoch=193
03/13/2022 21:59:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.240029 on epoch=196
03/13/2022 21:59:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.253055 on epoch=199
03/13/2022 21:59:34 - INFO - __main__ - Global step 600 Train loss 0.230263 Classification-F1 0.3130047340573656 on epoch=199
03/13/2022 21:59:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.284816 on epoch=203
03/13/2022 21:59:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.257162 on epoch=206
03/13/2022 21:59:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.185585 on epoch=209
03/13/2022 21:59:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.200171 on epoch=213
03/13/2022 21:59:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.157360 on epoch=216
03/13/2022 22:00:00 - INFO - __main__ - Global step 650 Train loss 0.217019 Classification-F1 0.2881656946744426 on epoch=216
03/13/2022 22:00:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.153506 on epoch=219
03/13/2022 22:00:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.143735 on epoch=223
03/13/2022 22:00:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.197638 on epoch=226
03/13/2022 22:00:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.144122 on epoch=229
03/13/2022 22:00:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.132062 on epoch=233
03/13/2022 22:00:26 - INFO - __main__ - Global step 700 Train loss 0.154212 Classification-F1 0.29317738791423004 on epoch=233
03/13/2022 22:00:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.134232 on epoch=236
03/13/2022 22:00:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.123514 on epoch=239
03/13/2022 22:00:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.119050 on epoch=243
03/13/2022 22:00:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.156953 on epoch=246
03/13/2022 22:00:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.108219 on epoch=249
03/13/2022 22:00:51 - INFO - __main__ - Global step 750 Train loss 0.128394 Classification-F1 0.27886710239651413 on epoch=249
03/13/2022 22:00:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.171096 on epoch=253
03/13/2022 22:01:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.220920 on epoch=256
03/13/2022 22:01:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.161816 on epoch=259
03/13/2022 22:01:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.164741 on epoch=263
03/13/2022 22:01:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.140147 on epoch=266
03/13/2022 22:01:17 - INFO - __main__ - Global step 800 Train loss 0.171744 Classification-F1 0.23791185280273855 on epoch=266
03/13/2022 22:01:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.184483 on epoch=269
03/13/2022 22:01:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.163978 on epoch=273
03/13/2022 22:01:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.162648 on epoch=276
03/13/2022 22:01:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.132932 on epoch=279
03/13/2022 22:01:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.118085 on epoch=283
03/13/2022 22:01:43 - INFO - __main__ - Global step 850 Train loss 0.152425 Classification-F1 0.3033154121863799 on epoch=283
03/13/2022 22:01:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.106894 on epoch=286
03/13/2022 22:01:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.134774 on epoch=289
03/13/2022 22:01:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.182726 on epoch=293
03/13/2022 22:02:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.144706 on epoch=296
03/13/2022 22:02:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.147847 on epoch=299
03/13/2022 22:02:09 - INFO - __main__ - Global step 900 Train loss 0.143390 Classification-F1 0.3637037037037037 on epoch=299
03/13/2022 22:02:09 - INFO - __main__ - save last model!
03/13/2022 22:02:16 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 22:02:16 - INFO - __main__ - Start tokenizing ... 1922 instances
03/13/2022 22:02:16 - INFO - __main__ - Printing 3 examples
03/13/2022 22:02:16 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/13/2022 22:02:16 - INFO - __main__ - ['normal']
03/13/2022 22:02:16 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltaran how to get away with murder gossip girl the last ship orphan black downton abbey
03/13/2022 22:02:16 - INFO - __main__ - ['normal']
03/13/2022 22:02:16 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/13/2022 22:02:16 - INFO - __main__ - ['normal']
03/13/2022 22:02:16 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 22:02:17 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:02:19 - INFO - __main__ - Loaded 1922 examples from test data
03/13/2022 22:03:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-hatexplain/hatexplain_16_87_0.0001_8_predictions.txt
03/13/2022 22:03:11 - INFO - __main__ - Classification-F1 on test data: 0.1602
03/13/2022 22:03:12 - INFO - __main__ - prefix=hatexplain_16_87, lr=0.0001, bsz=8, dev_performance=0.4518518518518519, test_performance=0.16015057578813022
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (16902): No such process
Task: circa, Checkpoint: None, Identifier: T5-large-ft-random
Output directory () already exists and is not empty.
03/13/2022 22:03:17 - INFO - __main__ - Namespace(task_dir='data/circa/', task_name='circa', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-circa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/13/2022 22:03:17 - INFO - __main__ - Namespace(task_dir='data/circa/', task_name='circa', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-circa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/13/2022 22:03:17 - INFO - __main__ - models/T5-large-ft-random/singletask-circa
03/13/2022 22:03:17 - INFO - __main__ - models/T5-large-ft-random/singletask-circa
03/13/2022 22:03:18 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/13/2022 22:03:18 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/13/2022 22:03:18 - INFO - __main__ - args.device: cuda:0
03/13/2022 22:03:18 - INFO - __main__ - Using 2 gpus
03/13/2022 22:03:18 - INFO - __main__ - args.device: cuda:1
03/13/2022 22:03:18 - INFO - __main__ - Using 2 gpus
03/13/2022 22:03:18 - INFO - __main__ - Fine-tuning the following samples: ['circa_16_100', 'circa_16_13', 'circa_16_21', 'circa_16_42', 'circa_16_87']
03/13/2022 22:03:18 - INFO - __main__ - Fine-tuning the following samples: ['circa_16_100', 'circa_16_13', 'circa_16_21', 'circa_16_42', 'circa_16_87']
03/13/2022 22:03:25 - INFO - __main__ - Running ... prefix=circa_16_100, lr=0.0005, bsz=8 ...
03/13/2022 22:03:26 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:03:26 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:03:26 - INFO - __main__ - Printing 3 examples
03/13/2022 22:03:26 - INFO - __main__ - Printing 3 examples
03/13/2022 22:03:26 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/13/2022 22:03:26 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/13/2022 22:03:26 - INFO - __main__ - ['No']
03/13/2022 22:03:26 - INFO - __main__ - ['No']
03/13/2022 22:03:26 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/13/2022 22:03:26 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/13/2022 22:03:26 - INFO - __main__ - ['No']
03/13/2022 22:03:26 - INFO - __main__ - ['No']
03/13/2022 22:03:26 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/13/2022 22:03:26 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/13/2022 22:03:26 - INFO - __main__ - ['No']
03/13/2022 22:03:26 - INFO - __main__ - ['No']
03/13/2022 22:03:26 - INFO - __main__ - Tokenizing Input ...
03/13/2022 22:03:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 22:03:26 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:03:26 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:03:26 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 22:03:26 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:03:26 - INFO - __main__ - Printing 3 examples
03/13/2022 22:03:26 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/13/2022 22:03:26 - INFO - __main__ - ['No']
03/13/2022 22:03:26 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/13/2022 22:03:26 - INFO - __main__ - ['No']
03/13/2022 22:03:26 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/13/2022 22:03:26 - INFO - __main__ - ['No']
03/13/2022 22:03:26 - INFO - __main__ - Tokenizing Input ...
03/13/2022 22:03:26 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 22:03:26 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:03:26 - INFO - __main__ - Printing 3 examples
03/13/2022 22:03:26 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/13/2022 22:03:26 - INFO - __main__ - ['No']
03/13/2022 22:03:26 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/13/2022 22:03:26 - INFO - __main__ - ['No']
03/13/2022 22:03:26 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/13/2022 22:03:26 - INFO - __main__ - ['No']
03/13/2022 22:03:26 - INFO - __main__ - Tokenizing Input ...
03/13/2022 22:03:26 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:03:26 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:03:26 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 22:03:26 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 22:03:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 22:03:39 - INFO - __main__ - Starting training!
03/13/2022 22:03:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 22:03:41 - INFO - __main__ - Starting training!
03/13/2022 22:03:46 - INFO - __main__ - Step 10 Global step 10 Train loss 20.915709 on epoch=1
03/13/2022 22:03:50 - INFO - __main__ - Step 20 Global step 20 Train loss 19.147511 on epoch=3
03/13/2022 22:03:55 - INFO - __main__ - Step 30 Global step 30 Train loss 14.519948 on epoch=5
03/13/2022 22:04:00 - INFO - __main__ - Step 40 Global step 40 Train loss 9.801844 on epoch=7
03/13/2022 22:04:05 - INFO - __main__ - Step 50 Global step 50 Train loss 8.504030 on epoch=9
03/13/2022 22:04:06 - INFO - __main__ - Global step 50 Train loss 14.577808 Classification-F1 0.0 on epoch=9
03/13/2022 22:04:12 - INFO - __main__ - Step 60 Global step 60 Train loss 7.253642 on epoch=11
03/13/2022 22:04:17 - INFO - __main__ - Step 70 Global step 70 Train loss 5.815516 on epoch=13
03/13/2022 22:04:22 - INFO - __main__ - Step 80 Global step 80 Train loss 4.887892 on epoch=15
03/13/2022 22:04:27 - INFO - __main__ - Step 90 Global step 90 Train loss 4.317996 on epoch=17
03/13/2022 22:04:32 - INFO - __main__ - Step 100 Global step 100 Train loss 2.890448 on epoch=19
03/13/2022 22:04:34 - INFO - __main__ - Global step 100 Train loss 5.033099 Classification-F1 0.03333333333333334 on epoch=19
03/13/2022 22:04:41 - INFO - __main__ - Step 110 Global step 110 Train loss 1.657146 on epoch=21
03/13/2022 22:04:46 - INFO - __main__ - Step 120 Global step 120 Train loss 1.701249 on epoch=23
03/13/2022 22:04:51 - INFO - __main__ - Step 130 Global step 130 Train loss 1.623520 on epoch=25
03/13/2022 22:04:56 - INFO - __main__ - Step 140 Global step 140 Train loss 1.359542 on epoch=27
03/13/2022 22:05:01 - INFO - __main__ - Step 150 Global step 150 Train loss 1.336877 on epoch=29
03/13/2022 22:05:03 - INFO - __main__ - Global step 150 Train loss 1.535667 Classification-F1 0.14383838383838382 on epoch=29
03/13/2022 22:05:09 - INFO - __main__ - Step 160 Global step 160 Train loss 1.322905 on epoch=31
03/13/2022 22:05:14 - INFO - __main__ - Step 170 Global step 170 Train loss 1.341506 on epoch=33
03/13/2022 22:05:19 - INFO - __main__ - Step 180 Global step 180 Train loss 1.370071 on epoch=35
03/13/2022 22:05:24 - INFO - __main__ - Step 190 Global step 190 Train loss 1.036265 on epoch=37
03/13/2022 22:05:29 - INFO - __main__ - Step 200 Global step 200 Train loss 1.166283 on epoch=39
03/13/2022 22:05:31 - INFO - __main__ - Global step 200 Train loss 1.247406 Classification-F1 0.162020202020202 on epoch=39
03/13/2022 22:05:38 - INFO - __main__ - Step 210 Global step 210 Train loss 1.260574 on epoch=41
03/13/2022 22:05:43 - INFO - __main__ - Step 220 Global step 220 Train loss 1.050671 on epoch=43
03/13/2022 22:05:48 - INFO - __main__ - Step 230 Global step 230 Train loss 1.186211 on epoch=45
03/13/2022 22:05:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.834449 on epoch=47
03/13/2022 22:05:57 - INFO - __main__ - Step 250 Global step 250 Train loss 1.097398 on epoch=49
03/13/2022 22:05:59 - INFO - __main__ - Global step 250 Train loss 1.085861 Classification-F1 0.14708887992470082 on epoch=49
03/13/2022 22:06:04 - INFO - __main__ - Step 260 Global step 260 Train loss 1.057595 on epoch=51
03/13/2022 22:06:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.957107 on epoch=53
03/13/2022 22:06:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.822789 on epoch=55
03/13/2022 22:06:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.879972 on epoch=57
03/13/2022 22:06:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.753611 on epoch=59
03/13/2022 22:06:26 - INFO - __main__ - Global step 300 Train loss 0.894215 Classification-F1 0.1994667781901824 on epoch=59
03/13/2022 22:06:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.853254 on epoch=61
03/13/2022 22:06:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.614893 on epoch=63
03/13/2022 22:06:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.602207 on epoch=65
03/13/2022 22:06:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.669077 on epoch=67
03/13/2022 22:06:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.592730 on epoch=69
03/13/2022 22:06:53 - INFO - __main__ - Global step 350 Train loss 0.666432 Classification-F1 0.15189915966386555 on epoch=69
03/13/2022 22:06:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.608313 on epoch=71
03/13/2022 22:07:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.540398 on epoch=73
03/13/2022 22:07:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.563307 on epoch=75
03/13/2022 22:07:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.538975 on epoch=77
03/13/2022 22:07:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.413911 on epoch=79
03/13/2022 22:07:20 - INFO - __main__ - Global step 400 Train loss 0.532981 Classification-F1 0.17497420020639837 on epoch=79
03/13/2022 22:07:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.494903 on epoch=81
03/13/2022 22:07:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.601855 on epoch=83
03/13/2022 22:07:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.474836 on epoch=85
03/13/2022 22:07:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.462641 on epoch=87
03/13/2022 22:07:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.440825 on epoch=89
03/13/2022 22:07:47 - INFO - __main__ - Global step 450 Train loss 0.495012 Classification-F1 0.20447031039136304 on epoch=89
03/13/2022 22:07:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.413825 on epoch=91
03/13/2022 22:07:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.473773 on epoch=93
03/13/2022 22:08:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.393604 on epoch=95
03/13/2022 22:08:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.414529 on epoch=97
03/13/2022 22:08:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.379911 on epoch=99
03/13/2022 22:08:14 - INFO - __main__ - Global step 500 Train loss 0.415128 Classification-F1 0.19570717839374557 on epoch=99
03/13/2022 22:08:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.416967 on epoch=101
03/13/2022 22:08:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.420691 on epoch=103
03/13/2022 22:08:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.376229 on epoch=105
03/13/2022 22:08:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.383796 on epoch=107
03/13/2022 22:08:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.393724 on epoch=109
03/13/2022 22:08:41 - INFO - __main__ - Global step 550 Train loss 0.398281 Classification-F1 0.17397470950102528 on epoch=109
03/13/2022 22:08:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.414442 on epoch=111
03/13/2022 22:08:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.361319 on epoch=113
03/13/2022 22:08:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.400791 on epoch=115
03/13/2022 22:09:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.360062 on epoch=117
03/13/2022 22:09:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.385810 on epoch=119
03/13/2022 22:09:08 - INFO - __main__ - Global step 600 Train loss 0.384485 Classification-F1 0.18413348946135832 on epoch=119
03/13/2022 22:09:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.363551 on epoch=121
03/13/2022 22:09:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.368032 on epoch=123
03/13/2022 22:09:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.344968 on epoch=125
03/13/2022 22:09:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.362072 on epoch=127
03/13/2022 22:09:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.386072 on epoch=129
03/13/2022 22:09:34 - INFO - __main__ - Global step 650 Train loss 0.364939 Classification-F1 0.18651515151515152 on epoch=129
03/13/2022 22:09:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.356645 on epoch=131
03/13/2022 22:09:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.371442 on epoch=133
03/13/2022 22:09:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.345611 on epoch=135
03/13/2022 22:09:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.329132 on epoch=137
03/13/2022 22:09:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.386474 on epoch=139
03/13/2022 22:10:01 - INFO - __main__ - Global step 700 Train loss 0.357861 Classification-F1 0.18627140974967063 on epoch=139
03/13/2022 22:10:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.350974 on epoch=141
03/13/2022 22:10:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.355991 on epoch=143
03/13/2022 22:10:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.344783 on epoch=145
03/13/2022 22:10:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.398407 on epoch=147
03/13/2022 22:10:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.334728 on epoch=149
03/13/2022 22:10:28 - INFO - __main__ - Global step 750 Train loss 0.356977 Classification-F1 0.20649572649572648 on epoch=149
03/13/2022 22:10:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.356195 on epoch=151
03/13/2022 22:10:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.334444 on epoch=153
03/13/2022 22:10:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.341139 on epoch=155
03/13/2022 22:10:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.331633 on epoch=157
03/13/2022 22:10:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.342949 on epoch=159
03/13/2022 22:10:55 - INFO - __main__ - Global step 800 Train loss 0.341272 Classification-F1 0.1891376451077944 on epoch=159
03/13/2022 22:11:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.484377 on epoch=161
03/13/2022 22:11:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.364159 on epoch=163
03/13/2022 22:11:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.371278 on epoch=165
03/13/2022 22:11:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.361007 on epoch=167
03/13/2022 22:11:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.346799 on epoch=169
03/13/2022 22:11:22 - INFO - __main__ - Global step 850 Train loss 0.385524 Classification-F1 0.17127715951245362 on epoch=169
03/13/2022 22:11:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.370880 on epoch=171
03/13/2022 22:11:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.350900 on epoch=173
03/13/2022 22:11:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.350354 on epoch=175
03/13/2022 22:11:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.347079 on epoch=177
03/13/2022 22:11:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.326621 on epoch=179
03/13/2022 22:11:48 - INFO - __main__ - Global step 900 Train loss 0.349167 Classification-F1 0.20621693121693122 on epoch=179
03/13/2022 22:11:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.354363 on epoch=181
03/13/2022 22:11:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.349239 on epoch=183
03/13/2022 22:12:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.333539 on epoch=185
03/13/2022 22:12:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.352071 on epoch=187
03/13/2022 22:12:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.343747 on epoch=189
03/13/2022 22:12:15 - INFO - __main__ - Global step 950 Train loss 0.346592 Classification-F1 0.14588859416445624 on epoch=189
03/13/2022 22:12:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.339023 on epoch=191
03/13/2022 22:12:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.350735 on epoch=193
03/13/2022 22:12:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.363486 on epoch=195
03/13/2022 22:12:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.324431 on epoch=197
03/13/2022 22:12:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.348494 on epoch=199
03/13/2022 22:12:41 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:12:41 - INFO - __main__ - Printing 3 examples
03/13/2022 22:12:41 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/13/2022 22:12:41 - INFO - __main__ - ['No']
03/13/2022 22:12:41 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/13/2022 22:12:41 - INFO - __main__ - ['No']
03/13/2022 22:12:41 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/13/2022 22:12:41 - INFO - __main__ - ['No']
03/13/2022 22:12:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 22:12:41 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:12:41 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 22:12:41 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:12:41 - INFO - __main__ - Printing 3 examples
03/13/2022 22:12:41 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/13/2022 22:12:41 - INFO - __main__ - ['No']
03/13/2022 22:12:41 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/13/2022 22:12:41 - INFO - __main__ - ['No']
03/13/2022 22:12:41 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/13/2022 22:12:41 - INFO - __main__ - ['No']
03/13/2022 22:12:41 - INFO - __main__ - Tokenizing Input ...
03/13/2022 22:12:41 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:12:41 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 22:12:41 - INFO - __main__ - Global step 1000 Train loss 0.345234 Classification-F1 0.1731797614150555 on epoch=199
03/13/2022 22:12:41 - INFO - __main__ - save last model!
03/13/2022 22:12:48 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 22:12:49 - INFO - __main__ - Start tokenizing ... 6700 instances
03/13/2022 22:12:49 - INFO - __main__ - Printing 3 examples
03/13/2022 22:12:49 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/13/2022 22:12:49 - INFO - __main__ - ['No']
03/13/2022 22:12:49 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/13/2022 22:12:49 - INFO - __main__ - ['Yes']
03/13/2022 22:12:49 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/13/2022 22:12:49 - INFO - __main__ - ['Yes']
03/13/2022 22:12:49 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 22:12:52 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:12:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 22:12:54 - INFO - __main__ - Starting training!
03/13/2022 22:12:59 - INFO - __main__ - Loaded 6700 examples from test data
03/13/2022 22:15:22 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_100_0.0005_8_predictions.txt
03/13/2022 22:15:22 - INFO - __main__ - Classification-F1 on test data: 0.0735
03/13/2022 22:15:23 - INFO - __main__ - prefix=circa_16_100, lr=0.0005, bsz=8, dev_performance=0.20649572649572648, test_performance=0.07348342671913186
03/13/2022 22:15:23 - INFO - __main__ - Running ... prefix=circa_16_100, lr=0.0003, bsz=8 ...
03/13/2022 22:15:23 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:15:23 - INFO - __main__ - Printing 3 examples
03/13/2022 22:15:23 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/13/2022 22:15:23 - INFO - __main__ - ['No']
03/13/2022 22:15:23 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/13/2022 22:15:23 - INFO - __main__ - ['No']
03/13/2022 22:15:23 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/13/2022 22:15:23 - INFO - __main__ - ['No']
03/13/2022 22:15:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 22:15:24 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:15:24 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 22:15:24 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:15:24 - INFO - __main__ - Printing 3 examples
03/13/2022 22:15:24 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/13/2022 22:15:24 - INFO - __main__ - ['No']
03/13/2022 22:15:24 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/13/2022 22:15:24 - INFO - __main__ - ['No']
03/13/2022 22:15:24 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/13/2022 22:15:24 - INFO - __main__ - ['No']
03/13/2022 22:15:24 - INFO - __main__ - Tokenizing Input ...
03/13/2022 22:15:24 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:15:24 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 22:15:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 22:15:36 - INFO - __main__ - Starting training!
03/13/2022 22:15:40 - INFO - __main__ - Step 10 Global step 10 Train loss 22.374599 on epoch=1
03/13/2022 22:15:45 - INFO - __main__ - Step 20 Global step 20 Train loss 17.399628 on epoch=3
03/13/2022 22:15:49 - INFO - __main__ - Step 30 Global step 30 Train loss 15.182302 on epoch=5
03/13/2022 22:15:54 - INFO - __main__ - Step 40 Global step 40 Train loss 14.271855 on epoch=7
03/13/2022 22:15:59 - INFO - __main__ - Step 50 Global step 50 Train loss 11.011417 on epoch=9
03/13/2022 22:16:20 - INFO - __main__ - Global step 50 Train loss 16.047962 Classification-F1 0.0 on epoch=9
03/13/2022 22:16:25 - INFO - __main__ - Step 60 Global step 60 Train loss 9.412561 on epoch=11
03/13/2022 22:16:30 - INFO - __main__ - Step 70 Global step 70 Train loss 8.980290 on epoch=13
03/13/2022 22:16:35 - INFO - __main__ - Step 80 Global step 80 Train loss 8.059987 on epoch=15
03/13/2022 22:16:40 - INFO - __main__ - Step 90 Global step 90 Train loss 6.804746 on epoch=17
03/13/2022 22:16:45 - INFO - __main__ - Step 100 Global step 100 Train loss 5.718792 on epoch=19
03/13/2022 22:16:48 - INFO - __main__ - Global step 100 Train loss 7.795276 Classification-F1 0.0 on epoch=19
03/13/2022 22:16:53 - INFO - __main__ - Step 110 Global step 110 Train loss 5.567099 on epoch=21
03/13/2022 22:16:58 - INFO - __main__ - Step 120 Global step 120 Train loss 4.523765 on epoch=23
03/13/2022 22:17:03 - INFO - __main__ - Step 130 Global step 130 Train loss 3.804136 on epoch=25
03/13/2022 22:17:08 - INFO - __main__ - Step 140 Global step 140 Train loss 3.157644 on epoch=27
03/13/2022 22:17:13 - INFO - __main__ - Step 150 Global step 150 Train loss 2.540342 on epoch=29
03/13/2022 22:17:15 - INFO - __main__ - Global step 150 Train loss 3.918597 Classification-F1 0.0101010101010101 on epoch=29
03/13/2022 22:17:21 - INFO - __main__ - Step 160 Global step 160 Train loss 1.784285 on epoch=31
03/13/2022 22:17:26 - INFO - __main__ - Step 170 Global step 170 Train loss 2.028046 on epoch=33
03/13/2022 22:17:31 - INFO - __main__ - Step 180 Global step 180 Train loss 1.771829 on epoch=35
03/13/2022 22:17:36 - INFO - __main__ - Step 190 Global step 190 Train loss 1.660543 on epoch=37
03/13/2022 22:17:41 - INFO - __main__ - Step 200 Global step 200 Train loss 1.648540 on epoch=39
03/13/2022 22:17:43 - INFO - __main__ - Global step 200 Train loss 1.778648 Classification-F1 0.11366846697133362 on epoch=39
03/13/2022 22:17:48 - INFO - __main__ - Step 210 Global step 210 Train loss 1.739341 on epoch=41
03/13/2022 22:17:53 - INFO - __main__ - Step 220 Global step 220 Train loss 1.635435 on epoch=43
03/13/2022 22:17:58 - INFO - __main__ - Step 230 Global step 230 Train loss 1.532590 on epoch=45
03/13/2022 22:18:03 - INFO - __main__ - Step 240 Global step 240 Train loss 1.390511 on epoch=47
03/13/2022 22:18:09 - INFO - __main__ - Step 250 Global step 250 Train loss 1.488656 on epoch=49
03/13/2022 22:18:10 - INFO - __main__ - Global step 250 Train loss 1.557307 Classification-F1 0.05393258426966292 on epoch=49
03/13/2022 22:18:15 - INFO - __main__ - Step 260 Global step 260 Train loss 1.496345 on epoch=51
03/13/2022 22:18:20 - INFO - __main__ - Step 270 Global step 270 Train loss 1.325804 on epoch=53
03/13/2022 22:18:25 - INFO - __main__ - Step 280 Global step 280 Train loss 1.585566 on epoch=55
03/13/2022 22:18:30 - INFO - __main__ - Step 290 Global step 290 Train loss 1.430218 on epoch=57
03/13/2022 22:18:35 - INFO - __main__ - Step 300 Global step 300 Train loss 1.251092 on epoch=59
03/13/2022 22:18:38 - INFO - __main__ - Global step 300 Train loss 1.417805 Classification-F1 0.07482517482517483 on epoch=59
03/13/2022 22:18:43 - INFO - __main__ - Step 310 Global step 310 Train loss 1.376333 on epoch=61
03/13/2022 22:18:48 - INFO - __main__ - Step 320 Global step 320 Train loss 1.224307 on epoch=63
03/13/2022 22:18:53 - INFO - __main__ - Step 330 Global step 330 Train loss 1.275161 on epoch=65
03/13/2022 22:18:58 - INFO - __main__ - Step 340 Global step 340 Train loss 1.151148 on epoch=67
03/13/2022 22:19:03 - INFO - __main__ - Step 350 Global step 350 Train loss 1.105225 on epoch=69
03/13/2022 22:19:05 - INFO - __main__ - Global step 350 Train loss 1.226435 Classification-F1 0.08498168498168499 on epoch=69
03/13/2022 22:19:10 - INFO - __main__ - Step 360 Global step 360 Train loss 1.087324 on epoch=71
03/13/2022 22:19:15 - INFO - __main__ - Step 370 Global step 370 Train loss 1.036557 on epoch=73
03/13/2022 22:19:20 - INFO - __main__ - Step 380 Global step 380 Train loss 1.085741 on epoch=75
03/13/2022 22:19:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.890462 on epoch=77
03/13/2022 22:19:30 - INFO - __main__ - Step 400 Global step 400 Train loss 1.001077 on epoch=79
03/13/2022 22:19:32 - INFO - __main__ - Global step 400 Train loss 1.020232 Classification-F1 0.11315136476426799 on epoch=79
03/13/2022 22:19:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.877391 on epoch=81
03/13/2022 22:19:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.942684 on epoch=83
03/13/2022 22:19:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.874249 on epoch=85
03/13/2022 22:19:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.932376 on epoch=87
03/13/2022 22:19:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.783580 on epoch=89
03/13/2022 22:19:59 - INFO - __main__ - Global step 450 Train loss 0.882056 Classification-F1 0.1582222222222222 on epoch=89
03/13/2022 22:20:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.693274 on epoch=91
03/13/2022 22:20:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.681124 on epoch=93
03/13/2022 22:20:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.609595 on epoch=95
03/13/2022 22:20:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.722121 on epoch=97
03/13/2022 22:20:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.602207 on epoch=99
03/13/2022 22:20:27 - INFO - __main__ - Global step 500 Train loss 0.661664 Classification-F1 0.11661290322580646 on epoch=99
03/13/2022 22:20:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.630087 on epoch=101
03/13/2022 22:20:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.575128 on epoch=103
03/13/2022 22:20:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.680714 on epoch=105
03/13/2022 22:20:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.559376 on epoch=107
03/13/2022 22:20:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.625953 on epoch=109
03/13/2022 22:20:54 - INFO - __main__ - Global step 550 Train loss 0.614252 Classification-F1 0.1525279751694846 on epoch=109
03/13/2022 22:20:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.571836 on epoch=111
03/13/2022 22:21:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.566840 on epoch=113
03/13/2022 22:21:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.558690 on epoch=115
03/13/2022 22:21:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.624918 on epoch=117
03/13/2022 22:21:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.503703 on epoch=119
03/13/2022 22:21:21 - INFO - __main__ - Global step 600 Train loss 0.565197 Classification-F1 0.17743233646334727 on epoch=119
03/13/2022 22:21:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.466102 on epoch=121
03/13/2022 22:21:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.480861 on epoch=123
03/13/2022 22:21:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.406812 on epoch=125
03/13/2022 22:21:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.435021 on epoch=127
03/13/2022 22:21:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.419108 on epoch=129
03/13/2022 22:21:49 - INFO - __main__ - Global step 650 Train loss 0.441581 Classification-F1 0.16871428571428573 on epoch=129
03/13/2022 22:21:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.385874 on epoch=131
03/13/2022 22:21:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.427539 on epoch=133
03/13/2022 22:22:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.431141 on epoch=135
03/13/2022 22:22:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.448460 on epoch=137
03/13/2022 22:22:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.426531 on epoch=139
03/13/2022 22:22:16 - INFO - __main__ - Global step 700 Train loss 0.423909 Classification-F1 0.20645962732919249 on epoch=139
03/13/2022 22:22:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.391050 on epoch=141
03/13/2022 22:22:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.413675 on epoch=143
03/13/2022 22:22:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.412949 on epoch=145
03/13/2022 22:22:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.399175 on epoch=147
03/13/2022 22:22:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.450976 on epoch=149
03/13/2022 22:22:44 - INFO - __main__ - Global step 750 Train loss 0.413565 Classification-F1 0.13501633501633503 on epoch=149
03/13/2022 22:22:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.436503 on epoch=151
03/13/2022 22:22:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.378143 on epoch=153
03/13/2022 22:22:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.380627 on epoch=155
03/13/2022 22:23:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.378153 on epoch=157
03/13/2022 22:23:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.389490 on epoch=159
03/13/2022 22:23:11 - INFO - __main__ - Global step 800 Train loss 0.392583 Classification-F1 0.16739393939393937 on epoch=159
03/13/2022 22:23:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.420490 on epoch=161
03/13/2022 22:23:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.420847 on epoch=163
03/13/2022 22:23:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.387485 on epoch=165
03/13/2022 22:23:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.440791 on epoch=167
03/13/2022 22:23:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.389494 on epoch=169
03/13/2022 22:23:38 - INFO - __main__ - Global step 850 Train loss 0.411822 Classification-F1 0.14952380952380953 on epoch=169
03/13/2022 22:23:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.381947 on epoch=171
03/13/2022 22:23:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.355752 on epoch=173
03/13/2022 22:23:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.380659 on epoch=175
03/13/2022 22:23:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.371889 on epoch=177
03/13/2022 22:24:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.374149 on epoch=179
03/13/2022 22:24:05 - INFO - __main__ - Global step 900 Train loss 0.372879 Classification-F1 0.1562799043062201 on epoch=179
03/13/2022 22:24:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.394817 on epoch=181
03/13/2022 22:24:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.420392 on epoch=183
03/13/2022 22:24:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.440670 on epoch=185
03/13/2022 22:24:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.452072 on epoch=187
03/13/2022 22:24:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.424532 on epoch=189
03/13/2022 22:24:32 - INFO - __main__ - Global step 950 Train loss 0.426497 Classification-F1 0.18733031674208145 on epoch=189
03/13/2022 22:24:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.444120 on epoch=191
03/13/2022 22:24:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.396693 on epoch=193
03/13/2022 22:24:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.459823 on epoch=195
03/13/2022 22:24:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.423450 on epoch=197
03/13/2022 22:24:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.441465 on epoch=199
03/13/2022 22:24:58 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:24:58 - INFO - __main__ - Printing 3 examples
03/13/2022 22:24:58 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/13/2022 22:24:58 - INFO - __main__ - ['No']
03/13/2022 22:24:58 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/13/2022 22:24:58 - INFO - __main__ - ['No']
03/13/2022 22:24:58 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/13/2022 22:24:58 - INFO - __main__ - ['No']
03/13/2022 22:24:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 22:24:58 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:24:58 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 22:24:58 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:24:58 - INFO - __main__ - Printing 3 examples
03/13/2022 22:24:58 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/13/2022 22:24:58 - INFO - __main__ - ['No']
03/13/2022 22:24:58 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/13/2022 22:24:58 - INFO - __main__ - ['No']
03/13/2022 22:24:58 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/13/2022 22:24:58 - INFO - __main__ - ['No']
03/13/2022 22:24:58 - INFO - __main__ - Tokenizing Input ...
03/13/2022 22:24:58 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:24:58 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 22:24:59 - INFO - __main__ - Global step 1000 Train loss 0.433110 Classification-F1 0.19589122883195834 on epoch=199
03/13/2022 22:24:59 - INFO - __main__ - save last model!
03/13/2022 22:25:06 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 22:25:07 - INFO - __main__ - Start tokenizing ... 6700 instances
03/13/2022 22:25:07 - INFO - __main__ - Printing 3 examples
03/13/2022 22:25:07 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/13/2022 22:25:07 - INFO - __main__ - ['No']
03/13/2022 22:25:07 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/13/2022 22:25:07 - INFO - __main__ - ['Yes']
03/13/2022 22:25:07 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/13/2022 22:25:07 - INFO - __main__ - ['Yes']
03/13/2022 22:25:07 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 22:25:10 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:25:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 22:25:11 - INFO - __main__ - Starting training!
03/13/2022 22:25:16 - INFO - __main__ - Loaded 6700 examples from test data
03/13/2022 22:27:37 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_100_0.0003_8_predictions.txt
03/13/2022 22:27:37 - INFO - __main__ - Classification-F1 on test data: 0.1693
03/13/2022 22:27:38 - INFO - __main__ - prefix=circa_16_100, lr=0.0003, bsz=8, dev_performance=0.20645962732919249, test_performance=0.16926512721533798
03/13/2022 22:27:38 - INFO - __main__ - Running ... prefix=circa_16_100, lr=0.0002, bsz=8 ...
03/13/2022 22:27:39 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:27:39 - INFO - __main__ - Printing 3 examples
03/13/2022 22:27:39 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/13/2022 22:27:39 - INFO - __main__ - ['No']
03/13/2022 22:27:39 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/13/2022 22:27:39 - INFO - __main__ - ['No']
03/13/2022 22:27:39 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/13/2022 22:27:39 - INFO - __main__ - ['No']
03/13/2022 22:27:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 22:27:39 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:27:39 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 22:27:39 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:27:39 - INFO - __main__ - Printing 3 examples
03/13/2022 22:27:39 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/13/2022 22:27:39 - INFO - __main__ - ['No']
03/13/2022 22:27:39 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/13/2022 22:27:39 - INFO - __main__ - ['No']
03/13/2022 22:27:39 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/13/2022 22:27:39 - INFO - __main__ - ['No']
03/13/2022 22:27:39 - INFO - __main__ - Tokenizing Input ...
03/13/2022 22:27:39 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:27:39 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 22:27:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 22:27:51 - INFO - __main__ - Starting training!
03/13/2022 22:27:56 - INFO - __main__ - Step 10 Global step 10 Train loss 22.459436 on epoch=1
03/13/2022 22:28:00 - INFO - __main__ - Step 20 Global step 20 Train loss 19.309376 on epoch=3
03/13/2022 22:28:05 - INFO - __main__ - Step 30 Global step 30 Train loss 12.226726 on epoch=5
03/13/2022 22:28:10 - INFO - __main__ - Step 40 Global step 40 Train loss 9.095033 on epoch=7
03/13/2022 22:28:15 - INFO - __main__ - Step 50 Global step 50 Train loss 7.991477 on epoch=9
03/13/2022 22:28:18 - INFO - __main__ - Global step 50 Train loss 14.216410 Classification-F1 0.0 on epoch=9
03/13/2022 22:28:23 - INFO - __main__ - Step 60 Global step 60 Train loss 7.747700 on epoch=11
03/13/2022 22:28:28 - INFO - __main__ - Step 70 Global step 70 Train loss 7.442988 on epoch=13
03/13/2022 22:28:33 - INFO - __main__ - Step 80 Global step 80 Train loss 7.149363 on epoch=15
03/13/2022 22:28:38 - INFO - __main__ - Step 90 Global step 90 Train loss 6.868800 on epoch=17
03/13/2022 22:28:43 - INFO - __main__ - Step 100 Global step 100 Train loss 6.413770 on epoch=19
03/13/2022 22:28:46 - INFO - __main__ - Global step 100 Train loss 7.124524 Classification-F1 0.0 on epoch=19
03/13/2022 22:28:51 - INFO - __main__ - Step 110 Global step 110 Train loss 5.986556 on epoch=21
03/13/2022 22:28:56 - INFO - __main__ - Step 120 Global step 120 Train loss 5.609616 on epoch=23
03/13/2022 22:29:01 - INFO - __main__ - Step 130 Global step 130 Train loss 5.544903 on epoch=25
03/13/2022 22:29:06 - INFO - __main__ - Step 140 Global step 140 Train loss 4.707889 on epoch=27
03/13/2022 22:29:11 - INFO - __main__ - Step 150 Global step 150 Train loss 4.485539 on epoch=29
03/13/2022 22:29:13 - INFO - __main__ - Global step 150 Train loss 5.266901 Classification-F1 0.0 on epoch=29
03/13/2022 22:29:18 - INFO - __main__ - Step 160 Global step 160 Train loss 3.396844 on epoch=31
03/13/2022 22:29:23 - INFO - __main__ - Step 170 Global step 170 Train loss 2.932217 on epoch=33
03/13/2022 22:29:28 - INFO - __main__ - Step 180 Global step 180 Train loss 2.722880 on epoch=35
03/13/2022 22:29:33 - INFO - __main__ - Step 190 Global step 190 Train loss 1.826070 on epoch=37
03/13/2022 22:29:38 - INFO - __main__ - Step 200 Global step 200 Train loss 1.802941 on epoch=39
03/13/2022 22:29:40 - INFO - __main__ - Global step 200 Train loss 2.536190 Classification-F1 0.2253869969040248 on epoch=39
03/13/2022 22:29:46 - INFO - __main__ - Step 210 Global step 210 Train loss 1.616465 on epoch=41
03/13/2022 22:29:50 - INFO - __main__ - Step 220 Global step 220 Train loss 1.673227 on epoch=43
03/13/2022 22:29:55 - INFO - __main__ - Step 230 Global step 230 Train loss 1.832597 on epoch=45
03/13/2022 22:30:00 - INFO - __main__ - Step 240 Global step 240 Train loss 1.841715 on epoch=47
03/13/2022 22:30:05 - INFO - __main__ - Step 250 Global step 250 Train loss 1.806029 on epoch=49
03/13/2022 22:30:07 - INFO - __main__ - Global step 250 Train loss 1.754007 Classification-F1 0.06666666666666668 on epoch=49
03/13/2022 22:30:12 - INFO - __main__ - Step 260 Global step 260 Train loss 2.948939 on epoch=51
03/13/2022 22:30:17 - INFO - __main__ - Step 270 Global step 270 Train loss 1.671279 on epoch=53
03/13/2022 22:30:22 - INFO - __main__ - Step 280 Global step 280 Train loss 1.849477 on epoch=55
03/13/2022 22:30:27 - INFO - __main__ - Step 290 Global step 290 Train loss 1.269108 on epoch=57
03/13/2022 22:30:32 - INFO - __main__ - Step 300 Global step 300 Train loss 1.158517 on epoch=59
03/13/2022 22:30:34 - INFO - __main__ - Global step 300 Train loss 1.779464 Classification-F1 0.22999999999999998 on epoch=59
03/13/2022 22:30:40 - INFO - __main__ - Step 310 Global step 310 Train loss 1.510036 on epoch=61
03/13/2022 22:30:45 - INFO - __main__ - Step 320 Global step 320 Train loss 1.600536 on epoch=63
03/13/2022 22:30:50 - INFO - __main__ - Step 330 Global step 330 Train loss 1.437688 on epoch=65
03/13/2022 22:30:55 - INFO - __main__ - Step 340 Global step 340 Train loss 1.692088 on epoch=67
03/13/2022 22:31:00 - INFO - __main__ - Step 350 Global step 350 Train loss 1.575832 on epoch=69
03/13/2022 22:31:02 - INFO - __main__ - Global step 350 Train loss 1.563236 Classification-F1 0.20315027671349511 on epoch=69
03/13/2022 22:31:07 - INFO - __main__ - Step 360 Global step 360 Train loss 1.357580 on epoch=71
03/13/2022 22:31:12 - INFO - __main__ - Step 370 Global step 370 Train loss 1.196401 on epoch=73
03/13/2022 22:31:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.959176 on epoch=75
03/13/2022 22:31:22 - INFO - __main__ - Step 390 Global step 390 Train loss 1.186558 on epoch=77
03/13/2022 22:31:27 - INFO - __main__ - Step 400 Global step 400 Train loss 1.467449 on epoch=79
03/13/2022 22:31:29 - INFO - __main__ - Global step 400 Train loss 1.233433 Classification-F1 0.2391656023839932 on epoch=79
03/13/2022 22:31:34 - INFO - __main__ - Step 410 Global step 410 Train loss 1.411247 on epoch=81
03/13/2022 22:31:39 - INFO - __main__ - Step 420 Global step 420 Train loss 1.349776 on epoch=83
03/13/2022 22:31:44 - INFO - __main__ - Step 430 Global step 430 Train loss 1.330680 on epoch=85
03/13/2022 22:31:49 - INFO - __main__ - Step 440 Global step 440 Train loss 1.111305 on epoch=87
03/13/2022 22:31:54 - INFO - __main__ - Step 450 Global step 450 Train loss 1.317616 on epoch=89
03/13/2022 22:31:56 - INFO - __main__ - Global step 450 Train loss 1.304125 Classification-F1 0.2123456790123457 on epoch=89
03/13/2022 22:32:01 - INFO - __main__ - Step 460 Global step 460 Train loss 1.034750 on epoch=91
03/13/2022 22:32:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.959030 on epoch=93
03/13/2022 22:32:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.871721 on epoch=95
03/13/2022 22:32:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.833281 on epoch=97
03/13/2022 22:32:21 - INFO - __main__ - Step 500 Global step 500 Train loss 1.001066 on epoch=99
03/13/2022 22:32:23 - INFO - __main__ - Global step 500 Train loss 0.939970 Classification-F1 0.21749999999999997 on epoch=99
03/13/2022 22:32:28 - INFO - __main__ - Step 510 Global step 510 Train loss 1.144808 on epoch=101
03/13/2022 22:32:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.745155 on epoch=103
03/13/2022 22:32:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.970597 on epoch=105
03/13/2022 22:32:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.839673 on epoch=107
03/13/2022 22:32:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.754212 on epoch=109
03/13/2022 22:32:50 - INFO - __main__ - Global step 550 Train loss 0.890889 Classification-F1 0.2011149825783972 on epoch=109
03/13/2022 22:32:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.854613 on epoch=111
03/13/2022 22:33:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.884955 on epoch=113
03/13/2022 22:33:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.857162 on epoch=115
03/13/2022 22:33:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.711416 on epoch=117
03/13/2022 22:33:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.695217 on epoch=119
03/13/2022 22:33:17 - INFO - __main__ - Global step 600 Train loss 0.800673 Classification-F1 0.23122171945701356 on epoch=119
03/13/2022 22:33:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.739071 on epoch=121
03/13/2022 22:33:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.665188 on epoch=123
03/13/2022 22:33:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.687541 on epoch=125
03/13/2022 22:33:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.757279 on epoch=127
03/13/2022 22:33:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.723879 on epoch=129
03/13/2022 22:33:44 - INFO - __main__ - Global step 650 Train loss 0.714592 Classification-F1 0.20575227241893906 on epoch=129
03/13/2022 22:33:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.512184 on epoch=131
03/13/2022 22:33:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.696941 on epoch=133
03/13/2022 22:33:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.637913 on epoch=135
03/13/2022 22:34:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.714738 on epoch=137
03/13/2022 22:34:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.526151 on epoch=139
03/13/2022 22:34:11 - INFO - __main__ - Global step 700 Train loss 0.617586 Classification-F1 0.14575424575424573 on epoch=139
03/13/2022 22:34:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.740118 on epoch=141
03/13/2022 22:34:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.581624 on epoch=143
03/13/2022 22:34:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.539878 on epoch=145
03/13/2022 22:34:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.524263 on epoch=147
03/13/2022 22:34:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.541285 on epoch=149
03/13/2022 22:34:38 - INFO - __main__ - Global step 750 Train loss 0.585433 Classification-F1 0.2163812033377251 on epoch=149
03/13/2022 22:34:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.538729 on epoch=151
03/13/2022 22:34:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.502082 on epoch=153
03/13/2022 22:34:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.418734 on epoch=155
03/13/2022 22:34:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.546151 on epoch=157
03/13/2022 22:35:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.531226 on epoch=159
03/13/2022 22:35:05 - INFO - __main__ - Global step 800 Train loss 0.507384 Classification-F1 0.2070175438596491 on epoch=159
03/13/2022 22:35:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.492417 on epoch=161
03/13/2022 22:35:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.406577 on epoch=163
03/13/2022 22:35:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.465013 on epoch=165
03/13/2022 22:35:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.424339 on epoch=167
03/13/2022 22:35:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.405257 on epoch=169
03/13/2022 22:35:32 - INFO - __main__ - Global step 850 Train loss 0.438720 Classification-F1 0.23499999999999996 on epoch=169
03/13/2022 22:35:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.440155 on epoch=171
03/13/2022 22:35:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.391359 on epoch=173
03/13/2022 22:35:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.431705 on epoch=175
03/13/2022 22:35:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.446421 on epoch=177
03/13/2022 22:35:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.405484 on epoch=179
03/13/2022 22:35:59 - INFO - __main__ - Global step 900 Train loss 0.423025 Classification-F1 0.21228070175438596 on epoch=179
03/13/2022 22:36:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.395479 on epoch=181
03/13/2022 22:36:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.397828 on epoch=183
03/13/2022 22:36:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.520200 on epoch=185
03/13/2022 22:36:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.494127 on epoch=187
03/13/2022 22:36:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.412127 on epoch=189
03/13/2022 22:36:26 - INFO - __main__ - Global step 950 Train loss 0.443952 Classification-F1 0.23581011351909184 on epoch=189
03/13/2022 22:36:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.362985 on epoch=191
03/13/2022 22:36:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.464782 on epoch=193
03/13/2022 22:36:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.406515 on epoch=195
03/13/2022 22:36:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.434552 on epoch=197
03/13/2022 22:36:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.424740 on epoch=199
03/13/2022 22:36:52 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:36:52 - INFO - __main__ - Printing 3 examples
03/13/2022 22:36:52 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/13/2022 22:36:52 - INFO - __main__ - ['No']
03/13/2022 22:36:52 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/13/2022 22:36:52 - INFO - __main__ - ['No']
03/13/2022 22:36:52 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/13/2022 22:36:52 - INFO - __main__ - ['No']
03/13/2022 22:36:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 22:36:52 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:36:52 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 22:36:52 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:36:52 - INFO - __main__ - Printing 3 examples
03/13/2022 22:36:52 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/13/2022 22:36:52 - INFO - __main__ - ['No']
03/13/2022 22:36:52 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/13/2022 22:36:52 - INFO - __main__ - ['No']
03/13/2022 22:36:52 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/13/2022 22:36:52 - INFO - __main__ - ['No']
03/13/2022 22:36:52 - INFO - __main__ - Tokenizing Input ...
03/13/2022 22:36:52 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:36:52 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 22:36:53 - INFO - __main__ - Global step 1000 Train loss 0.418715 Classification-F1 0.31611415580725044 on epoch=199
03/13/2022 22:36:53 - INFO - __main__ - save last model!
03/13/2022 22:37:00 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 22:37:01 - INFO - __main__ - Start tokenizing ... 6700 instances
03/13/2022 22:37:01 - INFO - __main__ - Printing 3 examples
03/13/2022 22:37:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/13/2022 22:37:01 - INFO - __main__ - ['No']
03/13/2022 22:37:01 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/13/2022 22:37:01 - INFO - __main__ - ['Yes']
03/13/2022 22:37:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/13/2022 22:37:01 - INFO - __main__ - ['Yes']
03/13/2022 22:37:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 22:37:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 22:37:03 - INFO - __main__ - Starting training!
03/13/2022 22:37:04 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:37:11 - INFO - __main__ - Loaded 6700 examples from test data
03/13/2022 22:39:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_100_0.0002_8_predictions.txt
03/13/2022 22:39:29 - INFO - __main__ - Classification-F1 on test data: 0.2204
03/13/2022 22:39:29 - INFO - __main__ - prefix=circa_16_100, lr=0.0002, bsz=8, dev_performance=0.31611415580725044, test_performance=0.22035745530758666
03/13/2022 22:39:29 - INFO - __main__ - Running ... prefix=circa_16_100, lr=0.0001, bsz=8 ...
03/13/2022 22:39:30 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:39:30 - INFO - __main__ - Printing 3 examples
03/13/2022 22:39:30 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/13/2022 22:39:30 - INFO - __main__ - ['No']
03/13/2022 22:39:30 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/13/2022 22:39:30 - INFO - __main__ - ['No']
03/13/2022 22:39:30 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/13/2022 22:39:30 - INFO - __main__ - ['No']
03/13/2022 22:39:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 22:39:30 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:39:30 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 22:39:30 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:39:30 - INFO - __main__ - Printing 3 examples
03/13/2022 22:39:30 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/13/2022 22:39:30 - INFO - __main__ - ['No']
03/13/2022 22:39:30 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/13/2022 22:39:30 - INFO - __main__ - ['No']
03/13/2022 22:39:30 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/13/2022 22:39:30 - INFO - __main__ - ['No']
03/13/2022 22:39:30 - INFO - __main__ - Tokenizing Input ...
03/13/2022 22:39:30 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:39:31 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 22:39:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 22:39:42 - INFO - __main__ - Starting training!
03/13/2022 22:39:46 - INFO - __main__ - Step 10 Global step 10 Train loss 21.335501 on epoch=1
03/13/2022 22:39:51 - INFO - __main__ - Step 20 Global step 20 Train loss 18.361118 on epoch=3
03/13/2022 22:39:56 - INFO - __main__ - Step 30 Global step 30 Train loss 17.455219 on epoch=5
03/13/2022 22:40:01 - INFO - __main__ - Step 40 Global step 40 Train loss 15.484818 on epoch=7
03/13/2022 22:40:06 - INFO - __main__ - Step 50 Global step 50 Train loss 12.375029 on epoch=9
03/13/2022 22:40:26 - INFO - __main__ - Global step 50 Train loss 17.002337 Classification-F1 0.0 on epoch=9
03/13/2022 22:40:32 - INFO - __main__ - Step 60 Global step 60 Train loss 10.997149 on epoch=11
03/13/2022 22:40:37 - INFO - __main__ - Step 70 Global step 70 Train loss 9.957998 on epoch=13
03/13/2022 22:40:42 - INFO - __main__ - Step 80 Global step 80 Train loss 9.005492 on epoch=15
03/13/2022 22:40:47 - INFO - __main__ - Step 90 Global step 90 Train loss 8.533400 on epoch=17
03/13/2022 22:40:52 - INFO - __main__ - Step 100 Global step 100 Train loss 8.378352 on epoch=19
03/13/2022 22:40:55 - INFO - __main__ - Global step 100 Train loss 9.374478 Classification-F1 0.0 on epoch=19
03/13/2022 22:41:00 - INFO - __main__ - Step 110 Global step 110 Train loss 7.929177 on epoch=21
03/13/2022 22:41:05 - INFO - __main__ - Step 120 Global step 120 Train loss 7.743350 on epoch=23
03/13/2022 22:41:10 - INFO - __main__ - Step 130 Global step 130 Train loss 7.301020 on epoch=25
03/13/2022 22:41:15 - INFO - __main__ - Step 140 Global step 140 Train loss 6.659499 on epoch=27
03/13/2022 22:41:20 - INFO - __main__ - Step 150 Global step 150 Train loss 7.503194 on epoch=29
03/13/2022 22:41:22 - INFO - __main__ - Global step 150 Train loss 7.427248 Classification-F1 0.0 on epoch=29
03/13/2022 22:41:27 - INFO - __main__ - Step 160 Global step 160 Train loss 6.876660 on epoch=31
03/13/2022 22:41:32 - INFO - __main__ - Step 170 Global step 170 Train loss 6.741200 on epoch=33
03/13/2022 22:41:37 - INFO - __main__ - Step 180 Global step 180 Train loss 6.388987 on epoch=35
03/13/2022 22:41:42 - INFO - __main__ - Step 190 Global step 190 Train loss 6.286591 on epoch=37
03/13/2022 22:41:47 - INFO - __main__ - Step 200 Global step 200 Train loss 6.042341 on epoch=39
03/13/2022 22:41:50 - INFO - __main__ - Global step 200 Train loss 6.467156 Classification-F1 0.0 on epoch=39
03/13/2022 22:41:55 - INFO - __main__ - Step 210 Global step 210 Train loss 6.074160 on epoch=41
03/13/2022 22:42:00 - INFO - __main__ - Step 220 Global step 220 Train loss 5.845398 on epoch=43
03/13/2022 22:42:05 - INFO - __main__ - Step 230 Global step 230 Train loss 5.518592 on epoch=45
03/13/2022 22:42:10 - INFO - __main__ - Step 240 Global step 240 Train loss 5.525958 on epoch=47
03/13/2022 22:42:15 - INFO - __main__ - Step 250 Global step 250 Train loss 5.559228 on epoch=49
03/13/2022 22:42:17 - INFO - __main__ - Global step 250 Train loss 5.704668 Classification-F1 0.0 on epoch=49
03/13/2022 22:42:22 - INFO - __main__ - Step 260 Global step 260 Train loss 5.230878 on epoch=51
03/13/2022 22:42:27 - INFO - __main__ - Step 270 Global step 270 Train loss 4.834360 on epoch=53
03/13/2022 22:42:32 - INFO - __main__ - Step 280 Global step 280 Train loss 4.746566 on epoch=55
03/13/2022 22:42:37 - INFO - __main__ - Step 290 Global step 290 Train loss 4.963229 on epoch=57
03/13/2022 22:42:42 - INFO - __main__ - Step 300 Global step 300 Train loss 4.389592 on epoch=59
03/13/2022 22:42:45 - INFO - __main__ - Global step 300 Train loss 4.832925 Classification-F1 0.0 on epoch=59
03/13/2022 22:42:50 - INFO - __main__ - Step 310 Global step 310 Train loss 4.033833 on epoch=61
03/13/2022 22:42:55 - INFO - __main__ - Step 320 Global step 320 Train loss 4.318946 on epoch=63
03/13/2022 22:43:00 - INFO - __main__ - Step 330 Global step 330 Train loss 3.901692 on epoch=65
03/13/2022 22:43:05 - INFO - __main__ - Step 340 Global step 340 Train loss 3.581323 on epoch=67
03/13/2022 22:43:10 - INFO - __main__ - Step 350 Global step 350 Train loss 3.316730 on epoch=69
03/13/2022 22:43:12 - INFO - __main__ - Global step 350 Train loss 3.830505 Classification-F1 0.0 on epoch=69
03/13/2022 22:43:17 - INFO - __main__ - Step 360 Global step 360 Train loss 2.663115 on epoch=71
03/13/2022 22:43:22 - INFO - __main__ - Step 370 Global step 370 Train loss 2.453192 on epoch=73
03/13/2022 22:43:28 - INFO - __main__ - Step 380 Global step 380 Train loss 2.438125 on epoch=75
03/13/2022 22:43:33 - INFO - __main__ - Step 390 Global step 390 Train loss 2.334732 on epoch=77
03/13/2022 22:43:38 - INFO - __main__ - Step 400 Global step 400 Train loss 1.917606 on epoch=79
03/13/2022 22:43:40 - INFO - __main__ - Global step 400 Train loss 2.361354 Classification-F1 0.1243421052631579 on epoch=79
03/13/2022 22:43:46 - INFO - __main__ - Step 410 Global step 410 Train loss 1.999053 on epoch=81
03/13/2022 22:43:51 - INFO - __main__ - Step 420 Global step 420 Train loss 2.061831 on epoch=83
03/13/2022 22:43:56 - INFO - __main__ - Step 430 Global step 430 Train loss 1.340501 on epoch=85
03/13/2022 22:44:01 - INFO - __main__ - Step 440 Global step 440 Train loss 1.652636 on epoch=87
03/13/2022 22:44:06 - INFO - __main__ - Step 450 Global step 450 Train loss 1.803803 on epoch=89
03/13/2022 22:44:08 - INFO - __main__ - Global step 450 Train loss 1.771565 Classification-F1 0.25772005772005774 on epoch=89
03/13/2022 22:44:13 - INFO - __main__ - Step 460 Global step 460 Train loss 1.518891 on epoch=91
03/13/2022 22:44:19 - INFO - __main__ - Step 470 Global step 470 Train loss 2.126703 on epoch=93
03/13/2022 22:44:24 - INFO - __main__ - Step 480 Global step 480 Train loss 1.446302 on epoch=95
03/13/2022 22:44:29 - INFO - __main__ - Step 490 Global step 490 Train loss 1.617996 on epoch=97
03/13/2022 22:44:34 - INFO - __main__ - Step 500 Global step 500 Train loss 1.691310 on epoch=99
03/13/2022 22:44:35 - INFO - __main__ - Global step 500 Train loss 1.680241 Classification-F1 0.16785577291906403 on epoch=99
03/13/2022 22:44:40 - INFO - __main__ - Step 510 Global step 510 Train loss 1.946748 on epoch=101
03/13/2022 22:44:45 - INFO - __main__ - Step 520 Global step 520 Train loss 1.562992 on epoch=103
03/13/2022 22:44:50 - INFO - __main__ - Step 530 Global step 530 Train loss 1.473251 on epoch=105
03/13/2022 22:44:55 - INFO - __main__ - Step 540 Global step 540 Train loss 1.369843 on epoch=107
03/13/2022 22:45:00 - INFO - __main__ - Step 550 Global step 550 Train loss 1.325531 on epoch=109
03/13/2022 22:45:02 - INFO - __main__ - Global step 550 Train loss 1.535673 Classification-F1 0.18261603375527424 on epoch=109
03/13/2022 22:45:07 - INFO - __main__ - Step 560 Global step 560 Train loss 1.461664 on epoch=111
03/13/2022 22:45:12 - INFO - __main__ - Step 570 Global step 570 Train loss 1.321395 on epoch=113
03/13/2022 22:45:17 - INFO - __main__ - Step 580 Global step 580 Train loss 1.371482 on epoch=115
03/13/2022 22:45:22 - INFO - __main__ - Step 590 Global step 590 Train loss 1.415874 on epoch=117
03/13/2022 22:45:27 - INFO - __main__ - Step 600 Global step 600 Train loss 1.593524 on epoch=119
03/13/2022 22:45:29 - INFO - __main__ - Global step 600 Train loss 1.432788 Classification-F1 0.21004566210045664 on epoch=119
03/13/2022 22:45:34 - INFO - __main__ - Step 610 Global step 610 Train loss 1.548111 on epoch=121
03/13/2022 22:45:39 - INFO - __main__ - Step 620 Global step 620 Train loss 1.321154 on epoch=123
03/13/2022 22:45:44 - INFO - __main__ - Step 630 Global step 630 Train loss 1.464806 on epoch=125
03/13/2022 22:45:49 - INFO - __main__ - Step 640 Global step 640 Train loss 1.370672 on epoch=127
03/13/2022 22:45:54 - INFO - __main__ - Step 650 Global step 650 Train loss 1.458791 on epoch=129
03/13/2022 22:45:56 - INFO - __main__ - Global step 650 Train loss 1.432707 Classification-F1 0.24222910216718266 on epoch=129
03/13/2022 22:46:01 - INFO - __main__ - Step 660 Global step 660 Train loss 1.357919 on epoch=131
03/13/2022 22:46:06 - INFO - __main__ - Step 670 Global step 670 Train loss 1.578286 on epoch=133
03/13/2022 22:46:11 - INFO - __main__ - Step 680 Global step 680 Train loss 1.191360 on epoch=135
03/13/2022 22:46:16 - INFO - __main__ - Step 690 Global step 690 Train loss 1.428641 on epoch=137
03/13/2022 22:46:21 - INFO - __main__ - Step 700 Global step 700 Train loss 1.606433 on epoch=139
03/13/2022 22:46:23 - INFO - __main__ - Global step 700 Train loss 1.432528 Classification-F1 0.22546663462308741 on epoch=139
03/13/2022 22:46:28 - INFO - __main__ - Step 710 Global step 710 Train loss 1.271314 on epoch=141
03/13/2022 22:46:33 - INFO - __main__ - Step 720 Global step 720 Train loss 1.336604 on epoch=143
03/13/2022 22:46:38 - INFO - __main__ - Step 730 Global step 730 Train loss 1.295448 on epoch=145
03/13/2022 22:46:42 - INFO - __main__ - Step 740 Global step 740 Train loss 1.442014 on epoch=147
03/13/2022 22:46:47 - INFO - __main__ - Step 750 Global step 750 Train loss 1.447662 on epoch=149
03/13/2022 22:46:49 - INFO - __main__ - Global step 750 Train loss 1.358608 Classification-F1 0.19166666666666665 on epoch=149
03/13/2022 22:46:54 - INFO - __main__ - Step 760 Global step 760 Train loss 1.065686 on epoch=151
03/13/2022 22:46:59 - INFO - __main__ - Step 770 Global step 770 Train loss 1.289785 on epoch=153
03/13/2022 22:47:04 - INFO - __main__ - Step 780 Global step 780 Train loss 1.062872 on epoch=155
03/13/2022 22:47:09 - INFO - __main__ - Step 790 Global step 790 Train loss 1.116266 on epoch=157
03/13/2022 22:47:14 - INFO - __main__ - Step 800 Global step 800 Train loss 1.201261 on epoch=159
03/13/2022 22:47:16 - INFO - __main__ - Global step 800 Train loss 1.147174 Classification-F1 0.22412955465587042 on epoch=159
03/13/2022 22:47:21 - INFO - __main__ - Step 810 Global step 810 Train loss 1.174188 on epoch=161
03/13/2022 22:47:26 - INFO - __main__ - Step 820 Global step 820 Train loss 1.172463 on epoch=163
03/13/2022 22:47:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.964853 on epoch=165
03/13/2022 22:47:36 - INFO - __main__ - Step 840 Global step 840 Train loss 1.138576 on epoch=167
03/13/2022 22:47:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.874633 on epoch=169
03/13/2022 22:47:43 - INFO - __main__ - Global step 850 Train loss 1.064943 Classification-F1 0.23403174603174604 on epoch=169
03/13/2022 22:47:48 - INFO - __main__ - Step 860 Global step 860 Train loss 1.126802 on epoch=171
03/13/2022 22:47:53 - INFO - __main__ - Step 870 Global step 870 Train loss 1.287985 on epoch=173
03/13/2022 22:47:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.936818 on epoch=175
03/13/2022 22:48:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.971594 on epoch=177
03/13/2022 22:48:08 - INFO - __main__ - Step 900 Global step 900 Train loss 1.097605 on epoch=179
03/13/2022 22:48:09 - INFO - __main__ - Global step 900 Train loss 1.084161 Classification-F1 0.238036454018227 on epoch=179
03/13/2022 22:48:14 - INFO - __main__ - Step 910 Global step 910 Train loss 1.087395 on epoch=181
03/13/2022 22:48:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.996640 on epoch=183
03/13/2022 22:48:25 - INFO - __main__ - Step 930 Global step 930 Train loss 1.235132 on epoch=185
03/13/2022 22:48:30 - INFO - __main__ - Step 940 Global step 940 Train loss 1.097079 on epoch=187
03/13/2022 22:48:35 - INFO - __main__ - Step 950 Global step 950 Train loss 1.031518 on epoch=189
03/13/2022 22:48:36 - INFO - __main__ - Global step 950 Train loss 1.089553 Classification-F1 0.19427601261217559 on epoch=189
03/13/2022 22:48:42 - INFO - __main__ - Step 960 Global step 960 Train loss 1.030525 on epoch=191
03/13/2022 22:48:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.981835 on epoch=193
03/13/2022 22:48:52 - INFO - __main__ - Step 980 Global step 980 Train loss 1.022797 on epoch=195
03/13/2022 22:48:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.999163 on epoch=197
03/13/2022 22:49:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.890310 on epoch=199
03/13/2022 22:49:03 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:49:03 - INFO - __main__ - Printing 3 examples
03/13/2022 22:49:03 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/13/2022 22:49:03 - INFO - __main__ - ['No']
03/13/2022 22:49:03 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/13/2022 22:49:03 - INFO - __main__ - ['No']
03/13/2022 22:49:03 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/13/2022 22:49:03 - INFO - __main__ - ['No']
03/13/2022 22:49:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 22:49:03 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:49:03 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 22:49:03 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:49:03 - INFO - __main__ - Printing 3 examples
03/13/2022 22:49:03 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/13/2022 22:49:03 - INFO - __main__ - ['No']
03/13/2022 22:49:03 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/13/2022 22:49:03 - INFO - __main__ - ['No']
03/13/2022 22:49:03 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/13/2022 22:49:03 - INFO - __main__ - ['No']
03/13/2022 22:49:03 - INFO - __main__ - Tokenizing Input ...
03/13/2022 22:49:03 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:49:03 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 22:49:04 - INFO - __main__ - Global step 1000 Train loss 0.984926 Classification-F1 0.1590909090909091 on epoch=199
03/13/2022 22:49:04 - INFO - __main__ - save last model!
03/13/2022 22:49:10 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 22:49:11 - INFO - __main__ - Start tokenizing ... 6700 instances
03/13/2022 22:49:11 - INFO - __main__ - Printing 3 examples
03/13/2022 22:49:11 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/13/2022 22:49:11 - INFO - __main__ - ['No']
03/13/2022 22:49:11 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/13/2022 22:49:11 - INFO - __main__ - ['Yes']
03/13/2022 22:49:11 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/13/2022 22:49:11 - INFO - __main__ - ['Yes']
03/13/2022 22:49:11 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 22:49:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 22:49:14 - INFO - __main__ - Starting training!
03/13/2022 22:49:14 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:49:21 - INFO - __main__ - Loaded 6700 examples from test data
03/13/2022 22:51:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_100_0.0001_8_predictions.txt
03/13/2022 22:51:43 - INFO - __main__ - Classification-F1 on test data: 0.1847
03/13/2022 22:51:43 - INFO - __main__ - prefix=circa_16_100, lr=0.0001, bsz=8, dev_performance=0.25772005772005774, test_performance=0.18470893071075875
03/13/2022 22:51:43 - INFO - __main__ - Running ... prefix=circa_16_13, lr=0.0005, bsz=8 ...
03/13/2022 22:51:44 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:51:44 - INFO - __main__ - Printing 3 examples
03/13/2022 22:51:44 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/13/2022 22:51:44 - INFO - __main__ - ['No']
03/13/2022 22:51:44 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/13/2022 22:51:44 - INFO - __main__ - ['No']
03/13/2022 22:51:44 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/13/2022 22:51:44 - INFO - __main__ - ['No']
03/13/2022 22:51:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 22:51:44 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:51:44 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 22:51:44 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 22:51:44 - INFO - __main__ - Printing 3 examples
03/13/2022 22:51:44 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/13/2022 22:51:44 - INFO - __main__ - ['No']
03/13/2022 22:51:44 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/13/2022 22:51:44 - INFO - __main__ - ['No']
03/13/2022 22:51:44 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/13/2022 22:51:44 - INFO - __main__ - ['No']
03/13/2022 22:51:44 - INFO - __main__ - Tokenizing Input ...
03/13/2022 22:51:44 - INFO - __main__ - Tokenizing Output ...
03/13/2022 22:51:44 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 22:51:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 22:51:56 - INFO - __main__ - Starting training!
03/13/2022 22:52:00 - INFO - __main__ - Step 10 Global step 10 Train loss 21.914249 on epoch=1
03/13/2022 22:52:04 - INFO - __main__ - Step 20 Global step 20 Train loss 17.663692 on epoch=3
03/13/2022 22:52:09 - INFO - __main__ - Step 30 Global step 30 Train loss 13.101756 on epoch=5
03/13/2022 22:52:14 - INFO - __main__ - Step 40 Global step 40 Train loss 10.062678 on epoch=7
03/13/2022 22:52:19 - INFO - __main__ - Step 50 Global step 50 Train loss 8.338186 on epoch=9
03/13/2022 22:52:22 - INFO - __main__ - Global step 50 Train loss 14.216113 Classification-F1 0.0 on epoch=9
03/13/2022 22:52:28 - INFO - __main__ - Step 60 Global step 60 Train loss 7.200233 on epoch=11
03/13/2022 22:52:33 - INFO - __main__ - Step 70 Global step 70 Train loss 6.259058 on epoch=13
03/13/2022 22:52:38 - INFO - __main__ - Step 80 Global step 80 Train loss 5.139096 on epoch=15
03/13/2022 22:52:43 - INFO - __main__ - Step 90 Global step 90 Train loss 3.773476 on epoch=17
03/13/2022 22:52:48 - INFO - __main__ - Step 100 Global step 100 Train loss 1.998023 on epoch=19
03/13/2022 22:52:50 - INFO - __main__ - Global step 100 Train loss 4.873978 Classification-F1 0.12045454545454545 on epoch=19
03/13/2022 22:52:56 - INFO - __main__ - Step 110 Global step 110 Train loss 1.747067 on epoch=21
03/13/2022 22:53:01 - INFO - __main__ - Step 120 Global step 120 Train loss 1.879536 on epoch=23
03/13/2022 22:53:06 - INFO - __main__ - Step 130 Global step 130 Train loss 1.244623 on epoch=25
03/13/2022 22:53:11 - INFO - __main__ - Step 140 Global step 140 Train loss 1.459927 on epoch=27
03/13/2022 22:53:16 - INFO - __main__ - Step 150 Global step 150 Train loss 1.223795 on epoch=29
03/13/2022 22:53:18 - INFO - __main__ - Global step 150 Train loss 1.510990 Classification-F1 0.23901234567901236 on epoch=29
03/13/2022 22:53:24 - INFO - __main__ - Step 160 Global step 160 Train loss 1.727839 on epoch=31
03/13/2022 22:53:29 - INFO - __main__ - Step 170 Global step 170 Train loss 1.283742 on epoch=33
03/13/2022 22:53:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.967580 on epoch=35
03/13/2022 22:53:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.984117 on epoch=37
03/13/2022 22:53:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.920513 on epoch=39
03/13/2022 22:53:46 - INFO - __main__ - Global step 200 Train loss 1.176758 Classification-F1 0.24675428116288334 on epoch=39
03/13/2022 22:53:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.825629 on epoch=41
03/13/2022 22:53:56 - INFO - __main__ - Step 220 Global step 220 Train loss 1.100969 on epoch=43
03/13/2022 22:54:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.927098 on epoch=45
03/13/2022 22:54:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.803747 on epoch=47
03/13/2022 22:54:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.898249 on epoch=49
03/13/2022 22:54:14 - INFO - __main__ - Global step 250 Train loss 0.911138 Classification-F1 0.21333333333333332 on epoch=49
03/13/2022 22:54:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.648367 on epoch=51
03/13/2022 22:54:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.833750 on epoch=53
03/13/2022 22:54:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.774091 on epoch=55
03/13/2022 22:54:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.703260 on epoch=57
03/13/2022 22:54:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.641079 on epoch=59
03/13/2022 22:54:41 - INFO - __main__ - Global step 300 Train loss 0.720109 Classification-F1 0.24369130257247856 on epoch=59
03/13/2022 22:54:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.593771 on epoch=61
03/13/2022 22:54:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.658498 on epoch=63
03/13/2022 22:54:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.518293 on epoch=65
03/13/2022 22:55:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.544023 on epoch=67
03/13/2022 22:55:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.512936 on epoch=69
03/13/2022 22:55:08 - INFO - __main__ - Global step 350 Train loss 0.565504 Classification-F1 0.24675428116288334 on epoch=69
03/13/2022 22:55:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.490324 on epoch=71
03/13/2022 22:55:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.557014 on epoch=73
03/13/2022 22:55:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.585865 on epoch=75
03/13/2022 22:55:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.478466 on epoch=77
03/13/2022 22:55:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.403914 on epoch=79
03/13/2022 22:55:35 - INFO - __main__ - Global step 400 Train loss 0.503117 Classification-F1 0.24675428116288334 on epoch=79
03/13/2022 22:55:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.418204 on epoch=81
03/13/2022 22:55:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.641240 on epoch=83
03/13/2022 22:55:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.405942 on epoch=85
03/13/2022 22:55:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.441938 on epoch=87
03/13/2022 22:56:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.397306 on epoch=89
03/13/2022 22:56:02 - INFO - __main__ - Global step 450 Train loss 0.460926 Classification-F1 0.23614035087719296 on epoch=89
03/13/2022 22:56:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.400238 on epoch=91
03/13/2022 22:56:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.414204 on epoch=93
03/13/2022 22:56:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.434851 on epoch=95
03/13/2022 22:56:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.355797 on epoch=97
03/13/2022 22:56:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.330343 on epoch=99
03/13/2022 22:56:29 - INFO - __main__ - Global step 500 Train loss 0.387087 Classification-F1 0.3759675524051401 on epoch=99
03/13/2022 22:56:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.354865 on epoch=101
03/13/2022 22:56:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.371227 on epoch=103
03/13/2022 22:56:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.395743 on epoch=105
03/13/2022 22:56:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.328713 on epoch=107
03/13/2022 22:56:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.287756 on epoch=109
03/13/2022 22:56:57 - INFO - __main__ - Global step 550 Train loss 0.347661 Classification-F1 0.44871945259042034 on epoch=109
03/13/2022 22:57:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.319484 on epoch=111
03/13/2022 22:57:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.275677 on epoch=113
03/13/2022 22:57:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.268038 on epoch=115
03/13/2022 22:57:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.246135 on epoch=117
03/13/2022 22:57:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.255586 on epoch=119
03/13/2022 22:57:25 - INFO - __main__ - Global step 600 Train loss 0.272984 Classification-F1 0.4357007319056655 on epoch=119
03/13/2022 22:57:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.213151 on epoch=121
03/13/2022 22:57:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.235910 on epoch=123
03/13/2022 22:57:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.232513 on epoch=125
03/13/2022 22:57:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.132219 on epoch=127
03/13/2022 22:57:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.143836 on epoch=129
03/13/2022 22:57:53 - INFO - __main__ - Global step 650 Train loss 0.191526 Classification-F1 0.5658066159014926 on epoch=129
03/13/2022 22:57:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.107347 on epoch=131
03/13/2022 22:58:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.340968 on epoch=133
03/13/2022 22:58:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.122851 on epoch=135
03/13/2022 22:58:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.088534 on epoch=137
03/13/2022 22:58:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.109833 on epoch=139
03/13/2022 22:58:21 - INFO - __main__ - Global step 700 Train loss 0.153906 Classification-F1 0.5628479921162848 on epoch=139
03/13/2022 22:58:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.035000 on epoch=141
03/13/2022 22:58:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.070957 on epoch=143
03/13/2022 22:58:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.023273 on epoch=145
03/13/2022 22:58:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.028205 on epoch=147
03/13/2022 22:58:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.011797 on epoch=149
03/13/2022 22:58:48 - INFO - __main__ - Global step 750 Train loss 0.033847 Classification-F1 0.39852968055733035 on epoch=149
03/13/2022 22:58:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.026694 on epoch=151
03/13/2022 22:58:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.004031 on epoch=153
03/13/2022 22:59:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.004557 on epoch=155
03/13/2022 22:59:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002486 on epoch=157
03/13/2022 22:59:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.001587 on epoch=159
03/13/2022 22:59:14 - INFO - __main__ - Global step 800 Train loss 0.007871 Classification-F1 0.4894290092432508 on epoch=159
03/13/2022 22:59:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.003650 on epoch=161
03/13/2022 22:59:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001301 on epoch=163
03/13/2022 22:59:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.006594 on epoch=165
03/13/2022 22:59:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000804 on epoch=167
03/13/2022 22:59:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.003335 on epoch=169
03/13/2022 22:59:41 - INFO - __main__ - Global step 850 Train loss 0.003137 Classification-F1 0.5063036378825853 on epoch=169
03/13/2022 22:59:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000680 on epoch=171
03/13/2022 22:59:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000504 on epoch=173
03/13/2022 22:59:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001096 on epoch=175
03/13/2022 23:00:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000210 on epoch=177
03/13/2022 23:00:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000380 on epoch=179
03/13/2022 23:00:08 - INFO - __main__ - Global step 900 Train loss 0.000574 Classification-F1 0.38880997474747475 on epoch=179
03/13/2022 23:00:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000599 on epoch=181
03/13/2022 23:00:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000369 on epoch=183
03/13/2022 23:00:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000216 on epoch=185
03/13/2022 23:00:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000391 on epoch=187
03/13/2022 23:00:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001172 on epoch=189
03/13/2022 23:00:36 - INFO - __main__ - Global step 950 Train loss 0.000549 Classification-F1 0.4487012130132467 on epoch=189
03/13/2022 23:00:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.005851 on epoch=191
03/13/2022 23:00:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001885 on epoch=193
03/13/2022 23:00:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.022397 on epoch=195
03/13/2022 23:00:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.006975 on epoch=197
03/13/2022 23:01:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.003310 on epoch=199
03/13/2022 23:01:02 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:01:02 - INFO - __main__ - Printing 3 examples
03/13/2022 23:01:02 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/13/2022 23:01:02 - INFO - __main__ - ['No']
03/13/2022 23:01:02 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/13/2022 23:01:02 - INFO - __main__ - ['No']
03/13/2022 23:01:02 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/13/2022 23:01:02 - INFO - __main__ - ['No']
03/13/2022 23:01:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 23:01:02 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:01:02 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 23:01:02 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:01:02 - INFO - __main__ - Printing 3 examples
03/13/2022 23:01:02 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/13/2022 23:01:02 - INFO - __main__ - ['No']
03/13/2022 23:01:02 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/13/2022 23:01:02 - INFO - __main__ - ['No']
03/13/2022 23:01:02 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/13/2022 23:01:02 - INFO - __main__ - ['No']
03/13/2022 23:01:02 - INFO - __main__ - Tokenizing Input ...
03/13/2022 23:01:02 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:01:02 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 23:01:03 - INFO - __main__ - Global step 1000 Train loss 0.008084 Classification-F1 0.3914374840845429 on epoch=199
03/13/2022 23:01:03 - INFO - __main__ - save last model!
03/13/2022 23:01:09 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 23:01:10 - INFO - __main__ - Start tokenizing ... 6700 instances
03/13/2022 23:01:10 - INFO - __main__ - Printing 3 examples
03/13/2022 23:01:10 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/13/2022 23:01:10 - INFO - __main__ - ['No']
03/13/2022 23:01:10 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/13/2022 23:01:10 - INFO - __main__ - ['Yes']
03/13/2022 23:01:10 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/13/2022 23:01:10 - INFO - __main__ - ['Yes']
03/13/2022 23:01:10 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 23:01:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 23:01:13 - INFO - __main__ - Starting training!
03/13/2022 23:01:13 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:01:19 - INFO - __main__ - Loaded 6700 examples from test data
03/13/2022 23:03:59 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_13_0.0005_8_predictions.txt
03/13/2022 23:03:59 - INFO - __main__ - Classification-F1 on test data: 0.4095
03/13/2022 23:03:59 - INFO - __main__ - prefix=circa_16_13, lr=0.0005, bsz=8, dev_performance=0.5658066159014926, test_performance=0.40948445830056635
03/13/2022 23:03:59 - INFO - __main__ - Running ... prefix=circa_16_13, lr=0.0003, bsz=8 ...
03/13/2022 23:04:00 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:04:00 - INFO - __main__ - Printing 3 examples
03/13/2022 23:04:00 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/13/2022 23:04:00 - INFO - __main__ - ['No']
03/13/2022 23:04:00 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/13/2022 23:04:00 - INFO - __main__ - ['No']
03/13/2022 23:04:00 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/13/2022 23:04:00 - INFO - __main__ - ['No']
03/13/2022 23:04:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 23:04:00 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:04:00 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 23:04:00 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:04:00 - INFO - __main__ - Printing 3 examples
03/13/2022 23:04:00 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/13/2022 23:04:00 - INFO - __main__ - ['No']
03/13/2022 23:04:00 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/13/2022 23:04:00 - INFO - __main__ - ['No']
03/13/2022 23:04:00 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/13/2022 23:04:00 - INFO - __main__ - ['No']
03/13/2022 23:04:00 - INFO - __main__ - Tokenizing Input ...
03/13/2022 23:04:00 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:04:01 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 23:04:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 23:04:13 - INFO - __main__ - Starting training!
03/13/2022 23:04:17 - INFO - __main__ - Step 10 Global step 10 Train loss 22.111042 on epoch=1
03/13/2022 23:04:22 - INFO - __main__ - Step 20 Global step 20 Train loss 18.693445 on epoch=3
03/13/2022 23:04:27 - INFO - __main__ - Step 30 Global step 30 Train loss 11.441360 on epoch=5
03/13/2022 23:04:32 - INFO - __main__ - Step 40 Global step 40 Train loss 8.361827 on epoch=7
03/13/2022 23:04:37 - INFO - __main__ - Step 50 Global step 50 Train loss 7.715576 on epoch=9
03/13/2022 23:04:41 - INFO - __main__ - Global step 50 Train loss 13.664650 Classification-F1 0.0 on epoch=9
03/13/2022 23:04:47 - INFO - __main__ - Step 60 Global step 60 Train loss 7.223548 on epoch=11
03/13/2022 23:04:52 - INFO - __main__ - Step 70 Global step 70 Train loss 6.866557 on epoch=13
03/13/2022 23:04:57 - INFO - __main__ - Step 80 Global step 80 Train loss 6.510062 on epoch=15
03/13/2022 23:05:02 - INFO - __main__ - Step 90 Global step 90 Train loss 5.701944 on epoch=17
03/13/2022 23:05:07 - INFO - __main__ - Step 100 Global step 100 Train loss 5.051482 on epoch=19
03/13/2022 23:05:09 - INFO - __main__ - Global step 100 Train loss 6.270719 Classification-F1 0.0 on epoch=19
03/13/2022 23:05:14 - INFO - __main__ - Step 110 Global step 110 Train loss 5.671807 on epoch=21
03/13/2022 23:05:19 - INFO - __main__ - Step 120 Global step 120 Train loss 3.807722 on epoch=23
03/13/2022 23:05:24 - INFO - __main__ - Step 130 Global step 130 Train loss 2.835338 on epoch=25
03/13/2022 23:05:29 - INFO - __main__ - Step 140 Global step 140 Train loss 2.059976 on epoch=27
03/13/2022 23:05:34 - INFO - __main__ - Step 150 Global step 150 Train loss 1.879190 on epoch=29
03/13/2022 23:05:36 - INFO - __main__ - Global step 150 Train loss 3.250806 Classification-F1 0.19587301587301587 on epoch=29
03/13/2022 23:05:42 - INFO - __main__ - Step 160 Global step 160 Train loss 1.800526 on epoch=31
03/13/2022 23:05:47 - INFO - __main__ - Step 170 Global step 170 Train loss 1.644018 on epoch=33
03/13/2022 23:05:52 - INFO - __main__ - Step 180 Global step 180 Train loss 1.589544 on epoch=35
03/13/2022 23:05:57 - INFO - __main__ - Step 190 Global step 190 Train loss 2.282097 on epoch=37
03/13/2022 23:06:02 - INFO - __main__ - Step 200 Global step 200 Train loss 1.687192 on epoch=39
03/13/2022 23:06:03 - INFO - __main__ - Global step 200 Train loss 1.800676 Classification-F1 0.15931677018633542 on epoch=39
03/13/2022 23:06:08 - INFO - __main__ - Step 210 Global step 210 Train loss 1.571861 on epoch=41
03/13/2022 23:06:13 - INFO - __main__ - Step 220 Global step 220 Train loss 1.637602 on epoch=43
03/13/2022 23:06:18 - INFO - __main__ - Step 230 Global step 230 Train loss 1.641541 on epoch=45
03/13/2022 23:06:24 - INFO - __main__ - Step 240 Global step 240 Train loss 1.886337 on epoch=47
03/13/2022 23:06:29 - INFO - __main__ - Step 250 Global step 250 Train loss 1.347281 on epoch=49
03/13/2022 23:06:31 - INFO - __main__ - Global step 250 Train loss 1.616925 Classification-F1 0.12805924695459578 on epoch=49
03/13/2022 23:06:36 - INFO - __main__ - Step 260 Global step 260 Train loss 1.482682 on epoch=51
03/13/2022 23:06:41 - INFO - __main__ - Step 270 Global step 270 Train loss 1.532850 on epoch=53
03/13/2022 23:06:45 - INFO - __main__ - Step 280 Global step 280 Train loss 1.193913 on epoch=55
03/13/2022 23:06:50 - INFO - __main__ - Step 290 Global step 290 Train loss 1.407802 on epoch=57
03/13/2022 23:06:55 - INFO - __main__ - Step 300 Global step 300 Train loss 1.181000 on epoch=59
03/13/2022 23:06:57 - INFO - __main__ - Global step 300 Train loss 1.359649 Classification-F1 0.12272727272727271 on epoch=59
03/13/2022 23:07:02 - INFO - __main__ - Step 310 Global step 310 Train loss 1.323418 on epoch=61
03/13/2022 23:07:07 - INFO - __main__ - Step 320 Global step 320 Train loss 1.320752 on epoch=63
03/13/2022 23:07:12 - INFO - __main__ - Step 330 Global step 330 Train loss 1.202819 on epoch=65
03/13/2022 23:07:17 - INFO - __main__ - Step 340 Global step 340 Train loss 1.069581 on epoch=67
03/13/2022 23:07:22 - INFO - __main__ - Step 350 Global step 350 Train loss 1.016834 on epoch=69
03/13/2022 23:07:24 - INFO - __main__ - Global step 350 Train loss 1.186681 Classification-F1 0.13852216748768473 on epoch=69
03/13/2022 23:07:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.986974 on epoch=71
03/13/2022 23:07:34 - INFO - __main__ - Step 370 Global step 370 Train loss 1.136360 on epoch=73
03/13/2022 23:07:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.939718 on epoch=75
03/13/2022 23:07:44 - INFO - __main__ - Step 390 Global step 390 Train loss 1.188425 on epoch=77
03/13/2022 23:07:49 - INFO - __main__ - Step 400 Global step 400 Train loss 1.171124 on epoch=79
03/13/2022 23:07:51 - INFO - __main__ - Global step 400 Train loss 1.084520 Classification-F1 0.09144659046117462 on epoch=79
03/13/2022 23:07:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.877280 on epoch=81
03/13/2022 23:08:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.990140 on epoch=83
03/13/2022 23:08:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.809760 on epoch=85
03/13/2022 23:08:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.732922 on epoch=87
03/13/2022 23:08:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.954798 on epoch=89
03/13/2022 23:08:18 - INFO - __main__ - Global step 450 Train loss 0.872980 Classification-F1 0.18458242837166403 on epoch=89
03/13/2022 23:08:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.697761 on epoch=91
03/13/2022 23:08:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.679531 on epoch=93
03/13/2022 23:08:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.761614 on epoch=95
03/13/2022 23:08:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.712434 on epoch=97
03/13/2022 23:08:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.568027 on epoch=99
03/13/2022 23:08:45 - INFO - __main__ - Global step 500 Train loss 0.683874 Classification-F1 0.14568040654997177 on epoch=99
03/13/2022 23:08:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.708085 on epoch=101
03/13/2022 23:08:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.681738 on epoch=103
03/13/2022 23:09:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.555141 on epoch=105
03/13/2022 23:09:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.549742 on epoch=107
03/13/2022 23:09:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.581833 on epoch=109
03/13/2022 23:09:12 - INFO - __main__ - Global step 550 Train loss 0.615308 Classification-F1 0.10838206627680311 on epoch=109
03/13/2022 23:09:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.598600 on epoch=111
03/13/2022 23:09:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.541045 on epoch=113
03/13/2022 23:09:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.472065 on epoch=115
03/13/2022 23:09:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.507611 on epoch=117
03/13/2022 23:09:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.505063 on epoch=119
03/13/2022 23:09:38 - INFO - __main__ - Global step 600 Train loss 0.524877 Classification-F1 0.15677620200622622 on epoch=119
03/13/2022 23:09:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.495666 on epoch=121
03/13/2022 23:09:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.461950 on epoch=123
03/13/2022 23:09:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.571861 on epoch=125
03/13/2022 23:09:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.465358 on epoch=127
03/13/2022 23:10:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.541928 on epoch=129
03/13/2022 23:10:05 - INFO - __main__ - Global step 650 Train loss 0.507353 Classification-F1 0.193984962406015 on epoch=129
03/13/2022 23:10:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.469168 on epoch=131
03/13/2022 23:10:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.495172 on epoch=133
03/13/2022 23:10:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.455957 on epoch=135
03/13/2022 23:10:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.473645 on epoch=137
03/13/2022 23:10:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.473438 on epoch=139
03/13/2022 23:10:32 - INFO - __main__ - Global step 700 Train loss 0.473476 Classification-F1 0.21850462615653915 on epoch=139
03/13/2022 23:10:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.412956 on epoch=141
03/13/2022 23:10:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.414485 on epoch=143
03/13/2022 23:10:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.434757 on epoch=145
03/13/2022 23:10:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.473649 on epoch=147
03/13/2022 23:10:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.399882 on epoch=149
03/13/2022 23:10:59 - INFO - __main__ - Global step 750 Train loss 0.427146 Classification-F1 0.21922438672438674 on epoch=149
03/13/2022 23:11:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.437589 on epoch=151
03/13/2022 23:11:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.431655 on epoch=153
03/13/2022 23:11:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.428851 on epoch=155
03/13/2022 23:11:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.392076 on epoch=157
03/13/2022 23:11:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.380930 on epoch=159
03/13/2022 23:11:26 - INFO - __main__ - Global step 800 Train loss 0.414221 Classification-F1 0.1599590373783922 on epoch=159
03/13/2022 23:11:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.411897 on epoch=161
03/13/2022 23:11:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.438460 on epoch=163
03/13/2022 23:11:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.411644 on epoch=165
03/13/2022 23:11:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.396243 on epoch=167
03/13/2022 23:11:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.390546 on epoch=169
03/13/2022 23:11:54 - INFO - __main__ - Global step 850 Train loss 0.409758 Classification-F1 0.21124973694495383 on epoch=169
03/13/2022 23:11:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.367290 on epoch=171
03/13/2022 23:12:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.371028 on epoch=173
03/13/2022 23:12:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.342734 on epoch=175
03/13/2022 23:12:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.429370 on epoch=177
03/13/2022 23:12:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.383007 on epoch=179
03/13/2022 23:12:21 - INFO - __main__ - Global step 900 Train loss 0.378686 Classification-F1 0.18658730158730158 on epoch=179
03/13/2022 23:12:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.409883 on epoch=181
03/13/2022 23:12:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.414971 on epoch=183
03/13/2022 23:12:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.368322 on epoch=185
03/13/2022 23:12:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.366025 on epoch=187
03/13/2022 23:12:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.370009 on epoch=189
03/13/2022 23:12:48 - INFO - __main__ - Global step 950 Train loss 0.385842 Classification-F1 0.18449197860962566 on epoch=189
03/13/2022 23:12:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.369739 on epoch=191
03/13/2022 23:12:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.396944 on epoch=193
03/13/2022 23:13:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.368549 on epoch=195
03/13/2022 23:13:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.362085 on epoch=197
03/13/2022 23:13:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.349537 on epoch=199
03/13/2022 23:13:14 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:13:14 - INFO - __main__ - Printing 3 examples
03/13/2022 23:13:14 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/13/2022 23:13:14 - INFO - __main__ - ['No']
03/13/2022 23:13:14 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/13/2022 23:13:14 - INFO - __main__ - ['No']
03/13/2022 23:13:14 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/13/2022 23:13:14 - INFO - __main__ - ['No']
03/13/2022 23:13:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 23:13:14 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:13:14 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 23:13:14 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:13:14 - INFO - __main__ - Printing 3 examples
03/13/2022 23:13:14 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/13/2022 23:13:14 - INFO - __main__ - ['No']
03/13/2022 23:13:14 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/13/2022 23:13:14 - INFO - __main__ - ['No']
03/13/2022 23:13:14 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/13/2022 23:13:14 - INFO - __main__ - ['No']
03/13/2022 23:13:14 - INFO - __main__ - Tokenizing Input ...
03/13/2022 23:13:14 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:13:14 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 23:13:15 - INFO - __main__ - Global step 1000 Train loss 0.369371 Classification-F1 0.17439178174883893 on epoch=199
03/13/2022 23:13:15 - INFO - __main__ - save last model!
03/13/2022 23:13:22 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 23:13:22 - INFO - __main__ - Start tokenizing ... 6700 instances
03/13/2022 23:13:22 - INFO - __main__ - Printing 3 examples
03/13/2022 23:13:22 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/13/2022 23:13:22 - INFO - __main__ - ['No']
03/13/2022 23:13:22 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/13/2022 23:13:22 - INFO - __main__ - ['Yes']
03/13/2022 23:13:22 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/13/2022 23:13:22 - INFO - __main__ - ['Yes']
03/13/2022 23:13:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 23:13:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 23:13:25 - INFO - __main__ - Starting training!
03/13/2022 23:13:25 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:13:32 - INFO - __main__ - Loaded 6700 examples from test data
03/13/2022 23:15:51 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_13_0.0003_8_predictions.txt
03/13/2022 23:15:51 - INFO - __main__ - Classification-F1 on test data: 0.2016
03/13/2022 23:15:52 - INFO - __main__ - prefix=circa_16_13, lr=0.0003, bsz=8, dev_performance=0.21922438672438674, test_performance=0.20161930310621048
03/13/2022 23:15:52 - INFO - __main__ - Running ... prefix=circa_16_13, lr=0.0002, bsz=8 ...
03/13/2022 23:15:53 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:15:53 - INFO - __main__ - Printing 3 examples
03/13/2022 23:15:53 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/13/2022 23:15:53 - INFO - __main__ - ['No']
03/13/2022 23:15:53 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/13/2022 23:15:53 - INFO - __main__ - ['No']
03/13/2022 23:15:53 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/13/2022 23:15:53 - INFO - __main__ - ['No']
03/13/2022 23:15:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 23:15:53 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:15:53 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 23:15:53 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:15:53 - INFO - __main__ - Printing 3 examples
03/13/2022 23:15:53 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/13/2022 23:15:53 - INFO - __main__ - ['No']
03/13/2022 23:15:53 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/13/2022 23:15:53 - INFO - __main__ - ['No']
03/13/2022 23:15:53 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/13/2022 23:15:53 - INFO - __main__ - ['No']
03/13/2022 23:15:53 - INFO - __main__ - Tokenizing Input ...
03/13/2022 23:15:53 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:15:53 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 23:16:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 23:16:04 - INFO - __main__ - Starting training!
03/13/2022 23:16:09 - INFO - __main__ - Step 10 Global step 10 Train loss 21.299072 on epoch=1
03/13/2022 23:16:13 - INFO - __main__ - Step 20 Global step 20 Train loss 19.635994 on epoch=3
03/13/2022 23:16:18 - INFO - __main__ - Step 30 Global step 30 Train loss 15.410991 on epoch=5
03/13/2022 23:16:22 - INFO - __main__ - Step 40 Global step 40 Train loss 12.891413 on epoch=7
03/13/2022 23:16:27 - INFO - __main__ - Step 50 Global step 50 Train loss 10.847811 on epoch=9
03/13/2022 23:16:41 - INFO - __main__ - Global step 50 Train loss 16.017054 Classification-F1 0.0 on epoch=9
03/13/2022 23:16:47 - INFO - __main__ - Step 60 Global step 60 Train loss 10.193201 on epoch=11
03/13/2022 23:16:52 - INFO - __main__ - Step 70 Global step 70 Train loss 9.911494 on epoch=13
03/13/2022 23:16:57 - INFO - __main__ - Step 80 Global step 80 Train loss 9.285715 on epoch=15
03/13/2022 23:17:02 - INFO - __main__ - Step 90 Global step 90 Train loss 8.801044 on epoch=17
03/13/2022 23:17:07 - INFO - __main__ - Step 100 Global step 100 Train loss 8.157845 on epoch=19
03/13/2022 23:17:10 - INFO - __main__ - Global step 100 Train loss 9.269859 Classification-F1 0.0 on epoch=19
03/13/2022 23:17:15 - INFO - __main__ - Step 110 Global step 110 Train loss 7.662153 on epoch=21
03/13/2022 23:17:20 - INFO - __main__ - Step 120 Global step 120 Train loss 7.210733 on epoch=23
03/13/2022 23:17:25 - INFO - __main__ - Step 130 Global step 130 Train loss 7.105544 on epoch=25
03/13/2022 23:17:30 - INFO - __main__ - Step 140 Global step 140 Train loss 6.008038 on epoch=27
03/13/2022 23:17:35 - INFO - __main__ - Step 150 Global step 150 Train loss 6.307357 on epoch=29
03/13/2022 23:17:38 - INFO - __main__ - Global step 150 Train loss 6.858765 Classification-F1 0.0 on epoch=29
03/13/2022 23:17:43 - INFO - __main__ - Step 160 Global step 160 Train loss 5.310142 on epoch=31
03/13/2022 23:17:48 - INFO - __main__ - Step 170 Global step 170 Train loss 4.323347 on epoch=33
03/13/2022 23:17:53 - INFO - __main__ - Step 180 Global step 180 Train loss 3.969625 on epoch=35
03/13/2022 23:17:58 - INFO - __main__ - Step 190 Global step 190 Train loss 3.612608 on epoch=37
03/13/2022 23:18:03 - INFO - __main__ - Step 200 Global step 200 Train loss 2.574716 on epoch=39
03/13/2022 23:18:05 - INFO - __main__ - Global step 200 Train loss 3.958087 Classification-F1 0.02689075630252101 on epoch=39
03/13/2022 23:18:11 - INFO - __main__ - Step 210 Global step 210 Train loss 2.056831 on epoch=41
03/13/2022 23:18:16 - INFO - __main__ - Step 220 Global step 220 Train loss 1.956831 on epoch=43
03/13/2022 23:18:21 - INFO - __main__ - Step 230 Global step 230 Train loss 2.146453 on epoch=45
03/13/2022 23:18:26 - INFO - __main__ - Step 240 Global step 240 Train loss 2.004445 on epoch=47
03/13/2022 23:18:31 - INFO - __main__ - Step 250 Global step 250 Train loss 1.858105 on epoch=49
03/13/2022 23:18:33 - INFO - __main__ - Global step 250 Train loss 2.004533 Classification-F1 0.06315789473684211 on epoch=49
03/13/2022 23:18:39 - INFO - __main__ - Step 260 Global step 260 Train loss 1.498290 on epoch=51
03/13/2022 23:18:44 - INFO - __main__ - Step 270 Global step 270 Train loss 1.938792 on epoch=53
03/13/2022 23:18:49 - INFO - __main__ - Step 280 Global step 280 Train loss 1.677977 on epoch=55
03/13/2022 23:18:54 - INFO - __main__ - Step 290 Global step 290 Train loss 1.719951 on epoch=57
03/13/2022 23:18:59 - INFO - __main__ - Step 300 Global step 300 Train loss 1.500065 on epoch=59
03/13/2022 23:19:01 - INFO - __main__ - Global step 300 Train loss 1.667015 Classification-F1 0.06808510638297872 on epoch=59
03/13/2022 23:19:07 - INFO - __main__ - Step 310 Global step 310 Train loss 1.408953 on epoch=61
03/13/2022 23:19:12 - INFO - __main__ - Step 320 Global step 320 Train loss 1.810955 on epoch=63
03/13/2022 23:19:17 - INFO - __main__ - Step 330 Global step 330 Train loss 1.505096 on epoch=65
03/13/2022 23:19:22 - INFO - __main__ - Step 340 Global step 340 Train loss 1.275050 on epoch=67
03/13/2022 23:19:27 - INFO - __main__ - Step 350 Global step 350 Train loss 1.452094 on epoch=69
03/13/2022 23:19:29 - INFO - __main__ - Global step 350 Train loss 1.490430 Classification-F1 0.10166028097062578 on epoch=69
03/13/2022 23:19:34 - INFO - __main__ - Step 360 Global step 360 Train loss 1.437718 on epoch=71
03/13/2022 23:19:40 - INFO - __main__ - Step 370 Global step 370 Train loss 1.385776 on epoch=73
03/13/2022 23:19:45 - INFO - __main__ - Step 380 Global step 380 Train loss 1.188514 on epoch=75
03/13/2022 23:19:50 - INFO - __main__ - Step 390 Global step 390 Train loss 1.260048 on epoch=77
03/13/2022 23:19:55 - INFO - __main__ - Step 400 Global step 400 Train loss 1.418206 on epoch=79
03/13/2022 23:19:57 - INFO - __main__ - Global step 400 Train loss 1.338053 Classification-F1 0.12740645986568597 on epoch=79
03/13/2022 23:20:02 - INFO - __main__ - Step 410 Global step 410 Train loss 1.507699 on epoch=81
03/13/2022 23:20:07 - INFO - __main__ - Step 420 Global step 420 Train loss 1.679873 on epoch=83
03/13/2022 23:20:12 - INFO - __main__ - Step 430 Global step 430 Train loss 1.302601 on epoch=85
03/13/2022 23:20:18 - INFO - __main__ - Step 440 Global step 440 Train loss 1.236890 on epoch=87
03/13/2022 23:20:23 - INFO - __main__ - Step 450 Global step 450 Train loss 1.139397 on epoch=89
03/13/2022 23:20:24 - INFO - __main__ - Global step 450 Train loss 1.373292 Classification-F1 0.11178260869565218 on epoch=89
03/13/2022 23:20:29 - INFO - __main__ - Step 460 Global step 460 Train loss 1.007799 on epoch=91
03/13/2022 23:20:34 - INFO - __main__ - Step 470 Global step 470 Train loss 1.293336 on epoch=93
03/13/2022 23:20:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.972362 on epoch=95
03/13/2022 23:20:44 - INFO - __main__ - Step 490 Global step 490 Train loss 1.197975 on epoch=97
03/13/2022 23:20:49 - INFO - __main__ - Step 500 Global step 500 Train loss 1.062584 on epoch=99
03/13/2022 23:20:51 - INFO - __main__ - Global step 500 Train loss 1.106811 Classification-F1 0.08956521739130434 on epoch=99
03/13/2022 23:20:56 - INFO - __main__ - Step 510 Global step 510 Train loss 1.187798 on epoch=101
03/13/2022 23:21:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.964130 on epoch=103
03/13/2022 23:21:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.931780 on epoch=105
03/13/2022 23:21:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.915976 on epoch=107
03/13/2022 23:21:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.990135 on epoch=109
03/13/2022 23:21:18 - INFO - __main__ - Global step 550 Train loss 0.997964 Classification-F1 0.09123434704830055 on epoch=109
03/13/2022 23:21:23 - INFO - __main__ - Step 560 Global step 560 Train loss 1.107083 on epoch=111
03/13/2022 23:21:28 - INFO - __main__ - Step 570 Global step 570 Train loss 1.014769 on epoch=113
03/13/2022 23:21:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.926788 on epoch=115
03/13/2022 23:21:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.998228 on epoch=117
03/13/2022 23:21:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.780185 on epoch=119
03/13/2022 23:21:45 - INFO - __main__ - Global step 600 Train loss 0.965411 Classification-F1 0.08102766798418973 on epoch=119
03/13/2022 23:21:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.937715 on epoch=121
03/13/2022 23:21:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.769421 on epoch=123
03/13/2022 23:22:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.900205 on epoch=125
03/13/2022 23:22:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.968650 on epoch=127
03/13/2022 23:22:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.940925 on epoch=129
03/13/2022 23:22:12 - INFO - __main__ - Global step 650 Train loss 0.903383 Classification-F1 0.082687338501292 on epoch=129
03/13/2022 23:22:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.882742 on epoch=131
03/13/2022 23:22:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.753136 on epoch=133
03/13/2022 23:22:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.600833 on epoch=135
03/13/2022 23:22:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.631927 on epoch=137
03/13/2022 23:22:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.767206 on epoch=139
03/13/2022 23:22:38 - INFO - __main__ - Global step 700 Train loss 0.727169 Classification-F1 0.10377936670071501 on epoch=139
03/13/2022 23:22:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.734581 on epoch=141
03/13/2022 23:22:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.635248 on epoch=143
03/13/2022 23:22:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.609303 on epoch=145
03/13/2022 23:22:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.589643 on epoch=147
03/13/2022 23:23:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.694203 on epoch=149
03/13/2022 23:23:05 - INFO - __main__ - Global step 750 Train loss 0.652596 Classification-F1 0.10884918986289989 on epoch=149
03/13/2022 23:23:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.731882 on epoch=151
03/13/2022 23:23:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.661034 on epoch=153
03/13/2022 23:23:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.640734 on epoch=155
03/13/2022 23:23:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.599467 on epoch=157
03/13/2022 23:23:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.623875 on epoch=159
03/13/2022 23:23:32 - INFO - __main__ - Global step 800 Train loss 0.651398 Classification-F1 0.10781767252355487 on epoch=159
03/13/2022 23:23:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.563208 on epoch=161
03/13/2022 23:23:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.608382 on epoch=163
03/13/2022 23:23:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.587416 on epoch=165
03/13/2022 23:23:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.528969 on epoch=167
03/13/2022 23:23:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.455078 on epoch=169
03/13/2022 23:23:59 - INFO - __main__ - Global step 850 Train loss 0.548611 Classification-F1 0.10612244897959182 on epoch=169
03/13/2022 23:24:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.513686 on epoch=171
03/13/2022 23:24:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.654304 on epoch=173
03/13/2022 23:24:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.594771 on epoch=175
03/13/2022 23:24:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.544450 on epoch=177
03/13/2022 23:24:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.493816 on epoch=179
03/13/2022 23:24:26 - INFO - __main__ - Global step 900 Train loss 0.560205 Classification-F1 0.10016569459172853 on epoch=179
03/13/2022 23:24:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.447061 on epoch=181
03/13/2022 23:24:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.507629 on epoch=183
03/13/2022 23:24:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.553591 on epoch=185
03/13/2022 23:24:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.451173 on epoch=187
03/13/2022 23:24:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.470179 on epoch=189
03/13/2022 23:24:53 - INFO - __main__ - Global step 950 Train loss 0.485927 Classification-F1 0.1413333333333333 on epoch=189
03/13/2022 23:24:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.514317 on epoch=191
03/13/2022 23:25:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.491755 on epoch=193
03/13/2022 23:25:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.542983 on epoch=195
03/13/2022 23:25:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.509102 on epoch=197
03/13/2022 23:25:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.484461 on epoch=199
03/13/2022 23:25:20 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:25:20 - INFO - __main__ - Printing 3 examples
03/13/2022 23:25:20 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/13/2022 23:25:20 - INFO - __main__ - ['No']
03/13/2022 23:25:20 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/13/2022 23:25:20 - INFO - __main__ - ['No']
03/13/2022 23:25:20 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/13/2022 23:25:20 - INFO - __main__ - ['No']
03/13/2022 23:25:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 23:25:20 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:25:20 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 23:25:20 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:25:20 - INFO - __main__ - Printing 3 examples
03/13/2022 23:25:20 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/13/2022 23:25:20 - INFO - __main__ - ['No']
03/13/2022 23:25:20 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/13/2022 23:25:20 - INFO - __main__ - ['No']
03/13/2022 23:25:20 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/13/2022 23:25:20 - INFO - __main__ - ['No']
03/13/2022 23:25:20 - INFO - __main__ - Tokenizing Input ...
03/13/2022 23:25:20 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:25:20 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 23:25:20 - INFO - __main__ - Global step 1000 Train loss 0.508524 Classification-F1 0.14200415092331437 on epoch=199
03/13/2022 23:25:21 - INFO - __main__ - save last model!
03/13/2022 23:25:28 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 23:25:29 - INFO - __main__ - Start tokenizing ... 6700 instances
03/13/2022 23:25:29 - INFO - __main__ - Printing 3 examples
03/13/2022 23:25:29 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/13/2022 23:25:29 - INFO - __main__ - ['No']
03/13/2022 23:25:29 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/13/2022 23:25:29 - INFO - __main__ - ['Yes']
03/13/2022 23:25:29 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/13/2022 23:25:29 - INFO - __main__ - ['Yes']
03/13/2022 23:25:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 23:25:32 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:25:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 23:25:33 - INFO - __main__ - Starting training!
03/13/2022 23:25:38 - INFO - __main__ - Loaded 6700 examples from test data
03/13/2022 23:27:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_13_0.0002_8_predictions.txt
03/13/2022 23:27:53 - INFO - __main__ - Classification-F1 on test data: 0.1509
03/13/2022 23:27:53 - INFO - __main__ - prefix=circa_16_13, lr=0.0002, bsz=8, dev_performance=0.14200415092331437, test_performance=0.15094803992873068
03/13/2022 23:27:53 - INFO - __main__ - Running ... prefix=circa_16_13, lr=0.0001, bsz=8 ...
03/13/2022 23:27:54 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:27:54 - INFO - __main__ - Printing 3 examples
03/13/2022 23:27:54 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/13/2022 23:27:54 - INFO - __main__ - ['No']
03/13/2022 23:27:54 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/13/2022 23:27:54 - INFO - __main__ - ['No']
03/13/2022 23:27:54 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/13/2022 23:27:54 - INFO - __main__ - ['No']
03/13/2022 23:27:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 23:27:54 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:27:54 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 23:27:54 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:27:54 - INFO - __main__ - Printing 3 examples
03/13/2022 23:27:54 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/13/2022 23:27:54 - INFO - __main__ - ['No']
03/13/2022 23:27:54 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/13/2022 23:27:54 - INFO - __main__ - ['No']
03/13/2022 23:27:54 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/13/2022 23:27:54 - INFO - __main__ - ['No']
03/13/2022 23:27:54 - INFO - __main__ - Tokenizing Input ...
03/13/2022 23:27:54 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:27:54 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 23:28:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 23:28:06 - INFO - __main__ - Starting training!
03/13/2022 23:28:10 - INFO - __main__ - Step 10 Global step 10 Train loss 22.471153 on epoch=1
03/13/2022 23:28:15 - INFO - __main__ - Step 20 Global step 20 Train loss 19.869114 on epoch=3
03/13/2022 23:28:20 - INFO - __main__ - Step 30 Global step 30 Train loss 16.611927 on epoch=5
03/13/2022 23:28:25 - INFO - __main__ - Step 40 Global step 40 Train loss 14.818647 on epoch=7
03/13/2022 23:28:30 - INFO - __main__ - Step 50 Global step 50 Train loss 13.197331 on epoch=9
03/13/2022 23:28:56 - INFO - __main__ - Global step 50 Train loss 17.393635 Classification-F1 0.0 on epoch=9
03/13/2022 23:29:02 - INFO - __main__ - Step 60 Global step 60 Train loss 11.733775 on epoch=11
03/13/2022 23:29:07 - INFO - __main__ - Step 70 Global step 70 Train loss 10.083858 on epoch=13
03/13/2022 23:29:12 - INFO - __main__ - Step 80 Global step 80 Train loss 9.511666 on epoch=15
03/13/2022 23:29:17 - INFO - __main__ - Step 90 Global step 90 Train loss 8.962573 on epoch=17
03/13/2022 23:29:22 - INFO - __main__ - Step 100 Global step 100 Train loss 8.192636 on epoch=19
03/13/2022 23:29:24 - INFO - __main__ - Global step 100 Train loss 9.696901 Classification-F1 0.0 on epoch=19
03/13/2022 23:29:29 - INFO - __main__ - Step 110 Global step 110 Train loss 8.350613 on epoch=21
03/13/2022 23:29:34 - INFO - __main__ - Step 120 Global step 120 Train loss 7.685246 on epoch=23
03/13/2022 23:29:39 - INFO - __main__ - Step 130 Global step 130 Train loss 7.359389 on epoch=25
03/13/2022 23:29:44 - INFO - __main__ - Step 140 Global step 140 Train loss 7.198956 on epoch=27
03/13/2022 23:29:49 - INFO - __main__ - Step 150 Global step 150 Train loss 7.285883 on epoch=29
03/13/2022 23:29:51 - INFO - __main__ - Global step 150 Train loss 7.576017 Classification-F1 0.0 on epoch=29
03/13/2022 23:29:56 - INFO - __main__ - Step 160 Global step 160 Train loss 6.704793 on epoch=31
03/13/2022 23:30:01 - INFO - __main__ - Step 170 Global step 170 Train loss 7.130423 on epoch=33
03/13/2022 23:30:06 - INFO - __main__ - Step 180 Global step 180 Train loss 6.477361 on epoch=35
03/13/2022 23:30:11 - INFO - __main__ - Step 190 Global step 190 Train loss 6.364402 on epoch=37
03/13/2022 23:30:16 - INFO - __main__ - Step 200 Global step 200 Train loss 6.208238 on epoch=39
03/13/2022 23:30:18 - INFO - __main__ - Global step 200 Train loss 6.577044 Classification-F1 0.0 on epoch=39
03/13/2022 23:30:23 - INFO - __main__ - Step 210 Global step 210 Train loss 6.043015 on epoch=41
03/13/2022 23:30:28 - INFO - __main__ - Step 220 Global step 220 Train loss 5.848975 on epoch=43
03/13/2022 23:30:33 - INFO - __main__ - Step 230 Global step 230 Train loss 5.462718 on epoch=45
03/13/2022 23:30:37 - INFO - __main__ - Step 240 Global step 240 Train loss 5.383129 on epoch=47
03/13/2022 23:30:43 - INFO - __main__ - Step 250 Global step 250 Train loss 5.146461 on epoch=49
03/13/2022 23:30:45 - INFO - __main__ - Global step 250 Train loss 5.576860 Classification-F1 0.0 on epoch=49
03/13/2022 23:30:50 - INFO - __main__ - Step 260 Global step 260 Train loss 4.758894 on epoch=51
03/13/2022 23:30:55 - INFO - __main__ - Step 270 Global step 270 Train loss 4.496003 on epoch=53
03/13/2022 23:31:00 - INFO - __main__ - Step 280 Global step 280 Train loss 4.495140 on epoch=55
03/13/2022 23:31:05 - INFO - __main__ - Step 290 Global step 290 Train loss 4.333980 on epoch=57
03/13/2022 23:31:10 - INFO - __main__ - Step 300 Global step 300 Train loss 4.051249 on epoch=59
03/13/2022 23:31:12 - INFO - __main__ - Global step 300 Train loss 4.427053 Classification-F1 0.0 on epoch=59
03/13/2022 23:31:17 - INFO - __main__ - Step 310 Global step 310 Train loss 4.107187 on epoch=61
03/13/2022 23:31:22 - INFO - __main__ - Step 320 Global step 320 Train loss 3.521281 on epoch=63
03/13/2022 23:31:28 - INFO - __main__ - Step 330 Global step 330 Train loss 2.547839 on epoch=65
03/13/2022 23:31:33 - INFO - __main__ - Step 340 Global step 340 Train loss 3.130257 on epoch=67
03/13/2022 23:31:38 - INFO - __main__ - Step 350 Global step 350 Train loss 2.427449 on epoch=69
03/13/2022 23:31:40 - INFO - __main__ - Global step 350 Train loss 3.146802 Classification-F1 0.0 on epoch=69
03/13/2022 23:31:45 - INFO - __main__ - Step 360 Global step 360 Train loss 2.382430 on epoch=71
03/13/2022 23:31:50 - INFO - __main__ - Step 370 Global step 370 Train loss 2.269307 on epoch=73
03/13/2022 23:31:55 - INFO - __main__ - Step 380 Global step 380 Train loss 1.853559 on epoch=75
03/13/2022 23:32:00 - INFO - __main__ - Step 390 Global step 390 Train loss 2.001001 on epoch=77
03/13/2022 23:32:05 - INFO - __main__ - Step 400 Global step 400 Train loss 1.718984 on epoch=79
03/13/2022 23:32:07 - INFO - __main__ - Global step 400 Train loss 2.045056 Classification-F1 0.2419753086419753 on epoch=79
03/13/2022 23:32:13 - INFO - __main__ - Step 410 Global step 410 Train loss 1.731251 on epoch=81
03/13/2022 23:32:18 - INFO - __main__ - Step 420 Global step 420 Train loss 1.553512 on epoch=83
03/13/2022 23:32:23 - INFO - __main__ - Step 430 Global step 430 Train loss 1.698616 on epoch=85
03/13/2022 23:32:28 - INFO - __main__ - Step 440 Global step 440 Train loss 1.859306 on epoch=87
03/13/2022 23:32:33 - INFO - __main__ - Step 450 Global step 450 Train loss 1.650427 on epoch=89
03/13/2022 23:32:35 - INFO - __main__ - Global step 450 Train loss 1.698622 Classification-F1 0.24774193548387097 on epoch=89
03/13/2022 23:32:41 - INFO - __main__ - Step 460 Global step 460 Train loss 1.741575 on epoch=91
03/13/2022 23:32:46 - INFO - __main__ - Step 470 Global step 470 Train loss 1.295258 on epoch=93
03/13/2022 23:32:51 - INFO - __main__ - Step 480 Global step 480 Train loss 1.341451 on epoch=95
03/13/2022 23:32:56 - INFO - __main__ - Step 490 Global step 490 Train loss 1.310419 on epoch=97
03/13/2022 23:33:01 - INFO - __main__ - Step 500 Global step 500 Train loss 1.366239 on epoch=99
03/13/2022 23:33:03 - INFO - __main__ - Global step 500 Train loss 1.410988 Classification-F1 0.25234567901234567 on epoch=99
03/13/2022 23:33:09 - INFO - __main__ - Step 510 Global step 510 Train loss 1.754068 on epoch=101
03/13/2022 23:33:14 - INFO - __main__ - Step 520 Global step 520 Train loss 1.595032 on epoch=103
03/13/2022 23:33:19 - INFO - __main__ - Step 530 Global step 530 Train loss 1.307335 on epoch=105
03/13/2022 23:33:24 - INFO - __main__ - Step 540 Global step 540 Train loss 1.209377 on epoch=107
03/13/2022 23:33:30 - INFO - __main__ - Step 550 Global step 550 Train loss 1.805743 on epoch=109
03/13/2022 23:33:32 - INFO - __main__ - Global step 550 Train loss 1.534311 Classification-F1 0.25832269050659856 on epoch=109
03/13/2022 23:33:37 - INFO - __main__ - Step 560 Global step 560 Train loss 1.353853 on epoch=111
03/13/2022 23:33:42 - INFO - __main__ - Step 570 Global step 570 Train loss 1.336257 on epoch=113
03/13/2022 23:33:48 - INFO - __main__ - Step 580 Global step 580 Train loss 1.495481 on epoch=115
03/13/2022 23:33:53 - INFO - __main__ - Step 590 Global step 590 Train loss 1.348400 on epoch=117
03/13/2022 23:33:58 - INFO - __main__ - Step 600 Global step 600 Train loss 1.390848 on epoch=119
03/13/2022 23:34:00 - INFO - __main__ - Global step 600 Train loss 1.384968 Classification-F1 0.25044091710758376 on epoch=119
03/13/2022 23:34:05 - INFO - __main__ - Step 610 Global step 610 Train loss 1.225031 on epoch=121
03/13/2022 23:34:10 - INFO - __main__ - Step 620 Global step 620 Train loss 1.180334 on epoch=123
03/13/2022 23:34:15 - INFO - __main__ - Step 630 Global step 630 Train loss 1.485400 on epoch=125
03/13/2022 23:34:20 - INFO - __main__ - Step 640 Global step 640 Train loss 1.356105 on epoch=127
03/13/2022 23:34:25 - INFO - __main__ - Step 650 Global step 650 Train loss 1.413155 on epoch=129
03/13/2022 23:34:27 - INFO - __main__ - Global step 650 Train loss 1.332005 Classification-F1 0.2328584995251662 on epoch=129
03/13/2022 23:34:32 - INFO - __main__ - Step 660 Global step 660 Train loss 1.482743 on epoch=131
03/13/2022 23:34:37 - INFO - __main__ - Step 670 Global step 670 Train loss 1.126298 on epoch=133
03/13/2022 23:34:42 - INFO - __main__ - Step 680 Global step 680 Train loss 1.164490 on epoch=135
03/13/2022 23:34:47 - INFO - __main__ - Step 690 Global step 690 Train loss 1.102262 on epoch=137
03/13/2022 23:34:52 - INFO - __main__ - Step 700 Global step 700 Train loss 1.203691 on epoch=139
03/13/2022 23:34:54 - INFO - __main__ - Global step 700 Train loss 1.215897 Classification-F1 0.25832269050659856 on epoch=139
03/13/2022 23:34:59 - INFO - __main__ - Step 710 Global step 710 Train loss 1.443423 on epoch=141
03/13/2022 23:35:04 - INFO - __main__ - Step 720 Global step 720 Train loss 1.201037 on epoch=143
03/13/2022 23:35:09 - INFO - __main__ - Step 730 Global step 730 Train loss 1.360847 on epoch=145
03/13/2022 23:35:14 - INFO - __main__ - Step 740 Global step 740 Train loss 1.417267 on epoch=147
03/13/2022 23:35:19 - INFO - __main__ - Step 750 Global step 750 Train loss 1.048165 on epoch=149
03/13/2022 23:35:22 - INFO - __main__ - Global step 750 Train loss 1.294148 Classification-F1 0.24774193548387097 on epoch=149
03/13/2022 23:35:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.901258 on epoch=151
03/13/2022 23:35:32 - INFO - __main__ - Step 770 Global step 770 Train loss 1.116390 on epoch=153
03/13/2022 23:35:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.922596 on epoch=155
03/13/2022 23:35:42 - INFO - __main__ - Step 790 Global step 790 Train loss 1.120753 on epoch=157
03/13/2022 23:35:47 - INFO - __main__ - Step 800 Global step 800 Train loss 1.034742 on epoch=159
03/13/2022 23:35:49 - INFO - __main__ - Global step 800 Train loss 1.019148 Classification-F1 0.25234567901234567 on epoch=159
03/13/2022 23:35:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.965387 on epoch=161
03/13/2022 23:35:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.985192 on epoch=163
03/13/2022 23:36:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.778634 on epoch=165
03/13/2022 23:36:09 - INFO - __main__ - Step 840 Global step 840 Train loss 1.114148 on epoch=167
03/13/2022 23:36:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.849163 on epoch=169
03/13/2022 23:36:16 - INFO - __main__ - Global step 850 Train loss 0.938505 Classification-F1 0.2445295870583227 on epoch=169
03/13/2022 23:36:21 - INFO - __main__ - Step 860 Global step 860 Train loss 1.088400 on epoch=171
03/13/2022 23:36:26 - INFO - __main__ - Step 870 Global step 870 Train loss 1.035466 on epoch=173
03/13/2022 23:36:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.794206 on epoch=175
03/13/2022 23:36:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.993892 on epoch=177
03/13/2022 23:36:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.843586 on epoch=179
03/13/2022 23:36:42 - INFO - __main__ - Global step 900 Train loss 0.951110 Classification-F1 0.2123456790123457 on epoch=179
03/13/2022 23:36:47 - INFO - __main__ - Step 910 Global step 910 Train loss 1.090828 on epoch=181
03/13/2022 23:36:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.862222 on epoch=183
03/13/2022 23:36:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.853211 on epoch=185
03/13/2022 23:37:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.793094 on epoch=187
03/13/2022 23:37:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.893334 on epoch=189
03/13/2022 23:37:09 - INFO - __main__ - Global step 950 Train loss 0.898538 Classification-F1 0.2307364836100468 on epoch=189
03/13/2022 23:37:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.901985 on epoch=191
03/13/2022 23:37:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.630406 on epoch=193
03/13/2022 23:37:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.665140 on epoch=195
03/13/2022 23:37:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.940864 on epoch=197
03/13/2022 23:37:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.805134 on epoch=199
03/13/2022 23:37:35 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:37:35 - INFO - __main__ - Printing 3 examples
03/13/2022 23:37:35 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/13/2022 23:37:35 - INFO - __main__ - ['Yes']
03/13/2022 23:37:35 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/13/2022 23:37:35 - INFO - __main__ - ['Yes']
03/13/2022 23:37:35 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/13/2022 23:37:35 - INFO - __main__ - ['Yes']
03/13/2022 23:37:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 23:37:35 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:37:35 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 23:37:35 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:37:35 - INFO - __main__ - Printing 3 examples
03/13/2022 23:37:35 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/13/2022 23:37:35 - INFO - __main__ - ['Yes']
03/13/2022 23:37:35 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/13/2022 23:37:35 - INFO - __main__ - ['Yes']
03/13/2022 23:37:35 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/13/2022 23:37:35 - INFO - __main__ - ['Yes']
03/13/2022 23:37:35 - INFO - __main__ - Tokenizing Input ...
03/13/2022 23:37:35 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:37:35 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 23:37:36 - INFO - __main__ - Global step 1000 Train loss 0.788706 Classification-F1 0.44116992737028077 on epoch=199
03/13/2022 23:37:36 - INFO - __main__ - save last model!
03/13/2022 23:37:43 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 23:37:44 - INFO - __main__ - Start tokenizing ... 6700 instances
03/13/2022 23:37:44 - INFO - __main__ - Printing 3 examples
03/13/2022 23:37:44 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/13/2022 23:37:44 - INFO - __main__ - ['No']
03/13/2022 23:37:44 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/13/2022 23:37:44 - INFO - __main__ - ['Yes']
03/13/2022 23:37:44 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/13/2022 23:37:44 - INFO - __main__ - ['Yes']
03/13/2022 23:37:44 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 23:37:47 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:37:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 23:37:48 - INFO - __main__ - Starting training!
03/13/2022 23:37:54 - INFO - __main__ - Loaded 6700 examples from test data
03/13/2022 23:40:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_13_0.0001_8_predictions.txt
03/13/2022 23:40:30 - INFO - __main__ - Classification-F1 on test data: 0.4295
03/13/2022 23:40:31 - INFO - __main__ - prefix=circa_16_13, lr=0.0001, bsz=8, dev_performance=0.44116992737028077, test_performance=0.4294621053951583
03/13/2022 23:40:31 - INFO - __main__ - Running ... prefix=circa_16_21, lr=0.0005, bsz=8 ...
03/13/2022 23:40:32 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:40:32 - INFO - __main__ - Printing 3 examples
03/13/2022 23:40:32 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/13/2022 23:40:32 - INFO - __main__ - ['Yes']
03/13/2022 23:40:32 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/13/2022 23:40:32 - INFO - __main__ - ['Yes']
03/13/2022 23:40:32 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/13/2022 23:40:32 - INFO - __main__ - ['Yes']
03/13/2022 23:40:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 23:40:32 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:40:32 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 23:40:32 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:40:32 - INFO - __main__ - Printing 3 examples
03/13/2022 23:40:32 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/13/2022 23:40:32 - INFO - __main__ - ['Yes']
03/13/2022 23:40:32 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/13/2022 23:40:32 - INFO - __main__ - ['Yes']
03/13/2022 23:40:32 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/13/2022 23:40:32 - INFO - __main__ - ['Yes']
03/13/2022 23:40:32 - INFO - __main__ - Tokenizing Input ...
03/13/2022 23:40:32 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:40:32 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 23:40:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 23:40:44 - INFO - __main__ - Starting training!
03/13/2022 23:40:48 - INFO - __main__ - Step 10 Global step 10 Train loss 22.213272 on epoch=1
03/13/2022 23:40:52 - INFO - __main__ - Step 20 Global step 20 Train loss 19.049923 on epoch=3
03/13/2022 23:40:57 - INFO - __main__ - Step 30 Global step 30 Train loss 11.453175 on epoch=5
03/13/2022 23:41:02 - INFO - __main__ - Step 40 Global step 40 Train loss 9.262170 on epoch=7
03/13/2022 23:41:07 - INFO - __main__ - Step 50 Global step 50 Train loss 8.578723 on epoch=9
03/13/2022 23:41:08 - INFO - __main__ - Global step 50 Train loss 14.111453 Classification-F1 0.0 on epoch=9
03/13/2022 23:41:14 - INFO - __main__ - Step 60 Global step 60 Train loss 7.781581 on epoch=11
03/13/2022 23:41:19 - INFO - __main__ - Step 70 Global step 70 Train loss 6.031386 on epoch=13
03/13/2022 23:41:24 - INFO - __main__ - Step 80 Global step 80 Train loss 4.646111 on epoch=15
03/13/2022 23:41:29 - INFO - __main__ - Step 90 Global step 90 Train loss 3.100638 on epoch=17
03/13/2022 23:41:34 - INFO - __main__ - Step 100 Global step 100 Train loss 2.179685 on epoch=19
03/13/2022 23:41:36 - INFO - __main__ - Global step 100 Train loss 4.747880 Classification-F1 0.06666666666666668 on epoch=19
03/13/2022 23:41:42 - INFO - __main__ - Step 110 Global step 110 Train loss 2.068079 on epoch=21
03/13/2022 23:41:47 - INFO - __main__ - Step 120 Global step 120 Train loss 1.745701 on epoch=23
03/13/2022 23:41:52 - INFO - __main__ - Step 130 Global step 130 Train loss 2.032417 on epoch=25
03/13/2022 23:41:57 - INFO - __main__ - Step 140 Global step 140 Train loss 1.713188 on epoch=27
03/13/2022 23:42:02 - INFO - __main__ - Step 150 Global step 150 Train loss 1.436567 on epoch=29
03/13/2022 23:42:04 - INFO - __main__ - Global step 150 Train loss 1.799190 Classification-F1 0.06666666666666668 on epoch=29
03/13/2022 23:42:09 - INFO - __main__ - Step 160 Global step 160 Train loss 1.281277 on epoch=31
03/13/2022 23:42:14 - INFO - __main__ - Step 170 Global step 170 Train loss 1.325949 on epoch=33
03/13/2022 23:42:19 - INFO - __main__ - Step 180 Global step 180 Train loss 1.180653 on epoch=35
03/13/2022 23:42:25 - INFO - __main__ - Step 190 Global step 190 Train loss 1.061614 on epoch=37
03/13/2022 23:42:30 - INFO - __main__ - Step 200 Global step 200 Train loss 1.283645 on epoch=39
03/13/2022 23:42:31 - INFO - __main__ - Global step 200 Train loss 1.226627 Classification-F1 0.06666666666666668 on epoch=39
03/13/2022 23:42:36 - INFO - __main__ - Step 210 Global step 210 Train loss 1.339964 on epoch=41
03/13/2022 23:42:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.955273 on epoch=43
03/13/2022 23:42:46 - INFO - __main__ - Step 230 Global step 230 Train loss 1.123446 on epoch=45
03/13/2022 23:42:51 - INFO - __main__ - Step 240 Global step 240 Train loss 1.096703 on epoch=47
03/13/2022 23:42:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.739796 on epoch=49
03/13/2022 23:42:58 - INFO - __main__ - Global step 250 Train loss 1.051036 Classification-F1 0.06666666666666668 on epoch=49
03/13/2022 23:43:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.893231 on epoch=51
03/13/2022 23:43:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.760737 on epoch=53
03/13/2022 23:43:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.841338 on epoch=55
03/13/2022 23:43:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.658145 on epoch=57
03/13/2022 23:43:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.782837 on epoch=59
03/13/2022 23:43:25 - INFO - __main__ - Global step 300 Train loss 0.787257 Classification-F1 0.13953153153153153 on epoch=59
03/13/2022 23:43:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.710367 on epoch=61
03/13/2022 23:43:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.666961 on epoch=63
03/13/2022 23:43:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.575209 on epoch=65
03/13/2022 23:43:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.633672 on epoch=67
03/13/2022 23:43:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.555254 on epoch=69
03/13/2022 23:43:53 - INFO - __main__ - Global step 350 Train loss 0.628293 Classification-F1 0.06808510638297872 on epoch=69
03/13/2022 23:43:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.528164 on epoch=71
03/13/2022 23:44:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.566301 on epoch=73
03/13/2022 23:44:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.626757 on epoch=75
03/13/2022 23:44:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.574787 on epoch=77
03/13/2022 23:44:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.550228 on epoch=79
03/13/2022 23:44:20 - INFO - __main__ - Global step 400 Train loss 0.569248 Classification-F1 0.11937092443421556 on epoch=79
03/13/2022 23:44:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.446822 on epoch=81
03/13/2022 23:44:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.481489 on epoch=83
03/13/2022 23:44:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.536346 on epoch=85
03/13/2022 23:44:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.460643 on epoch=87
03/13/2022 23:44:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.453124 on epoch=89
03/13/2022 23:44:47 - INFO - __main__ - Global step 450 Train loss 0.475685 Classification-F1 0.13422459893048128 on epoch=89
03/13/2022 23:44:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.610415 on epoch=91
03/13/2022 23:44:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.452808 on epoch=93
03/13/2022 23:45:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.399275 on epoch=95
03/13/2022 23:45:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.558897 on epoch=97
03/13/2022 23:45:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.421796 on epoch=99
03/13/2022 23:45:14 - INFO - __main__ - Global step 500 Train loss 0.488639 Classification-F1 0.11911111111111113 on epoch=99
03/13/2022 23:45:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.409757 on epoch=101
03/13/2022 23:45:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.414833 on epoch=103
03/13/2022 23:45:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.413217 on epoch=105
03/13/2022 23:45:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.350198 on epoch=107
03/13/2022 23:45:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.391494 on epoch=109
03/13/2022 23:45:41 - INFO - __main__ - Global step 550 Train loss 0.395900 Classification-F1 0.14096588990206013 on epoch=109
03/13/2022 23:45:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.409534 on epoch=111
03/13/2022 23:45:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.403126 on epoch=113
03/13/2022 23:45:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.378593 on epoch=115
03/13/2022 23:46:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.365854 on epoch=117
03/13/2022 23:46:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.350630 on epoch=119
03/13/2022 23:46:09 - INFO - __main__ - Global step 600 Train loss 0.381548 Classification-F1 0.14771006130544534 on epoch=119
03/13/2022 23:46:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.337340 on epoch=121
03/13/2022 23:46:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.359605 on epoch=123
03/13/2022 23:46:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.359515 on epoch=125
03/13/2022 23:46:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.370187 on epoch=127
03/13/2022 23:46:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.352585 on epoch=129
03/13/2022 23:46:36 - INFO - __main__ - Global step 650 Train loss 0.355847 Classification-F1 0.17796836287402323 on epoch=129
03/13/2022 23:46:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.352833 on epoch=131
03/13/2022 23:46:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.388038 on epoch=133
03/13/2022 23:46:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.351953 on epoch=135
03/13/2022 23:46:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.365032 on epoch=137
03/13/2022 23:47:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.335018 on epoch=139
03/13/2022 23:47:04 - INFO - __main__ - Global step 700 Train loss 0.358575 Classification-F1 0.16964094150312362 on epoch=139
03/13/2022 23:47:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.321881 on epoch=141
03/13/2022 23:47:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.344263 on epoch=143
03/13/2022 23:47:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.354576 on epoch=145
03/13/2022 23:47:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.351336 on epoch=147
03/13/2022 23:47:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.334328 on epoch=149
03/13/2022 23:47:31 - INFO - __main__ - Global step 750 Train loss 0.341277 Classification-F1 0.1995051022642666 on epoch=149
03/13/2022 23:47:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.338358 on epoch=151
03/13/2022 23:47:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.322099 on epoch=153
03/13/2022 23:47:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.313822 on epoch=155
03/13/2022 23:47:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.330882 on epoch=157
03/13/2022 23:47:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.296231 on epoch=159
03/13/2022 23:48:00 - INFO - __main__ - Global step 800 Train loss 0.320278 Classification-F1 0.17916159567275186 on epoch=159
03/13/2022 23:48:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.335572 on epoch=161
03/13/2022 23:48:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.290637 on epoch=163
03/13/2022 23:48:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.302974 on epoch=165
03/13/2022 23:48:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.301910 on epoch=167
03/13/2022 23:48:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.242777 on epoch=169
03/13/2022 23:48:27 - INFO - __main__ - Global step 850 Train loss 0.294774 Classification-F1 0.2927474454017231 on epoch=169
03/13/2022 23:48:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.254023 on epoch=171
03/13/2022 23:48:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.187187 on epoch=173
03/13/2022 23:48:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.161220 on epoch=175
03/13/2022 23:48:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.145428 on epoch=177
03/13/2022 23:48:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.123448 on epoch=179
03/13/2022 23:48:55 - INFO - __main__ - Global step 900 Train loss 0.174261 Classification-F1 0.41414865835918463 on epoch=179
03/13/2022 23:49:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.141299 on epoch=181
03/13/2022 23:49:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.088218 on epoch=183
03/13/2022 23:49:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.073232 on epoch=185
03/13/2022 23:49:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.060867 on epoch=187
03/13/2022 23:49:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.031061 on epoch=189
03/13/2022 23:49:23 - INFO - __main__ - Global step 950 Train loss 0.078935 Classification-F1 0.3462279324030476 on epoch=189
03/13/2022 23:49:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.049595 on epoch=191
03/13/2022 23:49:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.042490 on epoch=193
03/13/2022 23:49:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.019494 on epoch=195
03/13/2022 23:49:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.040579 on epoch=197
03/13/2022 23:49:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.017398 on epoch=199
03/13/2022 23:49:49 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:49:49 - INFO - __main__ - Printing 3 examples
03/13/2022 23:49:49 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/13/2022 23:49:49 - INFO - __main__ - ['Yes']
03/13/2022 23:49:49 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/13/2022 23:49:49 - INFO - __main__ - ['Yes']
03/13/2022 23:49:49 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/13/2022 23:49:49 - INFO - __main__ - ['Yes']
03/13/2022 23:49:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/13/2022 23:49:49 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:49:49 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 23:49:49 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:49:49 - INFO - __main__ - Printing 3 examples
03/13/2022 23:49:49 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/13/2022 23:49:49 - INFO - __main__ - ['Yes']
03/13/2022 23:49:49 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/13/2022 23:49:49 - INFO - __main__ - ['Yes']
03/13/2022 23:49:49 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/13/2022 23:49:49 - INFO - __main__ - ['Yes']
03/13/2022 23:49:49 - INFO - __main__ - Tokenizing Input ...
03/13/2022 23:49:49 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:49:50 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 23:49:50 - INFO - __main__ - Global step 1000 Train loss 0.033911 Classification-F1 0.33878498033599314 on epoch=199
03/13/2022 23:49:50 - INFO - __main__ - save last model!
03/13/2022 23:49:57 - INFO - __main__ - Loading checkpoint on the fly
03/13/2022 23:49:58 - INFO - __main__ - Start tokenizing ... 6700 instances
03/13/2022 23:49:58 - INFO - __main__ - Printing 3 examples
03/13/2022 23:49:58 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/13/2022 23:49:58 - INFO - __main__ - ['No']
03/13/2022 23:49:58 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/13/2022 23:49:58 - INFO - __main__ - ['Yes']
03/13/2022 23:49:58 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/13/2022 23:49:58 - INFO - __main__ - ['Yes']
03/13/2022 23:49:58 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 23:50:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 23:50:00 - INFO - __main__ - Starting training!
03/13/2022 23:50:01 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:50:08 - INFO - __main__ - Loaded 6700 examples from test data
03/13/2022 23:52:58 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_21_0.0005_8_predictions.txt
03/13/2022 23:52:58 - INFO - __main__ - Classification-F1 on test data: 0.2057
03/13/2022 23:52:58 - INFO - __main__ - prefix=circa_16_21, lr=0.0005, bsz=8, dev_performance=0.41414865835918463, test_performance=0.20565302943730024
03/13/2022 23:52:58 - INFO - __main__ - Running ... prefix=circa_16_21, lr=0.0003, bsz=8 ...
03/13/2022 23:52:59 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:52:59 - INFO - __main__ - Printing 3 examples
03/13/2022 23:52:59 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/13/2022 23:52:59 - INFO - __main__ - ['Yes']
03/13/2022 23:52:59 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/13/2022 23:52:59 - INFO - __main__ - ['Yes']
03/13/2022 23:52:59 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/13/2022 23:52:59 - INFO - __main__ - ['Yes']
03/13/2022 23:52:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/13/2022 23:52:59 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:52:59 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/13/2022 23:52:59 - INFO - __main__ - Start tokenizing ... 80 instances
03/13/2022 23:52:59 - INFO - __main__ - Printing 3 examples
03/13/2022 23:52:59 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/13/2022 23:52:59 - INFO - __main__ - ['Yes']
03/13/2022 23:52:59 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/13/2022 23:52:59 - INFO - __main__ - ['Yes']
03/13/2022 23:52:59 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/13/2022 23:52:59 - INFO - __main__ - ['Yes']
03/13/2022 23:52:59 - INFO - __main__ - Tokenizing Input ...
03/13/2022 23:52:59 - INFO - __main__ - Tokenizing Output ...
03/13/2022 23:52:59 - INFO - __main__ - Loaded 80 examples from dev data
03/13/2022 23:53:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/13/2022 23:53:11 - INFO - __main__ - Starting training!
03/13/2022 23:53:15 - INFO - __main__ - Step 10 Global step 10 Train loss 23.163902 on epoch=1
03/13/2022 23:53:20 - INFO - __main__ - Step 20 Global step 20 Train loss 19.349768 on epoch=3
03/13/2022 23:53:25 - INFO - __main__ - Step 30 Global step 30 Train loss 12.329464 on epoch=5
03/13/2022 23:53:30 - INFO - __main__ - Step 40 Global step 40 Train loss 10.367506 on epoch=7
03/13/2022 23:53:35 - INFO - __main__ - Step 50 Global step 50 Train loss 8.930617 on epoch=9
03/13/2022 23:53:36 - INFO - __main__ - Global step 50 Train loss 14.828250 Classification-F1 0.0 on epoch=9
03/13/2022 23:53:42 - INFO - __main__ - Step 60 Global step 60 Train loss 7.866602 on epoch=11
03/13/2022 23:53:47 - INFO - __main__ - Step 70 Global step 70 Train loss 7.402995 on epoch=13
03/13/2022 23:53:52 - INFO - __main__ - Step 80 Global step 80 Train loss 6.352173 on epoch=15
03/13/2022 23:53:57 - INFO - __main__ - Step 90 Global step 90 Train loss 5.637733 on epoch=17
03/13/2022 23:54:02 - INFO - __main__ - Step 100 Global step 100 Train loss 5.021174 on epoch=19
03/13/2022 23:54:04 - INFO - __main__ - Global step 100 Train loss 6.456135 Classification-F1 0.0 on epoch=19
03/13/2022 23:54:09 - INFO - __main__ - Step 110 Global step 110 Train loss 4.741100 on epoch=21
03/13/2022 23:54:14 - INFO - __main__ - Step 120 Global step 120 Train loss 3.099126 on epoch=23
03/13/2022 23:54:19 - INFO - __main__ - Step 130 Global step 130 Train loss 2.665260 on epoch=25
03/13/2022 23:54:24 - INFO - __main__ - Step 140 Global step 140 Train loss 2.083023 on epoch=27
03/13/2022 23:54:29 - INFO - __main__ - Step 150 Global step 150 Train loss 1.774037 on epoch=29
03/13/2022 23:54:31 - INFO - __main__ - Global step 150 Train loss 2.872509 Classification-F1 0.06736842105263158 on epoch=29
03/13/2022 23:54:37 - INFO - __main__ - Step 160 Global step 160 Train loss 1.807154 on epoch=31
03/13/2022 23:54:42 - INFO - __main__ - Step 170 Global step 170 Train loss 2.047069 on epoch=33
03/13/2022 23:54:47 - INFO - __main__ - Step 180 Global step 180 Train loss 1.473066 on epoch=35
03/13/2022 23:54:52 - INFO - __main__ - Step 190 Global step 190 Train loss 1.401317 on epoch=37
03/13/2022 23:54:57 - INFO - __main__ - Step 200 Global step 200 Train loss 1.438440 on epoch=39
03/13/2022 23:54:59 - INFO - __main__ - Global step 200 Train loss 1.633409 Classification-F1 0.06881720430107527 on epoch=39
03/13/2022 23:55:04 - INFO - __main__ - Step 210 Global step 210 Train loss 1.520817 on epoch=41
03/13/2022 23:55:09 - INFO - __main__ - Step 220 Global step 220 Train loss 1.466938 on epoch=43
03/13/2022 23:55:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.975921 on epoch=45
03/13/2022 23:55:19 - INFO - __main__ - Step 240 Global step 240 Train loss 1.567028 on epoch=47
03/13/2022 23:55:24 - INFO - __main__ - Step 250 Global step 250 Train loss 1.476820 on epoch=49
03/13/2022 23:55:27 - INFO - __main__ - Global step 250 Train loss 1.401505 Classification-F1 0.20476190476190476 on epoch=49
03/13/2022 23:55:33 - INFO - __main__ - Step 260 Global step 260 Train loss 1.318313 on epoch=51
03/13/2022 23:55:38 - INFO - __main__ - Step 270 Global step 270 Train loss 1.359598 on epoch=53
03/13/2022 23:55:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.952021 on epoch=55
03/13/2022 23:55:48 - INFO - __main__ - Step 290 Global step 290 Train loss 1.051266 on epoch=57
03/13/2022 23:55:53 - INFO - __main__ - Step 300 Global step 300 Train loss 1.296669 on epoch=59
03/13/2022 23:55:55 - INFO - __main__ - Global step 300 Train loss 1.195573 Classification-F1 0.11283422459893049 on epoch=59
03/13/2022 23:56:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.890907 on epoch=61
03/13/2022 23:56:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.996522 on epoch=63
03/13/2022 23:56:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.948552 on epoch=65
03/13/2022 23:56:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.911120 on epoch=67
03/13/2022 23:56:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.960118 on epoch=69
03/13/2022 23:56:22 - INFO - __main__ - Global step 350 Train loss 0.941444 Classification-F1 0.12258610954263127 on epoch=69
03/13/2022 23:56:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.735037 on epoch=71
03/13/2022 23:56:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.673794 on epoch=73
03/13/2022 23:56:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.834167 on epoch=75
03/13/2022 23:56:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.731280 on epoch=77
03/13/2022 23:56:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.674823 on epoch=79
03/13/2022 23:56:49 - INFO - __main__ - Global step 400 Train loss 0.729820 Classification-F1 0.10313725490196077 on epoch=79
03/13/2022 23:56:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.814782 on epoch=81
03/13/2022 23:56:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.759238 on epoch=83
03/13/2022 23:57:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.691708 on epoch=85
03/13/2022 23:57:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.616633 on epoch=87
03/13/2022 23:57:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.596945 on epoch=89
03/13/2022 23:57:17 - INFO - __main__ - Global step 450 Train loss 0.695861 Classification-F1 0.13416666666666668 on epoch=89
03/13/2022 23:57:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.603653 on epoch=91
03/13/2022 23:57:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.529559 on epoch=93
03/13/2022 23:57:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.623027 on epoch=95
03/13/2022 23:57:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.444453 on epoch=97
03/13/2022 23:57:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.370615 on epoch=99
03/13/2022 23:57:44 - INFO - __main__ - Global step 500 Train loss 0.514261 Classification-F1 0.35273504273504275 on epoch=99
03/13/2022 23:57:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.315475 on epoch=101
03/13/2022 23:57:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.276428 on epoch=103
03/13/2022 23:57:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.257443 on epoch=105
03/13/2022 23:58:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.228641 on epoch=107
03/13/2022 23:58:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.223243 on epoch=109
03/13/2022 23:58:11 - INFO - __main__ - Global step 550 Train loss 0.260246 Classification-F1 0.5199796282149223 on epoch=109
03/13/2022 23:58:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.201649 on epoch=111
03/13/2022 23:58:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.185085 on epoch=113
03/13/2022 23:58:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.120865 on epoch=115
03/13/2022 23:58:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.133625 on epoch=117
03/13/2022 23:58:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.095394 on epoch=119
03/13/2022 23:58:39 - INFO - __main__ - Global step 600 Train loss 0.147324 Classification-F1 0.3627221523995718 on epoch=119
03/13/2022 23:58:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.087962 on epoch=121
03/13/2022 23:58:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.060874 on epoch=123
03/13/2022 23:58:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.080672 on epoch=125
03/13/2022 23:58:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.048397 on epoch=127
03/13/2022 23:59:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.018937 on epoch=129
03/13/2022 23:59:07 - INFO - __main__ - Global step 650 Train loss 0.059369 Classification-F1 0.2989965334792921 on epoch=129
03/13/2022 23:59:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.009362 on epoch=131
03/13/2022 23:59:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.018536 on epoch=133
03/13/2022 23:59:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.021828 on epoch=135
03/13/2022 23:59:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.011646 on epoch=137
03/13/2022 23:59:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.009144 on epoch=139
03/13/2022 23:59:34 - INFO - __main__ - Global step 700 Train loss 0.014103 Classification-F1 0.3101372244229387 on epoch=139
03/13/2022 23:59:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.017955 on epoch=141
03/13/2022 23:59:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.119427 on epoch=143
03/13/2022 23:59:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.242568 on epoch=145
03/13/2022 23:59:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.036657 on epoch=147
03/13/2022 23:59:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.032376 on epoch=149
03/14/2022 00:00:01 - INFO - __main__ - Global step 750 Train loss 0.089797 Classification-F1 0.3815436978480457 on epoch=149
03/14/2022 00:00:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.026050 on epoch=151
03/14/2022 00:00:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.042909 on epoch=153
03/14/2022 00:00:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.084798 on epoch=155
03/14/2022 00:00:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.211469 on epoch=157
03/14/2022 00:00:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.600866 on epoch=159
03/14/2022 00:00:28 - INFO - __main__ - Global step 800 Train loss 0.193218 Classification-F1 0.06666666666666668 on epoch=159
03/14/2022 00:00:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.553479 on epoch=161
03/14/2022 00:00:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.561123 on epoch=163
03/14/2022 00:00:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.576244 on epoch=165
03/14/2022 00:00:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.444814 on epoch=167
03/14/2022 00:00:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.363839 on epoch=169
03/14/2022 00:00:55 - INFO - __main__ - Global step 850 Train loss 0.499900 Classification-F1 0.19781358443585226 on epoch=169
03/14/2022 00:01:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.386398 on epoch=171
03/14/2022 00:01:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.388593 on epoch=173
03/14/2022 00:01:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.353892 on epoch=175
03/14/2022 00:01:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.350648 on epoch=177
03/14/2022 00:01:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.351892 on epoch=179
03/14/2022 00:01:22 - INFO - __main__ - Global step 900 Train loss 0.366285 Classification-F1 0.21018965942565265 on epoch=179
03/14/2022 00:01:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.345122 on epoch=181
03/14/2022 00:01:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.313710 on epoch=183
03/14/2022 00:01:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.304764 on epoch=185
03/14/2022 00:01:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.242242 on epoch=187
03/14/2022 00:01:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.233092 on epoch=189
03/14/2022 00:01:49 - INFO - __main__ - Global step 950 Train loss 0.287786 Classification-F1 0.20750259130540818 on epoch=189
03/14/2022 00:01:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.219924 on epoch=191
03/14/2022 00:02:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.284191 on epoch=193
03/14/2022 00:02:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.265908 on epoch=195
03/14/2022 00:02:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.262253 on epoch=197
03/14/2022 00:02:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.252459 on epoch=199
03/14/2022 00:02:16 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:02:16 - INFO - __main__ - Printing 3 examples
03/14/2022 00:02:16 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/14/2022 00:02:16 - INFO - __main__ - ['Yes']
03/14/2022 00:02:16 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/14/2022 00:02:16 - INFO - __main__ - ['Yes']
03/14/2022 00:02:16 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/14/2022 00:02:16 - INFO - __main__ - ['Yes']
03/14/2022 00:02:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 00:02:16 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:02:16 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 00:02:16 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:02:16 - INFO - __main__ - Printing 3 examples
03/14/2022 00:02:16 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/14/2022 00:02:16 - INFO - __main__ - ['Yes']
03/14/2022 00:02:16 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/14/2022 00:02:16 - INFO - __main__ - ['Yes']
03/14/2022 00:02:16 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/14/2022 00:02:16 - INFO - __main__ - ['Yes']
03/14/2022 00:02:16 - INFO - __main__ - Tokenizing Input ...
03/14/2022 00:02:16 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:02:16 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 00:02:17 - INFO - __main__ - Global step 1000 Train loss 0.256947 Classification-F1 0.18957613814756671 on epoch=199
03/14/2022 00:02:17 - INFO - __main__ - save last model!
03/14/2022 00:02:24 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 00:02:25 - INFO - __main__ - Start tokenizing ... 6700 instances
03/14/2022 00:02:25 - INFO - __main__ - Printing 3 examples
03/14/2022 00:02:25 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/14/2022 00:02:25 - INFO - __main__ - ['No']
03/14/2022 00:02:25 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/14/2022 00:02:25 - INFO - __main__ - ['Yes']
03/14/2022 00:02:25 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/14/2022 00:02:25 - INFO - __main__ - ['Yes']
03/14/2022 00:02:25 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 00:02:28 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:02:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 00:02:29 - INFO - __main__ - Starting training!
03/14/2022 00:02:34 - INFO - __main__ - Loaded 6700 examples from test data
03/14/2022 00:05:08 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_21_0.0003_8_predictions.txt
03/14/2022 00:05:08 - INFO - __main__ - Classification-F1 on test data: 0.3647
03/14/2022 00:05:08 - INFO - __main__ - prefix=circa_16_21, lr=0.0003, bsz=8, dev_performance=0.5199796282149223, test_performance=0.36470007503368695
03/14/2022 00:05:08 - INFO - __main__ - Running ... prefix=circa_16_21, lr=0.0002, bsz=8 ...
03/14/2022 00:05:09 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:05:09 - INFO - __main__ - Printing 3 examples
03/14/2022 00:05:09 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/14/2022 00:05:09 - INFO - __main__ - ['Yes']
03/14/2022 00:05:09 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/14/2022 00:05:09 - INFO - __main__ - ['Yes']
03/14/2022 00:05:09 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/14/2022 00:05:09 - INFO - __main__ - ['Yes']
03/14/2022 00:05:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 00:05:09 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:05:09 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 00:05:09 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:05:09 - INFO - __main__ - Printing 3 examples
03/14/2022 00:05:09 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/14/2022 00:05:09 - INFO - __main__ - ['Yes']
03/14/2022 00:05:09 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/14/2022 00:05:09 - INFO - __main__ - ['Yes']
03/14/2022 00:05:09 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/14/2022 00:05:09 - INFO - __main__ - ['Yes']
03/14/2022 00:05:09 - INFO - __main__ - Tokenizing Input ...
03/14/2022 00:05:10 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:05:10 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 00:05:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 00:05:21 - INFO - __main__ - Starting training!
03/14/2022 00:05:26 - INFO - __main__ - Step 10 Global step 10 Train loss 21.530586 on epoch=1
03/14/2022 00:05:30 - INFO - __main__ - Step 20 Global step 20 Train loss 20.385798 on epoch=3
03/14/2022 00:05:35 - INFO - __main__ - Step 30 Global step 30 Train loss 13.029501 on epoch=5
03/14/2022 00:05:40 - INFO - __main__ - Step 40 Global step 40 Train loss 9.303514 on epoch=7
03/14/2022 00:05:45 - INFO - __main__ - Step 50 Global step 50 Train loss 8.240730 on epoch=9
03/14/2022 00:05:48 - INFO - __main__ - Global step 50 Train loss 14.498026 Classification-F1 0.0 on epoch=9
03/14/2022 00:05:53 - INFO - __main__ - Step 60 Global step 60 Train loss 8.283091 on epoch=11
03/14/2022 00:05:58 - INFO - __main__ - Step 70 Global step 70 Train loss 7.791836 on epoch=13
03/14/2022 00:06:04 - INFO - __main__ - Step 80 Global step 80 Train loss 6.817363 on epoch=15
03/14/2022 00:06:09 - INFO - __main__ - Step 90 Global step 90 Train loss 6.649808 on epoch=17
03/14/2022 00:06:14 - INFO - __main__ - Step 100 Global step 100 Train loss 6.114892 on epoch=19
03/14/2022 00:06:16 - INFO - __main__ - Global step 100 Train loss 7.131398 Classification-F1 0.0 on epoch=19
03/14/2022 00:06:21 - INFO - __main__ - Step 110 Global step 110 Train loss 6.179372 on epoch=21
03/14/2022 00:06:26 - INFO - __main__ - Step 120 Global step 120 Train loss 5.279142 on epoch=23
03/14/2022 00:06:31 - INFO - __main__ - Step 130 Global step 130 Train loss 5.441885 on epoch=25
03/14/2022 00:06:36 - INFO - __main__ - Step 140 Global step 140 Train loss 4.964999 on epoch=27
03/14/2022 00:06:41 - INFO - __main__ - Step 150 Global step 150 Train loss 4.721484 on epoch=29
03/14/2022 00:06:43 - INFO - __main__ - Global step 150 Train loss 5.317376 Classification-F1 0.0 on epoch=29
03/14/2022 00:06:48 - INFO - __main__ - Step 160 Global step 160 Train loss 4.366233 on epoch=31
03/14/2022 00:06:53 - INFO - __main__ - Step 170 Global step 170 Train loss 3.177216 on epoch=33
03/14/2022 00:06:59 - INFO - __main__ - Step 180 Global step 180 Train loss 2.703382 on epoch=35
03/14/2022 00:07:04 - INFO - __main__ - Step 190 Global step 190 Train loss 2.373825 on epoch=37
03/14/2022 00:07:09 - INFO - __main__ - Step 200 Global step 200 Train loss 1.825649 on epoch=39
03/14/2022 00:07:11 - INFO - __main__ - Global step 200 Train loss 2.889261 Classification-F1 0.06666666666666668 on epoch=39
03/14/2022 00:07:17 - INFO - __main__ - Step 210 Global step 210 Train loss 1.863411 on epoch=41
03/14/2022 00:07:22 - INFO - __main__ - Step 220 Global step 220 Train loss 1.794061 on epoch=43
03/14/2022 00:07:27 - INFO - __main__ - Step 230 Global step 230 Train loss 1.918293 on epoch=45
03/14/2022 00:07:32 - INFO - __main__ - Step 240 Global step 240 Train loss 1.621893 on epoch=47
03/14/2022 00:07:37 - INFO - __main__ - Step 250 Global step 250 Train loss 1.518578 on epoch=49
03/14/2022 00:07:39 - INFO - __main__ - Global step 250 Train loss 1.743247 Classification-F1 0.12987012987012986 on epoch=49
03/14/2022 00:07:45 - INFO - __main__ - Step 260 Global step 260 Train loss 1.195924 on epoch=51
03/14/2022 00:07:50 - INFO - __main__ - Step 270 Global step 270 Train loss 1.622217 on epoch=53
03/14/2022 00:07:55 - INFO - __main__ - Step 280 Global step 280 Train loss 1.498632 on epoch=55
03/14/2022 00:08:01 - INFO - __main__ - Step 290 Global step 290 Train loss 1.471717 on epoch=57
03/14/2022 00:08:06 - INFO - __main__ - Step 300 Global step 300 Train loss 1.487734 on epoch=59
03/14/2022 00:08:08 - INFO - __main__ - Global step 300 Train loss 1.455245 Classification-F1 0.10990287968107215 on epoch=59
03/14/2022 00:08:13 - INFO - __main__ - Step 310 Global step 310 Train loss 1.379425 on epoch=61
03/14/2022 00:08:18 - INFO - __main__ - Step 320 Global step 320 Train loss 1.607132 on epoch=63
03/14/2022 00:08:23 - INFO - __main__ - Step 330 Global step 330 Train loss 1.308133 on epoch=65
03/14/2022 00:08:28 - INFO - __main__ - Step 340 Global step 340 Train loss 1.764372 on epoch=67
03/14/2022 00:08:33 - INFO - __main__ - Step 350 Global step 350 Train loss 1.318770 on epoch=69
03/14/2022 00:08:35 - INFO - __main__ - Global step 350 Train loss 1.475566 Classification-F1 0.10999654098927705 on epoch=69
03/14/2022 00:08:40 - INFO - __main__ - Step 360 Global step 360 Train loss 1.279070 on epoch=71
03/14/2022 00:08:45 - INFO - __main__ - Step 370 Global step 370 Train loss 1.351292 on epoch=73
03/14/2022 00:08:51 - INFO - __main__ - Step 380 Global step 380 Train loss 1.225866 on epoch=75
03/14/2022 00:08:56 - INFO - __main__ - Step 390 Global step 390 Train loss 1.345874 on epoch=77
03/14/2022 00:09:01 - INFO - __main__ - Step 400 Global step 400 Train loss 1.114777 on epoch=79
03/14/2022 00:09:03 - INFO - __main__ - Global step 400 Train loss 1.263376 Classification-F1 0.1169823861748089 on epoch=79
03/14/2022 00:09:08 - INFO - __main__ - Step 410 Global step 410 Train loss 1.005817 on epoch=81
03/14/2022 00:09:13 - INFO - __main__ - Step 420 Global step 420 Train loss 1.184269 on epoch=83
03/14/2022 00:09:18 - INFO - __main__ - Step 430 Global step 430 Train loss 1.067467 on epoch=85
03/14/2022 00:09:23 - INFO - __main__ - Step 440 Global step 440 Train loss 1.002295 on epoch=87
03/14/2022 00:09:28 - INFO - __main__ - Step 450 Global step 450 Train loss 1.044542 on epoch=89
03/14/2022 00:09:30 - INFO - __main__ - Global step 450 Train loss 1.060878 Classification-F1 0.11111111111111112 on epoch=89
03/14/2022 00:09:35 - INFO - __main__ - Step 460 Global step 460 Train loss 1.033344 on epoch=91
03/14/2022 00:09:40 - INFO - __main__ - Step 470 Global step 470 Train loss 1.076228 on epoch=93
03/14/2022 00:09:45 - INFO - __main__ - Step 480 Global step 480 Train loss 1.104913 on epoch=95
03/14/2022 00:09:51 - INFO - __main__ - Step 490 Global step 490 Train loss 1.102979 on epoch=97
03/14/2022 00:09:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.768515 on epoch=99
03/14/2022 00:09:58 - INFO - __main__ - Global step 500 Train loss 1.017195 Classification-F1 0.07057057057057058 on epoch=99
03/14/2022 00:10:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.980841 on epoch=101
03/14/2022 00:10:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.987446 on epoch=103
03/14/2022 00:10:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.973899 on epoch=105
03/14/2022 00:10:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.944131 on epoch=107
03/14/2022 00:10:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.881575 on epoch=109
03/14/2022 00:10:25 - INFO - __main__ - Global step 550 Train loss 0.953578 Classification-F1 0.2196770997260593 on epoch=109
03/14/2022 00:10:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.852724 on epoch=111
03/14/2022 00:10:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.832591 on epoch=113
03/14/2022 00:10:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.826161 on epoch=115
03/14/2022 00:10:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.755660 on epoch=117
03/14/2022 00:10:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.644665 on epoch=119
03/14/2022 00:10:53 - INFO - __main__ - Global step 600 Train loss 0.782360 Classification-F1 0.1518139534883721 on epoch=119
03/14/2022 00:10:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.542032 on epoch=121
03/14/2022 00:11:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.668221 on epoch=123
03/14/2022 00:11:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.617166 on epoch=125
03/14/2022 00:11:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.599086 on epoch=127
03/14/2022 00:11:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.566442 on epoch=129
03/14/2022 00:11:21 - INFO - __main__ - Global step 650 Train loss 0.598589 Classification-F1 0.18914545853444492 on epoch=129
03/14/2022 00:11:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.539689 on epoch=131
03/14/2022 00:11:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.600827 on epoch=133
03/14/2022 00:11:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.625481 on epoch=135
03/14/2022 00:11:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.506055 on epoch=137
03/14/2022 00:11:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.526849 on epoch=139
03/14/2022 00:11:48 - INFO - __main__ - Global step 700 Train loss 0.559780 Classification-F1 0.1671765620948924 on epoch=139
03/14/2022 00:11:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.533608 on epoch=141
03/14/2022 00:11:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.467110 on epoch=143
03/14/2022 00:12:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.516277 on epoch=145
03/14/2022 00:12:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.523141 on epoch=147
03/14/2022 00:12:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.487225 on epoch=149
03/14/2022 00:12:16 - INFO - __main__ - Global step 750 Train loss 0.505472 Classification-F1 0.1836516424751719 on epoch=149
03/14/2022 00:12:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.539420 on epoch=151
03/14/2022 00:12:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.484735 on epoch=153
03/14/2022 00:12:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.484937 on epoch=155
03/14/2022 00:12:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.477556 on epoch=157
03/14/2022 00:12:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.480663 on epoch=159
03/14/2022 00:12:43 - INFO - __main__ - Global step 800 Train loss 0.493462 Classification-F1 0.17873563218390803 on epoch=159
03/14/2022 00:12:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.418066 on epoch=161
03/14/2022 00:12:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.471248 on epoch=163
03/14/2022 00:12:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.361706 on epoch=165
03/14/2022 00:13:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.423442 on epoch=167
03/14/2022 00:13:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.395788 on epoch=169
03/14/2022 00:13:11 - INFO - __main__ - Global step 850 Train loss 0.414050 Classification-F1 0.19757259528130672 on epoch=169
03/14/2022 00:13:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.404547 on epoch=171
03/14/2022 00:13:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.360914 on epoch=173
03/14/2022 00:13:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.413209 on epoch=175
03/14/2022 00:13:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.384449 on epoch=177
03/14/2022 00:13:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.372980 on epoch=179
03/14/2022 00:13:38 - INFO - __main__ - Global step 900 Train loss 0.387220 Classification-F1 0.32963265306122447 on epoch=179
03/14/2022 00:13:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.363945 on epoch=181
03/14/2022 00:13:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.370516 on epoch=183
03/14/2022 00:13:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.325319 on epoch=185
03/14/2022 00:14:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.319784 on epoch=187
03/14/2022 00:14:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.356535 on epoch=189
03/14/2022 00:14:07 - INFO - __main__ - Global step 950 Train loss 0.347220 Classification-F1 0.3442991790817878 on epoch=189
03/14/2022 00:14:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.294871 on epoch=191
03/14/2022 00:14:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.338010 on epoch=193
03/14/2022 00:14:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.392671 on epoch=195
03/14/2022 00:14:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.187774 on epoch=197
03/14/2022 00:14:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.371059 on epoch=199
03/14/2022 00:14:35 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:14:35 - INFO - __main__ - Printing 3 examples
03/14/2022 00:14:35 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/14/2022 00:14:35 - INFO - __main__ - ['Yes']
03/14/2022 00:14:35 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/14/2022 00:14:35 - INFO - __main__ - ['Yes']
03/14/2022 00:14:35 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/14/2022 00:14:35 - INFO - __main__ - ['Yes']
03/14/2022 00:14:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 00:14:35 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:14:35 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 00:14:35 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:14:35 - INFO - __main__ - Printing 3 examples
03/14/2022 00:14:35 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/14/2022 00:14:35 - INFO - __main__ - ['Yes']
03/14/2022 00:14:35 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/14/2022 00:14:35 - INFO - __main__ - ['Yes']
03/14/2022 00:14:35 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/14/2022 00:14:35 - INFO - __main__ - ['Yes']
03/14/2022 00:14:35 - INFO - __main__ - Tokenizing Input ...
03/14/2022 00:14:35 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:14:35 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 00:14:36 - INFO - __main__ - Global step 1000 Train loss 0.316877 Classification-F1 0.4676906313137045 on epoch=199
03/14/2022 00:14:36 - INFO - __main__ - save last model!
03/14/2022 00:14:43 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 00:14:44 - INFO - __main__ - Start tokenizing ... 6700 instances
03/14/2022 00:14:44 - INFO - __main__ - Printing 3 examples
03/14/2022 00:14:44 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/14/2022 00:14:44 - INFO - __main__ - ['No']
03/14/2022 00:14:44 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/14/2022 00:14:44 - INFO - __main__ - ['Yes']
03/14/2022 00:14:44 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/14/2022 00:14:44 - INFO - __main__ - ['Yes']
03/14/2022 00:14:44 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 00:14:47 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:14:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 00:14:48 - INFO - __main__ - Starting training!
03/14/2022 00:14:53 - INFO - __main__ - Loaded 6700 examples from test data
03/14/2022 00:17:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_21_0.0002_8_predictions.txt
03/14/2022 00:17:25 - INFO - __main__ - Classification-F1 on test data: 0.3674
03/14/2022 00:17:25 - INFO - __main__ - prefix=circa_16_21, lr=0.0002, bsz=8, dev_performance=0.4676906313137045, test_performance=0.36744866839870943
03/14/2022 00:17:25 - INFO - __main__ - Running ... prefix=circa_16_21, lr=0.0001, bsz=8 ...
03/14/2022 00:17:26 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:17:26 - INFO - __main__ - Printing 3 examples
03/14/2022 00:17:26 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/14/2022 00:17:26 - INFO - __main__ - ['Yes']
03/14/2022 00:17:26 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/14/2022 00:17:26 - INFO - __main__ - ['Yes']
03/14/2022 00:17:26 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/14/2022 00:17:26 - INFO - __main__ - ['Yes']
03/14/2022 00:17:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 00:17:26 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:17:26 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 00:17:26 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:17:26 - INFO - __main__ - Printing 3 examples
03/14/2022 00:17:26 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/14/2022 00:17:26 - INFO - __main__ - ['Yes']
03/14/2022 00:17:26 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/14/2022 00:17:26 - INFO - __main__ - ['Yes']
03/14/2022 00:17:26 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/14/2022 00:17:26 - INFO - __main__ - ['Yes']
03/14/2022 00:17:26 - INFO - __main__ - Tokenizing Input ...
03/14/2022 00:17:26 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:17:26 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 00:17:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 00:17:38 - INFO - __main__ - Starting training!
03/14/2022 00:17:42 - INFO - __main__ - Step 10 Global step 10 Train loss 21.809122 on epoch=1
03/14/2022 00:17:47 - INFO - __main__ - Step 20 Global step 20 Train loss 20.388189 on epoch=3
03/14/2022 00:17:52 - INFO - __main__ - Step 30 Global step 30 Train loss 17.950645 on epoch=5
03/14/2022 00:17:57 - INFO - __main__ - Step 40 Global step 40 Train loss 15.469638 on epoch=7
03/14/2022 00:18:02 - INFO - __main__ - Step 50 Global step 50 Train loss 13.829077 on epoch=9
03/14/2022 00:18:29 - INFO - __main__ - Global step 50 Train loss 17.889334 Classification-F1 0.0 on epoch=9
03/14/2022 00:18:35 - INFO - __main__ - Step 60 Global step 60 Train loss 12.408775 on epoch=11
03/14/2022 00:18:40 - INFO - __main__ - Step 70 Global step 70 Train loss 11.165551 on epoch=13
03/14/2022 00:18:45 - INFO - __main__ - Step 80 Global step 80 Train loss 10.607718 on epoch=15
03/14/2022 00:18:50 - INFO - __main__ - Step 90 Global step 90 Train loss 10.780055 on epoch=17
03/14/2022 00:18:55 - INFO - __main__ - Step 100 Global step 100 Train loss 10.663284 on epoch=19
03/14/2022 00:19:05 - INFO - __main__ - Global step 100 Train loss 11.125077 Classification-F1 0.0 on epoch=19
03/14/2022 00:19:10 - INFO - __main__ - Step 110 Global step 110 Train loss 10.334992 on epoch=21
03/14/2022 00:19:15 - INFO - __main__ - Step 120 Global step 120 Train loss 9.183972 on epoch=23
03/14/2022 00:19:20 - INFO - __main__ - Step 130 Global step 130 Train loss 8.814680 on epoch=25
03/14/2022 00:19:25 - INFO - __main__ - Step 140 Global step 140 Train loss 8.696943 on epoch=27
03/14/2022 00:19:30 - INFO - __main__ - Step 150 Global step 150 Train loss 8.953194 on epoch=29
03/14/2022 00:19:32 - INFO - __main__ - Global step 150 Train loss 9.196756 Classification-F1 0.0 on epoch=29
03/14/2022 00:19:37 - INFO - __main__ - Step 160 Global step 160 Train loss 8.293653 on epoch=31
03/14/2022 00:19:42 - INFO - __main__ - Step 170 Global step 170 Train loss 7.826429 on epoch=33
03/14/2022 00:19:47 - INFO - __main__ - Step 180 Global step 180 Train loss 7.498055 on epoch=35
03/14/2022 00:19:52 - INFO - __main__ - Step 190 Global step 190 Train loss 7.457934 on epoch=37
03/14/2022 00:19:57 - INFO - __main__ - Step 200 Global step 200 Train loss 7.348170 on epoch=39
03/14/2022 00:20:00 - INFO - __main__ - Global step 200 Train loss 7.684848 Classification-F1 0.0 on epoch=39
03/14/2022 00:20:05 - INFO - __main__ - Step 210 Global step 210 Train loss 6.889122 on epoch=41
03/14/2022 00:20:10 - INFO - __main__ - Step 220 Global step 220 Train loss 6.728723 on epoch=43
03/14/2022 00:20:15 - INFO - __main__ - Step 230 Global step 230 Train loss 6.041728 on epoch=45
03/14/2022 00:20:20 - INFO - __main__ - Step 240 Global step 240 Train loss 6.369912 on epoch=47
03/14/2022 00:20:25 - INFO - __main__ - Step 250 Global step 250 Train loss 5.730409 on epoch=49
03/14/2022 00:20:27 - INFO - __main__ - Global step 250 Train loss 6.351978 Classification-F1 0.0 on epoch=49
03/14/2022 00:20:32 - INFO - __main__ - Step 260 Global step 260 Train loss 5.635484 on epoch=51
03/14/2022 00:20:37 - INFO - __main__ - Step 270 Global step 270 Train loss 5.346237 on epoch=53
03/14/2022 00:20:42 - INFO - __main__ - Step 280 Global step 280 Train loss 4.807912 on epoch=55
03/14/2022 00:20:47 - INFO - __main__ - Step 290 Global step 290 Train loss 4.711511 on epoch=57
03/14/2022 00:20:52 - INFO - __main__ - Step 300 Global step 300 Train loss 4.487579 on epoch=59
03/14/2022 00:20:54 - INFO - __main__ - Global step 300 Train loss 4.997745 Classification-F1 0.0 on epoch=59
03/14/2022 00:20:59 - INFO - __main__ - Step 310 Global step 310 Train loss 4.290709 on epoch=61
03/14/2022 00:21:04 - INFO - __main__ - Step 320 Global step 320 Train loss 4.013482 on epoch=63
03/14/2022 00:21:09 - INFO - __main__ - Step 330 Global step 330 Train loss 3.492945 on epoch=65
03/14/2022 00:21:14 - INFO - __main__ - Step 340 Global step 340 Train loss 3.119143 on epoch=67
03/14/2022 00:21:19 - INFO - __main__ - Step 350 Global step 350 Train loss 2.753421 on epoch=69
03/14/2022 00:21:21 - INFO - __main__ - Global step 350 Train loss 3.533940 Classification-F1 0.0 on epoch=69
03/14/2022 00:21:26 - INFO - __main__ - Step 360 Global step 360 Train loss 2.847750 on epoch=71
03/14/2022 00:21:31 - INFO - __main__ - Step 370 Global step 370 Train loss 2.828325 on epoch=73
03/14/2022 00:21:36 - INFO - __main__ - Step 380 Global step 380 Train loss 2.707642 on epoch=75
03/14/2022 00:21:41 - INFO - __main__ - Step 390 Global step 390 Train loss 2.241628 on epoch=77
03/14/2022 00:21:46 - INFO - __main__ - Step 400 Global step 400 Train loss 1.981476 on epoch=79
03/14/2022 00:21:48 - INFO - __main__ - Global step 400 Train loss 2.521364 Classification-F1 0.1489795918367347 on epoch=79
03/14/2022 00:21:54 - INFO - __main__ - Step 410 Global step 410 Train loss 2.344543 on epoch=81
03/14/2022 00:21:59 - INFO - __main__ - Step 420 Global step 420 Train loss 2.049048 on epoch=83
03/14/2022 00:22:04 - INFO - __main__ - Step 430 Global step 430 Train loss 1.408753 on epoch=85
03/14/2022 00:22:09 - INFO - __main__ - Step 440 Global step 440 Train loss 2.040836 on epoch=87
03/14/2022 00:22:14 - INFO - __main__ - Step 450 Global step 450 Train loss 1.712984 on epoch=89
03/14/2022 00:22:16 - INFO - __main__ - Global step 450 Train loss 1.911233 Classification-F1 0.2091001011122346 on epoch=89
03/14/2022 00:22:21 - INFO - __main__ - Step 460 Global step 460 Train loss 1.637348 on epoch=91
03/14/2022 00:22:26 - INFO - __main__ - Step 470 Global step 470 Train loss 1.638329 on epoch=93
03/14/2022 00:22:31 - INFO - __main__ - Step 480 Global step 480 Train loss 1.254191 on epoch=95
03/14/2022 00:22:36 - INFO - __main__ - Step 490 Global step 490 Train loss 1.623240 on epoch=97
03/14/2022 00:22:41 - INFO - __main__ - Step 500 Global step 500 Train loss 1.727183 on epoch=99
03/14/2022 00:22:43 - INFO - __main__ - Global step 500 Train loss 1.576058 Classification-F1 0.2342512908777969 on epoch=99
03/14/2022 00:22:49 - INFO - __main__ - Step 510 Global step 510 Train loss 1.574426 on epoch=101
03/14/2022 00:22:54 - INFO - __main__ - Step 520 Global step 520 Train loss 1.199266 on epoch=103
03/14/2022 00:22:59 - INFO - __main__ - Step 530 Global step 530 Train loss 1.462292 on epoch=105
03/14/2022 00:23:04 - INFO - __main__ - Step 540 Global step 540 Train loss 1.433824 on epoch=107
03/14/2022 00:23:09 - INFO - __main__ - Step 550 Global step 550 Train loss 1.447176 on epoch=109
03/14/2022 00:23:11 - INFO - __main__ - Global step 550 Train loss 1.423397 Classification-F1 0.21699248120300757 on epoch=109
03/14/2022 00:23:16 - INFO - __main__ - Step 560 Global step 560 Train loss 1.476946 on epoch=111
03/14/2022 00:23:21 - INFO - __main__ - Step 570 Global step 570 Train loss 1.541511 on epoch=113
03/14/2022 00:23:25 - INFO - __main__ - Step 580 Global step 580 Train loss 1.510253 on epoch=115
03/14/2022 00:23:30 - INFO - __main__ - Step 590 Global step 590 Train loss 1.232952 on epoch=117
03/14/2022 00:23:35 - INFO - __main__ - Step 600 Global step 600 Train loss 1.250659 on epoch=119
03/14/2022 00:23:37 - INFO - __main__ - Global step 600 Train loss 1.402464 Classification-F1 0.22937062937062938 on epoch=119
03/14/2022 00:23:42 - INFO - __main__ - Step 610 Global step 610 Train loss 1.621571 on epoch=121
03/14/2022 00:23:47 - INFO - __main__ - Step 620 Global step 620 Train loss 1.142479 on epoch=123
03/14/2022 00:23:52 - INFO - __main__ - Step 630 Global step 630 Train loss 1.017133 on epoch=125
03/14/2022 00:23:57 - INFO - __main__ - Step 640 Global step 640 Train loss 1.460503 on epoch=127
03/14/2022 00:24:02 - INFO - __main__ - Step 650 Global step 650 Train loss 1.222157 on epoch=129
03/14/2022 00:24:04 - INFO - __main__ - Global step 650 Train loss 1.292769 Classification-F1 0.21959064327485378 on epoch=129
03/14/2022 00:24:09 - INFO - __main__ - Step 660 Global step 660 Train loss 1.491773 on epoch=131
03/14/2022 00:24:14 - INFO - __main__ - Step 670 Global step 670 Train loss 1.036312 on epoch=133
03/14/2022 00:24:19 - INFO - __main__ - Step 680 Global step 680 Train loss 1.376659 on epoch=135
03/14/2022 00:24:24 - INFO - __main__ - Step 690 Global step 690 Train loss 1.027189 on epoch=137
03/14/2022 00:24:29 - INFO - __main__ - Step 700 Global step 700 Train loss 1.257186 on epoch=139
03/14/2022 00:24:31 - INFO - __main__ - Global step 700 Train loss 1.237824 Classification-F1 0.22937062937062938 on epoch=139
03/14/2022 00:24:36 - INFO - __main__ - Step 710 Global step 710 Train loss 1.323740 on epoch=141
03/14/2022 00:24:41 - INFO - __main__ - Step 720 Global step 720 Train loss 1.300522 on epoch=143
03/14/2022 00:24:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.936050 on epoch=145
03/14/2022 00:24:51 - INFO - __main__ - Step 740 Global step 740 Train loss 1.269654 on epoch=147
03/14/2022 00:24:56 - INFO - __main__ - Step 750 Global step 750 Train loss 1.080312 on epoch=149
03/14/2022 00:24:58 - INFO - __main__ - Global step 750 Train loss 1.182056 Classification-F1 0.1993025283347864 on epoch=149
03/14/2022 00:25:03 - INFO - __main__ - Step 760 Global step 760 Train loss 1.257258 on epoch=151
03/14/2022 00:25:08 - INFO - __main__ - Step 770 Global step 770 Train loss 1.144553 on epoch=153
03/14/2022 00:25:13 - INFO - __main__ - Step 780 Global step 780 Train loss 1.113504 on epoch=155
03/14/2022 00:25:18 - INFO - __main__ - Step 790 Global step 790 Train loss 1.183990 on epoch=157
03/14/2022 00:25:23 - INFO - __main__ - Step 800 Global step 800 Train loss 1.137803 on epoch=159
03/14/2022 00:25:25 - INFO - __main__ - Global step 800 Train loss 1.167422 Classification-F1 0.2036036036036036 on epoch=159
03/14/2022 00:25:30 - INFO - __main__ - Step 810 Global step 810 Train loss 1.034880 on epoch=161
03/14/2022 00:25:35 - INFO - __main__ - Step 820 Global step 820 Train loss 1.043526 on epoch=163
03/14/2022 00:25:40 - INFO - __main__ - Step 830 Global step 830 Train loss 1.254615 on epoch=165
03/14/2022 00:25:45 - INFO - __main__ - Step 840 Global step 840 Train loss 1.069038 on epoch=167
03/14/2022 00:25:50 - INFO - __main__ - Step 850 Global step 850 Train loss 1.044146 on epoch=169
03/14/2022 00:25:53 - INFO - __main__ - Global step 850 Train loss 1.089241 Classification-F1 0.2367299527357206 on epoch=169
03/14/2022 00:25:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.966634 on epoch=171
03/14/2022 00:26:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.931667 on epoch=173
03/14/2022 00:26:08 - INFO - __main__ - Step 880 Global step 880 Train loss 1.086514 on epoch=175
03/14/2022 00:26:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.820493 on epoch=177
03/14/2022 00:26:18 - INFO - __main__ - Step 900 Global step 900 Train loss 1.023782 on epoch=179
03/14/2022 00:26:20 - INFO - __main__ - Global step 900 Train loss 0.965818 Classification-F1 0.1967391304347826 on epoch=179
03/14/2022 00:26:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.960742 on epoch=181
03/14/2022 00:26:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.821691 on epoch=183
03/14/2022 00:26:35 - INFO - __main__ - Step 930 Global step 930 Train loss 1.021982 on epoch=185
03/14/2022 00:26:40 - INFO - __main__ - Step 940 Global step 940 Train loss 1.176949 on epoch=187
03/14/2022 00:26:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.714173 on epoch=189
03/14/2022 00:26:47 - INFO - __main__ - Global step 950 Train loss 0.939107 Classification-F1 0.16277791252324358 on epoch=189
03/14/2022 00:26:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.896400 on epoch=191
03/14/2022 00:26:58 - INFO - __main__ - Step 970 Global step 970 Train loss 1.074332 on epoch=193
03/14/2022 00:27:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.747963 on epoch=195
03/14/2022 00:27:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.926001 on epoch=197
03/14/2022 00:27:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.994822 on epoch=199
03/14/2022 00:27:14 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:27:14 - INFO - __main__ - Printing 3 examples
03/14/2022 00:27:14 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/14/2022 00:27:14 - INFO - __main__ - ['Yes']
03/14/2022 00:27:14 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/14/2022 00:27:14 - INFO - __main__ - ['Yes']
03/14/2022 00:27:14 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/14/2022 00:27:14 - INFO - __main__ - ['Yes']
03/14/2022 00:27:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 00:27:14 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:27:14 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 00:27:14 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:27:14 - INFO - __main__ - Printing 3 examples
03/14/2022 00:27:14 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/14/2022 00:27:14 - INFO - __main__ - ['Yes']
03/14/2022 00:27:14 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/14/2022 00:27:14 - INFO - __main__ - ['Yes']
03/14/2022 00:27:14 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/14/2022 00:27:14 - INFO - __main__ - ['Yes']
03/14/2022 00:27:14 - INFO - __main__ - Tokenizing Input ...
03/14/2022 00:27:14 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:27:14 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 00:27:15 - INFO - __main__ - Global step 1000 Train loss 0.927904 Classification-F1 0.1955500078015291 on epoch=199
03/14/2022 00:27:15 - INFO - __main__ - save last model!
03/14/2022 00:27:22 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 00:27:22 - INFO - __main__ - Start tokenizing ... 6700 instances
03/14/2022 00:27:22 - INFO - __main__ - Printing 3 examples
03/14/2022 00:27:22 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/14/2022 00:27:22 - INFO - __main__ - ['No']
03/14/2022 00:27:22 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/14/2022 00:27:22 - INFO - __main__ - ['Yes']
03/14/2022 00:27:22 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/14/2022 00:27:22 - INFO - __main__ - ['Yes']
03/14/2022 00:27:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 00:27:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 00:27:25 - INFO - __main__ - Starting training!
03/14/2022 00:27:25 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:27:32 - INFO - __main__ - Loaded 6700 examples from test data
03/14/2022 00:30:16 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_21_0.0001_8_predictions.txt
03/14/2022 00:30:16 - INFO - __main__ - Classification-F1 on test data: 0.2137
03/14/2022 00:30:16 - INFO - __main__ - prefix=circa_16_21, lr=0.0001, bsz=8, dev_performance=0.2367299527357206, test_performance=0.21368192931910943
03/14/2022 00:30:16 - INFO - __main__ - Running ... prefix=circa_16_42, lr=0.0005, bsz=8 ...
03/14/2022 00:30:17 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:30:17 - INFO - __main__ - Printing 3 examples
03/14/2022 00:30:17 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/14/2022 00:30:17 - INFO - __main__ - ['Yes']
03/14/2022 00:30:17 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/14/2022 00:30:17 - INFO - __main__ - ['Yes']
03/14/2022 00:30:17 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/14/2022 00:30:17 - INFO - __main__ - ['Yes']
03/14/2022 00:30:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 00:30:17 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:30:17 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 00:30:17 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:30:17 - INFO - __main__ - Printing 3 examples
03/14/2022 00:30:17 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/14/2022 00:30:17 - INFO - __main__ - ['Yes']
03/14/2022 00:30:17 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/14/2022 00:30:17 - INFO - __main__ - ['Yes']
03/14/2022 00:30:17 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/14/2022 00:30:17 - INFO - __main__ - ['Yes']
03/14/2022 00:30:17 - INFO - __main__ - Tokenizing Input ...
03/14/2022 00:30:17 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:30:17 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 00:30:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 00:30:29 - INFO - __main__ - Starting training!
03/14/2022 00:30:33 - INFO - __main__ - Step 10 Global step 10 Train loss 22.118013 on epoch=1
03/14/2022 00:30:38 - INFO - __main__ - Step 20 Global step 20 Train loss 14.819254 on epoch=3
03/14/2022 00:30:43 - INFO - __main__ - Step 30 Global step 30 Train loss 10.006253 on epoch=5
03/14/2022 00:30:48 - INFO - __main__ - Step 40 Global step 40 Train loss 7.665141 on epoch=7
03/14/2022 00:30:53 - INFO - __main__ - Step 50 Global step 50 Train loss 6.800570 on epoch=9
03/14/2022 00:31:24 - INFO - __main__ - Global step 50 Train loss 12.281846 Classification-F1 0.0 on epoch=9
03/14/2022 00:31:30 - INFO - __main__ - Step 60 Global step 60 Train loss 5.940843 on epoch=11
03/14/2022 00:31:35 - INFO - __main__ - Step 70 Global step 70 Train loss 4.193280 on epoch=13
03/14/2022 00:31:40 - INFO - __main__ - Step 80 Global step 80 Train loss 2.559191 on epoch=15
03/14/2022 00:31:45 - INFO - __main__ - Step 90 Global step 90 Train loss 1.820133 on epoch=17
03/14/2022 00:31:50 - INFO - __main__ - Step 100 Global step 100 Train loss 1.828189 on epoch=19
03/14/2022 00:31:52 - INFO - __main__ - Global step 100 Train loss 3.268327 Classification-F1 0.1062999680204669 on epoch=19
03/14/2022 00:31:57 - INFO - __main__ - Step 110 Global step 110 Train loss 1.530768 on epoch=21
03/14/2022 00:32:02 - INFO - __main__ - Step 120 Global step 120 Train loss 1.771646 on epoch=23
03/14/2022 00:32:07 - INFO - __main__ - Step 130 Global step 130 Train loss 1.319366 on epoch=25
03/14/2022 00:32:12 - INFO - __main__ - Step 140 Global step 140 Train loss 1.680694 on epoch=27
03/14/2022 00:32:17 - INFO - __main__ - Step 150 Global step 150 Train loss 1.406748 on epoch=29
03/14/2022 00:32:19 - INFO - __main__ - Global step 150 Train loss 1.541844 Classification-F1 0.1055632183908046 on epoch=29
03/14/2022 00:32:24 - INFO - __main__ - Step 160 Global step 160 Train loss 1.634410 on epoch=31
03/14/2022 00:32:29 - INFO - __main__ - Step 170 Global step 170 Train loss 1.311443 on epoch=33
03/14/2022 00:32:35 - INFO - __main__ - Step 180 Global step 180 Train loss 1.187353 on epoch=35
03/14/2022 00:32:40 - INFO - __main__ - Step 190 Global step 190 Train loss 1.385259 on epoch=37
03/14/2022 00:32:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.931619 on epoch=39
03/14/2022 00:32:47 - INFO - __main__ - Global step 200 Train loss 1.290017 Classification-F1 0.0761904761904762 on epoch=39
03/14/2022 00:32:52 - INFO - __main__ - Step 210 Global step 210 Train loss 1.083875 on epoch=41
03/14/2022 00:32:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.864520 on epoch=43
03/14/2022 00:33:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.846057 on epoch=45
03/14/2022 00:33:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.976430 on epoch=47
03/14/2022 00:33:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.832821 on epoch=49
03/14/2022 00:33:14 - INFO - __main__ - Global step 250 Train loss 0.920740 Classification-F1 0.10521739130434785 on epoch=49
03/14/2022 00:33:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.754928 on epoch=51
03/14/2022 00:33:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.779820 on epoch=53
03/14/2022 00:33:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.590970 on epoch=55
03/14/2022 00:33:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.684592 on epoch=57
03/14/2022 00:33:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.657063 on epoch=59
03/14/2022 00:33:40 - INFO - __main__ - Global step 300 Train loss 0.693475 Classification-F1 0.08521739130434783 on epoch=59
03/14/2022 00:33:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.521156 on epoch=61
03/14/2022 00:33:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.521864 on epoch=63
03/14/2022 00:33:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.958228 on epoch=65
03/14/2022 00:34:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.481711 on epoch=67
03/14/2022 00:34:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.577741 on epoch=69
03/14/2022 00:34:07 - INFO - __main__ - Global step 350 Train loss 0.612140 Classification-F1 0.06808510638297872 on epoch=69
03/14/2022 00:34:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.538523 on epoch=71
03/14/2022 00:34:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.531526 on epoch=73
03/14/2022 00:34:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.481449 on epoch=75
03/14/2022 00:34:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.441895 on epoch=77
03/14/2022 00:34:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.480070 on epoch=79
03/14/2022 00:34:33 - INFO - __main__ - Global step 400 Train loss 0.494693 Classification-F1 0.1415321195465476 on epoch=79
03/14/2022 00:34:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.491527 on epoch=81
03/14/2022 00:34:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.463318 on epoch=83
03/14/2022 00:34:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.432248 on epoch=85
03/14/2022 00:34:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.472930 on epoch=87
03/14/2022 00:34:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.438264 on epoch=89
03/14/2022 00:35:00 - INFO - __main__ - Global step 450 Train loss 0.459657 Classification-F1 0.06666666666666668 on epoch=89
03/14/2022 00:35:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.405020 on epoch=91
03/14/2022 00:35:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.453761 on epoch=93
03/14/2022 00:35:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.396289 on epoch=95
03/14/2022 00:35:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.382339 on epoch=97
03/14/2022 00:35:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.417358 on epoch=99
03/14/2022 00:35:27 - INFO - __main__ - Global step 500 Train loss 0.410953 Classification-F1 0.07058823529411765 on epoch=99
03/14/2022 00:35:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.380708 on epoch=101
03/14/2022 00:35:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.391672 on epoch=103
03/14/2022 00:35:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.404703 on epoch=105
03/14/2022 00:35:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.329388 on epoch=107
03/14/2022 00:35:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.537202 on epoch=109
03/14/2022 00:35:54 - INFO - __main__ - Global step 550 Train loss 0.408735 Classification-F1 0.13763975155279504 on epoch=109
03/14/2022 00:35:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.310402 on epoch=111
03/14/2022 00:36:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.337555 on epoch=113
03/14/2022 00:36:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.255831 on epoch=115
03/14/2022 00:36:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.267042 on epoch=117
03/14/2022 00:36:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.269141 on epoch=119
03/14/2022 00:36:21 - INFO - __main__ - Global step 600 Train loss 0.287994 Classification-F1 0.1618376068376068 on epoch=119
03/14/2022 00:36:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.320280 on epoch=121
03/14/2022 00:36:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.359592 on epoch=123
03/14/2022 00:36:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.223068 on epoch=125
03/14/2022 00:36:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.210590 on epoch=127
03/14/2022 00:36:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.229430 on epoch=129
03/14/2022 00:36:48 - INFO - __main__ - Global step 650 Train loss 0.268592 Classification-F1 0.20961352657004834 on epoch=129
03/14/2022 00:36:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.149427 on epoch=131
03/14/2022 00:36:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.079619 on epoch=133
03/14/2022 00:37:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.128563 on epoch=135
03/14/2022 00:37:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.067676 on epoch=137
03/14/2022 00:37:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.068855 on epoch=139
03/14/2022 00:37:16 - INFO - __main__ - Global step 700 Train loss 0.098828 Classification-F1 0.16675716440422322 on epoch=139
03/14/2022 00:37:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.048013 on epoch=141
03/14/2022 00:37:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.016685 on epoch=143
03/14/2022 00:37:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.033877 on epoch=145
03/14/2022 00:37:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.077090 on epoch=147
03/14/2022 00:37:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.018175 on epoch=149
03/14/2022 00:37:44 - INFO - __main__ - Global step 750 Train loss 0.038768 Classification-F1 0.136118024922579 on epoch=149
03/14/2022 00:37:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.016084 on epoch=151
03/14/2022 00:37:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.008473 on epoch=153
03/14/2022 00:37:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.010681 on epoch=155
03/14/2022 00:38:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.022127 on epoch=157
03/14/2022 00:38:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.015338 on epoch=159
03/14/2022 00:38:11 - INFO - __main__ - Global step 800 Train loss 0.014541 Classification-F1 0.10584371679706366 on epoch=159
03/14/2022 00:38:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.009994 on epoch=161
03/14/2022 00:38:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.014343 on epoch=163
03/14/2022 00:38:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.003015 on epoch=165
03/14/2022 00:38:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.004915 on epoch=167
03/14/2022 00:38:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001003 on epoch=169
03/14/2022 00:38:38 - INFO - __main__ - Global step 850 Train loss 0.006654 Classification-F1 0.10666499666874761 on epoch=169
03/14/2022 00:38:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000748 on epoch=171
03/14/2022 00:38:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.006379 on epoch=173
03/14/2022 00:38:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001542 on epoch=175
03/14/2022 00:38:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002427 on epoch=177
03/14/2022 00:39:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001653 on epoch=179
03/14/2022 00:39:06 - INFO - __main__ - Global step 900 Train loss 0.002550 Classification-F1 0.13177772582027902 on epoch=179
03/14/2022 00:39:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.008458 on epoch=181
03/14/2022 00:39:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.002782 on epoch=183
03/14/2022 00:39:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000703 on epoch=185
03/14/2022 00:39:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.002830 on epoch=187
03/14/2022 00:39:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001353 on epoch=189
03/14/2022 00:39:33 - INFO - __main__ - Global step 950 Train loss 0.003225 Classification-F1 0.10603174603174605 on epoch=189
03/14/2022 00:39:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.010741 on epoch=191
03/14/2022 00:39:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.002183 on epoch=193
03/14/2022 00:39:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000652 on epoch=195
03/14/2022 00:39:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.004802 on epoch=197
03/14/2022 00:39:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.022528 on epoch=199
03/14/2022 00:39:59 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:39:59 - INFO - __main__ - Printing 3 examples
03/14/2022 00:39:59 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/14/2022 00:39:59 - INFO - __main__ - ['Yes']
03/14/2022 00:39:59 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/14/2022 00:39:59 - INFO - __main__ - ['Yes']
03/14/2022 00:39:59 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/14/2022 00:39:59 - INFO - __main__ - ['Yes']
03/14/2022 00:39:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 00:39:59 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:39:59 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 00:39:59 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:39:59 - INFO - __main__ - Printing 3 examples
03/14/2022 00:39:59 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/14/2022 00:39:59 - INFO - __main__ - ['Yes']
03/14/2022 00:39:59 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/14/2022 00:39:59 - INFO - __main__ - ['Yes']
03/14/2022 00:39:59 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/14/2022 00:39:59 - INFO - __main__ - ['Yes']
03/14/2022 00:39:59 - INFO - __main__ - Tokenizing Input ...
03/14/2022 00:39:59 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:39:59 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 00:40:00 - INFO - __main__ - Global step 1000 Train loss 0.008181 Classification-F1 0.15986545052758974 on epoch=199
03/14/2022 00:40:00 - INFO - __main__ - save last model!
03/14/2022 00:40:07 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 00:40:08 - INFO - __main__ - Start tokenizing ... 6700 instances
03/14/2022 00:40:08 - INFO - __main__ - Printing 3 examples
03/14/2022 00:40:08 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/14/2022 00:40:08 - INFO - __main__ - ['No']
03/14/2022 00:40:08 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/14/2022 00:40:08 - INFO - __main__ - ['Yes']
03/14/2022 00:40:08 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/14/2022 00:40:08 - INFO - __main__ - ['Yes']
03/14/2022 00:40:08 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 00:40:11 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:40:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 00:40:12 - INFO - __main__ - Starting training!
03/14/2022 00:40:17 - INFO - __main__ - Loaded 6700 examples from test data
03/14/2022 00:42:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_42_0.0005_8_predictions.txt
03/14/2022 00:42:33 - INFO - __main__ - Classification-F1 on test data: 0.1325
03/14/2022 00:42:34 - INFO - __main__ - prefix=circa_16_42, lr=0.0005, bsz=8, dev_performance=0.20961352657004834, test_performance=0.13253860775079837
03/14/2022 00:42:34 - INFO - __main__ - Running ... prefix=circa_16_42, lr=0.0003, bsz=8 ...
03/14/2022 00:42:35 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:42:35 - INFO - __main__ - Printing 3 examples
03/14/2022 00:42:35 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/14/2022 00:42:35 - INFO - __main__ - ['Yes']
03/14/2022 00:42:35 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/14/2022 00:42:35 - INFO - __main__ - ['Yes']
03/14/2022 00:42:35 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/14/2022 00:42:35 - INFO - __main__ - ['Yes']
03/14/2022 00:42:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 00:42:35 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:42:35 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 00:42:35 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:42:35 - INFO - __main__ - Printing 3 examples
03/14/2022 00:42:35 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/14/2022 00:42:35 - INFO - __main__ - ['Yes']
03/14/2022 00:42:35 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/14/2022 00:42:35 - INFO - __main__ - ['Yes']
03/14/2022 00:42:35 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/14/2022 00:42:35 - INFO - __main__ - ['Yes']
03/14/2022 00:42:35 - INFO - __main__ - Tokenizing Input ...
03/14/2022 00:42:35 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:42:35 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 00:42:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 00:42:47 - INFO - __main__ - Starting training!
03/14/2022 00:42:54 - INFO - __main__ - Step 10 Global step 10 Train loss 22.119038 on epoch=1
03/14/2022 00:42:59 - INFO - __main__ - Step 20 Global step 20 Train loss 19.147615 on epoch=3
03/14/2022 00:43:03 - INFO - __main__ - Step 30 Global step 30 Train loss 13.142879 on epoch=5
03/14/2022 00:43:08 - INFO - __main__ - Step 40 Global step 40 Train loss 11.364893 on epoch=7
03/14/2022 00:43:13 - INFO - __main__ - Step 50 Global step 50 Train loss 9.782454 on epoch=9
03/14/2022 00:43:17 - INFO - __main__ - Global step 50 Train loss 15.111376 Classification-F1 0.0 on epoch=9
03/14/2022 00:43:23 - INFO - __main__ - Step 60 Global step 60 Train loss 7.846972 on epoch=11
03/14/2022 00:43:28 - INFO - __main__ - Step 70 Global step 70 Train loss 7.066973 on epoch=13
03/14/2022 00:43:33 - INFO - __main__ - Step 80 Global step 80 Train loss 6.481506 on epoch=15
03/14/2022 00:43:38 - INFO - __main__ - Step 90 Global step 90 Train loss 5.769687 on epoch=17
03/14/2022 00:43:43 - INFO - __main__ - Step 100 Global step 100 Train loss 5.284818 on epoch=19
03/14/2022 00:43:45 - INFO - __main__ - Global step 100 Train loss 6.489992 Classification-F1 0.0 on epoch=19
03/14/2022 00:43:50 - INFO - __main__ - Step 110 Global step 110 Train loss 4.858901 on epoch=21
03/14/2022 00:43:55 - INFO - __main__ - Step 120 Global step 120 Train loss 4.131063 on epoch=23
03/14/2022 00:44:00 - INFO - __main__ - Step 130 Global step 130 Train loss 3.082386 on epoch=25
03/14/2022 00:44:05 - INFO - __main__ - Step 140 Global step 140 Train loss 2.465070 on epoch=27
03/14/2022 00:44:10 - INFO - __main__ - Step 150 Global step 150 Train loss 1.578662 on epoch=29
03/14/2022 00:44:12 - INFO - __main__ - Global step 150 Train loss 3.223217 Classification-F1 0.06666666666666668 on epoch=29
03/14/2022 00:44:17 - INFO - __main__ - Step 160 Global step 160 Train loss 1.722132 on epoch=31
03/14/2022 00:44:22 - INFO - __main__ - Step 170 Global step 170 Train loss 1.759547 on epoch=33
03/14/2022 00:44:27 - INFO - __main__ - Step 180 Global step 180 Train loss 1.613885 on epoch=35
03/14/2022 00:44:32 - INFO - __main__ - Step 190 Global step 190 Train loss 1.773302 on epoch=37
03/14/2022 00:44:37 - INFO - __main__ - Step 200 Global step 200 Train loss 1.397277 on epoch=39
03/14/2022 00:44:39 - INFO - __main__ - Global step 200 Train loss 1.653229 Classification-F1 0.06666666666666668 on epoch=39
03/14/2022 00:44:44 - INFO - __main__ - Step 210 Global step 210 Train loss 1.316235 on epoch=41
03/14/2022 00:44:49 - INFO - __main__ - Step 220 Global step 220 Train loss 1.316040 on epoch=43
03/14/2022 00:44:54 - INFO - __main__ - Step 230 Global step 230 Train loss 1.330705 on epoch=45
03/14/2022 00:44:59 - INFO - __main__ - Step 240 Global step 240 Train loss 1.529277 on epoch=47
03/14/2022 00:45:04 - INFO - __main__ - Step 250 Global step 250 Train loss 1.279218 on epoch=49
03/14/2022 00:45:06 - INFO - __main__ - Global step 250 Train loss 1.354295 Classification-F1 0.06666666666666668 on epoch=49
03/14/2022 00:45:11 - INFO - __main__ - Step 260 Global step 260 Train loss 1.385626 on epoch=51
03/14/2022 00:45:16 - INFO - __main__ - Step 270 Global step 270 Train loss 1.325842 on epoch=53
03/14/2022 00:45:21 - INFO - __main__ - Step 280 Global step 280 Train loss 1.437949 on epoch=55
03/14/2022 00:45:26 - INFO - __main__ - Step 290 Global step 290 Train loss 1.243997 on epoch=57
03/14/2022 00:45:31 - INFO - __main__ - Step 300 Global step 300 Train loss 1.329894 on epoch=59
03/14/2022 00:45:33 - INFO - __main__ - Global step 300 Train loss 1.344662 Classification-F1 0.06666666666666668 on epoch=59
03/14/2022 00:45:38 - INFO - __main__ - Step 310 Global step 310 Train loss 1.098292 on epoch=61
03/14/2022 00:45:43 - INFO - __main__ - Step 320 Global step 320 Train loss 1.229006 on epoch=63
03/14/2022 00:45:48 - INFO - __main__ - Step 330 Global step 330 Train loss 1.037986 on epoch=65
03/14/2022 00:45:53 - INFO - __main__ - Step 340 Global step 340 Train loss 1.245429 on epoch=67
03/14/2022 00:45:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.764208 on epoch=69
03/14/2022 00:46:00 - INFO - __main__ - Global step 350 Train loss 1.074984 Classification-F1 0.06736842105263158 on epoch=69
03/14/2022 00:46:06 - INFO - __main__ - Step 360 Global step 360 Train loss 1.037183 on epoch=71
03/14/2022 00:46:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.918960 on epoch=73
03/14/2022 00:46:16 - INFO - __main__ - Step 380 Global step 380 Train loss 1.086077 on epoch=75
03/14/2022 00:46:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.818882 on epoch=77
03/14/2022 00:46:26 - INFO - __main__ - Step 400 Global step 400 Train loss 1.096498 on epoch=79
03/14/2022 00:46:28 - INFO - __main__ - Global step 400 Train loss 0.991520 Classification-F1 0.06666666666666668 on epoch=79
03/14/2022 00:46:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.787529 on epoch=81
03/14/2022 00:46:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.800676 on epoch=83
03/14/2022 00:46:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.844444 on epoch=85
03/14/2022 00:46:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.846632 on epoch=87
03/14/2022 00:46:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.732183 on epoch=89
03/14/2022 00:46:55 - INFO - __main__ - Global step 450 Train loss 0.802293 Classification-F1 0.1025 on epoch=89
03/14/2022 00:47:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.607737 on epoch=91
03/14/2022 00:47:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.653907 on epoch=93
03/14/2022 00:47:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.670085 on epoch=95
03/14/2022 00:47:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.553864 on epoch=97
03/14/2022 00:47:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.663694 on epoch=99
03/14/2022 00:47:23 - INFO - __main__ - Global step 500 Train loss 0.629857 Classification-F1 0.05128205128205128 on epoch=99
03/14/2022 00:47:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.671905 on epoch=101
03/14/2022 00:47:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.696337 on epoch=103
03/14/2022 00:47:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.570474 on epoch=105
03/14/2022 00:47:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.558426 on epoch=107
03/14/2022 00:47:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.569084 on epoch=109
03/14/2022 00:47:50 - INFO - __main__ - Global step 550 Train loss 0.613245 Classification-F1 0.10176576576576575 on epoch=109
03/14/2022 00:47:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.534231 on epoch=111
03/14/2022 00:48:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.547063 on epoch=113
03/14/2022 00:48:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.523464 on epoch=115
03/14/2022 00:48:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.499085 on epoch=117
03/14/2022 00:48:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.491299 on epoch=119
03/14/2022 00:48:17 - INFO - __main__ - Global step 600 Train loss 0.519028 Classification-F1 0.12717948717948718 on epoch=119
03/14/2022 00:48:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.493498 on epoch=121
03/14/2022 00:48:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.485323 on epoch=123
03/14/2022 00:48:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.492293 on epoch=125
03/14/2022 00:48:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.544427 on epoch=127
03/14/2022 00:48:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.478370 on epoch=129
03/14/2022 00:48:45 - INFO - __main__ - Global step 650 Train loss 0.498782 Classification-F1 0.06086956521739131 on epoch=129
03/14/2022 00:48:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.464070 on epoch=131
03/14/2022 00:48:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.460282 on epoch=133
03/14/2022 00:49:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.519878 on epoch=135
03/14/2022 00:49:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.454621 on epoch=137
03/14/2022 00:49:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.477322 on epoch=139
03/14/2022 00:49:12 - INFO - __main__ - Global step 700 Train loss 0.475235 Classification-F1 0.06666666666666668 on epoch=139
03/14/2022 00:49:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.491909 on epoch=141
03/14/2022 00:49:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.545056 on epoch=143
03/14/2022 00:49:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.463239 on epoch=145
03/14/2022 00:49:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.452731 on epoch=147
03/14/2022 00:49:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.437106 on epoch=149
03/14/2022 00:49:39 - INFO - __main__ - Global step 750 Train loss 0.478009 Classification-F1 0.1055632183908046 on epoch=149
03/14/2022 00:49:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.461769 on epoch=151
03/14/2022 00:49:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.498551 on epoch=153
03/14/2022 00:49:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.455161 on epoch=155
03/14/2022 00:49:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.464436 on epoch=157
03/14/2022 00:50:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.480437 on epoch=159
03/14/2022 00:50:06 - INFO - __main__ - Global step 800 Train loss 0.472071 Classification-F1 0.09123434704830055 on epoch=159
03/14/2022 00:50:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.457590 on epoch=161
03/14/2022 00:50:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.438161 on epoch=163
03/14/2022 00:50:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.433864 on epoch=165
03/14/2022 00:50:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.406743 on epoch=167
03/14/2022 00:50:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.477397 on epoch=169
03/14/2022 00:50:32 - INFO - __main__ - Global step 850 Train loss 0.442751 Classification-F1 0.06666666666666668 on epoch=169
03/14/2022 00:50:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.407755 on epoch=171
03/14/2022 00:50:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.433296 on epoch=173
03/14/2022 00:50:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.424376 on epoch=175
03/14/2022 00:50:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.441314 on epoch=177
03/14/2022 00:50:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.414038 on epoch=179
03/14/2022 00:50:59 - INFO - __main__ - Global step 900 Train loss 0.424156 Classification-F1 0.13999999999999999 on epoch=179
03/14/2022 00:51:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.414961 on epoch=181
03/14/2022 00:51:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.441352 on epoch=183
03/14/2022 00:51:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.459042 on epoch=185
03/14/2022 00:51:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.457275 on epoch=187
03/14/2022 00:51:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.449728 on epoch=189
03/14/2022 00:51:26 - INFO - __main__ - Global step 950 Train loss 0.444472 Classification-F1 0.09444444444444444 on epoch=189
03/14/2022 00:51:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.445178 on epoch=191
03/14/2022 00:51:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.465805 on epoch=193
03/14/2022 00:51:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.392025 on epoch=195
03/14/2022 00:51:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.457491 on epoch=197
03/14/2022 00:51:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.414966 on epoch=199
03/14/2022 00:51:53 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:51:53 - INFO - __main__ - Printing 3 examples
03/14/2022 00:51:53 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/14/2022 00:51:53 - INFO - __main__ - ['Yes']
03/14/2022 00:51:53 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/14/2022 00:51:53 - INFO - __main__ - ['Yes']
03/14/2022 00:51:53 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/14/2022 00:51:53 - INFO - __main__ - ['Yes']
03/14/2022 00:51:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 00:51:53 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:51:53 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 00:51:53 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:51:53 - INFO - __main__ - Printing 3 examples
03/14/2022 00:51:53 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/14/2022 00:51:53 - INFO - __main__ - ['Yes']
03/14/2022 00:51:53 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/14/2022 00:51:53 - INFO - __main__ - ['Yes']
03/14/2022 00:51:53 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/14/2022 00:51:53 - INFO - __main__ - ['Yes']
03/14/2022 00:51:53 - INFO - __main__ - Tokenizing Input ...
03/14/2022 00:51:53 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:51:53 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 00:51:54 - INFO - __main__ - Global step 1000 Train loss 0.435093 Classification-F1 0.11047619047619046 on epoch=199
03/14/2022 00:51:54 - INFO - __main__ - save last model!
03/14/2022 00:52:01 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 00:52:01 - INFO - __main__ - Start tokenizing ... 6700 instances
03/14/2022 00:52:01 - INFO - __main__ - Printing 3 examples
03/14/2022 00:52:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/14/2022 00:52:01 - INFO - __main__ - ['No']
03/14/2022 00:52:01 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/14/2022 00:52:01 - INFO - __main__ - ['Yes']
03/14/2022 00:52:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/14/2022 00:52:01 - INFO - __main__ - ['Yes']
03/14/2022 00:52:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 00:52:04 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:52:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 00:52:06 - INFO - __main__ - Starting training!
03/14/2022 00:52:11 - INFO - __main__ - Loaded 6700 examples from test data
03/14/2022 00:54:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_42_0.0003_8_predictions.txt
03/14/2022 00:54:34 - INFO - __main__ - Classification-F1 on test data: 0.0725
03/14/2022 00:54:34 - INFO - __main__ - prefix=circa_16_42, lr=0.0003, bsz=8, dev_performance=0.13999999999999999, test_performance=0.07246660433777788
03/14/2022 00:54:34 - INFO - __main__ - Running ... prefix=circa_16_42, lr=0.0002, bsz=8 ...
03/14/2022 00:54:35 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:54:35 - INFO - __main__ - Printing 3 examples
03/14/2022 00:54:35 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/14/2022 00:54:35 - INFO - __main__ - ['Yes']
03/14/2022 00:54:35 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/14/2022 00:54:35 - INFO - __main__ - ['Yes']
03/14/2022 00:54:35 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/14/2022 00:54:35 - INFO - __main__ - ['Yes']
03/14/2022 00:54:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 00:54:35 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:54:35 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 00:54:35 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 00:54:35 - INFO - __main__ - Printing 3 examples
03/14/2022 00:54:35 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/14/2022 00:54:35 - INFO - __main__ - ['Yes']
03/14/2022 00:54:35 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/14/2022 00:54:35 - INFO - __main__ - ['Yes']
03/14/2022 00:54:35 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/14/2022 00:54:35 - INFO - __main__ - ['Yes']
03/14/2022 00:54:35 - INFO - __main__ - Tokenizing Input ...
03/14/2022 00:54:35 - INFO - __main__ - Tokenizing Output ...
03/14/2022 00:54:35 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 00:54:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 00:54:47 - INFO - __main__ - Starting training!
03/14/2022 00:54:53 - INFO - __main__ - Step 10 Global step 10 Train loss 22.402126 on epoch=1
03/14/2022 00:54:58 - INFO - __main__ - Step 20 Global step 20 Train loss 21.609142 on epoch=3
03/14/2022 00:55:03 - INFO - __main__ - Step 30 Global step 30 Train loss 15.511531 on epoch=5
03/14/2022 00:55:08 - INFO - __main__ - Step 40 Global step 40 Train loss 12.213833 on epoch=7
03/14/2022 00:55:13 - INFO - __main__ - Step 50 Global step 50 Train loss 10.799878 on epoch=9
03/14/2022 00:55:14 - INFO - __main__ - Global step 50 Train loss 16.507301 Classification-F1 0.0 on epoch=9
03/14/2022 00:55:20 - INFO - __main__ - Step 60 Global step 60 Train loss 9.752672 on epoch=11
03/14/2022 00:55:25 - INFO - __main__ - Step 70 Global step 70 Train loss 9.119184 on epoch=13
03/14/2022 00:55:30 - INFO - __main__ - Step 80 Global step 80 Train loss 8.476545 on epoch=15
03/14/2022 00:55:35 - INFO - __main__ - Step 90 Global step 90 Train loss 8.427515 on epoch=17
03/14/2022 00:55:40 - INFO - __main__ - Step 100 Global step 100 Train loss 7.557719 on epoch=19
03/14/2022 00:55:41 - INFO - __main__ - Global step 100 Train loss 8.666727 Classification-F1 0.0 on epoch=19
03/14/2022 00:55:46 - INFO - __main__ - Step 110 Global step 110 Train loss 7.439447 on epoch=21
03/14/2022 00:55:51 - INFO - __main__ - Step 120 Global step 120 Train loss 6.352874 on epoch=23
03/14/2022 00:55:56 - INFO - __main__ - Step 130 Global step 130 Train loss 6.073408 on epoch=25
03/14/2022 00:56:01 - INFO - __main__ - Step 140 Global step 140 Train loss 5.372757 on epoch=27
03/14/2022 00:56:06 - INFO - __main__ - Step 150 Global step 150 Train loss 4.620029 on epoch=29
03/14/2022 00:56:08 - INFO - __main__ - Global step 150 Train loss 5.971703 Classification-F1 0.0 on epoch=29
03/14/2022 00:56:13 - INFO - __main__ - Step 160 Global step 160 Train loss 4.280396 on epoch=31
03/14/2022 00:56:18 - INFO - __main__ - Step 170 Global step 170 Train loss 3.311606 on epoch=33
03/14/2022 00:56:23 - INFO - __main__ - Step 180 Global step 180 Train loss 3.003012 on epoch=35
03/14/2022 00:56:28 - INFO - __main__ - Step 190 Global step 190 Train loss 2.450720 on epoch=37
03/14/2022 00:56:33 - INFO - __main__ - Step 200 Global step 200 Train loss 1.987002 on epoch=39
03/14/2022 00:56:35 - INFO - __main__ - Global step 200 Train loss 3.006547 Classification-F1 0.23272727272727273 on epoch=39
03/14/2022 00:56:40 - INFO - __main__ - Step 210 Global step 210 Train loss 1.782573 on epoch=41
03/14/2022 00:56:45 - INFO - __main__ - Step 220 Global step 220 Train loss 1.621503 on epoch=43
03/14/2022 00:56:50 - INFO - __main__ - Step 230 Global step 230 Train loss 2.011875 on epoch=45
03/14/2022 00:56:55 - INFO - __main__ - Step 240 Global step 240 Train loss 1.673488 on epoch=47
03/14/2022 00:57:00 - INFO - __main__ - Step 250 Global step 250 Train loss 1.695147 on epoch=49
03/14/2022 00:57:02 - INFO - __main__ - Global step 250 Train loss 1.756917 Classification-F1 0.14383838383838382 on epoch=49
03/14/2022 00:57:07 - INFO - __main__ - Step 260 Global step 260 Train loss 2.108696 on epoch=51
03/14/2022 00:57:12 - INFO - __main__ - Step 270 Global step 270 Train loss 1.613105 on epoch=53
03/14/2022 00:57:17 - INFO - __main__ - Step 280 Global step 280 Train loss 1.415600 on epoch=55
03/14/2022 00:57:22 - INFO - __main__ - Step 290 Global step 290 Train loss 1.481255 on epoch=57
03/14/2022 00:57:27 - INFO - __main__ - Step 300 Global step 300 Train loss 1.255323 on epoch=59
03/14/2022 00:57:29 - INFO - __main__ - Global step 300 Train loss 1.574796 Classification-F1 0.2456463367855773 on epoch=59
03/14/2022 00:57:34 - INFO - __main__ - Step 310 Global step 310 Train loss 1.218555 on epoch=61
03/14/2022 00:57:40 - INFO - __main__ - Step 320 Global step 320 Train loss 1.660556 on epoch=63
03/14/2022 00:57:45 - INFO - __main__ - Step 330 Global step 330 Train loss 1.534493 on epoch=65
03/14/2022 00:57:50 - INFO - __main__ - Step 340 Global step 340 Train loss 1.469738 on epoch=67
03/14/2022 00:57:54 - INFO - __main__ - Step 350 Global step 350 Train loss 1.523667 on epoch=69
03/14/2022 00:57:56 - INFO - __main__ - Global step 350 Train loss 1.481402 Classification-F1 0.2533333333333333 on epoch=69
03/14/2022 00:58:02 - INFO - __main__ - Step 360 Global step 360 Train loss 1.480135 on epoch=71
03/14/2022 00:58:07 - INFO - __main__ - Step 370 Global step 370 Train loss 1.338824 on epoch=73
03/14/2022 00:58:12 - INFO - __main__ - Step 380 Global step 380 Train loss 1.077989 on epoch=75
03/14/2022 00:58:17 - INFO - __main__ - Step 390 Global step 390 Train loss 1.091666 on epoch=77
03/14/2022 00:58:22 - INFO - __main__ - Step 400 Global step 400 Train loss 1.038933 on epoch=79
03/14/2022 00:58:24 - INFO - __main__ - Global step 400 Train loss 1.205509 Classification-F1 0.255 on epoch=79
03/14/2022 00:58:29 - INFO - __main__ - Step 410 Global step 410 Train loss 1.124243 on epoch=81
03/14/2022 00:58:34 - INFO - __main__ - Step 420 Global step 420 Train loss 1.315060 on epoch=83
03/14/2022 00:58:39 - INFO - __main__ - Step 430 Global step 430 Train loss 1.079738 on epoch=85
03/14/2022 00:58:44 - INFO - __main__ - Step 440 Global step 440 Train loss 1.018747 on epoch=87
03/14/2022 00:58:49 - INFO - __main__ - Step 450 Global step 450 Train loss 1.114570 on epoch=89
03/14/2022 00:58:51 - INFO - __main__ - Global step 450 Train loss 1.130471 Classification-F1 0.255 on epoch=89
03/14/2022 00:58:56 - INFO - __main__ - Step 460 Global step 460 Train loss 1.136227 on epoch=91
03/14/2022 00:59:01 - INFO - __main__ - Step 470 Global step 470 Train loss 1.135830 on epoch=93
03/14/2022 00:59:06 - INFO - __main__ - Step 480 Global step 480 Train loss 1.113179 on epoch=95
03/14/2022 00:59:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.944469 on epoch=97
03/14/2022 00:59:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.881946 on epoch=99
03/14/2022 00:59:18 - INFO - __main__ - Global step 500 Train loss 1.042330 Classification-F1 0.255 on epoch=99
03/14/2022 00:59:23 - INFO - __main__ - Step 510 Global step 510 Train loss 1.040149 on epoch=101
03/14/2022 00:59:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.850069 on epoch=103
03/14/2022 00:59:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.729028 on epoch=105
03/14/2022 00:59:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.921643 on epoch=107
03/14/2022 00:59:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.662602 on epoch=109
03/14/2022 00:59:45 - INFO - __main__ - Global step 550 Train loss 0.840698 Classification-F1 0.2456140350877193 on epoch=109
03/14/2022 00:59:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.851396 on epoch=111
03/14/2022 00:59:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.897518 on epoch=113
03/14/2022 01:00:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.715464 on epoch=115
03/14/2022 01:00:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.668667 on epoch=117
03/14/2022 01:00:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.644385 on epoch=119
03/14/2022 01:00:12 - INFO - __main__ - Global step 600 Train loss 0.755486 Classification-F1 0.24774193548387097 on epoch=119
03/14/2022 01:00:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.802242 on epoch=121
03/14/2022 01:00:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.887823 on epoch=123
03/14/2022 01:00:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.620034 on epoch=125
03/14/2022 01:00:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.759256 on epoch=127
03/14/2022 01:00:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.615446 on epoch=129
03/14/2022 01:00:39 - INFO - __main__ - Global step 650 Train loss 0.736960 Classification-F1 0.2384493670886076 on epoch=129
03/14/2022 01:00:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.568711 on epoch=131
03/14/2022 01:00:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.699096 on epoch=133
03/14/2022 01:00:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.505672 on epoch=135
03/14/2022 01:00:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.577385 on epoch=137
03/14/2022 01:01:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.517787 on epoch=139
03/14/2022 01:01:06 - INFO - __main__ - Global step 700 Train loss 0.573730 Classification-F1 0.24662004662004663 on epoch=139
03/14/2022 01:01:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.547753 on epoch=141
03/14/2022 01:01:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.640291 on epoch=143
03/14/2022 01:01:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.472851 on epoch=145
03/14/2022 01:01:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.422040 on epoch=147
03/14/2022 01:01:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.559781 on epoch=149
03/14/2022 01:01:33 - INFO - __main__ - Global step 750 Train loss 0.528543 Classification-F1 0.23743315508021393 on epoch=149
03/14/2022 01:01:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.542769 on epoch=151
03/14/2022 01:01:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.494694 on epoch=153
03/14/2022 01:01:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.468467 on epoch=155
03/14/2022 01:01:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.431919 on epoch=157
03/14/2022 01:01:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.492165 on epoch=159
03/14/2022 01:02:00 - INFO - __main__ - Global step 800 Train loss 0.486003 Classification-F1 0.28585787751285646 on epoch=159
03/14/2022 01:02:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.428370 on epoch=161
03/14/2022 01:02:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.376860 on epoch=163
03/14/2022 01:02:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.436615 on epoch=165
03/14/2022 01:02:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.419126 on epoch=167
03/14/2022 01:02:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.431665 on epoch=169
03/14/2022 01:02:28 - INFO - __main__ - Global step 850 Train loss 0.418527 Classification-F1 0.25533662332048557 on epoch=169
03/14/2022 01:02:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.389007 on epoch=171
03/14/2022 01:02:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.393151 on epoch=173
03/14/2022 01:02:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.368645 on epoch=175
03/14/2022 01:02:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.414344 on epoch=177
03/14/2022 01:02:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.335108 on epoch=179
03/14/2022 01:02:56 - INFO - __main__ - Global step 900 Train loss 0.380051 Classification-F1 0.3472258820084907 on epoch=179
03/14/2022 01:03:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.361319 on epoch=181
03/14/2022 01:03:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.365991 on epoch=183
03/14/2022 01:03:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.373466 on epoch=185
03/14/2022 01:03:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.356530 on epoch=187
03/14/2022 01:03:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.323213 on epoch=189
03/14/2022 01:03:24 - INFO - __main__ - Global step 950 Train loss 0.356104 Classification-F1 0.40623438687954816 on epoch=189
03/14/2022 01:03:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.365666 on epoch=191
03/14/2022 01:03:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.316753 on epoch=193
03/14/2022 01:03:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.350829 on epoch=195
03/14/2022 01:03:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.335882 on epoch=197
03/14/2022 01:03:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.276126 on epoch=199
03/14/2022 01:03:51 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:03:51 - INFO - __main__ - Printing 3 examples
03/14/2022 01:03:51 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/14/2022 01:03:51 - INFO - __main__ - ['Yes']
03/14/2022 01:03:51 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/14/2022 01:03:51 - INFO - __main__ - ['Yes']
03/14/2022 01:03:51 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/14/2022 01:03:51 - INFO - __main__ - ['Yes']
03/14/2022 01:03:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 01:03:51 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:03:51 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 01:03:51 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:03:51 - INFO - __main__ - Printing 3 examples
03/14/2022 01:03:51 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/14/2022 01:03:51 - INFO - __main__ - ['Yes']
03/14/2022 01:03:51 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/14/2022 01:03:51 - INFO - __main__ - ['Yes']
03/14/2022 01:03:51 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/14/2022 01:03:51 - INFO - __main__ - ['Yes']
03/14/2022 01:03:51 - INFO - __main__ - Tokenizing Input ...
03/14/2022 01:03:51 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:03:52 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 01:03:52 - INFO - __main__ - Global step 1000 Train loss 0.329051 Classification-F1 0.3951612903225807 on epoch=199
03/14/2022 01:03:52 - INFO - __main__ - save last model!
03/14/2022 01:03:59 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 01:04:00 - INFO - __main__ - Start tokenizing ... 6700 instances
03/14/2022 01:04:00 - INFO - __main__ - Printing 3 examples
03/14/2022 01:04:00 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/14/2022 01:04:00 - INFO - __main__ - ['No']
03/14/2022 01:04:00 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/14/2022 01:04:00 - INFO - __main__ - ['Yes']
03/14/2022 01:04:00 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/14/2022 01:04:00 - INFO - __main__ - ['Yes']
03/14/2022 01:04:00 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 01:04:03 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:04:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 01:04:04 - INFO - __main__ - Starting training!
03/14/2022 01:04:09 - INFO - __main__ - Loaded 6700 examples from test data
03/14/2022 01:06:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_42_0.0002_8_predictions.txt
03/14/2022 01:06:44 - INFO - __main__ - Classification-F1 on test data: 0.2887
03/14/2022 01:06:44 - INFO - __main__ - prefix=circa_16_42, lr=0.0002, bsz=8, dev_performance=0.40623438687954816, test_performance=0.28871664388257745
03/14/2022 01:06:44 - INFO - __main__ - Running ... prefix=circa_16_42, lr=0.0001, bsz=8 ...
03/14/2022 01:06:45 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:06:45 - INFO - __main__ - Printing 3 examples
03/14/2022 01:06:45 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/14/2022 01:06:45 - INFO - __main__ - ['Yes']
03/14/2022 01:06:45 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/14/2022 01:06:45 - INFO - __main__ - ['Yes']
03/14/2022 01:06:45 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/14/2022 01:06:45 - INFO - __main__ - ['Yes']
03/14/2022 01:06:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 01:06:45 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:06:45 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 01:06:45 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:06:45 - INFO - __main__ - Printing 3 examples
03/14/2022 01:06:45 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/14/2022 01:06:45 - INFO - __main__ - ['Yes']
03/14/2022 01:06:45 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/14/2022 01:06:45 - INFO - __main__ - ['Yes']
03/14/2022 01:06:45 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/14/2022 01:06:45 - INFO - __main__ - ['Yes']
03/14/2022 01:06:45 - INFO - __main__ - Tokenizing Input ...
03/14/2022 01:06:46 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:06:46 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 01:06:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 01:06:57 - INFO - __main__ - Starting training!
03/14/2022 01:07:01 - INFO - __main__ - Step 10 Global step 10 Train loss 23.717073 on epoch=1
03/14/2022 01:07:06 - INFO - __main__ - Step 20 Global step 20 Train loss 20.841995 on epoch=3
03/14/2022 01:07:11 - INFO - __main__ - Step 30 Global step 30 Train loss 19.823889 on epoch=5
03/14/2022 01:07:16 - INFO - __main__ - Step 40 Global step 40 Train loss 15.702184 on epoch=7
03/14/2022 01:07:21 - INFO - __main__ - Step 50 Global step 50 Train loss 14.141713 on epoch=9
03/14/2022 01:07:45 - INFO - __main__ - Global step 50 Train loss 18.845371 Classification-F1 0.0 on epoch=9
03/14/2022 01:07:51 - INFO - __main__ - Step 60 Global step 60 Train loss 13.484396 on epoch=11
03/14/2022 01:07:56 - INFO - __main__ - Step 70 Global step 70 Train loss 11.432363 on epoch=13
03/14/2022 01:08:01 - INFO - __main__ - Step 80 Global step 80 Train loss 10.736794 on epoch=15
03/14/2022 01:08:06 - INFO - __main__ - Step 90 Global step 90 Train loss 10.519835 on epoch=17
03/14/2022 01:08:11 - INFO - __main__ - Step 100 Global step 100 Train loss 9.166281 on epoch=19
03/14/2022 01:08:30 - INFO - __main__ - Global step 100 Train loss 11.067933 Classification-F1 0.0 on epoch=19
03/14/2022 01:08:35 - INFO - __main__ - Step 110 Global step 110 Train loss 9.192182 on epoch=21
03/14/2022 01:08:40 - INFO - __main__ - Step 120 Global step 120 Train loss 8.925557 on epoch=23
03/14/2022 01:08:45 - INFO - __main__ - Step 130 Global step 130 Train loss 9.275672 on epoch=25
03/14/2022 01:08:50 - INFO - __main__ - Step 140 Global step 140 Train loss 8.819936 on epoch=27
03/14/2022 01:08:55 - INFO - __main__ - Step 150 Global step 150 Train loss 7.785811 on epoch=29
03/14/2022 01:08:59 - INFO - __main__ - Global step 150 Train loss 8.799831 Classification-F1 0.0 on epoch=29
03/14/2022 01:09:03 - INFO - __main__ - Step 160 Global step 160 Train loss 7.922964 on epoch=31
03/14/2022 01:09:09 - INFO - __main__ - Step 170 Global step 170 Train loss 7.849832 on epoch=33
03/14/2022 01:09:14 - INFO - __main__ - Step 180 Global step 180 Train loss 7.680831 on epoch=35
03/14/2022 01:09:19 - INFO - __main__ - Step 190 Global step 190 Train loss 7.581219 on epoch=37
03/14/2022 01:09:24 - INFO - __main__ - Step 200 Global step 200 Train loss 7.005315 on epoch=39
03/14/2022 01:09:27 - INFO - __main__ - Global step 200 Train loss 7.608032 Classification-F1 0.0 on epoch=39
03/14/2022 01:09:33 - INFO - __main__ - Step 210 Global step 210 Train loss 7.011652 on epoch=41
03/14/2022 01:09:38 - INFO - __main__ - Step 220 Global step 220 Train loss 6.690235 on epoch=43
03/14/2022 01:09:43 - INFO - __main__ - Step 230 Global step 230 Train loss 6.334819 on epoch=45
03/14/2022 01:09:48 - INFO - __main__ - Step 240 Global step 240 Train loss 5.882575 on epoch=47
03/14/2022 01:09:53 - INFO - __main__ - Step 250 Global step 250 Train loss 6.256360 on epoch=49
03/14/2022 01:09:55 - INFO - __main__ - Global step 250 Train loss 6.435128 Classification-F1 0.0 on epoch=49
03/14/2022 01:10:00 - INFO - __main__ - Step 260 Global step 260 Train loss 6.035525 on epoch=51
03/14/2022 01:10:05 - INFO - __main__ - Step 270 Global step 270 Train loss 5.401628 on epoch=53
03/14/2022 01:10:10 - INFO - __main__ - Step 280 Global step 280 Train loss 5.082784 on epoch=55
03/14/2022 01:10:15 - INFO - __main__ - Step 290 Global step 290 Train loss 4.870321 on epoch=57
03/14/2022 01:10:20 - INFO - __main__ - Step 300 Global step 300 Train loss 4.748192 on epoch=59
03/14/2022 01:10:23 - INFO - __main__ - Global step 300 Train loss 5.227690 Classification-F1 0.0 on epoch=59
03/14/2022 01:10:28 - INFO - __main__ - Step 310 Global step 310 Train loss 4.037606 on epoch=61
03/14/2022 01:10:33 - INFO - __main__ - Step 320 Global step 320 Train loss 4.066985 on epoch=63
03/14/2022 01:10:38 - INFO - __main__ - Step 330 Global step 330 Train loss 3.728410 on epoch=65
03/14/2022 01:10:43 - INFO - __main__ - Step 340 Global step 340 Train loss 3.573395 on epoch=67
03/14/2022 01:10:48 - INFO - __main__ - Step 350 Global step 350 Train loss 3.283595 on epoch=69
03/14/2022 01:10:51 - INFO - __main__ - Global step 350 Train loss 3.737998 Classification-F1 0.0 on epoch=69
03/14/2022 01:10:56 - INFO - __main__ - Step 360 Global step 360 Train loss 2.879467 on epoch=71
03/14/2022 01:11:01 - INFO - __main__ - Step 370 Global step 370 Train loss 2.365252 on epoch=73
03/14/2022 01:11:06 - INFO - __main__ - Step 380 Global step 380 Train loss 2.848670 on epoch=75
03/14/2022 01:11:11 - INFO - __main__ - Step 390 Global step 390 Train loss 2.170462 on epoch=77
03/14/2022 01:11:16 - INFO - __main__ - Step 400 Global step 400 Train loss 2.075630 on epoch=79
03/14/2022 01:11:18 - INFO - __main__ - Global step 400 Train loss 2.467896 Classification-F1 0.17551020408163268 on epoch=79
03/14/2022 01:11:24 - INFO - __main__ - Step 410 Global step 410 Train loss 1.769940 on epoch=81
03/14/2022 01:11:29 - INFO - __main__ - Step 420 Global step 420 Train loss 1.834163 on epoch=83
03/14/2022 01:11:34 - INFO - __main__ - Step 430 Global step 430 Train loss 2.046989 on epoch=85
03/14/2022 01:11:39 - INFO - __main__ - Step 440 Global step 440 Train loss 2.070043 on epoch=87
03/14/2022 01:11:44 - INFO - __main__ - Step 450 Global step 450 Train loss 1.858710 on epoch=89
03/14/2022 01:11:46 - INFO - __main__ - Global step 450 Train loss 1.915969 Classification-F1 0.23176178660049632 on epoch=89
03/14/2022 01:11:52 - INFO - __main__ - Step 460 Global step 460 Train loss 1.428381 on epoch=91
03/14/2022 01:11:57 - INFO - __main__ - Step 470 Global step 470 Train loss 1.601278 on epoch=93
03/14/2022 01:12:02 - INFO - __main__ - Step 480 Global step 480 Train loss 1.471108 on epoch=95
03/14/2022 01:12:07 - INFO - __main__ - Step 490 Global step 490 Train loss 1.749292 on epoch=97
03/14/2022 01:12:12 - INFO - __main__ - Step 500 Global step 500 Train loss 1.597204 on epoch=99
03/14/2022 01:12:14 - INFO - __main__ - Global step 500 Train loss 1.569453 Classification-F1 0.17822966507177032 on epoch=99
03/14/2022 01:12:19 - INFO - __main__ - Step 510 Global step 510 Train loss 1.311049 on epoch=101
03/14/2022 01:12:24 - INFO - __main__ - Step 520 Global step 520 Train loss 1.195334 on epoch=103
03/14/2022 01:12:29 - INFO - __main__ - Step 530 Global step 530 Train loss 1.431152 on epoch=105
03/14/2022 01:12:34 - INFO - __main__ - Step 540 Global step 540 Train loss 1.148317 on epoch=107
03/14/2022 01:12:39 - INFO - __main__ - Step 550 Global step 550 Train loss 1.835044 on epoch=109
03/14/2022 01:12:41 - INFO - __main__ - Global step 550 Train loss 1.384179 Classification-F1 0.2072954947597659 on epoch=109
03/14/2022 01:12:46 - INFO - __main__ - Step 560 Global step 560 Train loss 1.743072 on epoch=111
03/14/2022 01:12:51 - INFO - __main__ - Step 570 Global step 570 Train loss 1.463268 on epoch=113
03/14/2022 01:12:56 - INFO - __main__ - Step 580 Global step 580 Train loss 1.551819 on epoch=115
03/14/2022 01:13:01 - INFO - __main__ - Step 590 Global step 590 Train loss 1.505894 on epoch=117
03/14/2022 01:13:06 - INFO - __main__ - Step 600 Global step 600 Train loss 1.733820 on epoch=119
03/14/2022 01:13:08 - INFO - __main__ - Global step 600 Train loss 1.599575 Classification-F1 0.23508771929824562 on epoch=119
03/14/2022 01:13:14 - INFO - __main__ - Step 610 Global step 610 Train loss 1.228867 on epoch=121
03/14/2022 01:13:19 - INFO - __main__ - Step 620 Global step 620 Train loss 1.567001 on epoch=123
03/14/2022 01:13:24 - INFO - __main__ - Step 630 Global step 630 Train loss 1.456984 on epoch=125
03/14/2022 01:13:29 - INFO - __main__ - Step 640 Global step 640 Train loss 1.383398 on epoch=127
03/14/2022 01:13:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.934284 on epoch=129
03/14/2022 01:13:36 - INFO - __main__ - Global step 650 Train loss 1.314107 Classification-F1 0.24035087719298245 on epoch=129
03/14/2022 01:13:42 - INFO - __main__ - Step 660 Global step 660 Train loss 1.434777 on epoch=131
03/14/2022 01:13:47 - INFO - __main__ - Step 670 Global step 670 Train loss 1.250961 on epoch=133
03/14/2022 01:13:52 - INFO - __main__ - Step 680 Global step 680 Train loss 1.160867 on epoch=135
03/14/2022 01:13:57 - INFO - __main__ - Step 690 Global step 690 Train loss 1.175742 on epoch=137
03/14/2022 01:14:02 - INFO - __main__ - Step 700 Global step 700 Train loss 1.751466 on epoch=139
03/14/2022 01:14:04 - INFO - __main__ - Global step 700 Train loss 1.354762 Classification-F1 0.255 on epoch=139
03/14/2022 01:14:10 - INFO - __main__ - Step 710 Global step 710 Train loss 1.377529 on epoch=141
03/14/2022 01:14:15 - INFO - __main__ - Step 720 Global step 720 Train loss 1.415759 on epoch=143
03/14/2022 01:14:20 - INFO - __main__ - Step 730 Global step 730 Train loss 1.104647 on epoch=145
03/14/2022 01:14:25 - INFO - __main__ - Step 740 Global step 740 Train loss 1.388380 on epoch=147
03/14/2022 01:14:30 - INFO - __main__ - Step 750 Global step 750 Train loss 1.075301 on epoch=149
03/14/2022 01:14:31 - INFO - __main__ - Global step 750 Train loss 1.272323 Classification-F1 0.2456140350877193 on epoch=149
03/14/2022 01:14:36 - INFO - __main__ - Step 760 Global step 760 Train loss 1.185308 on epoch=151
03/14/2022 01:14:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.934035 on epoch=153
03/14/2022 01:14:46 - INFO - __main__ - Step 780 Global step 780 Train loss 1.501552 on epoch=155
03/14/2022 01:14:52 - INFO - __main__ - Step 790 Global step 790 Train loss 1.415024 on epoch=157
03/14/2022 01:14:57 - INFO - __main__ - Step 800 Global step 800 Train loss 1.206069 on epoch=159
03/14/2022 01:14:58 - INFO - __main__ - Global step 800 Train loss 1.248398 Classification-F1 0.2487545937117191 on epoch=159
03/14/2022 01:15:03 - INFO - __main__ - Step 810 Global step 810 Train loss 1.187551 on epoch=161
03/14/2022 01:15:08 - INFO - __main__ - Step 820 Global step 820 Train loss 1.022261 on epoch=163
03/14/2022 01:15:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.968750 on epoch=165
03/14/2022 01:15:18 - INFO - __main__ - Step 840 Global step 840 Train loss 1.191532 on epoch=167
03/14/2022 01:15:23 - INFO - __main__ - Step 850 Global step 850 Train loss 1.159593 on epoch=169
03/14/2022 01:15:25 - INFO - __main__ - Global step 850 Train loss 1.105937 Classification-F1 0.2307364836100468 on epoch=169
03/14/2022 01:15:30 - INFO - __main__ - Step 860 Global step 860 Train loss 1.158212 on epoch=171
03/14/2022 01:15:35 - INFO - __main__ - Step 870 Global step 870 Train loss 1.263324 on epoch=173
03/14/2022 01:15:40 - INFO - __main__ - Step 880 Global step 880 Train loss 1.087046 on epoch=175
03/14/2022 01:15:45 - INFO - __main__ - Step 890 Global step 890 Train loss 1.266907 on epoch=177
03/14/2022 01:15:50 - INFO - __main__ - Step 900 Global step 900 Train loss 1.070200 on epoch=179
03/14/2022 01:15:52 - INFO - __main__ - Global step 900 Train loss 1.169138 Classification-F1 0.23483870967741938 on epoch=179
03/14/2022 01:15:57 - INFO - __main__ - Step 910 Global step 910 Train loss 1.308649 on epoch=181
03/14/2022 01:16:02 - INFO - __main__ - Step 920 Global step 920 Train loss 1.096277 on epoch=183
03/14/2022 01:16:07 - INFO - __main__ - Step 930 Global step 930 Train loss 1.174508 on epoch=185
03/14/2022 01:16:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.901548 on epoch=187
03/14/2022 01:16:17 - INFO - __main__ - Step 950 Global step 950 Train loss 1.526532 on epoch=189
03/14/2022 01:16:19 - INFO - __main__ - Global step 950 Train loss 1.201503 Classification-F1 0.13931623931623932 on epoch=189
03/14/2022 01:16:24 - INFO - __main__ - Step 960 Global step 960 Train loss 1.154221 on epoch=191
03/14/2022 01:16:29 - INFO - __main__ - Step 970 Global step 970 Train loss 1.144126 on epoch=193
03/14/2022 01:16:34 - INFO - __main__ - Step 980 Global step 980 Train loss 1.023533 on epoch=195
03/14/2022 01:16:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.984813 on epoch=197
03/14/2022 01:16:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.018698 on epoch=199
03/14/2022 01:16:45 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:16:45 - INFO - __main__ - Printing 3 examples
03/14/2022 01:16:45 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/14/2022 01:16:45 - INFO - __main__ - ['No']
03/14/2022 01:16:45 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/14/2022 01:16:45 - INFO - __main__ - ['No']
03/14/2022 01:16:45 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/14/2022 01:16:45 - INFO - __main__ - ['No']
03/14/2022 01:16:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 01:16:45 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:16:45 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 01:16:45 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:16:45 - INFO - __main__ - Printing 3 examples
03/14/2022 01:16:45 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/14/2022 01:16:45 - INFO - __main__ - ['No']
03/14/2022 01:16:45 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/14/2022 01:16:45 - INFO - __main__ - ['No']
03/14/2022 01:16:45 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/14/2022 01:16:45 - INFO - __main__ - ['No']
03/14/2022 01:16:45 - INFO - __main__ - Tokenizing Input ...
03/14/2022 01:16:45 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:16:45 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 01:16:46 - INFO - __main__ - Global step 1000 Train loss 1.065078 Classification-F1 0.13702239789196313 on epoch=199
03/14/2022 01:16:46 - INFO - __main__ - save last model!
03/14/2022 01:16:53 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 01:16:54 - INFO - __main__ - Start tokenizing ... 6700 instances
03/14/2022 01:16:54 - INFO - __main__ - Printing 3 examples
03/14/2022 01:16:54 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/14/2022 01:16:54 - INFO - __main__ - ['No']
03/14/2022 01:16:54 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/14/2022 01:16:54 - INFO - __main__ - ['Yes']
03/14/2022 01:16:54 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/14/2022 01:16:54 - INFO - __main__ - ['Yes']
03/14/2022 01:16:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 01:16:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 01:16:56 - INFO - __main__ - Starting training!
03/14/2022 01:16:57 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:17:03 - INFO - __main__ - Loaded 6700 examples from test data
03/14/2022 01:19:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_42_0.0001_8_predictions.txt
03/14/2022 01:19:47 - INFO - __main__ - Classification-F1 on test data: 0.2115
03/14/2022 01:19:48 - INFO - __main__ - prefix=circa_16_42, lr=0.0001, bsz=8, dev_performance=0.255, test_performance=0.21148226479008075
03/14/2022 01:19:49 - INFO - __main__ - Running ... prefix=circa_16_87, lr=0.0005, bsz=8 ...
03/14/2022 01:19:49 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:19:49 - INFO - __main__ - Printing 3 examples
03/14/2022 01:19:49 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/14/2022 01:19:49 - INFO - __main__ - ['No']
03/14/2022 01:19:49 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/14/2022 01:19:49 - INFO - __main__ - ['No']
03/14/2022 01:19:49 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/14/2022 01:19:49 - INFO - __main__ - ['No']
03/14/2022 01:19:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 01:19:49 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:19:50 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 01:19:50 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:19:50 - INFO - __main__ - Printing 3 examples
03/14/2022 01:19:50 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/14/2022 01:19:50 - INFO - __main__ - ['No']
03/14/2022 01:19:50 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/14/2022 01:19:50 - INFO - __main__ - ['No']
03/14/2022 01:19:50 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/14/2022 01:19:50 - INFO - __main__ - ['No']
03/14/2022 01:19:50 - INFO - __main__ - Tokenizing Input ...
03/14/2022 01:19:50 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:19:50 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 01:20:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 01:20:01 - INFO - __main__ - Starting training!
03/14/2022 01:20:05 - INFO - __main__ - Step 10 Global step 10 Train loss 21.839954 on epoch=1
03/14/2022 01:20:10 - INFO - __main__ - Step 20 Global step 20 Train loss 16.979923 on epoch=3
03/14/2022 01:20:14 - INFO - __main__ - Step 30 Global step 30 Train loss 12.316036 on epoch=5
03/14/2022 01:20:19 - INFO - __main__ - Step 40 Global step 40 Train loss 9.460958 on epoch=7
03/14/2022 01:20:24 - INFO - __main__ - Step 50 Global step 50 Train loss 8.111687 on epoch=9
03/14/2022 01:20:26 - INFO - __main__ - Global step 50 Train loss 13.741712 Classification-F1 0.0 on epoch=9
03/14/2022 01:20:31 - INFO - __main__ - Step 60 Global step 60 Train loss 7.534404 on epoch=11
03/14/2022 01:20:36 - INFO - __main__ - Step 70 Global step 70 Train loss 5.505363 on epoch=13
03/14/2022 01:20:41 - INFO - __main__ - Step 80 Global step 80 Train loss 4.458056 on epoch=15
03/14/2022 01:20:46 - INFO - __main__ - Step 90 Global step 90 Train loss 3.192476 on epoch=17
03/14/2022 01:20:51 - INFO - __main__ - Step 100 Global step 100 Train loss 2.524526 on epoch=19
03/14/2022 01:20:53 - INFO - __main__ - Global step 100 Train loss 4.642965 Classification-F1 0.06666666666666668 on epoch=19
03/14/2022 01:20:59 - INFO - __main__ - Step 110 Global step 110 Train loss 1.677985 on epoch=21
03/14/2022 01:21:04 - INFO - __main__ - Step 120 Global step 120 Train loss 1.896105 on epoch=23
03/14/2022 01:21:08 - INFO - __main__ - Step 130 Global step 130 Train loss 1.574257 on epoch=25
03/14/2022 01:21:13 - INFO - __main__ - Step 140 Global step 140 Train loss 1.404685 on epoch=27
03/14/2022 01:21:18 - INFO - __main__ - Step 150 Global step 150 Train loss 1.148224 on epoch=29
03/14/2022 01:21:20 - INFO - __main__ - Global step 150 Train loss 1.540251 Classification-F1 0.06736842105263158 on epoch=29
03/14/2022 01:21:26 - INFO - __main__ - Step 160 Global step 160 Train loss 1.282844 on epoch=31
03/14/2022 01:21:31 - INFO - __main__ - Step 170 Global step 170 Train loss 1.315622 on epoch=33
03/14/2022 01:21:36 - INFO - __main__ - Step 180 Global step 180 Train loss 1.506051 on epoch=35
03/14/2022 01:21:41 - INFO - __main__ - Step 190 Global step 190 Train loss 1.233713 on epoch=37
03/14/2022 01:21:46 - INFO - __main__ - Step 200 Global step 200 Train loss 1.354427 on epoch=39
03/14/2022 01:21:48 - INFO - __main__ - Global step 200 Train loss 1.338531 Classification-F1 0.06666666666666668 on epoch=39
03/14/2022 01:21:52 - INFO - __main__ - Step 210 Global step 210 Train loss 1.204277 on epoch=41
03/14/2022 01:21:57 - INFO - __main__ - Step 220 Global step 220 Train loss 1.197968 on epoch=43
03/14/2022 01:22:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.951295 on epoch=45
03/14/2022 01:22:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.981339 on epoch=47
03/14/2022 01:22:12 - INFO - __main__ - Step 250 Global step 250 Train loss 1.045576 on epoch=49
03/14/2022 01:22:14 - INFO - __main__ - Global step 250 Train loss 1.076091 Classification-F1 0.1512882171075232 on epoch=49
03/14/2022 01:22:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.835638 on epoch=51
03/14/2022 01:22:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.861450 on epoch=53
03/14/2022 01:22:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.877052 on epoch=55
03/14/2022 01:22:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.697521 on epoch=57
03/14/2022 01:22:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.687881 on epoch=59
03/14/2022 01:22:41 - INFO - __main__ - Global step 300 Train loss 0.791908 Classification-F1 0.06666666666666668 on epoch=59
03/14/2022 01:22:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.782951 on epoch=61
03/14/2022 01:22:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.748140 on epoch=63
03/14/2022 01:22:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.614724 on epoch=65
03/14/2022 01:23:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.605836 on epoch=67
03/14/2022 01:23:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.574734 on epoch=69
03/14/2022 01:23:08 - INFO - __main__ - Global step 350 Train loss 0.665277 Classification-F1 0.06666666666666668 on epoch=69
03/14/2022 01:23:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.582597 on epoch=71
03/14/2022 01:23:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.565917 on epoch=73
03/14/2022 01:23:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.587064 on epoch=75
03/14/2022 01:23:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.551189 on epoch=77
03/14/2022 01:23:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.519392 on epoch=79
03/14/2022 01:23:34 - INFO - __main__ - Global step 400 Train loss 0.561232 Classification-F1 0.06666666666666668 on epoch=79
03/14/2022 01:23:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.520152 on epoch=81
03/14/2022 01:23:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.524109 on epoch=83
03/14/2022 01:23:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.504518 on epoch=85
03/14/2022 01:23:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.517300 on epoch=87
03/14/2022 01:23:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.497476 on epoch=89
03/14/2022 01:24:01 - INFO - __main__ - Global step 450 Train loss 0.512711 Classification-F1 0.06666666666666668 on epoch=89
03/14/2022 01:24:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.485402 on epoch=91
03/14/2022 01:24:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.452680 on epoch=93
03/14/2022 01:24:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.508098 on epoch=95
03/14/2022 01:24:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.435640 on epoch=97
03/14/2022 01:24:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.471328 on epoch=99
03/14/2022 01:24:27 - INFO - __main__ - Global step 500 Train loss 0.470630 Classification-F1 0.06666666666666668 on epoch=99
03/14/2022 01:24:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.457302 on epoch=101
03/14/2022 01:24:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.457464 on epoch=103
03/14/2022 01:24:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.447024 on epoch=105
03/14/2022 01:24:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.459136 on epoch=107
03/14/2022 01:24:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.441427 on epoch=109
03/14/2022 01:24:54 - INFO - __main__ - Global step 550 Train loss 0.452471 Classification-F1 0.06666666666666668 on epoch=109
03/14/2022 01:24:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.453421 on epoch=111
03/14/2022 01:25:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.439248 on epoch=113
03/14/2022 01:25:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.462520 on epoch=115
03/14/2022 01:25:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.452445 on epoch=117
03/14/2022 01:25:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.494475 on epoch=119
03/14/2022 01:25:20 - INFO - __main__ - Global step 600 Train loss 0.460422 Classification-F1 0.06666666666666668 on epoch=119
03/14/2022 01:25:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.563048 on epoch=121
03/14/2022 01:25:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.444426 on epoch=123
03/14/2022 01:25:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.425367 on epoch=125
03/14/2022 01:25:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.431729 on epoch=127
03/14/2022 01:25:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.405688 on epoch=129
03/14/2022 01:25:47 - INFO - __main__ - Global step 650 Train loss 0.454051 Classification-F1 0.06666666666666668 on epoch=129
03/14/2022 01:25:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.388485 on epoch=131
03/14/2022 01:25:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.401796 on epoch=133
03/14/2022 01:26:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.433689 on epoch=135
03/14/2022 01:26:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.408889 on epoch=137
03/14/2022 01:26:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.397650 on epoch=139
03/14/2022 01:26:14 - INFO - __main__ - Global step 700 Train loss 0.406102 Classification-F1 0.06666666666666668 on epoch=139
03/14/2022 01:26:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.431518 on epoch=141
03/14/2022 01:26:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.411485 on epoch=143
03/14/2022 01:26:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.415026 on epoch=145
03/14/2022 01:26:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.426893 on epoch=147
03/14/2022 01:26:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.403522 on epoch=149
03/14/2022 01:26:40 - INFO - __main__ - Global step 750 Train loss 0.417689 Classification-F1 0.06736842105263158 on epoch=149
03/14/2022 01:26:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.404015 on epoch=151
03/14/2022 01:26:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.410338 on epoch=153
03/14/2022 01:26:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.407293 on epoch=155
03/14/2022 01:27:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.411964 on epoch=157
03/14/2022 01:27:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.404875 on epoch=159
03/14/2022 01:27:07 - INFO - __main__ - Global step 800 Train loss 0.407697 Classification-F1 0.11503267973856209 on epoch=159
03/14/2022 01:27:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.412361 on epoch=161
03/14/2022 01:27:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.398585 on epoch=163
03/14/2022 01:27:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.412066 on epoch=165
03/14/2022 01:27:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.423964 on epoch=167
03/14/2022 01:27:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.428012 on epoch=169
03/14/2022 01:27:34 - INFO - __main__ - Global step 850 Train loss 0.414998 Classification-F1 0.06808510638297872 on epoch=169
03/14/2022 01:27:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.419462 on epoch=171
03/14/2022 01:27:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.415545 on epoch=173
03/14/2022 01:27:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.391585 on epoch=175
03/14/2022 01:27:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.390885 on epoch=177
03/14/2022 01:27:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.384074 on epoch=179
03/14/2022 01:28:00 - INFO - __main__ - Global step 900 Train loss 0.400310 Classification-F1 0.06666666666666668 on epoch=179
03/14/2022 01:28:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.410644 on epoch=181
03/14/2022 01:28:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.394665 on epoch=183
03/14/2022 01:28:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.429740 on epoch=185
03/14/2022 01:28:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.398142 on epoch=187
03/14/2022 01:28:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.395469 on epoch=189
03/14/2022 01:28:27 - INFO - __main__ - Global step 950 Train loss 0.405732 Classification-F1 0.08673835125448029 on epoch=189
03/14/2022 01:28:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.423663 on epoch=191
03/14/2022 01:28:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.393641 on epoch=193
03/14/2022 01:28:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.380417 on epoch=195
03/14/2022 01:28:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.394201 on epoch=197
03/14/2022 01:28:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.395957 on epoch=199
03/14/2022 01:28:53 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:28:53 - INFO - __main__ - Printing 3 examples
03/14/2022 01:28:53 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/14/2022 01:28:53 - INFO - __main__ - ['No']
03/14/2022 01:28:53 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/14/2022 01:28:53 - INFO - __main__ - ['No']
03/14/2022 01:28:53 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/14/2022 01:28:53 - INFO - __main__ - ['No']
03/14/2022 01:28:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 01:28:53 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:28:53 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 01:28:53 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:28:53 - INFO - __main__ - Printing 3 examples
03/14/2022 01:28:53 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/14/2022 01:28:53 - INFO - __main__ - ['No']
03/14/2022 01:28:53 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/14/2022 01:28:53 - INFO - __main__ - ['No']
03/14/2022 01:28:53 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/14/2022 01:28:53 - INFO - __main__ - ['No']
03/14/2022 01:28:53 - INFO - __main__ - Tokenizing Input ...
03/14/2022 01:28:53 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:28:53 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 01:28:54 - INFO - __main__ - Global step 1000 Train loss 0.397576 Classification-F1 0.10471968709256844 on epoch=199
03/14/2022 01:28:54 - INFO - __main__ - save last model!
03/14/2022 01:29:00 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 01:29:01 - INFO - __main__ - Start tokenizing ... 6700 instances
03/14/2022 01:29:01 - INFO - __main__ - Printing 3 examples
03/14/2022 01:29:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/14/2022 01:29:01 - INFO - __main__ - ['No']
03/14/2022 01:29:01 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/14/2022 01:29:01 - INFO - __main__ - ['Yes']
03/14/2022 01:29:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/14/2022 01:29:01 - INFO - __main__ - ['Yes']
03/14/2022 01:29:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 01:29:04 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:29:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 01:29:06 - INFO - __main__ - Starting training!
03/14/2022 01:29:11 - INFO - __main__ - Loaded 6700 examples from test data
03/14/2022 01:31:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_87_0.0005_8_predictions.txt
03/14/2022 01:31:32 - INFO - __main__ - Classification-F1 on test data: 0.0911
03/14/2022 01:31:32 - INFO - __main__ - prefix=circa_16_87, lr=0.0005, bsz=8, dev_performance=0.1512882171075232, test_performance=0.09110311577495671
03/14/2022 01:31:32 - INFO - __main__ - Running ... prefix=circa_16_87, lr=0.0003, bsz=8 ...
03/14/2022 01:31:33 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:31:33 - INFO - __main__ - Printing 3 examples
03/14/2022 01:31:33 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/14/2022 01:31:33 - INFO - __main__ - ['No']
03/14/2022 01:31:33 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/14/2022 01:31:33 - INFO - __main__ - ['No']
03/14/2022 01:31:33 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/14/2022 01:31:33 - INFO - __main__ - ['No']
03/14/2022 01:31:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 01:31:33 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:31:33 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 01:31:33 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:31:33 - INFO - __main__ - Printing 3 examples
03/14/2022 01:31:33 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/14/2022 01:31:33 - INFO - __main__ - ['No']
03/14/2022 01:31:33 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/14/2022 01:31:33 - INFO - __main__ - ['No']
03/14/2022 01:31:33 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/14/2022 01:31:33 - INFO - __main__ - ['No']
03/14/2022 01:31:33 - INFO - __main__ - Tokenizing Input ...
03/14/2022 01:31:33 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:31:33 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 01:31:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 01:31:45 - INFO - __main__ - Starting training!
03/14/2022 01:31:50 - INFO - __main__ - Step 10 Global step 10 Train loss 22.854206 on epoch=1
03/14/2022 01:31:55 - INFO - __main__ - Step 20 Global step 20 Train loss 19.410074 on epoch=3
03/14/2022 01:32:00 - INFO - __main__ - Step 30 Global step 30 Train loss 12.794450 on epoch=5
03/14/2022 01:32:05 - INFO - __main__ - Step 40 Global step 40 Train loss 9.049385 on epoch=7
03/14/2022 01:32:10 - INFO - __main__ - Step 50 Global step 50 Train loss 7.893237 on epoch=9
03/14/2022 01:32:12 - INFO - __main__ - Global step 50 Train loss 14.400270 Classification-F1 0.0 on epoch=9
03/14/2022 01:32:18 - INFO - __main__ - Step 60 Global step 60 Train loss 7.065166 on epoch=11
03/14/2022 01:32:23 - INFO - __main__ - Step 70 Global step 70 Train loss 6.211959 on epoch=13
03/14/2022 01:32:28 - INFO - __main__ - Step 80 Global step 80 Train loss 5.929328 on epoch=15
03/14/2022 01:32:33 - INFO - __main__ - Step 90 Global step 90 Train loss 5.197563 on epoch=17
03/14/2022 01:32:38 - INFO - __main__ - Step 100 Global step 100 Train loss 4.346048 on epoch=19
03/14/2022 01:32:40 - INFO - __main__ - Global step 100 Train loss 5.750013 Classification-F1 0.0 on epoch=19
03/14/2022 01:32:45 - INFO - __main__ - Step 110 Global step 110 Train loss 3.756532 on epoch=21
03/14/2022 01:32:50 - INFO - __main__ - Step 120 Global step 120 Train loss 3.116727 on epoch=23
03/14/2022 01:32:55 - INFO - __main__ - Step 130 Global step 130 Train loss 2.264037 on epoch=25
03/14/2022 01:33:00 - INFO - __main__ - Step 140 Global step 140 Train loss 1.580752 on epoch=27
03/14/2022 01:33:05 - INFO - __main__ - Step 150 Global step 150 Train loss 1.370278 on epoch=29
03/14/2022 01:33:07 - INFO - __main__ - Global step 150 Train loss 2.417665 Classification-F1 0.25641877856252593 on epoch=29
03/14/2022 01:33:13 - INFO - __main__ - Step 160 Global step 160 Train loss 1.478142 on epoch=31
03/14/2022 01:33:18 - INFO - __main__ - Step 170 Global step 170 Train loss 1.679793 on epoch=33
03/14/2022 01:33:23 - INFO - __main__ - Step 180 Global step 180 Train loss 1.636924 on epoch=35
03/14/2022 01:33:28 - INFO - __main__ - Step 190 Global step 190 Train loss 1.413150 on epoch=37
03/14/2022 01:33:33 - INFO - __main__ - Step 200 Global step 200 Train loss 1.315336 on epoch=39
03/14/2022 01:33:35 - INFO - __main__ - Global step 200 Train loss 1.504669 Classification-F1 0.25 on epoch=39
03/14/2022 01:33:40 - INFO - __main__ - Step 210 Global step 210 Train loss 1.367471 on epoch=41
03/14/2022 01:33:45 - INFO - __main__ - Step 220 Global step 220 Train loss 1.346437 on epoch=43
03/14/2022 01:33:50 - INFO - __main__ - Step 230 Global step 230 Train loss 1.123428 on epoch=45
03/14/2022 01:33:55 - INFO - __main__ - Step 240 Global step 240 Train loss 1.288267 on epoch=47
03/14/2022 01:34:00 - INFO - __main__ - Step 250 Global step 250 Train loss 1.409814 on epoch=49
03/14/2022 01:34:02 - INFO - __main__ - Global step 250 Train loss 1.307084 Classification-F1 0.2513821138211382 on epoch=49
03/14/2022 01:34:07 - INFO - __main__ - Step 260 Global step 260 Train loss 1.083796 on epoch=51
03/14/2022 01:34:12 - INFO - __main__ - Step 270 Global step 270 Train loss 1.235239 on epoch=53
03/14/2022 01:34:17 - INFO - __main__ - Step 280 Global step 280 Train loss 1.120172 on epoch=55
03/14/2022 01:34:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.952235 on epoch=57
03/14/2022 01:34:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.852692 on epoch=59
03/14/2022 01:34:30 - INFO - __main__ - Global step 300 Train loss 1.048826 Classification-F1 0.25965750696933493 on epoch=59
03/14/2022 01:34:35 - INFO - __main__ - Step 310 Global step 310 Train loss 1.115700 on epoch=61
03/14/2022 01:34:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.844797 on epoch=63
03/14/2022 01:34:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.863813 on epoch=65
03/14/2022 01:34:50 - INFO - __main__ - Step 340 Global step 340 Train loss 1.048207 on epoch=67
03/14/2022 01:34:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.805672 on epoch=69
03/14/2022 01:34:57 - INFO - __main__ - Global step 350 Train loss 0.935638 Classification-F1 0.25434599156118143 on epoch=69
03/14/2022 01:35:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.933729 on epoch=71
03/14/2022 01:35:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.689391 on epoch=73
03/14/2022 01:35:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.816907 on epoch=75
03/14/2022 01:35:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.773630 on epoch=77
03/14/2022 01:35:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.786331 on epoch=79
03/14/2022 01:35:25 - INFO - __main__ - Global step 400 Train loss 0.799998 Classification-F1 0.31177787729511863 on epoch=79
03/14/2022 01:35:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.771872 on epoch=81
03/14/2022 01:35:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.624602 on epoch=83
03/14/2022 01:35:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.654745 on epoch=85
03/14/2022 01:35:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.760695 on epoch=87
03/14/2022 01:35:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.712698 on epoch=89
03/14/2022 01:35:52 - INFO - __main__ - Global step 450 Train loss 0.704922 Classification-F1 0.22418478260869565 on epoch=89
03/14/2022 01:35:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.484322 on epoch=91
03/14/2022 01:36:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.619452 on epoch=93
03/14/2022 01:36:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.552501 on epoch=95
03/14/2022 01:36:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.591201 on epoch=97
03/14/2022 01:36:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.472532 on epoch=99
03/14/2022 01:36:18 - INFO - __main__ - Global step 500 Train loss 0.544002 Classification-F1 0.25641877856252593 on epoch=99
03/14/2022 01:36:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.496638 on epoch=101
03/14/2022 01:36:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.457376 on epoch=103
03/14/2022 01:36:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.461592 on epoch=105
03/14/2022 01:36:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.459804 on epoch=107
03/14/2022 01:36:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.401248 on epoch=109
03/14/2022 01:36:45 - INFO - __main__ - Global step 550 Train loss 0.455332 Classification-F1 0.12444444444444444 on epoch=109
03/14/2022 01:36:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.402430 on epoch=111
03/14/2022 01:36:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.405180 on epoch=113
03/14/2022 01:37:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.372911 on epoch=115
03/14/2022 01:37:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.353501 on epoch=117
03/14/2022 01:37:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.328838 on epoch=119
03/14/2022 01:37:12 - INFO - __main__ - Global step 600 Train loss 0.372572 Classification-F1 0.4071291816778178 on epoch=119
03/14/2022 01:37:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.340904 on epoch=121
03/14/2022 01:37:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.369536 on epoch=123
03/14/2022 01:37:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.368323 on epoch=125
03/14/2022 01:37:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.344791 on epoch=127
03/14/2022 01:37:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.267634 on epoch=129
03/14/2022 01:37:40 - INFO - __main__ - Global step 650 Train loss 0.338238 Classification-F1 0.49375 on epoch=129
03/14/2022 01:37:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.301698 on epoch=131
03/14/2022 01:37:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.449027 on epoch=133
03/14/2022 01:37:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.231865 on epoch=135
03/14/2022 01:38:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.289029 on epoch=137
03/14/2022 01:38:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.435430 on epoch=139
03/14/2022 01:38:08 - INFO - __main__ - Global step 700 Train loss 0.341410 Classification-F1 0.16156923473996643 on epoch=139
03/14/2022 01:38:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.276667 on epoch=141
03/14/2022 01:38:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.155000 on epoch=143
03/14/2022 01:38:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.149797 on epoch=145
03/14/2022 01:38:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.161071 on epoch=147
03/14/2022 01:38:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.175920 on epoch=149
03/14/2022 01:38:35 - INFO - __main__ - Global step 750 Train loss 0.183691 Classification-F1 0.5522030651340996 on epoch=149
03/14/2022 01:38:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.188899 on epoch=151
03/14/2022 01:38:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.187964 on epoch=153
03/14/2022 01:38:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.233360 on epoch=155
03/14/2022 01:38:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.176924 on epoch=157
03/14/2022 01:39:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.148803 on epoch=159
03/14/2022 01:39:03 - INFO - __main__ - Global step 800 Train loss 0.187190 Classification-F1 0.5304269586622528 on epoch=159
03/14/2022 01:39:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.148411 on epoch=161
03/14/2022 01:39:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.139823 on epoch=163
03/14/2022 01:39:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.103577 on epoch=165
03/14/2022 01:39:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.086858 on epoch=167
03/14/2022 01:39:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.093708 on epoch=169
03/14/2022 01:39:30 - INFO - __main__ - Global step 850 Train loss 0.114475 Classification-F1 0.39048368363867975 on epoch=169
03/14/2022 01:39:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.108578 on epoch=171
03/14/2022 01:39:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.089093 on epoch=173
03/14/2022 01:39:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.066004 on epoch=175
03/14/2022 01:39:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.054342 on epoch=177
03/14/2022 01:39:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.029346 on epoch=179
03/14/2022 01:39:58 - INFO - __main__ - Global step 900 Train loss 0.069473 Classification-F1 0.44751334858886355 on epoch=179
03/14/2022 01:40:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.049588 on epoch=181
03/14/2022 01:40:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.071106 on epoch=183
03/14/2022 01:40:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.046562 on epoch=185
03/14/2022 01:40:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.024538 on epoch=187
03/14/2022 01:40:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.012555 on epoch=189
03/14/2022 01:40:25 - INFO - __main__ - Global step 950 Train loss 0.040870 Classification-F1 0.5019383019383019 on epoch=189
03/14/2022 01:40:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.023805 on epoch=191
03/14/2022 01:40:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.028928 on epoch=193
03/14/2022 01:40:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.014163 on epoch=195
03/14/2022 01:40:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.018862 on epoch=197
03/14/2022 01:40:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.011322 on epoch=199
03/14/2022 01:40:52 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:40:52 - INFO - __main__ - Printing 3 examples
03/14/2022 01:40:52 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/14/2022 01:40:52 - INFO - __main__ - ['No']
03/14/2022 01:40:52 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/14/2022 01:40:52 - INFO - __main__ - ['No']
03/14/2022 01:40:52 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/14/2022 01:40:52 - INFO - __main__ - ['No']
03/14/2022 01:40:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 01:40:52 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:40:53 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 01:40:53 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:40:53 - INFO - __main__ - Printing 3 examples
03/14/2022 01:40:53 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/14/2022 01:40:53 - INFO - __main__ - ['No']
03/14/2022 01:40:53 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/14/2022 01:40:53 - INFO - __main__ - ['No']
03/14/2022 01:40:53 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/14/2022 01:40:53 - INFO - __main__ - ['No']
03/14/2022 01:40:53 - INFO - __main__ - Tokenizing Input ...
03/14/2022 01:40:53 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:40:53 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 01:40:53 - INFO - __main__ - Global step 1000 Train loss 0.019416 Classification-F1 0.3718005952380953 on epoch=199
03/14/2022 01:40:53 - INFO - __main__ - save last model!
03/14/2022 01:41:00 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 01:41:01 - INFO - __main__ - Start tokenizing ... 6700 instances
03/14/2022 01:41:01 - INFO - __main__ - Printing 3 examples
03/14/2022 01:41:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/14/2022 01:41:01 - INFO - __main__ - ['No']
03/14/2022 01:41:01 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/14/2022 01:41:01 - INFO - __main__ - ['Yes']
03/14/2022 01:41:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/14/2022 01:41:01 - INFO - __main__ - ['Yes']
03/14/2022 01:41:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 01:41:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 01:41:03 - INFO - __main__ - Starting training!
03/14/2022 01:41:04 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:41:10 - INFO - __main__ - Loaded 6700 examples from test data
03/14/2022 01:43:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_87_0.0003_8_predictions.txt
03/14/2022 01:43:40 - INFO - __main__ - Classification-F1 on test data: 0.2351
03/14/2022 01:43:40 - INFO - __main__ - prefix=circa_16_87, lr=0.0003, bsz=8, dev_performance=0.5522030651340996, test_performance=0.23512576448826808
03/14/2022 01:43:40 - INFO - __main__ - Running ... prefix=circa_16_87, lr=0.0002, bsz=8 ...
03/14/2022 01:43:41 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:43:41 - INFO - __main__ - Printing 3 examples
03/14/2022 01:43:41 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/14/2022 01:43:41 - INFO - __main__ - ['No']
03/14/2022 01:43:41 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/14/2022 01:43:41 - INFO - __main__ - ['No']
03/14/2022 01:43:41 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/14/2022 01:43:41 - INFO - __main__ - ['No']
03/14/2022 01:43:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 01:43:41 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:43:41 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 01:43:41 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:43:41 - INFO - __main__ - Printing 3 examples
03/14/2022 01:43:41 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/14/2022 01:43:41 - INFO - __main__ - ['No']
03/14/2022 01:43:41 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/14/2022 01:43:41 - INFO - __main__ - ['No']
03/14/2022 01:43:41 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/14/2022 01:43:41 - INFO - __main__ - ['No']
03/14/2022 01:43:41 - INFO - __main__ - Tokenizing Input ...
03/14/2022 01:43:41 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:43:41 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 01:43:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 01:43:53 - INFO - __main__ - Starting training!
03/14/2022 01:43:57 - INFO - __main__ - Step 10 Global step 10 Train loss 20.212152 on epoch=1
03/14/2022 01:44:02 - INFO - __main__ - Step 20 Global step 20 Train loss 19.334751 on epoch=3
03/14/2022 01:44:07 - INFO - __main__ - Step 30 Global step 30 Train loss 14.488292 on epoch=5
03/14/2022 01:44:12 - INFO - __main__ - Step 40 Global step 40 Train loss 10.122376 on epoch=7
03/14/2022 01:44:17 - INFO - __main__ - Step 50 Global step 50 Train loss 8.278204 on epoch=9
03/14/2022 01:44:20 - INFO - __main__ - Global step 50 Train loss 14.487156 Classification-F1 0.0 on epoch=9
03/14/2022 01:44:25 - INFO - __main__ - Step 60 Global step 60 Train loss 7.995216 on epoch=11
03/14/2022 01:44:30 - INFO - __main__ - Step 70 Global step 70 Train loss 7.124799 on epoch=13
03/14/2022 01:44:35 - INFO - __main__ - Step 80 Global step 80 Train loss 7.156874 on epoch=15
03/14/2022 01:44:40 - INFO - __main__ - Step 90 Global step 90 Train loss 6.520299 on epoch=17
03/14/2022 01:44:46 - INFO - __main__ - Step 100 Global step 100 Train loss 6.159905 on epoch=19
03/14/2022 01:44:48 - INFO - __main__ - Global step 100 Train loss 6.991419 Classification-F1 0.0 on epoch=19
03/14/2022 01:44:53 - INFO - __main__ - Step 110 Global step 110 Train loss 6.139790 on epoch=21
03/14/2022 01:44:58 - INFO - __main__ - Step 120 Global step 120 Train loss 5.399365 on epoch=23
03/14/2022 01:45:03 - INFO - __main__ - Step 130 Global step 130 Train loss 4.707232 on epoch=25
03/14/2022 01:45:08 - INFO - __main__ - Step 140 Global step 140 Train loss 4.461575 on epoch=27
03/14/2022 01:45:13 - INFO - __main__ - Step 150 Global step 150 Train loss 3.922060 on epoch=29
03/14/2022 01:45:15 - INFO - __main__ - Global step 150 Train loss 4.926004 Classification-F1 0.0 on epoch=29
03/14/2022 01:45:20 - INFO - __main__ - Step 160 Global step 160 Train loss 3.278603 on epoch=31
03/14/2022 01:45:25 - INFO - __main__ - Step 170 Global step 170 Train loss 2.382907 on epoch=33
03/14/2022 01:45:30 - INFO - __main__ - Step 180 Global step 180 Train loss 1.787919 on epoch=35
03/14/2022 01:45:35 - INFO - __main__ - Step 190 Global step 190 Train loss 1.714722 on epoch=37
03/14/2022 01:45:40 - INFO - __main__ - Step 200 Global step 200 Train loss 1.436890 on epoch=39
03/14/2022 01:45:42 - INFO - __main__ - Global step 200 Train loss 2.120208 Classification-F1 0.25641877856252593 on epoch=39
03/14/2022 01:45:48 - INFO - __main__ - Step 210 Global step 210 Train loss 1.842159 on epoch=41
03/14/2022 01:45:53 - INFO - __main__ - Step 220 Global step 220 Train loss 1.983757 on epoch=43
03/14/2022 01:45:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.679620 on epoch=45
03/14/2022 01:46:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.509773 on epoch=47
03/14/2022 01:46:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.394052 on epoch=49
03/14/2022 01:46:10 - INFO - __main__ - Global step 250 Train loss 1.081872 Classification-F1 0.595054945054945 on epoch=49
03/14/2022 01:46:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.384346 on epoch=51
03/14/2022 01:46:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.242501 on epoch=53
03/14/2022 01:46:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.248077 on epoch=55
03/14/2022 01:46:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.144959 on epoch=57
03/14/2022 01:46:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.144626 on epoch=59
03/14/2022 01:46:38 - INFO - __main__ - Global step 300 Train loss 0.232902 Classification-F1 0.5022222222222222 on epoch=59
03/14/2022 01:46:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.171191 on epoch=61
03/14/2022 01:46:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.060251 on epoch=63
03/14/2022 01:46:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.128157 on epoch=65
03/14/2022 01:46:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.128004 on epoch=67
03/14/2022 01:47:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.021210 on epoch=69
03/14/2022 01:47:06 - INFO - __main__ - Global step 350 Train loss 0.101763 Classification-F1 0.5022430628542683 on epoch=69
03/14/2022 01:47:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.138700 on epoch=71
03/14/2022 01:47:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.103578 on epoch=73
03/14/2022 01:47:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.061083 on epoch=75
03/14/2022 01:47:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.028800 on epoch=77
03/14/2022 01:47:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.010760 on epoch=79
03/14/2022 01:47:33 - INFO - __main__ - Global step 400 Train loss 0.068584 Classification-F1 0.5782123056119 on epoch=79
03/14/2022 01:47:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.041512 on epoch=81
03/14/2022 01:47:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.004678 on epoch=83
03/14/2022 01:47:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.008427 on epoch=85
03/14/2022 01:47:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.015349 on epoch=87
03/14/2022 01:47:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.005725 on epoch=89
03/14/2022 01:48:00 - INFO - __main__ - Global step 450 Train loss 0.015138 Classification-F1 0.4137421962707477 on epoch=89
03/14/2022 01:48:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.004150 on epoch=91
03/14/2022 01:48:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001947 on epoch=93
03/14/2022 01:48:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.009283 on epoch=95
03/14/2022 01:48:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.005855 on epoch=97
03/14/2022 01:48:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001867 on epoch=99
03/14/2022 01:48:28 - INFO - __main__ - Global step 500 Train loss 0.004620 Classification-F1 0.5057744937055283 on epoch=99
03/14/2022 01:48:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000919 on epoch=101
03/14/2022 01:48:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003963 on epoch=103
03/14/2022 01:48:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.006977 on epoch=105
03/14/2022 01:48:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004371 on epoch=107
03/14/2022 01:48:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.003143 on epoch=109
03/14/2022 01:48:55 - INFO - __main__ - Global step 550 Train loss 0.003875 Classification-F1 0.42522588835044856 on epoch=109
03/14/2022 01:49:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.008718 on epoch=111
03/14/2022 01:49:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001378 on epoch=113
03/14/2022 01:49:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.006731 on epoch=115
03/14/2022 01:49:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001178 on epoch=117
03/14/2022 01:49:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000269 on epoch=119
03/14/2022 01:49:22 - INFO - __main__ - Global step 600 Train loss 0.003655 Classification-F1 0.5031235604094336 on epoch=119
03/14/2022 01:49:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000173 on epoch=121
03/14/2022 01:49:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000472 on epoch=123
03/14/2022 01:49:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000617 on epoch=125
03/14/2022 01:49:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.003560 on epoch=127
03/14/2022 01:49:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.010932 on epoch=129
03/14/2022 01:49:50 - INFO - __main__ - Global step 650 Train loss 0.003151 Classification-F1 0.598990133699291 on epoch=129
03/14/2022 01:49:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001110 on epoch=131
03/14/2022 01:50:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001960 on epoch=133
03/14/2022 01:50:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000417 on epoch=135
03/14/2022 01:50:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000729 on epoch=137
03/14/2022 01:50:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.008133 on epoch=139
03/14/2022 01:50:18 - INFO - __main__ - Global step 700 Train loss 0.002470 Classification-F1 0.506998556998557 on epoch=139
03/14/2022 01:50:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.017822 on epoch=141
03/14/2022 01:50:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000907 on epoch=143
03/14/2022 01:50:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001001 on epoch=145
03/14/2022 01:50:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000790 on epoch=147
03/14/2022 01:50:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000240 on epoch=149
03/14/2022 01:50:45 - INFO - __main__ - Global step 750 Train loss 0.004152 Classification-F1 0.6283714068845443 on epoch=149
03/14/2022 01:50:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000568 on epoch=151
03/14/2022 01:50:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000680 on epoch=153
03/14/2022 01:51:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000162 on epoch=155
03/14/2022 01:51:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.003113 on epoch=157
03/14/2022 01:51:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000631 on epoch=159
03/14/2022 01:51:13 - INFO - __main__ - Global step 800 Train loss 0.001031 Classification-F1 0.5291261420571766 on epoch=159
03/14/2022 01:51:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000865 on epoch=161
03/14/2022 01:51:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000090 on epoch=163
03/14/2022 01:51:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.015216 on epoch=165
03/14/2022 01:51:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000071 on epoch=167
03/14/2022 01:51:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001514 on epoch=169
03/14/2022 01:51:40 - INFO - __main__ - Global step 850 Train loss 0.003551 Classification-F1 0.5120515499825845 on epoch=169
03/14/2022 01:51:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000242 on epoch=171
03/14/2022 01:51:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.004480 on epoch=173
03/14/2022 01:51:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.019809 on epoch=175
03/14/2022 01:52:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001448 on epoch=177
03/14/2022 01:52:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000247 on epoch=179
03/14/2022 01:52:08 - INFO - __main__ - Global step 900 Train loss 0.005245 Classification-F1 0.46719381308184466 on epoch=179
03/14/2022 01:52:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000072 on epoch=181
03/14/2022 01:52:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000127 on epoch=183
03/14/2022 01:52:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000077 on epoch=185
03/14/2022 01:52:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000101 on epoch=187
03/14/2022 01:52:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000164 on epoch=189
03/14/2022 01:52:35 - INFO - __main__ - Global step 950 Train loss 0.000108 Classification-F1 0.5703725063523214 on epoch=189
03/14/2022 01:52:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000115 on epoch=191
03/14/2022 01:52:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.003490 on epoch=193
03/14/2022 01:52:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.005755 on epoch=195
03/14/2022 01:52:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000120 on epoch=197
03/14/2022 01:53:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000074 on epoch=199
03/14/2022 01:53:01 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:53:01 - INFO - __main__ - Printing 3 examples
03/14/2022 01:53:01 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/14/2022 01:53:01 - INFO - __main__ - ['No']
03/14/2022 01:53:01 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/14/2022 01:53:01 - INFO - __main__ - ['No']
03/14/2022 01:53:01 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/14/2022 01:53:01 - INFO - __main__ - ['No']
03/14/2022 01:53:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 01:53:01 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:53:01 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 01:53:01 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:53:01 - INFO - __main__ - Printing 3 examples
03/14/2022 01:53:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/14/2022 01:53:01 - INFO - __main__ - ['No']
03/14/2022 01:53:01 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/14/2022 01:53:01 - INFO - __main__ - ['No']
03/14/2022 01:53:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/14/2022 01:53:01 - INFO - __main__ - ['No']
03/14/2022 01:53:01 - INFO - __main__ - Tokenizing Input ...
03/14/2022 01:53:01 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:53:02 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 01:53:02 - INFO - __main__ - Global step 1000 Train loss 0.001911 Classification-F1 0.6178384833557247 on epoch=199
03/14/2022 01:53:02 - INFO - __main__ - save last model!
03/14/2022 01:53:09 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 01:53:10 - INFO - __main__ - Start tokenizing ... 6700 instances
03/14/2022 01:53:10 - INFO - __main__ - Printing 3 examples
03/14/2022 01:53:10 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/14/2022 01:53:10 - INFO - __main__ - ['No']
03/14/2022 01:53:10 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/14/2022 01:53:10 - INFO - __main__ - ['Yes']
03/14/2022 01:53:10 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/14/2022 01:53:10 - INFO - __main__ - ['Yes']
03/14/2022 01:53:10 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 01:53:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 01:53:12 - INFO - __main__ - Starting training!
03/14/2022 01:53:13 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:53:19 - INFO - __main__ - Loaded 6700 examples from test data
03/14/2022 01:56:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_87_0.0002_8_predictions.txt
03/14/2022 01:56:11 - INFO - __main__ - Classification-F1 on test data: 0.0384
03/14/2022 01:56:11 - INFO - __main__ - prefix=circa_16_87, lr=0.0002, bsz=8, dev_performance=0.6283714068845443, test_performance=0.038373498599915665
03/14/2022 01:56:11 - INFO - __main__ - Running ... prefix=circa_16_87, lr=0.0001, bsz=8 ...
03/14/2022 01:56:12 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:56:12 - INFO - __main__ - Printing 3 examples
03/14/2022 01:56:12 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/14/2022 01:56:12 - INFO - __main__ - ['No']
03/14/2022 01:56:12 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/14/2022 01:56:12 - INFO - __main__ - ['No']
03/14/2022 01:56:12 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/14/2022 01:56:12 - INFO - __main__ - ['No']
03/14/2022 01:56:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 01:56:12 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:56:12 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/14/2022 01:56:12 - INFO - __main__ - Start tokenizing ... 80 instances
03/14/2022 01:56:12 - INFO - __main__ - Printing 3 examples
03/14/2022 01:56:12 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/14/2022 01:56:12 - INFO - __main__ - ['No']
03/14/2022 01:56:12 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/14/2022 01:56:12 - INFO - __main__ - ['No']
03/14/2022 01:56:12 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/14/2022 01:56:12 - INFO - __main__ - ['No']
03/14/2022 01:56:12 - INFO - __main__ - Tokenizing Input ...
03/14/2022 01:56:12 - INFO - __main__ - Tokenizing Output ...
03/14/2022 01:56:12 - INFO - __main__ - Loaded 80 examples from dev data
03/14/2022 01:56:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 01:56:24 - INFO - __main__ - Starting training!
03/14/2022 01:56:28 - INFO - __main__ - Step 10 Global step 10 Train loss 22.374983 on epoch=1
03/14/2022 01:56:32 - INFO - __main__ - Step 20 Global step 20 Train loss 21.113556 on epoch=3
03/14/2022 01:56:37 - INFO - __main__ - Step 30 Global step 30 Train loss 16.748638 on epoch=5
03/14/2022 01:56:42 - INFO - __main__ - Step 40 Global step 40 Train loss 14.007227 on epoch=7
03/14/2022 01:56:47 - INFO - __main__ - Step 50 Global step 50 Train loss 12.262419 on epoch=9
03/14/2022 01:57:06 - INFO - __main__ - Global step 50 Train loss 17.301367 Classification-F1 0.0 on epoch=9
03/14/2022 01:57:11 - INFO - __main__ - Step 60 Global step 60 Train loss 11.202893 on epoch=11
03/14/2022 01:57:16 - INFO - __main__ - Step 70 Global step 70 Train loss 9.845754 on epoch=13
03/14/2022 01:57:21 - INFO - __main__ - Step 80 Global step 80 Train loss 9.361286 on epoch=15
03/14/2022 01:57:26 - INFO - __main__ - Step 90 Global step 90 Train loss 8.952156 on epoch=17
03/14/2022 01:57:31 - INFO - __main__ - Step 100 Global step 100 Train loss 8.312069 on epoch=19
03/14/2022 01:57:34 - INFO - __main__ - Global step 100 Train loss 9.534832 Classification-F1 0.0 on epoch=19
03/14/2022 01:57:39 - INFO - __main__ - Step 110 Global step 110 Train loss 8.045927 on epoch=21
03/14/2022 01:57:44 - INFO - __main__ - Step 120 Global step 120 Train loss 7.848063 on epoch=23
03/14/2022 01:57:49 - INFO - __main__ - Step 130 Global step 130 Train loss 7.952287 on epoch=25
03/14/2022 01:57:54 - INFO - __main__ - Step 140 Global step 140 Train loss 7.420081 on epoch=27
03/14/2022 01:57:59 - INFO - __main__ - Step 150 Global step 150 Train loss 7.332385 on epoch=29
03/14/2022 01:58:01 - INFO - __main__ - Global step 150 Train loss 7.719748 Classification-F1 0.0 on epoch=29
03/14/2022 01:58:06 - INFO - __main__ - Step 160 Global step 160 Train loss 7.247988 on epoch=31
03/14/2022 01:58:11 - INFO - __main__ - Step 170 Global step 170 Train loss 6.305722 on epoch=33
03/14/2022 01:58:16 - INFO - __main__ - Step 180 Global step 180 Train loss 6.283229 on epoch=35
03/14/2022 01:58:21 - INFO - __main__ - Step 190 Global step 190 Train loss 6.313708 on epoch=37
03/14/2022 01:58:26 - INFO - __main__ - Step 200 Global step 200 Train loss 5.906036 on epoch=39
03/14/2022 01:58:28 - INFO - __main__ - Global step 200 Train loss 6.411337 Classification-F1 0.0 on epoch=39
03/14/2022 01:58:33 - INFO - __main__ - Step 210 Global step 210 Train loss 6.205678 on epoch=41
03/14/2022 01:58:38 - INFO - __main__ - Step 220 Global step 220 Train loss 5.632773 on epoch=43
03/14/2022 01:58:43 - INFO - __main__ - Step 230 Global step 230 Train loss 5.738802 on epoch=45
03/14/2022 01:58:48 - INFO - __main__ - Step 240 Global step 240 Train loss 5.282636 on epoch=47
03/14/2022 01:58:53 - INFO - __main__ - Step 250 Global step 250 Train loss 5.206976 on epoch=49
03/14/2022 01:58:55 - INFO - __main__ - Global step 250 Train loss 5.613373 Classification-F1 0.0 on epoch=49
03/14/2022 01:59:00 - INFO - __main__ - Step 260 Global step 260 Train loss 4.542179 on epoch=51
03/14/2022 01:59:05 - INFO - __main__ - Step 270 Global step 270 Train loss 4.588301 on epoch=53
03/14/2022 01:59:10 - INFO - __main__ - Step 280 Global step 280 Train loss 4.525074 on epoch=55
03/14/2022 01:59:15 - INFO - __main__ - Step 290 Global step 290 Train loss 3.990051 on epoch=57
03/14/2022 01:59:20 - INFO - __main__ - Step 300 Global step 300 Train loss 3.917551 on epoch=59
03/14/2022 01:59:22 - INFO - __main__ - Global step 300 Train loss 4.312631 Classification-F1 0.0 on epoch=59
03/14/2022 01:59:27 - INFO - __main__ - Step 310 Global step 310 Train loss 3.501085 on epoch=61
03/14/2022 01:59:32 - INFO - __main__ - Step 320 Global step 320 Train loss 3.005440 on epoch=63
03/14/2022 01:59:37 - INFO - __main__ - Step 330 Global step 330 Train loss 2.654507 on epoch=65
03/14/2022 01:59:42 - INFO - __main__ - Step 340 Global step 340 Train loss 2.485167 on epoch=67
03/14/2022 01:59:47 - INFO - __main__ - Step 350 Global step 350 Train loss 2.294729 on epoch=69
03/14/2022 01:59:49 - INFO - __main__ - Global step 350 Train loss 2.788186 Classification-F1 0.0 on epoch=69
03/14/2022 01:59:54 - INFO - __main__ - Step 360 Global step 360 Train loss 1.916082 on epoch=71
03/14/2022 01:59:59 - INFO - __main__ - Step 370 Global step 370 Train loss 2.278639 on epoch=73
03/14/2022 02:00:04 - INFO - __main__ - Step 380 Global step 380 Train loss 1.936230 on epoch=75
03/14/2022 02:00:09 - INFO - __main__ - Step 390 Global step 390 Train loss 1.905430 on epoch=77
03/14/2022 02:00:14 - INFO - __main__ - Step 400 Global step 400 Train loss 1.537656 on epoch=79
03/14/2022 02:00:16 - INFO - __main__ - Global step 400 Train loss 1.914807 Classification-F1 0.25 on epoch=79
03/14/2022 02:00:21 - INFO - __main__ - Step 410 Global step 410 Train loss 1.606993 on epoch=81
03/14/2022 02:00:26 - INFO - __main__ - Step 420 Global step 420 Train loss 1.628021 on epoch=83
03/14/2022 02:00:31 - INFO - __main__ - Step 430 Global step 430 Train loss 1.700102 on epoch=85
03/14/2022 02:00:36 - INFO - __main__ - Step 440 Global step 440 Train loss 1.344846 on epoch=87
03/14/2022 02:00:41 - INFO - __main__ - Step 450 Global step 450 Train loss 1.629017 on epoch=89
03/14/2022 02:00:43 - INFO - __main__ - Global step 450 Train loss 1.581796 Classification-F1 0.24853700516351118 on epoch=89
03/14/2022 02:00:48 - INFO - __main__ - Step 460 Global step 460 Train loss 1.656370 on epoch=91
03/14/2022 02:00:53 - INFO - __main__ - Step 470 Global step 470 Train loss 1.920115 on epoch=93
03/14/2022 02:00:58 - INFO - __main__ - Step 480 Global step 480 Train loss 1.524021 on epoch=95
03/14/2022 02:01:03 - INFO - __main__ - Step 490 Global step 490 Train loss 1.511158 on epoch=97
03/14/2022 02:01:08 - INFO - __main__ - Step 500 Global step 500 Train loss 1.704277 on epoch=99
03/14/2022 02:01:10 - INFO - __main__ - Global step 500 Train loss 1.663188 Classification-F1 0.23407407407407405 on epoch=99
03/14/2022 02:01:15 - INFO - __main__ - Step 510 Global step 510 Train loss 1.318817 on epoch=101
03/14/2022 02:01:20 - INFO - __main__ - Step 520 Global step 520 Train loss 1.156860 on epoch=103
03/14/2022 02:01:25 - INFO - __main__ - Step 530 Global step 530 Train loss 1.704813 on epoch=105
03/14/2022 02:01:30 - INFO - __main__ - Step 540 Global step 540 Train loss 1.692407 on epoch=107
03/14/2022 02:01:35 - INFO - __main__ - Step 550 Global step 550 Train loss 1.333190 on epoch=109
03/14/2022 02:01:37 - INFO - __main__ - Global step 550 Train loss 1.441217 Classification-F1 0.25 on epoch=109
03/14/2022 02:01:42 - INFO - __main__ - Step 560 Global step 560 Train loss 1.051101 on epoch=111
03/14/2022 02:01:48 - INFO - __main__ - Step 570 Global step 570 Train loss 1.165714 on epoch=113
03/14/2022 02:01:53 - INFO - __main__ - Step 580 Global step 580 Train loss 1.263567 on epoch=115
03/14/2022 02:01:58 - INFO - __main__ - Step 590 Global step 590 Train loss 1.601768 on epoch=117
03/14/2022 02:02:03 - INFO - __main__ - Step 600 Global step 600 Train loss 1.592000 on epoch=119
03/14/2022 02:02:05 - INFO - __main__ - Global step 600 Train loss 1.334830 Classification-F1 0.25 on epoch=119
03/14/2022 02:02:10 - INFO - __main__ - Step 610 Global step 610 Train loss 1.614679 on epoch=121
03/14/2022 02:02:15 - INFO - __main__ - Step 620 Global step 620 Train loss 1.235857 on epoch=123
03/14/2022 02:02:20 - INFO - __main__ - Step 630 Global step 630 Train loss 1.451906 on epoch=125
03/14/2022 02:02:25 - INFO - __main__ - Step 640 Global step 640 Train loss 1.222381 on epoch=127
03/14/2022 02:02:30 - INFO - __main__ - Step 650 Global step 650 Train loss 1.385618 on epoch=129
03/14/2022 02:02:32 - INFO - __main__ - Global step 650 Train loss 1.382089 Classification-F1 0.25 on epoch=129
03/14/2022 02:02:37 - INFO - __main__ - Step 660 Global step 660 Train loss 1.291658 on epoch=131
03/14/2022 02:02:42 - INFO - __main__ - Step 670 Global step 670 Train loss 1.377867 on epoch=133
03/14/2022 02:02:47 - INFO - __main__ - Step 680 Global step 680 Train loss 1.249171 on epoch=135
03/14/2022 02:02:52 - INFO - __main__ - Step 690 Global step 690 Train loss 1.329918 on epoch=137
03/14/2022 02:02:57 - INFO - __main__ - Step 700 Global step 700 Train loss 1.487145 on epoch=139
03/14/2022 02:02:59 - INFO - __main__ - Global step 700 Train loss 1.347152 Classification-F1 0.2456140350877193 on epoch=139
03/14/2022 02:03:04 - INFO - __main__ - Step 710 Global step 710 Train loss 1.050749 on epoch=141
03/14/2022 02:03:09 - INFO - __main__ - Step 720 Global step 720 Train loss 1.128155 on epoch=143
03/14/2022 02:03:14 - INFO - __main__ - Step 730 Global step 730 Train loss 1.012214 on epoch=145
03/14/2022 02:03:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.968836 on epoch=147
03/14/2022 02:03:24 - INFO - __main__ - Step 750 Global step 750 Train loss 1.103825 on epoch=149
03/14/2022 02:03:26 - INFO - __main__ - Global step 750 Train loss 1.052756 Classification-F1 0.25 on epoch=149
03/14/2022 02:03:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.954013 on epoch=151
03/14/2022 02:03:37 - INFO - __main__ - Step 770 Global step 770 Train loss 1.140570 on epoch=153
03/14/2022 02:03:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.954570 on epoch=155
03/14/2022 02:03:47 - INFO - __main__ - Step 790 Global step 790 Train loss 1.078874 on epoch=157
03/14/2022 02:03:52 - INFO - __main__ - Step 800 Global step 800 Train loss 1.025833 on epoch=159
03/14/2022 02:03:54 - INFO - __main__ - Global step 800 Train loss 1.030772 Classification-F1 0.25965750696933493 on epoch=159
03/14/2022 02:03:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.907576 on epoch=161
03/14/2022 02:04:04 - INFO - __main__ - Step 820 Global step 820 Train loss 1.124116 on epoch=163
03/14/2022 02:04:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.997066 on epoch=165
03/14/2022 02:04:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.899178 on epoch=167
03/14/2022 02:04:19 - INFO - __main__ - Step 850 Global step 850 Train loss 1.081922 on epoch=169
03/14/2022 02:04:21 - INFO - __main__ - Global step 850 Train loss 1.001972 Classification-F1 0.2647154471544716 on epoch=169
03/14/2022 02:04:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.707715 on epoch=171
03/14/2022 02:04:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.967632 on epoch=173
03/14/2022 02:04:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.976285 on epoch=175
03/14/2022 02:04:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.886233 on epoch=177
03/14/2022 02:04:47 - INFO - __main__ - Step 900 Global step 900 Train loss 1.086491 on epoch=179
03/14/2022 02:04:49 - INFO - __main__ - Global step 900 Train loss 0.924872 Classification-F1 0.25735912531539107 on epoch=179
03/14/2022 02:04:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.908882 on epoch=181
03/14/2022 02:05:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.736446 on epoch=183
03/14/2022 02:05:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.971412 on epoch=185
03/14/2022 02:05:10 - INFO - __main__ - Step 940 Global step 940 Train loss 1.024971 on epoch=187
03/14/2022 02:05:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.810899 on epoch=189
03/14/2022 02:05:17 - INFO - __main__ - Global step 950 Train loss 0.890522 Classification-F1 0.2647154471544716 on epoch=189
03/14/2022 02:05:22 - INFO - __main__ - Step 960 Global step 960 Train loss 1.028900 on epoch=191
03/14/2022 02:05:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.790580 on epoch=193
03/14/2022 02:05:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.811466 on epoch=195
03/14/2022 02:05:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.859309 on epoch=197
03/14/2022 02:05:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.003063 on epoch=199
03/14/2022 02:05:44 - INFO - __main__ - Global step 1000 Train loss 0.898664 Classification-F1 0.242741935483871 on epoch=199
03/14/2022 02:05:44 - INFO - __main__ - save last model!
03/14/2022 02:05:51 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 02:05:52 - INFO - __main__ - Start tokenizing ... 6700 instances
03/14/2022 02:05:52 - INFO - __main__ - Printing 3 examples
03/14/2022 02:05:52 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/14/2022 02:05:52 - INFO - __main__ - ['No']
03/14/2022 02:05:52 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/14/2022 02:05:52 - INFO - __main__ - ['Yes']
03/14/2022 02:05:52 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/14/2022 02:05:52 - INFO - __main__ - ['Yes']
03/14/2022 02:05:52 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 02:05:55 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:06:02 - INFO - __main__ - Loaded 6700 examples from test data
03/14/2022 02:08:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-circa/circa_16_87_0.0001_8_predictions.txt
03/14/2022 02:08:43 - INFO - __main__ - Classification-F1 on test data: 0.2983
03/14/2022 02:08:43 - INFO - __main__ - prefix=circa_16_87, lr=0.0001, bsz=8, dev_performance=0.2647154471544716, test_performance=0.2982517930406354
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (16928): No such process
Task: wiki_split, Checkpoint: None, Identifier: T5-large-ft-random
Output directory () already exists and is not empty.
03/14/2022 02:08:50 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/14/2022 02:08:50 - INFO - __main__ - models/T5-large-ft-random/singletask-wiki_split
03/14/2022 02:08:50 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/14/2022 02:08:50 - INFO - __main__ - models/T5-large-ft-random/singletask-wiki_split
03/14/2022 02:08:53 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/14/2022 02:08:53 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/14/2022 02:08:53 - INFO - __main__ - args.device: cuda:0
03/14/2022 02:08:53 - INFO - __main__ - Using 2 gpus
03/14/2022 02:08:53 - INFO - __main__ - args.device: cuda:1
03/14/2022 02:08:53 - INFO - __main__ - Using 2 gpus
03/14/2022 02:08:53 - INFO - __main__ - Fine-tuning the following samples: ['wiki_split_32_100', 'wiki_split_32_13', 'wiki_split_32_21', 'wiki_split_32_42', 'wiki_split_32_87']
03/14/2022 02:08:53 - INFO - __main__ - Fine-tuning the following samples: ['wiki_split_32_100', 'wiki_split_32_13', 'wiki_split_32_21', 'wiki_split_32_42', 'wiki_split_32_87']
03/14/2022 02:08:57 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.0005, bsz=8 ...
03/14/2022 02:08:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 02:08:58 - INFO - __main__ - Printing 3 examples
03/14/2022 02:08:58 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/14/2022 02:08:58 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/14/2022 02:08:58 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/14/2022 02:08:58 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/14/2022 02:08:58 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/14/2022 02:08:58 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/14/2022 02:08:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 02:08:58 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:08:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 02:08:58 - INFO - __main__ - Printing 3 examples
03/14/2022 02:08:58 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/14/2022 02:08:58 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/14/2022 02:08:58 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/14/2022 02:08:58 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/14/2022 02:08:58 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/14/2022 02:08:58 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/14/2022 02:08:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 02:08:58 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:08:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 02:08:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 02:08:58 - INFO - __main__ - Printing 3 examples
03/14/2022 02:08:58 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/14/2022 02:08:58 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/14/2022 02:08:58 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/14/2022 02:08:58 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/14/2022 02:08:58 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/14/2022 02:08:58 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/14/2022 02:08:58 - INFO - __main__ - Tokenizing Input ...
03/14/2022 02:08:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 02:08:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 02:08:58 - INFO - __main__ - Printing 3 examples
03/14/2022 02:08:58 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/14/2022 02:08:58 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/14/2022 02:08:58 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/14/2022 02:08:58 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/14/2022 02:08:58 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/14/2022 02:08:58 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/14/2022 02:08:58 - INFO - __main__ - Tokenizing Input ...
03/14/2022 02:08:58 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:08:58 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:08:58 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 02:08:58 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 02:09:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 02:09:11 - INFO - __main__ - Starting training!
03/14/2022 02:09:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 02:09:13 - INFO - __main__ - Starting training!
03/14/2022 02:09:17 - INFO - __main__ - Step 10 Global step 10 Train loss 20.017355 on epoch=4
03/14/2022 02:09:22 - INFO - __main__ - Step 20 Global step 20 Train loss 12.140493 on epoch=9
03/14/2022 02:09:27 - INFO - __main__ - Step 30 Global step 30 Train loss 8.205810 on epoch=14
03/14/2022 02:09:32 - INFO - __main__ - Step 40 Global step 40 Train loss 6.790160 on epoch=19
03/14/2022 02:09:37 - INFO - __main__ - Step 50 Global step 50 Train loss 4.984126 on epoch=24
03/14/2022 02:09:39 - INFO - __main__ - Global step 50 Train loss 10.427589 Rouge-L 0.007371794728705533 on epoch=24
03/14/2022 02:09:46 - INFO - __main__ - Step 60 Global step 60 Train loss 3.144051 on epoch=29
03/14/2022 02:09:51 - INFO - __main__ - Step 70 Global step 70 Train loss 1.735755 on epoch=34
03/14/2022 02:09:56 - INFO - __main__ - Step 80 Global step 80 Train loss 1.012194 on epoch=39
03/14/2022 02:10:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.623985 on epoch=44
03/14/2022 02:10:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.505155 on epoch=49
03/14/2022 02:10:11 - INFO - __main__ - Global step 100 Train loss 1.404228 Rouge-L 0.10203547812511775 on epoch=49
03/14/2022 02:10:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.433423 on epoch=54
03/14/2022 02:10:23 - INFO - __main__ - Step 120 Global step 120 Train loss 0.308389 on epoch=59
03/14/2022 02:10:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.276302 on epoch=64
03/14/2022 02:10:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.231380 on epoch=69
03/14/2022 02:10:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.234673 on epoch=74
03/14/2022 02:10:43 - INFO - __main__ - Global step 150 Train loss 0.296833 Rouge-L 0.12936345258641335 on epoch=74
03/14/2022 02:10:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.198208 on epoch=79
03/14/2022 02:10:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.180292 on epoch=84
03/14/2022 02:10:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.176970 on epoch=89
03/14/2022 02:11:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.177118 on epoch=94
03/14/2022 02:11:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.171097 on epoch=99
03/14/2022 02:11:19 - INFO - __main__ - Global step 200 Train loss 0.180737 Rouge-L 0.1167139549086538 on epoch=99
03/14/2022 02:11:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.181399 on epoch=104
03/14/2022 02:11:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.168806 on epoch=109
03/14/2022 02:11:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.143625 on epoch=114
03/14/2022 02:11:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.166919 on epoch=119
03/14/2022 02:11:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.146882 on epoch=124
03/14/2022 02:11:50 - INFO - __main__ - Global step 250 Train loss 0.161526 Rouge-L 0.12214769637969401 on epoch=124
03/14/2022 02:11:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.157249 on epoch=129
03/14/2022 02:12:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.113441 on epoch=134
03/14/2022 02:12:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.109943 on epoch=139
03/14/2022 02:12:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.135524 on epoch=144
03/14/2022 02:12:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.129360 on epoch=149
03/14/2022 02:12:21 - INFO - __main__ - Global step 300 Train loss 0.129103 Rouge-L 0.1268778064760512 on epoch=149
03/14/2022 02:12:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.133882 on epoch=154
03/14/2022 02:12:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.127068 on epoch=159
03/14/2022 02:12:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.104836 on epoch=164
03/14/2022 02:12:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.095075 on epoch=169
03/14/2022 02:12:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.104902 on epoch=174
03/14/2022 02:12:51 - INFO - __main__ - Global step 350 Train loss 0.113153 Rouge-L 0.12266391052527413 on epoch=174
03/14/2022 02:12:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.097942 on epoch=179
03/14/2022 02:13:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.105184 on epoch=184
03/14/2022 02:13:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.083284 on epoch=189
03/14/2022 02:13:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.097142 on epoch=194
03/14/2022 02:13:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.095447 on epoch=199
03/14/2022 02:13:21 - INFO - __main__ - Global step 400 Train loss 0.095800 Rouge-L 0.11324502784429386 on epoch=199
03/14/2022 02:13:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.096195 on epoch=204
03/14/2022 02:13:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.081139 on epoch=209
03/14/2022 02:13:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.078108 on epoch=214
03/14/2022 02:13:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.094804 on epoch=219
03/14/2022 02:13:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.083393 on epoch=224
03/14/2022 02:13:51 - INFO - __main__ - Global step 450 Train loss 0.086728 Rouge-L 0.11880757081202777 on epoch=224
03/14/2022 02:13:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.084964 on epoch=229
03/14/2022 02:14:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.083008 on epoch=234
03/14/2022 02:14:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.078421 on epoch=239
03/14/2022 02:14:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.084649 on epoch=244
03/14/2022 02:14:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.074505 on epoch=249
03/14/2022 02:14:22 - INFO - __main__ - Global step 500 Train loss 0.081109 Rouge-L 0.11893471541634215 on epoch=249
03/14/2022 02:14:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.075899 on epoch=254
03/14/2022 02:14:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.085785 on epoch=259
03/14/2022 02:14:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.076171 on epoch=264
03/14/2022 02:14:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.076031 on epoch=269
03/14/2022 02:14:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.071602 on epoch=274
03/14/2022 02:14:53 - INFO - __main__ - Global step 550 Train loss 0.077098 Rouge-L 0.11538296682845818 on epoch=274
03/14/2022 02:14:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.066487 on epoch=279
03/14/2022 02:15:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.069052 on epoch=284
03/14/2022 02:15:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.075201 on epoch=289
03/14/2022 02:15:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.072564 on epoch=294
03/14/2022 02:15:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.146750 on epoch=299
03/14/2022 02:15:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 02:15:20 - INFO - __main__ - Printing 3 examples
03/14/2022 02:15:20 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/14/2022 02:15:20 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/14/2022 02:15:20 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/14/2022 02:15:20 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/14/2022 02:15:20 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/14/2022 02:15:20 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/14/2022 02:15:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 02:15:20 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:15:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 02:15:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 02:15:20 - INFO - __main__ - Printing 3 examples
03/14/2022 02:15:20 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/14/2022 02:15:20 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/14/2022 02:15:20 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/14/2022 02:15:20 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/14/2022 02:15:20 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/14/2022 02:15:20 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/14/2022 02:15:20 - INFO - __main__ - Tokenizing Input ...
03/14/2022 02:15:20 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:15:20 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 02:15:24 - INFO - __main__ - Global step 600 Train loss 0.086011 Rouge-L 0.11157661468189312 on epoch=299
03/14/2022 02:15:24 - INFO - __main__ - save last model!
03/14/2022 02:15:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 02:15:31 - INFO - __main__ - Starting training!
03/14/2022 02:15:31 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 02:15:32 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 02:15:32 - INFO - __main__ - Printing 3 examples
03/14/2022 02:15:32 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 02:15:32 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 02:15:32 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 02:15:32 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 02:15:32 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 02:15:32 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 02:15:32 - INFO - __main__ - Tokenizing Input ...
03/14/2022 02:15:34 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:15:40 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 02:29:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_100_0.0005_8_predictions.txt
03/14/2022 02:29:36 - INFO - __main__ - Rouge-L on test data: 0.1170
03/14/2022 02:29:36 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.0005, bsz=8, dev_performance=0.12936345258641335, test_performance=0.11702490701953193
03/14/2022 02:29:36 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.0003, bsz=8 ...
03/14/2022 02:29:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 02:29:37 - INFO - __main__ - Printing 3 examples
03/14/2022 02:29:37 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/14/2022 02:29:37 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/14/2022 02:29:37 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/14/2022 02:29:37 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/14/2022 02:29:37 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/14/2022 02:29:37 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/14/2022 02:29:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 02:29:37 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:29:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 02:29:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 02:29:37 - INFO - __main__ - Printing 3 examples
03/14/2022 02:29:37 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/14/2022 02:29:37 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/14/2022 02:29:37 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/14/2022 02:29:37 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/14/2022 02:29:37 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/14/2022 02:29:37 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/14/2022 02:29:37 - INFO - __main__ - Tokenizing Input ...
03/14/2022 02:29:37 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:29:37 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 02:29:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 02:29:50 - INFO - __main__ - Starting training!
03/14/2022 02:29:55 - INFO - __main__ - Step 10 Global step 10 Train loss 20.773630 on epoch=4
03/14/2022 02:30:00 - INFO - __main__ - Step 20 Global step 20 Train loss 12.626114 on epoch=9
03/14/2022 02:30:05 - INFO - __main__ - Step 30 Global step 30 Train loss 6.810597 on epoch=14
03/14/2022 02:30:10 - INFO - __main__ - Step 40 Global step 40 Train loss 5.287593 on epoch=19
03/14/2022 02:30:15 - INFO - __main__ - Step 50 Global step 50 Train loss 3.643123 on epoch=24
03/14/2022 02:30:26 - INFO - __main__ - Global step 50 Train loss 9.828211 Rouge-L 0.5927536971873779 on epoch=24
03/14/2022 02:30:32 - INFO - __main__ - Step 60 Global step 60 Train loss 1.802545 on epoch=29
03/14/2022 02:30:37 - INFO - __main__ - Step 70 Global step 70 Train loss 1.154742 on epoch=34
03/14/2022 02:30:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.789670 on epoch=39
03/14/2022 02:30:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.626263 on epoch=44
03/14/2022 02:30:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.566645 on epoch=49
03/14/2022 02:31:01 - INFO - __main__ - Global step 100 Train loss 0.987973 Rouge-L 0.5846511478943477 on epoch=49
03/14/2022 02:31:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.475878 on epoch=54
03/14/2022 02:31:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.391499 on epoch=59
03/14/2022 02:31:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.339978 on epoch=64
03/14/2022 02:31:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.326742 on epoch=69
03/14/2022 02:31:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.268397 on epoch=74
03/14/2022 02:31:32 - INFO - __main__ - Global step 150 Train loss 0.360499 Rouge-L 0.6608185183321132 on epoch=74
03/14/2022 02:31:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.270338 on epoch=79
03/14/2022 02:31:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.229423 on epoch=84
03/14/2022 02:31:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.221320 on epoch=89
03/14/2022 02:31:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.211253 on epoch=94
03/14/2022 02:31:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.211658 on epoch=99
03/14/2022 02:32:05 - INFO - __main__ - Global step 200 Train loss 0.228798 Rouge-L 0.6120878131238572 on epoch=99
03/14/2022 02:32:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.194232 on epoch=104
03/14/2022 02:32:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.181964 on epoch=109
03/14/2022 02:32:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.167647 on epoch=114
03/14/2022 02:32:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.167595 on epoch=119
03/14/2022 02:32:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.152897 on epoch=124
03/14/2022 02:32:35 - INFO - __main__ - Global step 250 Train loss 0.172867 Rouge-L 0.6596541999565739 on epoch=124
03/14/2022 02:32:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.193269 on epoch=129
03/14/2022 02:32:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.169592 on epoch=134
03/14/2022 02:32:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.173114 on epoch=139
03/14/2022 02:32:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.140809 on epoch=144
03/14/2022 02:33:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.142669 on epoch=149
03/14/2022 02:33:05 - INFO - __main__ - Global step 300 Train loss 0.163890 Rouge-L 0.68431592023033 on epoch=149
03/14/2022 02:33:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.133416 on epoch=154
03/14/2022 02:33:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.126502 on epoch=159
03/14/2022 02:33:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.144203 on epoch=164
03/14/2022 02:33:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.129848 on epoch=169
03/14/2022 02:33:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.138306 on epoch=174
03/14/2022 02:33:38 - INFO - __main__ - Global step 350 Train loss 0.134455 Rouge-L 0.6908327562377765 on epoch=174
03/14/2022 02:33:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.119543 on epoch=179
03/14/2022 02:33:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.106885 on epoch=184
03/14/2022 02:33:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.128406 on epoch=189
03/14/2022 02:33:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.142109 on epoch=194
03/14/2022 02:34:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.105507 on epoch=199
03/14/2022 02:34:10 - INFO - __main__ - Global step 400 Train loss 0.120490 Rouge-L 0.6972147043953034 on epoch=199
03/14/2022 02:34:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.115196 on epoch=204
03/14/2022 02:34:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.109023 on epoch=209
03/14/2022 02:34:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.120151 on epoch=214
03/14/2022 02:34:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.108012 on epoch=219
03/14/2022 02:34:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.092232 on epoch=224
03/14/2022 02:34:40 - INFO - __main__ - Global step 450 Train loss 0.108923 Rouge-L 0.6091688766342065 on epoch=224
03/14/2022 02:34:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.114948 on epoch=229
03/14/2022 02:34:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.095009 on epoch=234
03/14/2022 02:34:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.102409 on epoch=239
03/14/2022 02:35:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.102144 on epoch=244
03/14/2022 02:35:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.096618 on epoch=249
03/14/2022 02:35:10 - INFO - __main__ - Global step 500 Train loss 0.102226 Rouge-L 0.5870323121085945 on epoch=249
03/14/2022 02:35:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.099824 on epoch=254
03/14/2022 02:35:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.098521 on epoch=259
03/14/2022 02:35:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.086819 on epoch=264
03/14/2022 02:35:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.095284 on epoch=269
03/14/2022 02:35:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.093097 on epoch=274
03/14/2022 02:35:41 - INFO - __main__ - Global step 550 Train loss 0.094709 Rouge-L 0.7078790459280717 on epoch=274
03/14/2022 02:35:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.095675 on epoch=279
03/14/2022 02:35:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.099441 on epoch=284
03/14/2022 02:35:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.094887 on epoch=289
03/14/2022 02:36:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.097164 on epoch=294
03/14/2022 02:36:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.089601 on epoch=299
03/14/2022 02:36:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 02:36:07 - INFO - __main__ - Printing 3 examples
03/14/2022 02:36:07 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/14/2022 02:36:07 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/14/2022 02:36:07 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/14/2022 02:36:07 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/14/2022 02:36:07 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/14/2022 02:36:07 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/14/2022 02:36:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 02:36:07 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:36:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 02:36:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 02:36:07 - INFO - __main__ - Printing 3 examples
03/14/2022 02:36:07 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/14/2022 02:36:07 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/14/2022 02:36:07 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/14/2022 02:36:07 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/14/2022 02:36:07 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/14/2022 02:36:07 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/14/2022 02:36:07 - INFO - __main__ - Tokenizing Input ...
03/14/2022 02:36:07 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:36:07 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 02:36:13 - INFO - __main__ - Global step 600 Train loss 0.095353 Rouge-L 0.6942100253475595 on epoch=299
03/14/2022 02:36:13 - INFO - __main__ - save last model!
03/14/2022 02:36:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 02:36:18 - INFO - __main__ - Starting training!
03/14/2022 02:36:20 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 02:36:20 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 02:36:20 - INFO - __main__ - Printing 3 examples
03/14/2022 02:36:20 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 02:36:20 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 02:36:20 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 02:36:20 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 02:36:20 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 02:36:20 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 02:36:20 - INFO - __main__ - Tokenizing Input ...
03/14/2022 02:36:23 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:36:28 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 02:57:28 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_100_0.0003_8_predictions.txt
03/14/2022 02:57:32 - INFO - __main__ - Rouge-L on test data: 0.6786
03/14/2022 02:57:33 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.0003, bsz=8, dev_performance=0.7078790459280717, test_performance=0.678598270548559
03/14/2022 02:57:33 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.0002, bsz=8 ...
03/14/2022 02:57:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 02:57:33 - INFO - __main__ - Printing 3 examples
03/14/2022 02:57:33 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/14/2022 02:57:33 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/14/2022 02:57:33 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/14/2022 02:57:33 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/14/2022 02:57:33 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/14/2022 02:57:33 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/14/2022 02:57:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 02:57:34 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:57:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 02:57:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 02:57:34 - INFO - __main__ - Printing 3 examples
03/14/2022 02:57:34 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/14/2022 02:57:34 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/14/2022 02:57:34 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/14/2022 02:57:34 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/14/2022 02:57:34 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/14/2022 02:57:34 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/14/2022 02:57:34 - INFO - __main__ - Tokenizing Input ...
03/14/2022 02:57:34 - INFO - __main__ - Tokenizing Output ...
03/14/2022 02:57:34 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 02:57:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 02:57:46 - INFO - __main__ - Starting training!
03/14/2022 02:57:52 - INFO - __main__ - Step 10 Global step 10 Train loss 20.308001 on epoch=4
03/14/2022 02:57:57 - INFO - __main__ - Step 20 Global step 20 Train loss 15.657033 on epoch=9
03/14/2022 02:58:01 - INFO - __main__ - Step 30 Global step 30 Train loss 6.952329 on epoch=14
03/14/2022 02:58:06 - INFO - __main__ - Step 40 Global step 40 Train loss 4.278533 on epoch=19
03/14/2022 02:58:11 - INFO - __main__ - Step 50 Global step 50 Train loss 2.492006 on epoch=24
03/14/2022 02:58:24 - INFO - __main__ - Global step 50 Train loss 9.937579 Rouge-L 0.6578872498551066 on epoch=24
03/14/2022 02:58:30 - INFO - __main__ - Step 60 Global step 60 Train loss 1.613108 on epoch=29
03/14/2022 02:58:35 - INFO - __main__ - Step 70 Global step 70 Train loss 1.256072 on epoch=34
03/14/2022 02:58:40 - INFO - __main__ - Step 80 Global step 80 Train loss 1.108444 on epoch=39
03/14/2022 02:58:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.920709 on epoch=44
03/14/2022 02:58:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.798804 on epoch=49
03/14/2022 02:58:59 - INFO - __main__ - Global step 100 Train loss 1.139427 Rouge-L 0.5494234227762939 on epoch=49
03/14/2022 02:59:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.729441 on epoch=54
03/14/2022 02:59:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.620067 on epoch=59
03/14/2022 02:59:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.536974 on epoch=64
03/14/2022 02:59:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.496062 on epoch=69
03/14/2022 02:59:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.436430 on epoch=74
03/14/2022 02:59:31 - INFO - __main__ - Global step 150 Train loss 0.563795 Rouge-L 0.6145337418428294 on epoch=74
03/14/2022 02:59:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.437206 on epoch=79
03/14/2022 02:59:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.390130 on epoch=84
03/14/2022 02:59:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.375455 on epoch=89
03/14/2022 02:59:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.348881 on epoch=94
03/14/2022 02:59:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.328430 on epoch=99
03/14/2022 03:00:01 - INFO - __main__ - Global step 200 Train loss 0.376020 Rouge-L 0.5291512521277402 on epoch=99
03/14/2022 03:00:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.298046 on epoch=104
03/14/2022 03:00:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.280190 on epoch=109
03/14/2022 03:00:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.256766 on epoch=114
03/14/2022 03:00:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.255530 on epoch=119
03/14/2022 03:00:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.236273 on epoch=124
03/14/2022 03:00:32 - INFO - __main__ - Global step 250 Train loss 0.265361 Rouge-L 0.7162372379039555 on epoch=124
03/14/2022 03:00:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.256631 on epoch=129
03/14/2022 03:00:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.229121 on epoch=134
03/14/2022 03:00:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.215050 on epoch=139
03/14/2022 03:00:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.194435 on epoch=144
03/14/2022 03:00:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.190984 on epoch=149
03/14/2022 03:01:06 - INFO - __main__ - Global step 300 Train loss 0.217244 Rouge-L 0.7190818406955295 on epoch=149
03/14/2022 03:01:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.191884 on epoch=154
03/14/2022 03:01:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.209752 on epoch=159
03/14/2022 03:01:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.190011 on epoch=164
03/14/2022 03:01:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.191917 on epoch=169
03/14/2022 03:01:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.185617 on epoch=174
03/14/2022 03:01:37 - INFO - __main__ - Global step 350 Train loss 0.193836 Rouge-L 0.681015068533886 on epoch=174
03/14/2022 03:01:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.149810 on epoch=179
03/14/2022 03:01:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.175619 on epoch=184
03/14/2022 03:01:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.173204 on epoch=189
03/14/2022 03:01:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.176766 on epoch=194
03/14/2022 03:02:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.155905 on epoch=199
03/14/2022 03:02:08 - INFO - __main__ - Global step 400 Train loss 0.166261 Rouge-L 0.6816204013521746 on epoch=199
03/14/2022 03:02:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.158318 on epoch=204
03/14/2022 03:02:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.165819 on epoch=209
03/14/2022 03:02:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.159272 on epoch=214
03/14/2022 03:02:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.156457 on epoch=219
03/14/2022 03:02:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.147130 on epoch=224
03/14/2022 03:02:39 - INFO - __main__ - Global step 450 Train loss 0.157399 Rouge-L 0.7098270528544144 on epoch=224
03/14/2022 03:02:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.150069 on epoch=229
03/14/2022 03:02:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.125683 on epoch=234
03/14/2022 03:02:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.147752 on epoch=239
03/14/2022 03:02:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.152410 on epoch=244
03/14/2022 03:03:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.160212 on epoch=249
03/14/2022 03:03:10 - INFO - __main__ - Global step 500 Train loss 0.147225 Rouge-L 0.6693795187805778 on epoch=249
03/14/2022 03:03:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.132699 on epoch=254
03/14/2022 03:03:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.134251 on epoch=259
03/14/2022 03:03:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.152068 on epoch=264
03/14/2022 03:03:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.111866 on epoch=269
03/14/2022 03:03:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.125093 on epoch=274
03/14/2022 03:03:41 - INFO - __main__ - Global step 550 Train loss 0.131195 Rouge-L 0.6934742442750428 on epoch=274
03/14/2022 03:03:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.146368 on epoch=279
03/14/2022 03:03:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.121804 on epoch=284
03/14/2022 03:03:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.120403 on epoch=289
03/14/2022 03:04:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.119906 on epoch=294
03/14/2022 03:04:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.115723 on epoch=299
03/14/2022 03:04:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 03:04:07 - INFO - __main__ - Printing 3 examples
03/14/2022 03:04:07 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/14/2022 03:04:07 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/14/2022 03:04:07 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/14/2022 03:04:07 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/14/2022 03:04:07 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/14/2022 03:04:07 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/14/2022 03:04:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 03:04:07 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:04:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 03:04:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 03:04:07 - INFO - __main__ - Printing 3 examples
03/14/2022 03:04:07 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/14/2022 03:04:07 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/14/2022 03:04:07 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/14/2022 03:04:07 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/14/2022 03:04:07 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/14/2022 03:04:07 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/14/2022 03:04:07 - INFO - __main__ - Tokenizing Input ...
03/14/2022 03:04:07 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:04:07 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 03:04:13 - INFO - __main__ - Global step 600 Train loss 0.124841 Rouge-L 0.6916853388032882 on epoch=299
03/14/2022 03:04:13 - INFO - __main__ - save last model!
03/14/2022 03:04:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 03:04:20 - INFO - __main__ - Starting training!
03/14/2022 03:04:20 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 03:04:21 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 03:04:21 - INFO - __main__ - Printing 3 examples
03/14/2022 03:04:21 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 03:04:21 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 03:04:21 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 03:04:21 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 03:04:21 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 03:04:21 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 03:04:21 - INFO - __main__ - Tokenizing Input ...
03/14/2022 03:04:23 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:04:29 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 03:22:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_100_0.0002_8_predictions.txt
03/14/2022 03:22:19 - INFO - __main__ - Rouge-L on test data: 0.7105
03/14/2022 03:22:21 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.0002, bsz=8, dev_performance=0.7190818406955295, test_performance=0.7104990658019786
03/14/2022 03:22:21 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.0001, bsz=8 ...
03/14/2022 03:22:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 03:22:22 - INFO - __main__ - Printing 3 examples
03/14/2022 03:22:22 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/14/2022 03:22:22 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/14/2022 03:22:22 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/14/2022 03:22:22 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/14/2022 03:22:22 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/14/2022 03:22:22 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/14/2022 03:22:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 03:22:22 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:22:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 03:22:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 03:22:22 - INFO - __main__ - Printing 3 examples
03/14/2022 03:22:22 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/14/2022 03:22:22 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/14/2022 03:22:22 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/14/2022 03:22:22 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/14/2022 03:22:22 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/14/2022 03:22:22 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/14/2022 03:22:22 - INFO - __main__ - Tokenizing Input ...
03/14/2022 03:22:22 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:22:22 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 03:22:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 03:22:33 - INFO - __main__ - Starting training!
03/14/2022 03:22:37 - INFO - __main__ - Step 10 Global step 10 Train loss 19.488070 on epoch=4
03/14/2022 03:22:42 - INFO - __main__ - Step 20 Global step 20 Train loss 16.196041 on epoch=9
03/14/2022 03:22:47 - INFO - __main__ - Step 30 Global step 30 Train loss 9.487062 on epoch=14
03/14/2022 03:22:52 - INFO - __main__ - Step 40 Global step 40 Train loss 7.446798 on epoch=19
03/14/2022 03:22:57 - INFO - __main__ - Step 50 Global step 50 Train loss 5.142791 on epoch=24
03/14/2022 03:23:09 - INFO - __main__ - Global step 50 Train loss 11.552152 Rouge-L 0.6119575284061446 on epoch=24
03/14/2022 03:23:15 - INFO - __main__ - Step 60 Global step 60 Train loss 3.496163 on epoch=29
03/14/2022 03:23:20 - INFO - __main__ - Step 70 Global step 70 Train loss 2.645323 on epoch=34
03/14/2022 03:23:25 - INFO - __main__ - Step 80 Global step 80 Train loss 2.171281 on epoch=39
03/14/2022 03:23:30 - INFO - __main__ - Step 90 Global step 90 Train loss 1.864072 on epoch=44
03/14/2022 03:23:35 - INFO - __main__ - Step 100 Global step 100 Train loss 1.578647 on epoch=49
03/14/2022 03:23:43 - INFO - __main__ - Global step 100 Train loss 2.351097 Rouge-L 0.5320347944238577 on epoch=49
03/14/2022 03:23:48 - INFO - __main__ - Step 110 Global step 110 Train loss 1.381302 on epoch=54
03/14/2022 03:23:54 - INFO - __main__ - Step 120 Global step 120 Train loss 1.288169 on epoch=59
03/14/2022 03:23:59 - INFO - __main__ - Step 130 Global step 130 Train loss 1.189465 on epoch=64
03/14/2022 03:24:04 - INFO - __main__ - Step 140 Global step 140 Train loss 1.123739 on epoch=69
03/14/2022 03:24:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.944753 on epoch=74
03/14/2022 03:24:19 - INFO - __main__ - Global step 150 Train loss 1.185486 Rouge-L 0.6125353410416633 on epoch=74
03/14/2022 03:24:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.839753 on epoch=79
03/14/2022 03:24:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.916055 on epoch=84
03/14/2022 03:24:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.821409 on epoch=89
03/14/2022 03:24:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.700144 on epoch=94
03/14/2022 03:24:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.681382 on epoch=99
03/14/2022 03:24:54 - INFO - __main__ - Global step 200 Train loss 0.791748 Rouge-L 0.591212956369239 on epoch=99
03/14/2022 03:24:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.656301 on epoch=104
03/14/2022 03:25:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.649851 on epoch=109
03/14/2022 03:25:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.562718 on epoch=114
03/14/2022 03:25:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.569336 on epoch=119
03/14/2022 03:25:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.531603 on epoch=124
03/14/2022 03:25:25 - INFO - __main__ - Global step 250 Train loss 0.593962 Rouge-L 0.6292965494044774 on epoch=124
03/14/2022 03:25:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.499289 on epoch=129
03/14/2022 03:25:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.515680 on epoch=134
03/14/2022 03:25:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.461600 on epoch=139
03/14/2022 03:25:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.463558 on epoch=144
03/14/2022 03:25:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.415348 on epoch=149
03/14/2022 03:25:57 - INFO - __main__ - Global step 300 Train loss 0.471095 Rouge-L 0.5957574926002717 on epoch=149
03/14/2022 03:26:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.457453 on epoch=154
03/14/2022 03:26:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.384948 on epoch=159
03/14/2022 03:26:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.381704 on epoch=164
03/14/2022 03:26:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.388833 on epoch=169
03/14/2022 03:26:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.357046 on epoch=174
03/14/2022 03:26:28 - INFO - __main__ - Global step 350 Train loss 0.393997 Rouge-L 0.6143508698713195 on epoch=174
03/14/2022 03:26:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.364324 on epoch=179
03/14/2022 03:26:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.349610 on epoch=184
03/14/2022 03:26:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.296691 on epoch=189
03/14/2022 03:26:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.296749 on epoch=194
03/14/2022 03:26:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.304384 on epoch=199
03/14/2022 03:27:00 - INFO - __main__ - Global step 400 Train loss 0.322352 Rouge-L 0.6094772837492106 on epoch=199
03/14/2022 03:27:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.293350 on epoch=204
03/14/2022 03:27:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.270970 on epoch=209
03/14/2022 03:27:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.264580 on epoch=214
03/14/2022 03:27:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.267199 on epoch=219
03/14/2022 03:27:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.251289 on epoch=224
03/14/2022 03:27:32 - INFO - __main__ - Global step 450 Train loss 0.269478 Rouge-L 0.582063413405824 on epoch=224
03/14/2022 03:27:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.233837 on epoch=229
03/14/2022 03:27:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.240065 on epoch=234
03/14/2022 03:27:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.231372 on epoch=239
03/14/2022 03:27:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.196426 on epoch=244
03/14/2022 03:27:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.217907 on epoch=249
03/14/2022 03:28:03 - INFO - __main__ - Global step 500 Train loss 0.223921 Rouge-L 0.601068379658065 on epoch=249
03/14/2022 03:28:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.216485 on epoch=254
03/14/2022 03:28:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.196824 on epoch=259
03/14/2022 03:28:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.198934 on epoch=264
03/14/2022 03:28:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.207408 on epoch=269
03/14/2022 03:28:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.188864 on epoch=274
03/14/2022 03:28:34 - INFO - __main__ - Global step 550 Train loss 0.201703 Rouge-L 0.6233745204351857 on epoch=274
03/14/2022 03:28:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.186143 on epoch=279
03/14/2022 03:28:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.174566 on epoch=284
03/14/2022 03:28:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.167915 on epoch=289
03/14/2022 03:28:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.184398 on epoch=294
03/14/2022 03:29:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.149945 on epoch=299
03/14/2022 03:29:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 03:29:01 - INFO - __main__ - Printing 3 examples
03/14/2022 03:29:01 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
03/14/2022 03:29:01 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
03/14/2022 03:29:01 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
03/14/2022 03:29:01 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
03/14/2022 03:29:01 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
03/14/2022 03:29:01 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
03/14/2022 03:29:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 03:29:01 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:29:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 03:29:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 03:29:01 - INFO - __main__ - Printing 3 examples
03/14/2022 03:29:01 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
03/14/2022 03:29:01 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
03/14/2022 03:29:01 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
03/14/2022 03:29:01 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
03/14/2022 03:29:01 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
03/14/2022 03:29:01 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
03/14/2022 03:29:01 - INFO - __main__ - Tokenizing Input ...
03/14/2022 03:29:01 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:29:01 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 03:29:07 - INFO - __main__ - Global step 600 Train loss 0.172593 Rouge-L 0.5993706966297065 on epoch=299
03/14/2022 03:29:07 - INFO - __main__ - save last model!
03/14/2022 03:29:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 03:29:12 - INFO - __main__ - Starting training!
03/14/2022 03:29:14 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 03:29:15 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 03:29:15 - INFO - __main__ - Printing 3 examples
03/14/2022 03:29:15 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 03:29:15 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 03:29:15 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 03:29:15 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 03:29:15 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 03:29:15 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 03:29:15 - INFO - __main__ - Tokenizing Input ...
03/14/2022 03:29:17 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:29:23 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 03:47:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_100_0.0001_8_predictions.txt
03/14/2022 03:47:34 - INFO - __main__ - Rouge-L on test data: 0.6088
03/14/2022 03:47:35 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.0001, bsz=8, dev_performance=0.6292965494044774, test_performance=0.6087629269802411
03/14/2022 03:47:35 - INFO - __main__ - Running ... prefix=wiki_split_32_13, lr=0.0005, bsz=8 ...
03/14/2022 03:47:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 03:47:36 - INFO - __main__ - Printing 3 examples
03/14/2022 03:47:36 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
03/14/2022 03:47:36 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
03/14/2022 03:47:36 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
03/14/2022 03:47:36 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
03/14/2022 03:47:36 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
03/14/2022 03:47:36 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
03/14/2022 03:47:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 03:47:36 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:47:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 03:47:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 03:47:36 - INFO - __main__ - Printing 3 examples
03/14/2022 03:47:36 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
03/14/2022 03:47:36 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
03/14/2022 03:47:36 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
03/14/2022 03:47:36 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
03/14/2022 03:47:36 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
03/14/2022 03:47:36 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
03/14/2022 03:47:36 - INFO - __main__ - Tokenizing Input ...
03/14/2022 03:47:36 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:47:36 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 03:47:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 03:47:47 - INFO - __main__ - Starting training!
03/14/2022 03:47:52 - INFO - __main__ - Step 10 Global step 10 Train loss 20.209846 on epoch=4
03/14/2022 03:47:56 - INFO - __main__ - Step 20 Global step 20 Train loss 12.670711 on epoch=9
03/14/2022 03:48:01 - INFO - __main__ - Step 30 Global step 30 Train loss 7.017684 on epoch=14
03/14/2022 03:48:06 - INFO - __main__ - Step 40 Global step 40 Train loss 5.868890 on epoch=19
03/14/2022 03:48:11 - INFO - __main__ - Step 50 Global step 50 Train loss 4.721710 on epoch=24
03/14/2022 03:48:25 - INFO - __main__ - Global step 50 Train loss 10.097769 Rouge-L 0.549172228981849 on epoch=24
03/14/2022 03:48:31 - INFO - __main__ - Step 60 Global step 60 Train loss 3.381677 on epoch=29
03/14/2022 03:48:35 - INFO - __main__ - Step 70 Global step 70 Train loss 1.617488 on epoch=34
03/14/2022 03:48:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.852333 on epoch=39
03/14/2022 03:48:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.659170 on epoch=44
03/14/2022 03:48:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.446376 on epoch=49
03/14/2022 03:48:57 - INFO - __main__ - Global step 100 Train loss 1.391409 Rouge-L 0.6620572537520188 on epoch=49
03/14/2022 03:49:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.371715 on epoch=54
03/14/2022 03:49:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.270872 on epoch=59
03/14/2022 03:49:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.256517 on epoch=64
03/14/2022 03:49:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.205714 on epoch=69
03/14/2022 03:49:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.200433 on epoch=74
03/14/2022 03:49:30 - INFO - __main__ - Global step 150 Train loss 0.261050 Rouge-L 0.7124163736588306 on epoch=74
03/14/2022 03:49:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.222869 on epoch=79
03/14/2022 03:49:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.198176 on epoch=84
03/14/2022 03:49:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.201303 on epoch=89
03/14/2022 03:49:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.211800 on epoch=94
03/14/2022 03:49:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.166253 on epoch=99
03/14/2022 03:49:59 - INFO - __main__ - Global step 200 Train loss 0.200080 Rouge-L 0.6023849957136089 on epoch=99
03/14/2022 03:50:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.160394 on epoch=104
03/14/2022 03:50:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.164033 on epoch=109
03/14/2022 03:50:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.137955 on epoch=114
03/14/2022 03:50:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.155506 on epoch=119
03/14/2022 03:50:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.141111 on epoch=124
03/14/2022 03:50:29 - INFO - __main__ - Global step 250 Train loss 0.151800 Rouge-L 0.5805929914931667 on epoch=124
03/14/2022 03:50:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.145425 on epoch=129
03/14/2022 03:50:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.135446 on epoch=134
03/14/2022 03:50:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.127620 on epoch=139
03/14/2022 03:50:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.110336 on epoch=144
03/14/2022 03:50:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.119228 on epoch=149
03/14/2022 03:51:03 - INFO - __main__ - Global step 300 Train loss 0.127611 Rouge-L 0.6032908562235012 on epoch=149
03/14/2022 03:51:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.120418 on epoch=154
03/14/2022 03:51:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.109111 on epoch=159
03/14/2022 03:51:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.115735 on epoch=164
03/14/2022 03:51:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.120489 on epoch=169
03/14/2022 03:51:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.109879 on epoch=174
03/14/2022 03:51:37 - INFO - __main__ - Global step 350 Train loss 0.115127 Rouge-L 0.5938027584301517 on epoch=174
03/14/2022 03:51:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.093719 on epoch=179
03/14/2022 03:51:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.093665 on epoch=184
03/14/2022 03:51:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.092789 on epoch=189
03/14/2022 03:51:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.186487 on epoch=194
03/14/2022 03:52:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.635428 on epoch=199
03/14/2022 03:52:07 - INFO - __main__ - Global step 400 Train loss 0.220417 Rouge-L 0.17000782802399922 on epoch=199
03/14/2022 03:52:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.256422 on epoch=204
03/14/2022 03:52:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.181148 on epoch=209
03/14/2022 03:52:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.124738 on epoch=214
03/14/2022 03:52:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.106853 on epoch=219
03/14/2022 03:52:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.101507 on epoch=224
03/14/2022 03:52:38 - INFO - __main__ - Global step 450 Train loss 0.154133 Rouge-L 0.10689522295275661 on epoch=224
03/14/2022 03:52:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.092008 on epoch=229
03/14/2022 03:52:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.083257 on epoch=234
03/14/2022 03:52:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.094924 on epoch=239
03/14/2022 03:52:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.120626 on epoch=244
03/14/2022 03:53:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.086136 on epoch=249
03/14/2022 03:53:08 - INFO - __main__ - Global step 500 Train loss 0.095390 Rouge-L 0.10182452713523268 on epoch=249
03/14/2022 03:53:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.080096 on epoch=254
03/14/2022 03:53:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.080583 on epoch=259
03/14/2022 03:53:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.080486 on epoch=264
03/14/2022 03:53:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.080471 on epoch=269
03/14/2022 03:53:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.082524 on epoch=274
03/14/2022 03:53:39 - INFO - __main__ - Global step 550 Train loss 0.080832 Rouge-L 0.11000054071327944 on epoch=274
03/14/2022 03:53:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.078173 on epoch=279
03/14/2022 03:53:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.083144 on epoch=284
03/14/2022 03:53:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.083388 on epoch=289
03/14/2022 03:53:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.084887 on epoch=294
03/14/2022 03:54:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.079326 on epoch=299
03/14/2022 03:54:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 03:54:06 - INFO - __main__ - Printing 3 examples
03/14/2022 03:54:06 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
03/14/2022 03:54:06 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
03/14/2022 03:54:06 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
03/14/2022 03:54:06 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
03/14/2022 03:54:06 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
03/14/2022 03:54:06 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
03/14/2022 03:54:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 03:54:06 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:54:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 03:54:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 03:54:06 - INFO - __main__ - Printing 3 examples
03/14/2022 03:54:06 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
03/14/2022 03:54:06 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
03/14/2022 03:54:06 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
03/14/2022 03:54:06 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
03/14/2022 03:54:06 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
03/14/2022 03:54:06 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
03/14/2022 03:54:06 - INFO - __main__ - Tokenizing Input ...
03/14/2022 03:54:06 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:54:06 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 03:54:09 - INFO - __main__ - Global step 600 Train loss 0.081784 Rouge-L 0.10615230099741549 on epoch=299
03/14/2022 03:54:09 - INFO - __main__ - save last model!
03/14/2022 03:54:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 03:54:16 - INFO - __main__ - Starting training!
03/14/2022 03:54:16 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 03:54:17 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 03:54:17 - INFO - __main__ - Printing 3 examples
03/14/2022 03:54:17 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 03:54:17 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 03:54:17 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 03:54:17 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 03:54:17 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 03:54:17 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 03:54:17 - INFO - __main__ - Tokenizing Input ...
03/14/2022 03:54:19 - INFO - __main__ - Tokenizing Output ...
03/14/2022 03:54:25 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 04:20:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_13_0.0005_8_predictions.txt
03/14/2022 04:20:10 - INFO - __main__ - Rouge-L on test data: 0.6870
03/14/2022 04:20:10 - INFO - __main__ - prefix=wiki_split_32_13, lr=0.0005, bsz=8, dev_performance=0.7124163736588306, test_performance=0.686961293458676
03/14/2022 04:20:10 - INFO - __main__ - Running ... prefix=wiki_split_32_13, lr=0.0003, bsz=8 ...
03/14/2022 04:20:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 04:20:11 - INFO - __main__ - Printing 3 examples
03/14/2022 04:20:11 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
03/14/2022 04:20:11 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
03/14/2022 04:20:11 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
03/14/2022 04:20:11 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
03/14/2022 04:20:11 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
03/14/2022 04:20:11 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
03/14/2022 04:20:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 04:20:11 - INFO - __main__ - Tokenizing Output ...
03/14/2022 04:20:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 04:20:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 04:20:11 - INFO - __main__ - Printing 3 examples
03/14/2022 04:20:11 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
03/14/2022 04:20:11 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
03/14/2022 04:20:11 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
03/14/2022 04:20:11 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
03/14/2022 04:20:11 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
03/14/2022 04:20:11 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
03/14/2022 04:20:11 - INFO - __main__ - Tokenizing Input ...
03/14/2022 04:20:11 - INFO - __main__ - Tokenizing Output ...
03/14/2022 04:20:11 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 04:20:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 04:20:24 - INFO - __main__ - Starting training!
03/14/2022 04:20:28 - INFO - __main__ - Step 10 Global step 10 Train loss 20.459198 on epoch=4
03/14/2022 04:20:32 - INFO - __main__ - Step 20 Global step 20 Train loss 14.237564 on epoch=9
03/14/2022 04:20:37 - INFO - __main__ - Step 30 Global step 30 Train loss 7.919997 on epoch=14
03/14/2022 04:20:42 - INFO - __main__ - Step 40 Global step 40 Train loss 6.277726 on epoch=19
03/14/2022 04:20:47 - INFO - __main__ - Step 50 Global step 50 Train loss 5.383982 on epoch=24
03/14/2022 04:20:59 - INFO - __main__ - Global step 50 Train loss 10.855694 Rouge-L 0.42617316628793217 on epoch=24
03/14/2022 04:21:05 - INFO - __main__ - Step 60 Global step 60 Train loss 4.444592 on epoch=29
03/14/2022 04:21:10 - INFO - __main__ - Step 70 Global step 70 Train loss 3.295568 on epoch=34
03/14/2022 04:21:15 - INFO - __main__ - Step 80 Global step 80 Train loss 1.833231 on epoch=39
03/14/2022 04:21:19 - INFO - __main__ - Step 90 Global step 90 Train loss 1.177541 on epoch=44
03/14/2022 04:21:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.915676 on epoch=49
03/14/2022 04:21:32 - INFO - __main__ - Global step 100 Train loss 2.333322 Rouge-L 0.5268047116401867 on epoch=49
03/14/2022 04:21:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.667430 on epoch=54
03/14/2022 04:21:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.554277 on epoch=59
03/14/2022 04:21:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.540522 on epoch=64
03/14/2022 04:21:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.448078 on epoch=69
03/14/2022 04:21:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.984194 on epoch=74
03/14/2022 04:22:07 - INFO - __main__ - Global step 150 Train loss 0.638900 Rouge-L 0.06759813344354895 on epoch=74
03/14/2022 04:22:12 - INFO - __main__ - Step 160 Global step 160 Train loss 1.620425 on epoch=79
03/14/2022 04:22:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.422065 on epoch=84
03/14/2022 04:22:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.371661 on epoch=89
03/14/2022 04:22:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.291911 on epoch=94
03/14/2022 04:22:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.246909 on epoch=99
03/14/2022 04:22:38 - INFO - __main__ - Global step 200 Train loss 0.590594 Rouge-L 0.46394803335183554 on epoch=99
03/14/2022 04:22:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.264087 on epoch=104
03/14/2022 04:22:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.232451 on epoch=109
03/14/2022 04:22:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.256818 on epoch=114
03/14/2022 04:22:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.215417 on epoch=119
03/14/2022 04:23:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.250720 on epoch=124
03/14/2022 04:23:08 - INFO - __main__ - Global step 250 Train loss 0.243898 Rouge-L 0.44687308401015624 on epoch=124
03/14/2022 04:23:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.208402 on epoch=129
03/14/2022 04:23:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.193940 on epoch=134
03/14/2022 04:23:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.184824 on epoch=139
03/14/2022 04:23:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.183666 on epoch=144
03/14/2022 04:23:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.136462 on epoch=149
03/14/2022 04:23:41 - INFO - __main__ - Global step 300 Train loss 0.181459 Rouge-L 0.47635004389483526 on epoch=149
03/14/2022 04:23:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.186743 on epoch=154
03/14/2022 04:23:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.169168 on epoch=159
03/14/2022 04:23:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.182948 on epoch=164
03/14/2022 04:24:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.172669 on epoch=169
03/14/2022 04:24:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.149541 on epoch=174
03/14/2022 04:24:13 - INFO - __main__ - Global step 350 Train loss 0.172214 Rouge-L 0.41968297536910626 on epoch=174
03/14/2022 04:24:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.167695 on epoch=179
03/14/2022 04:24:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.160594 on epoch=184
03/14/2022 04:24:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.141290 on epoch=189
03/14/2022 04:24:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.152633 on epoch=194
03/14/2022 04:24:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.136034 on epoch=199
03/14/2022 04:24:46 - INFO - __main__ - Global step 400 Train loss 0.151649 Rouge-L 0.39783967705876466 on epoch=199
03/14/2022 04:24:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.134103 on epoch=204
03/14/2022 04:24:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.124502 on epoch=209
03/14/2022 04:25:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.121716 on epoch=214
03/14/2022 04:25:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.123126 on epoch=219
03/14/2022 04:25:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.112152 on epoch=224
03/14/2022 04:25:19 - INFO - __main__ - Global step 450 Train loss 0.123120 Rouge-L 0.4389022199467504 on epoch=224
03/14/2022 04:25:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.134192 on epoch=229
03/14/2022 04:25:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.116460 on epoch=234
03/14/2022 04:25:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.117890 on epoch=239
03/14/2022 04:25:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.106358 on epoch=244
03/14/2022 04:25:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.113639 on epoch=249
03/14/2022 04:25:51 - INFO - __main__ - Global step 500 Train loss 0.117708 Rouge-L 0.44537976582531297 on epoch=249
03/14/2022 04:25:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.111626 on epoch=254
03/14/2022 04:26:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.103125 on epoch=259
03/14/2022 04:26:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.105434 on epoch=264
03/14/2022 04:26:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.103414 on epoch=269
03/14/2022 04:26:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.104147 on epoch=274
03/14/2022 04:26:25 - INFO - __main__ - Global step 550 Train loss 0.105549 Rouge-L 0.4481959124374595 on epoch=274
03/14/2022 04:26:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.101201 on epoch=279
03/14/2022 04:26:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.102082 on epoch=284
03/14/2022 04:26:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.097766 on epoch=289
03/14/2022 04:26:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.107611 on epoch=294
03/14/2022 04:26:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.089137 on epoch=299
03/14/2022 04:26:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 04:26:51 - INFO - __main__ - Printing 3 examples
03/14/2022 04:26:51 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
03/14/2022 04:26:51 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
03/14/2022 04:26:51 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
03/14/2022 04:26:51 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
03/14/2022 04:26:51 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
03/14/2022 04:26:51 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
03/14/2022 04:26:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 04:26:51 - INFO - __main__ - Tokenizing Output ...
03/14/2022 04:26:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 04:26:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 04:26:51 - INFO - __main__ - Printing 3 examples
03/14/2022 04:26:51 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
03/14/2022 04:26:51 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
03/14/2022 04:26:51 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
03/14/2022 04:26:51 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
03/14/2022 04:26:51 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
03/14/2022 04:26:51 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
03/14/2022 04:26:51 - INFO - __main__ - Tokenizing Input ...
03/14/2022 04:26:51 - INFO - __main__ - Tokenizing Output ...
03/14/2022 04:26:51 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 04:27:00 - INFO - __main__ - Global step 600 Train loss 0.099559 Rouge-L 0.4208554392918043 on epoch=299
03/14/2022 04:27:00 - INFO - __main__ - save last model!
03/14/2022 04:27:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 04:27:02 - INFO - __main__ - Starting training!
03/14/2022 04:27:07 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 04:27:07 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 04:27:07 - INFO - __main__ - Printing 3 examples
03/14/2022 04:27:07 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 04:27:07 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 04:27:07 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 04:27:07 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 04:27:07 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 04:27:07 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 04:27:07 - INFO - __main__ - Tokenizing Input ...
03/14/2022 04:27:10 - INFO - __main__ - Tokenizing Output ...
03/14/2022 04:27:16 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 04:41:50 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_13_0.0003_8_predictions.txt
03/14/2022 04:41:53 - INFO - __main__ - Rouge-L on test data: 0.5261
03/14/2022 04:41:53 - INFO - __main__ - prefix=wiki_split_32_13, lr=0.0003, bsz=8, dev_performance=0.5268047116401867, test_performance=0.5260726499559192
03/14/2022 04:41:53 - INFO - __main__ - Running ... prefix=wiki_split_32_13, lr=0.0002, bsz=8 ...
03/14/2022 04:41:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 04:41:54 - INFO - __main__ - Printing 3 examples
03/14/2022 04:41:54 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
03/14/2022 04:41:54 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
03/14/2022 04:41:54 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
03/14/2022 04:41:54 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
03/14/2022 04:41:54 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
03/14/2022 04:41:54 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
03/14/2022 04:41:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 04:41:54 - INFO - __main__ - Tokenizing Output ...
03/14/2022 04:41:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 04:41:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 04:41:54 - INFO - __main__ - Printing 3 examples
03/14/2022 04:41:54 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
03/14/2022 04:41:54 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
03/14/2022 04:41:54 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
03/14/2022 04:41:54 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
03/14/2022 04:41:54 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
03/14/2022 04:41:54 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
03/14/2022 04:41:54 - INFO - __main__ - Tokenizing Input ...
03/14/2022 04:41:54 - INFO - __main__ - Tokenizing Output ...
03/14/2022 04:41:54 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 04:42:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 04:42:05 - INFO - __main__ - Starting training!
03/14/2022 04:42:09 - INFO - __main__ - Step 10 Global step 10 Train loss 20.469997 on epoch=4
03/14/2022 04:42:13 - INFO - __main__ - Step 20 Global step 20 Train loss 15.774019 on epoch=9
03/14/2022 04:42:18 - INFO - __main__ - Step 30 Global step 30 Train loss 5.616348 on epoch=14
03/14/2022 04:42:23 - INFO - __main__ - Step 40 Global step 40 Train loss 2.635936 on epoch=19
03/14/2022 04:42:28 - INFO - __main__ - Step 50 Global step 50 Train loss 1.805532 on epoch=24
03/14/2022 04:42:38 - INFO - __main__ - Global step 50 Train loss 9.260366 Rouge-L 0.48620722818534234 on epoch=24
03/14/2022 04:42:44 - INFO - __main__ - Step 60 Global step 60 Train loss 1.387913 on epoch=29
03/14/2022 04:42:49 - INFO - __main__ - Step 70 Global step 70 Train loss 1.115203 on epoch=34
03/14/2022 04:42:53 - INFO - __main__ - Step 80 Global step 80 Train loss 0.992165 on epoch=39
03/14/2022 04:42:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.808429 on epoch=44
03/14/2022 04:43:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.725837 on epoch=49
03/14/2022 04:43:08 - INFO - __main__ - Global step 100 Train loss 1.005909 Rouge-L 0.46424483609351996 on epoch=49
03/14/2022 04:43:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.712989 on epoch=54
03/14/2022 04:43:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.607157 on epoch=59
03/14/2022 04:43:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.548383 on epoch=64
03/14/2022 04:43:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.507975 on epoch=69
03/14/2022 04:43:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.463734 on epoch=74
03/14/2022 04:43:38 - INFO - __main__ - Global step 150 Train loss 0.568048 Rouge-L 0.3155896366487534 on epoch=74
03/14/2022 04:43:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.438145 on epoch=79
03/14/2022 04:43:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.385388 on epoch=84
03/14/2022 04:43:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.323588 on epoch=89
03/14/2022 04:43:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.337474 on epoch=94
03/14/2022 04:44:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.330346 on epoch=99
03/14/2022 04:44:11 - INFO - __main__ - Global step 200 Train loss 0.362988 Rouge-L 0.3400940694142518 on epoch=99
03/14/2022 04:44:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.292933 on epoch=104
03/14/2022 04:44:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.280689 on epoch=109
03/14/2022 04:44:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.231590 on epoch=114
03/14/2022 04:44:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.242697 on epoch=119
03/14/2022 04:44:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.191981 on epoch=124
03/14/2022 04:44:40 - INFO - __main__ - Global step 250 Train loss 0.247978 Rouge-L 0.6789802179382871 on epoch=124
03/14/2022 04:44:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.218471 on epoch=129
03/14/2022 04:44:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.225346 on epoch=134
03/14/2022 04:44:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.202326 on epoch=139
03/14/2022 04:45:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.245338 on epoch=144
03/14/2022 04:45:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.223496 on epoch=149
03/14/2022 04:45:11 - INFO - __main__ - Global step 300 Train loss 0.222995 Rouge-L 0.7285661487780961 on epoch=149
03/14/2022 04:45:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.200527 on epoch=154
03/14/2022 04:45:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.207433 on epoch=159
03/14/2022 04:45:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.174021 on epoch=164
03/14/2022 04:45:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.203216 on epoch=169
03/14/2022 04:45:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.476323 on epoch=174
03/14/2022 04:45:42 - INFO - __main__ - Global step 350 Train loss 0.252304 Rouge-L 0.5727986940344021 on epoch=174
03/14/2022 04:45:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.991240 on epoch=179
03/14/2022 04:45:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.195926 on epoch=184
03/14/2022 04:45:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.247084 on epoch=189
03/14/2022 04:46:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.223729 on epoch=194
03/14/2022 04:46:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.244430 on epoch=199
03/14/2022 04:46:12 - INFO - __main__ - Global step 400 Train loss 0.380482 Rouge-L 0.6821386700678131 on epoch=199
03/14/2022 04:46:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.161912 on epoch=204
03/14/2022 04:46:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.214866 on epoch=209
03/14/2022 04:46:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.152918 on epoch=214
03/14/2022 04:46:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.150038 on epoch=219
03/14/2022 04:46:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.183963 on epoch=224
03/14/2022 04:46:42 - INFO - __main__ - Global step 450 Train loss 0.172739 Rouge-L 0.6949687701904353 on epoch=224
03/14/2022 04:46:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.178292 on epoch=229
03/14/2022 04:46:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.175540 on epoch=234
03/14/2022 04:46:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.305320 on epoch=239
03/14/2022 04:47:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.229501 on epoch=244
03/14/2022 04:47:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.188079 on epoch=249
03/14/2022 04:47:12 - INFO - __main__ - Global step 500 Train loss 0.215346 Rouge-L 0.5303143535254055 on epoch=249
03/14/2022 04:47:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.204042 on epoch=254
03/14/2022 04:47:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.216822 on epoch=259
03/14/2022 04:47:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.200476 on epoch=264
03/14/2022 04:47:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.191879 on epoch=269
03/14/2022 04:47:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.143750 on epoch=274
03/14/2022 04:47:41 - INFO - __main__ - Global step 550 Train loss 0.191394 Rouge-L 0.327638895060557 on epoch=274
03/14/2022 04:47:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.166068 on epoch=279
03/14/2022 04:47:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.171084 on epoch=284
03/14/2022 04:47:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.153149 on epoch=289
03/14/2022 04:48:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.154516 on epoch=294
03/14/2022 04:48:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.126839 on epoch=299
03/14/2022 04:48:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 04:48:07 - INFO - __main__ - Printing 3 examples
03/14/2022 04:48:07 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
03/14/2022 04:48:07 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
03/14/2022 04:48:07 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
03/14/2022 04:48:07 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
03/14/2022 04:48:07 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
03/14/2022 04:48:07 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
03/14/2022 04:48:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 04:48:07 - INFO - __main__ - Tokenizing Output ...
03/14/2022 04:48:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 04:48:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 04:48:07 - INFO - __main__ - Printing 3 examples
03/14/2022 04:48:07 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
03/14/2022 04:48:07 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
03/14/2022 04:48:07 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
03/14/2022 04:48:07 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
03/14/2022 04:48:07 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
03/14/2022 04:48:07 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
03/14/2022 04:48:07 - INFO - __main__ - Tokenizing Input ...
03/14/2022 04:48:07 - INFO - __main__ - Tokenizing Output ...
03/14/2022 04:48:07 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 04:48:11 - INFO - __main__ - Global step 600 Train loss 0.154331 Rouge-L 0.3192542828010353 on epoch=299
03/14/2022 04:48:11 - INFO - __main__ - save last model!
03/14/2022 04:48:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 04:48:17 - INFO - __main__ - Starting training!
03/14/2022 04:48:18 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 04:48:19 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 04:48:19 - INFO - __main__ - Printing 3 examples
03/14/2022 04:48:19 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 04:48:19 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 04:48:19 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 04:48:19 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 04:48:19 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 04:48:19 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 04:48:19 - INFO - __main__ - Tokenizing Input ...
03/14/2022 04:48:21 - INFO - __main__ - Tokenizing Output ...
03/14/2022 04:48:27 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 05:07:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_13_0.0002_8_predictions.txt
03/14/2022 05:07:39 - INFO - __main__ - Rouge-L on test data: 0.7259
03/14/2022 05:07:39 - INFO - __main__ - prefix=wiki_split_32_13, lr=0.0002, bsz=8, dev_performance=0.7285661487780961, test_performance=0.7258706108487951
03/14/2022 05:07:39 - INFO - __main__ - Running ... prefix=wiki_split_32_13, lr=0.0001, bsz=8 ...
03/14/2022 05:07:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 05:07:40 - INFO - __main__ - Printing 3 examples
03/14/2022 05:07:40 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
03/14/2022 05:07:40 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
03/14/2022 05:07:40 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
03/14/2022 05:07:40 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
03/14/2022 05:07:40 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
03/14/2022 05:07:40 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
03/14/2022 05:07:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 05:07:40 - INFO - __main__ - Tokenizing Output ...
03/14/2022 05:07:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 05:07:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 05:07:40 - INFO - __main__ - Printing 3 examples
03/14/2022 05:07:40 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
03/14/2022 05:07:40 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
03/14/2022 05:07:40 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
03/14/2022 05:07:40 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
03/14/2022 05:07:40 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
03/14/2022 05:07:40 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
03/14/2022 05:07:40 - INFO - __main__ - Tokenizing Input ...
03/14/2022 05:07:40 - INFO - __main__ - Tokenizing Output ...
03/14/2022 05:07:40 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 05:07:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 05:07:52 - INFO - __main__ - Starting training!
03/14/2022 05:07:56 - INFO - __main__ - Step 10 Global step 10 Train loss 20.728832 on epoch=4
03/14/2022 05:08:01 - INFO - __main__ - Step 20 Global step 20 Train loss 15.511333 on epoch=9
03/14/2022 05:08:06 - INFO - __main__ - Step 30 Global step 30 Train loss 9.786598 on epoch=14
03/14/2022 05:08:11 - INFO - __main__ - Step 40 Global step 40 Train loss 6.865205 on epoch=19
03/14/2022 05:08:16 - INFO - __main__ - Step 50 Global step 50 Train loss 5.001657 on epoch=24
03/14/2022 05:08:29 - INFO - __main__ - Global step 50 Train loss 11.578726 Rouge-L 0.6245235794490946 on epoch=24
03/14/2022 05:08:34 - INFO - __main__ - Step 60 Global step 60 Train loss 3.443733 on epoch=29
03/14/2022 05:08:39 - INFO - __main__ - Step 70 Global step 70 Train loss 2.984478 on epoch=34
03/14/2022 05:08:44 - INFO - __main__ - Step 80 Global step 80 Train loss 2.367554 on epoch=39
03/14/2022 05:08:49 - INFO - __main__ - Step 90 Global step 90 Train loss 2.011478 on epoch=44
03/14/2022 05:08:54 - INFO - __main__ - Step 100 Global step 100 Train loss 1.767211 on epoch=49
03/14/2022 05:09:04 - INFO - __main__ - Global step 100 Train loss 2.514891 Rouge-L 0.5894589469968902 on epoch=49
03/14/2022 05:09:09 - INFO - __main__ - Step 110 Global step 110 Train loss 1.478453 on epoch=54
03/14/2022 05:09:14 - INFO - __main__ - Step 120 Global step 120 Train loss 1.353685 on epoch=59
03/14/2022 05:09:19 - INFO - __main__ - Step 130 Global step 130 Train loss 1.221516 on epoch=64
03/14/2022 05:09:24 - INFO - __main__ - Step 140 Global step 140 Train loss 1.150048 on epoch=69
03/14/2022 05:09:28 - INFO - __main__ - Step 150 Global step 150 Train loss 1.044864 on epoch=74
03/14/2022 05:09:35 - INFO - __main__ - Global step 150 Train loss 1.249713 Rouge-L 0.6329604166878977 on epoch=74
03/14/2022 05:09:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.928086 on epoch=79
03/14/2022 05:09:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.873044 on epoch=84
03/14/2022 05:09:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.799707 on epoch=89
03/14/2022 05:09:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.803498 on epoch=94
03/14/2022 05:10:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.701003 on epoch=99
03/14/2022 05:10:10 - INFO - __main__ - Global step 200 Train loss 0.821068 Rouge-L 0.7056254190502042 on epoch=99
03/14/2022 05:10:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.667014 on epoch=104
03/14/2022 05:10:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.676320 on epoch=109
03/14/2022 05:10:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.586801 on epoch=114
03/14/2022 05:10:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.612253 on epoch=119
03/14/2022 05:10:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.525967 on epoch=124
03/14/2022 05:10:44 - INFO - __main__ - Global step 250 Train loss 0.613671 Rouge-L 0.6860830003894421 on epoch=124
03/14/2022 05:10:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.530227 on epoch=129
03/14/2022 05:10:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.519805 on epoch=134
03/14/2022 05:10:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.460815 on epoch=139
03/14/2022 05:11:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.434340 on epoch=144
03/14/2022 05:11:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.442860 on epoch=149
03/14/2022 05:11:15 - INFO - __main__ - Global step 300 Train loss 0.477609 Rouge-L 0.6950767795047297 on epoch=149
03/14/2022 05:11:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.447603 on epoch=154
03/14/2022 05:11:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.954607 on epoch=159
03/14/2022 05:11:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.811576 on epoch=164
03/14/2022 05:11:35 - INFO - __main__ - Step 340 Global step 340 Train loss 1.507350 on epoch=169
03/14/2022 05:11:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.998150 on epoch=174
03/14/2022 05:11:45 - INFO - __main__ - Global step 350 Train loss 0.943857 Rouge-L 0.6523554257333377 on epoch=174
03/14/2022 05:11:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.435221 on epoch=179
03/14/2022 05:11:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.421934 on epoch=184
03/14/2022 05:12:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.537817 on epoch=189
03/14/2022 05:12:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.368864 on epoch=194
03/14/2022 05:12:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.515044 on epoch=199
03/14/2022 05:12:19 - INFO - __main__ - Global step 400 Train loss 0.455776 Rouge-L 0.6833359853758116 on epoch=199
03/14/2022 05:12:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.523536 on epoch=204
03/14/2022 05:12:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.597187 on epoch=209
03/14/2022 05:12:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.353222 on epoch=214
03/14/2022 05:12:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.327496 on epoch=219
03/14/2022 05:12:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.369809 on epoch=224
03/14/2022 05:12:49 - INFO - __main__ - Global step 450 Train loss 0.434250 Rouge-L 0.6088320597241204 on epoch=224
03/14/2022 05:12:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.356376 on epoch=229
03/14/2022 05:12:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.330678 on epoch=234
03/14/2022 05:13:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.320589 on epoch=239
03/14/2022 05:13:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.264354 on epoch=244
03/14/2022 05:13:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.251595 on epoch=249
03/14/2022 05:13:20 - INFO - __main__ - Global step 500 Train loss 0.304718 Rouge-L 0.6116962035469606 on epoch=249
03/14/2022 05:13:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.274274 on epoch=254
03/14/2022 05:13:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.240449 on epoch=259
03/14/2022 05:13:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.253933 on epoch=264
03/14/2022 05:13:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.260772 on epoch=269
03/14/2022 05:13:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.269401 on epoch=274
03/14/2022 05:13:50 - INFO - __main__ - Global step 550 Train loss 0.259766 Rouge-L 0.6152104386112023 on epoch=274
03/14/2022 05:13:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.290466 on epoch=279
03/14/2022 05:14:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.212069 on epoch=284
03/14/2022 05:14:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.215283 on epoch=289
03/14/2022 05:14:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.244637 on epoch=294
03/14/2022 05:14:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.197084 on epoch=299
03/14/2022 05:14:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 05:14:16 - INFO - __main__ - Printing 3 examples
03/14/2022 05:14:16 - INFO - __main__ -  [wiki_split] sentence 1: As a screen writer and a newspaper correspondent , he became intensely interested in Israel .  [SEP] sentence 2:  The Persuasion Explosion , Art Stevens , 1985 This led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 .
03/14/2022 05:14:16 - INFO - __main__ - ["As a screen writer and a newspaper correspondent , he became intensely interested in Israel which led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 ."]
03/14/2022 05:14:16 - INFO - __main__ -  [wiki_split] sentence 1: Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an Australian politician .  [SEP] sentence 2:  He was an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton .
03/14/2022 05:14:16 - INFO - __main__ - ["Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton ."]
03/14/2022 05:14:16 - INFO - __main__ -  [wiki_split] sentence 1: He was not a candidate for reelection in 1894 to the Fifty - fourth Congress .  [SEP] sentence 2:  He resumed the practice of his profession .
03/14/2022 05:14:16 - INFO - __main__ - ['He was not a candidate for reelection in 1894 to the Fifty - fourth Congress , and instead resumed the practice of his profession .']
03/14/2022 05:14:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 05:14:16 - INFO - __main__ - Tokenizing Output ...
03/14/2022 05:14:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 05:14:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 05:14:16 - INFO - __main__ - Printing 3 examples
03/14/2022 05:14:16 - INFO - __main__ -  [wiki_split] sentence 1: The recovery efforts are hampered especially by low fertility of the remaining birds .  [SEP] sentence 2:  Genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .
03/14/2022 05:14:16 - INFO - __main__ - ['The recovery efforts are hampered especially by low fertility of the remaining birds ; genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .']
03/14/2022 05:14:16 - INFO - __main__ -  [wiki_split] sentence 1: It is depicted as Kalasha with eye on both sides .  [SEP] sentence 2:  It has a core made of a fully blossomed lotus .
03/14/2022 05:14:16 - INFO - __main__ - ['It is depicted as a Kalasha with an eye on both sides and a core made of a fully blossomed lotus .']
03/14/2022 05:14:16 - INFO - __main__ -  [wiki_split] sentence 1: In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre ( ORC ) .  [SEP] sentence 2:  ORC focuses on research on Rhinos , Lions , and carrying capacity of the reserves .
03/14/2022 05:14:16 - INFO - __main__ - ['In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre , whicg focuses on research on rhinos , lions , and carrying capacity of the reserves .']
03/14/2022 05:14:16 - INFO - __main__ - Tokenizing Input ...
03/14/2022 05:14:16 - INFO - __main__ - Tokenizing Output ...
03/14/2022 05:14:17 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 05:14:21 - INFO - __main__ - Global step 600 Train loss 0.231908 Rouge-L 0.5963358037165476 on epoch=299
03/14/2022 05:14:21 - INFO - __main__ - save last model!
03/14/2022 05:14:28 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 05:14:28 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 05:14:28 - INFO - __main__ - Printing 3 examples
03/14/2022 05:14:28 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 05:14:28 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 05:14:28 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 05:14:28 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 05:14:28 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 05:14:28 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 05:14:28 - INFO - __main__ - Tokenizing Input ...
03/14/2022 05:14:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 05:14:29 - INFO - __main__ - Starting training!
03/14/2022 05:14:31 - INFO - __main__ - Tokenizing Output ...
03/14/2022 05:14:36 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 05:39:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_13_0.0001_8_predictions.txt
03/14/2022 05:39:32 - INFO - __main__ - Rouge-L on test data: 0.6736
03/14/2022 05:39:32 - INFO - __main__ - prefix=wiki_split_32_13, lr=0.0001, bsz=8, dev_performance=0.7056254190502042, test_performance=0.6735876686851182
03/14/2022 05:39:32 - INFO - __main__ - Running ... prefix=wiki_split_32_21, lr=0.0005, bsz=8 ...
03/14/2022 05:39:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 05:39:33 - INFO - __main__ - Printing 3 examples
03/14/2022 05:39:33 - INFO - __main__ -  [wiki_split] sentence 1: As a screen writer and a newspaper correspondent , he became intensely interested in Israel .  [SEP] sentence 2:  The Persuasion Explosion , Art Stevens , 1985 This led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 .
03/14/2022 05:39:33 - INFO - __main__ - ["As a screen writer and a newspaper correspondent , he became intensely interested in Israel which led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 ."]
03/14/2022 05:39:33 - INFO - __main__ -  [wiki_split] sentence 1: Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an Australian politician .  [SEP] sentence 2:  He was an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton .
03/14/2022 05:39:33 - INFO - __main__ - ["Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton ."]
03/14/2022 05:39:33 - INFO - __main__ -  [wiki_split] sentence 1: He was not a candidate for reelection in 1894 to the Fifty - fourth Congress .  [SEP] sentence 2:  He resumed the practice of his profession .
03/14/2022 05:39:33 - INFO - __main__ - ['He was not a candidate for reelection in 1894 to the Fifty - fourth Congress , and instead resumed the practice of his profession .']
03/14/2022 05:39:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 05:39:33 - INFO - __main__ - Tokenizing Output ...
03/14/2022 05:39:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 05:39:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 05:39:33 - INFO - __main__ - Printing 3 examples
03/14/2022 05:39:33 - INFO - __main__ -  [wiki_split] sentence 1: The recovery efforts are hampered especially by low fertility of the remaining birds .  [SEP] sentence 2:  Genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .
03/14/2022 05:39:33 - INFO - __main__ - ['The recovery efforts are hampered especially by low fertility of the remaining birds ; genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .']
03/14/2022 05:39:33 - INFO - __main__ -  [wiki_split] sentence 1: It is depicted as Kalasha with eye on both sides .  [SEP] sentence 2:  It has a core made of a fully blossomed lotus .
03/14/2022 05:39:33 - INFO - __main__ - ['It is depicted as a Kalasha with an eye on both sides and a core made of a fully blossomed lotus .']
03/14/2022 05:39:33 - INFO - __main__ -  [wiki_split] sentence 1: In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre ( ORC ) .  [SEP] sentence 2:  ORC focuses on research on Rhinos , Lions , and carrying capacity of the reserves .
03/14/2022 05:39:33 - INFO - __main__ - ['In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre , whicg focuses on research on rhinos , lions , and carrying capacity of the reserves .']
03/14/2022 05:39:33 - INFO - __main__ - Tokenizing Input ...
03/14/2022 05:39:33 - INFO - __main__ - Tokenizing Output ...
03/14/2022 05:39:33 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 05:39:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 05:39:45 - INFO - __main__ - Starting training!
03/14/2022 05:39:49 - INFO - __main__ - Step 10 Global step 10 Train loss 20.023966 on epoch=4
03/14/2022 05:39:55 - INFO - __main__ - Step 20 Global step 20 Train loss 7.618102 on epoch=9
03/14/2022 05:40:00 - INFO - __main__ - Step 30 Global step 30 Train loss 2.023321 on epoch=14
03/14/2022 05:40:05 - INFO - __main__ - Step 40 Global step 40 Train loss 1.037679 on epoch=19
03/14/2022 05:40:09 - INFO - __main__ - Step 50 Global step 50 Train loss 0.721308 on epoch=24
03/14/2022 05:40:13 - INFO - __main__ - Global step 50 Train loss 6.284875 Rouge-L 0.5771177703616565 on epoch=24
03/14/2022 05:40:19 - INFO - __main__ - Step 60 Global step 60 Train loss 0.545872 on epoch=29
03/14/2022 05:40:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.448578 on epoch=34
03/14/2022 05:40:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.375569 on epoch=39
03/14/2022 05:40:33 - INFO - __main__ - Step 90 Global step 90 Train loss 0.307590 on epoch=44
03/14/2022 05:40:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.270813 on epoch=49
03/14/2022 05:40:44 - INFO - __main__ - Global step 100 Train loss 0.389685 Rouge-L 0.6943437396419717 on epoch=49
03/14/2022 05:40:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.242282 on epoch=54
03/14/2022 05:40:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.215476 on epoch=59
03/14/2022 05:40:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.196271 on epoch=64
03/14/2022 05:41:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.171304 on epoch=69
03/14/2022 05:41:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.181525 on epoch=74
03/14/2022 05:41:15 - INFO - __main__ - Global step 150 Train loss 0.201372 Rouge-L 0.7034863296663572 on epoch=74
03/14/2022 05:41:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.174062 on epoch=79
03/14/2022 05:41:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.153013 on epoch=84
03/14/2022 05:41:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.140542 on epoch=89
03/14/2022 05:41:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.138374 on epoch=94
03/14/2022 05:41:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.151808 on epoch=99
03/14/2022 05:41:46 - INFO - __main__ - Global step 200 Train loss 0.151560 Rouge-L 0.7310683819506818 on epoch=99
03/14/2022 05:41:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.147570 on epoch=104
03/14/2022 05:41:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.124294 on epoch=109
03/14/2022 05:42:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.126316 on epoch=114
03/14/2022 05:42:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.111323 on epoch=119
03/14/2022 05:42:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.119406 on epoch=124
03/14/2022 05:42:19 - INFO - __main__ - Global step 250 Train loss 0.125782 Rouge-L 0.707708728349058 on epoch=124
03/14/2022 05:42:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.111898 on epoch=129
03/14/2022 05:42:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.110872 on epoch=134
03/14/2022 05:42:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.152175 on epoch=139
03/14/2022 05:42:38 - INFO - __main__ - Step 290 Global step 290 Train loss 1.280095 on epoch=144
03/14/2022 05:42:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.162161 on epoch=149
03/14/2022 05:42:49 - INFO - __main__ - Global step 300 Train loss 0.363440 Rouge-L 0.6940951616979611 on epoch=149
03/14/2022 05:42:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.281597 on epoch=154
03/14/2022 05:42:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.107302 on epoch=159
03/14/2022 05:43:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.095916 on epoch=164
03/14/2022 05:43:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.100520 on epoch=169
03/14/2022 05:43:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.088313 on epoch=174
03/14/2022 05:43:20 - INFO - __main__ - Global step 350 Train loss 0.134730 Rouge-L 0.7367141870989464 on epoch=174
03/14/2022 05:43:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.090399 on epoch=179
03/14/2022 05:43:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.084228 on epoch=184
03/14/2022 05:43:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.090999 on epoch=189
03/14/2022 05:43:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.085828 on epoch=194
03/14/2022 05:43:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.083427 on epoch=199
03/14/2022 05:43:50 - INFO - __main__ - Global step 400 Train loss 0.086976 Rouge-L 0.6984149941090272 on epoch=199
03/14/2022 05:43:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.078656 on epoch=204
03/14/2022 05:44:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.085912 on epoch=209
03/14/2022 05:44:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.079123 on epoch=214
03/14/2022 05:44:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.077978 on epoch=219
03/14/2022 05:44:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.074030 on epoch=224
03/14/2022 05:44:24 - INFO - __main__ - Global step 450 Train loss 0.079140 Rouge-L 0.7268780661354701 on epoch=224
03/14/2022 05:44:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.080584 on epoch=229
03/14/2022 05:44:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.071814 on epoch=234
03/14/2022 05:44:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.075434 on epoch=239
03/14/2022 05:44:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.082638 on epoch=244
03/14/2022 05:44:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.092489 on epoch=249
03/14/2022 05:44:51 - INFO - __main__ - Global step 500 Train loss 0.080592 Rouge-L 0.5436860936865882 on epoch=249
03/14/2022 05:44:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.101967 on epoch=254
03/14/2022 05:45:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.080717 on epoch=259
03/14/2022 05:45:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.077492 on epoch=264
03/14/2022 05:45:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.094061 on epoch=269
03/14/2022 05:45:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.243242 on epoch=274
03/14/2022 05:45:23 - INFO - __main__ - Global step 550 Train loss 0.119496 Rouge-L 0.6458933165068137 on epoch=274
03/14/2022 05:45:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.301200 on epoch=279
03/14/2022 05:45:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.154821 on epoch=284
03/14/2022 05:45:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.125334 on epoch=289
03/14/2022 05:45:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.127110 on epoch=294
03/14/2022 05:45:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.089780 on epoch=299
03/14/2022 05:45:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 05:45:49 - INFO - __main__ - Printing 3 examples
03/14/2022 05:45:49 - INFO - __main__ -  [wiki_split] sentence 1: As a screen writer and a newspaper correspondent , he became intensely interested in Israel .  [SEP] sentence 2:  The Persuasion Explosion , Art Stevens , 1985 This led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 .
03/14/2022 05:45:49 - INFO - __main__ - ["As a screen writer and a newspaper correspondent , he became intensely interested in Israel which led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 ."]
03/14/2022 05:45:49 - INFO - __main__ -  [wiki_split] sentence 1: Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an Australian politician .  [SEP] sentence 2:  He was an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton .
03/14/2022 05:45:49 - INFO - __main__ - ["Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton ."]
03/14/2022 05:45:49 - INFO - __main__ -  [wiki_split] sentence 1: He was not a candidate for reelection in 1894 to the Fifty - fourth Congress .  [SEP] sentence 2:  He resumed the practice of his profession .
03/14/2022 05:45:49 - INFO - __main__ - ['He was not a candidate for reelection in 1894 to the Fifty - fourth Congress , and instead resumed the practice of his profession .']
03/14/2022 05:45:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 05:45:49 - INFO - __main__ - Tokenizing Output ...
03/14/2022 05:45:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 05:45:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 05:45:49 - INFO - __main__ - Printing 3 examples
03/14/2022 05:45:49 - INFO - __main__ -  [wiki_split] sentence 1: The recovery efforts are hampered especially by low fertility of the remaining birds .  [SEP] sentence 2:  Genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .
03/14/2022 05:45:49 - INFO - __main__ - ['The recovery efforts are hampered especially by low fertility of the remaining birds ; genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .']
03/14/2022 05:45:49 - INFO - __main__ -  [wiki_split] sentence 1: It is depicted as Kalasha with eye on both sides .  [SEP] sentence 2:  It has a core made of a fully blossomed lotus .
03/14/2022 05:45:49 - INFO - __main__ - ['It is depicted as a Kalasha with an eye on both sides and a core made of a fully blossomed lotus .']
03/14/2022 05:45:49 - INFO - __main__ -  [wiki_split] sentence 1: In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre ( ORC ) .  [SEP] sentence 2:  ORC focuses on research on Rhinos , Lions , and carrying capacity of the reserves .
03/14/2022 05:45:49 - INFO - __main__ - ['In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre , whicg focuses on research on rhinos , lions , and carrying capacity of the reserves .']
03/14/2022 05:45:49 - INFO - __main__ - Tokenizing Input ...
03/14/2022 05:45:49 - INFO - __main__ - Tokenizing Output ...
03/14/2022 05:45:49 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 05:45:53 - INFO - __main__ - Global step 600 Train loss 0.159649 Rouge-L 0.12222713572754138 on epoch=299
03/14/2022 05:45:53 - INFO - __main__ - save last model!
03/14/2022 05:45:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 05:45:59 - INFO - __main__ - Starting training!
03/14/2022 05:46:00 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 05:46:01 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 05:46:01 - INFO - __main__ - Printing 3 examples
03/14/2022 05:46:01 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 05:46:01 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 05:46:01 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 05:46:01 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 05:46:01 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 05:46:01 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 05:46:01 - INFO - __main__ - Tokenizing Input ...
03/14/2022 05:46:03 - INFO - __main__ - Tokenizing Output ...
03/14/2022 05:46:09 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 06:03:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_21_0.0005_8_predictions.txt
03/14/2022 06:03:42 - INFO - __main__ - Rouge-L on test data: 0.7042
03/14/2022 06:03:43 - INFO - __main__ - prefix=wiki_split_32_21, lr=0.0005, bsz=8, dev_performance=0.7367141870989464, test_performance=0.7041651499074982
03/14/2022 06:03:43 - INFO - __main__ - Running ... prefix=wiki_split_32_21, lr=0.0003, bsz=8 ...
03/14/2022 06:03:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 06:03:44 - INFO - __main__ - Printing 3 examples
03/14/2022 06:03:44 - INFO - __main__ -  [wiki_split] sentence 1: As a screen writer and a newspaper correspondent , he became intensely interested in Israel .  [SEP] sentence 2:  The Persuasion Explosion , Art Stevens , 1985 This led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 .
03/14/2022 06:03:44 - INFO - __main__ - ["As a screen writer and a newspaper correspondent , he became intensely interested in Israel which led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 ."]
03/14/2022 06:03:44 - INFO - __main__ -  [wiki_split] sentence 1: Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an Australian politician .  [SEP] sentence 2:  He was an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton .
03/14/2022 06:03:44 - INFO - __main__ - ["Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton ."]
03/14/2022 06:03:44 - INFO - __main__ -  [wiki_split] sentence 1: He was not a candidate for reelection in 1894 to the Fifty - fourth Congress .  [SEP] sentence 2:  He resumed the practice of his profession .
03/14/2022 06:03:44 - INFO - __main__ - ['He was not a candidate for reelection in 1894 to the Fifty - fourth Congress , and instead resumed the practice of his profession .']
03/14/2022 06:03:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 06:03:44 - INFO - __main__ - Tokenizing Output ...
03/14/2022 06:03:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 06:03:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 06:03:44 - INFO - __main__ - Printing 3 examples
03/14/2022 06:03:44 - INFO - __main__ -  [wiki_split] sentence 1: The recovery efforts are hampered especially by low fertility of the remaining birds .  [SEP] sentence 2:  Genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .
03/14/2022 06:03:44 - INFO - __main__ - ['The recovery efforts are hampered especially by low fertility of the remaining birds ; genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .']
03/14/2022 06:03:44 - INFO - __main__ -  [wiki_split] sentence 1: It is depicted as Kalasha with eye on both sides .  [SEP] sentence 2:  It has a core made of a fully blossomed lotus .
03/14/2022 06:03:44 - INFO - __main__ - ['It is depicted as a Kalasha with an eye on both sides and a core made of a fully blossomed lotus .']
03/14/2022 06:03:44 - INFO - __main__ -  [wiki_split] sentence 1: In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre ( ORC ) .  [SEP] sentence 2:  ORC focuses on research on Rhinos , Lions , and carrying capacity of the reserves .
03/14/2022 06:03:44 - INFO - __main__ - ['In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre , whicg focuses on research on rhinos , lions , and carrying capacity of the reserves .']
03/14/2022 06:03:44 - INFO - __main__ - Tokenizing Input ...
03/14/2022 06:03:44 - INFO - __main__ - Tokenizing Output ...
03/14/2022 06:03:44 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 06:03:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 06:03:56 - INFO - __main__ - Starting training!
03/14/2022 06:04:00 - INFO - __main__ - Step 10 Global step 10 Train loss 19.831129 on epoch=4
03/14/2022 06:04:05 - INFO - __main__ - Step 20 Global step 20 Train loss 10.965743 on epoch=9
03/14/2022 06:04:09 - INFO - __main__ - Step 30 Global step 30 Train loss 1.870515 on epoch=14
03/14/2022 06:04:14 - INFO - __main__ - Step 40 Global step 40 Train loss 1.252864 on epoch=19
03/14/2022 06:04:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.963118 on epoch=24
03/14/2022 06:04:23 - INFO - __main__ - Global step 50 Train loss 6.976674 Rouge-L 0.5524640273826966 on epoch=24
03/14/2022 06:04:29 - INFO - __main__ - Step 60 Global step 60 Train loss 0.827512 on epoch=29
03/14/2022 06:04:33 - INFO - __main__ - Step 70 Global step 70 Train loss 0.653173 on epoch=34
03/14/2022 06:04:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.545137 on epoch=39
03/14/2022 06:04:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.559070 on epoch=44
03/14/2022 06:04:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.469791 on epoch=49
03/14/2022 06:04:54 - INFO - __main__ - Global step 100 Train loss 0.610937 Rouge-L 0.6295315498252856 on epoch=49
03/14/2022 06:05:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.420959 on epoch=54
03/14/2022 06:05:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.360733 on epoch=59
03/14/2022 06:05:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.295069 on epoch=64
03/14/2022 06:05:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.297889 on epoch=69
03/14/2022 06:05:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.278975 on epoch=74
03/14/2022 06:05:25 - INFO - __main__ - Global step 150 Train loss 0.330725 Rouge-L 0.6123881420001631 on epoch=74
03/14/2022 06:05:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.246586 on epoch=79
03/14/2022 06:05:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.259147 on epoch=84
03/14/2022 06:05:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.191165 on epoch=89
03/14/2022 06:05:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.192048 on epoch=94
03/14/2022 06:05:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.202480 on epoch=99
03/14/2022 06:05:58 - INFO - __main__ - Global step 200 Train loss 0.218285 Rouge-L 0.7265071667592167 on epoch=99
03/14/2022 06:06:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.169204 on epoch=104
03/14/2022 06:06:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.186731 on epoch=109
03/14/2022 06:06:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.168444 on epoch=114
03/14/2022 06:06:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.175833 on epoch=119
03/14/2022 06:06:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.164037 on epoch=124
03/14/2022 06:06:30 - INFO - __main__ - Global step 250 Train loss 0.172850 Rouge-L 0.6575592516386161 on epoch=124
03/14/2022 06:06:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.182214 on epoch=129
03/14/2022 06:06:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.169436 on epoch=134
03/14/2022 06:06:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.156434 on epoch=139
03/14/2022 06:06:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.145061 on epoch=144
03/14/2022 06:06:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.154390 on epoch=149
03/14/2022 06:07:01 - INFO - __main__ - Global step 300 Train loss 0.161507 Rouge-L 0.7058456881643664 on epoch=149
03/14/2022 06:07:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.152333 on epoch=154
03/14/2022 06:07:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.127418 on epoch=159
03/14/2022 06:07:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.127935 on epoch=164
03/14/2022 06:07:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.119470 on epoch=169
03/14/2022 06:07:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.124749 on epoch=174
03/14/2022 06:07:33 - INFO - __main__ - Global step 350 Train loss 0.130381 Rouge-L 0.6807343690735497 on epoch=174
03/14/2022 06:07:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.123635 on epoch=179
03/14/2022 06:07:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.119717 on epoch=184
03/14/2022 06:07:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.112596 on epoch=189
03/14/2022 06:07:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.109152 on epoch=194
03/14/2022 06:07:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.109915 on epoch=199
03/14/2022 06:08:04 - INFO - __main__ - Global step 400 Train loss 0.115003 Rouge-L 0.6767769654840696 on epoch=199
03/14/2022 06:08:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.106700 on epoch=204
03/14/2022 06:08:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.114072 on epoch=209
03/14/2022 06:08:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.155052 on epoch=214
03/14/2022 06:08:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.114545 on epoch=219
03/14/2022 06:08:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.091699 on epoch=224
03/14/2022 06:08:34 - INFO - __main__ - Global step 450 Train loss 0.116414 Rouge-L 0.6644753305998423 on epoch=224
03/14/2022 06:08:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.097852 on epoch=229
03/14/2022 06:08:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.110513 on epoch=234
03/14/2022 06:08:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.102036 on epoch=239
03/14/2022 06:08:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.094253 on epoch=244
03/14/2022 06:08:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.089493 on epoch=249
03/14/2022 06:09:04 - INFO - __main__ - Global step 500 Train loss 0.098830 Rouge-L 0.6495830815776122 on epoch=249
03/14/2022 06:09:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.090489 on epoch=254
03/14/2022 06:09:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.097682 on epoch=259
03/14/2022 06:09:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.086440 on epoch=264
03/14/2022 06:09:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.100571 on epoch=269
03/14/2022 06:09:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.090604 on epoch=274
03/14/2022 06:09:34 - INFO - __main__ - Global step 550 Train loss 0.093157 Rouge-L 0.6613358896021093 on epoch=274
03/14/2022 06:09:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.086346 on epoch=279
03/14/2022 06:09:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.083694 on epoch=284
03/14/2022 06:09:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.085525 on epoch=289
03/14/2022 06:09:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.085864 on epoch=294
03/14/2022 06:09:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.091743 on epoch=299
03/14/2022 06:09:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 06:09:59 - INFO - __main__ - Printing 3 examples
03/14/2022 06:09:59 - INFO - __main__ -  [wiki_split] sentence 1: As a screen writer and a newspaper correspondent , he became intensely interested in Israel .  [SEP] sentence 2:  The Persuasion Explosion , Art Stevens , 1985 This led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 .
03/14/2022 06:09:59 - INFO - __main__ - ["As a screen writer and a newspaper correspondent , he became intensely interested in Israel which led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 ."]
03/14/2022 06:09:59 - INFO - __main__ -  [wiki_split] sentence 1: Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an Australian politician .  [SEP] sentence 2:  He was an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton .
03/14/2022 06:09:59 - INFO - __main__ - ["Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton ."]
03/14/2022 06:09:59 - INFO - __main__ -  [wiki_split] sentence 1: He was not a candidate for reelection in 1894 to the Fifty - fourth Congress .  [SEP] sentence 2:  He resumed the practice of his profession .
03/14/2022 06:09:59 - INFO - __main__ - ['He was not a candidate for reelection in 1894 to the Fifty - fourth Congress , and instead resumed the practice of his profession .']
03/14/2022 06:09:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 06:09:59 - INFO - __main__ - Tokenizing Output ...
03/14/2022 06:09:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 06:09:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 06:09:59 - INFO - __main__ - Printing 3 examples
03/14/2022 06:09:59 - INFO - __main__ -  [wiki_split] sentence 1: The recovery efforts are hampered especially by low fertility of the remaining birds .  [SEP] sentence 2:  Genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .
03/14/2022 06:09:59 - INFO - __main__ - ['The recovery efforts are hampered especially by low fertility of the remaining birds ; genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .']
03/14/2022 06:09:59 - INFO - __main__ -  [wiki_split] sentence 1: It is depicted as Kalasha with eye on both sides .  [SEP] sentence 2:  It has a core made of a fully blossomed lotus .
03/14/2022 06:09:59 - INFO - __main__ - ['It is depicted as a Kalasha with an eye on both sides and a core made of a fully blossomed lotus .']
03/14/2022 06:09:59 - INFO - __main__ -  [wiki_split] sentence 1: In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre ( ORC ) .  [SEP] sentence 2:  ORC focuses on research on Rhinos , Lions , and carrying capacity of the reserves .
03/14/2022 06:09:59 - INFO - __main__ - ['In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre , whicg focuses on research on rhinos , lions , and carrying capacity of the reserves .']
03/14/2022 06:09:59 - INFO - __main__ - Tokenizing Input ...
03/14/2022 06:09:59 - INFO - __main__ - Tokenizing Output ...
03/14/2022 06:09:59 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 06:10:04 - INFO - __main__ - Global step 600 Train loss 0.086634 Rouge-L 0.6834605538808735 on epoch=299
03/14/2022 06:10:04 - INFO - __main__ - save last model!
03/14/2022 06:10:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 06:10:10 - INFO - __main__ - Starting training!
03/14/2022 06:10:10 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 06:10:11 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 06:10:11 - INFO - __main__ - Printing 3 examples
03/14/2022 06:10:11 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 06:10:11 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 06:10:11 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 06:10:11 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 06:10:11 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 06:10:11 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 06:10:11 - INFO - __main__ - Tokenizing Input ...
03/14/2022 06:10:14 - INFO - __main__ - Tokenizing Output ...
03/14/2022 06:10:19 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 06:28:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_21_0.0003_8_predictions.txt
03/14/2022 06:28:11 - INFO - __main__ - Rouge-L on test data: 0.6908
03/14/2022 06:28:11 - INFO - __main__ - prefix=wiki_split_32_21, lr=0.0003, bsz=8, dev_performance=0.7265071667592167, test_performance=0.6908330533661236
03/14/2022 06:28:11 - INFO - __main__ - Running ... prefix=wiki_split_32_21, lr=0.0002, bsz=8 ...
03/14/2022 06:28:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 06:28:12 - INFO - __main__ - Printing 3 examples
03/14/2022 06:28:12 - INFO - __main__ -  [wiki_split] sentence 1: As a screen writer and a newspaper correspondent , he became intensely interested in Israel .  [SEP] sentence 2:  The Persuasion Explosion , Art Stevens , 1985 This led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 .
03/14/2022 06:28:12 - INFO - __main__ - ["As a screen writer and a newspaper correspondent , he became intensely interested in Israel which led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 ."]
03/14/2022 06:28:12 - INFO - __main__ -  [wiki_split] sentence 1: Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an Australian politician .  [SEP] sentence 2:  He was an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton .
03/14/2022 06:28:12 - INFO - __main__ - ["Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton ."]
03/14/2022 06:28:12 - INFO - __main__ -  [wiki_split] sentence 1: He was not a candidate for reelection in 1894 to the Fifty - fourth Congress .  [SEP] sentence 2:  He resumed the practice of his profession .
03/14/2022 06:28:12 - INFO - __main__ - ['He was not a candidate for reelection in 1894 to the Fifty - fourth Congress , and instead resumed the practice of his profession .']
03/14/2022 06:28:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 06:28:12 - INFO - __main__ - Tokenizing Output ...
03/14/2022 06:28:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 06:28:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 06:28:12 - INFO - __main__ - Printing 3 examples
03/14/2022 06:28:12 - INFO - __main__ -  [wiki_split] sentence 1: The recovery efforts are hampered especially by low fertility of the remaining birds .  [SEP] sentence 2:  Genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .
03/14/2022 06:28:12 - INFO - __main__ - ['The recovery efforts are hampered especially by low fertility of the remaining birds ; genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .']
03/14/2022 06:28:12 - INFO - __main__ -  [wiki_split] sentence 1: It is depicted as Kalasha with eye on both sides .  [SEP] sentence 2:  It has a core made of a fully blossomed lotus .
03/14/2022 06:28:12 - INFO - __main__ - ['It is depicted as a Kalasha with an eye on both sides and a core made of a fully blossomed lotus .']
03/14/2022 06:28:12 - INFO - __main__ -  [wiki_split] sentence 1: In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre ( ORC ) .  [SEP] sentence 2:  ORC focuses on research on Rhinos , Lions , and carrying capacity of the reserves .
03/14/2022 06:28:12 - INFO - __main__ - ['In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre , whicg focuses on research on rhinos , lions , and carrying capacity of the reserves .']
03/14/2022 06:28:12 - INFO - __main__ - Tokenizing Input ...
03/14/2022 06:28:12 - INFO - __main__ - Tokenizing Output ...
03/14/2022 06:28:12 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 06:28:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 06:28:25 - INFO - __main__ - Starting training!
03/14/2022 06:28:29 - INFO - __main__ - Step 10 Global step 10 Train loss 19.772408 on epoch=4
03/14/2022 06:28:34 - INFO - __main__ - Step 20 Global step 20 Train loss 13.919647 on epoch=9
03/14/2022 06:28:39 - INFO - __main__ - Step 30 Global step 30 Train loss 7.226688 on epoch=14
03/14/2022 06:28:43 - INFO - __main__ - Step 40 Global step 40 Train loss 5.067893 on epoch=19
03/14/2022 06:28:48 - INFO - __main__ - Step 50 Global step 50 Train loss 4.458667 on epoch=24
03/14/2022 06:29:00 - INFO - __main__ - Global step 50 Train loss 10.089060 Rouge-L 0.504455205150961 on epoch=24
03/14/2022 06:29:05 - INFO - __main__ - Step 60 Global step 60 Train loss 3.310632 on epoch=29
03/14/2022 06:29:10 - INFO - __main__ - Step 70 Global step 70 Train loss 3.208324 on epoch=34
03/14/2022 06:29:15 - INFO - __main__ - Step 80 Global step 80 Train loss 2.478820 on epoch=39
03/14/2022 06:29:20 - INFO - __main__ - Step 90 Global step 90 Train loss 1.920680 on epoch=44
03/14/2022 06:29:25 - INFO - __main__ - Step 100 Global step 100 Train loss 1.619274 on epoch=49
03/14/2022 06:29:29 - INFO - __main__ - Global step 100 Train loss 2.507546 Rouge-L 0.20692423137311222 on epoch=49
03/14/2022 06:29:34 - INFO - __main__ - Step 110 Global step 110 Train loss 1.400238 on epoch=54
03/14/2022 06:29:39 - INFO - __main__ - Step 120 Global step 120 Train loss 1.156321 on epoch=59
03/14/2022 06:29:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.946882 on epoch=64
03/14/2022 06:29:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.753473 on epoch=69
03/14/2022 06:29:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.659830 on epoch=74
03/14/2022 06:29:59 - INFO - __main__ - Global step 150 Train loss 0.983349 Rouge-L 0.16409329718576066 on epoch=74
03/14/2022 06:30:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.645891 on epoch=79
03/14/2022 06:30:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.562224 on epoch=84
03/14/2022 06:30:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.507699 on epoch=89
03/14/2022 06:30:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.456173 on epoch=94
03/14/2022 06:30:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.402948 on epoch=99
03/14/2022 06:30:29 - INFO - __main__ - Global step 200 Train loss 0.514987 Rouge-L 0.11773337507010578 on epoch=99
03/14/2022 06:30:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.394390 on epoch=104
03/14/2022 06:30:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.320953 on epoch=109
03/14/2022 06:30:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.329913 on epoch=114
03/14/2022 06:30:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.288111 on epoch=119
03/14/2022 06:30:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.284121 on epoch=124
03/14/2022 06:31:00 - INFO - __main__ - Global step 250 Train loss 0.323498 Rouge-L 0.11473179294764593 on epoch=124
03/14/2022 06:31:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.260217 on epoch=129
03/14/2022 06:31:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.242907 on epoch=134
03/14/2022 06:31:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.235906 on epoch=139
03/14/2022 06:31:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.236812 on epoch=144
03/14/2022 06:31:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.201844 on epoch=149
03/14/2022 06:31:31 - INFO - __main__ - Global step 300 Train loss 0.235537 Rouge-L 0.10997644016741873 on epoch=149
03/14/2022 06:31:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.236519 on epoch=154
03/14/2022 06:31:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.203277 on epoch=159
03/14/2022 06:31:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.188975 on epoch=164
03/14/2022 06:31:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.183808 on epoch=169
03/14/2022 06:31:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.170288 on epoch=174
03/14/2022 06:32:02 - INFO - __main__ - Global step 350 Train loss 0.196574 Rouge-L 0.0986652645256535 on epoch=174
03/14/2022 06:32:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.175907 on epoch=179
03/14/2022 06:32:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.176064 on epoch=184
03/14/2022 06:32:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.170374 on epoch=189
03/14/2022 06:32:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.158286 on epoch=194
03/14/2022 06:32:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.182159 on epoch=199
03/14/2022 06:32:33 - INFO - __main__ - Global step 400 Train loss 0.172558 Rouge-L 0.0960112195749366 on epoch=199
03/14/2022 06:32:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.182611 on epoch=204
03/14/2022 06:32:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.166459 on epoch=209
03/14/2022 06:32:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.140542 on epoch=214
03/14/2022 06:32:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.151787 on epoch=219
03/14/2022 06:32:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.153428 on epoch=224
03/14/2022 06:33:04 - INFO - __main__ - Global step 450 Train loss 0.158965 Rouge-L 0.10204719672476457 on epoch=224
03/14/2022 06:33:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.163210 on epoch=229
03/14/2022 06:33:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.147698 on epoch=234
03/14/2022 06:33:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.158741 on epoch=239
03/14/2022 06:33:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.137067 on epoch=244
03/14/2022 06:33:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.151744 on epoch=249
03/14/2022 06:33:34 - INFO - __main__ - Global step 500 Train loss 0.151692 Rouge-L 0.09077606729794335 on epoch=249
03/14/2022 06:33:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.148703 on epoch=254
03/14/2022 06:33:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.134260 on epoch=259
03/14/2022 06:33:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.139893 on epoch=264
03/14/2022 06:33:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.135822 on epoch=269
03/14/2022 06:33:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.147575 on epoch=274
03/14/2022 06:34:05 - INFO - __main__ - Global step 550 Train loss 0.141251 Rouge-L 0.09541881977870831 on epoch=274
03/14/2022 06:34:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.126618 on epoch=279
03/14/2022 06:34:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.143333 on epoch=284
03/14/2022 06:34:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.131097 on epoch=289
03/14/2022 06:34:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.112508 on epoch=294
03/14/2022 06:34:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.117907 on epoch=299
03/14/2022 06:34:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 06:34:31 - INFO - __main__ - Printing 3 examples
03/14/2022 06:34:31 - INFO - __main__ -  [wiki_split] sentence 1: As a screen writer and a newspaper correspondent , he became intensely interested in Israel .  [SEP] sentence 2:  The Persuasion Explosion , Art Stevens , 1985 This led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 .
03/14/2022 06:34:31 - INFO - __main__ - ["As a screen writer and a newspaper correspondent , he became intensely interested in Israel which led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 ."]
03/14/2022 06:34:31 - INFO - __main__ -  [wiki_split] sentence 1: Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an Australian politician .  [SEP] sentence 2:  He was an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton .
03/14/2022 06:34:31 - INFO - __main__ - ["Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton ."]
03/14/2022 06:34:31 - INFO - __main__ -  [wiki_split] sentence 1: He was not a candidate for reelection in 1894 to the Fifty - fourth Congress .  [SEP] sentence 2:  He resumed the practice of his profession .
03/14/2022 06:34:31 - INFO - __main__ - ['He was not a candidate for reelection in 1894 to the Fifty - fourth Congress , and instead resumed the practice of his profession .']
03/14/2022 06:34:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 06:34:31 - INFO - __main__ - Tokenizing Output ...
03/14/2022 06:34:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 06:34:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 06:34:31 - INFO - __main__ - Printing 3 examples
03/14/2022 06:34:31 - INFO - __main__ -  [wiki_split] sentence 1: The recovery efforts are hampered especially by low fertility of the remaining birds .  [SEP] sentence 2:  Genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .
03/14/2022 06:34:31 - INFO - __main__ - ['The recovery efforts are hampered especially by low fertility of the remaining birds ; genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .']
03/14/2022 06:34:31 - INFO - __main__ -  [wiki_split] sentence 1: It is depicted as Kalasha with eye on both sides .  [SEP] sentence 2:  It has a core made of a fully blossomed lotus .
03/14/2022 06:34:31 - INFO - __main__ - ['It is depicted as a Kalasha with an eye on both sides and a core made of a fully blossomed lotus .']
03/14/2022 06:34:31 - INFO - __main__ -  [wiki_split] sentence 1: In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre ( ORC ) .  [SEP] sentence 2:  ORC focuses on research on Rhinos , Lions , and carrying capacity of the reserves .
03/14/2022 06:34:31 - INFO - __main__ - ['In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre , whicg focuses on research on rhinos , lions , and carrying capacity of the reserves .']
03/14/2022 06:34:31 - INFO - __main__ - Tokenizing Input ...
03/14/2022 06:34:31 - INFO - __main__ - Tokenizing Output ...
03/14/2022 06:34:31 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 06:34:36 - INFO - __main__ - Global step 600 Train loss 0.126293 Rouge-L 0.09638820120347874 on epoch=299
03/14/2022 06:34:36 - INFO - __main__ - save last model!
03/14/2022 06:34:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 06:34:42 - INFO - __main__ - Starting training!
03/14/2022 06:34:43 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 06:34:43 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 06:34:43 - INFO - __main__ - Printing 3 examples
03/14/2022 06:34:43 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 06:34:43 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 06:34:43 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 06:34:43 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 06:34:43 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 06:34:43 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 06:34:43 - INFO - __main__ - Tokenizing Input ...
03/14/2022 06:34:46 - INFO - __main__ - Tokenizing Output ...
03/14/2022 06:34:51 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 07:04:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_21_0.0002_8_predictions.txt
03/14/2022 07:04:20 - INFO - __main__ - Rouge-L on test data: 0.5272
03/14/2022 07:04:22 - INFO - __main__ - prefix=wiki_split_32_21, lr=0.0002, bsz=8, dev_performance=0.504455205150961, test_performance=0.5271835289696402
03/14/2022 07:04:22 - INFO - __main__ - Running ... prefix=wiki_split_32_21, lr=0.0001, bsz=8 ...
03/14/2022 07:04:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 07:04:23 - INFO - __main__ - Printing 3 examples
03/14/2022 07:04:23 - INFO - __main__ -  [wiki_split] sentence 1: As a screen writer and a newspaper correspondent , he became intensely interested in Israel .  [SEP] sentence 2:  The Persuasion Explosion , Art Stevens , 1985 This led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 .
03/14/2022 07:04:23 - INFO - __main__ - ["As a screen writer and a newspaper correspondent , he became intensely interested in Israel which led to his best - known work , '' Exodus '' , which is about Jewish history from the late 19 th century through the founding of the state of Israel in 1948 ."]
03/14/2022 07:04:23 - INFO - __main__ -  [wiki_split] sentence 1: Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an Australian politician .  [SEP] sentence 2:  He was an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton .
03/14/2022 07:04:23 - INFO - __main__ - ["Anthony '' Tony '' David McRae ( born April 7 , 1957 ) in Tumut , New South Wales is an ALP member in the Western Australian Legislative Assembly from 2001 to 2008 , representing the electorate of Riverton ."]
03/14/2022 07:04:23 - INFO - __main__ -  [wiki_split] sentence 1: He was not a candidate for reelection in 1894 to the Fifty - fourth Congress .  [SEP] sentence 2:  He resumed the practice of his profession .
03/14/2022 07:04:23 - INFO - __main__ - ['He was not a candidate for reelection in 1894 to the Fifty - fourth Congress , and instead resumed the practice of his profession .']
03/14/2022 07:04:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 07:04:23 - INFO - __main__ - Tokenizing Output ...
03/14/2022 07:04:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 07:04:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 07:04:23 - INFO - __main__ - Printing 3 examples
03/14/2022 07:04:23 - INFO - __main__ -  [wiki_split] sentence 1: The recovery efforts are hampered especially by low fertility of the remaining birds .  [SEP] sentence 2:  Genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .
03/14/2022 07:04:23 - INFO - __main__ - ['The recovery efforts are hampered especially by low fertility of the remaining birds ; genetic analyses have been employed to select captive breeding stock in an effort to preserve the maximum genetic diversity .']
03/14/2022 07:04:23 - INFO - __main__ -  [wiki_split] sentence 1: It is depicted as Kalasha with eye on both sides .  [SEP] sentence 2:  It has a core made of a fully blossomed lotus .
03/14/2022 07:04:23 - INFO - __main__ - ['It is depicted as a Kalasha with an eye on both sides and a core made of a fully blossomed lotus .']
03/14/2022 07:04:23 - INFO - __main__ -  [wiki_split] sentence 1: In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre ( ORC ) .  [SEP] sentence 2:  ORC focuses on research on Rhinos , Lions , and carrying capacity of the reserves .
03/14/2022 07:04:23 - INFO - __main__ - ['In 2006 the managing directors of West Midland Safari Park officially opened the Ongava Research Centre , whicg focuses on research on rhinos , lions , and carrying capacity of the reserves .']
03/14/2022 07:04:23 - INFO - __main__ - Tokenizing Input ...
03/14/2022 07:04:23 - INFO - __main__ - Tokenizing Output ...
03/14/2022 07:04:23 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 07:04:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 07:04:35 - INFO - __main__ - Starting training!
03/14/2022 07:04:39 - INFO - __main__ - Step 10 Global step 10 Train loss 20.356899 on epoch=4
03/14/2022 07:04:44 - INFO - __main__ - Step 20 Global step 20 Train loss 17.063755 on epoch=9
03/14/2022 07:04:49 - INFO - __main__ - Step 30 Global step 30 Train loss 10.071368 on epoch=14
03/14/2022 07:04:54 - INFO - __main__ - Step 40 Global step 40 Train loss 7.767886 on epoch=19
03/14/2022 07:04:59 - INFO - __main__ - Step 50 Global step 50 Train loss 6.168899 on epoch=24
03/14/2022 07:05:12 - INFO - __main__ - Global step 50 Train loss 12.285762 Rouge-L 0.5845605107867629 on epoch=24
03/14/2022 07:05:17 - INFO - __main__ - Step 60 Global step 60 Train loss 5.624504 on epoch=29
03/14/2022 07:05:22 - INFO - __main__ - Step 70 Global step 70 Train loss 4.903429 on epoch=34
03/14/2022 07:05:27 - INFO - __main__ - Step 80 Global step 80 Train loss 3.859550 on epoch=39
03/14/2022 07:05:32 - INFO - __main__ - Step 90 Global step 90 Train loss 2.807339 on epoch=44
03/14/2022 07:05:37 - INFO - __main__ - Step 100 Global step 100 Train loss 2.433830 on epoch=49
03/14/2022 07:05:47 - INFO - __main__ - Global step 100 Train loss 3.925731 Rouge-L 0.6046159929162838 on epoch=49
03/14/2022 07:05:53 - INFO - __main__ - Step 110 Global step 110 Train loss 1.984857 on epoch=54
03/14/2022 07:05:58 - INFO - __main__ - Step 120 Global step 120 Train loss 1.603193 on epoch=59
03/14/2022 07:06:03 - INFO - __main__ - Step 130 Global step 130 Train loss 1.353253 on epoch=64
03/14/2022 07:06:08 - INFO - __main__ - Step 140 Global step 140 Train loss 1.126134 on epoch=69
03/14/2022 07:06:13 - INFO - __main__ - Step 150 Global step 150 Train loss 1.086000 on epoch=74
03/14/2022 07:06:17 - INFO - __main__ - Global step 150 Train loss 1.430687 Rouge-L 0.4472854353920293 on epoch=74
03/14/2022 07:06:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.871758 on epoch=79
03/14/2022 07:06:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.843264 on epoch=84
03/14/2022 07:06:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.774710 on epoch=89
03/14/2022 07:06:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.696301 on epoch=94
03/14/2022 07:06:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.664049 on epoch=99
03/14/2022 07:06:52 - INFO - __main__ - Global step 200 Train loss 0.770016 Rouge-L 0.40492263325601136 on epoch=99
03/14/2022 07:06:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.684245 on epoch=104
03/14/2022 07:07:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.612652 on epoch=109
03/14/2022 07:07:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.633672 on epoch=114
03/14/2022 07:07:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.526521 on epoch=119
03/14/2022 07:07:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.591645 on epoch=124
03/14/2022 07:07:23 - INFO - __main__ - Global step 250 Train loss 0.609747 Rouge-L 0.3106078768131215 on epoch=124
03/14/2022 07:07:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.498486 on epoch=129
03/14/2022 07:07:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.463593 on epoch=134
03/14/2022 07:07:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.493827 on epoch=139
03/14/2022 07:07:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.417757 on epoch=144
03/14/2022 07:07:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.436044 on epoch=149
03/14/2022 07:07:54 - INFO - __main__ - Global step 300 Train loss 0.461941 Rouge-L 0.28327127898627785 on epoch=149
03/14/2022 07:07:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.406331 on epoch=154
03/14/2022 07:08:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.390691 on epoch=159
03/14/2022 07:08:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.389647 on epoch=164
03/14/2022 07:08:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.367098 on epoch=169
03/14/2022 07:08:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.364151 on epoch=174
03/14/2022 07:08:25 - INFO - __main__ - Global step 350 Train loss 0.383584 Rouge-L 0.22900238700371375 on epoch=174
03/14/2022 07:08:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.340423 on epoch=179
03/14/2022 07:08:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.322692 on epoch=184
03/14/2022 07:08:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.293415 on epoch=189
03/14/2022 07:08:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.297332 on epoch=194
03/14/2022 07:08:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.302675 on epoch=199
03/14/2022 07:08:58 - INFO - __main__ - Global step 400 Train loss 0.311307 Rouge-L 0.2280626537775325 on epoch=199
03/14/2022 07:09:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.293308 on epoch=204
03/14/2022 07:09:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.283178 on epoch=209
03/14/2022 07:09:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.239199 on epoch=214
03/14/2022 07:09:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.257648 on epoch=219
03/14/2022 07:09:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.234964 on epoch=224
03/14/2022 07:09:28 - INFO - __main__ - Global step 450 Train loss 0.261659 Rouge-L 0.21499653959502862 on epoch=224
03/14/2022 07:09:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.249983 on epoch=229
03/14/2022 07:09:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.208286 on epoch=234
03/14/2022 07:09:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.260265 on epoch=239
03/14/2022 07:09:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.214103 on epoch=244
03/14/2022 07:09:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.236054 on epoch=249
03/14/2022 07:09:59 - INFO - __main__ - Global step 500 Train loss 0.233738 Rouge-L 0.4604530253112212 on epoch=249
03/14/2022 07:10:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.239114 on epoch=254
03/14/2022 07:10:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.209594 on epoch=259
03/14/2022 07:10:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.229707 on epoch=264
03/14/2022 07:10:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.197081 on epoch=269
03/14/2022 07:10:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.190945 on epoch=274
03/14/2022 07:10:29 - INFO - __main__ - Global step 550 Train loss 0.213288 Rouge-L 0.6615439737404776 on epoch=274
03/14/2022 07:10:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.201008 on epoch=279
03/14/2022 07:10:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.201528 on epoch=284
03/14/2022 07:10:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.184754 on epoch=289
03/14/2022 07:10:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.189739 on epoch=294
03/14/2022 07:10:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.178894 on epoch=299
03/14/2022 07:10:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 07:10:56 - INFO - __main__ - Printing 3 examples
03/14/2022 07:10:56 - INFO - __main__ -  [wiki_split] sentence 1: After the very heavy '' Dragontown '' album Alice decided to return to his roots .  [SEP] sentence 2:  His place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges .
03/14/2022 07:10:56 - INFO - __main__ - ["After the very heavy '' Dragontown '' album Alice decided to return to his roots , his place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges ."]
03/14/2022 07:10:56 - INFO - __main__ -  [wiki_split] sentence 1: The museum runs a library with photographic books and magazines .  [SEP] sentence 2:  There is also a small museum store that sells postcards , posters and more .
03/14/2022 07:10:56 - INFO - __main__ - ['The museum runs a library with photographic books and magazines , and a small museum store that sells postcards , posters and more .']
03/14/2022 07:10:56 - INFO - __main__ -  [wiki_split] sentence 1: Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs .  [SEP] sentence 2:  Some 35 billion tonnes of icebergs calve off and pass out of the fjord every year .
03/14/2022 07:10:56 - INFO - __main__ - ['Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs some 35 billion tonnes of icebergs calved off and passing out of the fjord every year .']
03/14/2022 07:10:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 07:10:56 - INFO - __main__ - Tokenizing Output ...
03/14/2022 07:10:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 07:10:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 07:10:56 - INFO - __main__ - Printing 3 examples
03/14/2022 07:10:56 - INFO - __main__ -  [wiki_split] sentence 1: One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , refusing to leave his pumping station despite the danger .  [SEP] sentence 2:  The award was later downgraded to a Mention in Despatches .
03/14/2022 07:10:56 - INFO - __main__ - ['One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , but the award was later downgraded to a Mention in Despatches .']
03/14/2022 07:10:56 - INFO - __main__ -  [wiki_split] sentence 1: He forged a strong partnership with fellow Irish U- 21 international Shane McFaul .  [SEP] sentence 2:  This partnership helped UCD reach the semi finals of both the League and FAI Cups .
03/14/2022 07:10:56 - INFO - __main__ - ['He forged a partnership with fellow Irish U- 21 international Shane McFaul , helping U.C.D. reach the semi finals of both the League and FAI Cups .']
03/14/2022 07:10:56 - INFO - __main__ -  [wiki_split] sentence 1: The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority .  [SEP] sentence 2:  It raised membership requirements and committee composition to include more independent directors and financial expertise .
03/14/2022 07:10:56 - INFO - __main__ - ["The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority , and raised membership requirements and committee composition to include more independent directors and financial expertise ."]
03/14/2022 07:10:56 - INFO - __main__ - Tokenizing Input ...
03/14/2022 07:10:56 - INFO - __main__ - Tokenizing Output ...
03/14/2022 07:10:56 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 07:11:01 - INFO - __main__ - Global step 600 Train loss 0.191185 Rouge-L 0.671510563934135 on epoch=299
03/14/2022 07:11:02 - INFO - __main__ - save last model!
03/14/2022 07:11:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 07:11:06 - INFO - __main__ - Starting training!
03/14/2022 07:11:09 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 07:11:09 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 07:11:09 - INFO - __main__ - Printing 3 examples
03/14/2022 07:11:09 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 07:11:09 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 07:11:09 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 07:11:09 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 07:11:09 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 07:11:09 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 07:11:09 - INFO - __main__ - Tokenizing Input ...
03/14/2022 07:11:12 - INFO - __main__ - Tokenizing Output ...
03/14/2022 07:11:17 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 07:29:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_21_0.0001_8_predictions.txt
03/14/2022 07:29:39 - INFO - __main__ - Rouge-L on test data: 0.6661
03/14/2022 07:29:39 - INFO - __main__ - prefix=wiki_split_32_21, lr=0.0001, bsz=8, dev_performance=0.671510563934135, test_performance=0.6660629875625099
03/14/2022 07:29:39 - INFO - __main__ - Running ... prefix=wiki_split_32_42, lr=0.0005, bsz=8 ...
03/14/2022 07:29:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 07:29:40 - INFO - __main__ - Printing 3 examples
03/14/2022 07:29:40 - INFO - __main__ -  [wiki_split] sentence 1: After the very heavy '' Dragontown '' album Alice decided to return to his roots .  [SEP] sentence 2:  His place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges .
03/14/2022 07:29:40 - INFO - __main__ - ["After the very heavy '' Dragontown '' album Alice decided to return to his roots , his place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges ."]
03/14/2022 07:29:40 - INFO - __main__ -  [wiki_split] sentence 1: The museum runs a library with photographic books and magazines .  [SEP] sentence 2:  There is also a small museum store that sells postcards , posters and more .
03/14/2022 07:29:40 - INFO - __main__ - ['The museum runs a library with photographic books and magazines , and a small museum store that sells postcards , posters and more .']
03/14/2022 07:29:40 - INFO - __main__ -  [wiki_split] sentence 1: Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs .  [SEP] sentence 2:  Some 35 billion tonnes of icebergs calve off and pass out of the fjord every year .
03/14/2022 07:29:40 - INFO - __main__ - ['Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs some 35 billion tonnes of icebergs calved off and passing out of the fjord every year .']
03/14/2022 07:29:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 07:29:40 - INFO - __main__ - Tokenizing Output ...
03/14/2022 07:29:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 07:29:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 07:29:40 - INFO - __main__ - Printing 3 examples
03/14/2022 07:29:40 - INFO - __main__ -  [wiki_split] sentence 1: One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , refusing to leave his pumping station despite the danger .  [SEP] sentence 2:  The award was later downgraded to a Mention in Despatches .
03/14/2022 07:29:40 - INFO - __main__ - ['One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , but the award was later downgraded to a Mention in Despatches .']
03/14/2022 07:29:40 - INFO - __main__ -  [wiki_split] sentence 1: He forged a strong partnership with fellow Irish U- 21 international Shane McFaul .  [SEP] sentence 2:  This partnership helped UCD reach the semi finals of both the League and FAI Cups .
03/14/2022 07:29:40 - INFO - __main__ - ['He forged a partnership with fellow Irish U- 21 international Shane McFaul , helping U.C.D. reach the semi finals of both the League and FAI Cups .']
03/14/2022 07:29:40 - INFO - __main__ -  [wiki_split] sentence 1: The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority .  [SEP] sentence 2:  It raised membership requirements and committee composition to include more independent directors and financial expertise .
03/14/2022 07:29:40 - INFO - __main__ - ["The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority , and raised membership requirements and committee composition to include more independent directors and financial expertise ."]
03/14/2022 07:29:40 - INFO - __main__ - Tokenizing Input ...
03/14/2022 07:29:40 - INFO - __main__ - Tokenizing Output ...
03/14/2022 07:29:40 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 07:29:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 07:29:51 - INFO - __main__ - Starting training!
03/14/2022 07:29:55 - INFO - __main__ - Step 10 Global step 10 Train loss 20.254108 on epoch=4
03/14/2022 07:30:00 - INFO - __main__ - Step 20 Global step 20 Train loss 11.756684 on epoch=9
03/14/2022 07:30:05 - INFO - __main__ - Step 30 Global step 30 Train loss 2.982604 on epoch=14
03/14/2022 07:30:10 - INFO - __main__ - Step 40 Global step 40 Train loss 1.218943 on epoch=19
03/14/2022 07:30:14 - INFO - __main__ - Step 50 Global step 50 Train loss 0.821350 on epoch=24
03/14/2022 07:30:23 - INFO - __main__ - Global step 50 Train loss 7.406738 Rouge-L 0.021874999843923614 on epoch=24
03/14/2022 07:30:28 - INFO - __main__ - Step 60 Global step 60 Train loss 0.638180 on epoch=29
03/14/2022 07:30:33 - INFO - __main__ - Step 70 Global step 70 Train loss 0.539327 on epoch=34
03/14/2022 07:30:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.413969 on epoch=39
03/14/2022 07:30:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.330380 on epoch=44
03/14/2022 07:30:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.285695 on epoch=49
03/14/2022 07:30:54 - INFO - __main__ - Global step 100 Train loss 0.441510 Rouge-L 0.4605248437453717 on epoch=49
03/14/2022 07:31:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.217127 on epoch=54
03/14/2022 07:31:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.198243 on epoch=59
03/14/2022 07:31:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.204512 on epoch=64
03/14/2022 07:31:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.185260 on epoch=69
03/14/2022 07:31:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.221951 on epoch=74
03/14/2022 07:31:26 - INFO - __main__ - Global step 150 Train loss 0.205419 Rouge-L 0.6839777868023095 on epoch=74
03/14/2022 07:31:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.143389 on epoch=79
03/14/2022 07:31:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.163574 on epoch=84
03/14/2022 07:31:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.180825 on epoch=89
03/14/2022 07:31:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.147148 on epoch=94
03/14/2022 07:31:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.168497 on epoch=99
03/14/2022 07:32:02 - INFO - __main__ - Global step 200 Train loss 0.160687 Rouge-L 0.6975276834440098 on epoch=99
03/14/2022 07:32:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.136938 on epoch=104
03/14/2022 07:32:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.126207 on epoch=109
03/14/2022 07:32:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.139320 on epoch=114
03/14/2022 07:32:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.189537 on epoch=119
03/14/2022 07:32:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.139724 on epoch=124
03/14/2022 07:32:34 - INFO - __main__ - Global step 250 Train loss 0.146345 Rouge-L 0.71477116522886 on epoch=124
03/14/2022 07:32:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.120081 on epoch=129
03/14/2022 07:32:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.130822 on epoch=134
03/14/2022 07:32:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.125042 on epoch=139
03/14/2022 07:32:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.118839 on epoch=144
03/14/2022 07:33:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.102133 on epoch=149
03/14/2022 07:33:10 - INFO - __main__ - Global step 300 Train loss 0.119383 Rouge-L 0.700975867145471 on epoch=149
03/14/2022 07:33:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.099796 on epoch=154
03/14/2022 07:33:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.103817 on epoch=159
03/14/2022 07:33:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.095012 on epoch=164
03/14/2022 07:33:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.093698 on epoch=169
03/14/2022 07:33:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.089399 on epoch=174
03/14/2022 07:33:42 - INFO - __main__ - Global step 350 Train loss 0.096345 Rouge-L 0.6958859417056589 on epoch=174
03/14/2022 07:33:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.086519 on epoch=179
03/14/2022 07:33:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.083171 on epoch=184
03/14/2022 07:33:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.088976 on epoch=189
03/14/2022 07:34:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.081380 on epoch=194
03/14/2022 07:34:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.078818 on epoch=199
03/14/2022 07:34:13 - INFO - __main__ - Global step 400 Train loss 0.083773 Rouge-L 0.670963765713541 on epoch=199
03/14/2022 07:34:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.084757 on epoch=204
03/14/2022 07:34:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.076015 on epoch=209
03/14/2022 07:34:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.089677 on epoch=214
03/14/2022 07:34:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.084927 on epoch=219
03/14/2022 07:34:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.112238 on epoch=224
03/14/2022 07:34:45 - INFO - __main__ - Global step 450 Train loss 0.089523 Rouge-L 0.742733053868196 on epoch=224
03/14/2022 07:34:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.091027 on epoch=229
03/14/2022 07:34:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.069377 on epoch=234
03/14/2022 07:35:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.078303 on epoch=239
03/14/2022 07:35:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.070654 on epoch=244
03/14/2022 07:35:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.082449 on epoch=249
03/14/2022 07:35:17 - INFO - __main__ - Global step 500 Train loss 0.078362 Rouge-L 0.7213023233429855 on epoch=249
03/14/2022 07:35:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.074348 on epoch=254
03/14/2022 07:35:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.075388 on epoch=259
03/14/2022 07:35:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.069738 on epoch=264
03/14/2022 07:35:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.063549 on epoch=269
03/14/2022 07:35:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.071929 on epoch=274
03/14/2022 07:35:49 - INFO - __main__ - Global step 550 Train loss 0.070990 Rouge-L 0.7012067845890803 on epoch=274
03/14/2022 07:35:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.068846 on epoch=279
03/14/2022 07:36:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.064065 on epoch=284
03/14/2022 07:36:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.071231 on epoch=289
03/14/2022 07:36:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.068139 on epoch=294
03/14/2022 07:36:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.069741 on epoch=299
03/14/2022 07:36:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 07:36:16 - INFO - __main__ - Printing 3 examples
03/14/2022 07:36:16 - INFO - __main__ -  [wiki_split] sentence 1: After the very heavy '' Dragontown '' album Alice decided to return to his roots .  [SEP] sentence 2:  His place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges .
03/14/2022 07:36:16 - INFO - __main__ - ["After the very heavy '' Dragontown '' album Alice decided to return to his roots , his place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges ."]
03/14/2022 07:36:16 - INFO - __main__ -  [wiki_split] sentence 1: The museum runs a library with photographic books and magazines .  [SEP] sentence 2:  There is also a small museum store that sells postcards , posters and more .
03/14/2022 07:36:16 - INFO - __main__ - ['The museum runs a library with photographic books and magazines , and a small museum store that sells postcards , posters and more .']
03/14/2022 07:36:16 - INFO - __main__ -  [wiki_split] sentence 1: Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs .  [SEP] sentence 2:  Some 35 billion tonnes of icebergs calve off and pass out of the fjord every year .
03/14/2022 07:36:16 - INFO - __main__ - ['Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs some 35 billion tonnes of icebergs calved off and passing out of the fjord every year .']
03/14/2022 07:36:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 07:36:16 - INFO - __main__ - Tokenizing Output ...
03/14/2022 07:36:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 07:36:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 07:36:16 - INFO - __main__ - Printing 3 examples
03/14/2022 07:36:16 - INFO - __main__ -  [wiki_split] sentence 1: One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , refusing to leave his pumping station despite the danger .  [SEP] sentence 2:  The award was later downgraded to a Mention in Despatches .
03/14/2022 07:36:16 - INFO - __main__ - ['One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , but the award was later downgraded to a Mention in Despatches .']
03/14/2022 07:36:16 - INFO - __main__ -  [wiki_split] sentence 1: He forged a strong partnership with fellow Irish U- 21 international Shane McFaul .  [SEP] sentence 2:  This partnership helped UCD reach the semi finals of both the League and FAI Cups .
03/14/2022 07:36:16 - INFO - __main__ - ['He forged a partnership with fellow Irish U- 21 international Shane McFaul , helping U.C.D. reach the semi finals of both the League and FAI Cups .']
03/14/2022 07:36:16 - INFO - __main__ -  [wiki_split] sentence 1: The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority .  [SEP] sentence 2:  It raised membership requirements and committee composition to include more independent directors and financial expertise .
03/14/2022 07:36:16 - INFO - __main__ - ["The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority , and raised membership requirements and committee composition to include more independent directors and financial expertise ."]
03/14/2022 07:36:16 - INFO - __main__ - Tokenizing Input ...
03/14/2022 07:36:16 - INFO - __main__ - Tokenizing Output ...
03/14/2022 07:36:16 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 07:36:25 - INFO - __main__ - Global step 600 Train loss 0.068405 Rouge-L 0.6741864588008899 on epoch=299
03/14/2022 07:36:25 - INFO - __main__ - save last model!
03/14/2022 07:36:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 07:36:29 - INFO - __main__ - Starting training!
03/14/2022 07:36:32 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 07:36:32 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 07:36:32 - INFO - __main__ - Printing 3 examples
03/14/2022 07:36:32 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 07:36:32 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 07:36:32 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 07:36:32 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 07:36:32 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 07:36:32 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 07:36:32 - INFO - __main__ - Tokenizing Input ...
03/14/2022 07:36:35 - INFO - __main__ - Tokenizing Output ...
03/14/2022 07:36:40 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 07:57:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_42_0.0005_8_predictions.txt
03/14/2022 07:57:28 - INFO - __main__ - Rouge-L on test data: 0.7196
03/14/2022 07:57:28 - INFO - __main__ - prefix=wiki_split_32_42, lr=0.0005, bsz=8, dev_performance=0.742733053868196, test_performance=0.7196032255549214
03/14/2022 07:57:28 - INFO - __main__ - Running ... prefix=wiki_split_32_42, lr=0.0003, bsz=8 ...
03/14/2022 07:57:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 07:57:29 - INFO - __main__ - Printing 3 examples
03/14/2022 07:57:29 - INFO - __main__ -  [wiki_split] sentence 1: After the very heavy '' Dragontown '' album Alice decided to return to his roots .  [SEP] sentence 2:  His place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges .
03/14/2022 07:57:29 - INFO - __main__ - ["After the very heavy '' Dragontown '' album Alice decided to return to his roots , his place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges ."]
03/14/2022 07:57:29 - INFO - __main__ -  [wiki_split] sentence 1: The museum runs a library with photographic books and magazines .  [SEP] sentence 2:  There is also a small museum store that sells postcards , posters and more .
03/14/2022 07:57:29 - INFO - __main__ - ['The museum runs a library with photographic books and magazines , and a small museum store that sells postcards , posters and more .']
03/14/2022 07:57:29 - INFO - __main__ -  [wiki_split] sentence 1: Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs .  [SEP] sentence 2:  Some 35 billion tonnes of icebergs calve off and pass out of the fjord every year .
03/14/2022 07:57:29 - INFO - __main__ - ['Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs some 35 billion tonnes of icebergs calved off and passing out of the fjord every year .']
03/14/2022 07:57:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 07:57:29 - INFO - __main__ - Tokenizing Output ...
03/14/2022 07:57:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 07:57:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 07:57:29 - INFO - __main__ - Printing 3 examples
03/14/2022 07:57:29 - INFO - __main__ -  [wiki_split] sentence 1: One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , refusing to leave his pumping station despite the danger .  [SEP] sentence 2:  The award was later downgraded to a Mention in Despatches .
03/14/2022 07:57:29 - INFO - __main__ - ['One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , but the award was later downgraded to a Mention in Despatches .']
03/14/2022 07:57:29 - INFO - __main__ -  [wiki_split] sentence 1: He forged a strong partnership with fellow Irish U- 21 international Shane McFaul .  [SEP] sentence 2:  This partnership helped UCD reach the semi finals of both the League and FAI Cups .
03/14/2022 07:57:29 - INFO - __main__ - ['He forged a partnership with fellow Irish U- 21 international Shane McFaul , helping U.C.D. reach the semi finals of both the League and FAI Cups .']
03/14/2022 07:57:29 - INFO - __main__ -  [wiki_split] sentence 1: The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority .  [SEP] sentence 2:  It raised membership requirements and committee composition to include more independent directors and financial expertise .
03/14/2022 07:57:29 - INFO - __main__ - ["The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority , and raised membership requirements and committee composition to include more independent directors and financial expertise ."]
03/14/2022 07:57:29 - INFO - __main__ - Tokenizing Input ...
03/14/2022 07:57:29 - INFO - __main__ - Tokenizing Output ...
03/14/2022 07:57:29 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 07:57:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 07:57:40 - INFO - __main__ - Starting training!
03/14/2022 07:57:44 - INFO - __main__ - Step 10 Global step 10 Train loss 19.099115 on epoch=4
03/14/2022 07:57:48 - INFO - __main__ - Step 20 Global step 20 Train loss 15.032919 on epoch=9
03/14/2022 07:57:53 - INFO - __main__ - Step 30 Global step 30 Train loss 9.757630 on epoch=14
03/14/2022 07:57:58 - INFO - __main__ - Step 40 Global step 40 Train loss 8.942204 on epoch=19
03/14/2022 07:58:02 - INFO - __main__ - Step 50 Global step 50 Train loss 7.937916 on epoch=24
03/14/2022 07:58:15 - INFO - __main__ - Global step 50 Train loss 12.153956 Rouge-L 0.15557784005094205 on epoch=24
03/14/2022 07:58:20 - INFO - __main__ - Step 60 Global step 60 Train loss 7.538006 on epoch=29
03/14/2022 07:58:25 - INFO - __main__ - Step 70 Global step 70 Train loss 7.178277 on epoch=34
03/14/2022 07:58:30 - INFO - __main__ - Step 80 Global step 80 Train loss 6.872468 on epoch=39
03/14/2022 07:58:34 - INFO - __main__ - Step 90 Global step 90 Train loss 6.083623 on epoch=44
03/14/2022 07:58:39 - INFO - __main__ - Step 100 Global step 100 Train loss 5.117484 on epoch=49
03/14/2022 07:58:51 - INFO - __main__ - Global step 100 Train loss 6.557972 Rouge-L 0.28889979746059413 on epoch=49
03/14/2022 07:58:56 - INFO - __main__ - Step 110 Global step 110 Train loss 4.776105 on epoch=54
03/14/2022 07:59:01 - INFO - __main__ - Step 120 Global step 120 Train loss 3.430595 on epoch=59
03/14/2022 07:59:06 - INFO - __main__ - Step 130 Global step 130 Train loss 2.485353 on epoch=64
03/14/2022 07:59:11 - INFO - __main__ - Step 140 Global step 140 Train loss 1.659436 on epoch=69
03/14/2022 07:59:16 - INFO - __main__ - Step 150 Global step 150 Train loss 1.170393 on epoch=74
03/14/2022 07:59:25 - INFO - __main__ - Global step 150 Train loss 2.704376 Rouge-L 0.18989623062421862 on epoch=74
03/14/2022 07:59:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.798239 on epoch=79
03/14/2022 07:59:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.603431 on epoch=84
03/14/2022 07:59:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.467230 on epoch=89
03/14/2022 07:59:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.389298 on epoch=94
03/14/2022 07:59:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.329768 on epoch=99
03/14/2022 07:59:55 - INFO - __main__ - Global step 200 Train loss 0.517593 Rouge-L 0.11193823540666648 on epoch=99
03/14/2022 07:59:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.297435 on epoch=104
03/14/2022 08:00:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.262555 on epoch=109
03/14/2022 08:00:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.250403 on epoch=114
03/14/2022 08:00:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.242816 on epoch=119
03/14/2022 08:00:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.237497 on epoch=124
03/14/2022 08:00:24 - INFO - __main__ - Global step 250 Train loss 0.258142 Rouge-L 0.09652343792353635 on epoch=124
03/14/2022 08:00:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.223712 on epoch=129
03/14/2022 08:00:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.214127 on epoch=134
03/14/2022 08:00:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.181625 on epoch=139
03/14/2022 08:00:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.179800 on epoch=144
03/14/2022 08:00:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.165409 on epoch=149
03/14/2022 08:00:54 - INFO - __main__ - Global step 300 Train loss 0.192935 Rouge-L 0.11201197993055606 on epoch=149
03/14/2022 08:00:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.163185 on epoch=154
03/14/2022 08:01:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.152002 on epoch=159
03/14/2022 08:01:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.177781 on epoch=164
03/14/2022 08:01:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.162892 on epoch=169
03/14/2022 08:01:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.155187 on epoch=174
03/14/2022 08:01:24 - INFO - __main__ - Global step 350 Train loss 0.162209 Rouge-L 0.11501752328675514 on epoch=174
03/14/2022 08:01:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.175413 on epoch=179
03/14/2022 08:01:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.153568 on epoch=184
03/14/2022 08:01:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.142885 on epoch=189
03/14/2022 08:01:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.125255 on epoch=194
03/14/2022 08:01:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.132855 on epoch=199
03/14/2022 08:01:53 - INFO - __main__ - Global step 400 Train loss 0.145995 Rouge-L 0.11096136918682141 on epoch=199
03/14/2022 08:01:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.121760 on epoch=204
03/14/2022 08:02:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.130205 on epoch=209
03/14/2022 08:02:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.134737 on epoch=214
03/14/2022 08:02:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.107591 on epoch=219
03/14/2022 08:02:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.112537 on epoch=224
03/14/2022 08:02:22 - INFO - __main__ - Global step 450 Train loss 0.121366 Rouge-L 0.10603958579081599 on epoch=224
03/14/2022 08:02:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.119425 on epoch=229
03/14/2022 08:02:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.130511 on epoch=234
03/14/2022 08:02:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.131985 on epoch=239
03/14/2022 08:02:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.118168 on epoch=244
03/14/2022 08:02:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.122493 on epoch=249
03/14/2022 08:02:52 - INFO - __main__ - Global step 500 Train loss 0.124516 Rouge-L 0.11087620269285511 on epoch=249
03/14/2022 08:02:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.102506 on epoch=254
03/14/2022 08:03:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.104738 on epoch=259
03/14/2022 08:03:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.108092 on epoch=264
03/14/2022 08:03:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.110590 on epoch=269
03/14/2022 08:03:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.096838 on epoch=274
03/14/2022 08:03:21 - INFO - __main__ - Global step 550 Train loss 0.104553 Rouge-L 0.1055134237461195 on epoch=274
03/14/2022 08:03:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.096560 on epoch=279
03/14/2022 08:03:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.091403 on epoch=284
03/14/2022 08:03:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.101718 on epoch=289
03/14/2022 08:03:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.104795 on epoch=294
03/14/2022 08:03:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.099993 on epoch=299
03/14/2022 08:03:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 08:03:46 - INFO - __main__ - Printing 3 examples
03/14/2022 08:03:46 - INFO - __main__ -  [wiki_split] sentence 1: After the very heavy '' Dragontown '' album Alice decided to return to his roots .  [SEP] sentence 2:  His place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges .
03/14/2022 08:03:46 - INFO - __main__ - ["After the very heavy '' Dragontown '' album Alice decided to return to his roots , his place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges ."]
03/14/2022 08:03:46 - INFO - __main__ -  [wiki_split] sentence 1: The museum runs a library with photographic books and magazines .  [SEP] sentence 2:  There is also a small museum store that sells postcards , posters and more .
03/14/2022 08:03:46 - INFO - __main__ - ['The museum runs a library with photographic books and magazines , and a small museum store that sells postcards , posters and more .']
03/14/2022 08:03:46 - INFO - __main__ -  [wiki_split] sentence 1: Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs .  [SEP] sentence 2:  Some 35 billion tonnes of icebergs calve off and pass out of the fjord every year .
03/14/2022 08:03:46 - INFO - __main__ - ['Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs some 35 billion tonnes of icebergs calved off and passing out of the fjord every year .']
03/14/2022 08:03:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 08:03:46 - INFO - __main__ - Tokenizing Output ...
03/14/2022 08:03:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 08:03:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 08:03:46 - INFO - __main__ - Printing 3 examples
03/14/2022 08:03:46 - INFO - __main__ -  [wiki_split] sentence 1: One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , refusing to leave his pumping station despite the danger .  [SEP] sentence 2:  The award was later downgraded to a Mention in Despatches .
03/14/2022 08:03:46 - INFO - __main__ - ['One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , but the award was later downgraded to a Mention in Despatches .']
03/14/2022 08:03:46 - INFO - __main__ -  [wiki_split] sentence 1: He forged a strong partnership with fellow Irish U- 21 international Shane McFaul .  [SEP] sentence 2:  This partnership helped UCD reach the semi finals of both the League and FAI Cups .
03/14/2022 08:03:46 - INFO - __main__ - ['He forged a partnership with fellow Irish U- 21 international Shane McFaul , helping U.C.D. reach the semi finals of both the League and FAI Cups .']
03/14/2022 08:03:46 - INFO - __main__ -  [wiki_split] sentence 1: The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority .  [SEP] sentence 2:  It raised membership requirements and committee composition to include more independent directors and financial expertise .
03/14/2022 08:03:46 - INFO - __main__ - ["The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority , and raised membership requirements and committee composition to include more independent directors and financial expertise ."]
03/14/2022 08:03:46 - INFO - __main__ - Tokenizing Input ...
03/14/2022 08:03:46 - INFO - __main__ - Tokenizing Output ...
03/14/2022 08:03:46 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 08:03:50 - INFO - __main__ - Global step 600 Train loss 0.098894 Rouge-L 0.12297653010381701 on epoch=299
03/14/2022 08:03:50 - INFO - __main__ - save last model!
03/14/2022 08:03:57 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 08:03:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 08:03:57 - INFO - __main__ - Starting training!
03/14/2022 08:03:58 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 08:03:58 - INFO - __main__ - Printing 3 examples
03/14/2022 08:03:58 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 08:03:58 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 08:03:58 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 08:03:58 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 08:03:58 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 08:03:58 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 08:03:58 - INFO - __main__ - Tokenizing Input ...
03/14/2022 08:04:00 - INFO - __main__ - Tokenizing Output ...
03/14/2022 08:04:06 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 08:35:10 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_42_0.0003_8_predictions.txt
03/14/2022 08:35:21 - INFO - __main__ - Rouge-L on test data: 0.2209
03/14/2022 08:35:22 - INFO - __main__ - prefix=wiki_split_32_42, lr=0.0003, bsz=8, dev_performance=0.28889979746059413, test_performance=0.2208942529067567
03/14/2022 08:35:22 - INFO - __main__ - Running ... prefix=wiki_split_32_42, lr=0.0002, bsz=8 ...
03/14/2022 08:35:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 08:35:23 - INFO - __main__ - Printing 3 examples
03/14/2022 08:35:23 - INFO - __main__ -  [wiki_split] sentence 1: After the very heavy '' Dragontown '' album Alice decided to return to his roots .  [SEP] sentence 2:  His place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges .
03/14/2022 08:35:23 - INFO - __main__ - ["After the very heavy '' Dragontown '' album Alice decided to return to his roots , his place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges ."]
03/14/2022 08:35:23 - INFO - __main__ -  [wiki_split] sentence 1: The museum runs a library with photographic books and magazines .  [SEP] sentence 2:  There is also a small museum store that sells postcards , posters and more .
03/14/2022 08:35:23 - INFO - __main__ - ['The museum runs a library with photographic books and magazines , and a small museum store that sells postcards , posters and more .']
03/14/2022 08:35:23 - INFO - __main__ -  [wiki_split] sentence 1: Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs .  [SEP] sentence 2:  Some 35 billion tonnes of icebergs calve off and pass out of the fjord every year .
03/14/2022 08:35:23 - INFO - __main__ - ['Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs some 35 billion tonnes of icebergs calved off and passing out of the fjord every year .']
03/14/2022 08:35:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 08:35:23 - INFO - __main__ - Tokenizing Output ...
03/14/2022 08:35:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 08:35:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 08:35:23 - INFO - __main__ - Printing 3 examples
03/14/2022 08:35:23 - INFO - __main__ -  [wiki_split] sentence 1: One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , refusing to leave his pumping station despite the danger .  [SEP] sentence 2:  The award was later downgraded to a Mention in Despatches .
03/14/2022 08:35:23 - INFO - __main__ - ['One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , but the award was later downgraded to a Mention in Despatches .']
03/14/2022 08:35:23 - INFO - __main__ -  [wiki_split] sentence 1: He forged a strong partnership with fellow Irish U- 21 international Shane McFaul .  [SEP] sentence 2:  This partnership helped UCD reach the semi finals of both the League and FAI Cups .
03/14/2022 08:35:23 - INFO - __main__ - ['He forged a partnership with fellow Irish U- 21 international Shane McFaul , helping U.C.D. reach the semi finals of both the League and FAI Cups .']
03/14/2022 08:35:23 - INFO - __main__ -  [wiki_split] sentence 1: The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority .  [SEP] sentence 2:  It raised membership requirements and committee composition to include more independent directors and financial expertise .
03/14/2022 08:35:23 - INFO - __main__ - ["The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority , and raised membership requirements and committee composition to include more independent directors and financial expertise ."]
03/14/2022 08:35:23 - INFO - __main__ - Tokenizing Input ...
03/14/2022 08:35:23 - INFO - __main__ - Tokenizing Output ...
03/14/2022 08:35:23 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 08:35:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 08:35:34 - INFO - __main__ - Starting training!
03/14/2022 08:35:37 - INFO - __main__ - Step 10 Global step 10 Train loss 20.015743 on epoch=4
03/14/2022 08:35:42 - INFO - __main__ - Step 20 Global step 20 Train loss 15.518309 on epoch=9
03/14/2022 08:35:47 - INFO - __main__ - Step 30 Global step 30 Train loss 8.338591 on epoch=14
03/14/2022 08:35:51 - INFO - __main__ - Step 40 Global step 40 Train loss 7.482226 on epoch=19
03/14/2022 08:35:56 - INFO - __main__ - Step 50 Global step 50 Train loss 7.512180 on epoch=24
03/14/2022 08:36:08 - INFO - __main__ - Global step 50 Train loss 11.773408 Rouge-L 0.12194563928533009 on epoch=24
03/14/2022 08:36:13 - INFO - __main__ - Step 60 Global step 60 Train loss 7.472711 on epoch=29
03/14/2022 08:36:18 - INFO - __main__ - Step 70 Global step 70 Train loss 7.715296 on epoch=34
03/14/2022 08:36:23 - INFO - __main__ - Step 80 Global step 80 Train loss 7.277415 on epoch=39
03/14/2022 08:36:28 - INFO - __main__ - Step 90 Global step 90 Train loss 6.497050 on epoch=44
03/14/2022 08:36:32 - INFO - __main__ - Step 100 Global step 100 Train loss 5.613293 on epoch=49
03/14/2022 08:36:44 - INFO - __main__ - Global step 100 Train loss 6.915153 Rouge-L 0.09891670919184027 on epoch=49
03/14/2022 08:36:49 - INFO - __main__ - Step 110 Global step 110 Train loss 4.423838 on epoch=54
03/14/2022 08:36:53 - INFO - __main__ - Step 120 Global step 120 Train loss 3.476351 on epoch=59
03/14/2022 08:36:58 - INFO - __main__ - Step 130 Global step 130 Train loss 2.791473 on epoch=64
03/14/2022 08:37:03 - INFO - __main__ - Step 140 Global step 140 Train loss 2.160583 on epoch=69
03/14/2022 08:37:08 - INFO - __main__ - Step 150 Global step 150 Train loss 1.708990 on epoch=74
03/14/2022 08:37:13 - INFO - __main__ - Global step 150 Train loss 2.912247 Rouge-L 0.09405479804325044 on epoch=74
03/14/2022 08:37:18 - INFO - __main__ - Step 160 Global step 160 Train loss 1.326132 on epoch=79
03/14/2022 08:37:23 - INFO - __main__ - Step 170 Global step 170 Train loss 1.030967 on epoch=84
03/14/2022 08:37:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.900487 on epoch=89
03/14/2022 08:37:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.705123 on epoch=94
03/14/2022 08:37:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.627035 on epoch=99
03/14/2022 08:37:43 - INFO - __main__ - Global step 200 Train loss 0.917949 Rouge-L 0.0986575065953308 on epoch=99
03/14/2022 08:37:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.526676 on epoch=104
03/14/2022 08:37:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.468295 on epoch=109
03/14/2022 08:37:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.416871 on epoch=114
03/14/2022 08:38:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.366623 on epoch=119
03/14/2022 08:38:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.365825 on epoch=124
03/14/2022 08:38:12 - INFO - __main__ - Global step 250 Train loss 0.428858 Rouge-L 0.09483198952673888 on epoch=124
03/14/2022 08:38:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.304416 on epoch=129
03/14/2022 08:38:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.326194 on epoch=134
03/14/2022 08:38:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.279683 on epoch=139
03/14/2022 08:38:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.271688 on epoch=144
03/14/2022 08:38:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.240009 on epoch=149
03/14/2022 08:38:41 - INFO - __main__ - Global step 300 Train loss 0.284398 Rouge-L 0.08649807749008827 on epoch=149
03/14/2022 08:38:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.275116 on epoch=154
03/14/2022 08:38:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.243236 on epoch=159
03/14/2022 08:38:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.227381 on epoch=164
03/14/2022 08:39:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.230881 on epoch=169
03/14/2022 08:39:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.201416 on epoch=174
03/14/2022 08:39:11 - INFO - __main__ - Global step 350 Train loss 0.235606 Rouge-L 0.10567226132303391 on epoch=174
03/14/2022 08:39:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.194903 on epoch=179
03/14/2022 08:39:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.189766 on epoch=184
03/14/2022 08:39:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.188817 on epoch=189
03/14/2022 08:39:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.179194 on epoch=194
03/14/2022 08:39:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.194359 on epoch=199
03/14/2022 08:39:40 - INFO - __main__ - Global step 400 Train loss 0.189408 Rouge-L 0.09880900181199137 on epoch=199
03/14/2022 08:39:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.185274 on epoch=204
03/14/2022 08:39:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.206589 on epoch=209
03/14/2022 08:39:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.175677 on epoch=214
03/14/2022 08:40:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.185448 on epoch=219
03/14/2022 08:40:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.175885 on epoch=224
03/14/2022 08:40:10 - INFO - __main__ - Global step 450 Train loss 0.185775 Rouge-L 0.10801615324803254 on epoch=224
03/14/2022 08:40:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.164021 on epoch=229
03/14/2022 08:40:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.164952 on epoch=234
03/14/2022 08:40:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.163974 on epoch=239
03/14/2022 08:40:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.151862 on epoch=244
03/14/2022 08:40:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.159892 on epoch=249
03/14/2022 08:40:39 - INFO - __main__ - Global step 500 Train loss 0.160940 Rouge-L 0.10404472016436483 on epoch=249
03/14/2022 08:40:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.146944 on epoch=254
03/14/2022 08:40:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.171643 on epoch=259
03/14/2022 08:40:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.160165 on epoch=264
03/14/2022 08:40:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.143061 on epoch=269
03/14/2022 08:41:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.171489 on epoch=274
03/14/2022 08:41:09 - INFO - __main__ - Global step 550 Train loss 0.158660 Rouge-L 0.11117206574101525 on epoch=274
03/14/2022 08:41:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.149561 on epoch=279
03/14/2022 08:41:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.140413 on epoch=284
03/14/2022 08:41:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.151125 on epoch=289
03/14/2022 08:41:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.147243 on epoch=294
03/14/2022 08:41:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.141868 on epoch=299
03/14/2022 08:41:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 08:41:35 - INFO - __main__ - Printing 3 examples
03/14/2022 08:41:35 - INFO - __main__ -  [wiki_split] sentence 1: After the very heavy '' Dragontown '' album Alice decided to return to his roots .  [SEP] sentence 2:  His place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges .
03/14/2022 08:41:35 - INFO - __main__ - ["After the very heavy '' Dragontown '' album Alice decided to return to his roots , his place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges ."]
03/14/2022 08:41:35 - INFO - __main__ -  [wiki_split] sentence 1: The museum runs a library with photographic books and magazines .  [SEP] sentence 2:  There is also a small museum store that sells postcards , posters and more .
03/14/2022 08:41:35 - INFO - __main__ - ['The museum runs a library with photographic books and magazines , and a small museum store that sells postcards , posters and more .']
03/14/2022 08:41:35 - INFO - __main__ -  [wiki_split] sentence 1: Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs .  [SEP] sentence 2:  Some 35 billion tonnes of icebergs calve off and pass out of the fjord every year .
03/14/2022 08:41:35 - INFO - __main__ - ['Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs some 35 billion tonnes of icebergs calved off and passing out of the fjord every year .']
03/14/2022 08:41:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 08:41:35 - INFO - __main__ - Tokenizing Output ...
03/14/2022 08:41:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 08:41:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 08:41:35 - INFO - __main__ - Printing 3 examples
03/14/2022 08:41:35 - INFO - __main__ -  [wiki_split] sentence 1: One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , refusing to leave his pumping station despite the danger .  [SEP] sentence 2:  The award was later downgraded to a Mention in Despatches .
03/14/2022 08:41:35 - INFO - __main__ - ['One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , but the award was later downgraded to a Mention in Despatches .']
03/14/2022 08:41:35 - INFO - __main__ -  [wiki_split] sentence 1: He forged a strong partnership with fellow Irish U- 21 international Shane McFaul .  [SEP] sentence 2:  This partnership helped UCD reach the semi finals of both the League and FAI Cups .
03/14/2022 08:41:35 - INFO - __main__ - ['He forged a partnership with fellow Irish U- 21 international Shane McFaul , helping U.C.D. reach the semi finals of both the League and FAI Cups .']
03/14/2022 08:41:35 - INFO - __main__ -  [wiki_split] sentence 1: The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority .  [SEP] sentence 2:  It raised membership requirements and committee composition to include more independent directors and financial expertise .
03/14/2022 08:41:35 - INFO - __main__ - ["The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority , and raised membership requirements and committee composition to include more independent directors and financial expertise ."]
03/14/2022 08:41:35 - INFO - __main__ - Tokenizing Input ...
03/14/2022 08:41:35 - INFO - __main__ - Tokenizing Output ...
03/14/2022 08:41:35 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 08:41:39 - INFO - __main__ - Global step 600 Train loss 0.146042 Rouge-L 0.10723590755760576 on epoch=299
03/14/2022 08:41:39 - INFO - __main__ - save last model!
03/14/2022 08:41:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 08:41:46 - INFO - __main__ - Starting training!
03/14/2022 08:41:46 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 08:41:46 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 08:41:46 - INFO - __main__ - Printing 3 examples
03/14/2022 08:41:46 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 08:41:46 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 08:41:46 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 08:41:46 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 08:41:46 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 08:41:46 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 08:41:46 - INFO - __main__ - Tokenizing Input ...
03/14/2022 08:41:49 - INFO - __main__ - Tokenizing Output ...
03/14/2022 08:41:54 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 09:13:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_42_0.0002_8_predictions.txt
03/14/2022 09:13:34 - INFO - __main__ - Rouge-L on test data: 0.1157
03/14/2022 09:13:35 - INFO - __main__ - prefix=wiki_split_32_42, lr=0.0002, bsz=8, dev_performance=0.12194563928533009, test_performance=0.11570829453534648
03/14/2022 09:13:35 - INFO - __main__ - Running ... prefix=wiki_split_32_42, lr=0.0001, bsz=8 ...
03/14/2022 09:13:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 09:13:36 - INFO - __main__ - Printing 3 examples
03/14/2022 09:13:36 - INFO - __main__ -  [wiki_split] sentence 1: After the very heavy '' Dragontown '' album Alice decided to return to his roots .  [SEP] sentence 2:  His place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges .
03/14/2022 09:13:36 - INFO - __main__ - ["After the very heavy '' Dragontown '' album Alice decided to return to his roots , his place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges ."]
03/14/2022 09:13:36 - INFO - __main__ -  [wiki_split] sentence 1: The museum runs a library with photographic books and magazines .  [SEP] sentence 2:  There is also a small museum store that sells postcards , posters and more .
03/14/2022 09:13:36 - INFO - __main__ - ['The museum runs a library with photographic books and magazines , and a small museum store that sells postcards , posters and more .']
03/14/2022 09:13:36 - INFO - __main__ -  [wiki_split] sentence 1: Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs .  [SEP] sentence 2:  Some 35 billion tonnes of icebergs calve off and pass out of the fjord every year .
03/14/2022 09:13:36 - INFO - __main__ - ['Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs some 35 billion tonnes of icebergs calved off and passing out of the fjord every year .']
03/14/2022 09:13:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 09:13:36 - INFO - __main__ - Tokenizing Output ...
03/14/2022 09:13:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 09:13:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 09:13:36 - INFO - __main__ - Printing 3 examples
03/14/2022 09:13:36 - INFO - __main__ -  [wiki_split] sentence 1: One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , refusing to leave his pumping station despite the danger .  [SEP] sentence 2:  The award was later downgraded to a Mention in Despatches .
03/14/2022 09:13:36 - INFO - __main__ - ['One member of the battalion , Private William Cox , was subsequently nominated for a posthumous Victoria Cross for his actions during the raid , but the award was later downgraded to a Mention in Despatches .']
03/14/2022 09:13:36 - INFO - __main__ -  [wiki_split] sentence 1: He forged a strong partnership with fellow Irish U- 21 international Shane McFaul .  [SEP] sentence 2:  This partnership helped UCD reach the semi finals of both the League and FAI Cups .
03/14/2022 09:13:36 - INFO - __main__ - ['He forged a partnership with fellow Irish U- 21 international Shane McFaul , helping U.C.D. reach the semi finals of both the League and FAI Cups .']
03/14/2022 09:13:36 - INFO - __main__ -  [wiki_split] sentence 1: The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority .  [SEP] sentence 2:  It raised membership requirements and committee composition to include more independent directors and financial expertise .
03/14/2022 09:13:36 - INFO - __main__ - ["The Sarbanes - Oxley Act of 2002 increased audit committees '' responsibilities and authority , and raised membership requirements and committee composition to include more independent directors and financial expertise ."]
03/14/2022 09:13:36 - INFO - __main__ - Tokenizing Input ...
03/14/2022 09:13:36 - INFO - __main__ - Tokenizing Output ...
03/14/2022 09:13:36 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 09:13:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 09:13:48 - INFO - __main__ - Starting training!
03/14/2022 09:13:53 - INFO - __main__ - Step 10 Global step 10 Train loss 19.838758 on epoch=4
03/14/2022 09:13:58 - INFO - __main__ - Step 20 Global step 20 Train loss 15.459210 on epoch=9
03/14/2022 09:14:03 - INFO - __main__ - Step 30 Global step 30 Train loss 9.057439 on epoch=14
03/14/2022 09:14:07 - INFO - __main__ - Step 40 Global step 40 Train loss 7.841361 on epoch=19
03/14/2022 09:14:12 - INFO - __main__ - Step 50 Global step 50 Train loss 6.205829 on epoch=24
03/14/2022 09:14:26 - INFO - __main__ - Global step 50 Train loss 11.680520 Rouge-L 0.3274931412308759 on epoch=24
03/14/2022 09:14:32 - INFO - __main__ - Step 60 Global step 60 Train loss 4.862848 on epoch=29
03/14/2022 09:14:37 - INFO - __main__ - Step 70 Global step 70 Train loss 4.617204 on epoch=34
03/14/2022 09:14:42 - INFO - __main__ - Step 80 Global step 80 Train loss 4.494806 on epoch=39
03/14/2022 09:14:47 - INFO - __main__ - Step 90 Global step 90 Train loss 5.940052 on epoch=44
03/14/2022 09:14:52 - INFO - __main__ - Step 100 Global step 100 Train loss 5.492675 on epoch=49
03/14/2022 09:15:04 - INFO - __main__ - Global step 100 Train loss 5.081517 Rouge-L 0.022526726673761033 on epoch=49
03/14/2022 09:15:09 - INFO - __main__ - Step 110 Global step 110 Train loss 5.117290 on epoch=54
03/14/2022 09:15:14 - INFO - __main__ - Step 120 Global step 120 Train loss 4.399460 on epoch=59
03/14/2022 09:15:19 - INFO - __main__ - Step 130 Global step 130 Train loss 3.919037 on epoch=64
03/14/2022 09:15:24 - INFO - __main__ - Step 140 Global step 140 Train loss 3.525040 on epoch=69
03/14/2022 09:15:29 - INFO - __main__ - Step 150 Global step 150 Train loss 3.310870 on epoch=74
03/14/2022 09:15:32 - INFO - __main__ - Global step 150 Train loss 4.054339 Rouge-L 0.0016891891426223534 on epoch=74
03/14/2022 09:15:37 - INFO - __main__ - Step 160 Global step 160 Train loss 2.841698 on epoch=79
03/14/2022 09:15:42 - INFO - __main__ - Step 170 Global step 170 Train loss 2.566493 on epoch=84
03/14/2022 09:15:47 - INFO - __main__ - Step 180 Global step 180 Train loss 2.311599 on epoch=89
03/14/2022 09:15:52 - INFO - __main__ - Step 190 Global step 190 Train loss 1.858545 on epoch=94
03/14/2022 09:15:57 - INFO - __main__ - Step 200 Global step 200 Train loss 1.733829 on epoch=99
03/14/2022 09:16:06 - INFO - __main__ - Global step 200 Train loss 2.262433 Rouge-L 0.03313380305412397 on epoch=99
03/14/2022 09:16:11 - INFO - __main__ - Step 210 Global step 210 Train loss 1.469546 on epoch=104
03/14/2022 09:16:16 - INFO - __main__ - Step 220 Global step 220 Train loss 1.306686 on epoch=109
03/14/2022 09:16:21 - INFO - __main__ - Step 230 Global step 230 Train loss 1.171669 on epoch=114
03/14/2022 09:16:26 - INFO - __main__ - Step 240 Global step 240 Train loss 1.018027 on epoch=119
03/14/2022 09:16:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.954917 on epoch=124
03/14/2022 09:16:35 - INFO - __main__ - Global step 250 Train loss 1.184169 Rouge-L 0.05210283709200552 on epoch=124
03/14/2022 09:16:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.829411 on epoch=129
03/14/2022 09:16:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.782565 on epoch=134
03/14/2022 09:16:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.723334 on epoch=139
03/14/2022 09:16:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.700015 on epoch=144
03/14/2022 09:17:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.634497 on epoch=149
03/14/2022 09:17:05 - INFO - __main__ - Global step 300 Train loss 0.733964 Rouge-L 0.056201631422593976 on epoch=149
03/14/2022 09:17:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.582602 on epoch=154
03/14/2022 09:17:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.569619 on epoch=159
03/14/2022 09:17:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.539689 on epoch=164
03/14/2022 09:17:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.484480 on epoch=169
03/14/2022 09:17:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.464765 on epoch=174
03/14/2022 09:17:34 - INFO - __main__ - Global step 350 Train loss 0.528231 Rouge-L 0.03263750855833902 on epoch=174
03/14/2022 09:17:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.470384 on epoch=179
03/14/2022 09:17:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.416911 on epoch=184
03/14/2022 09:17:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.425150 on epoch=189
03/14/2022 09:17:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.384443 on epoch=194
03/14/2022 09:17:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.386341 on epoch=199
03/14/2022 09:18:04 - INFO - __main__ - Global step 400 Train loss 0.416646 Rouge-L 0.05013366996980575 on epoch=199
03/14/2022 09:18:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.371837 on epoch=204
03/14/2022 09:18:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.348759 on epoch=209
03/14/2022 09:18:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.333972 on epoch=214
03/14/2022 09:18:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.294075 on epoch=219
03/14/2022 09:18:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.296419 on epoch=224
03/14/2022 09:18:34 - INFO - __main__ - Global step 450 Train loss 0.329012 Rouge-L 0.03116106145511231 on epoch=224
03/14/2022 09:18:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.275346 on epoch=229
03/14/2022 09:18:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.278894 on epoch=234
03/14/2022 09:18:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.260609 on epoch=239
03/14/2022 09:18:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.282212 on epoch=244
03/14/2022 09:18:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.292505 on epoch=249
03/14/2022 09:19:03 - INFO - __main__ - Global step 500 Train loss 0.277913 Rouge-L 0.11151674421161503 on epoch=249
03/14/2022 09:19:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.243064 on epoch=254
03/14/2022 09:19:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.273923 on epoch=259
03/14/2022 09:19:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.262154 on epoch=264
03/14/2022 09:19:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.234523 on epoch=269
03/14/2022 09:19:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.233933 on epoch=274
03/14/2022 09:19:33 - INFO - __main__ - Global step 550 Train loss 0.249520 Rouge-L 0.11000023086595491 on epoch=274
03/14/2022 09:19:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.223042 on epoch=279
03/14/2022 09:19:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.242681 on epoch=284
03/14/2022 09:19:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.223329 on epoch=289
03/14/2022 09:19:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.213056 on epoch=294
03/14/2022 09:19:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.208559 on epoch=299
03/14/2022 09:19:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 09:19:59 - INFO - __main__ - Printing 3 examples
03/14/2022 09:19:59 - INFO - __main__ -  [wiki_split] sentence 1: Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese .  [SEP] sentence 2:  Originating in the English village of Cheddar in Somerset , it is now produced in several countries around the world .
03/14/2022 09:19:59 - INFO - __main__ - ['Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese , produced in several countries around the world .']
03/14/2022 09:19:59 - INFO - __main__ -  [wiki_split] sentence 1: He was the son of a wealthy farming family from Tno , Iwate .  [SEP] sentence 2:  He attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .
03/14/2022 09:19:59 - INFO - __main__ - ['He was the son of a wealthy farming family from Tno , Iwate , and attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .']
03/14/2022 09:19:59 - INFO - __main__ -  [wiki_split] sentence 1: In England it was the only such qualification from 1430 until 1832 .  [SEP] sentence 2:  It remained one of the qualifications ( after 1918 at a higher financial level ) until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .
03/14/2022 09:19:59 - INFO - __main__ - ['In England it was the only such qualification from the 1430s until 1832 and remained one of the qualifications until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .']
03/14/2022 09:19:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 09:19:59 - INFO - __main__ - Tokenizing Output ...
03/14/2022 09:19:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 09:19:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 09:19:59 - INFO - __main__ - Printing 3 examples
03/14/2022 09:19:59 - INFO - __main__ -  [wiki_split] sentence 1: Larten teaches Darren all about vampirism , but is sometimes shown to regret blooding him .  [SEP] sentence 2:  He decides to take Darren to the Cirque du Freak .
03/14/2022 09:19:59 - INFO - __main__ - ['Larten teaches Darren all about vampirism and sometimes is shown to regret blooding Darren and decides to take him to the Cirque du Freak .']
03/14/2022 09:19:59 - INFO - __main__ -  [wiki_split] sentence 1: Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 .  [SEP] sentence 2:  It specialises in creating humorous , factual documentaries .
03/14/2022 09:19:59 - INFO - __main__ - ['Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 , specialising in creating humorous , factual documentaries .']
03/14/2022 09:19:59 - INFO - __main__ -  [wiki_split] sentence 1: At the time of sale , the Drake Linden Hall building was listed at .  [SEP] sentence 2:  The sale included the additions made in the mid- 1970s , the gym and '' new '' classrooms building .
03/14/2022 09:19:59 - INFO - __main__ - ["At the time of sale , Drake Linden Hall measured , including additions made in the mid- 1970s , the gym and '' new '' classrooms building ."]
03/14/2022 09:19:59 - INFO - __main__ - Tokenizing Input ...
03/14/2022 09:19:59 - INFO - __main__ - Tokenizing Output ...
03/14/2022 09:19:59 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 09:20:03 - INFO - __main__ - Global step 600 Train loss 0.222133 Rouge-L 0.12154839486436528 on epoch=299
03/14/2022 09:20:03 - INFO - __main__ - save last model!
03/14/2022 09:20:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 09:20:10 - INFO - __main__ - Starting training!
03/14/2022 09:20:10 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 09:20:10 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 09:20:10 - INFO - __main__ - Printing 3 examples
03/14/2022 09:20:10 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 09:20:10 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 09:20:10 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 09:20:10 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 09:20:10 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 09:20:10 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 09:20:10 - INFO - __main__ - Tokenizing Input ...
03/14/2022 09:20:13 - INFO - __main__ - Tokenizing Output ...
03/14/2022 09:20:18 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 09:50:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_42_0.0001_8_predictions.txt
03/14/2022 09:50:28 - INFO - __main__ - Rouge-L on test data: 0.3523
03/14/2022 09:50:29 - INFO - __main__ - prefix=wiki_split_32_42, lr=0.0001, bsz=8, dev_performance=0.3274931412308759, test_performance=0.3522534802077212
03/14/2022 09:50:29 - INFO - __main__ - Running ... prefix=wiki_split_32_87, lr=0.0005, bsz=8 ...
03/14/2022 09:50:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 09:50:29 - INFO - __main__ - Printing 3 examples
03/14/2022 09:50:29 - INFO - __main__ -  [wiki_split] sentence 1: Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese .  [SEP] sentence 2:  Originating in the English village of Cheddar in Somerset , it is now produced in several countries around the world .
03/14/2022 09:50:29 - INFO - __main__ - ['Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese , produced in several countries around the world .']
03/14/2022 09:50:29 - INFO - __main__ -  [wiki_split] sentence 1: He was the son of a wealthy farming family from Tno , Iwate .  [SEP] sentence 2:  He attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .
03/14/2022 09:50:29 - INFO - __main__ - ['He was the son of a wealthy farming family from Tno , Iwate , and attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .']
03/14/2022 09:50:29 - INFO - __main__ -  [wiki_split] sentence 1: In England it was the only such qualification from 1430 until 1832 .  [SEP] sentence 2:  It remained one of the qualifications ( after 1918 at a higher financial level ) until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .
03/14/2022 09:50:29 - INFO - __main__ - ['In England it was the only such qualification from the 1430s until 1832 and remained one of the qualifications until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .']
03/14/2022 09:50:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 09:50:29 - INFO - __main__ - Tokenizing Output ...
03/14/2022 09:50:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 09:50:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 09:50:30 - INFO - __main__ - Printing 3 examples
03/14/2022 09:50:30 - INFO - __main__ -  [wiki_split] sentence 1: Larten teaches Darren all about vampirism , but is sometimes shown to regret blooding him .  [SEP] sentence 2:  He decides to take Darren to the Cirque du Freak .
03/14/2022 09:50:30 - INFO - __main__ - ['Larten teaches Darren all about vampirism and sometimes is shown to regret blooding Darren and decides to take him to the Cirque du Freak .']
03/14/2022 09:50:30 - INFO - __main__ -  [wiki_split] sentence 1: Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 .  [SEP] sentence 2:  It specialises in creating humorous , factual documentaries .
03/14/2022 09:50:30 - INFO - __main__ - ['Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 , specialising in creating humorous , factual documentaries .']
03/14/2022 09:50:30 - INFO - __main__ -  [wiki_split] sentence 1: At the time of sale , the Drake Linden Hall building was listed at .  [SEP] sentence 2:  The sale included the additions made in the mid- 1970s , the gym and '' new '' classrooms building .
03/14/2022 09:50:30 - INFO - __main__ - ["At the time of sale , Drake Linden Hall measured , including additions made in the mid- 1970s , the gym and '' new '' classrooms building ."]
03/14/2022 09:50:30 - INFO - __main__ - Tokenizing Input ...
03/14/2022 09:50:30 - INFO - __main__ - Tokenizing Output ...
03/14/2022 09:50:30 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 09:50:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 09:50:41 - INFO - __main__ - Starting training!
03/14/2022 09:50:45 - INFO - __main__ - Step 10 Global step 10 Train loss 19.710028 on epoch=4
03/14/2022 09:50:49 - INFO - __main__ - Step 20 Global step 20 Train loss 13.729296 on epoch=9
03/14/2022 09:50:53 - INFO - __main__ - Step 30 Global step 30 Train loss 8.799895 on epoch=14
03/14/2022 09:50:58 - INFO - __main__ - Step 40 Global step 40 Train loss 7.153211 on epoch=19
03/14/2022 09:51:03 - INFO - __main__ - Step 50 Global step 50 Train loss 7.536978 on epoch=24
03/14/2022 09:51:15 - INFO - __main__ - Global step 50 Train loss 11.385882 Rouge-L 0.1424426155894657 on epoch=24
03/14/2022 09:51:21 - INFO - __main__ - Step 60 Global step 60 Train loss 7.037097 on epoch=29
03/14/2022 09:51:26 - INFO - __main__ - Step 70 Global step 70 Train loss 5.225403 on epoch=34
03/14/2022 09:51:31 - INFO - __main__ - Step 80 Global step 80 Train loss 3.122509 on epoch=39
03/14/2022 09:51:36 - INFO - __main__ - Step 90 Global step 90 Train loss 1.989940 on epoch=44
03/14/2022 09:51:41 - INFO - __main__ - Step 100 Global step 100 Train loss 1.118538 on epoch=49
03/14/2022 09:51:50 - INFO - __main__ - Global step 100 Train loss 3.698698 Rouge-L 0.10298256346323717 on epoch=49
03/14/2022 09:51:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.698214 on epoch=54
03/14/2022 09:52:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.489659 on epoch=59
03/14/2022 09:52:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.405214 on epoch=64
03/14/2022 09:52:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.343036 on epoch=69
03/14/2022 09:52:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.249790 on epoch=74
03/14/2022 09:52:20 - INFO - __main__ - Global step 150 Train loss 0.437183 Rouge-L 0.09420264909349374 on epoch=74
03/14/2022 09:52:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.237678 on epoch=79
03/14/2022 09:52:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.206744 on epoch=84
03/14/2022 09:52:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.230152 on epoch=89
03/14/2022 09:52:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.196839 on epoch=94
03/14/2022 09:52:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.197193 on epoch=99
03/14/2022 09:52:51 - INFO - __main__ - Global step 200 Train loss 0.213721 Rouge-L 0.10828573521333199 on epoch=99
03/14/2022 09:52:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.183845 on epoch=104
03/14/2022 09:53:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.170462 on epoch=109
03/14/2022 09:53:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.170186 on epoch=114
03/14/2022 09:53:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.161886 on epoch=119
03/14/2022 09:53:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.160433 on epoch=124
03/14/2022 09:53:19 - INFO - __main__ - Global step 250 Train loss 0.169362 Rouge-L 0.112125596097775 on epoch=124
03/14/2022 09:53:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.137742 on epoch=129
03/14/2022 09:53:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.160159 on epoch=134
03/14/2022 09:53:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.158081 on epoch=139
03/14/2022 09:53:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.133356 on epoch=144
03/14/2022 09:53:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.142201 on epoch=149
03/14/2022 09:53:49 - INFO - __main__ - Global step 300 Train loss 0.146308 Rouge-L 0.10896617922706887 on epoch=149
03/14/2022 09:53:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.133558 on epoch=154
03/14/2022 09:53:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.137520 on epoch=159
03/14/2022 09:54:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.131153 on epoch=164
03/14/2022 09:54:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.120589 on epoch=169
03/14/2022 09:54:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.208133 on epoch=174
03/14/2022 09:54:18 - INFO - __main__ - Global step 350 Train loss 0.146190 Rouge-L 0.10852944127457226 on epoch=174
03/14/2022 09:54:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.122588 on epoch=179
03/14/2022 09:54:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.116764 on epoch=184
03/14/2022 09:54:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.104433 on epoch=189
03/14/2022 09:54:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.109987 on epoch=194
03/14/2022 09:54:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.112441 on epoch=199
03/14/2022 09:54:49 - INFO - __main__ - Global step 400 Train loss 0.113242 Rouge-L 0.10281461989500534 on epoch=199
03/14/2022 09:54:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.111900 on epoch=204
03/14/2022 09:54:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.114368 on epoch=209
03/14/2022 09:55:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.103519 on epoch=214
03/14/2022 09:55:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.094022 on epoch=219
03/14/2022 09:55:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.098857 on epoch=224
03/14/2022 09:55:18 - INFO - __main__ - Global step 450 Train loss 0.104533 Rouge-L 0.10906133660041688 on epoch=224
03/14/2022 09:55:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.104553 on epoch=229
03/14/2022 09:55:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.100947 on epoch=234
03/14/2022 09:55:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.107639 on epoch=239
03/14/2022 09:55:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.092322 on epoch=244
03/14/2022 09:55:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.102049 on epoch=249
03/14/2022 09:55:48 - INFO - __main__ - Global step 500 Train loss 0.101502 Rouge-L 0.10510765356894111 on epoch=249
03/14/2022 09:55:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.101486 on epoch=254
03/14/2022 09:55:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.093625 on epoch=259
03/14/2022 09:56:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.097786 on epoch=264
03/14/2022 09:56:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.096571 on epoch=269
03/14/2022 09:56:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.089811 on epoch=274
03/14/2022 09:56:19 - INFO - __main__ - Global step 550 Train loss 0.095856 Rouge-L 0.10215889370442885 on epoch=274
03/14/2022 09:56:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.114388 on epoch=279
03/14/2022 09:56:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.101431 on epoch=284
03/14/2022 09:56:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.112018 on epoch=289
03/14/2022 09:56:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.098233 on epoch=294
03/14/2022 09:56:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.088523 on epoch=299
03/14/2022 09:56:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 09:56:45 - INFO - __main__ - Printing 3 examples
03/14/2022 09:56:45 - INFO - __main__ -  [wiki_split] sentence 1: Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese .  [SEP] sentence 2:  Originating in the English village of Cheddar in Somerset , it is now produced in several countries around the world .
03/14/2022 09:56:45 - INFO - __main__ - ['Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese , produced in several countries around the world .']
03/14/2022 09:56:45 - INFO - __main__ -  [wiki_split] sentence 1: He was the son of a wealthy farming family from Tno , Iwate .  [SEP] sentence 2:  He attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .
03/14/2022 09:56:45 - INFO - __main__ - ['He was the son of a wealthy farming family from Tno , Iwate , and attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .']
03/14/2022 09:56:45 - INFO - __main__ -  [wiki_split] sentence 1: In England it was the only such qualification from 1430 until 1832 .  [SEP] sentence 2:  It remained one of the qualifications ( after 1918 at a higher financial level ) until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .
03/14/2022 09:56:45 - INFO - __main__ - ['In England it was the only such qualification from the 1430s until 1832 and remained one of the qualifications until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .']
03/14/2022 09:56:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 09:56:45 - INFO - __main__ - Tokenizing Output ...
03/14/2022 09:56:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 09:56:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 09:56:45 - INFO - __main__ - Printing 3 examples
03/14/2022 09:56:45 - INFO - __main__ -  [wiki_split] sentence 1: Larten teaches Darren all about vampirism , but is sometimes shown to regret blooding him .  [SEP] sentence 2:  He decides to take Darren to the Cirque du Freak .
03/14/2022 09:56:45 - INFO - __main__ - ['Larten teaches Darren all about vampirism and sometimes is shown to regret blooding Darren and decides to take him to the Cirque du Freak .']
03/14/2022 09:56:45 - INFO - __main__ -  [wiki_split] sentence 1: Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 .  [SEP] sentence 2:  It specialises in creating humorous , factual documentaries .
03/14/2022 09:56:45 - INFO - __main__ - ['Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 , specialising in creating humorous , factual documentaries .']
03/14/2022 09:56:45 - INFO - __main__ -  [wiki_split] sentence 1: At the time of sale , the Drake Linden Hall building was listed at .  [SEP] sentence 2:  The sale included the additions made in the mid- 1970s , the gym and '' new '' classrooms building .
03/14/2022 09:56:45 - INFO - __main__ - ["At the time of sale , Drake Linden Hall measured , including additions made in the mid- 1970s , the gym and '' new '' classrooms building ."]
03/14/2022 09:56:45 - INFO - __main__ - Tokenizing Input ...
03/14/2022 09:56:45 - INFO - __main__ - Tokenizing Output ...
03/14/2022 09:56:45 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 09:56:49 - INFO - __main__ - Global step 600 Train loss 0.102919 Rouge-L 0.10600685891787782 on epoch=299
03/14/2022 09:56:49 - INFO - __main__ - save last model!
03/14/2022 09:56:56 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 09:56:57 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 09:56:57 - INFO - __main__ - Printing 3 examples
03/14/2022 09:56:57 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 09:56:57 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 09:56:57 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 09:56:57 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 09:56:57 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 09:56:57 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 09:56:57 - INFO - __main__ - Tokenizing Input ...
03/14/2022 09:56:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 09:56:58 - INFO - __main__ - Starting training!
03/14/2022 09:56:59 - INFO - __main__ - Tokenizing Output ...
03/14/2022 09:57:05 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 10:27:52 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_87_0.0005_8_predictions.txt
03/14/2022 10:27:55 - INFO - __main__ - Rouge-L on test data: 0.1616
03/14/2022 10:27:56 - INFO - __main__ - prefix=wiki_split_32_87, lr=0.0005, bsz=8, dev_performance=0.1424426155894657, test_performance=0.161624105423697
03/14/2022 10:27:56 - INFO - __main__ - Running ... prefix=wiki_split_32_87, lr=0.0003, bsz=8 ...
03/14/2022 10:27:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 10:27:57 - INFO - __main__ - Printing 3 examples
03/14/2022 10:27:57 - INFO - __main__ -  [wiki_split] sentence 1: Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese .  [SEP] sentence 2:  Originating in the English village of Cheddar in Somerset , it is now produced in several countries around the world .
03/14/2022 10:27:57 - INFO - __main__ - ['Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese , produced in several countries around the world .']
03/14/2022 10:27:57 - INFO - __main__ -  [wiki_split] sentence 1: He was the son of a wealthy farming family from Tno , Iwate .  [SEP] sentence 2:  He attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .
03/14/2022 10:27:57 - INFO - __main__ - ['He was the son of a wealthy farming family from Tno , Iwate , and attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .']
03/14/2022 10:27:57 - INFO - __main__ -  [wiki_split] sentence 1: In England it was the only such qualification from 1430 until 1832 .  [SEP] sentence 2:  It remained one of the qualifications ( after 1918 at a higher financial level ) until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .
03/14/2022 10:27:57 - INFO - __main__ - ['In England it was the only such qualification from the 1430s until 1832 and remained one of the qualifications until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .']
03/14/2022 10:27:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 10:27:57 - INFO - __main__ - Tokenizing Output ...
03/14/2022 10:27:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 10:27:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 10:27:57 - INFO - __main__ - Printing 3 examples
03/14/2022 10:27:57 - INFO - __main__ -  [wiki_split] sentence 1: Larten teaches Darren all about vampirism , but is sometimes shown to regret blooding him .  [SEP] sentence 2:  He decides to take Darren to the Cirque du Freak .
03/14/2022 10:27:57 - INFO - __main__ - ['Larten teaches Darren all about vampirism and sometimes is shown to regret blooding Darren and decides to take him to the Cirque du Freak .']
03/14/2022 10:27:57 - INFO - __main__ -  [wiki_split] sentence 1: Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 .  [SEP] sentence 2:  It specialises in creating humorous , factual documentaries .
03/14/2022 10:27:57 - INFO - __main__ - ['Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 , specialising in creating humorous , factual documentaries .']
03/14/2022 10:27:57 - INFO - __main__ -  [wiki_split] sentence 1: At the time of sale , the Drake Linden Hall building was listed at .  [SEP] sentence 2:  The sale included the additions made in the mid- 1970s , the gym and '' new '' classrooms building .
03/14/2022 10:27:57 - INFO - __main__ - ["At the time of sale , Drake Linden Hall measured , including additions made in the mid- 1970s , the gym and '' new '' classrooms building ."]
03/14/2022 10:27:57 - INFO - __main__ - Tokenizing Input ...
03/14/2022 10:27:57 - INFO - __main__ - Tokenizing Output ...
03/14/2022 10:27:57 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 10:28:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 10:28:09 - INFO - __main__ - Starting training!
03/14/2022 10:28:13 - INFO - __main__ - Step 10 Global step 10 Train loss 20.183701 on epoch=4
03/14/2022 10:28:18 - INFO - __main__ - Step 20 Global step 20 Train loss 11.554560 on epoch=9
03/14/2022 10:28:23 - INFO - __main__ - Step 30 Global step 30 Train loss 4.854958 on epoch=14
03/14/2022 10:28:28 - INFO - __main__ - Step 40 Global step 40 Train loss 3.121984 on epoch=19
03/14/2022 10:28:33 - INFO - __main__ - Step 50 Global step 50 Train loss 2.898808 on epoch=24
03/14/2022 10:28:43 - INFO - __main__ - Global step 50 Train loss 8.522802 Rouge-L 0.46657444424887995 on epoch=24
03/14/2022 10:28:49 - INFO - __main__ - Step 60 Global step 60 Train loss 1.805011 on epoch=29
03/14/2022 10:28:54 - INFO - __main__ - Step 70 Global step 70 Train loss 1.174770 on epoch=34
03/14/2022 10:28:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.893297 on epoch=39
03/14/2022 10:29:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.695124 on epoch=44
03/14/2022 10:29:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.645546 on epoch=49
03/14/2022 10:29:18 - INFO - __main__ - Global step 100 Train loss 1.042750 Rouge-L 0.6138490970905565 on epoch=49
03/14/2022 10:29:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.532275 on epoch=54
03/14/2022 10:29:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.509447 on epoch=59
03/14/2022 10:29:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.416330 on epoch=64
03/14/2022 10:29:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.392199 on epoch=69
03/14/2022 10:29:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.329492 on epoch=74
03/14/2022 10:29:53 - INFO - __main__ - Global step 150 Train loss 0.435948 Rouge-L 0.5981215590245148 on epoch=74
03/14/2022 10:29:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.324847 on epoch=79
03/14/2022 10:30:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.311498 on epoch=84
03/14/2022 10:30:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.251925 on epoch=89
03/14/2022 10:30:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.266203 on epoch=94
03/14/2022 10:30:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.218820 on epoch=99
03/14/2022 10:30:27 - INFO - __main__ - Global step 200 Train loss 0.274659 Rouge-L 0.6337727481073836 on epoch=99
03/14/2022 10:30:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.217365 on epoch=104
03/14/2022 10:30:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.210187 on epoch=109
03/14/2022 10:30:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.203635 on epoch=114
03/14/2022 10:30:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.196452 on epoch=119
03/14/2022 10:30:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.203953 on epoch=124
03/14/2022 10:31:02 - INFO - __main__ - Global step 250 Train loss 0.206318 Rouge-L 0.6391646486111799 on epoch=124
03/14/2022 10:31:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.184227 on epoch=129
03/14/2022 10:31:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.186468 on epoch=134
03/14/2022 10:31:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.179548 on epoch=139
03/14/2022 10:31:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.190523 on epoch=144
03/14/2022 10:31:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.179705 on epoch=149
03/14/2022 10:31:38 - INFO - __main__ - Global step 300 Train loss 0.184094 Rouge-L 0.6379678669958687 on epoch=149
03/14/2022 10:31:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.136818 on epoch=154
03/14/2022 10:31:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.153399 on epoch=159
03/14/2022 10:31:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.168320 on epoch=164
03/14/2022 10:31:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.148538 on epoch=169
03/14/2022 10:32:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.155442 on epoch=174
03/14/2022 10:32:09 - INFO - __main__ - Global step 350 Train loss 0.152503 Rouge-L 0.6704266321769663 on epoch=174
03/14/2022 10:32:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.159666 on epoch=179
03/14/2022 10:32:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.154985 on epoch=184
03/14/2022 10:32:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.161946 on epoch=189
03/14/2022 10:32:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.152310 on epoch=194
03/14/2022 10:32:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.169509 on epoch=199
03/14/2022 10:32:42 - INFO - __main__ - Global step 400 Train loss 0.159683 Rouge-L 0.6554551222429487 on epoch=199
03/14/2022 10:32:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.148870 on epoch=204
03/14/2022 10:32:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.127699 on epoch=209
03/14/2022 10:32:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.127417 on epoch=214
03/14/2022 10:33:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.120711 on epoch=219
03/14/2022 10:33:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.105178 on epoch=224
03/14/2022 10:33:16 - INFO - __main__ - Global step 450 Train loss 0.125975 Rouge-L 0.6722595803948399 on epoch=224
03/14/2022 10:33:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.117493 on epoch=229
03/14/2022 10:33:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.123544 on epoch=234
03/14/2022 10:33:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.105703 on epoch=239
03/14/2022 10:33:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.120365 on epoch=244
03/14/2022 10:33:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.115152 on epoch=249
03/14/2022 10:33:47 - INFO - __main__ - Global step 500 Train loss 0.116451 Rouge-L 0.6362651888248417 on epoch=249
03/14/2022 10:33:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.124844 on epoch=254
03/14/2022 10:33:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.112347 on epoch=259
03/14/2022 10:34:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.113770 on epoch=264
03/14/2022 10:34:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.094137 on epoch=269
03/14/2022 10:34:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.130836 on epoch=274
03/14/2022 10:34:22 - INFO - __main__ - Global step 550 Train loss 0.115187 Rouge-L 0.6561230829879383 on epoch=274
03/14/2022 10:34:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.102915 on epoch=279
03/14/2022 10:34:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.110633 on epoch=284
03/14/2022 10:34:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.116750 on epoch=289
03/14/2022 10:34:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.102935 on epoch=294
03/14/2022 10:34:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.101478 on epoch=299
03/14/2022 10:34:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 10:34:48 - INFO - __main__ - Printing 3 examples
03/14/2022 10:34:48 - INFO - __main__ -  [wiki_split] sentence 1: Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese .  [SEP] sentence 2:  Originating in the English village of Cheddar in Somerset , it is now produced in several countries around the world .
03/14/2022 10:34:48 - INFO - __main__ - ['Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese , produced in several countries around the world .']
03/14/2022 10:34:48 - INFO - __main__ -  [wiki_split] sentence 1: He was the son of a wealthy farming family from Tno , Iwate .  [SEP] sentence 2:  He attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .
03/14/2022 10:34:48 - INFO - __main__ - ['He was the son of a wealthy farming family from Tno , Iwate , and attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .']
03/14/2022 10:34:48 - INFO - __main__ -  [wiki_split] sentence 1: In England it was the only such qualification from 1430 until 1832 .  [SEP] sentence 2:  It remained one of the qualifications ( after 1918 at a higher financial level ) until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .
03/14/2022 10:34:48 - INFO - __main__ - ['In England it was the only such qualification from the 1430s until 1832 and remained one of the qualifications until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .']
03/14/2022 10:34:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 10:34:48 - INFO - __main__ - Tokenizing Output ...
03/14/2022 10:34:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 10:34:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 10:34:48 - INFO - __main__ - Printing 3 examples
03/14/2022 10:34:48 - INFO - __main__ -  [wiki_split] sentence 1: Larten teaches Darren all about vampirism , but is sometimes shown to regret blooding him .  [SEP] sentence 2:  He decides to take Darren to the Cirque du Freak .
03/14/2022 10:34:48 - INFO - __main__ - ['Larten teaches Darren all about vampirism and sometimes is shown to regret blooding Darren and decides to take him to the Cirque du Freak .']
03/14/2022 10:34:48 - INFO - __main__ -  [wiki_split] sentence 1: Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 .  [SEP] sentence 2:  It specialises in creating humorous , factual documentaries .
03/14/2022 10:34:48 - INFO - __main__ - ['Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 , specialising in creating humorous , factual documentaries .']
03/14/2022 10:34:48 - INFO - __main__ -  [wiki_split] sentence 1: At the time of sale , the Drake Linden Hall building was listed at .  [SEP] sentence 2:  The sale included the additions made in the mid- 1970s , the gym and '' new '' classrooms building .
03/14/2022 10:34:48 - INFO - __main__ - ["At the time of sale , Drake Linden Hall measured , including additions made in the mid- 1970s , the gym and '' new '' classrooms building ."]
03/14/2022 10:34:48 - INFO - __main__ - Tokenizing Input ...
03/14/2022 10:34:48 - INFO - __main__ - Tokenizing Output ...
03/14/2022 10:34:48 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 10:34:54 - INFO - __main__ - Global step 600 Train loss 0.106942 Rouge-L 0.6499038780592354 on epoch=299
03/14/2022 10:34:54 - INFO - __main__ - save last model!
03/14/2022 10:34:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 10:34:59 - INFO - __main__ - Starting training!
03/14/2022 10:35:01 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 10:35:02 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 10:35:02 - INFO - __main__ - Printing 3 examples
03/14/2022 10:35:02 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 10:35:02 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 10:35:02 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 10:35:02 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 10:35:02 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 10:35:02 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 10:35:02 - INFO - __main__ - Tokenizing Input ...
03/14/2022 10:35:05 - INFO - __main__ - Tokenizing Output ...
03/14/2022 10:35:10 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 10:53:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_87_0.0003_8_predictions.txt
03/14/2022 10:53:42 - INFO - __main__ - Rouge-L on test data: 0.6571
03/14/2022 10:53:43 - INFO - __main__ - prefix=wiki_split_32_87, lr=0.0003, bsz=8, dev_performance=0.6722595803948399, test_performance=0.6570937537110109
03/14/2022 10:53:43 - INFO - __main__ - Running ... prefix=wiki_split_32_87, lr=0.0002, bsz=8 ...
03/14/2022 10:53:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 10:53:44 - INFO - __main__ - Printing 3 examples
03/14/2022 10:53:44 - INFO - __main__ -  [wiki_split] sentence 1: Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese .  [SEP] sentence 2:  Originating in the English village of Cheddar in Somerset , it is now produced in several countries around the world .
03/14/2022 10:53:44 - INFO - __main__ - ['Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese , produced in several countries around the world .']
03/14/2022 10:53:44 - INFO - __main__ -  [wiki_split] sentence 1: He was the son of a wealthy farming family from Tno , Iwate .  [SEP] sentence 2:  He attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .
03/14/2022 10:53:44 - INFO - __main__ - ['He was the son of a wealthy farming family from Tno , Iwate , and attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .']
03/14/2022 10:53:44 - INFO - __main__ -  [wiki_split] sentence 1: In England it was the only such qualification from 1430 until 1832 .  [SEP] sentence 2:  It remained one of the qualifications ( after 1918 at a higher financial level ) until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .
03/14/2022 10:53:44 - INFO - __main__ - ['In England it was the only such qualification from the 1430s until 1832 and remained one of the qualifications until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .']
03/14/2022 10:53:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 10:53:44 - INFO - __main__ - Tokenizing Output ...
03/14/2022 10:53:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 10:53:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 10:53:44 - INFO - __main__ - Printing 3 examples
03/14/2022 10:53:44 - INFO - __main__ -  [wiki_split] sentence 1: Larten teaches Darren all about vampirism , but is sometimes shown to regret blooding him .  [SEP] sentence 2:  He decides to take Darren to the Cirque du Freak .
03/14/2022 10:53:44 - INFO - __main__ - ['Larten teaches Darren all about vampirism and sometimes is shown to regret blooding Darren and decides to take him to the Cirque du Freak .']
03/14/2022 10:53:44 - INFO - __main__ -  [wiki_split] sentence 1: Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 .  [SEP] sentence 2:  It specialises in creating humorous , factual documentaries .
03/14/2022 10:53:44 - INFO - __main__ - ['Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 , specialising in creating humorous , factual documentaries .']
03/14/2022 10:53:44 - INFO - __main__ -  [wiki_split] sentence 1: At the time of sale , the Drake Linden Hall building was listed at .  [SEP] sentence 2:  The sale included the additions made in the mid- 1970s , the gym and '' new '' classrooms building .
03/14/2022 10:53:44 - INFO - __main__ - ["At the time of sale , Drake Linden Hall measured , including additions made in the mid- 1970s , the gym and '' new '' classrooms building ."]
03/14/2022 10:53:44 - INFO - __main__ - Tokenizing Input ...
03/14/2022 10:53:44 - INFO - __main__ - Tokenizing Output ...
03/14/2022 10:53:44 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 10:53:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 10:53:56 - INFO - __main__ - Starting training!
03/14/2022 10:54:00 - INFO - __main__ - Step 10 Global step 10 Train loss 18.975431 on epoch=4
03/14/2022 10:54:05 - INFO - __main__ - Step 20 Global step 20 Train loss 12.277798 on epoch=9
03/14/2022 10:54:10 - INFO - __main__ - Step 30 Global step 30 Train loss 4.966611 on epoch=14
03/14/2022 10:54:14 - INFO - __main__ - Step 40 Global step 40 Train loss 2.583403 on epoch=19
03/14/2022 10:54:19 - INFO - __main__ - Step 50 Global step 50 Train loss 1.802323 on epoch=24
03/14/2022 10:54:29 - INFO - __main__ - Global step 50 Train loss 8.121113 Rouge-L 0.4789529335112375 on epoch=24
03/14/2022 10:54:34 - INFO - __main__ - Step 60 Global step 60 Train loss 1.344842 on epoch=29
03/14/2022 10:54:39 - INFO - __main__ - Step 70 Global step 70 Train loss 1.199487 on epoch=34
03/14/2022 10:54:44 - INFO - __main__ - Step 80 Global step 80 Train loss 1.184533 on epoch=39
03/14/2022 10:54:49 - INFO - __main__ - Step 90 Global step 90 Train loss 0.884748 on epoch=44
03/14/2022 10:54:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.793982 on epoch=49
03/14/2022 10:54:59 - INFO - __main__ - Global step 100 Train loss 1.081518 Rouge-L 0.527702453364103 on epoch=49
03/14/2022 10:55:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.697604 on epoch=54
03/14/2022 10:55:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.672847 on epoch=59
03/14/2022 10:55:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.581683 on epoch=64
03/14/2022 10:55:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.570792 on epoch=69
03/14/2022 10:55:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.513517 on epoch=74
03/14/2022 10:55:30 - INFO - __main__ - Global step 150 Train loss 0.607289 Rouge-L 0.5151989992451349 on epoch=74
03/14/2022 10:55:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.464348 on epoch=79
03/14/2022 10:55:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.413844 on epoch=84
03/14/2022 10:55:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.417775 on epoch=89
03/14/2022 10:55:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.351640 on epoch=94
03/14/2022 10:55:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.319763 on epoch=99
03/14/2022 10:55:59 - INFO - __main__ - Global step 200 Train loss 0.393474 Rouge-L 0.4499577462835131 on epoch=99
03/14/2022 10:56:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.331038 on epoch=104
03/14/2022 10:56:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.323850 on epoch=109
03/14/2022 10:56:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.310407 on epoch=114
03/14/2022 10:56:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.278084 on epoch=119
03/14/2022 10:56:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.246290 on epoch=124
03/14/2022 10:56:29 - INFO - __main__ - Global step 250 Train loss 0.297934 Rouge-L 0.6612319584691133 on epoch=124
03/14/2022 10:56:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.260334 on epoch=129
03/14/2022 10:56:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.238915 on epoch=134
03/14/2022 10:56:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.239844 on epoch=139
03/14/2022 10:56:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.261236 on epoch=144
03/14/2022 10:56:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.214783 on epoch=149
03/14/2022 10:57:01 - INFO - __main__ - Global step 300 Train loss 0.243022 Rouge-L 0.6902108583363323 on epoch=149
03/14/2022 10:57:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.176906 on epoch=154
03/14/2022 10:57:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.208581 on epoch=159
03/14/2022 10:57:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.200797 on epoch=164
03/14/2022 10:57:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.182268 on epoch=169
03/14/2022 10:57:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.197807 on epoch=174
03/14/2022 10:57:33 - INFO - __main__ - Global step 350 Train loss 0.193272 Rouge-L 0.6888945645737511 on epoch=174
03/14/2022 10:57:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.171274 on epoch=179
03/14/2022 10:57:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.191705 on epoch=184
03/14/2022 10:57:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.187479 on epoch=189
03/14/2022 10:57:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.192810 on epoch=194
03/14/2022 10:57:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.191897 on epoch=199
03/14/2022 10:58:03 - INFO - __main__ - Global step 400 Train loss 0.187033 Rouge-L 0.696651782160347 on epoch=199
03/14/2022 10:58:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.198553 on epoch=204
03/14/2022 10:58:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.185236 on epoch=209
03/14/2022 10:58:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.179779 on epoch=214
03/14/2022 10:58:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.165557 on epoch=219
03/14/2022 10:58:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.153029 on epoch=224
03/14/2022 10:58:34 - INFO - __main__ - Global step 450 Train loss 0.176431 Rouge-L 0.6746499792206164 on epoch=224
03/14/2022 10:58:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.175444 on epoch=229
03/14/2022 10:58:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.164540 on epoch=234
03/14/2022 10:58:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.154598 on epoch=239
03/14/2022 10:58:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.143936 on epoch=244
03/14/2022 10:58:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.143054 on epoch=249
03/14/2022 10:59:05 - INFO - __main__ - Global step 500 Train loss 0.156315 Rouge-L 0.7040823015004897 on epoch=249
03/14/2022 10:59:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.156547 on epoch=254
03/14/2022 10:59:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.163255 on epoch=259
03/14/2022 10:59:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.156650 on epoch=264
03/14/2022 10:59:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.140233 on epoch=269
03/14/2022 10:59:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.142756 on epoch=274
03/14/2022 10:59:36 - INFO - __main__ - Global step 550 Train loss 0.151888 Rouge-L 0.6834744832147248 on epoch=274
03/14/2022 10:59:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.150613 on epoch=279
03/14/2022 10:59:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.144680 on epoch=284
03/14/2022 10:59:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.134192 on epoch=289
03/14/2022 10:59:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.136026 on epoch=294
03/14/2022 11:00:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.135557 on epoch=299
03/14/2022 11:00:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:00:02 - INFO - __main__ - Printing 3 examples
03/14/2022 11:00:02 - INFO - __main__ -  [wiki_split] sentence 1: Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese .  [SEP] sentence 2:  Originating in the English village of Cheddar in Somerset , it is now produced in several countries around the world .
03/14/2022 11:00:02 - INFO - __main__ - ['Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese , produced in several countries around the world .']
03/14/2022 11:00:02 - INFO - __main__ -  [wiki_split] sentence 1: He was the son of a wealthy farming family from Tno , Iwate .  [SEP] sentence 2:  He attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .
03/14/2022 11:00:02 - INFO - __main__ - ['He was the son of a wealthy farming family from Tno , Iwate , and attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .']
03/14/2022 11:00:02 - INFO - __main__ -  [wiki_split] sentence 1: In England it was the only such qualification from 1430 until 1832 .  [SEP] sentence 2:  It remained one of the qualifications ( after 1918 at a higher financial level ) until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .
03/14/2022 11:00:02 - INFO - __main__ - ['In England it was the only such qualification from the 1430s until 1832 and remained one of the qualifications until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .']
03/14/2022 11:00:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 11:00:02 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:00:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 11:00:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:00:02 - INFO - __main__ - Printing 3 examples
03/14/2022 11:00:02 - INFO - __main__ -  [wiki_split] sentence 1: Larten teaches Darren all about vampirism , but is sometimes shown to regret blooding him .  [SEP] sentence 2:  He decides to take Darren to the Cirque du Freak .
03/14/2022 11:00:02 - INFO - __main__ - ['Larten teaches Darren all about vampirism and sometimes is shown to regret blooding Darren and decides to take him to the Cirque du Freak .']
03/14/2022 11:00:02 - INFO - __main__ -  [wiki_split] sentence 1: Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 .  [SEP] sentence 2:  It specialises in creating humorous , factual documentaries .
03/14/2022 11:00:02 - INFO - __main__ - ['Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 , specialising in creating humorous , factual documentaries .']
03/14/2022 11:00:02 - INFO - __main__ -  [wiki_split] sentence 1: At the time of sale , the Drake Linden Hall building was listed at .  [SEP] sentence 2:  The sale included the additions made in the mid- 1970s , the gym and '' new '' classrooms building .
03/14/2022 11:00:02 - INFO - __main__ - ["At the time of sale , Drake Linden Hall measured , including additions made in the mid- 1970s , the gym and '' new '' classrooms building ."]
03/14/2022 11:00:02 - INFO - __main__ - Tokenizing Input ...
03/14/2022 11:00:02 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:00:02 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 11:00:07 - INFO - __main__ - Global step 600 Train loss 0.140213 Rouge-L 0.7028584295597948 on epoch=299
03/14/2022 11:00:07 - INFO - __main__ - save last model!
03/14/2022 11:00:13 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 11:00:14 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 11:00:14 - INFO - __main__ - Printing 3 examples
03/14/2022 11:00:14 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 11:00:14 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 11:00:14 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 11:00:14 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 11:00:14 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 11:00:14 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 11:00:14 - INFO - __main__ - Tokenizing Input ...
03/14/2022 11:00:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 11:00:15 - INFO - __main__ - Starting training!
03/14/2022 11:00:17 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:00:22 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 11:15:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_87_0.0002_8_predictions.txt
03/14/2022 11:15:51 - INFO - __main__ - Rouge-L on test data: 0.7205
03/14/2022 11:15:52 - INFO - __main__ - prefix=wiki_split_32_87, lr=0.0002, bsz=8, dev_performance=0.7040823015004897, test_performance=0.7205338296187636
03/14/2022 11:15:52 - INFO - __main__ - Running ... prefix=wiki_split_32_87, lr=0.0001, bsz=8 ...
03/14/2022 11:15:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:15:53 - INFO - __main__ - Printing 3 examples
03/14/2022 11:15:53 - INFO - __main__ -  [wiki_split] sentence 1: Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese .  [SEP] sentence 2:  Originating in the English village of Cheddar in Somerset , it is now produced in several countries around the world .
03/14/2022 11:15:53 - INFO - __main__ - ['Cheddar cheese is a relatively hard , pale yellow to off - white ( unless artificially coloured ) , and sometimes sharp - tasting cheese , produced in several countries around the world .']
03/14/2022 11:15:53 - INFO - __main__ -  [wiki_split] sentence 1: He was the son of a wealthy farming family from Tno , Iwate .  [SEP] sentence 2:  He attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .
03/14/2022 11:15:53 - INFO - __main__ - ['He was the son of a wealthy farming family from Tno , Iwate , and attended Shiritsu Tetsugakukan ( now Toyo University ) and then graduated with a degree in literature from Waseda University in 1905 .']
03/14/2022 11:15:53 - INFO - __main__ -  [wiki_split] sentence 1: In England it was the only such qualification from 1430 until 1832 .  [SEP] sentence 2:  It remained one of the qualifications ( after 1918 at a higher financial level ) until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .
03/14/2022 11:15:53 - INFO - __main__ - ['In England it was the only such qualification from the 1430s until 1832 and remained one of the qualifications until the mid twentieth century , although with declining importance after Reform Acts gradually enfranchised voters who were not freeholders .']
03/14/2022 11:15:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 11:15:53 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:15:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 11:15:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:15:53 - INFO - __main__ - Printing 3 examples
03/14/2022 11:15:53 - INFO - __main__ -  [wiki_split] sentence 1: Larten teaches Darren all about vampirism , but is sometimes shown to regret blooding him .  [SEP] sentence 2:  He decides to take Darren to the Cirque du Freak .
03/14/2022 11:15:53 - INFO - __main__ - ['Larten teaches Darren all about vampirism and sometimes is shown to regret blooding Darren and decides to take him to the Cirque du Freak .']
03/14/2022 11:15:53 - INFO - __main__ -  [wiki_split] sentence 1: Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 .  [SEP] sentence 2:  It specialises in creating humorous , factual documentaries .
03/14/2022 11:15:53 - INFO - __main__ - ['Productions is a British television production company formed by comedian Jo Brand and film maker Claire Whalley in 2009 , specialising in creating humorous , factual documentaries .']
03/14/2022 11:15:53 - INFO - __main__ -  [wiki_split] sentence 1: At the time of sale , the Drake Linden Hall building was listed at .  [SEP] sentence 2:  The sale included the additions made in the mid- 1970s , the gym and '' new '' classrooms building .
03/14/2022 11:15:53 - INFO - __main__ - ["At the time of sale , Drake Linden Hall measured , including additions made in the mid- 1970s , the gym and '' new '' classrooms building ."]
03/14/2022 11:15:53 - INFO - __main__ - Tokenizing Input ...
03/14/2022 11:15:53 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:15:53 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 11:16:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 11:16:05 - INFO - __main__ - Starting training!
03/14/2022 11:16:09 - INFO - __main__ - Step 10 Global step 10 Train loss 20.235218 on epoch=4
03/14/2022 11:16:14 - INFO - __main__ - Step 20 Global step 20 Train loss 15.428411 on epoch=9
03/14/2022 11:16:19 - INFO - __main__ - Step 30 Global step 30 Train loss 9.188475 on epoch=14
03/14/2022 11:16:24 - INFO - __main__ - Step 40 Global step 40 Train loss 7.044084 on epoch=19
03/14/2022 11:16:29 - INFO - __main__ - Step 50 Global step 50 Train loss 5.404967 on epoch=24
03/14/2022 11:16:41 - INFO - __main__ - Global step 50 Train loss 11.460230 Rouge-L 0.6016575733646159 on epoch=24
03/14/2022 11:16:46 - INFO - __main__ - Step 60 Global step 60 Train loss 4.687707 on epoch=29
03/14/2022 11:16:51 - INFO - __main__ - Step 70 Global step 70 Train loss 3.586468 on epoch=34
03/14/2022 11:16:56 - INFO - __main__ - Step 80 Global step 80 Train loss 2.532735 on epoch=39
03/14/2022 11:17:01 - INFO - __main__ - Step 90 Global step 90 Train loss 2.253379 on epoch=44
03/14/2022 11:17:06 - INFO - __main__ - Step 100 Global step 100 Train loss 1.967260 on epoch=49
03/14/2022 11:17:15 - INFO - __main__ - Global step 100 Train loss 3.005510 Rouge-L 0.46818841650868614 on epoch=49
03/14/2022 11:17:20 - INFO - __main__ - Step 110 Global step 110 Train loss 1.690652 on epoch=54
03/14/2022 11:17:25 - INFO - __main__ - Step 120 Global step 120 Train loss 1.430201 on epoch=59
03/14/2022 11:17:30 - INFO - __main__ - Step 130 Global step 130 Train loss 1.298364 on epoch=64
03/14/2022 11:17:35 - INFO - __main__ - Step 140 Global step 140 Train loss 1.218196 on epoch=69
03/14/2022 11:17:40 - INFO - __main__ - Step 150 Global step 150 Train loss 1.123551 on epoch=74
03/14/2022 11:17:50 - INFO - __main__ - Global step 150 Train loss 1.352193 Rouge-L 0.4600092080304558 on epoch=74
03/14/2022 11:17:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.986568 on epoch=79
03/14/2022 11:18:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.988679 on epoch=84
03/14/2022 11:18:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.890625 on epoch=89
03/14/2022 11:18:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.833399 on epoch=94
03/14/2022 11:18:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.788048 on epoch=99
03/14/2022 11:18:24 - INFO - __main__ - Global step 200 Train loss 0.897464 Rouge-L 0.46230085448973857 on epoch=99
03/14/2022 11:18:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.817060 on epoch=104
03/14/2022 11:18:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.699311 on epoch=109
03/14/2022 11:18:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.679200 on epoch=114
03/14/2022 11:18:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.645240 on epoch=119
03/14/2022 11:18:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.580399 on epoch=124
03/14/2022 11:18:57 - INFO - __main__ - Global step 250 Train loss 0.684242 Rouge-L 0.24475693841896481 on epoch=124
03/14/2022 11:19:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.571880 on epoch=129
03/14/2022 11:19:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.591368 on epoch=134
03/14/2022 11:19:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.545106 on epoch=139
03/14/2022 11:19:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.561339 on epoch=144
03/14/2022 11:19:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.486867 on epoch=149
03/14/2022 11:19:29 - INFO - __main__ - Global step 300 Train loss 0.551312 Rouge-L 0.25742350608451975 on epoch=149
03/14/2022 11:19:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.492653 on epoch=154
03/14/2022 11:19:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.491740 on epoch=159
03/14/2022 11:19:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.470747 on epoch=164
03/14/2022 11:19:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.432733 on epoch=169
03/14/2022 11:19:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.469433 on epoch=174
03/14/2022 11:20:02 - INFO - __main__ - Global step 350 Train loss 0.471461 Rouge-L 0.1995890364883931 on epoch=174
03/14/2022 11:20:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.425561 on epoch=179
03/14/2022 11:20:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.382889 on epoch=184
03/14/2022 11:20:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.379736 on epoch=189
03/14/2022 11:20:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.361315 on epoch=194
03/14/2022 11:20:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.352294 on epoch=199
03/14/2022 11:20:33 - INFO - __main__ - Global step 400 Train loss 0.380359 Rouge-L 0.1891772716948069 on epoch=199
03/14/2022 11:20:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.343814 on epoch=204
03/14/2022 11:20:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.324526 on epoch=209
03/14/2022 11:20:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.329154 on epoch=214
03/14/2022 11:20:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.316230 on epoch=219
03/14/2022 11:20:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.298079 on epoch=224
03/14/2022 11:21:07 - INFO - __main__ - Global step 450 Train loss 0.322361 Rouge-L 0.20512702423917115 on epoch=224
03/14/2022 11:21:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.278914 on epoch=229
03/14/2022 11:21:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.306133 on epoch=234
03/14/2022 11:21:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.268121 on epoch=239
03/14/2022 11:21:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.271453 on epoch=244
03/14/2022 11:21:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.247080 on epoch=249
03/14/2022 11:21:41 - INFO - __main__ - Global step 500 Train loss 0.274340 Rouge-L 0.3283835336387495 on epoch=249
03/14/2022 11:21:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.235666 on epoch=254
03/14/2022 11:21:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.263671 on epoch=259
03/14/2022 11:21:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.253945 on epoch=264
03/14/2022 11:22:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.227203 on epoch=269
03/14/2022 11:22:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.251981 on epoch=274
03/14/2022 11:22:12 - INFO - __main__ - Global step 550 Train loss 0.246493 Rouge-L 0.6482922889050118 on epoch=274
03/14/2022 11:22:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.201770 on epoch=279
03/14/2022 11:22:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.213485 on epoch=284
03/14/2022 11:22:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.234424 on epoch=289
03/14/2022 11:22:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.208593 on epoch=294
03/14/2022 11:22:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.216632 on epoch=299
03/14/2022 11:22:42 - INFO - __main__ - Global step 600 Train loss 0.214981 Rouge-L 0.6587466114113519 on epoch=299
03/14/2022 11:22:43 - INFO - __main__ - save last model!
03/14/2022 11:22:50 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 11:22:50 - INFO - __main__ - Start tokenizing ... 5000 instances
03/14/2022 11:22:50 - INFO - __main__ - Printing 3 examples
03/14/2022 11:22:50 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/14/2022 11:22:50 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/14/2022 11:22:50 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/14/2022 11:22:50 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/14/2022 11:22:50 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/14/2022 11:22:50 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/14/2022 11:22:50 - INFO - __main__ - Tokenizing Input ...
03/14/2022 11:22:53 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:22:58 - INFO - __main__ - Loaded 5000 examples from test data
03/14/2022 11:40:39 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-wiki_split/wiki_split_32_87_0.0001_8_predictions.txt
03/14/2022 11:40:43 - INFO - __main__ - Rouge-L on test data: 0.6866
03/14/2022 11:40:43 - INFO - __main__ - prefix=wiki_split_32_87, lr=0.0001, bsz=8, dev_performance=0.6587466114113519, test_performance=0.6865688511413964
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (17089): No such process
Task: yelp_polarity, Checkpoint: None, Identifier: T5-large-ft-random
Output directory () already exists and is not empty.
03/14/2022 11:40:49 - INFO - __main__ - Namespace(task_dir='data/yelp_polarity/', task_name='yelp_polarity', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-yelp_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/14/2022 11:40:49 - INFO - __main__ - models/T5-large-ft-random/singletask-yelp_polarity
03/14/2022 11:40:49 - INFO - __main__ - Namespace(task_dir='data/yelp_polarity/', task_name='yelp_polarity', identifier='T5-large-ft-random', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-random/singletask-yelp_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='0,1')
03/14/2022 11:40:49 - INFO - __main__ - models/T5-large-ft-random/singletask-yelp_polarity
03/14/2022 11:40:51 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/14/2022 11:40:51 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/14/2022 11:40:51 - INFO - __main__ - args.device: cuda:0
03/14/2022 11:40:51 - INFO - __main__ - Using 2 gpus
03/14/2022 11:40:51 - INFO - __main__ - Fine-tuning the following samples: ['yelp_polarity_16_100', 'yelp_polarity_16_13', 'yelp_polarity_16_21', 'yelp_polarity_16_42', 'yelp_polarity_16_87']
03/14/2022 11:40:51 - INFO - __main__ - args.device: cuda:1
03/14/2022 11:40:51 - INFO - __main__ - Using 2 gpus
03/14/2022 11:40:51 - INFO - __main__ - Fine-tuning the following samples: ['yelp_polarity_16_100', 'yelp_polarity_16_13', 'yelp_polarity_16_21', 'yelp_polarity_16_42', 'yelp_polarity_16_87']
03/14/2022 11:40:56 - INFO - __main__ - Running ... prefix=yelp_polarity_16_100, lr=0.0005, bsz=8 ...
03/14/2022 11:40:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:40:57 - INFO - __main__ - Printing 3 examples
03/14/2022 11:40:57 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/14/2022 11:40:57 - INFO - __main__ - ['negative']
03/14/2022 11:40:57 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/14/2022 11:40:57 - INFO - __main__ - ['negative']
03/14/2022 11:40:57 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/14/2022 11:40:57 - INFO - __main__ - ['negative']
03/14/2022 11:40:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 11:40:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:40:57 - INFO - __main__ - Printing 3 examples
03/14/2022 11:40:57 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/14/2022 11:40:57 - INFO - __main__ - ['negative']
03/14/2022 11:40:57 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/14/2022 11:40:57 - INFO - __main__ - ['negative']
03/14/2022 11:40:57 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/14/2022 11:40:57 - INFO - __main__ - ['negative']
03/14/2022 11:40:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 11:40:57 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:40:57 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:40:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 11:40:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:40:57 - INFO - __main__ - Printing 3 examples
03/14/2022 11:40:57 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/14/2022 11:40:57 - INFO - __main__ - ['negative']
03/14/2022 11:40:57 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/14/2022 11:40:57 - INFO - __main__ - ['negative']
03/14/2022 11:40:57 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/14/2022 11:40:57 - INFO - __main__ - ['negative']
03/14/2022 11:40:57 - INFO - __main__ - Tokenizing Input ...
03/14/2022 11:40:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 11:40:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:40:57 - INFO - __main__ - Printing 3 examples
03/14/2022 11:40:57 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/14/2022 11:40:57 - INFO - __main__ - ['negative']
03/14/2022 11:40:57 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/14/2022 11:40:57 - INFO - __main__ - ['negative']
03/14/2022 11:40:57 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/14/2022 11:40:57 - INFO - __main__ - ['negative']
03/14/2022 11:40:57 - INFO - __main__ - Tokenizing Input ...
03/14/2022 11:40:57 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:40:57 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:40:57 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 11:40:57 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 11:41:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 11:41:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 11:41:10 - INFO - __main__ - Starting training!
03/14/2022 11:41:10 - INFO - __main__ - Starting training!
03/14/2022 11:41:16 - INFO - __main__ - Step 10 Global step 10 Train loss 23.968962 on epoch=4
03/14/2022 11:41:22 - INFO - __main__ - Step 20 Global step 20 Train loss 16.910526 on epoch=9
03/14/2022 11:41:28 - INFO - __main__ - Step 30 Global step 30 Train loss 14.691599 on epoch=14
03/14/2022 11:41:34 - INFO - __main__ - Step 40 Global step 40 Train loss 12.035636 on epoch=19
03/14/2022 11:41:41 - INFO - __main__ - Step 50 Global step 50 Train loss 10.325590 on epoch=24
03/14/2022 11:41:41 - INFO - __main__ - Global step 50 Train loss 15.586464 Classification-F1 0.0 on epoch=24
03/14/2022 11:41:49 - INFO - __main__ - Step 60 Global step 60 Train loss 4.838543 on epoch=29
03/14/2022 11:41:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.461858 on epoch=34
03/14/2022 11:42:01 - INFO - __main__ - Step 80 Global step 80 Train loss 0.146874 on epoch=39
03/14/2022 11:42:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.027347 on epoch=44
03/14/2022 11:42:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.008077 on epoch=49
03/14/2022 11:42:14 - INFO - __main__ - Global step 100 Train loss 1.096540 Classification-F1 0.592741935483871 on epoch=49
03/14/2022 11:42:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.011242 on epoch=54
03/14/2022 11:42:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.005399 on epoch=59
03/14/2022 11:42:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.006691 on epoch=64
03/14/2022 11:42:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.011508 on epoch=69
03/14/2022 11:42:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.004086 on epoch=74
03/14/2022 11:42:48 - INFO - __main__ - Global step 150 Train loss 0.007785 Classification-F1 0.906158357771261 on epoch=74
03/14/2022 11:42:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.006911 on epoch=79
03/14/2022 11:43:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.000334 on epoch=84
03/14/2022 11:43:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.000466 on epoch=89
03/14/2022 11:43:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.000547 on epoch=94
03/14/2022 11:43:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.000112 on epoch=99
03/14/2022 11:43:20 - INFO - __main__ - Global step 200 Train loss 0.001674 Classification-F1 0.9375 on epoch=99
03/14/2022 11:43:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.000728 on epoch=104
03/14/2022 11:43:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000192 on epoch=109
03/14/2022 11:43:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.000353 on epoch=114
03/14/2022 11:43:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000062 on epoch=119
03/14/2022 11:43:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.943909 on epoch=124
03/14/2022 11:43:52 - INFO - __main__ - Global step 250 Train loss 0.189049 Classification-F1 0.5134502923976608 on epoch=124
03/14/2022 11:43:57 - INFO - __main__ - Step 260 Global step 260 Train loss 1.603685 on epoch=129
03/14/2022 11:44:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.910416 on epoch=134
03/14/2022 11:44:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.231928 on epoch=139
03/14/2022 11:44:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.107916 on epoch=144
03/14/2022 11:44:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.506300 on epoch=149
03/14/2022 11:44:22 - INFO - __main__ - Global step 300 Train loss 0.672049 Classification-F1 0.8435972629521017 on epoch=149
03/14/2022 11:44:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.191934 on epoch=154
03/14/2022 11:44:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.083739 on epoch=159
03/14/2022 11:44:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.282345 on epoch=164
03/14/2022 11:44:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.170215 on epoch=169
03/14/2022 11:44:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.004507 on epoch=174
03/14/2022 11:44:53 - INFO - __main__ - Global step 350 Train loss 0.146548 Classification-F1 0.906158357771261 on epoch=174
03/14/2022 11:44:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.006225 on epoch=179
03/14/2022 11:45:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001789 on epoch=184
03/14/2022 11:45:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.002369 on epoch=189
03/14/2022 11:45:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.159406 on epoch=194
03/14/2022 11:45:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.059915 on epoch=199
03/14/2022 11:45:24 - INFO - __main__ - Global step 400 Train loss 0.045941 Classification-F1 0.906158357771261 on epoch=199
03/14/2022 11:45:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.098521 on epoch=204
03/14/2022 11:45:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.167214 on epoch=209
03/14/2022 11:45:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.009450 on epoch=214
03/14/2022 11:45:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.014378 on epoch=219
03/14/2022 11:45:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.079208 on epoch=224
03/14/2022 11:45:55 - INFO - __main__ - Global step 450 Train loss 0.073754 Classification-F1 0.8125 on epoch=224
03/14/2022 11:46:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.256863 on epoch=229
03/14/2022 11:46:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.182220 on epoch=234
03/14/2022 11:46:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.030807 on epoch=239
03/14/2022 11:46:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.420154 on epoch=244
03/14/2022 11:46:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.342867 on epoch=249
03/14/2022 11:46:26 - INFO - __main__ - Global step 500 Train loss 0.246582 Classification-F1 0.3992490613266583 on epoch=249
03/14/2022 11:46:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.385619 on epoch=254
03/14/2022 11:46:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.338765 on epoch=259
03/14/2022 11:46:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.300514 on epoch=264
03/14/2022 11:46:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.268324 on epoch=269
03/14/2022 11:46:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.246207 on epoch=274
03/14/2022 11:46:57 - INFO - __main__ - Global step 550 Train loss 0.307886 Classification-F1 0.6101882613510521 on epoch=274
03/14/2022 11:47:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.230289 on epoch=279
03/14/2022 11:47:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.189329 on epoch=284
03/14/2022 11:47:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.211513 on epoch=289
03/14/2022 11:47:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.099602 on epoch=294
03/14/2022 11:47:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.081705 on epoch=299
03/14/2022 11:47:28 - INFO - __main__ - Global step 600 Train loss 0.162488 Classification-F1 0.6666666666666667 on epoch=299
03/14/2022 11:47:28 - INFO - __main__ - save last model!
03/14/2022 11:47:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:47:28 - INFO - __main__ - Printing 3 examples
03/14/2022 11:47:28 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/14/2022 11:47:28 - INFO - __main__ - ['negative']
03/14/2022 11:47:28 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/14/2022 11:47:28 - INFO - __main__ - ['negative']
03/14/2022 11:47:28 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/14/2022 11:47:28 - INFO - __main__ - ['negative']
03/14/2022 11:47:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 11:47:29 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:47:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 11:47:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:47:29 - INFO - __main__ - Printing 3 examples
03/14/2022 11:47:29 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/14/2022 11:47:29 - INFO - __main__ - ['negative']
03/14/2022 11:47:29 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/14/2022 11:47:29 - INFO - __main__ - ['negative']
03/14/2022 11:47:29 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/14/2022 11:47:29 - INFO - __main__ - ['negative']
03/14/2022 11:47:29 - INFO - __main__ - Tokenizing Input ...
03/14/2022 11:47:29 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:47:29 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 11:47:35 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 11:47:36 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 11:47:36 - INFO - __main__ - Printing 3 examples
03/14/2022 11:47:36 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 11:47:36 - INFO - __main__ - ['negative']
03/14/2022 11:47:36 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 11:47:36 - INFO - __main__ - ['negative']
03/14/2022 11:47:36 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 11:47:36 - INFO - __main__ - ['negative']
03/14/2022 11:47:36 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 11:47:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 11:47:39 - INFO - __main__ - Starting training!
03/14/2022 11:47:42 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:47:50 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 11:50:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_100_0.0005_8_predictions.txt
03/14/2022 11:50:31 - INFO - __main__ - Classification-F1 on test data: 0.9431
03/14/2022 11:50:32 - INFO - __main__ - prefix=yelp_polarity_16_100, lr=0.0005, bsz=8, dev_performance=0.9375, test_performance=0.943148051948052
03/14/2022 11:50:32 - INFO - __main__ - Running ... prefix=yelp_polarity_16_100, lr=0.0003, bsz=8 ...
03/14/2022 11:50:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:50:32 - INFO - __main__ - Printing 3 examples
03/14/2022 11:50:32 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/14/2022 11:50:32 - INFO - __main__ - ['negative']
03/14/2022 11:50:32 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/14/2022 11:50:32 - INFO - __main__ - ['negative']
03/14/2022 11:50:32 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/14/2022 11:50:32 - INFO - __main__ - ['negative']
03/14/2022 11:50:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 11:50:32 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:50:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 11:50:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:50:32 - INFO - __main__ - Printing 3 examples
03/14/2022 11:50:32 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/14/2022 11:50:32 - INFO - __main__ - ['negative']
03/14/2022 11:50:32 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/14/2022 11:50:32 - INFO - __main__ - ['negative']
03/14/2022 11:50:32 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/14/2022 11:50:32 - INFO - __main__ - ['negative']
03/14/2022 11:50:32 - INFO - __main__ - Tokenizing Input ...
03/14/2022 11:50:33 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:50:33 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 11:50:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 11:50:43 - INFO - __main__ - Starting training!
03/14/2022 11:50:49 - INFO - __main__ - Step 10 Global step 10 Train loss 24.261126 on epoch=4
03/14/2022 11:50:55 - INFO - __main__ - Step 20 Global step 20 Train loss 17.266132 on epoch=9
03/14/2022 11:51:01 - INFO - __main__ - Step 30 Global step 30 Train loss 15.970775 on epoch=14
03/14/2022 11:51:07 - INFO - __main__ - Step 40 Global step 40 Train loss 14.536761 on epoch=19
03/14/2022 11:51:13 - INFO - __main__ - Step 50 Global step 50 Train loss 13.269529 on epoch=24
03/14/2022 11:51:22 - INFO - __main__ - Global step 50 Train loss 17.060865 Classification-F1 0.0 on epoch=24
03/14/2022 11:51:29 - INFO - __main__ - Step 60 Global step 60 Train loss 11.930053 on epoch=29
03/14/2022 11:51:35 - INFO - __main__ - Step 70 Global step 70 Train loss 11.126322 on epoch=34
03/14/2022 11:51:41 - INFO - __main__ - Step 80 Global step 80 Train loss 8.261042 on epoch=39
03/14/2022 11:51:47 - INFO - __main__ - Step 90 Global step 90 Train loss 6.993138 on epoch=44
03/14/2022 11:51:53 - INFO - __main__ - Step 100 Global step 100 Train loss 2.446302 on epoch=49
03/14/2022 11:51:53 - INFO - __main__ - Global step 100 Train loss 8.151370 Classification-F1 0.3333333333333333 on epoch=49
03/14/2022 11:52:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.440965 on epoch=54
03/14/2022 11:52:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.223194 on epoch=59
03/14/2022 11:52:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.164206 on epoch=64
03/14/2022 11:52:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.110992 on epoch=69
03/14/2022 11:52:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.076290 on epoch=74
03/14/2022 11:52:26 - INFO - __main__ - Global step 150 Train loss 0.203129 Classification-F1 0.9375 on epoch=74
03/14/2022 11:52:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.026769 on epoch=79
03/14/2022 11:52:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.074274 on epoch=84
03/14/2022 11:52:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.059164 on epoch=89
03/14/2022 11:52:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.060352 on epoch=94
03/14/2022 11:52:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.101774 on epoch=99
03/14/2022 11:52:58 - INFO - __main__ - Global step 200 Train loss 0.064467 Classification-F1 0.9375 on epoch=99
03/14/2022 11:53:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.044061 on epoch=104
03/14/2022 11:53:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.054393 on epoch=109
03/14/2022 11:53:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.094860 on epoch=114
03/14/2022 11:53:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.102085 on epoch=119
03/14/2022 11:53:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.082013 on epoch=124
03/14/2022 11:53:30 - INFO - __main__ - Global step 250 Train loss 0.075482 Classification-F1 0.9687194525904204 on epoch=124
03/14/2022 11:53:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.073626 on epoch=129
03/14/2022 11:53:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.041751 on epoch=134
03/14/2022 11:53:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.027332 on epoch=139
03/14/2022 11:53:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.083500 on epoch=144
03/14/2022 11:54:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.038344 on epoch=149
03/14/2022 11:54:03 - INFO - __main__ - Global step 300 Train loss 0.052910 Classification-F1 0.9687194525904204 on epoch=149
03/14/2022 11:54:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.041564 on epoch=154
03/14/2022 11:54:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.104230 on epoch=159
03/14/2022 11:54:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.122866 on epoch=164
03/14/2022 11:54:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.056257 on epoch=169
03/14/2022 11:54:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.506999 on epoch=174
03/14/2022 11:54:34 - INFO - __main__ - Global step 350 Train loss 0.166383 Classification-F1 0.3333333333333333 on epoch=174
03/14/2022 11:54:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.364859 on epoch=179
03/14/2022 11:54:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.106071 on epoch=184
03/14/2022 11:54:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.034566 on epoch=189
03/14/2022 11:54:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.023333 on epoch=194
03/14/2022 11:55:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.061855 on epoch=199
03/14/2022 11:55:06 - INFO - __main__ - Global step 400 Train loss 0.118137 Classification-F1 0.9687194525904204 on epoch=199
03/14/2022 11:55:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.013749 on epoch=204
03/14/2022 11:55:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.014570 on epoch=209
03/14/2022 11:55:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.014100 on epoch=214
03/14/2022 11:55:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.019803 on epoch=219
03/14/2022 11:55:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.024073 on epoch=224
03/14/2022 11:55:38 - INFO - __main__ - Global step 450 Train loss 0.017259 Classification-F1 0.9687194525904204 on epoch=224
03/14/2022 11:55:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.033012 on epoch=229
03/14/2022 11:55:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.021048 on epoch=234
03/14/2022 11:55:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.020466 on epoch=239
03/14/2022 11:56:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.018218 on epoch=244
03/14/2022 11:56:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.017488 on epoch=249
03/14/2022 11:56:09 - INFO - __main__ - Global step 500 Train loss 0.022046 Classification-F1 0.9375 on epoch=249
03/14/2022 11:56:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.009813 on epoch=254
03/14/2022 11:56:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.009907 on epoch=259
03/14/2022 11:56:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.008992 on epoch=264
03/14/2022 11:56:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.018393 on epoch=269
03/14/2022 11:56:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.022354 on epoch=274
03/14/2022 11:56:41 - INFO - __main__ - Global step 550 Train loss 0.013892 Classification-F1 0.9687194525904204 on epoch=274
03/14/2022 11:56:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.017785 on epoch=279
03/14/2022 11:56:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.011843 on epoch=284
03/14/2022 11:56:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.019444 on epoch=289
03/14/2022 11:57:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.009984 on epoch=294
03/14/2022 11:57:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.025546 on epoch=299
03/14/2022 11:57:12 - INFO - __main__ - Global step 600 Train loss 0.016920 Classification-F1 0.9687194525904204 on epoch=299
03/14/2022 11:57:12 - INFO - __main__ - save last model!
03/14/2022 11:57:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:57:13 - INFO - __main__ - Printing 3 examples
03/14/2022 11:57:13 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/14/2022 11:57:13 - INFO - __main__ - ['negative']
03/14/2022 11:57:13 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/14/2022 11:57:13 - INFO - __main__ - ['negative']
03/14/2022 11:57:13 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/14/2022 11:57:13 - INFO - __main__ - ['negative']
03/14/2022 11:57:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 11:57:13 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:57:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 11:57:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 11:57:13 - INFO - __main__ - Printing 3 examples
03/14/2022 11:57:13 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/14/2022 11:57:13 - INFO - __main__ - ['negative']
03/14/2022 11:57:13 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/14/2022 11:57:13 - INFO - __main__ - ['negative']
03/14/2022 11:57:13 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/14/2022 11:57:13 - INFO - __main__ - ['negative']
03/14/2022 11:57:13 - INFO - __main__ - Tokenizing Input ...
03/14/2022 11:57:13 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:57:13 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 11:57:19 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 11:57:20 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 11:57:20 - INFO - __main__ - Printing 3 examples
03/14/2022 11:57:20 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 11:57:20 - INFO - __main__ - ['negative']
03/14/2022 11:57:20 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 11:57:20 - INFO - __main__ - ['negative']
03/14/2022 11:57:20 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 11:57:20 - INFO - __main__ - ['negative']
03/14/2022 11:57:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 11:57:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 11:57:23 - INFO - __main__ - Starting training!
03/14/2022 11:57:26 - INFO - __main__ - Tokenizing Output ...
03/14/2022 11:57:34 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 12:00:18 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_100_0.0003_8_predictions.txt
03/14/2022 12:00:18 - INFO - __main__ - Classification-F1 on test data: 0.9404
03/14/2022 12:00:20 - INFO - __main__ - prefix=yelp_polarity_16_100, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.940364546393717
03/14/2022 12:00:20 - INFO - __main__ - Running ... prefix=yelp_polarity_16_100, lr=0.0002, bsz=8 ...
03/14/2022 12:00:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:00:20 - INFO - __main__ - Printing 3 examples
03/14/2022 12:00:20 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/14/2022 12:00:20 - INFO - __main__ - ['negative']
03/14/2022 12:00:20 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/14/2022 12:00:20 - INFO - __main__ - ['negative']
03/14/2022 12:00:20 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/14/2022 12:00:20 - INFO - __main__ - ['negative']
03/14/2022 12:00:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:00:20 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:00:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:00:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:00:21 - INFO - __main__ - Printing 3 examples
03/14/2022 12:00:21 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/14/2022 12:00:21 - INFO - __main__ - ['negative']
03/14/2022 12:00:21 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/14/2022 12:00:21 - INFO - __main__ - ['negative']
03/14/2022 12:00:21 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/14/2022 12:00:21 - INFO - __main__ - ['negative']
03/14/2022 12:00:21 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:00:21 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:00:21 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:00:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:00:31 - INFO - __main__ - Starting training!
03/14/2022 12:00:37 - INFO - __main__ - Step 10 Global step 10 Train loss 23.632687 on epoch=4
03/14/2022 12:00:43 - INFO - __main__ - Step 20 Global step 20 Train loss 18.946789 on epoch=9
03/14/2022 12:00:49 - INFO - __main__ - Step 30 Global step 30 Train loss 15.606997 on epoch=14
03/14/2022 12:00:55 - INFO - __main__ - Step 40 Global step 40 Train loss 16.291832 on epoch=19
03/14/2022 12:01:01 - INFO - __main__ - Step 50 Global step 50 Train loss 15.110929 on epoch=24
03/14/2022 12:01:12 - INFO - __main__ - Global step 50 Train loss 17.917845 Classification-F1 0.0 on epoch=24
03/14/2022 12:01:19 - INFO - __main__ - Step 60 Global step 60 Train loss 14.055799 on epoch=29
03/14/2022 12:01:25 - INFO - __main__ - Step 70 Global step 70 Train loss 12.795126 on epoch=34
03/14/2022 12:01:31 - INFO - __main__ - Step 80 Global step 80 Train loss 12.584316 on epoch=39
03/14/2022 12:01:37 - INFO - __main__ - Step 90 Global step 90 Train loss 11.800443 on epoch=44
03/14/2022 12:01:43 - INFO - __main__ - Step 100 Global step 100 Train loss 10.121588 on epoch=49
03/14/2022 12:01:54 - INFO - __main__ - Global step 100 Train loss 12.271454 Classification-F1 0.0 on epoch=49
03/14/2022 12:02:00 - INFO - __main__ - Step 110 Global step 110 Train loss 9.426647 on epoch=54
03/14/2022 12:02:06 - INFO - __main__ - Step 120 Global step 120 Train loss 5.908662 on epoch=59
03/14/2022 12:02:11 - INFO - __main__ - Step 130 Global step 130 Train loss 5.295270 on epoch=64
03/14/2022 12:02:17 - INFO - __main__ - Step 140 Global step 140 Train loss 2.504963 on epoch=69
03/14/2022 12:02:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.572093 on epoch=74
03/14/2022 12:02:24 - INFO - __main__ - Global step 150 Train loss 4.741527 Classification-F1 0.9375 on epoch=74
03/14/2022 12:02:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.458854 on epoch=79
03/14/2022 12:02:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.175801 on epoch=84
03/14/2022 12:02:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.090706 on epoch=89
03/14/2022 12:02:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.057047 on epoch=94
03/14/2022 12:02:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.038197 on epoch=99
03/14/2022 12:02:56 - INFO - __main__ - Global step 200 Train loss 0.164121 Classification-F1 0.9375 on epoch=99
03/14/2022 12:03:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.030395 on epoch=104
03/14/2022 12:03:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.026871 on epoch=109
03/14/2022 12:03:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.642668 on epoch=114
03/14/2022 12:03:20 - INFO - __main__ - Step 240 Global step 240 Train loss 1.261088 on epoch=119
03/14/2022 12:03:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.895182 on epoch=124
03/14/2022 12:03:27 - INFO - __main__ - Global step 250 Train loss 0.571241 Classification-F1 0.6000000000000001 on epoch=124
03/14/2022 12:03:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.482419 on epoch=129
03/14/2022 12:03:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.209947 on epoch=134
03/14/2022 12:03:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.133534 on epoch=139
03/14/2022 12:03:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.137819 on epoch=144
03/14/2022 12:03:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.091114 on epoch=149
03/14/2022 12:03:58 - INFO - __main__ - Global step 300 Train loss 0.210966 Classification-F1 0.873015873015873 on epoch=149
03/14/2022 12:04:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.108442 on epoch=154
03/14/2022 12:04:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.115441 on epoch=159
03/14/2022 12:04:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.059920 on epoch=164
03/14/2022 12:04:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.072601 on epoch=169
03/14/2022 12:04:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.029541 on epoch=174
03/14/2022 12:04:29 - INFO - __main__ - Global step 350 Train loss 0.077189 Classification-F1 0.8398398398398398 on epoch=174
03/14/2022 12:04:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.026113 on epoch=179
03/14/2022 12:04:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.051927 on epoch=184
03/14/2022 12:04:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.022134 on epoch=189
03/14/2022 12:04:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.025867 on epoch=194
03/14/2022 12:04:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.009961 on epoch=199
03/14/2022 12:05:00 - INFO - __main__ - Global step 400 Train loss 0.027200 Classification-F1 0.873015873015873 on epoch=199
03/14/2022 12:05:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.024108 on epoch=204
03/14/2022 12:05:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.042109 on epoch=209
03/14/2022 12:05:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.027744 on epoch=214
03/14/2022 12:05:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.011467 on epoch=219
03/14/2022 12:05:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.009683 on epoch=224
03/14/2022 12:05:31 - INFO - __main__ - Global step 450 Train loss 0.023022 Classification-F1 0.8423645320197044 on epoch=224
03/14/2022 12:05:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.022041 on epoch=229
03/14/2022 12:05:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.028282 on epoch=234
03/14/2022 12:05:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004260 on epoch=239
03/14/2022 12:05:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.021549 on epoch=244
03/14/2022 12:06:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013992 on epoch=249
03/14/2022 12:06:02 - INFO - __main__ - Global step 500 Train loss 0.018025 Classification-F1 0.8398398398398398 on epoch=249
03/14/2022 12:06:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002464 on epoch=254
03/14/2022 12:06:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.004162 on epoch=259
03/14/2022 12:06:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.004177 on epoch=264
03/14/2022 12:06:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.003591 on epoch=269
03/14/2022 12:06:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002948 on epoch=274
03/14/2022 12:06:33 - INFO - __main__ - Global step 550 Train loss 0.003468 Classification-F1 0.805668016194332 on epoch=274
03/14/2022 12:06:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.029553 on epoch=279
03/14/2022 12:06:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.006874 on epoch=284
03/14/2022 12:06:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.021443 on epoch=289
03/14/2022 12:06:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001936 on epoch=294
03/14/2022 12:07:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001120 on epoch=299
03/14/2022 12:07:04 - INFO - __main__ - Global step 600 Train loss 0.012185 Classification-F1 0.8423645320197044 on epoch=299
03/14/2022 12:07:04 - INFO - __main__ - save last model!
03/14/2022 12:07:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:07:04 - INFO - __main__ - Printing 3 examples
03/14/2022 12:07:04 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/14/2022 12:07:04 - INFO - __main__ - ['negative']
03/14/2022 12:07:04 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/14/2022 12:07:04 - INFO - __main__ - ['negative']
03/14/2022 12:07:04 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/14/2022 12:07:04 - INFO - __main__ - ['negative']
03/14/2022 12:07:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 12:07:04 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:07:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:07:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:07:04 - INFO - __main__ - Printing 3 examples
03/14/2022 12:07:04 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/14/2022 12:07:04 - INFO - __main__ - ['negative']
03/14/2022 12:07:04 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/14/2022 12:07:04 - INFO - __main__ - ['negative']
03/14/2022 12:07:04 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/14/2022 12:07:04 - INFO - __main__ - ['negative']
03/14/2022 12:07:04 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:07:04 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:07:05 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:07:11 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 12:07:11 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 12:07:11 - INFO - __main__ - Printing 3 examples
03/14/2022 12:07:11 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 12:07:11 - INFO - __main__ - ['negative']
03/14/2022 12:07:11 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 12:07:11 - INFO - __main__ - ['negative']
03/14/2022 12:07:11 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 12:07:11 - INFO - __main__ - ['negative']
03/14/2022 12:07:11 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:07:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:07:15 - INFO - __main__ - Starting training!
03/14/2022 12:07:18 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:07:25 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 12:09:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_100_0.0002_8_predictions.txt
03/14/2022 12:09:56 - INFO - __main__ - Classification-F1 on test data: 0.8498
03/14/2022 12:09:56 - INFO - __main__ - prefix=yelp_polarity_16_100, lr=0.0002, bsz=8, dev_performance=0.9375, test_performance=0.8497829848095886
03/14/2022 12:09:56 - INFO - __main__ - Running ... prefix=yelp_polarity_16_100, lr=0.0001, bsz=8 ...
03/14/2022 12:09:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:09:57 - INFO - __main__ - Printing 3 examples
03/14/2022 12:09:57 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/14/2022 12:09:57 - INFO - __main__ - ['negative']
03/14/2022 12:09:57 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/14/2022 12:09:57 - INFO - __main__ - ['negative']
03/14/2022 12:09:57 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/14/2022 12:09:57 - INFO - __main__ - ['negative']
03/14/2022 12:09:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:09:57 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:09:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:09:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:09:57 - INFO - __main__ - Printing 3 examples
03/14/2022 12:09:57 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/14/2022 12:09:57 - INFO - __main__ - ['negative']
03/14/2022 12:09:57 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/14/2022 12:09:57 - INFO - __main__ - ['negative']
03/14/2022 12:09:57 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/14/2022 12:09:57 - INFO - __main__ - ['negative']
03/14/2022 12:09:57 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:09:57 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:09:57 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:10:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:10:08 - INFO - __main__ - Starting training!
03/14/2022 12:10:13 - INFO - __main__ - Step 10 Global step 10 Train loss 23.560270 on epoch=4
03/14/2022 12:10:19 - INFO - __main__ - Step 20 Global step 20 Train loss 19.783438 on epoch=9
03/14/2022 12:10:25 - INFO - __main__ - Step 30 Global step 30 Train loss 18.575932 on epoch=14
03/14/2022 12:10:31 - INFO - __main__ - Step 40 Global step 40 Train loss 16.623589 on epoch=19
03/14/2022 12:10:37 - INFO - __main__ - Step 50 Global step 50 Train loss 16.288242 on epoch=24
03/14/2022 12:10:49 - INFO - __main__ - Global step 50 Train loss 18.966295 Classification-F1 0.0 on epoch=24
03/14/2022 12:10:56 - INFO - __main__ - Step 60 Global step 60 Train loss 16.290216 on epoch=29
03/14/2022 12:11:02 - INFO - __main__ - Step 70 Global step 70 Train loss 15.263115 on epoch=34
03/14/2022 12:11:08 - INFO - __main__ - Step 80 Global step 80 Train loss 14.820120 on epoch=39
03/14/2022 12:11:14 - INFO - __main__ - Step 90 Global step 90 Train loss 14.903806 on epoch=44
03/14/2022 12:11:20 - INFO - __main__ - Step 100 Global step 100 Train loss 14.430057 on epoch=49
03/14/2022 12:11:32 - INFO - __main__ - Global step 100 Train loss 15.141461 Classification-F1 0.0 on epoch=49
03/14/2022 12:11:38 - INFO - __main__ - Step 110 Global step 110 Train loss 13.496310 on epoch=54
03/14/2022 12:11:44 - INFO - __main__ - Step 120 Global step 120 Train loss 13.852350 on epoch=59
03/14/2022 12:11:50 - INFO - __main__ - Step 130 Global step 130 Train loss 12.616032 on epoch=64
03/14/2022 12:11:56 - INFO - __main__ - Step 140 Global step 140 Train loss 12.478420 on epoch=69
03/14/2022 12:12:02 - INFO - __main__ - Step 150 Global step 150 Train loss 12.291012 on epoch=74
03/14/2022 12:12:14 - INFO - __main__ - Global step 150 Train loss 12.946824 Classification-F1 0.0 on epoch=74
03/14/2022 12:12:20 - INFO - __main__ - Step 160 Global step 160 Train loss 11.081976 on epoch=79
03/14/2022 12:12:26 - INFO - __main__ - Step 170 Global step 170 Train loss 11.349352 on epoch=84
03/14/2022 12:12:33 - INFO - __main__ - Step 180 Global step 180 Train loss 11.377690 on epoch=89
03/14/2022 12:12:39 - INFO - __main__ - Step 190 Global step 190 Train loss 10.119073 on epoch=94
03/14/2022 12:12:45 - INFO - __main__ - Step 200 Global step 200 Train loss 8.298039 on epoch=99
03/14/2022 12:12:48 - INFO - __main__ - Global step 200 Train loss 10.445226 Classification-F1 0.03838383838383838 on epoch=99
03/14/2022 12:12:54 - INFO - __main__ - Step 210 Global step 210 Train loss 5.617471 on epoch=104
03/14/2022 12:13:01 - INFO - __main__ - Step 220 Global step 220 Train loss 1.699453 on epoch=109
03/14/2022 12:13:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.542090 on epoch=114
03/14/2022 12:13:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.329769 on epoch=119
03/14/2022 12:13:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.384263 on epoch=124
03/14/2022 12:13:19 - INFO - __main__ - Global step 250 Train loss 1.714610 Classification-F1 0.9372549019607843 on epoch=124
03/14/2022 12:13:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.211281 on epoch=129
03/14/2022 12:13:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.245012 on epoch=134
03/14/2022 12:13:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.182942 on epoch=139
03/14/2022 12:13:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.126855 on epoch=144
03/14/2022 12:13:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.156607 on epoch=149
03/14/2022 12:13:51 - INFO - __main__ - Global step 300 Train loss 0.184539 Classification-F1 1.0 on epoch=149
03/14/2022 12:13:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.099229 on epoch=154
03/14/2022 12:14:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.090187 on epoch=159
03/14/2022 12:14:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.020904 on epoch=164
03/14/2022 12:14:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.022281 on epoch=169
03/14/2022 12:14:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.025196 on epoch=174
03/14/2022 12:14:23 - INFO - __main__ - Global step 350 Train loss 0.051559 Classification-F1 0.9687194525904204 on epoch=174
03/14/2022 12:14:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.159810 on epoch=179
03/14/2022 12:14:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.026505 on epoch=184
03/14/2022 12:14:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.035999 on epoch=189
03/14/2022 12:14:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.046825 on epoch=194
03/14/2022 12:14:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.037797 on epoch=199
03/14/2022 12:14:54 - INFO - __main__ - Global step 400 Train loss 0.061387 Classification-F1 0.6559139784946236 on epoch=199
03/14/2022 12:15:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.009588 on epoch=204
03/14/2022 12:15:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.065752 on epoch=209
03/14/2022 12:15:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.005915 on epoch=214
03/14/2022 12:15:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.012312 on epoch=219
03/14/2022 12:15:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.011535 on epoch=224
03/14/2022 12:15:25 - INFO - __main__ - Global step 450 Train loss 0.021021 Classification-F1 0.9687194525904204 on epoch=224
03/14/2022 12:15:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001134 on epoch=229
03/14/2022 12:15:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.002736 on epoch=234
03/14/2022 12:15:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.002482 on epoch=239
03/14/2022 12:15:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.005638 on epoch=244
03/14/2022 12:15:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.003600 on epoch=249
03/14/2022 12:15:56 - INFO - __main__ - Global step 500 Train loss 0.003118 Classification-F1 0.9687194525904204 on epoch=249
03/14/2022 12:16:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.009992 on epoch=254
03/14/2022 12:16:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.048879 on epoch=259
03/14/2022 12:16:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.105264 on epoch=264
03/14/2022 12:16:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.033484 on epoch=269
03/14/2022 12:16:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.036968 on epoch=274
03/14/2022 12:16:27 - INFO - __main__ - Global step 550 Train loss 0.046918 Classification-F1 1.0 on epoch=274
03/14/2022 12:16:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.007640 on epoch=279
03/14/2022 12:16:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000505 on epoch=284
03/14/2022 12:16:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.017296 on epoch=289
03/14/2022 12:16:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.047724 on epoch=294
03/14/2022 12:16:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000585 on epoch=299
03/14/2022 12:16:59 - INFO - __main__ - Global step 600 Train loss 0.014750 Classification-F1 1.0 on epoch=299
03/14/2022 12:16:59 - INFO - __main__ - save last model!
03/14/2022 12:17:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:17:00 - INFO - __main__ - Printing 3 examples
03/14/2022 12:17:00 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/14/2022 12:17:00 - INFO - __main__ - ['negative']
03/14/2022 12:17:00 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/14/2022 12:17:00 - INFO - __main__ - ['negative']
03/14/2022 12:17:00 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/14/2022 12:17:00 - INFO - __main__ - ['negative']
03/14/2022 12:17:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 12:17:00 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:17:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:17:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:17:00 - INFO - __main__ - Printing 3 examples
03/14/2022 12:17:00 - INFO - __main__ -  [yelp_polarity] good store
03/14/2022 12:17:00 - INFO - __main__ - ['negative']
03/14/2022 12:17:00 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/14/2022 12:17:00 - INFO - __main__ - ['negative']
03/14/2022 12:17:00 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/14/2022 12:17:00 - INFO - __main__ - ['negative']
03/14/2022 12:17:00 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:17:00 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:17:00 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:17:06 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 12:17:07 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 12:17:07 - INFO - __main__ - Printing 3 examples
03/14/2022 12:17:07 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 12:17:07 - INFO - __main__ - ['negative']
03/14/2022 12:17:07 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 12:17:07 - INFO - __main__ - ['negative']
03/14/2022 12:17:07 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 12:17:07 - INFO - __main__ - ['negative']
03/14/2022 12:17:07 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:17:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:17:10 - INFO - __main__ - Starting training!
03/14/2022 12:17:13 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:17:21 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 12:20:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_100_0.0001_8_predictions.txt
03/14/2022 12:20:05 - INFO - __main__ - Classification-F1 on test data: 0.6191
03/14/2022 12:20:05 - INFO - __main__ - prefix=yelp_polarity_16_100, lr=0.0001, bsz=8, dev_performance=1.0, test_performance=0.619147631393219
03/14/2022 12:20:05 - INFO - __main__ - Running ... prefix=yelp_polarity_16_13, lr=0.0005, bsz=8 ...
03/14/2022 12:20:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:20:06 - INFO - __main__ - Printing 3 examples
03/14/2022 12:20:06 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/14/2022 12:20:06 - INFO - __main__ - ['negative']
03/14/2022 12:20:06 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/14/2022 12:20:06 - INFO - __main__ - ['negative']
03/14/2022 12:20:06 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/14/2022 12:20:06 - INFO - __main__ - ['negative']
03/14/2022 12:20:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:20:06 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:20:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:20:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:20:06 - INFO - __main__ - Printing 3 examples
03/14/2022 12:20:06 - INFO - __main__ -  [yelp_polarity] good store
03/14/2022 12:20:06 - INFO - __main__ - ['negative']
03/14/2022 12:20:06 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/14/2022 12:20:06 - INFO - __main__ - ['negative']
03/14/2022 12:20:06 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/14/2022 12:20:06 - INFO - __main__ - ['negative']
03/14/2022 12:20:06 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:20:06 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:20:06 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:20:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:20:19 - INFO - __main__ - Starting training!
03/14/2022 12:20:26 - INFO - __main__ - Step 10 Global step 10 Train loss 22.382744 on epoch=4
03/14/2022 12:20:33 - INFO - __main__ - Step 20 Global step 20 Train loss 16.538494 on epoch=9
03/14/2022 12:20:38 - INFO - __main__ - Step 30 Global step 30 Train loss 14.636528 on epoch=14
03/14/2022 12:20:45 - INFO - __main__ - Step 40 Global step 40 Train loss 12.750735 on epoch=19
03/14/2022 12:20:51 - INFO - __main__ - Step 50 Global step 50 Train loss 9.305380 on epoch=24
03/14/2022 12:20:52 - INFO - __main__ - Global step 50 Train loss 15.122777 Classification-F1 0.4444444444444445 on epoch=24
03/14/2022 12:20:58 - INFO - __main__ - Step 60 Global step 60 Train loss 6.174947 on epoch=29
03/14/2022 12:21:04 - INFO - __main__ - Step 70 Global step 70 Train loss 1.066854 on epoch=34
03/14/2022 12:21:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.263325 on epoch=39
03/14/2022 12:21:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.073649 on epoch=44
03/14/2022 12:21:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.124679 on epoch=49
03/14/2022 12:21:23 - INFO - __main__ - Global step 100 Train loss 1.540691 Classification-F1 0.8435972629521017 on epoch=49
03/14/2022 12:21:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.053786 on epoch=54
03/14/2022 12:21:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.024040 on epoch=59
03/14/2022 12:21:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.004504 on epoch=64
03/14/2022 12:21:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.036797 on epoch=69
03/14/2022 12:21:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.055166 on epoch=74
03/14/2022 12:21:55 - INFO - __main__ - Global step 150 Train loss 0.034859 Classification-F1 0.875 on epoch=74
03/14/2022 12:22:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.005479 on epoch=79
03/14/2022 12:22:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.003312 on epoch=84
03/14/2022 12:22:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.001025 on epoch=89
03/14/2022 12:22:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.048177 on epoch=94
03/14/2022 12:22:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.003020 on epoch=99
03/14/2022 12:22:27 - INFO - __main__ - Global step 200 Train loss 0.012203 Classification-F1 0.8435972629521017 on epoch=99
03/14/2022 12:22:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.008643 on epoch=104
03/14/2022 12:22:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.002821 on epoch=109
03/14/2022 12:22:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001414 on epoch=114
03/14/2022 12:22:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.012336 on epoch=119
03/14/2022 12:22:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000583 on epoch=124
03/14/2022 12:22:58 - INFO - __main__ - Global step 250 Train loss 0.005159 Classification-F1 0.906158357771261 on epoch=124
03/14/2022 12:23:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000209 on epoch=129
03/14/2022 12:23:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.001872 on epoch=134
03/14/2022 12:23:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.034431 on epoch=139
03/14/2022 12:23:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.001012 on epoch=144
03/14/2022 12:23:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.024053 on epoch=149
03/14/2022 12:23:29 - INFO - __main__ - Global step 300 Train loss 0.012315 Classification-F1 0.875 on epoch=149
03/14/2022 12:23:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000448 on epoch=154
03/14/2022 12:23:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000142 on epoch=159
03/14/2022 12:23:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.029003 on epoch=164
03/14/2022 12:23:54 - INFO - __main__ - Step 340 Global step 340 Train loss 1.000219 on epoch=169
03/14/2022 12:24:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.234845 on epoch=174
03/14/2022 12:24:01 - INFO - __main__ - Global step 350 Train loss 0.252932 Classification-F1 0.8423645320197044 on epoch=174
03/14/2022 12:24:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.032690 on epoch=179
03/14/2022 12:24:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.010749 on epoch=184
03/14/2022 12:24:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.010111 on epoch=189
03/14/2022 12:24:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.005042 on epoch=194
03/14/2022 12:24:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.004210 on epoch=199
03/14/2022 12:24:32 - INFO - __main__ - Global step 400 Train loss 0.012560 Classification-F1 0.906158357771261 on epoch=199
03/14/2022 12:24:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.010083 on epoch=204
03/14/2022 12:24:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000849 on epoch=209
03/14/2022 12:24:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001332 on epoch=214
03/14/2022 12:24:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001932 on epoch=219
03/14/2022 12:25:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000419 on epoch=224
03/14/2022 12:25:03 - INFO - __main__ - Global step 450 Train loss 0.002923 Classification-F1 0.8745098039215686 on epoch=224
03/14/2022 12:25:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.205609 on epoch=229
03/14/2022 12:25:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.024593 on epoch=234
03/14/2022 12:25:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.007156 on epoch=239
03/14/2022 12:25:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.078926 on epoch=244
03/14/2022 12:25:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.072305 on epoch=249
03/14/2022 12:25:33 - INFO - __main__ - Global step 500 Train loss 0.077718 Classification-F1 0.5933528836754642 on epoch=249
03/14/2022 12:25:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.011511 on epoch=254
03/14/2022 12:25:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.005843 on epoch=259
03/14/2022 12:25:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.019279 on epoch=264
03/14/2022 12:25:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002464 on epoch=269
03/14/2022 12:26:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000924 on epoch=274
03/14/2022 12:26:04 - INFO - __main__ - Global step 550 Train loss 0.008004 Classification-F1 0.6235294117647059 on epoch=274
03/14/2022 12:26:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001584 on epoch=279
03/14/2022 12:26:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000746 on epoch=284
03/14/2022 12:26:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001464 on epoch=289
03/14/2022 12:26:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000653 on epoch=294
03/14/2022 12:26:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000452 on epoch=299
03/14/2022 12:26:35 - INFO - __main__ - Global step 600 Train loss 0.000980 Classification-F1 0.7184750733137829 on epoch=299
03/14/2022 12:26:35 - INFO - __main__ - save last model!
03/14/2022 12:26:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:26:36 - INFO - __main__ - Printing 3 examples
03/14/2022 12:26:36 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/14/2022 12:26:36 - INFO - __main__ - ['negative']
03/14/2022 12:26:36 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/14/2022 12:26:36 - INFO - __main__ - ['negative']
03/14/2022 12:26:36 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/14/2022 12:26:36 - INFO - __main__ - ['negative']
03/14/2022 12:26:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 12:26:36 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:26:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:26:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:26:36 - INFO - __main__ - Printing 3 examples
03/14/2022 12:26:36 - INFO - __main__ -  [yelp_polarity] good store
03/14/2022 12:26:36 - INFO - __main__ - ['negative']
03/14/2022 12:26:36 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/14/2022 12:26:36 - INFO - __main__ - ['negative']
03/14/2022 12:26:36 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/14/2022 12:26:36 - INFO - __main__ - ['negative']
03/14/2022 12:26:36 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:26:36 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:26:36 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:26:42 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 12:26:43 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 12:26:43 - INFO - __main__ - Printing 3 examples
03/14/2022 12:26:43 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 12:26:43 - INFO - __main__ - ['negative']
03/14/2022 12:26:43 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 12:26:43 - INFO - __main__ - ['negative']
03/14/2022 12:26:43 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 12:26:43 - INFO - __main__ - ['negative']
03/14/2022 12:26:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:26:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:26:47 - INFO - __main__ - Starting training!
03/14/2022 12:26:50 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:26:57 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 12:29:41 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_13_0.0005_8_predictions.txt
03/14/2022 12:29:41 - INFO - __main__ - Classification-F1 on test data: 0.9523
03/14/2022 12:29:41 - INFO - __main__ - prefix=yelp_polarity_16_13, lr=0.0005, bsz=8, dev_performance=0.906158357771261, test_performance=0.9523467691933987
03/14/2022 12:29:41 - INFO - __main__ - Running ... prefix=yelp_polarity_16_13, lr=0.0003, bsz=8 ...
03/14/2022 12:29:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:29:42 - INFO - __main__ - Printing 3 examples
03/14/2022 12:29:42 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/14/2022 12:29:42 - INFO - __main__ - ['negative']
03/14/2022 12:29:42 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/14/2022 12:29:42 - INFO - __main__ - ['negative']
03/14/2022 12:29:42 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/14/2022 12:29:42 - INFO - __main__ - ['negative']
03/14/2022 12:29:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:29:42 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:29:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:29:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:29:42 - INFO - __main__ - Printing 3 examples
03/14/2022 12:29:42 - INFO - __main__ -  [yelp_polarity] good store
03/14/2022 12:29:42 - INFO - __main__ - ['negative']
03/14/2022 12:29:42 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/14/2022 12:29:42 - INFO - __main__ - ['negative']
03/14/2022 12:29:42 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/14/2022 12:29:42 - INFO - __main__ - ['negative']
03/14/2022 12:29:42 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:29:42 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:29:42 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:29:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:29:55 - INFO - __main__ - Starting training!
03/14/2022 12:30:01 - INFO - __main__ - Step 10 Global step 10 Train loss 23.003292 on epoch=4
03/14/2022 12:30:07 - INFO - __main__ - Step 20 Global step 20 Train loss 17.055460 on epoch=9
03/14/2022 12:30:13 - INFO - __main__ - Step 30 Global step 30 Train loss 16.067924 on epoch=14
03/14/2022 12:30:19 - INFO - __main__ - Step 40 Global step 40 Train loss 14.814878 on epoch=19
03/14/2022 12:30:25 - INFO - __main__ - Step 50 Global step 50 Train loss 12.896375 on epoch=24
03/14/2022 12:30:31 - INFO - __main__ - Global step 50 Train loss 16.767586 Classification-F1 0.0 on epoch=24
03/14/2022 12:30:37 - INFO - __main__ - Step 60 Global step 60 Train loss 12.329859 on epoch=29
03/14/2022 12:30:43 - INFO - __main__ - Step 70 Global step 70 Train loss 10.732346 on epoch=34
03/14/2022 12:30:49 - INFO - __main__ - Step 80 Global step 80 Train loss 6.827376 on epoch=39
03/14/2022 12:30:55 - INFO - __main__ - Step 90 Global step 90 Train loss 2.021800 on epoch=44
03/14/2022 12:31:01 - INFO - __main__ - Step 100 Global step 100 Train loss 1.521758 on epoch=49
03/14/2022 12:31:02 - INFO - __main__ - Global step 100 Train loss 6.686627 Classification-F1 0.4385964912280702 on epoch=49
03/14/2022 12:31:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.307936 on epoch=54
03/14/2022 12:31:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.658474 on epoch=59
03/14/2022 12:31:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.102511 on epoch=64
03/14/2022 12:31:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.041991 on epoch=69
03/14/2022 12:31:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.039061 on epoch=74
03/14/2022 12:31:34 - INFO - __main__ - Global step 150 Train loss 0.229994 Classification-F1 0.906158357771261 on epoch=74
03/14/2022 12:31:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.177732 on epoch=79
03/14/2022 12:31:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.053045 on epoch=84
03/14/2022 12:31:52 - INFO - __main__ - Step 180 Global step 180 Train loss 1.225033 on epoch=89
03/14/2022 12:31:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.271661 on epoch=94
03/14/2022 12:32:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.201018 on epoch=99
03/14/2022 12:32:05 - INFO - __main__ - Global step 200 Train loss 0.385698 Classification-F1 0.746031746031746 on epoch=99
03/14/2022 12:32:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.143150 on epoch=104
03/14/2022 12:32:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.015035 on epoch=109
03/14/2022 12:32:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.038709 on epoch=114
03/14/2022 12:32:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.022895 on epoch=119
03/14/2022 12:32:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.011317 on epoch=124
03/14/2022 12:32:36 - INFO - __main__ - Global step 250 Train loss 0.046221 Classification-F1 0.906158357771261 on epoch=124
03/14/2022 12:32:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.007492 on epoch=129
03/14/2022 12:32:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.028516 on epoch=134
03/14/2022 12:32:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.010072 on epoch=139
03/14/2022 12:33:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.062604 on epoch=144
03/14/2022 12:33:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.010515 on epoch=149
03/14/2022 12:33:07 - INFO - __main__ - Global step 300 Train loss 0.023840 Classification-F1 0.875 on epoch=149
03/14/2022 12:33:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.056142 on epoch=154
03/14/2022 12:33:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.070694 on epoch=159
03/14/2022 12:33:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.035310 on epoch=164
03/14/2022 12:33:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.096757 on epoch=169
03/14/2022 12:33:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.113974 on epoch=174
03/14/2022 12:33:38 - INFO - __main__ - Global step 350 Train loss 0.074575 Classification-F1 0.7117117117117117 on epoch=174
03/14/2022 12:33:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.055965 on epoch=179
03/14/2022 12:33:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.100078 on epoch=184
03/14/2022 12:33:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.430361 on epoch=189
03/14/2022 12:34:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.416842 on epoch=194
03/14/2022 12:34:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.337403 on epoch=199
03/14/2022 12:34:09 - INFO - __main__ - Global step 400 Train loss 0.268130 Classification-F1 0.4666666666666667 on epoch=199
03/14/2022 12:34:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.352920 on epoch=204
03/14/2022 12:34:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.336898 on epoch=209
03/14/2022 12:34:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.286562 on epoch=214
03/14/2022 12:34:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.318389 on epoch=219
03/14/2022 12:34:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.329061 on epoch=224
03/14/2022 12:34:41 - INFO - __main__ - Global step 450 Train loss 0.324766 Classification-F1 0.625 on epoch=224
03/14/2022 12:34:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.313347 on epoch=229
03/14/2022 12:34:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.266033 on epoch=234
03/14/2022 12:34:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.331404 on epoch=239
03/14/2022 12:35:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.281807 on epoch=244
03/14/2022 12:35:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.324985 on epoch=249
03/14/2022 12:35:12 - INFO - __main__ - Global step 500 Train loss 0.303515 Classification-F1 0.5733333333333335 on epoch=249
03/14/2022 12:35:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.276905 on epoch=254
03/14/2022 12:35:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.244166 on epoch=259
03/14/2022 12:35:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.270270 on epoch=264
03/14/2022 12:35:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.305467 on epoch=269
03/14/2022 12:35:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.293565 on epoch=274
03/14/2022 12:35:43 - INFO - __main__ - Global step 550 Train loss 0.278075 Classification-F1 0.5733333333333335 on epoch=274
03/14/2022 12:35:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.282097 on epoch=279
03/14/2022 12:35:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.262035 on epoch=284
03/14/2022 12:36:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.269392 on epoch=289
03/14/2022 12:36:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.295022 on epoch=294
03/14/2022 12:36:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.285817 on epoch=299
03/14/2022 12:36:14 - INFO - __main__ - Global step 600 Train loss 0.278873 Classification-F1 0.39756367663344405 on epoch=299
03/14/2022 12:36:14 - INFO - __main__ - save last model!
03/14/2022 12:36:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:36:15 - INFO - __main__ - Printing 3 examples
03/14/2022 12:36:15 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/14/2022 12:36:15 - INFO - __main__ - ['negative']
03/14/2022 12:36:15 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/14/2022 12:36:15 - INFO - __main__ - ['negative']
03/14/2022 12:36:15 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/14/2022 12:36:15 - INFO - __main__ - ['negative']
03/14/2022 12:36:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 12:36:15 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:36:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:36:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:36:15 - INFO - __main__ - Printing 3 examples
03/14/2022 12:36:15 - INFO - __main__ -  [yelp_polarity] good store
03/14/2022 12:36:15 - INFO - __main__ - ['negative']
03/14/2022 12:36:15 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/14/2022 12:36:15 - INFO - __main__ - ['negative']
03/14/2022 12:36:15 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/14/2022 12:36:15 - INFO - __main__ - ['negative']
03/14/2022 12:36:15 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:36:15 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:36:15 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:36:21 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 12:36:21 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 12:36:21 - INFO - __main__ - Printing 3 examples
03/14/2022 12:36:21 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 12:36:21 - INFO - __main__ - ['negative']
03/14/2022 12:36:21 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 12:36:21 - INFO - __main__ - ['negative']
03/14/2022 12:36:21 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 12:36:21 - INFO - __main__ - ['negative']
03/14/2022 12:36:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:36:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:36:28 - INFO - __main__ - Starting training!
03/14/2022 12:36:28 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:36:35 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 12:39:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_13_0.0003_8_predictions.txt
03/14/2022 12:39:20 - INFO - __main__ - Classification-F1 on test data: 0.6420
03/14/2022 12:39:20 - INFO - __main__ - prefix=yelp_polarity_16_13, lr=0.0003, bsz=8, dev_performance=0.906158357771261, test_performance=0.6419698468630979
03/14/2022 12:39:20 - INFO - __main__ - Running ... prefix=yelp_polarity_16_13, lr=0.0002, bsz=8 ...
03/14/2022 12:39:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:39:21 - INFO - __main__ - Printing 3 examples
03/14/2022 12:39:21 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/14/2022 12:39:21 - INFO - __main__ - ['negative']
03/14/2022 12:39:21 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/14/2022 12:39:21 - INFO - __main__ - ['negative']
03/14/2022 12:39:21 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/14/2022 12:39:21 - INFO - __main__ - ['negative']
03/14/2022 12:39:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:39:21 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:39:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:39:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:39:21 - INFO - __main__ - Printing 3 examples
03/14/2022 12:39:21 - INFO - __main__ -  [yelp_polarity] good store
03/14/2022 12:39:21 - INFO - __main__ - ['negative']
03/14/2022 12:39:21 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/14/2022 12:39:21 - INFO - __main__ - ['negative']
03/14/2022 12:39:21 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/14/2022 12:39:21 - INFO - __main__ - ['negative']
03/14/2022 12:39:21 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:39:21 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:39:21 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:39:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:39:32 - INFO - __main__ - Starting training!
03/14/2022 12:39:37 - INFO - __main__ - Step 10 Global step 10 Train loss 23.551332 on epoch=4
03/14/2022 12:39:43 - INFO - __main__ - Step 20 Global step 20 Train loss 18.678638 on epoch=9
03/14/2022 12:39:49 - INFO - __main__ - Step 30 Global step 30 Train loss 16.796072 on epoch=14
03/14/2022 12:39:55 - INFO - __main__ - Step 40 Global step 40 Train loss 15.274791 on epoch=19
03/14/2022 12:40:01 - INFO - __main__ - Step 50 Global step 50 Train loss 14.622679 on epoch=24
03/14/2022 12:40:09 - INFO - __main__ - Global step 50 Train loss 17.784700 Classification-F1 0.0 on epoch=24
03/14/2022 12:40:16 - INFO - __main__ - Step 60 Global step 60 Train loss 14.866760 on epoch=29
03/14/2022 12:40:22 - INFO - __main__ - Step 70 Global step 70 Train loss 13.279032 on epoch=34
03/14/2022 12:40:28 - INFO - __main__ - Step 80 Global step 80 Train loss 12.454967 on epoch=39
03/14/2022 12:40:34 - INFO - __main__ - Step 90 Global step 90 Train loss 12.091654 on epoch=44
03/14/2022 12:40:40 - INFO - __main__ - Step 100 Global step 100 Train loss 10.794875 on epoch=49
03/14/2022 12:40:42 - INFO - __main__ - Global step 100 Train loss 12.697457 Classification-F1 0.04 on epoch=49
03/14/2022 12:40:49 - INFO - __main__ - Step 110 Global step 110 Train loss 9.972924 on epoch=54
03/14/2022 12:40:55 - INFO - __main__ - Step 120 Global step 120 Train loss 5.253943 on epoch=59
03/14/2022 12:41:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.787070 on epoch=64
03/14/2022 12:41:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.967089 on epoch=69
03/14/2022 12:41:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.249817 on epoch=74
03/14/2022 12:41:14 - INFO - __main__ - Global step 150 Train loss 3.446169 Classification-F1 0.8435972629521017 on epoch=74
03/14/2022 12:41:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.073021 on epoch=79
03/14/2022 12:41:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.041467 on epoch=84
03/14/2022 12:41:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.007622 on epoch=89
03/14/2022 12:41:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.034744 on epoch=94
03/14/2022 12:41:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.004608 on epoch=99
03/14/2022 12:41:45 - INFO - __main__ - Global step 200 Train loss 0.032293 Classification-F1 0.8745098039215686 on epoch=99
03/14/2022 12:41:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.004202 on epoch=104
03/14/2022 12:41:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.013301 on epoch=109
03/14/2022 12:42:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.022681 on epoch=114
03/14/2022 12:42:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001157 on epoch=119
03/14/2022 12:42:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.002207 on epoch=124
03/14/2022 12:42:17 - INFO - __main__ - Global step 250 Train loss 0.008710 Classification-F1 0.8745098039215686 on epoch=124
03/14/2022 12:42:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.002354 on epoch=129
03/14/2022 12:42:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.003403 on epoch=134
03/14/2022 12:42:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000576 on epoch=139
03/14/2022 12:42:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000625 on epoch=144
03/14/2022 12:42:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.008367 on epoch=149
03/14/2022 12:42:48 - INFO - __main__ - Global step 300 Train loss 0.003065 Classification-F1 0.8745098039215686 on epoch=149
03/14/2022 12:42:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.001965 on epoch=154
03/14/2022 12:43:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000821 on epoch=159
03/14/2022 12:43:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000815 on epoch=164
03/14/2022 12:43:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001072 on epoch=169
03/14/2022 12:43:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000148 on epoch=174
03/14/2022 12:43:18 - INFO - __main__ - Global step 350 Train loss 0.000964 Classification-F1 0.8745098039215686 on epoch=174
03/14/2022 12:43:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001131 on epoch=179
03/14/2022 12:43:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000156 on epoch=184
03/14/2022 12:43:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000180 on epoch=189
03/14/2022 12:43:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.016059 on epoch=194
03/14/2022 12:43:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000494 on epoch=199
03/14/2022 12:43:49 - INFO - __main__ - Global step 400 Train loss 0.003604 Classification-F1 0.8745098039215686 on epoch=199
03/14/2022 12:43:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000151 on epoch=204
03/14/2022 12:44:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000595 on epoch=209
03/14/2022 12:44:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000461 on epoch=214
03/14/2022 12:44:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000608 on epoch=219
03/14/2022 12:44:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.002073 on epoch=224
03/14/2022 12:44:20 - INFO - __main__ - Global step 450 Train loss 0.000778 Classification-F1 0.8745098039215686 on epoch=224
03/14/2022 12:44:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000225 on epoch=229
03/14/2022 12:44:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000645 on epoch=234
03/14/2022 12:44:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000186 on epoch=239
03/14/2022 12:44:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000130 on epoch=244
03/14/2022 12:44:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.005428 on epoch=249
03/14/2022 12:44:51 - INFO - __main__ - Global step 500 Train loss 0.001323 Classification-F1 0.8095238095238095 on epoch=249
03/14/2022 12:44:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.044987 on epoch=254
03/14/2022 12:45:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001348 on epoch=259
03/14/2022 12:45:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000236 on epoch=264
03/14/2022 12:45:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000673 on epoch=269
03/14/2022 12:45:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000131 on epoch=274
03/14/2022 12:45:21 - INFO - __main__ - Global step 550 Train loss 0.009475 Classification-F1 0.906158357771261 on epoch=274
03/14/2022 12:45:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000342 on epoch=279
03/14/2022 12:45:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000191 on epoch=284
03/14/2022 12:45:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000171 on epoch=289
03/14/2022 12:45:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000149 on epoch=294
03/14/2022 12:45:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000115 on epoch=299
03/14/2022 12:45:53 - INFO - __main__ - Global step 600 Train loss 0.000193 Classification-F1 0.906158357771261 on epoch=299
03/14/2022 12:45:53 - INFO - __main__ - save last model!
03/14/2022 12:45:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:45:54 - INFO - __main__ - Printing 3 examples
03/14/2022 12:45:54 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/14/2022 12:45:54 - INFO - __main__ - ['negative']
03/14/2022 12:45:54 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/14/2022 12:45:54 - INFO - __main__ - ['negative']
03/14/2022 12:45:54 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/14/2022 12:45:54 - INFO - __main__ - ['negative']
03/14/2022 12:45:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 12:45:54 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:45:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:45:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:45:54 - INFO - __main__ - Printing 3 examples
03/14/2022 12:45:54 - INFO - __main__ -  [yelp_polarity] good store
03/14/2022 12:45:54 - INFO - __main__ - ['negative']
03/14/2022 12:45:54 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/14/2022 12:45:54 - INFO - __main__ - ['negative']
03/14/2022 12:45:54 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/14/2022 12:45:54 - INFO - __main__ - ['negative']
03/14/2022 12:45:54 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:45:54 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:45:54 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:46:00 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 12:46:01 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 12:46:01 - INFO - __main__ - Printing 3 examples
03/14/2022 12:46:01 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 12:46:01 - INFO - __main__ - ['negative']
03/14/2022 12:46:01 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 12:46:01 - INFO - __main__ - ['negative']
03/14/2022 12:46:01 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 12:46:01 - INFO - __main__ - ['negative']
03/14/2022 12:46:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:46:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:46:05 - INFO - __main__ - Starting training!
03/14/2022 12:46:07 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:46:15 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 12:48:57 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_13_0.0002_8_predictions.txt
03/14/2022 12:48:57 - INFO - __main__ - Classification-F1 on test data: 0.9622
03/14/2022 12:48:58 - INFO - __main__ - prefix=yelp_polarity_16_13, lr=0.0002, bsz=8, dev_performance=0.906158357771261, test_performance=0.9622367891478518
03/14/2022 12:48:58 - INFO - __main__ - Running ... prefix=yelp_polarity_16_13, lr=0.0001, bsz=8 ...
03/14/2022 12:48:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:48:59 - INFO - __main__ - Printing 3 examples
03/14/2022 12:48:59 - INFO - __main__ -  [yelp_polarity] This time, I only ordered a boba milk tea, and I was completely underwhelmed. The tea itself tasted like my first attempt making my own milk tea when I was in middle school; I had to concentrate extremely hard to discern the faint tea flavor.  However, the boba's texture was definitely on point.
03/14/2022 12:48:59 - INFO - __main__ - ['negative']
03/14/2022 12:48:59 - INFO - __main__ -  [yelp_polarity] High ticket prices, seats don't recline, tiny theater and dirty bathrooms.   That sums this place up. If it hadn't been for a time crunch and a convenient show time i doubt I would have come here and after this won't be coming back. Charging $10 a ticket for a screen the same size as I have seen in houses with crapier surround sound to boot.
03/14/2022 12:48:59 - INFO - __main__ - ['negative']
03/14/2022 12:48:59 - INFO - __main__ -  [yelp_polarity] Way too expensive for shooting guns.  They do you have some pretty cool assault rifles that I haven't seen elsewhere, but $40 for 50 rounds of a glock 9mm is overpriced.    For $50 you get 2 magazines of an assault rifle.  My friend shot an M4 w/ a red dot sight, just like in Modern Warfare. It took about 1 min for him to finish both rounds.
03/14/2022 12:48:59 - INFO - __main__ - ['negative']
03/14/2022 12:48:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:48:59 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:48:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:48:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:48:59 - INFO - __main__ - Printing 3 examples
03/14/2022 12:48:59 - INFO - __main__ -  [yelp_polarity] good store
03/14/2022 12:48:59 - INFO - __main__ - ['negative']
03/14/2022 12:48:59 - INFO - __main__ -  [yelp_polarity] now i've discovered why i've never given this place a bother.  as previous reviews have attested-- service is non-existent; instead, its the peripheral glance of the same servers rushing past your table as if they were real fucking busy. really? we've been waiting for half an hour or so already. deciding that my ass had become numb from waiting, we managed to flag down a hastily walking server and finally gave our order.   so, pomegranate margarita is OK. seafood kibis is frozen product, thawed, fried, not very seafood. lamb kibis is the less appetizing $12.95 unfolded version of much tastier gyro that can be found elsewhere for half the price. i suppose its good we were saved the task of having to answer to \""how's everything tasting?\""...because, quite precisely, everything is not tasting [good] at all.   fez is one of those places that you go to, wait a long time (in hopes that the food redeems itself), and receive a server who places more importance on their strut and getting your friends number. ultimately you are left with underwhelming food, money and time, wasted.
03/14/2022 12:48:59 - INFO - __main__ - ['negative']
03/14/2022 12:48:59 - INFO - __main__ -  [yelp_polarity] I didn't like it. Yes it was cheap but the massage style was too rough for me. I prefer the Swedish from massage envy. Save your money and treat yourself right.
03/14/2022 12:48:59 - INFO - __main__ - ['negative']
03/14/2022 12:48:59 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:48:59 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:48:59 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:49:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:49:11 - INFO - __main__ - Starting training!
03/14/2022 12:49:17 - INFO - __main__ - Step 10 Global step 10 Train loss 23.254255 on epoch=4
03/14/2022 12:49:23 - INFO - __main__ - Step 20 Global step 20 Train loss 19.929539 on epoch=9
03/14/2022 12:49:29 - INFO - __main__ - Step 30 Global step 30 Train loss 18.473780 on epoch=14
03/14/2022 12:49:35 - INFO - __main__ - Step 40 Global step 40 Train loss 17.295391 on epoch=19
03/14/2022 12:49:41 - INFO - __main__ - Step 50 Global step 50 Train loss 16.851187 on epoch=24
03/14/2022 12:49:54 - INFO - __main__ - Global step 50 Train loss 19.160830 Classification-F1 0.0 on epoch=24
03/14/2022 12:50:00 - INFO - __main__ - Step 60 Global step 60 Train loss 15.960180 on epoch=29
03/14/2022 12:50:06 - INFO - __main__ - Step 70 Global step 70 Train loss 15.738312 on epoch=34
03/14/2022 12:50:13 - INFO - __main__ - Step 80 Global step 80 Train loss 15.407425 on epoch=39
03/14/2022 12:50:19 - INFO - __main__ - Step 90 Global step 90 Train loss 14.857478 on epoch=44
03/14/2022 12:50:25 - INFO - __main__ - Step 100 Global step 100 Train loss 14.150952 on epoch=49
03/14/2022 12:50:37 - INFO - __main__ - Global step 100 Train loss 15.222870 Classification-F1 0.0 on epoch=49
03/14/2022 12:50:43 - INFO - __main__ - Step 110 Global step 110 Train loss 13.657030 on epoch=54
03/14/2022 12:50:49 - INFO - __main__ - Step 120 Global step 120 Train loss 13.809735 on epoch=59
03/14/2022 12:50:55 - INFO - __main__ - Step 130 Global step 130 Train loss 13.430605 on epoch=64
03/14/2022 12:51:01 - INFO - __main__ - Step 140 Global step 140 Train loss 13.300647 on epoch=69
03/14/2022 12:51:08 - INFO - __main__ - Step 150 Global step 150 Train loss 12.681955 on epoch=74
03/14/2022 12:51:16 - INFO - __main__ - Global step 150 Train loss 13.375995 Classification-F1 0.0 on epoch=74
03/14/2022 12:51:22 - INFO - __main__ - Step 160 Global step 160 Train loss 12.211585 on epoch=79
03/14/2022 12:51:28 - INFO - __main__ - Step 170 Global step 170 Train loss 11.486865 on epoch=84
03/14/2022 12:51:34 - INFO - __main__ - Step 180 Global step 180 Train loss 11.224695 on epoch=89
03/14/2022 12:51:40 - INFO - __main__ - Step 190 Global step 190 Train loss 10.584871 on epoch=94
03/14/2022 12:51:47 - INFO - __main__ - Step 200 Global step 200 Train loss 10.033457 on epoch=99
03/14/2022 12:51:53 - INFO - __main__ - Global step 200 Train loss 11.108294 Classification-F1 0.016993464052287584 on epoch=99
03/14/2022 12:51:59 - INFO - __main__ - Step 210 Global step 210 Train loss 9.455308 on epoch=104
03/14/2022 12:52:06 - INFO - __main__ - Step 220 Global step 220 Train loss 6.511902 on epoch=109
03/14/2022 12:52:12 - INFO - __main__ - Step 230 Global step 230 Train loss 4.429627 on epoch=114
03/14/2022 12:52:18 - INFO - __main__ - Step 240 Global step 240 Train loss 1.357264 on epoch=119
03/14/2022 12:52:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.835521 on epoch=124
03/14/2022 12:52:25 - INFO - __main__ - Global step 250 Train loss 4.517924 Classification-F1 0.5717171717171717 on epoch=124
03/14/2022 12:52:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.311919 on epoch=129
03/14/2022 12:52:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.058982 on epoch=134
03/14/2022 12:52:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.044676 on epoch=139
03/14/2022 12:52:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.041231 on epoch=144
03/14/2022 12:52:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.245558 on epoch=149
03/14/2022 12:52:57 - INFO - __main__ - Global step 300 Train loss 0.140473 Classification-F1 0.906158357771261 on epoch=149
03/14/2022 12:53:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.048225 on epoch=154
03/14/2022 12:53:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.121016 on epoch=159
03/14/2022 12:53:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.135319 on epoch=164
03/14/2022 12:53:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.019736 on epoch=169
03/14/2022 12:53:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.010061 on epoch=174
03/14/2022 12:53:29 - INFO - __main__ - Global step 350 Train loss 0.066871 Classification-F1 0.8745098039215686 on epoch=174
03/14/2022 12:53:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.016854 on epoch=179
03/14/2022 12:53:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.008774 on epoch=184
03/14/2022 12:53:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.005926 on epoch=189
03/14/2022 12:53:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.004854 on epoch=194
03/14/2022 12:54:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.003798 on epoch=199
03/14/2022 12:54:01 - INFO - __main__ - Global step 400 Train loss 0.008041 Classification-F1 0.906158357771261 on epoch=199
03/14/2022 12:54:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.016000 on epoch=204
03/14/2022 12:54:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.078779 on epoch=209
03/14/2022 12:54:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000786 on epoch=214
03/14/2022 12:54:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.009436 on epoch=219
03/14/2022 12:54:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.025596 on epoch=224
03/14/2022 12:54:33 - INFO - __main__ - Global step 450 Train loss 0.026119 Classification-F1 0.9687194525904204 on epoch=224
03/14/2022 12:54:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.172121 on epoch=229
03/14/2022 12:54:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000375 on epoch=234
03/14/2022 12:54:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004835 on epoch=239
03/14/2022 12:54:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.095745 on epoch=244
03/14/2022 12:55:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001964 on epoch=249
03/14/2022 12:55:05 - INFO - __main__ - Global step 500 Train loss 0.055008 Classification-F1 0.9375 on epoch=249
03/14/2022 12:55:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001633 on epoch=254
03/14/2022 12:55:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000539 on epoch=259
03/14/2022 12:55:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000257 on epoch=264
03/14/2022 12:55:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000300 on epoch=269
03/14/2022 12:55:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000266 on epoch=274
03/14/2022 12:55:36 - INFO - __main__ - Global step 550 Train loss 0.000599 Classification-F1 0.9687194525904204 on epoch=274
03/14/2022 12:55:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000918 on epoch=279
03/14/2022 12:55:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000252 on epoch=284
03/14/2022 12:55:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.009660 on epoch=289
03/14/2022 12:56:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000158 on epoch=294
03/14/2022 12:56:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000296 on epoch=299
03/14/2022 12:56:08 - INFO - __main__ - Global step 600 Train loss 0.002257 Classification-F1 0.9687194525904204 on epoch=299
03/14/2022 12:56:08 - INFO - __main__ - save last model!
03/14/2022 12:56:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:56:08 - INFO - __main__ - Printing 3 examples
03/14/2022 12:56:08 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/14/2022 12:56:08 - INFO - __main__ - ['positive']
03/14/2022 12:56:08 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/14/2022 12:56:08 - INFO - __main__ - ['positive']
03/14/2022 12:56:08 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/14/2022 12:56:08 - INFO - __main__ - ['positive']
03/14/2022 12:56:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 12:56:09 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:56:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:56:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:56:09 - INFO - __main__ - Printing 3 examples
03/14/2022 12:56:09 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/14/2022 12:56:09 - INFO - __main__ - ['positive']
03/14/2022 12:56:09 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/14/2022 12:56:09 - INFO - __main__ - ['positive']
03/14/2022 12:56:09 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/14/2022 12:56:09 - INFO - __main__ - ['positive']
03/14/2022 12:56:09 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:56:09 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:56:09 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:56:14 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 12:56:15 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 12:56:15 - INFO - __main__ - Printing 3 examples
03/14/2022 12:56:15 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 12:56:15 - INFO - __main__ - ['negative']
03/14/2022 12:56:15 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 12:56:15 - INFO - __main__ - ['negative']
03/14/2022 12:56:15 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 12:56:15 - INFO - __main__ - ['negative']
03/14/2022 12:56:15 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:56:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:56:21 - INFO - __main__ - Starting training!
03/14/2022 12:56:22 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:56:29 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 12:59:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_13_0.0001_8_predictions.txt
03/14/2022 12:59:11 - INFO - __main__ - Classification-F1 on test data: 0.9651
03/14/2022 12:59:11 - INFO - __main__ - prefix=yelp_polarity_16_13, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9651227382619615
03/14/2022 12:59:11 - INFO - __main__ - Running ... prefix=yelp_polarity_16_21, lr=0.0005, bsz=8 ...
03/14/2022 12:59:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:59:12 - INFO - __main__ - Printing 3 examples
03/14/2022 12:59:12 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/14/2022 12:59:12 - INFO - __main__ - ['positive']
03/14/2022 12:59:12 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/14/2022 12:59:12 - INFO - __main__ - ['positive']
03/14/2022 12:59:12 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/14/2022 12:59:12 - INFO - __main__ - ['positive']
03/14/2022 12:59:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 12:59:12 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:59:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 12:59:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 12:59:12 - INFO - __main__ - Printing 3 examples
03/14/2022 12:59:12 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/14/2022 12:59:12 - INFO - __main__ - ['positive']
03/14/2022 12:59:12 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/14/2022 12:59:12 - INFO - __main__ - ['positive']
03/14/2022 12:59:12 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/14/2022 12:59:12 - INFO - __main__ - ['positive']
03/14/2022 12:59:12 - INFO - __main__ - Tokenizing Input ...
03/14/2022 12:59:12 - INFO - __main__ - Tokenizing Output ...
03/14/2022 12:59:12 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 12:59:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 12:59:25 - INFO - __main__ - Starting training!
03/14/2022 12:59:31 - INFO - __main__ - Step 10 Global step 10 Train loss 24.371519 on epoch=4
03/14/2022 12:59:37 - INFO - __main__ - Step 20 Global step 20 Train loss 17.296124 on epoch=9
03/14/2022 12:59:43 - INFO - __main__ - Step 30 Global step 30 Train loss 15.277510 on epoch=14
03/14/2022 12:59:49 - INFO - __main__ - Step 40 Global step 40 Train loss 14.108066 on epoch=19
03/14/2022 12:59:56 - INFO - __main__ - Step 50 Global step 50 Train loss 11.527250 on epoch=24
03/14/2022 12:59:57 - INFO - __main__ - Global step 50 Train loss 16.516092 Classification-F1 0.0 on epoch=24
03/14/2022 13:00:04 - INFO - __main__ - Step 60 Global step 60 Train loss 8.205577 on epoch=29
03/14/2022 13:00:11 - INFO - __main__ - Step 70 Global step 70 Train loss 1.541016 on epoch=34
03/14/2022 13:00:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.633138 on epoch=39
03/14/2022 13:00:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.314885 on epoch=44
03/14/2022 13:00:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.427242 on epoch=49
03/14/2022 13:00:30 - INFO - __main__ - Global step 100 Train loss 2.224371 Classification-F1 0.3992490613266583 on epoch=49
03/14/2022 13:00:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.354840 on epoch=54
03/14/2022 13:00:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.179669 on epoch=59
03/14/2022 13:00:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.187012 on epoch=64
03/14/2022 13:00:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.059341 on epoch=69
03/14/2022 13:01:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.401351 on epoch=74
03/14/2022 13:01:03 - INFO - __main__ - Global step 150 Train loss 0.236443 Classification-F1 0.6101882613510521 on epoch=74
03/14/2022 13:01:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.664137 on epoch=79
03/14/2022 13:01:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.493109 on epoch=84
03/14/2022 13:01:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.411941 on epoch=89
03/14/2022 13:01:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.381129 on epoch=94
03/14/2022 13:01:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.378953 on epoch=99
03/14/2022 13:01:37 - INFO - __main__ - Global step 200 Train loss 0.465854 Classification-F1 0.7490196078431373 on epoch=99
03/14/2022 13:01:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.432096 on epoch=104
03/14/2022 13:01:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.364073 on epoch=109
03/14/2022 13:01:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.325204 on epoch=114
03/14/2022 13:02:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.337307 on epoch=119
03/14/2022 13:02:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.338802 on epoch=124
03/14/2022 13:02:10 - INFO - __main__ - Global step 250 Train loss 0.359496 Classification-F1 0.36374269005847953 on epoch=124
03/14/2022 13:02:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.313740 on epoch=129
03/14/2022 13:02:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.325456 on epoch=134
03/14/2022 13:02:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.294900 on epoch=139
03/14/2022 13:02:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.280768 on epoch=144
03/14/2022 13:02:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.285009 on epoch=149
03/14/2022 13:02:42 - INFO - __main__ - Global step 300 Train loss 0.299974 Classification-F1 0.6101882613510521 on epoch=149
03/14/2022 13:02:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.308810 on epoch=154
03/14/2022 13:02:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.278281 on epoch=159
03/14/2022 13:03:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.329841 on epoch=164
03/14/2022 13:03:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.295394 on epoch=169
03/14/2022 13:03:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.654743 on epoch=174
03/14/2022 13:03:15 - INFO - __main__ - Global step 350 Train loss 0.373414 Classification-F1 0.6267232237539766 on epoch=174
03/14/2022 13:03:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.334807 on epoch=179
03/14/2022 13:03:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.255704 on epoch=184
03/14/2022 13:03:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.314526 on epoch=189
03/14/2022 13:03:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.205354 on epoch=194
03/14/2022 13:03:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.262121 on epoch=199
03/14/2022 13:03:47 - INFO - __main__ - Global step 400 Train loss 0.274502 Classification-F1 0.5588547189819725 on epoch=199
03/14/2022 13:03:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.263637 on epoch=204
03/14/2022 13:04:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.184777 on epoch=209
03/14/2022 13:04:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.148170 on epoch=214
03/14/2022 13:04:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.269794 on epoch=219
03/14/2022 13:04:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.152107 on epoch=224
03/14/2022 13:04:20 - INFO - __main__ - Global step 450 Train loss 0.203697 Classification-F1 0.6825396825396826 on epoch=224
03/14/2022 13:04:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.058914 on epoch=229
03/14/2022 13:04:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.118121 on epoch=234
03/14/2022 13:04:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.036586 on epoch=239
03/14/2022 13:04:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.063515 on epoch=244
03/14/2022 13:04:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.101461 on epoch=249
03/14/2022 13:04:53 - INFO - __main__ - Global step 500 Train loss 0.075719 Classification-F1 0.5465587044534412 on epoch=249
03/14/2022 13:04:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.027339 on epoch=254
03/14/2022 13:05:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.007637 on epoch=259
03/14/2022 13:05:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.006960 on epoch=264
03/14/2022 13:05:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.021724 on epoch=269
03/14/2022 13:05:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.008599 on epoch=274
03/14/2022 13:05:25 - INFO - __main__ - Global step 550 Train loss 0.014452 Classification-F1 0.5733333333333335 on epoch=274
03/14/2022 13:05:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.031514 on epoch=279
03/14/2022 13:05:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.142808 on epoch=284
03/14/2022 13:05:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.092332 on epoch=289
03/14/2022 13:05:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.029609 on epoch=294
03/14/2022 13:05:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.008488 on epoch=299
03/14/2022 13:05:58 - INFO - __main__ - Global step 600 Train loss 0.060950 Classification-F1 0.7793103448275862 on epoch=299
03/14/2022 13:05:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:05:58 - INFO - __main__ - Printing 3 examples
03/14/2022 13:05:58 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/14/2022 13:05:58 - INFO - __main__ - ['positive']
03/14/2022 13:05:58 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/14/2022 13:05:58 - INFO - __main__ - ['positive']
03/14/2022 13:05:58 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/14/2022 13:05:58 - INFO - __main__ - ['positive']
03/14/2022 13:05:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 13:05:58 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:05:58 - INFO - __main__ - save last model!
03/14/2022 13:05:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 13:05:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:05:59 - INFO - __main__ - Printing 3 examples
03/14/2022 13:05:59 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/14/2022 13:05:59 - INFO - __main__ - ['positive']
03/14/2022 13:05:59 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/14/2022 13:05:59 - INFO - __main__ - ['positive']
03/14/2022 13:05:59 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/14/2022 13:05:59 - INFO - __main__ - ['positive']
03/14/2022 13:05:59 - INFO - __main__ - Tokenizing Input ...
03/14/2022 13:05:59 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:05:59 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 13:06:05 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 13:06:06 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 13:06:06 - INFO - __main__ - Printing 3 examples
03/14/2022 13:06:06 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 13:06:06 - INFO - __main__ - ['negative']
03/14/2022 13:06:06 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 13:06:06 - INFO - __main__ - ['negative']
03/14/2022 13:06:06 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 13:06:06 - INFO - __main__ - ['negative']
03/14/2022 13:06:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 13:06:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 13:06:10 - INFO - __main__ - Starting training!
03/14/2022 13:06:13 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:06:20 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 13:09:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_21_0.0005_8_predictions.txt
03/14/2022 13:09:05 - INFO - __main__ - Classification-F1 on test data: 0.4841
03/14/2022 13:09:05 - INFO - __main__ - prefix=yelp_polarity_16_21, lr=0.0005, bsz=8, dev_performance=0.7793103448275862, test_performance=0.4841488981683786
03/14/2022 13:09:05 - INFO - __main__ - Running ... prefix=yelp_polarity_16_21, lr=0.0003, bsz=8 ...
03/14/2022 13:09:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:09:06 - INFO - __main__ - Printing 3 examples
03/14/2022 13:09:06 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/14/2022 13:09:06 - INFO - __main__ - ['positive']
03/14/2022 13:09:06 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/14/2022 13:09:06 - INFO - __main__ - ['positive']
03/14/2022 13:09:06 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/14/2022 13:09:06 - INFO - __main__ - ['positive']
03/14/2022 13:09:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 13:09:06 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:09:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 13:09:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:09:06 - INFO - __main__ - Printing 3 examples
03/14/2022 13:09:06 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/14/2022 13:09:06 - INFO - __main__ - ['positive']
03/14/2022 13:09:06 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/14/2022 13:09:06 - INFO - __main__ - ['positive']
03/14/2022 13:09:06 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/14/2022 13:09:06 - INFO - __main__ - ['positive']
03/14/2022 13:09:06 - INFO - __main__ - Tokenizing Input ...
03/14/2022 13:09:06 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:09:06 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 13:09:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 13:09:19 - INFO - __main__ - Starting training!
03/14/2022 13:09:26 - INFO - __main__ - Step 10 Global step 10 Train loss 22.474869 on epoch=4
03/14/2022 13:09:33 - INFO - __main__ - Step 20 Global step 20 Train loss 18.658798 on epoch=9
03/14/2022 13:09:39 - INFO - __main__ - Step 30 Global step 30 Train loss 16.231548 on epoch=14
03/14/2022 13:09:46 - INFO - __main__ - Step 40 Global step 40 Train loss 15.064084 on epoch=19
03/14/2022 13:09:52 - INFO - __main__ - Step 50 Global step 50 Train loss 13.640371 on epoch=24
03/14/2022 13:10:00 - INFO - __main__ - Global step 50 Train loss 17.213934 Classification-F1 0.0 on epoch=24
03/14/2022 13:10:07 - INFO - __main__ - Step 60 Global step 60 Train loss 12.603197 on epoch=29
03/14/2022 13:10:14 - INFO - __main__ - Step 70 Global step 70 Train loss 10.809740 on epoch=34
03/14/2022 13:10:20 - INFO - __main__ - Step 80 Global step 80 Train loss 1.696496 on epoch=39
03/14/2022 13:10:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.522555 on epoch=44
03/14/2022 13:10:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.197831 on epoch=49
03/14/2022 13:10:33 - INFO - __main__ - Global step 100 Train loss 5.165964 Classification-F1 0.8745098039215686 on epoch=49
03/14/2022 13:10:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.126959 on epoch=54
03/14/2022 13:10:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.043081 on epoch=59
03/14/2022 13:10:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.011491 on epoch=64
03/14/2022 13:10:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.007960 on epoch=69
03/14/2022 13:11:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.002865 on epoch=74
03/14/2022 13:11:07 - INFO - __main__ - Global step 150 Train loss 0.038471 Classification-F1 0.9372549019607843 on epoch=74
03/14/2022 13:11:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.003832 on epoch=79
03/14/2022 13:11:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.003042 on epoch=84
03/14/2022 13:11:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.000955 on epoch=89
03/14/2022 13:11:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.001115 on epoch=94
03/14/2022 13:11:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.000563 on epoch=99
03/14/2022 13:11:40 - INFO - __main__ - Global step 200 Train loss 0.001902 Classification-F1 0.9372549019607843 on epoch=99
03/14/2022 13:11:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.000438 on epoch=104
03/14/2022 13:11:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.064431 on epoch=109
03/14/2022 13:11:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.185217 on epoch=114
03/14/2022 13:12:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000922 on epoch=119
03/14/2022 13:12:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001507 on epoch=124
03/14/2022 13:12:13 - INFO - __main__ - Global step 250 Train loss 0.050503 Classification-F1 0.9372549019607843 on epoch=124
03/14/2022 13:12:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000497 on epoch=129
03/14/2022 13:12:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000487 on epoch=134
03/14/2022 13:12:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.291807 on epoch=139
03/14/2022 13:12:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000217 on epoch=144
03/14/2022 13:12:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000217 on epoch=149
03/14/2022 13:12:45 - INFO - __main__ - Global step 300 Train loss 0.058645 Classification-F1 0.9372549019607843 on epoch=149
03/14/2022 13:12:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000163 on epoch=154
03/14/2022 13:12:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000472 on epoch=159
03/14/2022 13:13:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000142 on epoch=164
03/14/2022 13:13:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000244 on epoch=169
03/14/2022 13:13:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000164 on epoch=174
03/14/2022 13:13:18 - INFO - __main__ - Global step 350 Train loss 0.000237 Classification-F1 0.9372549019607843 on epoch=174
03/14/2022 13:13:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000114 on epoch=179
03/14/2022 13:13:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000074 on epoch=184
03/14/2022 13:13:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000243 on epoch=189
03/14/2022 13:13:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000347 on epoch=194
03/14/2022 13:13:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000068 on epoch=199
03/14/2022 13:13:51 - INFO - __main__ - Global step 400 Train loss 0.000169 Classification-F1 0.9372549019607843 on epoch=199
03/14/2022 13:13:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000041 on epoch=204
03/14/2022 13:14:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.388907 on epoch=209
03/14/2022 13:14:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.629377 on epoch=214
03/14/2022 13:14:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.214791 on epoch=219
03/14/2022 13:14:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.185003 on epoch=224
03/14/2022 13:14:24 - INFO - __main__ - Global step 450 Train loss 0.283624 Classification-F1 0.9372549019607843 on epoch=224
03/14/2022 13:14:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.201840 on epoch=229
03/14/2022 13:14:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000029 on epoch=234
03/14/2022 13:14:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000221 on epoch=239
03/14/2022 13:14:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.006660 on epoch=244
03/14/2022 13:14:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000073 on epoch=249
03/14/2022 13:14:57 - INFO - __main__ - Global step 500 Train loss 0.041765 Classification-F1 0.9372549019607843 on epoch=249
03/14/2022 13:15:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.181901 on epoch=254
03/14/2022 13:15:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000025 on epoch=259
03/14/2022 13:15:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000013 on epoch=264
03/14/2022 13:15:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000024 on epoch=269
03/14/2022 13:15:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004477 on epoch=274
03/14/2022 13:15:30 - INFO - __main__ - Global step 550 Train loss 0.037288 Classification-F1 0.9372549019607843 on epoch=274
03/14/2022 13:15:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.105165 on epoch=279
03/14/2022 13:15:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004607 on epoch=284
03/14/2022 13:15:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.176096 on epoch=289
03/14/2022 13:15:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000056 on epoch=294
03/14/2022 13:16:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000038 on epoch=299
03/14/2022 13:16:02 - INFO - __main__ - Global step 600 Train loss 0.057192 Classification-F1 0.9372549019607843 on epoch=299
03/14/2022 13:16:02 - INFO - __main__ - save last model!
03/14/2022 13:16:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:16:03 - INFO - __main__ - Printing 3 examples
03/14/2022 13:16:03 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/14/2022 13:16:03 - INFO - __main__ - ['positive']
03/14/2022 13:16:03 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/14/2022 13:16:03 - INFO - __main__ - ['positive']
03/14/2022 13:16:03 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/14/2022 13:16:03 - INFO - __main__ - ['positive']
03/14/2022 13:16:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 13:16:03 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:16:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 13:16:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:16:03 - INFO - __main__ - Printing 3 examples
03/14/2022 13:16:03 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/14/2022 13:16:03 - INFO - __main__ - ['positive']
03/14/2022 13:16:03 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/14/2022 13:16:03 - INFO - __main__ - ['positive']
03/14/2022 13:16:03 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/14/2022 13:16:03 - INFO - __main__ - ['positive']
03/14/2022 13:16:03 - INFO - __main__ - Tokenizing Input ...
03/14/2022 13:16:03 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:16:03 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 13:16:09 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 13:16:10 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 13:16:10 - INFO - __main__ - Printing 3 examples
03/14/2022 13:16:10 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 13:16:10 - INFO - __main__ - ['negative']
03/14/2022 13:16:10 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 13:16:10 - INFO - __main__ - ['negative']
03/14/2022 13:16:10 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 13:16:10 - INFO - __main__ - ['negative']
03/14/2022 13:16:10 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 13:16:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 13:16:14 - INFO - __main__ - Starting training!
03/14/2022 13:16:16 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:16:24 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 13:19:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_21_0.0003_8_predictions.txt
03/14/2022 13:19:07 - INFO - __main__ - Classification-F1 on test data: 0.9564
03/14/2022 13:19:07 - INFO - __main__ - prefix=yelp_polarity_16_21, lr=0.0003, bsz=8, dev_performance=0.9372549019607843, test_performance=0.9564455579256333
03/14/2022 13:19:07 - INFO - __main__ - Running ... prefix=yelp_polarity_16_21, lr=0.0002, bsz=8 ...
03/14/2022 13:19:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:19:08 - INFO - __main__ - Printing 3 examples
03/14/2022 13:19:08 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/14/2022 13:19:08 - INFO - __main__ - ['positive']
03/14/2022 13:19:08 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/14/2022 13:19:08 - INFO - __main__ - ['positive']
03/14/2022 13:19:08 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/14/2022 13:19:08 - INFO - __main__ - ['positive']
03/14/2022 13:19:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 13:19:08 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:19:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 13:19:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:19:08 - INFO - __main__ - Printing 3 examples
03/14/2022 13:19:08 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/14/2022 13:19:08 - INFO - __main__ - ['positive']
03/14/2022 13:19:08 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/14/2022 13:19:08 - INFO - __main__ - ['positive']
03/14/2022 13:19:08 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/14/2022 13:19:08 - INFO - __main__ - ['positive']
03/14/2022 13:19:08 - INFO - __main__ - Tokenizing Input ...
03/14/2022 13:19:08 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:19:08 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 13:19:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 13:19:19 - INFO - __main__ - Starting training!
03/14/2022 13:19:26 - INFO - __main__ - Step 10 Global step 10 Train loss 23.065794 on epoch=4
03/14/2022 13:19:32 - INFO - __main__ - Step 20 Global step 20 Train loss 19.218761 on epoch=9
03/14/2022 13:19:38 - INFO - __main__ - Step 30 Global step 30 Train loss 17.221134 on epoch=14
03/14/2022 13:19:45 - INFO - __main__ - Step 40 Global step 40 Train loss 16.066298 on epoch=19
03/14/2022 13:19:51 - INFO - __main__ - Step 50 Global step 50 Train loss 15.178558 on epoch=24
03/14/2022 13:20:03 - INFO - __main__ - Global step 50 Train loss 18.150110 Classification-F1 0.0 on epoch=24
03/14/2022 13:20:10 - INFO - __main__ - Step 60 Global step 60 Train loss 14.408873 on epoch=29
03/14/2022 13:20:16 - INFO - __main__ - Step 70 Global step 70 Train loss 13.802834 on epoch=34
03/14/2022 13:20:23 - INFO - __main__ - Step 80 Global step 80 Train loss 13.176405 on epoch=39
03/14/2022 13:20:29 - INFO - __main__ - Step 90 Global step 90 Train loss 13.298639 on epoch=44
03/14/2022 13:20:35 - INFO - __main__ - Step 100 Global step 100 Train loss 10.923406 on epoch=49
03/14/2022 13:20:49 - INFO - __main__ - Global step 100 Train loss 13.122031 Classification-F1 0.004273504273504273 on epoch=49
03/14/2022 13:20:56 - INFO - __main__ - Step 110 Global step 110 Train loss 3.816837 on epoch=54
03/14/2022 13:21:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.987559 on epoch=59
03/14/2022 13:21:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.425747 on epoch=64
03/14/2022 13:21:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.502011 on epoch=69
03/14/2022 13:21:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.409381 on epoch=74
03/14/2022 13:21:22 - INFO - __main__ - Global step 150 Train loss 1.228307 Classification-F1 0.3816425120772947 on epoch=74
03/14/2022 13:21:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.428491 on epoch=79
03/14/2022 13:21:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.386345 on epoch=84
03/14/2022 13:21:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.399416 on epoch=89
03/14/2022 13:21:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.424160 on epoch=94
03/14/2022 13:21:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.365107 on epoch=99
03/14/2022 13:22:01 - INFO - __main__ - Global step 200 Train loss 0.400704 Classification-F1 0.05755693581780538 on epoch=99
03/14/2022 13:22:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.343035 on epoch=104
03/14/2022 13:22:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.357678 on epoch=109
03/14/2022 13:22:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.390222 on epoch=114
03/14/2022 13:22:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.397128 on epoch=119
03/14/2022 13:22:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.371456 on epoch=124
03/14/2022 13:22:35 - INFO - __main__ - Global step 250 Train loss 0.371904 Classification-F1 0.19516483516483513 on epoch=124
03/14/2022 13:22:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.359969 on epoch=129
03/14/2022 13:22:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.342906 on epoch=134
03/14/2022 13:22:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.677620 on epoch=139
03/14/2022 13:23:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.297737 on epoch=144
03/14/2022 13:23:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.350678 on epoch=149
03/14/2022 13:23:07 - INFO - __main__ - Global step 300 Train loss 0.405782 Classification-F1 0.3333333333333333 on epoch=149
03/14/2022 13:23:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.350534 on epoch=154
03/14/2022 13:23:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.344348 on epoch=159
03/14/2022 13:23:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.359070 on epoch=164
03/14/2022 13:23:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.295741 on epoch=169
03/14/2022 13:23:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.289007 on epoch=174
03/14/2022 13:23:40 - INFO - __main__ - Global step 350 Train loss 0.327740 Classification-F1 0.25720720720720724 on epoch=174
03/14/2022 13:23:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.548631 on epoch=179
03/14/2022 13:23:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.252363 on epoch=184
03/14/2022 13:23:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.294716 on epoch=189
03/14/2022 13:24:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.268930 on epoch=194
03/14/2022 13:24:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.254723 on epoch=199
03/14/2022 13:24:13 - INFO - __main__ - Global step 400 Train loss 0.323872 Classification-F1 0.40641711229946526 on epoch=199
03/14/2022 13:24:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.267049 on epoch=204
03/14/2022 13:24:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.286660 on epoch=209
03/14/2022 13:24:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.198220 on epoch=214
03/14/2022 13:24:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.183169 on epoch=219
03/14/2022 13:24:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.213252 on epoch=224
03/14/2022 13:24:46 - INFO - __main__ - Global step 450 Train loss 0.229670 Classification-F1 0.6235294117647059 on epoch=224
03/14/2022 13:24:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.227982 on epoch=229
03/14/2022 13:24:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.215262 on epoch=234
03/14/2022 13:25:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.153144 on epoch=239
03/14/2022 13:25:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.188372 on epoch=244
03/14/2022 13:25:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.179377 on epoch=249
03/14/2022 13:25:19 - INFO - __main__ - Global step 500 Train loss 0.192827 Classification-F1 0.716256157635468 on epoch=249
03/14/2022 13:25:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.097229 on epoch=254
03/14/2022 13:25:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.098593 on epoch=259
03/14/2022 13:25:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.086727 on epoch=264
03/14/2022 13:25:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.164738 on epoch=269
03/14/2022 13:25:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.227955 on epoch=274
03/14/2022 13:25:52 - INFO - __main__ - Global step 550 Train loss 0.135048 Classification-F1 0.3992490613266583 on epoch=274
03/14/2022 13:25:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.177193 on epoch=279
03/14/2022 13:26:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.114149 on epoch=284
03/14/2022 13:26:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.197320 on epoch=289
03/14/2022 13:26:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.082613 on epoch=294
03/14/2022 13:26:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.046920 on epoch=299
03/14/2022 13:26:24 - INFO - __main__ - Global step 600 Train loss 0.123639 Classification-F1 0.7793103448275862 on epoch=299
03/14/2022 13:26:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:26:25 - INFO - __main__ - Printing 3 examples
03/14/2022 13:26:25 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/14/2022 13:26:25 - INFO - __main__ - ['positive']
03/14/2022 13:26:25 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/14/2022 13:26:25 - INFO - __main__ - ['positive']
03/14/2022 13:26:25 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/14/2022 13:26:25 - INFO - __main__ - ['positive']
03/14/2022 13:26:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 13:26:25 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:26:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 13:26:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:26:25 - INFO - __main__ - Printing 3 examples
03/14/2022 13:26:25 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/14/2022 13:26:25 - INFO - __main__ - ['positive']
03/14/2022 13:26:25 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/14/2022 13:26:25 - INFO - __main__ - ['positive']
03/14/2022 13:26:25 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/14/2022 13:26:25 - INFO - __main__ - ['positive']
03/14/2022 13:26:25 - INFO - __main__ - Tokenizing Input ...
03/14/2022 13:26:25 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:26:25 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 13:26:25 - INFO - __main__ - save last model!
03/14/2022 13:26:32 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 13:26:33 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 13:26:33 - INFO - __main__ - Printing 3 examples
03/14/2022 13:26:33 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 13:26:33 - INFO - __main__ - ['negative']
03/14/2022 13:26:33 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 13:26:33 - INFO - __main__ - ['negative']
03/14/2022 13:26:33 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 13:26:33 - INFO - __main__ - ['negative']
03/14/2022 13:26:33 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 13:26:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 13:26:37 - INFO - __main__ - Starting training!
03/14/2022 13:26:39 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:26:46 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 13:29:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_21_0.0002_8_predictions.txt
03/14/2022 13:29:33 - INFO - __main__ - Classification-F1 on test data: 0.8352
03/14/2022 13:29:33 - INFO - __main__ - prefix=yelp_polarity_16_21, lr=0.0002, bsz=8, dev_performance=0.7793103448275862, test_performance=0.8352164159584325
03/14/2022 13:29:33 - INFO - __main__ - Running ... prefix=yelp_polarity_16_21, lr=0.0001, bsz=8 ...
03/14/2022 13:29:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:29:34 - INFO - __main__ - Printing 3 examples
03/14/2022 13:29:34 - INFO - __main__ -  [yelp_polarity] Stopped into eat while attending the 2011 ironman.  There was also a Badger football game going one so the place was packed.  Even though we had over 10 people in our group they seated us as soon as they could.  Staff was very friendly, service was prompt and food really good.  Would go back and recommend!
03/14/2022 13:29:34 - INFO - __main__ - ['positive']
03/14/2022 13:29:34 - INFO - __main__ -  [yelp_polarity] We were provided samples of several flavors of the house made gelato by the owner's husband.  Each was delicious and after much consideration we finally settled on a 2-scoop cup of tiramisu and ciccolate y peperoncino (chocolate and pepperoncini). The latter was slightly spicy, but it did not overwhelm the chocolate flavor.  I am certain future trips will include a return visit or two to field test a few more flavors.
03/14/2022 13:29:34 - INFO - __main__ - ['positive']
03/14/2022 13:29:34 - INFO - __main__ -  [yelp_polarity] Best. Sangria. PERIOD.  Came by myself but ended up finding another friendly diner who was willing to split a selection of tapas dishes with me. We tried the artichoke toasts, stuffed dates, padron peppers, warm spinach salad, roasted eggplant cannelloni, manchego mac 'n' cheese, a fruit and cheese platter, and of course the sangria. Each dish was delightful, although the warm spinach salad was my least favorite. To be fair, it was up against some really flavorful, memorable dishes. I would eat the stuffed dates every day if my arteries would allow it.  Not to beat a dead horse, but this place has ruined all other sangria for me. I eagerly await my next trip to Vegas, if only for the chance to dine here again.
03/14/2022 13:29:34 - INFO - __main__ - ['positive']
03/14/2022 13:29:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 13:29:34 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:29:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 13:29:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:29:34 - INFO - __main__ - Printing 3 examples
03/14/2022 13:29:34 - INFO - __main__ -  [yelp_polarity] My husband and I stopped in yesterday to get food to go.  The bread on the perfect grilled cheese is fantastic.  We also tried two salads and both were great.
03/14/2022 13:29:34 - INFO - __main__ - ['positive']
03/14/2022 13:29:34 - INFO - __main__ -  [yelp_polarity] Fantastic service!!! Steak was great. However, I ordered my ribeye cap rare-plus and it came out almost medium. Might have been the sizzling plate. Nevertheless it was tender and flavorful. Great atmosphere! Very unsuspecting place in a surprisingly upscale center.   Dress code: none. We came in jeans and tee and even a baseball cap. No dirty looks. No questions/comments/etc. Granted a good portion of diners were in more dapper attire. Oh wells!
03/14/2022 13:29:34 - INFO - __main__ - ['positive']
03/14/2022 13:29:34 - INFO - __main__ -  [yelp_polarity] This was my home for 4 days and 3 nights, I shared it with a friend and there was plenty of room for drunken stumbling, recovery and attempting to remember what we did, who we talked to and why'd we drink like that again...   The room was a good size, with a kitchenette, microwave, sink and all the normal pots, pans, plates and flatware you'd expect in a kitchen. There was even a blender, fridge and a toaster for use. I guess these are actually high rise condos and some people live in these places. Sickness.   The shower was big, the spa tub was nice, sinks nice and the toilet was as expected. My only problem with the bathroom was that there is NO fan in the toilet area...the guy I was sharing the room with is a health nut and was taking protein powder...lemme just say....BLEH!   We stayed on the 34th floor and had a nice view of the strip during the day and night. The windows are tinted so if you're too drunk to close em the night before you aren't getting beamed in the eyes by the dreaded Vegas sun. There's complimentary Internet access so I was able to post my shenanigans on FB and let others know what type of drunken debauchery was going down.   The problem I had with the hotel was that it was a bit of a journey to get to the main hotel as well as the food areas. But other than that, the staff was helpful, the maids were nice and they don't bother you if you and a friend walk in stumbling and your friend barfs in the empty vase on display in the Lobby.   Fun times! I'll be back, dunno if I'll get accepted back but I'll try! : )
03/14/2022 13:29:34 - INFO - __main__ - ['positive']
03/14/2022 13:29:34 - INFO - __main__ - Tokenizing Input ...
03/14/2022 13:29:34 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:29:34 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 13:29:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 13:29:45 - INFO - __main__ - Starting training!
03/14/2022 13:29:51 - INFO - __main__ - Step 10 Global step 10 Train loss 23.817799 on epoch=4
03/14/2022 13:29:58 - INFO - __main__ - Step 20 Global step 20 Train loss 18.130354 on epoch=9
03/14/2022 13:30:04 - INFO - __main__ - Step 30 Global step 30 Train loss 18.526516 on epoch=14
03/14/2022 13:30:10 - INFO - __main__ - Step 40 Global step 40 Train loss 17.000025 on epoch=19
03/14/2022 13:30:17 - INFO - __main__ - Step 50 Global step 50 Train loss 17.070560 on epoch=24
03/14/2022 13:30:28 - INFO - __main__ - Global step 50 Train loss 18.909050 Classification-F1 0.0 on epoch=24
03/14/2022 13:30:35 - INFO - __main__ - Step 60 Global step 60 Train loss 15.906360 on epoch=29
03/14/2022 13:30:41 - INFO - __main__ - Step 70 Global step 70 Train loss 15.670329 on epoch=34
03/14/2022 13:30:48 - INFO - __main__ - Step 80 Global step 80 Train loss 15.152122 on epoch=39
03/14/2022 13:30:54 - INFO - __main__ - Step 90 Global step 90 Train loss 14.957959 on epoch=44
03/14/2022 13:31:00 - INFO - __main__ - Step 100 Global step 100 Train loss 13.790210 on epoch=49
03/14/2022 13:31:09 - INFO - __main__ - Global step 100 Train loss 15.095397 Classification-F1 0.0 on epoch=49
03/14/2022 13:31:15 - INFO - __main__ - Step 110 Global step 110 Train loss 13.858406 on epoch=54
03/14/2022 13:31:22 - INFO - __main__ - Step 120 Global step 120 Train loss 13.883451 on epoch=59
03/14/2022 13:31:28 - INFO - __main__ - Step 130 Global step 130 Train loss 13.656683 on epoch=64
03/14/2022 13:31:34 - INFO - __main__ - Step 140 Global step 140 Train loss 13.813522 on epoch=69
03/14/2022 13:31:41 - INFO - __main__ - Step 150 Global step 150 Train loss 12.843862 on epoch=74
03/14/2022 13:31:51 - INFO - __main__ - Global step 150 Train loss 13.611184 Classification-F1 0.0 on epoch=74
03/14/2022 13:31:57 - INFO - __main__ - Step 160 Global step 160 Train loss 12.109190 on epoch=79
03/14/2022 13:32:03 - INFO - __main__ - Step 170 Global step 170 Train loss 11.190024 on epoch=84
03/14/2022 13:32:10 - INFO - __main__ - Step 180 Global step 180 Train loss 10.864467 on epoch=89
03/14/2022 13:32:16 - INFO - __main__ - Step 190 Global step 190 Train loss 10.565975 on epoch=94
03/14/2022 13:32:22 - INFO - __main__ - Step 200 Global step 200 Train loss 8.967246 on epoch=99
03/14/2022 13:32:31 - INFO - __main__ - Global step 200 Train loss 10.739381 Classification-F1 0.0056022408963585435 on epoch=99
03/14/2022 13:32:38 - INFO - __main__ - Step 210 Global step 210 Train loss 7.249264 on epoch=104
03/14/2022 13:32:44 - INFO - __main__ - Step 220 Global step 220 Train loss 3.388799 on epoch=109
03/14/2022 13:32:51 - INFO - __main__ - Step 230 Global step 230 Train loss 2.606259 on epoch=114
03/14/2022 13:32:57 - INFO - __main__ - Step 240 Global step 240 Train loss 2.450350 on epoch=119
03/14/2022 13:33:03 - INFO - __main__ - Step 250 Global step 250 Train loss 1.754191 on epoch=124
03/14/2022 13:33:08 - INFO - __main__ - Global step 250 Train loss 3.489773 Classification-F1 0.04132231404958678 on epoch=124
03/14/2022 13:33:15 - INFO - __main__ - Step 260 Global step 260 Train loss 1.565219 on epoch=129
03/14/2022 13:33:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.525344 on epoch=134
03/14/2022 13:33:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.309514 on epoch=139
03/14/2022 13:33:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.365461 on epoch=144
03/14/2022 13:33:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.346980 on epoch=149
03/14/2022 13:33:41 - INFO - __main__ - Global step 300 Train loss 0.622503 Classification-F1 0.8745098039215686 on epoch=149
03/14/2022 13:33:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.288018 on epoch=154
03/14/2022 13:33:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.261406 on epoch=159
03/14/2022 13:34:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.326772 on epoch=164
03/14/2022 13:34:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.349700 on epoch=169
03/14/2022 13:34:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.259599 on epoch=174
03/14/2022 13:34:14 - INFO - __main__ - Global step 350 Train loss 0.297099 Classification-F1 0.906158357771261 on epoch=174
03/14/2022 13:34:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.448156 on epoch=179
03/14/2022 13:34:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.271685 on epoch=184
03/14/2022 13:34:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.276448 on epoch=189
03/14/2022 13:34:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.234398 on epoch=194
03/14/2022 13:34:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.269994 on epoch=199
03/14/2022 13:34:48 - INFO - __main__ - Global step 400 Train loss 0.300136 Classification-F1 0.5717171717171717 on epoch=199
03/14/2022 13:34:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.278955 on epoch=204
03/14/2022 13:35:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.227080 on epoch=209
03/14/2022 13:35:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.283788 on epoch=214
03/14/2022 13:35:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.250931 on epoch=219
03/14/2022 13:35:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.225762 on epoch=224
03/14/2022 13:35:20 - INFO - __main__ - Global step 450 Train loss 0.253303 Classification-F1 0.906158357771261 on epoch=224
03/14/2022 13:35:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.233558 on epoch=229
03/14/2022 13:35:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.255782 on epoch=234
03/14/2022 13:35:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.294722 on epoch=239
03/14/2022 13:35:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.363795 on epoch=244
03/14/2022 13:35:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.236295 on epoch=249
03/14/2022 13:35:52 - INFO - __main__ - Global step 500 Train loss 0.276830 Classification-F1 0.9375 on epoch=249
03/14/2022 13:35:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.302736 on epoch=254
03/14/2022 13:36:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.243894 on epoch=259
03/14/2022 13:36:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.250094 on epoch=264
03/14/2022 13:36:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.247496 on epoch=269
03/14/2022 13:36:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.226141 on epoch=274
03/14/2022 13:36:25 - INFO - __main__ - Global step 550 Train loss 0.254072 Classification-F1 0.9372549019607843 on epoch=274
03/14/2022 13:36:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.221883 on epoch=279
03/14/2022 13:36:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.290785 on epoch=284
03/14/2022 13:36:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.286917 on epoch=289
03/14/2022 13:36:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.243669 on epoch=294
03/14/2022 13:36:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.270371 on epoch=299
03/14/2022 13:36:58 - INFO - __main__ - Global step 600 Train loss 0.262725 Classification-F1 0.9372549019607843 on epoch=299
03/14/2022 13:36:58 - INFO - __main__ - save last model!
03/14/2022 13:36:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:36:58 - INFO - __main__ - Printing 3 examples
03/14/2022 13:36:58 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/14/2022 13:36:58 - INFO - __main__ - ['positive']
03/14/2022 13:36:58 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/14/2022 13:36:58 - INFO - __main__ - ['positive']
03/14/2022 13:36:58 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/14/2022 13:36:58 - INFO - __main__ - ['positive']
03/14/2022 13:36:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 13:36:58 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:36:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 13:36:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:36:58 - INFO - __main__ - Printing 3 examples
03/14/2022 13:36:58 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/14/2022 13:36:58 - INFO - __main__ - ['positive']
03/14/2022 13:36:58 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/14/2022 13:36:58 - INFO - __main__ - ['positive']
03/14/2022 13:36:58 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/14/2022 13:36:58 - INFO - __main__ - ['positive']
03/14/2022 13:36:58 - INFO - __main__ - Tokenizing Input ...
03/14/2022 13:36:58 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:36:58 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 13:37:05 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 13:37:06 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 13:37:06 - INFO - __main__ - Printing 3 examples
03/14/2022 13:37:06 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 13:37:06 - INFO - __main__ - ['negative']
03/14/2022 13:37:06 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 13:37:06 - INFO - __main__ - ['negative']
03/14/2022 13:37:06 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 13:37:06 - INFO - __main__ - ['negative']
03/14/2022 13:37:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 13:37:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 13:37:09 - INFO - __main__ - Starting training!
03/14/2022 13:37:12 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:37:19 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 13:40:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_21_0.0001_8_predictions.txt
03/14/2022 13:40:07 - INFO - __main__ - Classification-F1 on test data: 0.4627
03/14/2022 13:40:07 - INFO - __main__ - prefix=yelp_polarity_16_21, lr=0.0001, bsz=8, dev_performance=0.9375, test_performance=0.4626828265074024
03/14/2022 13:40:07 - INFO - __main__ - Running ... prefix=yelp_polarity_16_42, lr=0.0005, bsz=8 ...
03/14/2022 13:40:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:40:08 - INFO - __main__ - Printing 3 examples
03/14/2022 13:40:08 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/14/2022 13:40:08 - INFO - __main__ - ['positive']
03/14/2022 13:40:08 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/14/2022 13:40:08 - INFO - __main__ - ['positive']
03/14/2022 13:40:08 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/14/2022 13:40:08 - INFO - __main__ - ['positive']
03/14/2022 13:40:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 13:40:08 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:40:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 13:40:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:40:08 - INFO - __main__ - Printing 3 examples
03/14/2022 13:40:08 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/14/2022 13:40:08 - INFO - __main__ - ['positive']
03/14/2022 13:40:08 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/14/2022 13:40:08 - INFO - __main__ - ['positive']
03/14/2022 13:40:08 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/14/2022 13:40:08 - INFO - __main__ - ['positive']
03/14/2022 13:40:08 - INFO - __main__ - Tokenizing Input ...
03/14/2022 13:40:08 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:40:08 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 13:40:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 13:40:21 - INFO - __main__ - Starting training!
03/14/2022 13:40:27 - INFO - __main__ - Step 10 Global step 10 Train loss 22.229599 on epoch=4
03/14/2022 13:40:33 - INFO - __main__ - Step 20 Global step 20 Train loss 18.160662 on epoch=9
03/14/2022 13:40:39 - INFO - __main__ - Step 30 Global step 30 Train loss 15.638596 on epoch=14
03/14/2022 13:40:45 - INFO - __main__ - Step 40 Global step 40 Train loss 13.741450 on epoch=19
03/14/2022 13:40:52 - INFO - __main__ - Step 50 Global step 50 Train loss 11.271267 on epoch=24
03/14/2022 13:40:57 - INFO - __main__ - Global step 50 Train loss 16.208315 Classification-F1 0.018518518518518517 on epoch=24
03/14/2022 13:41:04 - INFO - __main__ - Step 60 Global step 60 Train loss 7.130945 on epoch=29
03/14/2022 13:41:11 - INFO - __main__ - Step 70 Global step 70 Train loss 2.321344 on epoch=34
03/14/2022 13:41:17 - INFO - __main__ - Step 80 Global step 80 Train loss 2.641981 on epoch=39
03/14/2022 13:41:23 - INFO - __main__ - Step 90 Global step 90 Train loss 1.071441 on epoch=44
03/14/2022 13:41:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.848478 on epoch=49
03/14/2022 13:41:30 - INFO - __main__ - Global step 100 Train loss 2.802838 Classification-F1 0.3333333333333333 on epoch=49
03/14/2022 13:41:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.559268 on epoch=54
03/14/2022 13:41:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.142888 on epoch=59
03/14/2022 13:41:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.064708 on epoch=64
03/14/2022 13:41:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.011673 on epoch=69
03/14/2022 13:42:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.018310 on epoch=74
03/14/2022 13:42:02 - INFO - __main__ - Global step 150 Train loss 0.159369 Classification-F1 0.9375 on epoch=74
03/14/2022 13:42:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.026255 on epoch=79
03/14/2022 13:42:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.025186 on epoch=84
03/14/2022 13:42:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.003098 on epoch=89
03/14/2022 13:42:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.004243 on epoch=94
03/14/2022 13:42:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.207537 on epoch=99
03/14/2022 13:42:35 - INFO - __main__ - Global step 200 Train loss 0.053264 Classification-F1 0.9372549019607843 on epoch=99
03/14/2022 13:42:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.006690 on epoch=104
03/14/2022 13:42:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.012671 on epoch=109
03/14/2022 13:42:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.010461 on epoch=114
03/14/2022 13:43:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001258 on epoch=119
03/14/2022 13:43:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.001273 on epoch=124
03/14/2022 13:43:06 - INFO - __main__ - Global step 250 Train loss 0.006471 Classification-F1 0.873015873015873 on epoch=124
03/14/2022 13:43:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.003974 on epoch=129
03/14/2022 13:43:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000873 on epoch=134
03/14/2022 13:43:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.002177 on epoch=139
03/14/2022 13:43:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000456 on epoch=144
03/14/2022 13:43:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000333 on epoch=149
03/14/2022 13:43:38 - INFO - __main__ - Global step 300 Train loss 0.001563 Classification-F1 0.9375 on epoch=149
03/14/2022 13:43:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000674 on epoch=154
03/14/2022 13:43:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000295 on epoch=159
03/14/2022 13:43:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000297 on epoch=164
03/14/2022 13:44:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000233 on epoch=169
03/14/2022 13:44:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.004930 on epoch=174
03/14/2022 13:44:10 - INFO - __main__ - Global step 350 Train loss 0.001286 Classification-F1 0.9375 on epoch=174
03/14/2022 13:44:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001664 on epoch=179
03/14/2022 13:44:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000910 on epoch=184
03/14/2022 13:44:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000181 on epoch=189
03/14/2022 13:44:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000453 on epoch=194
03/14/2022 13:44:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.333401 on epoch=199
03/14/2022 13:44:41 - INFO - __main__ - Global step 400 Train loss 0.067322 Classification-F1 0.9054187192118226 on epoch=199
03/14/2022 13:44:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.030384 on epoch=204
03/14/2022 13:44:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.003007 on epoch=209
03/14/2022 13:45:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000551 on epoch=214
03/14/2022 13:45:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000326 on epoch=219
03/14/2022 13:45:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000167 on epoch=224
03/14/2022 13:45:13 - INFO - __main__ - Global step 450 Train loss 0.006887 Classification-F1 0.9375 on epoch=224
03/14/2022 13:45:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000239 on epoch=229
03/14/2022 13:45:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000134 on epoch=234
03/14/2022 13:45:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000187 on epoch=239
03/14/2022 13:45:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001615 on epoch=244
03/14/2022 13:45:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000307 on epoch=249
03/14/2022 13:45:45 - INFO - __main__ - Global step 500 Train loss 0.000496 Classification-F1 0.906158357771261 on epoch=249
03/14/2022 13:45:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000091 on epoch=254
03/14/2022 13:45:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000070 on epoch=259
03/14/2022 13:46:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000075 on epoch=264
03/14/2022 13:46:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000091 on epoch=269
03/14/2022 13:46:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000087 on epoch=274
03/14/2022 13:46:16 - INFO - __main__ - Global step 550 Train loss 0.000083 Classification-F1 0.906158357771261 on epoch=274
03/14/2022 13:46:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000112 on epoch=279
03/14/2022 13:46:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000168 on epoch=284
03/14/2022 13:46:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000071 on epoch=289
03/14/2022 13:46:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000041 on epoch=294
03/14/2022 13:46:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000064 on epoch=299
03/14/2022 13:46:49 - INFO - __main__ - Global step 600 Train loss 0.000091 Classification-F1 0.9375 on epoch=299
03/14/2022 13:46:49 - INFO - __main__ - save last model!
03/14/2022 13:46:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:46:49 - INFO - __main__ - Printing 3 examples
03/14/2022 13:46:49 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/14/2022 13:46:49 - INFO - __main__ - ['positive']
03/14/2022 13:46:49 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/14/2022 13:46:49 - INFO - __main__ - ['positive']
03/14/2022 13:46:49 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/14/2022 13:46:49 - INFO - __main__ - ['positive']
03/14/2022 13:46:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 13:46:49 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:46:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 13:46:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:46:49 - INFO - __main__ - Printing 3 examples
03/14/2022 13:46:49 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/14/2022 13:46:49 - INFO - __main__ - ['positive']
03/14/2022 13:46:49 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/14/2022 13:46:49 - INFO - __main__ - ['positive']
03/14/2022 13:46:49 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/14/2022 13:46:49 - INFO - __main__ - ['positive']
03/14/2022 13:46:49 - INFO - __main__ - Tokenizing Input ...
03/14/2022 13:46:49 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:46:49 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 13:46:56 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 13:46:56 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 13:46:56 - INFO - __main__ - Printing 3 examples
03/14/2022 13:46:56 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 13:46:56 - INFO - __main__ - ['negative']
03/14/2022 13:46:56 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 13:46:56 - INFO - __main__ - ['negative']
03/14/2022 13:46:56 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 13:46:56 - INFO - __main__ - ['negative']
03/14/2022 13:46:56 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 13:47:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 13:47:00 - INFO - __main__ - Starting training!
03/14/2022 13:47:03 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:47:10 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 13:49:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_42_0.0005_8_predictions.txt
03/14/2022 13:49:53 - INFO - __main__ - Classification-F1 on test data: 0.9146
03/14/2022 13:49:53 - INFO - __main__ - prefix=yelp_polarity_16_42, lr=0.0005, bsz=8, dev_performance=0.9375, test_performance=0.9145757843302209
03/14/2022 13:49:53 - INFO - __main__ - Running ... prefix=yelp_polarity_16_42, lr=0.0003, bsz=8 ...
03/14/2022 13:49:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:49:54 - INFO - __main__ - Printing 3 examples
03/14/2022 13:49:54 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/14/2022 13:49:54 - INFO - __main__ - ['positive']
03/14/2022 13:49:54 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/14/2022 13:49:54 - INFO - __main__ - ['positive']
03/14/2022 13:49:54 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/14/2022 13:49:54 - INFO - __main__ - ['positive']
03/14/2022 13:49:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 13:49:54 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:49:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 13:49:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:49:54 - INFO - __main__ - Printing 3 examples
03/14/2022 13:49:54 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/14/2022 13:49:54 - INFO - __main__ - ['positive']
03/14/2022 13:49:54 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/14/2022 13:49:54 - INFO - __main__ - ['positive']
03/14/2022 13:49:54 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/14/2022 13:49:54 - INFO - __main__ - ['positive']
03/14/2022 13:49:54 - INFO - __main__ - Tokenizing Input ...
03/14/2022 13:49:54 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:49:54 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 13:50:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 13:50:07 - INFO - __main__ - Starting training!
03/14/2022 13:50:13 - INFO - __main__ - Step 10 Global step 10 Train loss 23.460339 on epoch=4
03/14/2022 13:50:19 - INFO - __main__ - Step 20 Global step 20 Train loss 18.575138 on epoch=9
03/14/2022 13:50:25 - INFO - __main__ - Step 30 Global step 30 Train loss 16.741501 on epoch=14
03/14/2022 13:50:31 - INFO - __main__ - Step 40 Global step 40 Train loss 15.415495 on epoch=19
03/14/2022 13:50:37 - INFO - __main__ - Step 50 Global step 50 Train loss 15.550125 on epoch=24
03/14/2022 13:50:47 - INFO - __main__ - Global step 50 Train loss 17.948519 Classification-F1 0.0 on epoch=24
03/14/2022 13:50:54 - INFO - __main__ - Step 60 Global step 60 Train loss 14.438855 on epoch=29
03/14/2022 13:51:01 - INFO - __main__ - Step 70 Global step 70 Train loss 12.517298 on epoch=34
03/14/2022 13:51:07 - INFO - __main__ - Step 80 Global step 80 Train loss 11.292316 on epoch=39
03/14/2022 13:51:13 - INFO - __main__ - Step 90 Global step 90 Train loss 8.568943 on epoch=44
03/14/2022 13:51:19 - INFO - __main__ - Step 100 Global step 100 Train loss 5.313498 on epoch=49
03/14/2022 13:51:20 - INFO - __main__ - Global step 100 Train loss 10.426182 Classification-F1 0.13333333333333336 on epoch=49
03/14/2022 13:51:27 - INFO - __main__ - Step 110 Global step 110 Train loss 3.468857 on epoch=54
03/14/2022 13:51:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.719550 on epoch=59
03/14/2022 13:51:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.181697 on epoch=64
03/14/2022 13:51:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.103067 on epoch=69
03/14/2022 13:51:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.054019 on epoch=74
03/14/2022 13:51:52 - INFO - __main__ - Global step 150 Train loss 0.905438 Classification-F1 1.0 on epoch=74
03/14/2022 13:51:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.012641 on epoch=79
03/14/2022 13:52:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.814242 on epoch=84
03/14/2022 13:52:12 - INFO - __main__ - Step 180 Global step 180 Train loss 1.860721 on epoch=89
03/14/2022 13:52:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.133708 on epoch=94
03/14/2022 13:52:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.016315 on epoch=99
03/14/2022 13:52:25 - INFO - __main__ - Global step 200 Train loss 0.567525 Classification-F1 0.9687194525904204 on epoch=99
03/14/2022 13:52:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.020844 on epoch=104
03/14/2022 13:52:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.004771 on epoch=109
03/14/2022 13:52:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.003700 on epoch=114
03/14/2022 13:52:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001647 on epoch=119
03/14/2022 13:52:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.005064 on epoch=124
03/14/2022 13:52:56 - INFO - __main__ - Global step 250 Train loss 0.007205 Classification-F1 0.9372549019607843 on epoch=124
03/14/2022 13:53:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.041387 on epoch=129
03/14/2022 13:53:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.007989 on epoch=134
03/14/2022 13:53:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.001646 on epoch=139
03/14/2022 13:53:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.001585 on epoch=144
03/14/2022 13:53:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.001564 on epoch=149
03/14/2022 13:53:28 - INFO - __main__ - Global step 300 Train loss 0.010834 Classification-F1 0.9687194525904204 on epoch=149
03/14/2022 13:53:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.003889 on epoch=154
03/14/2022 13:53:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000710 on epoch=159
03/14/2022 13:53:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.004972 on epoch=164
03/14/2022 13:53:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.004188 on epoch=169
03/14/2022 13:53:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.005358 on epoch=174
03/14/2022 13:53:59 - INFO - __main__ - Global step 350 Train loss 0.003823 Classification-F1 0.9687194525904204 on epoch=174
03/14/2022 13:54:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000448 on epoch=179
03/14/2022 13:54:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.026867 on epoch=184
03/14/2022 13:54:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000778 on epoch=189
03/14/2022 13:54:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.002042 on epoch=194
03/14/2022 13:54:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000688 on epoch=199
03/14/2022 13:54:30 - INFO - __main__ - Global step 400 Train loss 0.006165 Classification-F1 0.9687194525904204 on epoch=199
03/14/2022 13:54:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000684 on epoch=204
03/14/2022 13:54:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000202 on epoch=209
03/14/2022 13:54:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000430 on epoch=214
03/14/2022 13:54:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.012532 on epoch=219
03/14/2022 13:55:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000459 on epoch=224
03/14/2022 13:55:02 - INFO - __main__ - Global step 450 Train loss 0.002862 Classification-F1 1.0 on epoch=224
03/14/2022 13:55:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000262 on epoch=229
03/14/2022 13:55:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000278 on epoch=234
03/14/2022 13:55:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000206 on epoch=239
03/14/2022 13:55:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000321 on epoch=244
03/14/2022 13:55:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000168 on epoch=249
03/14/2022 13:55:34 - INFO - __main__ - Global step 500 Train loss 0.000247 Classification-F1 0.9687194525904204 on epoch=249
03/14/2022 13:55:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000604 on epoch=254
03/14/2022 13:55:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000700 on epoch=259
03/14/2022 13:55:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000375 on epoch=264
03/14/2022 13:55:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000585 on epoch=269
03/14/2022 13:56:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000494 on epoch=274
03/14/2022 13:56:05 - INFO - __main__ - Global step 550 Train loss 0.000552 Classification-F1 0.9687194525904204 on epoch=274
03/14/2022 13:56:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001098 on epoch=279
03/14/2022 13:56:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000091 on epoch=284
03/14/2022 13:56:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.013197 on epoch=289
03/14/2022 13:56:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000377 on epoch=294
03/14/2022 13:56:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000804 on epoch=299
03/14/2022 13:56:37 - INFO - __main__ - Global step 600 Train loss 0.003113 Classification-F1 1.0 on epoch=299
03/14/2022 13:56:37 - INFO - __main__ - save last model!
03/14/2022 13:56:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:56:37 - INFO - __main__ - Printing 3 examples
03/14/2022 13:56:37 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/14/2022 13:56:37 - INFO - __main__ - ['positive']
03/14/2022 13:56:37 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/14/2022 13:56:37 - INFO - __main__ - ['positive']
03/14/2022 13:56:37 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/14/2022 13:56:37 - INFO - __main__ - ['positive']
03/14/2022 13:56:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 13:56:37 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:56:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 13:56:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:56:37 - INFO - __main__ - Printing 3 examples
03/14/2022 13:56:37 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/14/2022 13:56:37 - INFO - __main__ - ['positive']
03/14/2022 13:56:37 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/14/2022 13:56:37 - INFO - __main__ - ['positive']
03/14/2022 13:56:37 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/14/2022 13:56:37 - INFO - __main__ - ['positive']
03/14/2022 13:56:37 - INFO - __main__ - Tokenizing Input ...
03/14/2022 13:56:37 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:56:37 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 13:56:43 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 13:56:44 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 13:56:44 - INFO - __main__ - Printing 3 examples
03/14/2022 13:56:44 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 13:56:44 - INFO - __main__ - ['negative']
03/14/2022 13:56:44 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 13:56:44 - INFO - __main__ - ['negative']
03/14/2022 13:56:44 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 13:56:44 - INFO - __main__ - ['negative']
03/14/2022 13:56:44 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 13:56:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 13:56:50 - INFO - __main__ - Starting training!
03/14/2022 13:56:51 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:56:58 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 13:59:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_42_0.0003_8_predictions.txt
03/14/2022 13:59:43 - INFO - __main__ - Classification-F1 on test data: 0.9553
03/14/2022 13:59:44 - INFO - __main__ - prefix=yelp_polarity_16_42, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.9552557180733445
03/14/2022 13:59:44 - INFO - __main__ - Running ... prefix=yelp_polarity_16_42, lr=0.0002, bsz=8 ...
03/14/2022 13:59:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:59:45 - INFO - __main__ - Printing 3 examples
03/14/2022 13:59:45 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/14/2022 13:59:45 - INFO - __main__ - ['positive']
03/14/2022 13:59:45 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/14/2022 13:59:45 - INFO - __main__ - ['positive']
03/14/2022 13:59:45 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/14/2022 13:59:45 - INFO - __main__ - ['positive']
03/14/2022 13:59:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 13:59:45 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:59:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 13:59:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 13:59:45 - INFO - __main__ - Printing 3 examples
03/14/2022 13:59:45 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/14/2022 13:59:45 - INFO - __main__ - ['positive']
03/14/2022 13:59:45 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/14/2022 13:59:45 - INFO - __main__ - ['positive']
03/14/2022 13:59:45 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/14/2022 13:59:45 - INFO - __main__ - ['positive']
03/14/2022 13:59:45 - INFO - __main__ - Tokenizing Input ...
03/14/2022 13:59:45 - INFO - __main__ - Tokenizing Output ...
03/14/2022 13:59:45 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 13:59:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 13:59:55 - INFO - __main__ - Starting training!
03/14/2022 14:00:01 - INFO - __main__ - Step 10 Global step 10 Train loss 25.701365 on epoch=4
03/14/2022 14:00:07 - INFO - __main__ - Step 20 Global step 20 Train loss 19.662073 on epoch=9
03/14/2022 14:00:13 - INFO - __main__ - Step 30 Global step 30 Train loss 17.521709 on epoch=14
03/14/2022 14:00:19 - INFO - __main__ - Step 40 Global step 40 Train loss 15.774341 on epoch=19
03/14/2022 14:00:25 - INFO - __main__ - Step 50 Global step 50 Train loss 14.892940 on epoch=24
03/14/2022 14:00:37 - INFO - __main__ - Global step 50 Train loss 18.710485 Classification-F1 0.0 on epoch=24
03/14/2022 14:00:44 - INFO - __main__ - Step 60 Global step 60 Train loss 13.834857 on epoch=29
03/14/2022 14:00:50 - INFO - __main__ - Step 70 Global step 70 Train loss 13.019053 on epoch=34
03/14/2022 14:00:56 - INFO - __main__ - Step 80 Global step 80 Train loss 12.782738 on epoch=39
03/14/2022 14:01:02 - INFO - __main__ - Step 90 Global step 90 Train loss 11.789937 on epoch=44
03/14/2022 14:01:08 - INFO - __main__ - Step 100 Global step 100 Train loss 10.576070 on epoch=49
03/14/2022 14:01:13 - INFO - __main__ - Global step 100 Train loss 12.400531 Classification-F1 0.014705882352941176 on epoch=49
03/14/2022 14:01:19 - INFO - __main__ - Step 110 Global step 110 Train loss 7.197553 on epoch=54
03/14/2022 14:01:26 - INFO - __main__ - Step 120 Global step 120 Train loss 3.571910 on epoch=59
03/14/2022 14:01:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.436023 on epoch=64
03/14/2022 14:01:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.271191 on epoch=69
03/14/2022 14:01:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.471671 on epoch=74
03/14/2022 14:01:45 - INFO - __main__ - Global step 150 Train loss 2.389669 Classification-F1 0.9372549019607843 on epoch=74
03/14/2022 14:01:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.474816 on epoch=79
03/14/2022 14:01:58 - INFO - __main__ - Step 170 Global step 170 Train loss 1.414002 on epoch=84
03/14/2022 14:02:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.363756 on epoch=89
03/14/2022 14:02:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.137610 on epoch=94
03/14/2022 14:02:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.158878 on epoch=99
03/14/2022 14:02:17 - INFO - __main__ - Global step 200 Train loss 0.509812 Classification-F1 0.9687194525904204 on epoch=99
03/14/2022 14:02:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.083180 on epoch=104
03/14/2022 14:02:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.073873 on epoch=109
03/14/2022 14:02:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.091711 on epoch=114
03/14/2022 14:02:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.011840 on epoch=119
03/14/2022 14:02:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.058690 on epoch=124
03/14/2022 14:02:49 - INFO - __main__ - Global step 250 Train loss 0.063859 Classification-F1 0.9687194525904204 on epoch=124
03/14/2022 14:02:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.018476 on epoch=129
03/14/2022 14:03:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.017720 on epoch=134
03/14/2022 14:03:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.003980 on epoch=139
03/14/2022 14:03:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002086 on epoch=144
03/14/2022 14:03:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.007650 on epoch=149
03/14/2022 14:03:20 - INFO - __main__ - Global step 300 Train loss 0.009982 Classification-F1 0.9687194525904204 on epoch=149
03/14/2022 14:03:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.001469 on epoch=154
03/14/2022 14:03:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.031579 on epoch=159
03/14/2022 14:03:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.057977 on epoch=164
03/14/2022 14:03:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.019036 on epoch=169
03/14/2022 14:03:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000442 on epoch=174
03/14/2022 14:03:52 - INFO - __main__ - Global step 350 Train loss 0.022100 Classification-F1 0.9687194525904204 on epoch=174
03/14/2022 14:03:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001920 on epoch=179
03/14/2022 14:04:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.005985 on epoch=184
03/14/2022 14:04:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.001553 on epoch=189
03/14/2022 14:04:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001011 on epoch=194
03/14/2022 14:04:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.007872 on epoch=199
03/14/2022 14:04:23 - INFO - __main__ - Global step 400 Train loss 0.003668 Classification-F1 1.0 on epoch=199
03/14/2022 14:04:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000537 on epoch=204
03/14/2022 14:04:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000604 on epoch=209
03/14/2022 14:04:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000302 on epoch=214
03/14/2022 14:04:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001071 on epoch=219
03/14/2022 14:04:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000192 on epoch=224
03/14/2022 14:04:55 - INFO - __main__ - Global step 450 Train loss 0.000541 Classification-F1 0.9687194525904204 on epoch=224
03/14/2022 14:05:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.020579 on epoch=229
03/14/2022 14:05:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000161 on epoch=234
03/14/2022 14:05:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001425 on epoch=239
03/14/2022 14:05:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001023 on epoch=244
03/14/2022 14:05:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000186 on epoch=249
03/14/2022 14:05:27 - INFO - __main__ - Global step 500 Train loss 0.004675 Classification-F1 0.9687194525904204 on epoch=249
03/14/2022 14:05:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000210 on epoch=254
03/14/2022 14:05:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000143 on epoch=259
03/14/2022 14:05:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000202 on epoch=264
03/14/2022 14:05:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000109 on epoch=269
03/14/2022 14:05:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000370 on epoch=274
03/14/2022 14:05:59 - INFO - __main__ - Global step 550 Train loss 0.000207 Classification-F1 0.9687194525904204 on epoch=274
03/14/2022 14:06:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000441 on epoch=279
03/14/2022 14:06:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000072 on epoch=284
03/14/2022 14:06:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.023125 on epoch=289
03/14/2022 14:06:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.037307 on epoch=294
03/14/2022 14:06:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000189 on epoch=299
03/14/2022 14:06:30 - INFO - __main__ - Global step 600 Train loss 0.012227 Classification-F1 0.9687194525904204 on epoch=299
03/14/2022 14:06:30 - INFO - __main__ - save last model!
03/14/2022 14:06:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:06:31 - INFO - __main__ - Printing 3 examples
03/14/2022 14:06:31 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/14/2022 14:06:31 - INFO - __main__ - ['positive']
03/14/2022 14:06:31 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/14/2022 14:06:31 - INFO - __main__ - ['positive']
03/14/2022 14:06:31 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/14/2022 14:06:31 - INFO - __main__ - ['positive']
03/14/2022 14:06:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 14:06:31 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:06:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 14:06:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:06:31 - INFO - __main__ - Printing 3 examples
03/14/2022 14:06:31 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/14/2022 14:06:31 - INFO - __main__ - ['positive']
03/14/2022 14:06:31 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/14/2022 14:06:31 - INFO - __main__ - ['positive']
03/14/2022 14:06:31 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/14/2022 14:06:31 - INFO - __main__ - ['positive']
03/14/2022 14:06:31 - INFO - __main__ - Tokenizing Input ...
03/14/2022 14:06:31 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:06:31 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 14:06:36 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 14:06:37 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 14:06:37 - INFO - __main__ - Printing 3 examples
03/14/2022 14:06:37 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 14:06:37 - INFO - __main__ - ['negative']
03/14/2022 14:06:37 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 14:06:37 - INFO - __main__ - ['negative']
03/14/2022 14:06:37 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 14:06:37 - INFO - __main__ - ['negative']
03/14/2022 14:06:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 14:06:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 14:06:43 - INFO - __main__ - Starting training!
03/14/2022 14:06:44 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:06:51 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 14:09:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_42_0.0002_8_predictions.txt
03/14/2022 14:09:35 - INFO - __main__ - Classification-F1 on test data: 0.9548
03/14/2022 14:09:35 - INFO - __main__ - prefix=yelp_polarity_16_42, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.9548302261612871
03/14/2022 14:09:35 - INFO - __main__ - Running ... prefix=yelp_polarity_16_42, lr=0.0001, bsz=8 ...
03/14/2022 14:09:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:09:36 - INFO - __main__ - Printing 3 examples
03/14/2022 14:09:36 - INFO - __main__ -  [yelp_polarity] This place is one of my favorite comic shops. I actually live closer to a different one, but I drive to chandler just to go to this one. I like their selection and when they have the dollar sale you can get some ridiculous deals. The staff is ridiculously friendly and I usually always walk out with whatever I wanted. They also have some cool events from time to time and i've found their prices to be reasonable and comparable to other comic shops.
03/14/2022 14:09:36 - INFO - __main__ - ['positive']
03/14/2022 14:09:36 - INFO - __main__ -  [yelp_polarity] Mill Avenue has a serious issue with parking. While I am a fan of the various restaurants on this street, the parking situation is infuriating. I had to park in a residential area and risk getting my car towed, because I was in a rush and simply needed to get my food quickly and go.  The restaurant itself is just fine. There was good music, lots of friendly people, and the food was delicious. The line was long but it moved fairly quickly. I would definitely visit this restaurant again but the parking situation seriously needs to be addressed.
03/14/2022 14:09:36 - INFO - __main__ - ['positive']
03/14/2022 14:09:36 - INFO - __main__ -  [yelp_polarity] Favorite sushi place in NV!  Price is reasonable and food is incredible!  I will eat there every time I go to Las Vegas.
03/14/2022 14:09:36 - INFO - __main__ - ['positive']
03/14/2022 14:09:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 14:09:36 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:09:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 14:09:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:09:36 - INFO - __main__ - Printing 3 examples
03/14/2022 14:09:36 - INFO - __main__ -  [yelp_polarity] Probably the best show we've seen so far! It's really worth it, nice theater built especially for this and the show is amazing! Try to not seat in the first 3 rows, you'll get wet. You can get drinks and popcorn, but they are pretty expensive.
03/14/2022 14:09:36 - INFO - __main__ - ['positive']
03/14/2022 14:09:36 - INFO - __main__ -  [yelp_polarity] Dr. Amy is the BEST! She made me feel like family and took care of me and my dental situation. I did not go out of there in tears like my last dentist. Her front office staff Karla was understanding and very helpful. Thank you Dr. Amy and your staff for making me feel so comfortable and welcome.
03/14/2022 14:09:36 - INFO - __main__ - ['positive']
03/14/2022 14:09:36 - INFO - __main__ -  [yelp_polarity] I've been here a few times and let me tell you it's been a great experience in every visit, one of my two favorite appetizers is the nachos with carne Asada and the chorizo with queso, I could just keep coming back for these two,along with a beer at their newly added bar which by the way the variety it's pretty awesome, they have all kinds of tequilas you just wanna try them all haha! I'm sure they won't have a problem with that!,they also have a patio with a mister system which I haven't personally been into just yet because I've been there during the winter but that's the one feature I'm looking forward to go to during the summer; over all I thing it's a great place  to go and enjoy a nice meal with familly or friends, I will definitely come back!!
03/14/2022 14:09:36 - INFO - __main__ - ['positive']
03/14/2022 14:09:36 - INFO - __main__ - Tokenizing Input ...
03/14/2022 14:09:36 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:09:36 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 14:09:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 14:09:46 - INFO - __main__ - Starting training!
03/14/2022 14:09:55 - INFO - __main__ - Step 10 Global step 10 Train loss 23.431227 on epoch=4
03/14/2022 14:10:01 - INFO - __main__ - Step 20 Global step 20 Train loss 20.214491 on epoch=9
03/14/2022 14:10:07 - INFO - __main__ - Step 30 Global step 30 Train loss 17.705877 on epoch=14
03/14/2022 14:10:13 - INFO - __main__ - Step 40 Global step 40 Train loss 17.225437 on epoch=19
03/14/2022 14:10:19 - INFO - __main__ - Step 50 Global step 50 Train loss 16.804588 on epoch=24
03/14/2022 14:10:33 - INFO - __main__ - Global step 50 Train loss 19.076326 Classification-F1 0.0 on epoch=24
03/14/2022 14:10:40 - INFO - __main__ - Step 60 Global step 60 Train loss 15.959699 on epoch=29
03/14/2022 14:10:46 - INFO - __main__ - Step 70 Global step 70 Train loss 15.410213 on epoch=34
03/14/2022 14:10:52 - INFO - __main__ - Step 80 Global step 80 Train loss 15.124631 on epoch=39
03/14/2022 14:10:58 - INFO - __main__ - Step 90 Global step 90 Train loss 14.461634 on epoch=44
03/14/2022 14:11:04 - INFO - __main__ - Step 100 Global step 100 Train loss 14.340212 on epoch=49
03/14/2022 14:11:17 - INFO - __main__ - Global step 100 Train loss 15.059278 Classification-F1 0.0 on epoch=49
03/14/2022 14:11:23 - INFO - __main__ - Step 110 Global step 110 Train loss 14.286154 on epoch=54
03/14/2022 14:11:29 - INFO - __main__ - Step 120 Global step 120 Train loss 13.550273 on epoch=59
03/14/2022 14:11:36 - INFO - __main__ - Step 130 Global step 130 Train loss 13.442592 on epoch=64
03/14/2022 14:11:42 - INFO - __main__ - Step 140 Global step 140 Train loss 13.253566 on epoch=69
03/14/2022 14:11:48 - INFO - __main__ - Step 150 Global step 150 Train loss 12.282158 on epoch=74
03/14/2022 14:12:00 - INFO - __main__ - Global step 150 Train loss 13.362949 Classification-F1 0.0 on epoch=74
03/14/2022 14:12:06 - INFO - __main__ - Step 160 Global step 160 Train loss 12.457760 on epoch=79
03/14/2022 14:12:12 - INFO - __main__ - Step 170 Global step 170 Train loss 11.771916 on epoch=84
03/14/2022 14:12:19 - INFO - __main__ - Step 180 Global step 180 Train loss 10.347776 on epoch=89
03/14/2022 14:12:25 - INFO - __main__ - Step 190 Global step 190 Train loss 9.284361 on epoch=94
03/14/2022 14:12:31 - INFO - __main__ - Step 200 Global step 200 Train loss 8.395842 on epoch=99
03/14/2022 14:12:42 - INFO - __main__ - Global step 200 Train loss 10.451530 Classification-F1 0.14814814814814814 on epoch=99
03/14/2022 14:12:49 - INFO - __main__ - Step 210 Global step 210 Train loss 4.602535 on epoch=104
03/14/2022 14:12:56 - INFO - __main__ - Step 220 Global step 220 Train loss 2.439025 on epoch=109
03/14/2022 14:13:02 - INFO - __main__ - Step 230 Global step 230 Train loss 1.568153 on epoch=114
03/14/2022 14:13:08 - INFO - __main__ - Step 240 Global step 240 Train loss 1.590576 on epoch=119
03/14/2022 14:13:14 - INFO - __main__ - Step 250 Global step 250 Train loss 1.093961 on epoch=124
03/14/2022 14:13:15 - INFO - __main__ - Global step 250 Train loss 2.258850 Classification-F1 0.9375 on epoch=124
03/14/2022 14:13:22 - INFO - __main__ - Step 260 Global step 260 Train loss 1.799839 on epoch=129
03/14/2022 14:13:28 - INFO - __main__ - Step 270 Global step 270 Train loss 1.683654 on epoch=134
03/14/2022 14:13:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.435587 on epoch=139
03/14/2022 14:13:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.197330 on epoch=144
03/14/2022 14:13:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.213099 on epoch=149
03/14/2022 14:13:47 - INFO - __main__ - Global step 300 Train loss 0.865902 Classification-F1 0.9687194525904204 on epoch=149
03/14/2022 14:13:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.144122 on epoch=154
03/14/2022 14:14:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.430988 on epoch=159
03/14/2022 14:14:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.475881 on epoch=164
03/14/2022 14:14:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.468017 on epoch=169
03/14/2022 14:14:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.653699 on epoch=174
03/14/2022 14:14:19 - INFO - __main__ - Global step 350 Train loss 0.434541 Classification-F1 0.9687194525904204 on epoch=174
03/14/2022 14:14:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.268224 on epoch=179
03/14/2022 14:14:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.190900 on epoch=184
03/14/2022 14:14:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.284659 on epoch=189
03/14/2022 14:14:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.446053 on epoch=194
03/14/2022 14:14:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.479809 on epoch=199
03/14/2022 14:14:51 - INFO - __main__ - Global step 400 Train loss 0.333929 Classification-F1 0.9687194525904204 on epoch=199
03/14/2022 14:14:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.315216 on epoch=204
03/14/2022 14:15:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.145984 on epoch=209
03/14/2022 14:15:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.276338 on epoch=214
03/14/2022 14:15:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.293144 on epoch=219
03/14/2022 14:15:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.099834 on epoch=224
03/14/2022 14:15:22 - INFO - __main__ - Global step 450 Train loss 0.226103 Classification-F1 0.9687194525904204 on epoch=224
03/14/2022 14:15:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.283262 on epoch=229
03/14/2022 14:15:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.109148 on epoch=234
03/14/2022 14:15:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.117989 on epoch=239
03/14/2022 14:15:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.040671 on epoch=244
03/14/2022 14:15:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.053729 on epoch=249
03/14/2022 14:15:54 - INFO - __main__ - Global step 500 Train loss 0.120960 Classification-F1 0.9687194525904204 on epoch=249
03/14/2022 14:16:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.039128 on epoch=254
03/14/2022 14:16:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.051134 on epoch=259
03/14/2022 14:16:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.057514 on epoch=264
03/14/2022 14:16:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.034752 on epoch=269
03/14/2022 14:16:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.017384 on epoch=274
03/14/2022 14:16:26 - INFO - __main__ - Global step 550 Train loss 0.039982 Classification-F1 0.9687194525904204 on epoch=274
03/14/2022 14:16:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.053778 on epoch=279
03/14/2022 14:16:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.028958 on epoch=284
03/14/2022 14:16:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.018075 on epoch=289
03/14/2022 14:16:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.068642 on epoch=294
03/14/2022 14:16:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.042410 on epoch=299
03/14/2022 14:16:57 - INFO - __main__ - Global step 600 Train loss 0.042373 Classification-F1 0.9375 on epoch=299
03/14/2022 14:16:57 - INFO - __main__ - save last model!
03/14/2022 14:16:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:16:58 - INFO - __main__ - Printing 3 examples
03/14/2022 14:16:58 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/14/2022 14:16:58 - INFO - __main__ - ['positive']
03/14/2022 14:16:58 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/14/2022 14:16:58 - INFO - __main__ - ['positive']
03/14/2022 14:16:58 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/14/2022 14:16:58 - INFO - __main__ - ['positive']
03/14/2022 14:16:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 14:16:58 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:16:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 14:16:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:16:58 - INFO - __main__ - Printing 3 examples
03/14/2022 14:16:58 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/14/2022 14:16:58 - INFO - __main__ - ['positive']
03/14/2022 14:16:58 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/14/2022 14:16:58 - INFO - __main__ - ['positive']
03/14/2022 14:16:58 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/14/2022 14:16:58 - INFO - __main__ - ['positive']
03/14/2022 14:16:58 - INFO - __main__ - Tokenizing Input ...
03/14/2022 14:16:58 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:16:58 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 14:17:04 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 14:17:05 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 14:17:05 - INFO - __main__ - Printing 3 examples
03/14/2022 14:17:05 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 14:17:05 - INFO - __main__ - ['negative']
03/14/2022 14:17:05 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 14:17:05 - INFO - __main__ - ['negative']
03/14/2022 14:17:05 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 14:17:05 - INFO - __main__ - ['negative']
03/14/2022 14:17:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 14:17:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 14:17:11 - INFO - __main__ - Starting training!
03/14/2022 14:17:11 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:17:19 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 14:20:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_42_0.0001_8_predictions.txt
03/14/2022 14:20:02 - INFO - __main__ - Classification-F1 on test data: 0.3177
03/14/2022 14:20:02 - INFO - __main__ - prefix=yelp_polarity_16_42, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.317672489853538
03/14/2022 14:20:02 - INFO - __main__ - Running ... prefix=yelp_polarity_16_87, lr=0.0005, bsz=8 ...
03/14/2022 14:20:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:20:03 - INFO - __main__ - Printing 3 examples
03/14/2022 14:20:03 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/14/2022 14:20:03 - INFO - __main__ - ['positive']
03/14/2022 14:20:03 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/14/2022 14:20:03 - INFO - __main__ - ['positive']
03/14/2022 14:20:03 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/14/2022 14:20:03 - INFO - __main__ - ['positive']
03/14/2022 14:20:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 14:20:03 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:20:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 14:20:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:20:03 - INFO - __main__ - Printing 3 examples
03/14/2022 14:20:03 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/14/2022 14:20:03 - INFO - __main__ - ['positive']
03/14/2022 14:20:03 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/14/2022 14:20:03 - INFO - __main__ - ['positive']
03/14/2022 14:20:03 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/14/2022 14:20:03 - INFO - __main__ - ['positive']
03/14/2022 14:20:03 - INFO - __main__ - Tokenizing Input ...
03/14/2022 14:20:03 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:20:03 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 14:20:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 14:20:14 - INFO - __main__ - Starting training!
03/14/2022 14:20:19 - INFO - __main__ - Step 10 Global step 10 Train loss 23.295023 on epoch=4
03/14/2022 14:20:25 - INFO - __main__ - Step 20 Global step 20 Train loss 16.832111 on epoch=9
03/14/2022 14:20:32 - INFO - __main__ - Step 30 Global step 30 Train loss 14.708758 on epoch=14
03/14/2022 14:20:38 - INFO - __main__ - Step 40 Global step 40 Train loss 12.034437 on epoch=19
03/14/2022 14:20:44 - INFO - __main__ - Step 50 Global step 50 Train loss 9.616117 on epoch=24
03/14/2022 14:20:58 - INFO - __main__ - Global step 50 Train loss 15.297291 Classification-F1 0.0 on epoch=24
03/14/2022 14:21:05 - INFO - __main__ - Step 60 Global step 60 Train loss 3.935359 on epoch=29
03/14/2022 14:21:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.474516 on epoch=34
03/14/2022 14:21:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.342183 on epoch=39
03/14/2022 14:21:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.266720 on epoch=44
03/14/2022 14:21:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.224508 on epoch=49
03/14/2022 14:21:30 - INFO - __main__ - Global step 100 Train loss 1.048657 Classification-F1 0.8435972629521017 on epoch=49
03/14/2022 14:21:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.073764 on epoch=54
03/14/2022 14:21:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.148695 on epoch=59
03/14/2022 14:21:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.058448 on epoch=64
03/14/2022 14:21:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.004281 on epoch=69
03/14/2022 14:22:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.076984 on epoch=74
03/14/2022 14:22:02 - INFO - __main__ - Global step 150 Train loss 0.072434 Classification-F1 0.8745098039215686 on epoch=74
03/14/2022 14:22:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.005640 on epoch=79
03/14/2022 14:22:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.002549 on epoch=84
03/14/2022 14:22:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.002183 on epoch=89
03/14/2022 14:22:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.002312 on epoch=94
03/14/2022 14:22:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.001024 on epoch=99
03/14/2022 14:22:34 - INFO - __main__ - Global step 200 Train loss 0.002742 Classification-F1 0.906158357771261 on epoch=99
03/14/2022 14:22:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.000933 on epoch=104
03/14/2022 14:22:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000449 on epoch=109
03/14/2022 14:22:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001638 on epoch=114
03/14/2022 14:22:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000586 on epoch=119
03/14/2022 14:23:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000298 on epoch=124
03/14/2022 14:23:06 - INFO - __main__ - Global step 250 Train loss 0.000781 Classification-F1 0.9375 on epoch=124
03/14/2022 14:23:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000607 on epoch=129
03/14/2022 14:23:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.021749 on epoch=134
03/14/2022 14:23:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.019590 on epoch=139
03/14/2022 14:23:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.001474 on epoch=144
03/14/2022 14:23:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000544 on epoch=149
03/14/2022 14:23:38 - INFO - __main__ - Global step 300 Train loss 0.008793 Classification-F1 0.906158357771261 on epoch=149
03/14/2022 14:23:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000469 on epoch=154
03/14/2022 14:23:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000674 on epoch=159
03/14/2022 14:23:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000396 on epoch=164
03/14/2022 14:24:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000523 on epoch=169
03/14/2022 14:24:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000595 on epoch=174
03/14/2022 14:24:09 - INFO - __main__ - Global step 350 Train loss 0.000531 Classification-F1 0.8745098039215686 on epoch=174
03/14/2022 14:24:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.069719 on epoch=179
03/14/2022 14:24:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.232150 on epoch=184
03/14/2022 14:24:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.087419 on epoch=189
03/14/2022 14:24:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.017179 on epoch=194
03/14/2022 14:24:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.012645 on epoch=199
03/14/2022 14:24:40 - INFO - __main__ - Global step 400 Train loss 0.083822 Classification-F1 0.8125 on epoch=199
03/14/2022 14:24:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.064269 on epoch=204
03/14/2022 14:24:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.131253 on epoch=209
03/14/2022 14:24:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.010810 on epoch=214
03/14/2022 14:25:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.004281 on epoch=219
03/14/2022 14:25:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.001442 on epoch=224
03/14/2022 14:25:12 - INFO - __main__ - Global step 450 Train loss 0.042411 Classification-F1 0.8435972629521017 on epoch=224
03/14/2022 14:25:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000952 on epoch=229
03/14/2022 14:25:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000562 on epoch=234
03/14/2022 14:25:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001324 on epoch=239
03/14/2022 14:25:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.007596 on epoch=244
03/14/2022 14:25:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.064255 on epoch=249
03/14/2022 14:25:43 - INFO - __main__ - Global step 500 Train loss 0.014938 Classification-F1 0.7793103448275862 on epoch=249
03/14/2022 14:25:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.011445 on epoch=254
03/14/2022 14:25:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000477 on epoch=259
03/14/2022 14:26:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001354 on epoch=264
03/14/2022 14:26:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001503 on epoch=269
03/14/2022 14:26:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001277 on epoch=274
03/14/2022 14:26:14 - INFO - __main__ - Global step 550 Train loss 0.003211 Classification-F1 0.8125 on epoch=274
03/14/2022 14:26:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000294 on epoch=279
03/14/2022 14:26:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000140 on epoch=284
03/14/2022 14:26:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.004711 on epoch=289
03/14/2022 14:26:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000270 on epoch=294
03/14/2022 14:26:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.006865 on epoch=299
03/14/2022 14:26:45 - INFO - __main__ - Global step 600 Train loss 0.002456 Classification-F1 0.8435972629521017 on epoch=299
03/14/2022 14:26:45 - INFO - __main__ - save last model!
03/14/2022 14:26:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:26:46 - INFO - __main__ - Printing 3 examples
03/14/2022 14:26:46 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/14/2022 14:26:46 - INFO - __main__ - ['positive']
03/14/2022 14:26:46 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/14/2022 14:26:46 - INFO - __main__ - ['positive']
03/14/2022 14:26:46 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/14/2022 14:26:46 - INFO - __main__ - ['positive']
03/14/2022 14:26:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 14:26:46 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:26:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 14:26:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:26:46 - INFO - __main__ - Printing 3 examples
03/14/2022 14:26:46 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/14/2022 14:26:46 - INFO - __main__ - ['positive']
03/14/2022 14:26:46 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/14/2022 14:26:46 - INFO - __main__ - ['positive']
03/14/2022 14:26:46 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/14/2022 14:26:46 - INFO - __main__ - ['positive']
03/14/2022 14:26:46 - INFO - __main__ - Tokenizing Input ...
03/14/2022 14:26:46 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:26:46 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 14:26:52 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 14:26:53 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 14:26:53 - INFO - __main__ - Printing 3 examples
03/14/2022 14:26:53 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 14:26:53 - INFO - __main__ - ['negative']
03/14/2022 14:26:53 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 14:26:53 - INFO - __main__ - ['negative']
03/14/2022 14:26:53 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 14:26:53 - INFO - __main__ - ['negative']
03/14/2022 14:26:53 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 14:26:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 14:26:58 - INFO - __main__ - Starting training!
03/14/2022 14:26:59 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:27:07 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 14:29:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_87_0.0005_8_predictions.txt
03/14/2022 14:29:53 - INFO - __main__ - Classification-F1 on test data: 0.9389
03/14/2022 14:29:53 - INFO - __main__ - prefix=yelp_polarity_16_87, lr=0.0005, bsz=8, dev_performance=0.9375, test_performance=0.9389435629644507
03/14/2022 14:29:53 - INFO - __main__ - Running ... prefix=yelp_polarity_16_87, lr=0.0003, bsz=8 ...
03/14/2022 14:29:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:29:54 - INFO - __main__ - Printing 3 examples
03/14/2022 14:29:54 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/14/2022 14:29:54 - INFO - __main__ - ['positive']
03/14/2022 14:29:54 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/14/2022 14:29:54 - INFO - __main__ - ['positive']
03/14/2022 14:29:54 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/14/2022 14:29:54 - INFO - __main__ - ['positive']
03/14/2022 14:29:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 14:29:54 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:29:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 14:29:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:29:54 - INFO - __main__ - Printing 3 examples
03/14/2022 14:29:54 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/14/2022 14:29:54 - INFO - __main__ - ['positive']
03/14/2022 14:29:54 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/14/2022 14:29:54 - INFO - __main__ - ['positive']
03/14/2022 14:29:54 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/14/2022 14:29:54 - INFO - __main__ - ['positive']
03/14/2022 14:29:54 - INFO - __main__ - Tokenizing Input ...
03/14/2022 14:29:54 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:29:54 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 14:30:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 14:30:07 - INFO - __main__ - Starting training!
03/14/2022 14:30:13 - INFO - __main__ - Step 10 Global step 10 Train loss 22.898594 on epoch=4
03/14/2022 14:30:19 - INFO - __main__ - Step 20 Global step 20 Train loss 17.528791 on epoch=9
03/14/2022 14:30:25 - INFO - __main__ - Step 30 Global step 30 Train loss 16.368593 on epoch=14
03/14/2022 14:30:31 - INFO - __main__ - Step 40 Global step 40 Train loss 14.821452 on epoch=19
03/14/2022 14:30:37 - INFO - __main__ - Step 50 Global step 50 Train loss 13.642047 on epoch=24
03/14/2022 14:30:41 - INFO - __main__ - Global step 50 Train loss 17.051895 Classification-F1 0.0 on epoch=24
03/14/2022 14:30:48 - INFO - __main__ - Step 60 Global step 60 Train loss 12.665302 on epoch=29
03/14/2022 14:30:54 - INFO - __main__ - Step 70 Global step 70 Train loss 11.179861 on epoch=34
03/14/2022 14:31:00 - INFO - __main__ - Step 80 Global step 80 Train loss 4.953536 on epoch=39
03/14/2022 14:31:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.517180 on epoch=44
03/14/2022 14:31:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.053532 on epoch=49
03/14/2022 14:31:13 - INFO - __main__ - Global step 100 Train loss 5.873882 Classification-F1 0.6125760649087221 on epoch=49
03/14/2022 14:31:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.121848 on epoch=54
03/14/2022 14:31:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.028342 on epoch=59
03/14/2022 14:31:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.003386 on epoch=64
03/14/2022 14:31:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.006123 on epoch=69
03/14/2022 14:31:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.002296 on epoch=74
03/14/2022 14:31:46 - INFO - __main__ - Global step 150 Train loss 0.032399 Classification-F1 0.6559139784946236 on epoch=74
03/14/2022 14:31:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.000942 on epoch=79
03/14/2022 14:31:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.019883 on epoch=84
03/14/2022 14:32:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.034680 on epoch=89
03/14/2022 14:32:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.001322 on epoch=94
03/14/2022 14:32:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.001987 on epoch=99
03/14/2022 14:32:18 - INFO - __main__ - Global step 200 Train loss 0.011763 Classification-F1 1.0 on epoch=99
03/14/2022 14:32:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.001188 on epoch=104
03/14/2022 14:32:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000969 on epoch=109
03/14/2022 14:32:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.001502 on epoch=114
03/14/2022 14:32:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.005788 on epoch=119
03/14/2022 14:32:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000245 on epoch=124
03/14/2022 14:32:50 - INFO - __main__ - Global step 250 Train loss 0.001938 Classification-F1 0.9687194525904204 on epoch=124
03/14/2022 14:32:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000171 on epoch=129
03/14/2022 14:33:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.099141 on epoch=134
03/14/2022 14:33:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.002450 on epoch=139
03/14/2022 14:33:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.008471 on epoch=144
03/14/2022 14:33:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000639 on epoch=149
03/14/2022 14:33:21 - INFO - __main__ - Global step 300 Train loss 0.022174 Classification-F1 0.9372549019607843 on epoch=149
03/14/2022 14:33:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000286 on epoch=154
03/14/2022 14:33:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000437 on epoch=159
03/14/2022 14:33:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000354 on epoch=164
03/14/2022 14:33:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000503 on epoch=169
03/14/2022 14:33:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000197 on epoch=174
03/14/2022 14:33:52 - INFO - __main__ - Global step 350 Train loss 0.000356 Classification-F1 0.9372549019607843 on epoch=174
03/14/2022 14:33:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000132 on epoch=179
03/14/2022 14:34:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000123 on epoch=184
03/14/2022 14:34:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000085 on epoch=189
03/14/2022 14:34:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000096 on epoch=194
03/14/2022 14:34:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000157 on epoch=199
03/14/2022 14:34:23 - INFO - __main__ - Global step 400 Train loss 0.000119 Classification-F1 0.9372549019607843 on epoch=199
03/14/2022 14:34:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000131 on epoch=204
03/14/2022 14:34:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000099 on epoch=209
03/14/2022 14:34:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000052 on epoch=214
03/14/2022 14:34:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000093 on epoch=219
03/14/2022 14:34:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000825 on epoch=224
03/14/2022 14:34:54 - INFO - __main__ - Global step 450 Train loss 0.000240 Classification-F1 0.9687194525904204 on epoch=224
03/14/2022 14:35:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000350 on epoch=229
03/14/2022 14:35:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000115 on epoch=234
03/14/2022 14:35:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000036 on epoch=239
03/14/2022 14:35:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000068 on epoch=244
03/14/2022 14:35:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000993 on epoch=249
03/14/2022 14:35:25 - INFO - __main__ - Global step 500 Train loss 0.000312 Classification-F1 0.9687194525904204 on epoch=249
03/14/2022 14:35:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000190 on epoch=254
03/14/2022 14:35:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000045 on epoch=259
03/14/2022 14:35:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000033 on epoch=264
03/14/2022 14:35:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000040 on epoch=269
03/14/2022 14:35:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000016 on epoch=274
03/14/2022 14:35:56 - INFO - __main__ - Global step 550 Train loss 0.000065 Classification-F1 0.9687194525904204 on epoch=274
03/14/2022 14:36:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000034 on epoch=279
03/14/2022 14:36:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000011 on epoch=284
03/14/2022 14:36:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000023 on epoch=289
03/14/2022 14:36:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000018 on epoch=294
03/14/2022 14:36:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000035 on epoch=299
03/14/2022 14:36:27 - INFO - __main__ - Global step 600 Train loss 0.000024 Classification-F1 0.9372549019607843 on epoch=299
03/14/2022 14:36:27 - INFO - __main__ - save last model!
03/14/2022 14:36:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:36:28 - INFO - __main__ - Printing 3 examples
03/14/2022 14:36:28 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/14/2022 14:36:28 - INFO - __main__ - ['positive']
03/14/2022 14:36:28 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/14/2022 14:36:28 - INFO - __main__ - ['positive']
03/14/2022 14:36:28 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/14/2022 14:36:28 - INFO - __main__ - ['positive']
03/14/2022 14:36:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 14:36:28 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:36:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 14:36:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:36:28 - INFO - __main__ - Printing 3 examples
03/14/2022 14:36:28 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/14/2022 14:36:28 - INFO - __main__ - ['positive']
03/14/2022 14:36:28 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/14/2022 14:36:28 - INFO - __main__ - ['positive']
03/14/2022 14:36:28 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/14/2022 14:36:28 - INFO - __main__ - ['positive']
03/14/2022 14:36:28 - INFO - __main__ - Tokenizing Input ...
03/14/2022 14:36:28 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:36:28 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 14:36:34 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 14:36:35 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 14:36:35 - INFO - __main__ - Printing 3 examples
03/14/2022 14:36:35 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 14:36:35 - INFO - __main__ - ['negative']
03/14/2022 14:36:35 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 14:36:35 - INFO - __main__ - ['negative']
03/14/2022 14:36:35 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 14:36:35 - INFO - __main__ - ['negative']
03/14/2022 14:36:35 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 14:36:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 14:36:39 - INFO - __main__ - Starting training!
03/14/2022 14:36:42 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:36:49 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 14:40:01 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_87_0.0003_8_predictions.txt
03/14/2022 14:40:01 - INFO - __main__ - Classification-F1 on test data: 0.1598
03/14/2022 14:40:02 - INFO - __main__ - prefix=yelp_polarity_16_87, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.15977575154846732
03/14/2022 14:40:02 - INFO - __main__ - Running ... prefix=yelp_polarity_16_87, lr=0.0002, bsz=8 ...
03/14/2022 14:40:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:40:03 - INFO - __main__ - Printing 3 examples
03/14/2022 14:40:03 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/14/2022 14:40:03 - INFO - __main__ - ['positive']
03/14/2022 14:40:03 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/14/2022 14:40:03 - INFO - __main__ - ['positive']
03/14/2022 14:40:03 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/14/2022 14:40:03 - INFO - __main__ - ['positive']
03/14/2022 14:40:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 14:40:03 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:40:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 14:40:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:40:03 - INFO - __main__ - Printing 3 examples
03/14/2022 14:40:03 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/14/2022 14:40:03 - INFO - __main__ - ['positive']
03/14/2022 14:40:03 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/14/2022 14:40:03 - INFO - __main__ - ['positive']
03/14/2022 14:40:03 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/14/2022 14:40:03 - INFO - __main__ - ['positive']
03/14/2022 14:40:03 - INFO - __main__ - Tokenizing Input ...
03/14/2022 14:40:03 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:40:03 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 14:40:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 14:40:15 - INFO - __main__ - Starting training!
03/14/2022 14:40:22 - INFO - __main__ - Step 10 Global step 10 Train loss 23.768965 on epoch=4
03/14/2022 14:40:28 - INFO - __main__ - Step 20 Global step 20 Train loss 17.935154 on epoch=9
03/14/2022 14:40:35 - INFO - __main__ - Step 30 Global step 30 Train loss 16.290628 on epoch=14
03/14/2022 14:40:41 - INFO - __main__ - Step 40 Global step 40 Train loss 15.966875 on epoch=19
03/14/2022 14:40:47 - INFO - __main__ - Step 50 Global step 50 Train loss 15.522962 on epoch=24
03/14/2022 14:41:01 - INFO - __main__ - Global step 50 Train loss 17.896917 Classification-F1 0.0 on epoch=24
03/14/2022 14:41:07 - INFO - __main__ - Step 60 Global step 60 Train loss 14.730677 on epoch=29
03/14/2022 14:41:13 - INFO - __main__ - Step 70 Global step 70 Train loss 13.437895 on epoch=34
03/14/2022 14:41:19 - INFO - __main__ - Step 80 Global step 80 Train loss 13.135287 on epoch=39
03/14/2022 14:41:25 - INFO - __main__ - Step 90 Global step 90 Train loss 12.045431 on epoch=44
03/14/2022 14:41:31 - INFO - __main__ - Step 100 Global step 100 Train loss 10.799685 on epoch=49
03/14/2022 14:41:32 - INFO - __main__ - Global step 100 Train loss 12.829795 Classification-F1 0.0 on epoch=49
03/14/2022 14:41:38 - INFO - __main__ - Step 110 Global step 110 Train loss 9.320612 on epoch=54
03/14/2022 14:41:44 - INFO - __main__ - Step 120 Global step 120 Train loss 1.911997 on epoch=59
03/14/2022 14:41:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.636729 on epoch=64
03/14/2022 14:41:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.274779 on epoch=69
03/14/2022 14:42:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.020491 on epoch=74
03/14/2022 14:42:03 - INFO - __main__ - Global step 150 Train loss 2.432922 Classification-F1 0.6559139784946236 on epoch=74
03/14/2022 14:42:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.037456 on epoch=79
03/14/2022 14:42:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.003737 on epoch=84
03/14/2022 14:42:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.011395 on epoch=89
03/14/2022 14:42:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.006590 on epoch=94
03/14/2022 14:42:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.003071 on epoch=99
03/14/2022 14:42:35 - INFO - __main__ - Global step 200 Train loss 0.012450 Classification-F1 0.6559139784946236 on epoch=99
03/14/2022 14:42:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.004042 on epoch=104
03/14/2022 14:42:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.002968 on epoch=109
03/14/2022 14:42:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.670138 on epoch=114
03/14/2022 14:42:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.509821 on epoch=119
03/14/2022 14:43:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.005272 on epoch=124
03/14/2022 14:43:06 - INFO - __main__ - Global step 250 Train loss 0.238448 Classification-F1 0.6559139784946236 on epoch=124
03/14/2022 14:43:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.001428 on epoch=129
03/14/2022 14:43:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.036839 on epoch=134
03/14/2022 14:43:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.007065 on epoch=139
03/14/2022 14:43:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.081840 on epoch=144
03/14/2022 14:43:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.131198 on epoch=149
03/14/2022 14:43:37 - INFO - __main__ - Global step 300 Train loss 0.051674 Classification-F1 0.9687194525904204 on epoch=149
03/14/2022 14:43:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.035061 on epoch=154
03/14/2022 14:43:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.032271 on epoch=159
03/14/2022 14:43:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.054988 on epoch=164
03/14/2022 14:44:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.096124 on epoch=169
03/14/2022 14:44:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.052626 on epoch=174
03/14/2022 14:44:09 - INFO - __main__ - Global step 350 Train loss 0.054214 Classification-F1 1.0 on epoch=174
03/14/2022 14:44:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.063624 on epoch=179
03/14/2022 14:44:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.276331 on epoch=184
03/14/2022 14:44:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.113349 on epoch=189
03/14/2022 14:44:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.094030 on epoch=194
03/14/2022 14:44:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.027402 on epoch=199
03/14/2022 14:44:41 - INFO - __main__ - Global step 400 Train loss 0.114947 Classification-F1 1.0 on epoch=199
03/14/2022 14:44:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.330847 on epoch=204
03/14/2022 14:44:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.077157 on epoch=209
03/14/2022 14:44:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.149402 on epoch=214
03/14/2022 14:45:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.094314 on epoch=219
03/14/2022 14:45:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.066862 on epoch=224
03/14/2022 14:45:12 - INFO - __main__ - Global step 450 Train loss 0.143716 Classification-F1 0.9054187192118226 on epoch=224
03/14/2022 14:45:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.067808 on epoch=229
03/14/2022 14:45:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.046220 on epoch=234
03/14/2022 14:45:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.060586 on epoch=239
03/14/2022 14:45:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.051064 on epoch=244
03/14/2022 14:45:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.028208 on epoch=249
03/14/2022 14:45:43 - INFO - __main__ - Global step 500 Train loss 0.050777 Classification-F1 0.9054187192118226 on epoch=249
03/14/2022 14:45:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.056714 on epoch=254
03/14/2022 14:45:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.078501 on epoch=259
03/14/2022 14:46:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.061049 on epoch=264
03/14/2022 14:46:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.213245 on epoch=269
03/14/2022 14:46:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.020388 on epoch=274
03/14/2022 14:46:15 - INFO - __main__ - Global step 550 Train loss 0.085979 Classification-F1 1.0 on epoch=274
03/14/2022 14:46:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.034676 on epoch=279
03/14/2022 14:46:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.017368 on epoch=284
03/14/2022 14:46:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.043022 on epoch=289
03/14/2022 14:46:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.033389 on epoch=294
03/14/2022 14:46:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.010686 on epoch=299
03/14/2022 14:46:46 - INFO - __main__ - Global step 600 Train loss 0.027828 Classification-F1 0.9687194525904204 on epoch=299
03/14/2022 14:46:46 - INFO - __main__ - save last model!
03/14/2022 14:46:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:46:46 - INFO - __main__ - Printing 3 examples
03/14/2022 14:46:46 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/14/2022 14:46:46 - INFO - __main__ - ['positive']
03/14/2022 14:46:46 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/14/2022 14:46:46 - INFO - __main__ - ['positive']
03/14/2022 14:46:46 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/14/2022 14:46:46 - INFO - __main__ - ['positive']
03/14/2022 14:46:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/14/2022 14:46:46 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:46:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 14:46:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:46:46 - INFO - __main__ - Printing 3 examples
03/14/2022 14:46:46 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/14/2022 14:46:46 - INFO - __main__ - ['positive']
03/14/2022 14:46:46 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/14/2022 14:46:46 - INFO - __main__ - ['positive']
03/14/2022 14:46:46 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/14/2022 14:46:46 - INFO - __main__ - ['positive']
03/14/2022 14:46:46 - INFO - __main__ - Tokenizing Input ...
03/14/2022 14:46:46 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:46:46 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 14:46:52 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 14:46:53 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 14:46:53 - INFO - __main__ - Printing 3 examples
03/14/2022 14:46:53 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 14:46:53 - INFO - __main__ - ['negative']
03/14/2022 14:46:53 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 14:46:53 - INFO - __main__ - ['negative']
03/14/2022 14:46:53 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 14:46:53 - INFO - __main__ - ['negative']
03/14/2022 14:46:53 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 14:46:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 14:46:57 - INFO - __main__ - Starting training!
03/14/2022 14:47:00 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:47:07 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 14:49:51 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_87_0.0002_8_predictions.txt
03/14/2022 14:49:51 - INFO - __main__ - Classification-F1 on test data: 0.6446
03/14/2022 14:49:52 - INFO - __main__ - prefix=yelp_polarity_16_87, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.6446456747108301
03/14/2022 14:49:52 - INFO - __main__ - Running ... prefix=yelp_polarity_16_87, lr=0.0001, bsz=8 ...
03/14/2022 14:49:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:49:52 - INFO - __main__ - Printing 3 examples
03/14/2022 14:49:52 - INFO - __main__ -  [yelp_polarity] Freddy's Frozen Custard & Steakburgers has a drive thru! That may have honestly been my big reason for coming here - try some place new and too lazy to get out of the car to get it. I'm so glad I went here though because the food is rocking.  I ordered a Single Steakburger California Style (steakburger patty, Freddy's Sauce, cheese, onions, lettuce and tomato) with a large shoestring fries and a small Chocolate Brownie Delight Concrete for dessert (chocolate custard, hot fudge, brownie, and whipped cream with a cherry on top). I had ordered this separately, but the lady at the drive thru made it a combo to make it a little cheaper for me.  Actually, the best part was how great and friendly that lady at the drive thru was. I was very surprised. She was bubbly and patient as I ordered for myself and the passengers in my car. A really close second was how delicious and filling my food was when I got home. I was stuffed and I enjoyed every bite of food I ate. And now that I've written this review, I want to go back and buy some food here (but alas, it's closed right now). Yum!
03/14/2022 14:49:52 - INFO - __main__ - ['positive']
03/14/2022 14:49:52 - INFO - __main__ -  [yelp_polarity] I recently used Vicky and Pierre to complete a new home purchase.  The process was seamless!  Vicky is SUPER responsive and we were able to close in less than 30 days.  These guys will do whatever it takes to get the job done.  I would definitely recommend the Cornerstone team, they are great!!
03/14/2022 14:49:52 - INFO - __main__ - ['positive']
03/14/2022 14:49:52 - INFO - __main__ -  [yelp_polarity] I feel sorry for these guys.  It's not their fault.  Half the Borgotta is shut-down.  Retailers gone.  It's practically a ghost town here.   Every time I slip into Dolce, I can't leave without dashing in here to get a piece of Lavash to go.  OK, it's not a 'piece', it's as long as my arm!  I am so addicted to it!  They use it for the base of their flatbread pizza, which makes their pizza very unique and light and surprisingly very good (it's very difficult to please me with pizza - has to be very thin crust, extremely fresh ingredients). Fresh chopped tomatoes for the sauce, yellow, red & orange peppers for toppings, along with cherry tomatoes and I suppose mostly anything else you may want, but that was mine.  (mushrooms?  No thank you).    Unfortunately, something tells me that they won't be around very long, seemingly due to management from Westcor, from what I've heard.  Shame that they (Borgota/Westcor) don't appear to want to work with the local retailers, because this place used to be pretty unique and special, where I'd bring any visitors to - not anymore.
03/14/2022 14:49:52 - INFO - __main__ - ['positive']
03/14/2022 14:49:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 14:49:52 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:49:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/14/2022 14:49:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/14/2022 14:49:53 - INFO - __main__ - Printing 3 examples
03/14/2022 14:49:53 - INFO - __main__ -  [yelp_polarity] Finally a big Chinese buffet that's good!!  A buddy and myself checked this place out last week and I couldn't be more happier.  Well I could, but I was very satisfied..  They had a huge assortment of foods from all your popular Chinese dishes to even a couple American ones.  Buffalo wings..! yum.  They even have a BBQ station.  I wasn't too impressed with it, so passed it up, but they also had some fresh Sushi for people to try out.    Would i go back? Oh heck yeah.  Very good food and service!
03/14/2022 14:49:53 - INFO - __main__ - ['positive']
03/14/2022 14:49:53 - INFO - __main__ -  [yelp_polarity] Not too shabby.  A nice getaway from the flashy lights of the Vegas Strip.  They had an ok selection of drafts (I drank Bass) and it was a place I could actually hold a conversation with my friends without talking loudly like most bars we had been to all week.  Thank goodness there are low key places to hang out at in Vegas! haha
03/14/2022 14:49:53 - INFO - __main__ - ['positive']
03/14/2022 14:49:53 - INFO - __main__ -  [yelp_polarity] So I've been off work randomly on the same day SOHO has been closed for the past month and finally this afternoon I got to enjoy the place.   Every single thing I've read and head is true. He place is just superb. The staff is super attentive and probably the happiest, most friendly staff at a restaurant I have ever come across. I ordered just about 4 dishes at the sushi bar. The first thing was the uni sampler of 2 different types (mind you I've never thought of uni as being something I'd like because of the texture and I use to step on those bastards when surfing in hawaii) both were very fresh, smooth, and tasty like I'd never expect by looking at it. The second was the Cajun albacore on happy hour that the server, who was very friendly and sweet, suggested and was very good, not too spicy, not over seasoned. Third I got the sashimi sampler of about 8-10 pieces and wow, the presentation was great, the fish was super fresh and he cuts were huge compared to other places. Third I ordered was dessert and my god the green tea tiramisu was recommended and highly appreciated! I was so full and yet I stayed to eat every last bite of what was obviously prepared with love and skill.   Best sushi I've very had and best deal. I grew up in hawaii and have had the best of the best from my Japanese heritage, yet this place gave me 5 diamond service with super good attitudes and great service for an amazing deal. Love it!
03/14/2022 14:49:53 - INFO - __main__ - ['positive']
03/14/2022 14:49:53 - INFO - __main__ - Tokenizing Input ...
03/14/2022 14:49:53 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:49:53 - INFO - __main__ - Loaded 32 examples from dev data
03/14/2022 14:50:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
03/14/2022 14:50:03 - INFO - __main__ - Starting training!
03/14/2022 14:50:09 - INFO - __main__ - Step 10 Global step 10 Train loss 22.673130 on epoch=4
03/14/2022 14:50:15 - INFO - __main__ - Step 20 Global step 20 Train loss 19.229366 on epoch=9
03/14/2022 14:50:21 - INFO - __main__ - Step 30 Global step 30 Train loss 18.238201 on epoch=14
03/14/2022 14:50:27 - INFO - __main__ - Step 40 Global step 40 Train loss 17.850704 on epoch=19
03/14/2022 14:50:33 - INFO - __main__ - Step 50 Global step 50 Train loss 17.097223 on epoch=24
03/14/2022 14:50:44 - INFO - __main__ - Global step 50 Train loss 19.017725 Classification-F1 0.0 on epoch=24
03/14/2022 14:50:51 - INFO - __main__ - Step 60 Global step 60 Train loss 16.261385 on epoch=29
03/14/2022 14:50:57 - INFO - __main__ - Step 70 Global step 70 Train loss 16.010738 on epoch=34
03/14/2022 14:51:03 - INFO - __main__ - Step 80 Global step 80 Train loss 15.384074 on epoch=39
03/14/2022 14:51:09 - INFO - __main__ - Step 90 Global step 90 Train loss 14.531021 on epoch=44
03/14/2022 14:51:15 - INFO - __main__ - Step 100 Global step 100 Train loss 14.855609 on epoch=49
03/14/2022 14:51:24 - INFO - __main__ - Global step 100 Train loss 15.408566 Classification-F1 0.0 on epoch=49
03/14/2022 14:51:30 - INFO - __main__ - Step 110 Global step 110 Train loss 14.344086 on epoch=54
03/14/2022 14:51:36 - INFO - __main__ - Step 120 Global step 120 Train loss 14.536428 on epoch=59
03/14/2022 14:51:42 - INFO - __main__ - Step 130 Global step 130 Train loss 13.442311 on epoch=64
03/14/2022 14:51:48 - INFO - __main__ - Step 140 Global step 140 Train loss 13.196417 on epoch=69
03/14/2022 14:51:54 - INFO - __main__ - Step 150 Global step 150 Train loss 13.339305 on epoch=74
03/14/2022 14:51:55 - INFO - __main__ - Global step 150 Train loss 13.771709 Classification-F1 0.0 on epoch=74
03/14/2022 14:52:01 - INFO - __main__ - Step 160 Global step 160 Train loss 12.978668 on epoch=79
03/14/2022 14:52:08 - INFO - __main__ - Step 170 Global step 170 Train loss 12.024139 on epoch=84
03/14/2022 14:52:14 - INFO - __main__ - Step 180 Global step 180 Train loss 11.096582 on epoch=89
03/14/2022 14:52:20 - INFO - __main__ - Step 190 Global step 190 Train loss 10.757922 on epoch=94
03/14/2022 14:52:26 - INFO - __main__ - Step 200 Global step 200 Train loss 10.044832 on epoch=99
03/14/2022 14:52:28 - INFO - __main__ - Global step 200 Train loss 11.380428 Classification-F1 0.0 on epoch=99
03/14/2022 14:52:34 - INFO - __main__ - Step 210 Global step 210 Train loss 9.767197 on epoch=104
03/14/2022 14:52:40 - INFO - __main__ - Step 220 Global step 220 Train loss 8.036098 on epoch=109
03/14/2022 14:52:46 - INFO - __main__ - Step 230 Global step 230 Train loss 5.522786 on epoch=114
03/14/2022 14:52:52 - INFO - __main__ - Step 240 Global step 240 Train loss 1.533545 on epoch=119
03/14/2022 14:52:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.841084 on epoch=124
03/14/2022 14:52:59 - INFO - __main__ - Global step 250 Train loss 5.140142 Classification-F1 0.6336917562724015 on epoch=124
03/14/2022 14:53:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.303476 on epoch=129
03/14/2022 14:53:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.416484 on epoch=134
03/14/2022 14:53:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.340092 on epoch=139
03/14/2022 14:53:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.092488 on epoch=144
03/14/2022 14:53:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.135896 on epoch=149
03/14/2022 14:53:31 - INFO - __main__ - Global step 300 Train loss 0.257687 Classification-F1 0.6343434343434343 on epoch=149
03/14/2022 14:53:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.085828 on epoch=154
03/14/2022 14:53:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.003258 on epoch=159
03/14/2022 14:53:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.004380 on epoch=164
03/14/2022 14:53:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.008710 on epoch=169
03/14/2022 14:54:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.047777 on epoch=174
03/14/2022 14:54:03 - INFO - __main__ - Global step 350 Train loss 0.029991 Classification-F1 0.6559139784946236 on epoch=174
03/14/2022 14:54:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.011713 on epoch=179
03/14/2022 14:54:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.002006 on epoch=184
03/14/2022 14:54:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.006033 on epoch=189
03/14/2022 14:54:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.002767 on epoch=194
03/14/2022 14:54:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.019397 on epoch=199
03/14/2022 14:54:34 - INFO - __main__ - Global step 400 Train loss 0.008383 Classification-F1 0.6559139784946236 on epoch=199
03/14/2022 14:54:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.038094 on epoch=204
03/14/2022 14:54:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.107252 on epoch=209
03/14/2022 14:54:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.129561 on epoch=214
03/14/2022 14:54:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.090663 on epoch=219
03/14/2022 14:55:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.001039 on epoch=224
03/14/2022 14:55:06 - INFO - __main__ - Global step 450 Train loss 0.073322 Classification-F1 0.6559139784946236 on epoch=224
03/14/2022 14:55:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.049762 on epoch=229
03/14/2022 14:55:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.038506 on epoch=234
03/14/2022 14:55:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.068782 on epoch=239
03/14/2022 14:55:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000618 on epoch=244
03/14/2022 14:55:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.002030 on epoch=249
03/14/2022 14:55:37 - INFO - __main__ - Global step 500 Train loss 0.031940 Classification-F1 0.6559139784946236 on epoch=249
03/14/2022 14:55:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001042 on epoch=254
03/14/2022 14:55:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.004043 on epoch=259
03/14/2022 14:55:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001366 on epoch=264
03/14/2022 14:56:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000477 on epoch=269
03/14/2022 14:56:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.064216 on epoch=274
03/14/2022 14:56:08 - INFO - __main__ - Global step 550 Train loss 0.014229 Classification-F1 0.6559139784946236 on epoch=274
03/14/2022 14:56:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.047061 on epoch=279
03/14/2022 14:56:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000380 on epoch=284
03/14/2022 14:56:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000411 on epoch=289
03/14/2022 14:56:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000657 on epoch=294
03/14/2022 14:56:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000282 on epoch=299
03/14/2022 14:56:40 - INFO - __main__ - Global step 600 Train loss 0.009758 Classification-F1 0.6343434343434343 on epoch=299
03/14/2022 14:56:40 - INFO - __main__ - save last model!
03/14/2022 14:56:47 - INFO - __main__ - Loading checkpoint on the fly
03/14/2022 14:56:48 - INFO - __main__ - Start tokenizing ... 7600 instances
03/14/2022 14:56:48 - INFO - __main__ - Printing 3 examples
03/14/2022 14:56:48 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/14/2022 14:56:48 - INFO - __main__ - ['negative']
03/14/2022 14:56:48 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/14/2022 14:56:48 - INFO - __main__ - ['negative']
03/14/2022 14:56:48 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/14/2022 14:56:48 - INFO - __main__ - ['negative']
03/14/2022 14:56:48 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/14/2022 14:56:54 - INFO - __main__ - Tokenizing Output ...
03/14/2022 14:57:02 - INFO - __main__ - Loaded 7600 examples from test data
03/14/2022 14:59:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-random/singletask-yelp_polarity/yelp_polarity_16_87_0.0001_8_predictions.txt
03/14/2022 14:59:45 - INFO - __main__ - Classification-F1 on test data: 0.2388
03/14/2022 14:59:46 - INFO - __main__ - prefix=yelp_polarity_16_87, lr=0.0001, bsz=8, dev_performance=0.6559139784946236, test_performance=0.23883116731794998
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (17115): No such process
