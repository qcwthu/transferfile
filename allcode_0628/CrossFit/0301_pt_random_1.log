nohup: ignoring input
Task: break-QDMR, Checkpoint: None, Identifier: T5-large
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_hps_singletask_ddp_prompt.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29544
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_fmmruanp/none_topy7rtw
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29544
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_fmmruanp/none_topy7rtw/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_fmmruanp/none_topy7rtw/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/01/2022 15:17:34 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 15:17:34 - INFO - __main__ - models/T5-large/singletask-break-QDMR
03/01/2022 15:17:34 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 15:17:34 - INFO - __main__ - models/T5-large/singletask-break-QDMR
03/01/2022 15:17:34 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 15:17:34 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 15:17:34 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/01/2022 15:17:34 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/01/2022 15:17:34 - INFO - __main__ - args.device: cuda:0
03/01/2022 15:17:34 - INFO - __main__ - Using 2 gpus
03/01/2022 15:17:34 - INFO - __main__ - args.device: cuda:1
03/01/2022 15:17:34 - INFO - __main__ - Using 2 gpus
03/01/2022 15:17:34 - INFO - __main__ - Fine-tuning the following samples: ['break-QDMR_32_100', 'break-QDMR_32_13', 'break-QDMR_32_21', 'break-QDMR_32_42', 'break-QDMR_32_87']
03/01/2022 15:17:34 - INFO - __main__ - Fine-tuning the following samples: ['break-QDMR_32_100', 'break-QDMR_32_13', 'break-QDMR_32_21', 'break-QDMR_32_42', 'break-QDMR_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/01/2022 15:17:39 - INFO - __main__ - Running ... prefix=break-QDMR_32_100, lr=0.5, bsz=8 ...
03/01/2022 15:17:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 15:17:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 15:17:40 - INFO - __main__ - Printing 3 examples
03/01/2022 15:17:40 - INFO - __main__ - Printing 3 examples
03/01/2022 15:17:40 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/01/2022 15:17:40 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/01/2022 15:17:40 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/01/2022 15:17:40 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/01/2022 15:17:40 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/01/2022 15:17:40 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/01/2022 15:17:40 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/01/2022 15:17:40 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/01/2022 15:17:40 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/01/2022 15:17:40 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/01/2022 15:17:40 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/01/2022 15:17:40 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/01/2022 15:17:40 - INFO - __main__ - Tokenizing Input ...
03/01/2022 15:17:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 15:17:40 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:17:40 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:17:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 15:17:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 15:17:40 - INFO - __main__ - Printing 3 examples
03/01/2022 15:17:40 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/01/2022 15:17:40 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/01/2022 15:17:40 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/01/2022 15:17:40 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/01/2022 15:17:40 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/01/2022 15:17:40 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/01/2022 15:17:40 - INFO - __main__ - Tokenizing Input ...
03/01/2022 15:17:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 15:17:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 15:17:40 - INFO - __main__ - Printing 3 examples
03/01/2022 15:17:40 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/01/2022 15:17:40 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/01/2022 15:17:40 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/01/2022 15:17:40 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/01/2022 15:17:40 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/01/2022 15:17:40 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/01/2022 15:17:40 - INFO - __main__ - Tokenizing Input ...
03/01/2022 15:17:40 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:17:40 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:17:40 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 15:17:40 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 15:19:01 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 15:19:01 - INFO - __main__ - task name: break-QDMR
03/01/2022 15:19:01 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 15:19:01 - INFO - __main__ - task name: break-QDMR
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446), (3702, 272314), (6894, 155616), (3001, 351252), (499, 1766405), (3731, 203441), (6478, 174299), (1287, 709613), (4382, 247856), (851, 1026262), (5946, 169484), (1409, 319874), (3527, 259912), (6373, 162161), (1640, 475640), (801, 1149556), (357, 1674058), (2402, 404464), (2536, 323855), (887, 1021213), (2386, 396013), (1103, 849279), (4237, 246742), (3032, 335423), (2922, 206677), (3889, 257596), (1683, 558497), (4149, 253554), (3634, 263727), (1955, 437347), (2891, 347678), (770, 1128621), (6298, 154205), (1904, 528496), (5956, 175083), (2943, 377167), (1220, 555589), (5103, 202299), (4313, 246924), (2734, 344156), (5735, 168028), (2404, 417090), (3626, 290198), (605, 1425064), (2324, 403572), (235, 2453612), (2733, 298680), (4166, 233368), (2422, 345969), (676, 1204693), (2272, 432795), (4569, 164817), (3285, 299538), (2136, 430918), (5166, 187635), (2040, 237038), (5037, 203882), (1556, 625505), (2836, 350538), (2079, 374647)]
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446), (3702, 272314), (6894, 155616), (3001, 351252), (499, 1766405), (3731, 203441), (6478, 174299), (1287, 709613), (4382, 247856), (851, 1026262), (5946, 169484), (1409, 319874), (3527, 259912), (6373, 162161), (1640, 475640), (801, 1149556), (357, 1674058), (2402, 404464), (2536, 323855), (887, 1021213), (2386, 396013), (1103, 849279), (4237, 246742), (3032, 335423), (2922, 206677), (3889, 257596), (1683, 558497), (4149, 253554), (3634, 263727), (1955, 437347), (2891, 347678), (770, 1128621), (6298, 154205), (1904, 528496), (5956, 175083), (2943, 377167), (1220, 555589), (5103, 202299), (4313, 246924), (2734, 344156), (5735, 168028), (2404, 417090), (3626, 290198), (605, 1425064), (2324, 403572), (235, 2453612), (2733, 298680), (4166, 233368), (2422, 345969), (676, 1204693), (2272, 432795), (4569, 164817), (3285, 299538), (2136, 430918), (5166, 187635), (2040, 237038), (5037, 203882), (1556, 625505), (2836, 350538), (2079, 374647)]
03/01/2022 15:19:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 15:19:02 - INFO - __main__ - Starting training!
03/01/2022 15:19:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 15:19:02 - INFO - __main__ - Starting training!
03/01/2022 15:19:05 - INFO - __main__ - Step 10 Global step 10 Train loss 3.26 on epoch=4
03/01/2022 15:19:07 - INFO - __main__ - Step 20 Global step 20 Train loss 2.21 on epoch=9
03/01/2022 15:19:09 - INFO - __main__ - Step 30 Global step 30 Train loss 1.68 on epoch=14
03/01/2022 15:19:11 - INFO - __main__ - Step 40 Global step 40 Train loss 1.50 on epoch=19
03/01/2022 15:19:13 - INFO - __main__ - Step 50 Global step 50 Train loss 1.36 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/01/2022 15:19:32 - INFO - __main__ - Global step 50 Train loss 2.00 EM 0.0 on epoch=24
03/01/2022 15:19:32 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 15:19:35 - INFO - __main__ - Step 60 Global step 60 Train loss 1.30 on epoch=29
03/01/2022 15:19:37 - INFO - __main__ - Step 70 Global step 70 Train loss 1.17 on epoch=34
03/01/2022 15:19:39 - INFO - __main__ - Step 80 Global step 80 Train loss 1.11 on epoch=39
03/01/2022 15:19:41 - INFO - __main__ - Step 90 Global step 90 Train loss 1.07 on epoch=44
03/01/2022 15:19:43 - INFO - __main__ - Step 100 Global step 100 Train loss 1.06 on epoch=49
03/01/2022 15:19:56 - INFO - __main__ - Global step 100 Train loss 1.14 EM 0.0 on epoch=49
03/01/2022 15:19:59 - INFO - __main__ - Step 110 Global step 110 Train loss 1.02 on epoch=54
03/01/2022 15:20:01 - INFO - __main__ - Step 120 Global step 120 Train loss 1.02 on epoch=59
03/01/2022 15:20:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.98 on epoch=64
03/01/2022 15:20:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.95 on epoch=69
03/01/2022 15:20:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=74
03/01/2022 15:20:13 - INFO - __main__ - Global step 150 Train loss 0.97 EM 0.0 on epoch=74
03/01/2022 15:20:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.86 on epoch=79
03/01/2022 15:20:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.87 on epoch=84
03/01/2022 15:20:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.84 on epoch=89
03/01/2022 15:20:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.82 on epoch=94
03/01/2022 15:20:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.83 on epoch=99
03/01/2022 15:20:29 - INFO - __main__ - Global step 200 Train loss 0.84 EM 0.09375 on epoch=99
03/01/2022 15:20:29 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.09375 on epoch=99, global_step=200
03/01/2022 15:20:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.79 on epoch=104
03/01/2022 15:20:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=109
03/01/2022 15:20:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.79 on epoch=114
03/01/2022 15:20:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.74 on epoch=119
03/01/2022 15:20:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.77 on epoch=124
03/01/2022 15:20:45 - INFO - __main__ - Global step 250 Train loss 0.78 EM 0.0625 on epoch=124
03/01/2022 15:20:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.76 on epoch=129
03/01/2022 15:20:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.73 on epoch=134
03/01/2022 15:20:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.71 on epoch=139
03/01/2022 15:20:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.72 on epoch=144
03/01/2022 15:20:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.71 on epoch=149
03/01/2022 15:21:01 - INFO - __main__ - Global step 300 Train loss 0.73 EM 0.0 on epoch=149
03/01/2022 15:21:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.69 on epoch=154
03/01/2022 15:21:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.67 on epoch=159
03/01/2022 15:21:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.67 on epoch=164
03/01/2022 15:21:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.67 on epoch=169
03/01/2022 15:21:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.67 on epoch=174
03/01/2022 15:21:17 - INFO - __main__ - Global step 350 Train loss 0.67 EM 0.03125 on epoch=174
03/01/2022 15:21:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.63 on epoch=179
03/01/2022 15:21:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.66 on epoch=184
03/01/2022 15:21:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.64 on epoch=189
03/01/2022 15:21:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.62 on epoch=194
03/01/2022 15:21:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=199
03/01/2022 15:21:32 - INFO - __main__ - Global step 400 Train loss 0.63 EM 0.0 on epoch=199
03/01/2022 15:21:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.60 on epoch=204
03/01/2022 15:21:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.56 on epoch=209
03/01/2022 15:21:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=214
03/01/2022 15:21:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.57 on epoch=219
03/01/2022 15:21:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.59 on epoch=224
03/01/2022 15:21:48 - INFO - __main__ - Global step 450 Train loss 0.58 EM 0.0625 on epoch=224
03/01/2022 15:21:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.57 on epoch=229
03/01/2022 15:21:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.56 on epoch=234
03/01/2022 15:21:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.54 on epoch=239
03/01/2022 15:21:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.53 on epoch=244
03/01/2022 15:21:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.55 on epoch=249
03/01/2022 15:22:04 - INFO - __main__ - Global step 500 Train loss 0.55 EM 0.0625 on epoch=249
03/01/2022 15:22:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=254
03/01/2022 15:22:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.53 on epoch=259
03/01/2022 15:22:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.51 on epoch=264
03/01/2022 15:22:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.51 on epoch=269
03/01/2022 15:22:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=274
03/01/2022 15:22:20 - INFO - __main__ - Global step 550 Train loss 0.52 EM 0.125 on epoch=274
03/01/2022 15:22:20 - INFO - __main__ - Saving model with best EM: 0.09375 -> 0.125 on epoch=274, global_step=550
03/01/2022 15:22:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.52 on epoch=279
03/01/2022 15:22:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=284
03/01/2022 15:22:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=289
03/01/2022 15:22:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=294
03/01/2022 15:22:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=299
03/01/2022 15:22:36 - INFO - __main__ - Global step 600 Train loss 0.50 EM 0.03125 on epoch=299
03/01/2022 15:22:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=304
03/01/2022 15:22:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=309
03/01/2022 15:22:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=314
03/01/2022 15:22:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=319
03/01/2022 15:22:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=324
03/01/2022 15:22:51 - INFO - __main__ - Global step 650 Train loss 0.47 EM 0.09375 on epoch=324
03/01/2022 15:22:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=329
03/01/2022 15:22:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=334
03/01/2022 15:22:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=339
03/01/2022 15:22:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=344
03/01/2022 15:23:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=349
03/01/2022 15:23:08 - INFO - __main__ - Global step 700 Train loss 0.45 EM 0.0625 on epoch=349
03/01/2022 15:23:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=354
03/01/2022 15:23:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=359
03/01/2022 15:23:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=364
03/01/2022 15:23:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=369
03/01/2022 15:23:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=374
03/01/2022 15:23:25 - INFO - __main__ - Global step 750 Train loss 0.42 EM 0.0625 on epoch=374
03/01/2022 15:23:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=379
03/01/2022 15:23:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=384
03/01/2022 15:23:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=389
03/01/2022 15:23:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=394
03/01/2022 15:23:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=399
03/01/2022 15:23:41 - INFO - __main__ - Global step 800 Train loss 0.41 EM 0.0625 on epoch=399
03/01/2022 15:23:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=404
03/01/2022 15:23:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=409
03/01/2022 15:23:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=414
03/01/2022 15:23:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=419
03/01/2022 15:23:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=424
03/01/2022 15:23:56 - INFO - __main__ - Global step 850 Train loss 0.40 EM 0.09375 on epoch=424
03/01/2022 15:23:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=429
03/01/2022 15:24:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=434
03/01/2022 15:24:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=439
03/01/2022 15:24:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=444
03/01/2022 15:24:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=449
03/01/2022 15:24:12 - INFO - __main__ - Global step 900 Train loss 0.39 EM 0.09375 on epoch=449
03/01/2022 15:24:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=454
03/01/2022 15:24:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=459
03/01/2022 15:24:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=464
03/01/2022 15:24:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=469
03/01/2022 15:24:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=474
03/01/2022 15:24:28 - INFO - __main__ - Global step 950 Train loss 0.36 EM 0.125 on epoch=474
03/01/2022 15:24:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=479
03/01/2022 15:24:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=484
03/01/2022 15:24:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=489
03/01/2022 15:24:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=494
03/01/2022 15:24:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=499
03/01/2022 15:24:44 - INFO - __main__ - Global step 1000 Train loss 0.36 EM 0.09375 on epoch=499
03/01/2022 15:24:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=504
03/01/2022 15:24:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=509
03/01/2022 15:24:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=514
03/01/2022 15:24:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=519
03/01/2022 15:24:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=524
03/01/2022 15:24:59 - INFO - __main__ - Global step 1050 Train loss 0.34 EM 0.09375 on epoch=524
03/01/2022 15:25:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=529
03/01/2022 15:25:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=534
03/01/2022 15:25:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=539
03/01/2022 15:25:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.33 on epoch=544
03/01/2022 15:25:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=549
03/01/2022 15:25:15 - INFO - __main__ - Global step 1100 Train loss 0.33 EM 0.09375 on epoch=549
03/01/2022 15:25:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=554
03/01/2022 15:25:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=559
03/01/2022 15:25:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=564
03/01/2022 15:25:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=569
03/01/2022 15:25:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.31 on epoch=574
03/01/2022 15:25:32 - INFO - __main__ - Global step 1150 Train loss 0.32 EM 0.0625 on epoch=574
03/01/2022 15:25:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=579
03/01/2022 15:25:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.32 on epoch=584
03/01/2022 15:25:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.31 on epoch=589
03/01/2022 15:25:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.29 on epoch=594
03/01/2022 15:25:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=599
03/01/2022 15:25:51 - INFO - __main__ - Global step 1200 Train loss 0.31 EM 0.09375 on epoch=599
03/01/2022 15:25:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=604
03/01/2022 15:25:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=609
03/01/2022 15:25:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=614
03/01/2022 15:26:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=619
03/01/2022 15:26:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.30 on epoch=624
03/01/2022 15:26:08 - INFO - __main__ - Global step 1250 Train loss 0.29 EM 0.09375 on epoch=624
03/01/2022 15:26:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=629
03/01/2022 15:26:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.29 on epoch=634
03/01/2022 15:26:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.29 on epoch=639
03/01/2022 15:26:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=644
03/01/2022 15:26:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.28 on epoch=649
03/01/2022 15:26:24 - INFO - __main__ - Global step 1300 Train loss 0.28 EM 0.09375 on epoch=649
03/01/2022 15:26:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.29 on epoch=654
03/01/2022 15:26:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=659
03/01/2022 15:26:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.29 on epoch=664
03/01/2022 15:26:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.27 on epoch=669
03/01/2022 15:26:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.31 on epoch=674
03/01/2022 15:26:41 - INFO - __main__ - Global step 1350 Train loss 0.29 EM 0.09375 on epoch=674
03/01/2022 15:26:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=679
03/01/2022 15:26:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.28 on epoch=684
03/01/2022 15:26:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=689
03/01/2022 15:26:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.28 on epoch=694
03/01/2022 15:26:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.28 on epoch=699
03/01/2022 15:27:03 - INFO - __main__ - Global step 1400 Train loss 0.27 EM 0.09375 on epoch=699
03/01/2022 15:27:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=704
03/01/2022 15:27:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.28 on epoch=709
03/01/2022 15:27:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=714
03/01/2022 15:27:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=719
03/01/2022 15:27:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.28 on epoch=724
03/01/2022 15:27:20 - INFO - __main__ - Global step 1450 Train loss 0.28 EM 0.09375 on epoch=724
03/01/2022 15:27:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.26 on epoch=729
03/01/2022 15:27:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=734
03/01/2022 15:27:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=739
03/01/2022 15:27:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.26 on epoch=744
03/01/2022 15:27:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=749
03/01/2022 15:27:36 - INFO - __main__ - Global step 1500 Train loss 0.26 EM 0.0625 on epoch=749
03/01/2022 15:27:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=754
03/01/2022 15:27:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=759
03/01/2022 15:27:42 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=764
03/01/2022 15:27:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=769
03/01/2022 15:27:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=774
03/01/2022 15:27:59 - INFO - __main__ - Global step 1550 Train loss 0.26 EM 0.0625 on epoch=774
03/01/2022 15:28:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=779
03/01/2022 15:28:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.26 on epoch=784
03/01/2022 15:28:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=789
03/01/2022 15:28:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=794
03/01/2022 15:28:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=799
03/01/2022 15:28:20 - INFO - __main__ - Global step 1600 Train loss 0.25 EM 0.0625 on epoch=799
03/01/2022 15:28:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=804
03/01/2022 15:28:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=809
03/01/2022 15:28:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=814
03/01/2022 15:28:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=819
03/01/2022 15:28:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=824
03/01/2022 15:28:43 - INFO - __main__ - Global step 1650 Train loss 0.24 EM 0.0625 on epoch=824
03/01/2022 15:28:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.24 on epoch=829
03/01/2022 15:28:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=834
03/01/2022 15:28:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=839
03/01/2022 15:28:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=844
03/01/2022 15:28:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=849
03/01/2022 15:29:00 - INFO - __main__ - Global step 1700 Train loss 0.24 EM 0.09375 on epoch=849
03/01/2022 15:29:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=854
03/01/2022 15:29:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=859
03/01/2022 15:29:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=864
03/01/2022 15:29:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=869
03/01/2022 15:29:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=874
03/01/2022 15:29:22 - INFO - __main__ - Global step 1750 Train loss 0.23 EM 0.0625 on epoch=874
03/01/2022 15:29:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=879
03/01/2022 15:29:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=884
03/01/2022 15:29:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=889
03/01/2022 15:29:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=894
03/01/2022 15:29:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=899
03/01/2022 15:29:39 - INFO - __main__ - Global step 1800 Train loss 0.22 EM 0.09375 on epoch=899
03/01/2022 15:29:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=904
03/01/2022 15:29:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=909
03/01/2022 15:29:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.22 on epoch=914
03/01/2022 15:29:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=919
03/01/2022 15:29:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.22 on epoch=924
03/01/2022 15:29:58 - INFO - __main__ - Global step 1850 Train loss 0.22 EM 0.09375 on epoch=924
03/01/2022 15:30:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=929
03/01/2022 15:30:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.23 on epoch=934
03/01/2022 15:30:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=939
03/01/2022 15:30:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=944
03/01/2022 15:30:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.23 on epoch=949
03/01/2022 15:30:16 - INFO - __main__ - Global step 1900 Train loss 0.23 EM 0.0625 on epoch=949
03/01/2022 15:30:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=954
03/01/2022 15:30:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.22 on epoch=959
03/01/2022 15:30:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=964
03/01/2022 15:30:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=969
03/01/2022 15:30:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=974
03/01/2022 15:30:37 - INFO - __main__ - Global step 1950 Train loss 0.21 EM 0.09375 on epoch=974
03/01/2022 15:30:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=979
03/01/2022 15:30:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=984
03/01/2022 15:30:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=989
03/01/2022 15:30:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=994
03/01/2022 15:30:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=999
03/01/2022 15:30:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 15:30:49 - INFO - __main__ - Printing 3 examples
03/01/2022 15:30:49 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/01/2022 15:30:49 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/01/2022 15:30:49 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/01/2022 15:30:49 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/01/2022 15:30:49 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/01/2022 15:30:49 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/01/2022 15:30:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 15:30:49 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:30:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 15:30:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 15:30:49 - INFO - __main__ - Printing 3 examples
03/01/2022 15:30:49 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/01/2022 15:30:49 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/01/2022 15:30:49 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/01/2022 15:30:49 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/01/2022 15:30:49 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/01/2022 15:30:49 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/01/2022 15:30:49 - INFO - __main__ - Tokenizing Input ...
03/01/2022 15:30:49 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:30:50 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 15:30:55 - INFO - __main__ - Global step 2000 Train loss 0.21 EM 0.0625 on epoch=999
03/01/2022 15:30:55 - INFO - __main__ - save last model!
03/01/2022 15:30:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 15:30:55 - INFO - __main__ - Start tokenizing ... 7760 instances
03/01/2022 15:30:55 - INFO - __main__ - Printing 3 examples
03/01/2022 15:30:55 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/01/2022 15:30:55 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/01/2022 15:30:55 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/01/2022 15:30:55 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/01/2022 15:30:55 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/01/2022 15:30:55 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/01/2022 15:30:55 - INFO - __main__ - Tokenizing Input ...
03/01/2022 15:30:58 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:31:03 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 15:31:03 - INFO - __main__ - task name: break-QDMR
initialize from c4
[(6579, 166537), (5880, 173333), (2252, 353035), (6881, 160252), (4773, 218071), (6404, 160401), (3371, 234531), (4187, 259804), (1925, 417452), (968, 645839), (2729, 183765), (5538, 189511), (987, 914447), (144, 1635131), (1079, 796417), (1653, 586845), (1731, 565272), (3424, 221936), (5680, 157495), (731, 1250799), (4298, 243616), (3238, 245783), (6415, 157560), (2406, 200208), (5775, 177401), (2721, 367851), (6309, 169174), (135, 7037199), (1214, 767509), (5962, 174066), (2867, 348102), (3209, 276527), (1189, 784250), (891, 319194), (5015, 214805), (1701, 571183), (3845, 206498), (26, 28035103), (2809, 352164), (5121, 187048), (1892, 508575), (3574, 184323), (996, 821524), (3263, 314166), (6363, 154270), (90, 459181), (4226, 250683), (1751, 560750), (5058, 173227), (5205, 176939), (7, 158399866), (6008, 156882), (3474, 290209), (3351, 190868), (225, 4092648), (1142, 782542), (154, 259007), (3278, 305705), (2637, 387504), (532, 1388213), (2597, 385302), (6237, 164778), (851, 1026262), (793, 959415), (2719, 191820), (788, 1153138), (4209, 175746), (1393, 703212), (1438, 688034), (5340, 197066), (3995, 170269), (1376, 549699), (2909, 350152), (5773, 177903), (6551, 154998), (4799, 221150), (2335, 431458), (6082, 173261), (1824, 455362), (3242, 301888), (4627, 234647), (3797, 250903), (4027, 213331), (6464, 181012), (5210, 207673), (1137, 734426), (2765, 373166), (2451, 408517), (708, 1240422), (3108, 278320), (243, 3854645), (4830, 159383), (5630, 169060), (2134, 401363), (4490, 159360), (2342, 415802), (80, 11392202), (758, 1131386), (652, 1360888)]
03/01/2022 15:31:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 15:31:04 - INFO - __main__ - Starting training!
03/01/2022 15:31:06 - INFO - __main__ - Loaded 7760 examples from test data
[E ProcessGroupNCCL.cpp:566] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(OpType=BROADCAST, Timeout(ms)=1800000) ran for 1800340 milliseconds before timing out.
03/01/2022 16:03:55 - INFO - __main__ - Saved prediction in models/T5-large/singletask-break-QDMR/break-QDMR_32_100_0.5_8_predictions.txt
03/01/2022 16:03:56 - INFO - __main__ - EM on test data: 0.0293
03/01/2022 16:03:56 - INFO - __main__ - prefix=break-QDMR_32_100, lr=0.5, bsz=8, dev_performance=0.125, test_performance=0.02925257731958763
03/01/2022 16:03:56 - INFO - __main__ - Running ... prefix=break-QDMR_32_100, lr=0.4, bsz=8 ...
03/01/2022 16:03:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:03:57 - INFO - __main__ - Printing 3 examples
03/01/2022 16:03:57 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/01/2022 16:03:57 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/01/2022 16:03:57 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/01/2022 16:03:57 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/01/2022 16:03:57 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/01/2022 16:03:57 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/01/2022 16:03:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 16:03:57 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:03:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 16:03:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:03:57 - INFO - __main__ - Printing 3 examples
03/01/2022 16:03:57 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/01/2022 16:03:57 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/01/2022 16:03:57 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/01/2022 16:03:57 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/01/2022 16:03:57 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/01/2022 16:03:57 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/01/2022 16:03:57 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:03:57 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:03:57 - INFO - __main__ - Loaded 32 examples from dev data
[E ProcessGroupNCCL.cpp:325] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. To avoid this inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(OpType=BROADCAST, Timeout(ms)=1800000) ran for 1800340 milliseconds before timing out.
03/01/2022 16:04:09 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 16:04:09 - INFO - __main__ - task name: break-QDMR
initialize from c4
[(6579, 166537), (5880, 173333), (2252, 353035), (6881, 160252), (4773, 218071), (6404, 160401), (3371, 234531), (4187, 259804), (1925, 417452), (968, 645839), (2729, 183765), (5538, 189511), (987, 914447), (144, 1635131), (1079, 796417), (1653, 586845), (1731, 565272), (3424, 221936), (5680, 157495), (731, 1250799), (4298, 243616), (3238, 245783), (6415, 157560), (2406, 200208), (5775, 177401), (2721, 367851), (6309, 169174), (135, 7037199), (1214, 767509), (5962, 174066), (2867, 348102), (3209, 276527), (1189, 784250), (891, 319194), (5015, 214805), (1701, 571183), (3845, 206498), (26, 28035103), (2809, 352164), (5121, 187048), (1892, 508575), (3574, 184323), (996, 821524), (3263, 314166), (6363, 154270), (90, 459181), (4226, 250683), (1751, 560750), (5058, 173227), (5205, 176939), (7, 158399866), (6008, 156882), (3474, 290209), (3351, 190868), (225, 4092648), (1142, 782542), (154, 259007), (3278, 305705), (2637, 387504), (532, 1388213), (2597, 385302), (6237, 164778), (851, 1026262), (793, 959415), (2719, 191820), (788, 1153138), (4209, 175746), (1393, 703212), (1438, 688034), (5340, 197066), (3995, 170269), (1376, 549699), (2909, 350152), (5773, 177903), (6551, 154998), (4799, 221150), (2335, 431458), (6082, 173261), (1824, 455362), (3242, 301888), (4627, 234647), (3797, 250903), (4027, 213331), (6464, 181012), (5210, 207673), (1137, 734426), (2765, 373166), (2451, 408517), (708, 1240422), (3108, 278320), (243, 3854645), (4830, 159383), (5630, 169060), (2134, 401363), (4490, 159360), (2342, 415802), (80, 11392202), (758, 1131386), (652, 1360888)]
03/01/2022 16:04:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 16:04:10 - INFO - __main__ - Starting training!
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
terminate called after throwing an instance of 'c10::CUDAError'
  what():  CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Exception raised from query at /pytorch/aten/src/ATen/cuda/CUDAEvent.h:95 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f925ece6a22 in /opt/conda/envs/meta/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x132 (0x7f93040d20e2 in /opt/conda/envs/meta/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x50 (0x7f93040d3d40 in /opt/conda/envs/meta/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #3: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x11c (0x7f93040d475c in /opt/conda/envs/meta/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #4: <unknown function> + 0xbd6df (0x7f9304c7c6df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x76db (0x7f93152026db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x3f (0x7f9314f2ba3f in /lib/x86_64-linux-gnu/libc.so.6)

ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 1 (pid: 2129) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29544
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_fmmruanp/none_topy7rtw/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_fmmruanp/none_topy7rtw/attempt_1/1/error.json
Output directory () already exists and is not empty.
03/01/2022 16:07:35 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 16:07:35 - INFO - __main__ - models/T5-large/singletask-break-QDMR
03/01/2022 16:07:35 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 16:07:35 - INFO - __main__ - models/T5-large/singletask-break-QDMR
03/01/2022 16:07:35 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 16:07:35 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 16:07:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:07:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:07:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:07:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:08:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:08:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:08:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:08:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:08:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:08:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:08:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:08:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:08:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:08:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:08:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:08:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:09:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:09:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:09:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:09:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:09:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:09:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:09:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:09:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:09:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:09:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:09:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:09:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:10:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:10:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:10:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:10:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:10:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:10:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:10:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:10:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:10:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:10:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:10:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:10:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:11:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:11:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:11:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:11:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:11:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:11:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:11:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:11:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:11:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:11:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:11:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:11:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:12:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:12:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:12:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:12:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:12:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:12:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:12:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:12:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:12:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:12:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:12:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:12:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:13:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:13:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:13:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:13:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:13:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:13:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:13:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:13:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:13:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:13:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:13:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:13:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:14:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:14:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:14:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:14:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:14:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:14:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:14:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:14:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:14:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:14:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:14:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:14:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:15:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:15:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:15:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:15:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:15:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:15:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:15:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:15:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:15:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:15:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:15:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:15:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:16:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:16:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:16:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:16:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:16:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:16:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:16:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:16:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:16:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:16:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:16:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:16:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:17:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:17:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:17:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:17:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:17:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:17:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:17:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:17:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:17:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:17:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:17:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:17:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:18:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:18:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:18:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:18:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:18:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:18:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:18:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:18:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:18:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:18:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:18:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:18:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:19:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:19:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:19:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:19:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:19:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:19:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:19:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:19:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:19:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:19:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:19:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:19:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:20:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:20:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:20:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:20:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:20:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:20:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:20:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:20:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:20:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:20:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:20:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:20:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:21:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:21:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:21:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:21:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:21:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:21:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:21:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:21:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:21:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:21:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:21:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:21:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:22:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:22:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:22:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:22:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:22:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:22:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:22:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:22:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:22:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:22:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:22:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:22:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:23:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:23:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:23:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:23:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:23:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:23:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:23:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:23:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:23:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:23:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:23:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:23:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:24:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:24:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:24:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:24:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:24:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:24:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:24:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:24:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:24:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:24:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:24:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:24:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:25:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:25:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:25:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:25:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:25:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:25:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:25:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:25:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:25:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:25:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:25:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:25:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:26:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:26:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:26:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:26:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:26:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:26:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:26:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:26:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:26:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:26:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:26:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:26:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:27:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:27:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:27:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:27:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:27:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:27:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:27:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:27:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:27:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:27:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:27:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:27:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:28:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:28:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:28:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:28:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:28:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:28:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:28:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:28:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:28:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:28:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:28:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:28:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:29:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:29:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:29:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:29:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:29:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:29:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:29:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:29:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:29:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:29:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:29:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:29:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:30:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:30:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:30:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:30:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:30:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:30:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:30:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:30:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:30:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:30:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:30:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:30:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:31:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:31:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:31:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:31:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:31:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:31:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:31:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:31:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:31:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:31:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:31:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:31:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:32:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:32:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:32:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:32:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:32:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:32:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:32:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:32:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:32:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:32:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:32:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:32:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:33:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:33:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:33:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:33:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:33:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:33:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:33:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:33:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:33:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:33:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:33:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:33:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:34:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:34:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:34:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:34:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:34:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:34:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:34:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:34:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:34:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:34:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:34:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:34:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:35:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:35:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:35:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:35:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:35:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:35:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:35:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:35:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:35:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:35:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:35:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:35:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:36:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:36:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:36:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:36:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:36:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:36:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:36:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:36:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:36:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:36:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:36:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:36:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:37:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:37:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:37:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:37:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:37:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 16:37:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 156, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2296) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29544
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_fmmruanp/none_topy7rtw/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_fmmruanp/none_topy7rtw/attempt_2/1/error.json
Output directory () already exists and is not empty.
03/01/2022 16:37:42 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 16:37:42 - INFO - __main__ - models/T5-large/singletask-break-QDMR
03/01/2022 16:37:42 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 16:37:42 - INFO - __main__ - models/T5-large/singletask-break-QDMR
03/01/2022 16:37:42 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 16:37:42 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 16:37:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:37:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:38:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:38:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:38:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:38:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:38:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:38:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:38:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:38:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:38:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:38:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:38:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:38:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:39:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:39:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:39:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:39:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:39:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:39:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:39:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:39:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:39:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:39:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:39:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:39:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:40:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:40:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:40:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:40:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:40:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:40:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:40:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:40:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:40:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:40:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:40:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:40:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:41:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:41:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:41:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:41:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:41:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:41:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:41:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:41:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:41:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:41:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:41:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:41:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:42:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:42:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:42:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:42:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:42:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:42:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:42:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:42:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:42:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:42:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:42:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:42:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:43:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:43:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:43:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:43:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:43:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:43:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:43:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:43:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:43:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:43:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:43:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:43:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:44:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:44:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:44:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:44:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:44:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:44:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:44:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:44:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:44:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:44:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:44:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:44:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:45:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:45:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:45:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:45:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:45:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:45:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:45:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:45:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:45:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:45:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:45:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:45:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:46:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:46:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:46:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:46:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:46:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:46:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:46:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:46:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:46:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:46:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:46:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:46:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:47:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:47:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:47:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:47:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:47:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:47:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:47:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:47:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:47:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:47:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:47:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:47:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:48:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:48:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:48:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:48:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:48:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:48:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:48:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:48:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:48:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:48:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:48:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:48:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:49:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:49:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:49:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:49:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:49:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:49:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:49:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:49:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:49:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:49:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:49:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:49:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:50:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:50:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:50:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:50:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:50:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:50:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:50:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:50:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:50:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:50:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:50:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:50:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:51:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:51:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:51:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:51:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:51:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:51:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:51:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:51:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:51:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:51:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:51:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:51:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:52:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:52:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:52:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:52:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:52:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:52:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:52:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:52:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:52:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:52:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:52:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:52:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:53:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:53:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:53:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:53:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:53:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:53:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:53:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:53:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:53:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:53:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:53:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:53:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:54:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:54:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:54:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:54:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:54:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:54:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:54:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:54:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:54:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:54:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:54:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:54:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:55:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:55:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:55:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:55:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:55:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:55:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:55:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:55:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:55:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:55:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:55:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:55:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:56:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:56:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:56:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:56:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:56:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:56:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:56:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:56:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:56:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:56:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:56:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:56:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:57:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:57:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:57:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:57:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:57:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:57:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:57:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:57:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:57:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:57:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:57:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:57:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:58:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:58:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:58:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:58:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:58:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:58:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:58:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:58:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:58:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:58:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:58:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:58:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:59:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:59:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:59:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:59:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:59:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:59:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:59:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:59:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:59:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:59:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:59:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 16:59:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:00:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:00:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:00:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:00:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:00:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:00:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:00:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:00:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:00:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:00:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:00:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:00:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:01:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:01:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:01:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:01:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:01:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:01:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:01:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:01:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:01:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:01:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:01:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:01:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:02:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:02:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:02:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:02:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:02:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:02:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:02:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:02:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:02:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:02:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:02:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:02:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:03:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:03:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:03:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:03:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:03:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:03:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:03:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:03:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:03:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:03:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:03:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:03:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:04:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:04:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:04:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:04:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:04:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:04:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:04:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:04:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:04:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:04:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:04:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:04:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:05:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:05:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:05:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:05:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:05:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:05:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:05:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:05:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:05:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:05:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:05:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:05:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:06:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:06:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:06:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:06:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:06:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:06:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:06:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:06:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:06:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:06:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:06:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:06:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:07:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:07:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:07:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:07:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:07:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:07:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:07:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 17:07:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 156, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2327) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29544
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_fmmruanp/none_topy7rtw/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_fmmruanp/none_topy7rtw/attempt_3/1/error.json
Output directory () already exists and is not empty.
03/01/2022 17:07:48 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 17:07:48 - INFO - __main__ - models/T5-large/singletask-break-QDMR
03/01/2022 17:07:48 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 17:07:48 - INFO - __main__ - models/T5-large/singletask-break-QDMR
03/01/2022 17:07:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 17:07:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 17:08:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:08:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:08:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:08:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:08:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:08:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:08:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:08:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:08:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:08:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:08:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:08:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:09:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:09:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:09:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:09:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:09:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:09:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:09:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:09:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:09:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:09:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:09:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:09:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:10:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:10:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:10:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:10:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:10:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:10:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:10:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:10:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:10:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:10:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:10:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:10:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:11:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:11:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:11:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:11:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:11:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:11:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:11:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:11:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:11:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:11:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:11:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:11:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:12:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:12:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:12:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:12:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:12:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:12:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:12:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:12:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:12:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:12:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:12:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:12:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:13:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:13:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:13:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:13:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:13:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:13:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:13:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:13:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:13:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:13:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:13:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:13:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:14:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:14:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:14:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:14:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:14:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:14:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:14:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:14:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:14:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:14:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:14:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:14:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:15:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:15:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:15:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:15:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:15:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:15:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:15:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:15:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:15:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:15:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:15:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:15:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:16:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:16:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:16:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:16:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:16:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:16:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:16:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:16:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:16:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:16:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:16:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:16:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:17:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:17:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:17:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:17:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:17:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:17:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:17:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:17:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:17:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:17:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:17:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:17:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:18:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:18:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:18:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:18:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:18:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:18:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:18:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:18:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:18:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:18:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:18:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:18:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:19:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:19:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:19:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:19:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:19:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:19:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:19:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:19:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:19:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:19:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:19:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:19:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:20:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:20:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:20:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:20:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:20:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:20:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:20:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:20:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:20:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:20:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:20:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:20:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:21:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:21:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:21:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:21:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:21:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:21:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:21:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:21:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:21:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:21:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:21:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:21:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:22:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:22:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:22:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:22:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:22:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:22:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:22:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:22:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:22:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:22:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:22:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:22:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:23:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:23:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:23:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:23:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:23:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:23:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:23:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:23:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:23:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:23:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:23:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:23:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:24:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:24:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:24:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:24:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:24:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:24:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:24:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:24:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:24:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:24:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:24:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:24:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:25:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:25:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:25:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:25:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:25:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:25:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:25:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:25:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:25:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:25:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:25:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:25:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:26:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:26:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:26:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:26:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:26:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:26:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:26:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:26:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:26:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:26:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:26:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:26:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:27:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:27:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:27:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:27:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:27:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:27:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:27:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:27:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:27:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:27:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:27:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:27:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:28:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:28:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:28:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:28:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:28:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:28:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:28:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:28:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:28:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:28:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:28:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:28:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:29:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:29:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:29:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:29:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:29:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:29:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:29:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:29:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:29:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:29:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:29:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:29:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:30:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:30:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:30:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:30:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:30:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:30:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:30:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:30:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:30:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:30:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:30:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:30:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:31:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:31:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:31:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:31:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:31:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:31:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:31:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:31:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:31:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:31:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:31:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:31:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:32:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:32:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:32:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:32:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:32:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:32:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:32:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:32:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:32:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:32:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:32:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:32:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:33:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:33:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:33:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:33:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:33:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:33:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:33:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:33:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:33:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:33:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:33:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:33:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:34:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:34:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:34:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:34:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:34:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:34:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:34:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:34:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:34:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:34:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:34:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:34:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:35:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:35:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:35:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:35:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:35:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:35:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:35:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:35:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:35:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:35:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:35:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:35:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:36:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:36:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:36:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:36:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:36:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:36:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:36:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:36:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:36:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:36:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:36:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:36:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:37:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:37:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:37:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:37:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:37:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:37:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:37:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:37:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:37:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 17:37:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 156, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2339) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003371238708496094 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "2339", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 8421, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "2340", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 8421, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 8421, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2339 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
********************************************
  tune_hps_singletask_ddp_prompt.py FAILED  
============================================
Root Cause:
[0]:
  time: 2022-03-01_17:37:53
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2339)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
============================================
Other Failures:
[1]:
  time: 2022-03-01_17:37:53
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2340)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
********************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (2364): No such process
Task: freebase_qa, Checkpoint: None, Identifier: T5-large
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_hps_singletask_ddp_prompt.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29544
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_mh8y1dsm/none_a_ex5mt4
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29544
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_mh8y1dsm/none_a_ex5mt4/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_mh8y1dsm/none_a_ex5mt4/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/01/2022 17:37:57 - INFO - __main__ - Namespace(task_dir='data/freebase_qa/', task_name='freebase_qa', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-freebase_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 17:37:57 - INFO - __main__ - models/T5-large/singletask-freebase_qa
03/01/2022 17:37:57 - INFO - __main__ - Namespace(task_dir='data/freebase_qa/', task_name='freebase_qa', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-freebase_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 17:37:57 - INFO - __main__ - models/T5-large/singletask-freebase_qa
03/01/2022 17:37:57 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 17:37:57 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 17:37:57 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/01/2022 17:37:57 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/01/2022 17:37:57 - INFO - __main__ - args.device: cuda:0
03/01/2022 17:37:57 - INFO - __main__ - args.device: cuda:1
03/01/2022 17:37:57 - INFO - __main__ - Using 2 gpus
03/01/2022 17:37:57 - INFO - __main__ - Using 2 gpus
03/01/2022 17:37:57 - INFO - __main__ - Fine-tuning the following samples: ['freebase_qa_32_100', 'freebase_qa_32_13', 'freebase_qa_32_21', 'freebase_qa_32_42', 'freebase_qa_32_87']
03/01/2022 17:37:57 - INFO - __main__ - Fine-tuning the following samples: ['freebase_qa_32_100', 'freebase_qa_32_13', 'freebase_qa_32_21', 'freebase_qa_32_42', 'freebase_qa_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/01/2022 17:38:02 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.5, bsz=8 ...
03/01/2022 17:38:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:38:03 - INFO - __main__ - Printing 3 examples
03/01/2022 17:38:03 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 17:38:03 - INFO - __main__ - ['orange is the new black']
03/01/2022 17:38:03 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 17:38:03 - INFO - __main__ - ['western australia']
03/01/2022 17:38:03 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 17:38:03 - INFO - __main__ - ['turkey']
03/01/2022 17:38:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 17:38:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:38:03 - INFO - __main__ - Printing 3 examples
03/01/2022 17:38:03 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 17:38:03 - INFO - __main__ - ['orange is the new black']
03/01/2022 17:38:03 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 17:38:03 - INFO - __main__ - ['western australia']
03/01/2022 17:38:03 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 17:38:03 - INFO - __main__ - ['turkey']
03/01/2022 17:38:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 17:38:03 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:38:03 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:38:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:38:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:38:03 - INFO - __main__ - Printing 3 examples
03/01/2022 17:38:03 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 17:38:03 - INFO - __main__ - ['benito mussolini']
03/01/2022 17:38:03 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 17:38:03 - INFO - __main__ - ['the kinks']
03/01/2022 17:38:03 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 17:38:03 - INFO - __main__ - ['saigon']
03/01/2022 17:38:03 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:38:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:38:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:38:03 - INFO - __main__ - Printing 3 examples
03/01/2022 17:38:03 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 17:38:03 - INFO - __main__ - ['benito mussolini']
03/01/2022 17:38:03 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 17:38:03 - INFO - __main__ - ['the kinks']
03/01/2022 17:38:03 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 17:38:03 - INFO - __main__ - ['saigon']
03/01/2022 17:38:03 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:38:03 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:38:03 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:38:03 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:38:03 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:38:18 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 17:38:18 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446), (3702, 272314), (6894, 155616), (3001, 351252), (499, 1766405), (3731, 203441), (6478, 174299), (1287, 709613), (4382, 247856), (851, 1026262), (5946, 169484), (1409, 319874), (3527, 259912), (6373, 162161), (1640, 475640), (801, 1149556), (357, 1674058), (2402, 404464), (2536, 323855), (887, 1021213), (2386, 396013), (1103, 849279), (4237, 246742), (3032, 335423), (2922, 206677), (3889, 257596), (1683, 558497), (4149, 253554), (3634, 263727), (1955, 437347), (2891, 347678), (770, 1128621), (6298, 154205), (1904, 528496), (5956, 175083), (2943, 377167), (1220, 555589), (5103, 202299), (4313, 246924), (2734, 344156), (5735, 168028), (2404, 417090), (3626, 290198), (605, 1425064), (2324, 403572), (235, 2453612), (2733, 298680), (4166, 233368), (2422, 345969), (676, 1204693), (2272, 432795), (4569, 164817), (3285, 299538), (2136, 430918), (5166, 187635), (2040, 237038), (5037, 203882), (1556, 625505), (2836, 350538), (2079, 374647)]
03/01/2022 17:38:18 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 17:38:18 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446), (3702, 272314), (6894, 155616), (3001, 351252), (499, 1766405), (3731, 203441), (6478, 174299), (1287, 709613), (4382, 247856), (851, 1026262), (5946, 169484), (1409, 319874), (3527, 259912), (6373, 162161), (1640, 475640), (801, 1149556), (357, 1674058), (2402, 404464), (2536, 323855), (887, 1021213), (2386, 396013), (1103, 849279), (4237, 246742), (3032, 335423), (2922, 206677), (3889, 257596), (1683, 558497), (4149, 253554), (3634, 263727), (1955, 437347), (2891, 347678), (770, 1128621), (6298, 154205), (1904, 528496), (5956, 175083), (2943, 377167), (1220, 555589), (5103, 202299), (4313, 246924), (2734, 344156), (5735, 168028), (2404, 417090), (3626, 290198), (605, 1425064), (2324, 403572), (235, 2453612), (2733, 298680), (4166, 233368), (2422, 345969), (676, 1204693), (2272, 432795), (4569, 164817), (3285, 299538), (2136, 430918), (5166, 187635), (2040, 237038), (5037, 203882), (1556, 625505), (2836, 350538), (2079, 374647)]
03/01/2022 17:38:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:38:19 - INFO - __main__ - Starting training!
03/01/2022 17:38:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:38:20 - INFO - __main__ - Starting training!
03/01/2022 17:38:24 - INFO - __main__ - Step 10 Global step 10 Train loss 4.85 on epoch=4
03/01/2022 17:38:26 - INFO - __main__ - Step 20 Global step 20 Train loss 4.03 on epoch=9
03/01/2022 17:38:28 - INFO - __main__ - Step 30 Global step 30 Train loss 3.30 on epoch=14
03/01/2022 17:38:30 - INFO - __main__ - Step 40 Global step 40 Train loss 2.94 on epoch=19
03/01/2022 17:38:32 - INFO - __main__ - Step 50 Global step 50 Train loss 2.80 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/01/2022 17:38:34 - INFO - __main__ - Global step 50 Train loss 3.58 EM 0.0 on epoch=24
03/01/2022 17:38:34 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 17:38:36 - INFO - __main__ - Step 60 Global step 60 Train loss 2.53 on epoch=29
03/01/2022 17:38:38 - INFO - __main__ - Step 70 Global step 70 Train loss 2.44 on epoch=34
03/01/2022 17:38:40 - INFO - __main__ - Step 80 Global step 80 Train loss 2.40 on epoch=39
03/01/2022 17:38:43 - INFO - __main__ - Step 90 Global step 90 Train loss 2.12 on epoch=44
03/01/2022 17:38:45 - INFO - __main__ - Step 100 Global step 100 Train loss 2.24 on epoch=49
03/01/2022 17:38:46 - INFO - __main__ - Global step 100 Train loss 2.35 EM 0.0 on epoch=49
03/01/2022 17:38:48 - INFO - __main__ - Step 110 Global step 110 Train loss 2.12 on epoch=54
03/01/2022 17:38:51 - INFO - __main__ - Step 120 Global step 120 Train loss 2.15 on epoch=59
03/01/2022 17:38:53 - INFO - __main__ - Step 130 Global step 130 Train loss 2.02 on epoch=64
03/01/2022 17:38:55 - INFO - __main__ - Step 140 Global step 140 Train loss 1.96 on epoch=69
03/01/2022 17:38:57 - INFO - __main__ - Step 150 Global step 150 Train loss 1.86 on epoch=74
03/01/2022 17:38:59 - INFO - __main__ - Global step 150 Train loss 2.02 EM 0.0 on epoch=74
03/01/2022 17:39:01 - INFO - __main__ - Step 160 Global step 160 Train loss 1.77 on epoch=79
03/01/2022 17:39:03 - INFO - __main__ - Step 170 Global step 170 Train loss 1.73 on epoch=84
03/01/2022 17:39:06 - INFO - __main__ - Step 180 Global step 180 Train loss 1.79 on epoch=89
03/01/2022 17:39:08 - INFO - __main__ - Step 190 Global step 190 Train loss 1.62 on epoch=94
03/01/2022 17:39:10 - INFO - __main__ - Step 200 Global step 200 Train loss 1.60 on epoch=99
03/01/2022 17:39:11 - INFO - __main__ - Global step 200 Train loss 1.70 EM 0.0 on epoch=99
03/01/2022 17:39:13 - INFO - __main__ - Step 210 Global step 210 Train loss 1.60 on epoch=104
03/01/2022 17:39:16 - INFO - __main__ - Step 220 Global step 220 Train loss 1.56 on epoch=109
03/01/2022 17:39:18 - INFO - __main__ - Step 230 Global step 230 Train loss 1.47 on epoch=114
03/01/2022 17:39:20 - INFO - __main__ - Step 240 Global step 240 Train loss 1.51 on epoch=119
03/01/2022 17:39:22 - INFO - __main__ - Step 250 Global step 250 Train loss 1.57 on epoch=124
03/01/2022 17:39:23 - INFO - __main__ - Global step 250 Train loss 1.54 EM 0.0 on epoch=124
03/01/2022 17:39:26 - INFO - __main__ - Step 260 Global step 260 Train loss 1.45 on epoch=129
03/01/2022 17:39:28 - INFO - __main__ - Step 270 Global step 270 Train loss 1.39 on epoch=134
03/01/2022 17:39:30 - INFO - __main__ - Step 280 Global step 280 Train loss 1.35 on epoch=139
03/01/2022 17:39:32 - INFO - __main__ - Step 290 Global step 290 Train loss 1.34 on epoch=144
03/01/2022 17:39:34 - INFO - __main__ - Step 300 Global step 300 Train loss 1.36 on epoch=149
03/01/2022 17:39:35 - INFO - __main__ - Global step 300 Train loss 1.38 EM 0.0 on epoch=149
03/01/2022 17:39:37 - INFO - __main__ - Step 310 Global step 310 Train loss 1.31 on epoch=154
03/01/2022 17:39:40 - INFO - __main__ - Step 320 Global step 320 Train loss 1.24 on epoch=159
03/01/2022 17:39:42 - INFO - __main__ - Step 330 Global step 330 Train loss 1.21 on epoch=164
03/01/2022 17:39:44 - INFO - __main__ - Step 340 Global step 340 Train loss 1.18 on epoch=169
03/01/2022 17:39:46 - INFO - __main__ - Step 350 Global step 350 Train loss 1.14 on epoch=174
03/01/2022 17:39:47 - INFO - __main__ - Global step 350 Train loss 1.22 EM 0.0 on epoch=174
03/01/2022 17:39:49 - INFO - __main__ - Step 360 Global step 360 Train loss 1.19 on epoch=179
03/01/2022 17:39:52 - INFO - __main__ - Step 370 Global step 370 Train loss 1.23 on epoch=184
03/01/2022 17:39:54 - INFO - __main__ - Step 380 Global step 380 Train loss 1.15 on epoch=189
03/01/2022 17:39:56 - INFO - __main__ - Step 390 Global step 390 Train loss 1.11 on epoch=194
03/01/2022 17:39:58 - INFO - __main__ - Step 400 Global step 400 Train loss 1.13 on epoch=199
03/01/2022 17:39:59 - INFO - __main__ - Global step 400 Train loss 1.16 EM 0.0 on epoch=199
03/01/2022 17:40:01 - INFO - __main__ - Step 410 Global step 410 Train loss 1.09 on epoch=204
03/01/2022 17:40:03 - INFO - __main__ - Step 420 Global step 420 Train loss 1.08 on epoch=209
03/01/2022 17:40:06 - INFO - __main__ - Step 430 Global step 430 Train loss 1.07 on epoch=214
03/01/2022 17:40:08 - INFO - __main__ - Step 440 Global step 440 Train loss 1.09 on epoch=219
03/01/2022 17:40:10 - INFO - __main__ - Step 450 Global step 450 Train loss 1.02 on epoch=224
03/01/2022 17:40:11 - INFO - __main__ - Global step 450 Train loss 1.07 EM 0.0 on epoch=224
03/01/2022 17:40:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.97 on epoch=229
03/01/2022 17:40:15 - INFO - __main__ - Step 470 Global step 470 Train loss 1.03 on epoch=234
03/01/2022 17:40:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.91 on epoch=239
03/01/2022 17:40:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.95 on epoch=244
03/01/2022 17:40:22 - INFO - __main__ - Step 500 Global step 500 Train loss 1.04 on epoch=249
03/01/2022 17:40:23 - INFO - __main__ - Global step 500 Train loss 0.98 EM 0.0 on epoch=249
03/01/2022 17:40:25 - INFO - __main__ - Step 510 Global step 510 Train loss 1.03 on epoch=254
03/01/2022 17:40:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.90 on epoch=259
03/01/2022 17:40:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.91 on epoch=264
03/01/2022 17:40:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.88 on epoch=269
03/01/2022 17:40:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.89 on epoch=274
03/01/2022 17:40:35 - INFO - __main__ - Global step 550 Train loss 0.92 EM 0.0 on epoch=274
03/01/2022 17:40:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.85 on epoch=279
03/01/2022 17:40:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.91 on epoch=284
03/01/2022 17:40:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.81 on epoch=289
03/01/2022 17:40:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.83 on epoch=294
03/01/2022 17:40:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.79 on epoch=299
03/01/2022 17:40:47 - INFO - __main__ - Global step 600 Train loss 0.84 EM 0.0 on epoch=299
03/01/2022 17:40:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.83 on epoch=304
03/01/2022 17:40:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=309
03/01/2022 17:40:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.73 on epoch=314
03/01/2022 17:40:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.81 on epoch=319
03/01/2022 17:40:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.85 on epoch=324
03/01/2022 17:40:59 - INFO - __main__ - Global step 650 Train loss 0.80 EM 0.0 on epoch=324
03/01/2022 17:41:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.76 on epoch=329
03/01/2022 17:41:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.78 on epoch=334
03/01/2022 17:41:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.84 on epoch=339
03/01/2022 17:41:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.78 on epoch=344
03/01/2022 17:41:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.72 on epoch=349
03/01/2022 17:41:11 - INFO - __main__ - Global step 700 Train loss 0.78 EM 0.0 on epoch=349
03/01/2022 17:41:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.78 on epoch=354
03/01/2022 17:41:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.74 on epoch=359
03/01/2022 17:41:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.77 on epoch=364
03/01/2022 17:41:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.68 on epoch=369
03/01/2022 17:41:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.68 on epoch=374
03/01/2022 17:41:23 - INFO - __main__ - Global step 750 Train loss 0.73 EM 0.0 on epoch=374
03/01/2022 17:41:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.70 on epoch=379
03/01/2022 17:41:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.72 on epoch=384
03/01/2022 17:41:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.70 on epoch=389
03/01/2022 17:41:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.66 on epoch=394
03/01/2022 17:41:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.66 on epoch=399
03/01/2022 17:41:35 - INFO - __main__ - Global step 800 Train loss 0.69 EM 0.0 on epoch=399
03/01/2022 17:41:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.69 on epoch=404
03/01/2022 17:41:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.71 on epoch=409
03/01/2022 17:41:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.63 on epoch=414
03/01/2022 17:41:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.68 on epoch=419
03/01/2022 17:41:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.58 on epoch=424
03/01/2022 17:41:47 - INFO - __main__ - Global step 850 Train loss 0.66 EM 0.0 on epoch=424
03/01/2022 17:41:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.67 on epoch=429
03/01/2022 17:41:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.62 on epoch=434
03/01/2022 17:41:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.63 on epoch=439
03/01/2022 17:41:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.63 on epoch=444
03/01/2022 17:41:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.58 on epoch=449
03/01/2022 17:41:58 - INFO - __main__ - Global step 900 Train loss 0.63 EM 0.0 on epoch=449
03/01/2022 17:42:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.57 on epoch=454
03/01/2022 17:42:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.63 on epoch=459
03/01/2022 17:42:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.62 on epoch=464
03/01/2022 17:42:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.59 on epoch=469
03/01/2022 17:42:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.55 on epoch=474
03/01/2022 17:42:11 - INFO - __main__ - Global step 950 Train loss 0.59 EM 0.0 on epoch=474
03/01/2022 17:42:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.51 on epoch=479
03/01/2022 17:42:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.53 on epoch=484
03/01/2022 17:42:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.54 on epoch=489
03/01/2022 17:42:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.50 on epoch=494
03/01/2022 17:42:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.54 on epoch=499
03/01/2022 17:42:23 - INFO - __main__ - Global step 1000 Train loss 0.53 EM 0.0 on epoch=499
03/01/2022 17:42:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.51 on epoch=504
03/01/2022 17:42:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.54 on epoch=509
03/01/2022 17:42:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.56 on epoch=514
03/01/2022 17:42:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=519
03/01/2022 17:42:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.50 on epoch=524
03/01/2022 17:42:35 - INFO - __main__ - Global step 1050 Train loss 0.51 EM 0.0 on epoch=524
03/01/2022 17:42:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=529
03/01/2022 17:42:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=534
03/01/2022 17:42:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.51 on epoch=539
03/01/2022 17:42:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
03/01/2022 17:42:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=549
03/01/2022 17:42:47 - INFO - __main__ - Global step 1100 Train loss 0.47 EM 0.0 on epoch=549
03/01/2022 17:42:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.53 on epoch=554
03/01/2022 17:42:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=559
03/01/2022 17:42:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=564
03/01/2022 17:42:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=569
03/01/2022 17:42:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=574
03/01/2022 17:42:59 - INFO - __main__ - Global step 1150 Train loss 0.45 EM 0.0 on epoch=574
03/01/2022 17:43:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=579
03/01/2022 17:43:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=584
03/01/2022 17:43:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=589
03/01/2022 17:43:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=594
03/01/2022 17:43:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=599
03/01/2022 17:43:11 - INFO - __main__ - Global step 1200 Train loss 0.38 EM 0.0 on epoch=599
03/01/2022 17:43:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=604
03/01/2022 17:43:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=609
03/01/2022 17:43:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=614
03/01/2022 17:43:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=619
03/01/2022 17:43:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=624
03/01/2022 17:43:23 - INFO - __main__ - Global step 1250 Train loss 0.36 EM 0.0 on epoch=624
03/01/2022 17:43:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=629
03/01/2022 17:43:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=634
03/01/2022 17:43:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=639
03/01/2022 17:43:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=644
03/01/2022 17:43:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=649
03/01/2022 17:43:35 - INFO - __main__ - Global step 1300 Train loss 0.37 EM 0.0 on epoch=649
03/01/2022 17:43:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=654
03/01/2022 17:43:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=659
03/01/2022 17:43:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=664
03/01/2022 17:43:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=669
03/01/2022 17:43:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=674
03/01/2022 17:43:48 - INFO - __main__ - Global step 1350 Train loss 0.36 EM 0.0 on epoch=674
03/01/2022 17:43:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=679
03/01/2022 17:43:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.28 on epoch=684
03/01/2022 17:43:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=689
03/01/2022 17:43:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=694
03/01/2022 17:43:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.32 on epoch=699
03/01/2022 17:44:00 - INFO - __main__ - Global step 1400 Train loss 0.30 EM 0.0 on epoch=699
03/01/2022 17:44:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=704
03/01/2022 17:44:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.25 on epoch=709
03/01/2022 17:44:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=714
03/01/2022 17:44:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=719
03/01/2022 17:44:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=724
03/01/2022 17:44:12 - INFO - __main__ - Global step 1450 Train loss 0.24 EM 0.0 on epoch=724
03/01/2022 17:44:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=729
03/01/2022 17:44:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=734
03/01/2022 17:44:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=739
03/01/2022 17:44:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=744
03/01/2022 17:44:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=749
03/01/2022 17:44:24 - INFO - __main__ - Global step 1500 Train loss 0.22 EM 0.0 on epoch=749
03/01/2022 17:44:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=754
03/01/2022 17:44:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=759
03/01/2022 17:44:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=764
03/01/2022 17:44:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=769
03/01/2022 17:44:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.16 on epoch=774
03/01/2022 17:44:36 - INFO - __main__ - Global step 1550 Train loss 0.21 EM 0.0 on epoch=774
03/01/2022 17:44:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=779
03/01/2022 17:44:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=784
03/01/2022 17:44:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=789
03/01/2022 17:44:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=794
03/01/2022 17:44:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.22 on epoch=799
03/01/2022 17:44:48 - INFO - __main__ - Global step 1600 Train loss 0.21 EM 0.0 on epoch=799
03/01/2022 17:44:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=804
03/01/2022 17:44:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=809
03/01/2022 17:44:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=814
03/01/2022 17:44:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=819
03/01/2022 17:44:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.15 on epoch=824
03/01/2022 17:45:00 - INFO - __main__ - Global step 1650 Train loss 0.18 EM 0.0 on epoch=824
03/01/2022 17:45:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=829
03/01/2022 17:45:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=834
03/01/2022 17:45:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=839
03/01/2022 17:45:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=844
03/01/2022 17:45:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=849
03/01/2022 17:45:12 - INFO - __main__ - Global step 1700 Train loss 0.17 EM 0.0 on epoch=849
03/01/2022 17:45:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=854
03/01/2022 17:45:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=859
03/01/2022 17:45:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=864
03/01/2022 17:45:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=869
03/01/2022 17:45:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=874
03/01/2022 17:45:24 - INFO - __main__ - Global step 1750 Train loss 0.16 EM 0.0 on epoch=874
03/01/2022 17:45:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.16 on epoch=879
03/01/2022 17:45:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.16 on epoch=884
03/01/2022 17:45:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=889
03/01/2022 17:45:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=894
03/01/2022 17:45:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=899
03/01/2022 17:45:36 - INFO - __main__ - Global step 1800 Train loss 0.15 EM 0.0 on epoch=899
03/01/2022 17:45:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=904
03/01/2022 17:45:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=909
03/01/2022 17:45:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=914
03/01/2022 17:45:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=919
03/01/2022 17:45:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/01/2022 17:45:49 - INFO - __main__ - Global step 1850 Train loss 0.13 EM 0.0 on epoch=924
03/01/2022 17:45:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=929
03/01/2022 17:45:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=934
03/01/2022 17:45:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=939
03/01/2022 17:45:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=944
03/01/2022 17:46:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=949
03/01/2022 17:46:01 - INFO - __main__ - Global step 1900 Train loss 0.14 EM 0.0 on epoch=949
03/01/2022 17:46:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=954
03/01/2022 17:46:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=959
03/01/2022 17:46:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=964
03/01/2022 17:46:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=969
03/01/2022 17:46:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=974
03/01/2022 17:46:13 - INFO - __main__ - Global step 1950 Train loss 0.12 EM 0.0 on epoch=974
03/01/2022 17:46:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=979
03/01/2022 17:46:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=984
03/01/2022 17:46:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=989
03/01/2022 17:46:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=994
03/01/2022 17:46:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=999
03/01/2022 17:46:25 - INFO - __main__ - Global step 2000 Train loss 0.13 EM 0.0 on epoch=999
03/01/2022 17:46:25 - INFO - __main__ - save last model!
03/01/2022 17:46:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 17:46:25 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 17:46:25 - INFO - __main__ - Printing 3 examples
03/01/2022 17:46:25 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 17:46:25 - INFO - __main__ - ['taming of the shrew']
03/01/2022 17:46:25 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 17:46:25 - INFO - __main__ - ['henry fonda']
03/01/2022 17:46:25 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 17:46:25 - INFO - __main__ - ['tchaikovsky']
03/01/2022 17:46:25 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:46:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:46:25 - INFO - __main__ - Printing 3 examples
03/01/2022 17:46:25 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 17:46:25 - INFO - __main__ - ['orange is the new black']
03/01/2022 17:46:25 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 17:46:25 - INFO - __main__ - ['western australia']
03/01/2022 17:46:25 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 17:46:25 - INFO - __main__ - ['turkey']
03/01/2022 17:46:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 17:46:25 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:46:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:46:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:46:25 - INFO - __main__ - Printing 3 examples
03/01/2022 17:46:25 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 17:46:25 - INFO - __main__ - ['benito mussolini']
03/01/2022 17:46:25 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 17:46:25 - INFO - __main__ - ['the kinks']
03/01/2022 17:46:25 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 17:46:25 - INFO - __main__ - ['saigon']
03/01/2022 17:46:25 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:46:25 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:46:25 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:46:27 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:46:31 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 17:46:37 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 17:46:37 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(6579, 166537), (5880, 173333), (2252, 353035), (6881, 160252), (4773, 218071), (6404, 160401), (3371, 234531), (4187, 259804), (1925, 417452), (968, 645839), (2729, 183765), (5538, 189511), (987, 914447), (144, 1635131), (1079, 796417), (1653, 586845), (1731, 565272), (3424, 221936), (5680, 157495), (731, 1250799), (4298, 243616), (3238, 245783), (6415, 157560), (2406, 200208), (5775, 177401), (2721, 367851), (6309, 169174), (135, 7037199), (1214, 767509), (5962, 174066), (2867, 348102), (3209, 276527), (1189, 784250), (891, 319194), (5015, 214805), (1701, 571183), (3845, 206498), (26, 28035103), (2809, 352164), (5121, 187048), (1892, 508575), (3574, 184323), (996, 821524), (3263, 314166), (6363, 154270), (90, 459181), (4226, 250683), (1751, 560750), (5058, 173227), (5205, 176939), (7, 158399866), (6008, 156882), (3474, 290209), (3351, 190868), (225, 4092648), (1142, 782542), (154, 259007), (3278, 305705), (2637, 387504), (532, 1388213), (2597, 385302), (6237, 164778), (851, 1026262), (793, 959415), (2719, 191820), (788, 1153138), (4209, 175746), (1393, 703212), (1438, 688034), (5340, 197066), (3995, 170269), (1376, 549699), (2909, 350152), (5773, 177903), (6551, 154998), (4799, 221150), (2335, 431458), (6082, 173261), (1824, 455362), (3242, 301888), (4627, 234647), (3797, 250903), (4027, 213331), (6464, 181012), (5210, 207673), (1137, 734426), (2765, 373166), (2451, 408517), (708, 1240422), (3108, 278320), (243, 3854645), (4830, 159383), (5630, 169060), (2134, 401363), (4490, 159360), (2342, 415802), (80, 11392202), (758, 1131386), (652, 1360888)]
03/01/2022 17:46:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:46:38 - INFO - __main__ - Starting training!
03/01/2022 17:48:51 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_100_0.5_8_predictions.txt
03/01/2022 17:48:51 - INFO - __main__ - EM on test data: 0.0065
03/01/2022 17:48:52 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.5, bsz=8, dev_performance=0.0, test_performance=0.006509764646970456
03/01/2022 17:48:52 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.4, bsz=8 ...
03/01/2022 17:48:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:48:53 - INFO - __main__ - Printing 3 examples
03/01/2022 17:48:53 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 17:48:53 - INFO - __main__ - ['orange is the new black']
03/01/2022 17:48:53 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 17:48:53 - INFO - __main__ - ['western australia']
03/01/2022 17:48:53 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 17:48:53 - INFO - __main__ - ['turkey']
03/01/2022 17:48:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 17:48:53 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:48:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:48:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:48:53 - INFO - __main__ - Printing 3 examples
03/01/2022 17:48:53 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 17:48:53 - INFO - __main__ - ['benito mussolini']
03/01/2022 17:48:53 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 17:48:53 - INFO - __main__ - ['the kinks']
03/01/2022 17:48:53 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 17:48:53 - INFO - __main__ - ['saigon']
03/01/2022 17:48:53 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:48:53 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:48:53 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:49:07 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 17:49:07 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(6579, 166537), (5880, 173333), (2252, 353035), (6881, 160252), (4773, 218071), (6404, 160401), (3371, 234531), (4187, 259804), (1925, 417452), (968, 645839), (2729, 183765), (5538, 189511), (987, 914447), (144, 1635131), (1079, 796417), (1653, 586845), (1731, 565272), (3424, 221936), (5680, 157495), (731, 1250799), (4298, 243616), (3238, 245783), (6415, 157560), (2406, 200208), (5775, 177401), (2721, 367851), (6309, 169174), (135, 7037199), (1214, 767509), (5962, 174066), (2867, 348102), (3209, 276527), (1189, 784250), (891, 319194), (5015, 214805), (1701, 571183), (3845, 206498), (26, 28035103), (2809, 352164), (5121, 187048), (1892, 508575), (3574, 184323), (996, 821524), (3263, 314166), (6363, 154270), (90, 459181), (4226, 250683), (1751, 560750), (5058, 173227), (5205, 176939), (7, 158399866), (6008, 156882), (3474, 290209), (3351, 190868), (225, 4092648), (1142, 782542), (154, 259007), (3278, 305705), (2637, 387504), (532, 1388213), (2597, 385302), (6237, 164778), (851, 1026262), (793, 959415), (2719, 191820), (788, 1153138), (4209, 175746), (1393, 703212), (1438, 688034), (5340, 197066), (3995, 170269), (1376, 549699), (2909, 350152), (5773, 177903), (6551, 154998), (4799, 221150), (2335, 431458), (6082, 173261), (1824, 455362), (3242, 301888), (4627, 234647), (3797, 250903), (4027, 213331), (6464, 181012), (5210, 207673), (1137, 734426), (2765, 373166), (2451, 408517), (708, 1240422), (3108, 278320), (243, 3854645), (4830, 159383), (5630, 169060), (2134, 401363), (4490, 159360), (2342, 415802), (80, 11392202), (758, 1131386), (652, 1360888)]
03/01/2022 17:49:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:49:08 - INFO - __main__ - Starting training!
03/01/2022 17:49:10 - INFO - __main__ - Step 10 Global step 10 Train loss 4.81 on epoch=4
03/01/2022 17:49:12 - INFO - __main__ - Step 20 Global step 20 Train loss 4.03 on epoch=9
03/01/2022 17:49:15 - INFO - __main__ - Step 30 Global step 30 Train loss 3.25 on epoch=14
03/01/2022 17:49:17 - INFO - __main__ - Step 40 Global step 40 Train loss 2.88 on epoch=19
03/01/2022 17:49:19 - INFO - __main__ - Step 50 Global step 50 Train loss 2.70 on epoch=24
03/01/2022 17:49:20 - INFO - __main__ - Global step 50 Train loss 3.54 EM 0.0 on epoch=24
03/01/2022 17:49:20 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 17:49:23 - INFO - __main__ - Step 60 Global step 60 Train loss 2.61 on epoch=29
03/01/2022 17:49:25 - INFO - __main__ - Step 70 Global step 70 Train loss 2.47 on epoch=34
03/01/2022 17:49:27 - INFO - __main__ - Step 80 Global step 80 Train loss 2.24 on epoch=39
03/01/2022 17:49:29 - INFO - __main__ - Step 90 Global step 90 Train loss 2.23 on epoch=44
03/01/2022 17:49:31 - INFO - __main__ - Step 100 Global step 100 Train loss 2.25 on epoch=49
03/01/2022 17:49:32 - INFO - __main__ - Global step 100 Train loss 2.36 EM 0.0 on epoch=49
03/01/2022 17:49:34 - INFO - __main__ - Step 110 Global step 110 Train loss 2.10 on epoch=54
03/01/2022 17:49:37 - INFO - __main__ - Step 120 Global step 120 Train loss 2.07 on epoch=59
03/01/2022 17:49:39 - INFO - __main__ - Step 130 Global step 130 Train loss 2.06 on epoch=64
03/01/2022 17:49:41 - INFO - __main__ - Step 140 Global step 140 Train loss 1.96 on epoch=69
03/01/2022 17:49:43 - INFO - __main__ - Step 150 Global step 150 Train loss 1.93 on epoch=74
03/01/2022 17:49:44 - INFO - __main__ - Global step 150 Train loss 2.02 EM 0.0 on epoch=74
03/01/2022 17:49:47 - INFO - __main__ - Step 160 Global step 160 Train loss 1.77 on epoch=79
03/01/2022 17:49:49 - INFO - __main__ - Step 170 Global step 170 Train loss 1.85 on epoch=84
03/01/2022 17:49:51 - INFO - __main__ - Step 180 Global step 180 Train loss 1.74 on epoch=89
03/01/2022 17:49:53 - INFO - __main__ - Step 190 Global step 190 Train loss 1.78 on epoch=94
03/01/2022 17:49:55 - INFO - __main__ - Step 200 Global step 200 Train loss 1.70 on epoch=99
03/01/2022 17:49:57 - INFO - __main__ - Global step 200 Train loss 1.77 EM 0.0 on epoch=99
03/01/2022 17:49:59 - INFO - __main__ - Step 210 Global step 210 Train loss 1.71 on epoch=104
03/01/2022 17:50:01 - INFO - __main__ - Step 220 Global step 220 Train loss 1.60 on epoch=109
03/01/2022 17:50:03 - INFO - __main__ - Step 230 Global step 230 Train loss 1.64 on epoch=114
03/01/2022 17:50:05 - INFO - __main__ - Step 240 Global step 240 Train loss 1.54 on epoch=119
03/01/2022 17:50:07 - INFO - __main__ - Step 250 Global step 250 Train loss 1.50 on epoch=124
03/01/2022 17:50:09 - INFO - __main__ - Global step 250 Train loss 1.60 EM 0.0 on epoch=124
03/01/2022 17:50:11 - INFO - __main__ - Step 260 Global step 260 Train loss 1.52 on epoch=129
03/01/2022 17:50:13 - INFO - __main__ - Step 270 Global step 270 Train loss 1.37 on epoch=134
03/01/2022 17:50:15 - INFO - __main__ - Step 280 Global step 280 Train loss 1.50 on epoch=139
03/01/2022 17:50:17 - INFO - __main__ - Step 290 Global step 290 Train loss 1.36 on epoch=144
03/01/2022 17:50:20 - INFO - __main__ - Step 300 Global step 300 Train loss 1.33 on epoch=149
03/01/2022 17:50:21 - INFO - __main__ - Global step 300 Train loss 1.41 EM 0.0 on epoch=149
03/01/2022 17:50:23 - INFO - __main__ - Step 310 Global step 310 Train loss 1.30 on epoch=154
03/01/2022 17:50:25 - INFO - __main__ - Step 320 Global step 320 Train loss 1.34 on epoch=159
03/01/2022 17:50:27 - INFO - __main__ - Step 330 Global step 330 Train loss 1.25 on epoch=164
03/01/2022 17:50:29 - INFO - __main__ - Step 340 Global step 340 Train loss 1.20 on epoch=169
03/01/2022 17:50:32 - INFO - __main__ - Step 350 Global step 350 Train loss 1.21 on epoch=174
03/01/2022 17:50:33 - INFO - __main__ - Global step 350 Train loss 1.26 EM 0.0 on epoch=174
03/01/2022 17:50:35 - INFO - __main__ - Step 360 Global step 360 Train loss 1.14 on epoch=179
03/01/2022 17:50:37 - INFO - __main__ - Step 370 Global step 370 Train loss 1.29 on epoch=184
03/01/2022 17:50:39 - INFO - __main__ - Step 380 Global step 380 Train loss 1.21 on epoch=189
03/01/2022 17:50:41 - INFO - __main__ - Step 390 Global step 390 Train loss 1.13 on epoch=194
03/01/2022 17:50:44 - INFO - __main__ - Step 400 Global step 400 Train loss 1.16 on epoch=199
03/01/2022 17:50:45 - INFO - __main__ - Global step 400 Train loss 1.19 EM 0.0 on epoch=199
03/01/2022 17:50:47 - INFO - __main__ - Step 410 Global step 410 Train loss 1.20 on epoch=204
03/01/2022 17:50:49 - INFO - __main__ - Step 420 Global step 420 Train loss 1.13 on epoch=209
03/01/2022 17:50:51 - INFO - __main__ - Step 430 Global step 430 Train loss 1.13 on epoch=214
03/01/2022 17:50:53 - INFO - __main__ - Step 440 Global step 440 Train loss 1.03 on epoch=219
03/01/2022 17:50:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.98 on epoch=224
03/01/2022 17:50:57 - INFO - __main__ - Global step 450 Train loss 1.10 EM 0.0 on epoch=224
03/01/2022 17:50:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.98 on epoch=229
03/01/2022 17:51:01 - INFO - __main__ - Step 470 Global step 470 Train loss 1.01 on epoch=234
03/01/2022 17:51:03 - INFO - __main__ - Step 480 Global step 480 Train loss 1.01 on epoch=239
03/01/2022 17:51:05 - INFO - __main__ - Step 490 Global step 490 Train loss 1.04 on epoch=244
03/01/2022 17:51:07 - INFO - __main__ - Step 500 Global step 500 Train loss 1.07 on epoch=249
03/01/2022 17:51:09 - INFO - __main__ - Global step 500 Train loss 1.02 EM 0.0 on epoch=249
03/01/2022 17:51:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.97 on epoch=254
03/01/2022 17:51:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.93 on epoch=259
03/01/2022 17:51:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.97 on epoch=264
03/01/2022 17:51:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.90 on epoch=269
03/01/2022 17:51:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.94 on epoch=274
03/01/2022 17:51:21 - INFO - __main__ - Global step 550 Train loss 0.94 EM 0.0 on epoch=274
03/01/2022 17:51:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.85 on epoch=279
03/01/2022 17:51:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.87 on epoch=284
03/01/2022 17:51:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=289
03/01/2022 17:51:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.84 on epoch=294
03/01/2022 17:51:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.87 on epoch=299
03/01/2022 17:51:33 - INFO - __main__ - Global step 600 Train loss 0.85 EM 0.0 on epoch=299
03/01/2022 17:51:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.89 on epoch=304
03/01/2022 17:51:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.83 on epoch=309
03/01/2022 17:51:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.80 on epoch=314
03/01/2022 17:51:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.87 on epoch=319
03/01/2022 17:51:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.73 on epoch=324
03/01/2022 17:51:45 - INFO - __main__ - Global step 650 Train loss 0.82 EM 0.0 on epoch=324
03/01/2022 17:51:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.78 on epoch=329
03/01/2022 17:51:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.81 on epoch=334
03/01/2022 17:51:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.73 on epoch=339
03/01/2022 17:51:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.73 on epoch=344
03/01/2022 17:51:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.79 on epoch=349
03/01/2022 17:51:57 - INFO - __main__ - Global step 700 Train loss 0.77 EM 0.0 on epoch=349
03/01/2022 17:51:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.68 on epoch=354
03/01/2022 17:52:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.69 on epoch=359
03/01/2022 17:52:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.74 on epoch=364
03/01/2022 17:52:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.73 on epoch=369
03/01/2022 17:52:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.69 on epoch=374
03/01/2022 17:52:09 - INFO - __main__ - Global step 750 Train loss 0.71 EM 0.0 on epoch=374
03/01/2022 17:52:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.66 on epoch=379
03/01/2022 17:52:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.68 on epoch=384
03/01/2022 17:52:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.66 on epoch=389
03/01/2022 17:52:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.69 on epoch=394
03/01/2022 17:52:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.60 on epoch=399
03/01/2022 17:52:21 - INFO - __main__ - Global step 800 Train loss 0.66 EM 0.0 on epoch=399
03/01/2022 17:52:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.65 on epoch=404
03/01/2022 17:52:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.62 on epoch=409
03/01/2022 17:52:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.62 on epoch=414
03/01/2022 17:52:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.70 on epoch=419
03/01/2022 17:52:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.61 on epoch=424
03/01/2022 17:52:33 - INFO - __main__ - Global step 850 Train loss 0.64 EM 0.0 on epoch=424
03/01/2022 17:52:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.66 on epoch=429
03/01/2022 17:52:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.58 on epoch=434
03/01/2022 17:52:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.58 on epoch=439
03/01/2022 17:52:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.60 on epoch=444
03/01/2022 17:52:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.56 on epoch=449
03/01/2022 17:52:45 - INFO - __main__ - Global step 900 Train loss 0.59 EM 0.0 on epoch=449
03/01/2022 17:52:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.54 on epoch=454
03/01/2022 17:52:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.57 on epoch=459
03/01/2022 17:52:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.54 on epoch=464
03/01/2022 17:52:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.59 on epoch=469
03/01/2022 17:52:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.56 on epoch=474
03/01/2022 17:52:58 - INFO - __main__ - Global step 950 Train loss 0.56 EM 0.0 on epoch=474
03/01/2022 17:53:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.57 on epoch=479
03/01/2022 17:53:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.50 on epoch=484
03/01/2022 17:53:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.50 on epoch=489
03/01/2022 17:53:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.52 on epoch=494
03/01/2022 17:53:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.54 on epoch=499
03/01/2022 17:53:10 - INFO - __main__ - Global step 1000 Train loss 0.53 EM 0.0 on epoch=499
03/01/2022 17:53:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.52 on epoch=504
03/01/2022 17:53:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.52 on epoch=509
03/01/2022 17:53:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=514
03/01/2022 17:53:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=519
03/01/2022 17:53:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.52 on epoch=524
03/01/2022 17:53:22 - INFO - __main__ - Global step 1050 Train loss 0.49 EM 0.0 on epoch=524
03/01/2022 17:53:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=529
03/01/2022 17:53:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=534
03/01/2022 17:53:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=539
03/01/2022 17:53:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.47 on epoch=544
03/01/2022 17:53:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.51 on epoch=549
03/01/2022 17:53:34 - INFO - __main__ - Global step 1100 Train loss 0.44 EM 0.0 on epoch=549
03/01/2022 17:53:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.49 on epoch=554
03/01/2022 17:53:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=559
03/01/2022 17:53:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=564
03/01/2022 17:53:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=569
03/01/2022 17:53:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=574
03/01/2022 17:53:46 - INFO - __main__ - Global step 1150 Train loss 0.43 EM 0.0 on epoch=574
03/01/2022 17:53:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=579
03/01/2022 17:53:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=584
03/01/2022 17:53:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=589
03/01/2022 17:53:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=594
03/01/2022 17:53:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=599
03/01/2022 17:53:58 - INFO - __main__ - Global step 1200 Train loss 0.36 EM 0.0 on epoch=599
03/01/2022 17:54:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.35 on epoch=604
03/01/2022 17:54:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=609
03/01/2022 17:54:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=614
03/01/2022 17:54:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.37 on epoch=619
03/01/2022 17:54:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.34 on epoch=624
03/01/2022 17:54:10 - INFO - __main__ - Global step 1250 Train loss 0.37 EM 0.0 on epoch=624
03/01/2022 17:54:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=629
03/01/2022 17:54:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=634
03/01/2022 17:54:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=639
03/01/2022 17:54:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=644
03/01/2022 17:54:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=649
03/01/2022 17:54:22 - INFO - __main__ - Global step 1300 Train loss 0.36 EM 0.0 on epoch=649
03/01/2022 17:54:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=654
03/01/2022 17:54:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.31 on epoch=659
03/01/2022 17:54:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.32 on epoch=664
03/01/2022 17:54:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.31 on epoch=669
03/01/2022 17:54:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=674
03/01/2022 17:54:34 - INFO - __main__ - Global step 1350 Train loss 0.31 EM 0.0 on epoch=674
03/01/2022 17:54:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=679
03/01/2022 17:54:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=684
03/01/2022 17:54:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=689
03/01/2022 17:54:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=694
03/01/2022 17:54:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=699
03/01/2022 17:54:46 - INFO - __main__ - Global step 1400 Train loss 0.30 EM 0.0 on epoch=699
03/01/2022 17:54:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.35 on epoch=704
03/01/2022 17:54:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=709
03/01/2022 17:54:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=714
03/01/2022 17:54:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.30 on epoch=719
03/01/2022 17:54:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=724
03/01/2022 17:54:58 - INFO - __main__ - Global step 1450 Train loss 0.27 EM 0.0 on epoch=724
03/01/2022 17:55:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.29 on epoch=729
03/01/2022 17:55:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=734
03/01/2022 17:55:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=739
03/01/2022 17:55:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=744
03/01/2022 17:55:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=749
03/01/2022 17:55:11 - INFO - __main__ - Global step 1500 Train loss 0.25 EM 0.0 on epoch=749
03/01/2022 17:55:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.23 on epoch=754
03/01/2022 17:55:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=759
03/01/2022 17:55:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=764
03/01/2022 17:55:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=769
03/01/2022 17:55:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=774
03/01/2022 17:55:23 - INFO - __main__ - Global step 1550 Train loss 0.24 EM 0.0 on epoch=774
03/01/2022 17:55:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=779
03/01/2022 17:55:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.17 on epoch=784
03/01/2022 17:55:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=789
03/01/2022 17:55:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=794
03/01/2022 17:55:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.19 on epoch=799
03/01/2022 17:55:35 - INFO - __main__ - Global step 1600 Train loss 0.20 EM 0.0 on epoch=799
03/01/2022 17:55:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=804
03/01/2022 17:55:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=809
03/01/2022 17:55:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=814
03/01/2022 17:55:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=819
03/01/2022 17:55:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=824
03/01/2022 17:55:47 - INFO - __main__ - Global step 1650 Train loss 0.19 EM 0.0 on epoch=824
03/01/2022 17:55:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=829
03/01/2022 17:55:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=834
03/01/2022 17:55:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=839
03/01/2022 17:55:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=844
03/01/2022 17:55:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=849
03/01/2022 17:55:59 - INFO - __main__ - Global step 1700 Train loss 0.22 EM 0.0 on epoch=849
03/01/2022 17:56:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=854
03/01/2022 17:56:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=859
03/01/2022 17:56:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=864
03/01/2022 17:56:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=869
03/01/2022 17:56:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=874
03/01/2022 17:56:11 - INFO - __main__ - Global step 1750 Train loss 0.17 EM 0.0 on epoch=874
03/01/2022 17:56:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=879
03/01/2022 17:56:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=884
03/01/2022 17:56:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.19 on epoch=889
03/01/2022 17:56:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.16 on epoch=894
03/01/2022 17:56:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=899
03/01/2022 17:56:24 - INFO - __main__ - Global step 1800 Train loss 0.19 EM 0.0 on epoch=899
03/01/2022 17:56:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.16 on epoch=904
03/01/2022 17:56:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=909
03/01/2022 17:56:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=914
03/01/2022 17:56:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=919
03/01/2022 17:56:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=924
03/01/2022 17:56:36 - INFO - __main__ - Global step 1850 Train loss 0.15 EM 0.0 on epoch=924
03/01/2022 17:56:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=929
03/01/2022 17:56:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=934
03/01/2022 17:56:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.17 on epoch=939
03/01/2022 17:56:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=944
03/01/2022 17:56:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=949
03/01/2022 17:56:48 - INFO - __main__ - Global step 1900 Train loss 0.16 EM 0.0 on epoch=949
03/01/2022 17:56:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=954
03/01/2022 17:56:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=959
03/01/2022 17:56:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=964
03/01/2022 17:56:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=969
03/01/2022 17:56:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=974
03/01/2022 17:57:00 - INFO - __main__ - Global step 1950 Train loss 0.13 EM 0.0 on epoch=974
03/01/2022 17:57:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=979
03/01/2022 17:57:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=984
03/01/2022 17:57:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=989
03/01/2022 17:57:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=994
03/01/2022 17:57:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=999
03/01/2022 17:57:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:57:12 - INFO - __main__ - Printing 3 examples
03/01/2022 17:57:12 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 17:57:12 - INFO - __main__ - ['orange is the new black']
03/01/2022 17:57:12 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 17:57:12 - INFO - __main__ - ['western australia']
03/01/2022 17:57:12 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 17:57:12 - INFO - __main__ - ['turkey']
03/01/2022 17:57:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 17:57:12 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:57:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:57:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:57:12 - INFO - __main__ - Printing 3 examples
03/01/2022 17:57:12 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 17:57:12 - INFO - __main__ - ['benito mussolini']
03/01/2022 17:57:12 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 17:57:12 - INFO - __main__ - ['the kinks']
03/01/2022 17:57:12 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 17:57:12 - INFO - __main__ - ['saigon']
03/01/2022 17:57:12 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:57:12 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:57:12 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:57:12 - INFO - __main__ - Global step 2000 Train loss 0.14 EM 0.0 on epoch=999
03/01/2022 17:57:12 - INFO - __main__ - save last model!
03/01/2022 17:57:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 17:57:12 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 17:57:12 - INFO - __main__ - Printing 3 examples
03/01/2022 17:57:12 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 17:57:12 - INFO - __main__ - ['taming of the shrew']
03/01/2022 17:57:12 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 17:57:12 - INFO - __main__ - ['henry fonda']
03/01/2022 17:57:12 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 17:57:12 - INFO - __main__ - ['tchaikovsky']
03/01/2022 17:57:12 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:57:14 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:57:18 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 17:57:26 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 17:57:26 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(2594, 403711), (738, 1187017), (73, 2525281), (3583, 284375), (773, 1132610), (2099, 182268), (1757, 390014), (3023, 334230), (7161, 192247), (2231, 427234), (6082, 173261), (1461, 672164), (3573, 163617), (6209, 162555), (5259, 197913), (2697, 380850), (4225, 197872), (2319, 230472), (1994, 479967), (993, 886924), (1069, 868644), (4404, 216707), (4001, 264258), (4585, 221032), (4434, 228933), (5229, 200589), (629, 1463788), (1101, 860462), (661, 1317687), (3147, 232857), (2132, 277175), (1102, 798392), (2542, 371822), (2503, 477739), (2036, 480551), (5480, 174200), (4952, 209263), (397, 637384), (4682, 222111), (2030, 497571), (5234, 202177), (2665, 369855), (757, 1071575), (4370, 210982), (4895, 169946), (1085, 862457), (705, 1548305), (4508, 172897), (167, 5559739), (1112, 892303), (2193, 392132), (1396, 545283), (4374, 230866), (3385, 191872), (5816, 194070), (2358, 427447), (4191, 233374), (646, 1377071), (1790, 550995), (4264, 247591), (22, 38484130), (2415, 239286), (2392, 349952), (4190, 205854), (2833, 327549), (5032, 221170), (6353, 168404), (5324, 191543), (1518, 582487), (1894, 481117), (3228, 315556), (2265, 421208), (636, 1378362), (6548, 161733), (6098, 172422), (696, 1306625), (3452, 300052), (717, 1408825), (573, 1561543), (4562, 160268), (4277, 195942), (5640, 186094), (2551, 176659), (1249, 573784), (631, 1412370), (4905, 184294), (877, 1019947), (1758, 490726), (767, 1164520), (2558, 157757), (518, 1170336), (1534, 393440), (3774, 232302), (1269, 737181), (6699, 164021), (2888, 375431), (6496, 161755), (4973, 157902), (348, 1943901)]
03/01/2022 17:57:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:57:27 - INFO - __main__ - Starting training!
03/01/2022 17:59:52 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_100_0.4_8_predictions.txt
03/01/2022 17:59:52 - INFO - __main__ - EM on test data: 0.0075
03/01/2022 17:59:53 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.4, bsz=8, dev_performance=0.0, test_performance=0.007511266900350526
03/01/2022 17:59:53 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.3, bsz=8 ...
03/01/2022 17:59:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:59:54 - INFO - __main__ - Printing 3 examples
03/01/2022 17:59:54 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 17:59:54 - INFO - __main__ - ['orange is the new black']
03/01/2022 17:59:54 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 17:59:54 - INFO - __main__ - ['western australia']
03/01/2022 17:59:54 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 17:59:54 - INFO - __main__ - ['turkey']
03/01/2022 17:59:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 17:59:54 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:59:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:59:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:59:54 - INFO - __main__ - Printing 3 examples
03/01/2022 17:59:54 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 17:59:54 - INFO - __main__ - ['benito mussolini']
03/01/2022 17:59:54 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 17:59:54 - INFO - __main__ - ['the kinks']
03/01/2022 17:59:54 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 17:59:54 - INFO - __main__ - ['saigon']
03/01/2022 17:59:54 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:59:54 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:59:54 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:00:09 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 18:00:09 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(2594, 403711), (738, 1187017), (73, 2525281), (3583, 284375), (773, 1132610), (2099, 182268), (1757, 390014), (3023, 334230), (7161, 192247), (2231, 427234), (6082, 173261), (1461, 672164), (3573, 163617), (6209, 162555), (5259, 197913), (2697, 380850), (4225, 197872), (2319, 230472), (1994, 479967), (993, 886924), (1069, 868644), (4404, 216707), (4001, 264258), (4585, 221032), (4434, 228933), (5229, 200589), (629, 1463788), (1101, 860462), (661, 1317687), (3147, 232857), (2132, 277175), (1102, 798392), (2542, 371822), (2503, 477739), (2036, 480551), (5480, 174200), (4952, 209263), (397, 637384), (4682, 222111), (2030, 497571), (5234, 202177), (2665, 369855), (757, 1071575), (4370, 210982), (4895, 169946), (1085, 862457), (705, 1548305), (4508, 172897), (167, 5559739), (1112, 892303), (2193, 392132), (1396, 545283), (4374, 230866), (3385, 191872), (5816, 194070), (2358, 427447), (4191, 233374), (646, 1377071), (1790, 550995), (4264, 247591), (22, 38484130), (2415, 239286), (2392, 349952), (4190, 205854), (2833, 327549), (5032, 221170), (6353, 168404), (5324, 191543), (1518, 582487), (1894, 481117), (3228, 315556), (2265, 421208), (636, 1378362), (6548, 161733), (6098, 172422), (696, 1306625), (3452, 300052), (717, 1408825), (573, 1561543), (4562, 160268), (4277, 195942), (5640, 186094), (2551, 176659), (1249, 573784), (631, 1412370), (4905, 184294), (877, 1019947), (1758, 490726), (767, 1164520), (2558, 157757), (518, 1170336), (1534, 393440), (3774, 232302), (1269, 737181), (6699, 164021), (2888, 375431), (6496, 161755), (4973, 157902), (348, 1943901)]
03/01/2022 18:00:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:00:09 - INFO - __main__ - Starting training!
03/01/2022 18:00:12 - INFO - __main__ - Step 10 Global step 10 Train loss 4.72 on epoch=4
03/01/2022 18:00:14 - INFO - __main__ - Step 20 Global step 20 Train loss 3.93 on epoch=9
03/01/2022 18:00:17 - INFO - __main__ - Step 30 Global step 30 Train loss 3.19 on epoch=14
03/01/2022 18:00:19 - INFO - __main__ - Step 40 Global step 40 Train loss 2.96 on epoch=19
03/01/2022 18:00:21 - INFO - __main__ - Step 50 Global step 50 Train loss 2.78 on epoch=24
03/01/2022 18:00:23 - INFO - __main__ - Global step 50 Train loss 3.52 EM 0.0 on epoch=24
03/01/2022 18:00:23 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 18:00:25 - INFO - __main__ - Step 60 Global step 60 Train loss 2.69 on epoch=29
03/01/2022 18:00:27 - INFO - __main__ - Step 70 Global step 70 Train loss 2.67 on epoch=34
03/01/2022 18:00:30 - INFO - __main__ - Step 80 Global step 80 Train loss 2.56 on epoch=39
03/01/2022 18:00:32 - INFO - __main__ - Step 90 Global step 90 Train loss 2.39 on epoch=44
03/01/2022 18:00:34 - INFO - __main__ - Step 100 Global step 100 Train loss 2.33 on epoch=49
03/01/2022 18:00:36 - INFO - __main__ - Global step 100 Train loss 2.53 EM 0.03125 on epoch=49
03/01/2022 18:00:36 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=49, global_step=100
03/01/2022 18:00:38 - INFO - __main__ - Step 110 Global step 110 Train loss 2.27 on epoch=54
03/01/2022 18:00:40 - INFO - __main__ - Step 120 Global step 120 Train loss 2.22 on epoch=59
03/01/2022 18:00:43 - INFO - __main__ - Step 130 Global step 130 Train loss 2.18 on epoch=64
03/01/2022 18:00:45 - INFO - __main__ - Step 140 Global step 140 Train loss 2.08 on epoch=69
03/01/2022 18:00:47 - INFO - __main__ - Step 150 Global step 150 Train loss 2.03 on epoch=74
03/01/2022 18:00:48 - INFO - __main__ - Global step 150 Train loss 2.16 EM 0.0 on epoch=74
03/01/2022 18:00:51 - INFO - __main__ - Step 160 Global step 160 Train loss 2.07 on epoch=79
03/01/2022 18:00:53 - INFO - __main__ - Step 170 Global step 170 Train loss 2.01 on epoch=84
03/01/2022 18:00:55 - INFO - __main__ - Step 180 Global step 180 Train loss 1.90 on epoch=89
03/01/2022 18:00:58 - INFO - __main__ - Step 190 Global step 190 Train loss 1.94 on epoch=94
03/01/2022 18:01:00 - INFO - __main__ - Step 200 Global step 200 Train loss 1.88 on epoch=99
03/01/2022 18:01:01 - INFO - __main__ - Global step 200 Train loss 1.96 EM 0.0 on epoch=99
03/01/2022 18:01:03 - INFO - __main__ - Step 210 Global step 210 Train loss 1.91 on epoch=104
03/01/2022 18:01:06 - INFO - __main__ - Step 220 Global step 220 Train loss 1.85 on epoch=109
03/01/2022 18:01:08 - INFO - __main__ - Step 230 Global step 230 Train loss 1.83 on epoch=114
03/01/2022 18:01:10 - INFO - __main__ - Step 240 Global step 240 Train loss 1.80 on epoch=119
03/01/2022 18:01:13 - INFO - __main__ - Step 250 Global step 250 Train loss 1.87 on epoch=124
03/01/2022 18:01:14 - INFO - __main__ - Global step 250 Train loss 1.85 EM 0.0 on epoch=124
03/01/2022 18:01:16 - INFO - __main__ - Step 260 Global step 260 Train loss 1.78 on epoch=129
03/01/2022 18:01:18 - INFO - __main__ - Step 270 Global step 270 Train loss 1.73 on epoch=134
03/01/2022 18:01:21 - INFO - __main__ - Step 280 Global step 280 Train loss 1.65 on epoch=139
03/01/2022 18:01:23 - INFO - __main__ - Step 290 Global step 290 Train loss 1.68 on epoch=144
03/01/2022 18:01:25 - INFO - __main__ - Step 300 Global step 300 Train loss 1.60 on epoch=149
03/01/2022 18:01:27 - INFO - __main__ - Global step 300 Train loss 1.69 EM 0.0 on epoch=149
03/01/2022 18:01:29 - INFO - __main__ - Step 310 Global step 310 Train loss 1.65 on epoch=154
03/01/2022 18:01:31 - INFO - __main__ - Step 320 Global step 320 Train loss 1.64 on epoch=159
03/01/2022 18:01:34 - INFO - __main__ - Step 330 Global step 330 Train loss 1.54 on epoch=164
03/01/2022 18:01:36 - INFO - __main__ - Step 340 Global step 340 Train loss 1.50 on epoch=169
03/01/2022 18:01:38 - INFO - __main__ - Step 350 Global step 350 Train loss 1.48 on epoch=174
03/01/2022 18:01:39 - INFO - __main__ - Global step 350 Train loss 1.56 EM 0.0 on epoch=174
03/01/2022 18:01:42 - INFO - __main__ - Step 360 Global step 360 Train loss 1.48 on epoch=179
03/01/2022 18:01:44 - INFO - __main__ - Step 370 Global step 370 Train loss 1.49 on epoch=184
03/01/2022 18:01:46 - INFO - __main__ - Step 380 Global step 380 Train loss 1.42 on epoch=189
03/01/2022 18:01:48 - INFO - __main__ - Step 390 Global step 390 Train loss 1.41 on epoch=194
03/01/2022 18:01:51 - INFO - __main__ - Step 400 Global step 400 Train loss 1.48 on epoch=199
03/01/2022 18:01:52 - INFO - __main__ - Global step 400 Train loss 1.46 EM 0.0 on epoch=199
03/01/2022 18:01:54 - INFO - __main__ - Step 410 Global step 410 Train loss 1.36 on epoch=204
03/01/2022 18:01:56 - INFO - __main__ - Step 420 Global step 420 Train loss 1.29 on epoch=209
03/01/2022 18:01:59 - INFO - __main__ - Step 430 Global step 430 Train loss 1.35 on epoch=214
03/01/2022 18:02:01 - INFO - __main__ - Step 440 Global step 440 Train loss 1.33 on epoch=219
03/01/2022 18:02:03 - INFO - __main__ - Step 450 Global step 450 Train loss 1.25 on epoch=224
03/01/2022 18:02:04 - INFO - __main__ - Global step 450 Train loss 1.32 EM 0.0 on epoch=224
03/01/2022 18:02:06 - INFO - __main__ - Step 460 Global step 460 Train loss 1.28 on epoch=229
03/01/2022 18:02:09 - INFO - __main__ - Step 470 Global step 470 Train loss 1.34 on epoch=234
03/01/2022 18:02:11 - INFO - __main__ - Step 480 Global step 480 Train loss 1.27 on epoch=239
03/01/2022 18:02:13 - INFO - __main__ - Step 490 Global step 490 Train loss 1.17 on epoch=244
03/01/2022 18:02:16 - INFO - __main__ - Step 500 Global step 500 Train loss 1.22 on epoch=249
03/01/2022 18:02:17 - INFO - __main__ - Global step 500 Train loss 1.26 EM 0.0 on epoch=249
03/01/2022 18:02:19 - INFO - __main__ - Step 510 Global step 510 Train loss 1.21 on epoch=254
03/01/2022 18:02:21 - INFO - __main__ - Step 520 Global step 520 Train loss 1.10 on epoch=259
03/01/2022 18:02:24 - INFO - __main__ - Step 530 Global step 530 Train loss 1.24 on epoch=264
03/01/2022 18:02:26 - INFO - __main__ - Step 540 Global step 540 Train loss 1.19 on epoch=269
03/01/2022 18:02:28 - INFO - __main__ - Step 550 Global step 550 Train loss 1.09 on epoch=274
03/01/2022 18:02:29 - INFO - __main__ - Global step 550 Train loss 1.17 EM 0.0 on epoch=274
03/01/2022 18:02:32 - INFO - __main__ - Step 560 Global step 560 Train loss 1.10 on epoch=279
03/01/2022 18:02:34 - INFO - __main__ - Step 570 Global step 570 Train loss 1.06 on epoch=284
03/01/2022 18:02:36 - INFO - __main__ - Step 580 Global step 580 Train loss 1.09 on epoch=289
03/01/2022 18:02:39 - INFO - __main__ - Step 590 Global step 590 Train loss 1.02 on epoch=294
03/01/2022 18:02:41 - INFO - __main__ - Step 600 Global step 600 Train loss 1.15 on epoch=299
03/01/2022 18:02:42 - INFO - __main__ - Global step 600 Train loss 1.09 EM 0.0 on epoch=299
03/01/2022 18:02:44 - INFO - __main__ - Step 610 Global step 610 Train loss 1.07 on epoch=304
03/01/2022 18:02:47 - INFO - __main__ - Step 620 Global step 620 Train loss 1.00 on epoch=309
03/01/2022 18:02:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.97 on epoch=314
03/01/2022 18:02:51 - INFO - __main__ - Step 640 Global step 640 Train loss 1.01 on epoch=319
03/01/2022 18:02:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.93 on epoch=324
03/01/2022 18:02:55 - INFO - __main__ - Global step 650 Train loss 1.00 EM 0.0 on epoch=324
03/01/2022 18:02:57 - INFO - __main__ - Step 660 Global step 660 Train loss 1.01 on epoch=329
03/01/2022 18:02:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.88 on epoch=334
03/01/2022 18:03:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.97 on epoch=339
03/01/2022 18:03:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.91 on epoch=344
03/01/2022 18:03:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.90 on epoch=349
03/01/2022 18:03:08 - INFO - __main__ - Global step 700 Train loss 0.93 EM 0.0 on epoch=349
03/01/2022 18:03:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.95 on epoch=354
03/01/2022 18:03:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.94 on epoch=359
03/01/2022 18:03:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.94 on epoch=364
03/01/2022 18:03:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.91 on epoch=369
03/01/2022 18:03:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.84 on epoch=374
03/01/2022 18:03:20 - INFO - __main__ - Global step 750 Train loss 0.92 EM 0.0 on epoch=374
03/01/2022 18:03:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.90 on epoch=379
03/01/2022 18:03:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.82 on epoch=384
03/01/2022 18:03:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.90 on epoch=389
03/01/2022 18:03:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.82 on epoch=394
03/01/2022 18:03:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.81 on epoch=399
03/01/2022 18:03:33 - INFO - __main__ - Global step 800 Train loss 0.85 EM 0.0 on epoch=399
03/01/2022 18:03:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.87 on epoch=404
03/01/2022 18:03:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.89 on epoch=409
03/01/2022 18:03:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.89 on epoch=414
03/01/2022 18:03:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.85 on epoch=419
03/01/2022 18:03:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.80 on epoch=424
03/01/2022 18:03:46 - INFO - __main__ - Global step 850 Train loss 0.86 EM 0.0 on epoch=424
03/01/2022 18:03:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.84 on epoch=429
03/01/2022 18:03:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.80 on epoch=434
03/01/2022 18:03:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.79 on epoch=439
03/01/2022 18:03:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.79 on epoch=444
03/01/2022 18:03:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.78 on epoch=449
03/01/2022 18:03:59 - INFO - __main__ - Global step 900 Train loss 0.80 EM 0.0 on epoch=449
03/01/2022 18:04:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.76 on epoch=454
03/01/2022 18:04:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.71 on epoch=459
03/01/2022 18:04:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.73 on epoch=464
03/01/2022 18:04:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.74 on epoch=469
03/01/2022 18:04:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.73 on epoch=474
03/01/2022 18:04:12 - INFO - __main__ - Global step 950 Train loss 0.74 EM 0.0 on epoch=474
03/01/2022 18:04:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.74 on epoch=479
03/01/2022 18:04:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.77 on epoch=484
03/01/2022 18:04:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.74 on epoch=489
03/01/2022 18:04:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.69 on epoch=494
03/01/2022 18:04:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.70 on epoch=499
03/01/2022 18:04:24 - INFO - __main__ - Global step 1000 Train loss 0.73 EM 0.0 on epoch=499
03/01/2022 18:04:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.61 on epoch=504
03/01/2022 18:04:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.67 on epoch=509
03/01/2022 18:04:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.69 on epoch=514
03/01/2022 18:04:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.65 on epoch=519
03/01/2022 18:04:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.71 on epoch=524
03/01/2022 18:04:37 - INFO - __main__ - Global step 1050 Train loss 0.66 EM 0.0 on epoch=524
03/01/2022 18:04:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.63 on epoch=529
03/01/2022 18:04:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.66 on epoch=534
03/01/2022 18:04:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.63 on epoch=539
03/01/2022 18:04:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.59 on epoch=544
03/01/2022 18:04:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.57 on epoch=549
03/01/2022 18:04:50 - INFO - __main__ - Global step 1100 Train loss 0.62 EM 0.0 on epoch=549
03/01/2022 18:04:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.60 on epoch=554
03/01/2022 18:04:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.56 on epoch=559
03/01/2022 18:04:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.58 on epoch=564
03/01/2022 18:04:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.68 on epoch=569
03/01/2022 18:05:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.56 on epoch=574
03/01/2022 18:05:03 - INFO - __main__ - Global step 1150 Train loss 0.60 EM 0.0 on epoch=574
03/01/2022 18:05:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.54 on epoch=579
03/01/2022 18:05:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.55 on epoch=584
03/01/2022 18:05:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.58 on epoch=589
03/01/2022 18:05:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.56 on epoch=594
03/01/2022 18:05:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.51 on epoch=599
03/01/2022 18:05:15 - INFO - __main__ - Global step 1200 Train loss 0.55 EM 0.0 on epoch=599
03/01/2022 18:05:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.55 on epoch=604
03/01/2022 18:05:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
03/01/2022 18:05:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=614
03/01/2022 18:05:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.52 on epoch=619
03/01/2022 18:05:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.49 on epoch=624
03/01/2022 18:05:28 - INFO - __main__ - Global step 1250 Train loss 0.52 EM 0.0 on epoch=624
03/01/2022 18:05:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=629
03/01/2022 18:05:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=634
03/01/2022 18:05:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=639
03/01/2022 18:05:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=644
03/01/2022 18:05:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.53 on epoch=649
03/01/2022 18:05:41 - INFO - __main__ - Global step 1300 Train loss 0.49 EM 0.0 on epoch=649
03/01/2022 18:05:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=654
03/01/2022 18:05:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=659
03/01/2022 18:05:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.48 on epoch=664
03/01/2022 18:05:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.51 on epoch=669
03/01/2022 18:05:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=674
03/01/2022 18:05:53 - INFO - __main__ - Global step 1350 Train loss 0.49 EM 0.0 on epoch=674
03/01/2022 18:05:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=679
03/01/2022 18:05:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.47 on epoch=684
03/01/2022 18:06:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.47 on epoch=689
03/01/2022 18:06:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=694
03/01/2022 18:06:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.52 on epoch=699
03/01/2022 18:06:06 - INFO - __main__ - Global step 1400 Train loss 0.46 EM 0.0 on epoch=699
03/01/2022 18:06:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=704
03/01/2022 18:06:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=709
03/01/2022 18:06:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=714
03/01/2022 18:06:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=719
03/01/2022 18:06:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=724
03/01/2022 18:06:19 - INFO - __main__ - Global step 1450 Train loss 0.40 EM 0.0 on epoch=724
03/01/2022 18:06:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.48 on epoch=729
03/01/2022 18:06:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.38 on epoch=734
03/01/2022 18:06:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=739
03/01/2022 18:06:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=744
03/01/2022 18:06:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=749
03/01/2022 18:06:32 - INFO - __main__ - Global step 1500 Train loss 0.40 EM 0.0 on epoch=749
03/01/2022 18:06:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=754
03/01/2022 18:06:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=759
03/01/2022 18:06:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=764
03/01/2022 18:06:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=769
03/01/2022 18:06:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=774
03/01/2022 18:06:44 - INFO - __main__ - Global step 1550 Train loss 0.36 EM 0.0 on epoch=774
03/01/2022 18:06:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=779
03/01/2022 18:06:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.34 on epoch=784
03/01/2022 18:06:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=789
03/01/2022 18:06:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=794
03/01/2022 18:06:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=799
03/01/2022 18:06:57 - INFO - __main__ - Global step 1600 Train loss 0.36 EM 0.0 on epoch=799
03/01/2022 18:06:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=804
03/01/2022 18:07:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=809
03/01/2022 18:07:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=814
03/01/2022 18:07:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=819
03/01/2022 18:07:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=824
03/01/2022 18:07:10 - INFO - __main__ - Global step 1650 Train loss 0.36 EM 0.0 on epoch=824
03/01/2022 18:07:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=829
03/01/2022 18:07:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=834
03/01/2022 18:07:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=839
03/01/2022 18:07:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=844
03/01/2022 18:07:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=849
03/01/2022 18:07:23 - INFO - __main__ - Global step 1700 Train loss 0.32 EM 0.0 on epoch=849
03/01/2022 18:07:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.27 on epoch=854
03/01/2022 18:07:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=859
03/01/2022 18:07:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=864
03/01/2022 18:07:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=869
03/01/2022 18:07:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=874
03/01/2022 18:07:35 - INFO - __main__ - Global step 1750 Train loss 0.30 EM 0.0 on epoch=874
03/01/2022 18:07:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=879
03/01/2022 18:07:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=884
03/01/2022 18:07:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=889
03/01/2022 18:07:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.30 on epoch=894
03/01/2022 18:07:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.29 on epoch=899
03/01/2022 18:07:48 - INFO - __main__ - Global step 1800 Train loss 0.28 EM 0.0 on epoch=899
03/01/2022 18:07:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=904
03/01/2022 18:07:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
03/01/2022 18:07:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=914
03/01/2022 18:07:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=919
03/01/2022 18:08:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=924
03/01/2022 18:08:01 - INFO - __main__ - Global step 1850 Train loss 0.30 EM 0.0 on epoch=924
03/01/2022 18:08:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=929
03/01/2022 18:08:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.26 on epoch=934
03/01/2022 18:08:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=939
03/01/2022 18:08:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=944
03/01/2022 18:08:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.23 on epoch=949
03/01/2022 18:08:14 - INFO - __main__ - Global step 1900 Train loss 0.24 EM 0.0 on epoch=949
03/01/2022 18:08:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.24 on epoch=954
03/01/2022 18:08:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=959
03/01/2022 18:08:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=964
03/01/2022 18:08:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=969
03/01/2022 18:08:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=974
03/01/2022 18:08:26 - INFO - __main__ - Global step 1950 Train loss 0.24 EM 0.0 on epoch=974
03/01/2022 18:08:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=979
03/01/2022 18:08:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=984
03/01/2022 18:08:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.25 on epoch=989
03/01/2022 18:08:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=994
03/01/2022 18:08:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=999
03/01/2022 18:08:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:08:39 - INFO - __main__ - Printing 3 examples
03/01/2022 18:08:39 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 18:08:39 - INFO - __main__ - ['orange is the new black']
03/01/2022 18:08:39 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 18:08:39 - INFO - __main__ - ['western australia']
03/01/2022 18:08:39 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 18:08:39 - INFO - __main__ - ['turkey']
03/01/2022 18:08:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 18:08:39 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:08:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:08:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:08:39 - INFO - __main__ - Printing 3 examples
03/01/2022 18:08:39 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 18:08:39 - INFO - __main__ - ['benito mussolini']
03/01/2022 18:08:39 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 18:08:39 - INFO - __main__ - ['the kinks']
03/01/2022 18:08:39 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 18:08:39 - INFO - __main__ - ['saigon']
03/01/2022 18:08:39 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:08:39 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:08:39 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:08:39 - INFO - __main__ - Global step 2000 Train loss 0.21 EM 0.0 on epoch=999
03/01/2022 18:08:39 - INFO - __main__ - save last model!
03/01/2022 18:08:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 18:08:39 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 18:08:39 - INFO - __main__ - Printing 3 examples
03/01/2022 18:08:39 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 18:08:39 - INFO - __main__ - ['taming of the shrew']
03/01/2022 18:08:39 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 18:08:39 - INFO - __main__ - ['henry fonda']
03/01/2022 18:08:39 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 18:08:39 - INFO - __main__ - ['tchaikovsky']
03/01/2022 18:08:39 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:08:41 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:08:45 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 18:08:53 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 18:08:53 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(907, 999819), (4328, 223698), (7511, 160346), (5954, 165280), (4511, 179764), (3531, 297895), (2852, 355056), (1714, 447470), (3237, 299613), (2554, 389221), (1462, 349360), (5120, 236973), (1511, 676741), (2846, 312704), (4848, 204967), (3155, 298023), (976, 1102538), (99, 8503604), (4345, 204361), (5077, 166084), (1077, 853741), (930, 1093140), (5712, 173941), (2441, 429114), (5696, 184838), (2392, 349952), (1363, 669374), (2856, 268051), (554, 1156528), (2131, 378214), (3105, 254084), (3315, 327974), (1735, 571995), (4183, 213298), (6331, 171986), (3243, 311061), (5189, 177490), (91, 9930046), (4320, 168819), (2577, 313511), (1273, 833860), (1466, 664308), (3001, 351252), (955, 764542), (1170, 817452), (6309, 169174), (986, 581568), (2381, 341017), (3127, 331271), (7594, 155434), (2456, 433793), (3716, 273699), (2024, 448261), (2411, 351989), (5529, 185095), (3351, 190868), (2717, 368013), (580, 1542140), (1019, 901860), (4585, 221032), (2887, 336279), (514, 1732295), (42, 24506569), (3827, 282190), (1368, 677011), (2835, 354320), (1751, 560750), (4764, 211573), (6349, 169524), (4554, 218517), (6817, 166736), (103, 8154869), (1142, 782542), (809, 1073970), (1651, 598254), (5782, 171468), (616, 2195072), (4145, 254483), (6957, 160765), (6212, 169413), (1519, 600961), (4894, 217353), (944, 693547), (124, 1842230), (2122, 305087), (3931, 257603), (453, 1935608), (4016, 262231), (1677, 434886), (2591, 370033), (953, 840707), (1022, 264436), (4487, 166978), (4374, 230866), (1499, 582683), (2605, 391810), (1675, 558878), (948, 512142), (4647, 228104)]
03/01/2022 18:08:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:08:54 - INFO - __main__ - Starting training!
03/01/2022 18:11:10 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_100_0.3_8_predictions.txt
03/01/2022 18:11:10 - INFO - __main__ - EM on test data: 0.0053
03/01/2022 18:11:10 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.3, bsz=8, dev_performance=0.03125, test_performance=0.005257886830245368
03/01/2022 18:11:10 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.2, bsz=8 ...
03/01/2022 18:11:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:11:11 - INFO - __main__ - Printing 3 examples
03/01/2022 18:11:11 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 18:11:11 - INFO - __main__ - ['orange is the new black']
03/01/2022 18:11:11 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 18:11:11 - INFO - __main__ - ['western australia']
03/01/2022 18:11:11 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 18:11:11 - INFO - __main__ - ['turkey']
03/01/2022 18:11:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 18:11:11 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:11:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:11:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:11:11 - INFO - __main__ - Printing 3 examples
03/01/2022 18:11:11 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 18:11:11 - INFO - __main__ - ['benito mussolini']
03/01/2022 18:11:11 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 18:11:11 - INFO - __main__ - ['the kinks']
03/01/2022 18:11:11 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 18:11:11 - INFO - __main__ - ['saigon']
03/01/2022 18:11:11 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:11:11 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:11:11 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:11:25 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 18:11:25 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(907, 999819), (4328, 223698), (7511, 160346), (5954, 165280), (4511, 179764), (3531, 297895), (2852, 355056), (1714, 447470), (3237, 299613), (2554, 389221), (1462, 349360), (5120, 236973), (1511, 676741), (2846, 312704), (4848, 204967), (3155, 298023), (976, 1102538), (99, 8503604), (4345, 204361), (5077, 166084), (1077, 853741), (930, 1093140), (5712, 173941), (2441, 429114), (5696, 184838), (2392, 349952), (1363, 669374), (2856, 268051), (554, 1156528), (2131, 378214), (3105, 254084), (3315, 327974), (1735, 571995), (4183, 213298), (6331, 171986), (3243, 311061), (5189, 177490), (91, 9930046), (4320, 168819), (2577, 313511), (1273, 833860), (1466, 664308), (3001, 351252), (955, 764542), (1170, 817452), (6309, 169174), (986, 581568), (2381, 341017), (3127, 331271), (7594, 155434), (2456, 433793), (3716, 273699), (2024, 448261), (2411, 351989), (5529, 185095), (3351, 190868), (2717, 368013), (580, 1542140), (1019, 901860), (4585, 221032), (2887, 336279), (514, 1732295), (42, 24506569), (3827, 282190), (1368, 677011), (2835, 354320), (1751, 560750), (4764, 211573), (6349, 169524), (4554, 218517), (6817, 166736), (103, 8154869), (1142, 782542), (809, 1073970), (1651, 598254), (5782, 171468), (616, 2195072), (4145, 254483), (6957, 160765), (6212, 169413), (1519, 600961), (4894, 217353), (944, 693547), (124, 1842230), (2122, 305087), (3931, 257603), (453, 1935608), (4016, 262231), (1677, 434886), (2591, 370033), (953, 840707), (1022, 264436), (4487, 166978), (4374, 230866), (1499, 582683), (2605, 391810), (1675, 558878), (948, 512142), (4647, 228104)]
03/01/2022 18:11:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:11:26 - INFO - __main__ - Starting training!
03/01/2022 18:11:28 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
03/01/2022 18:11:30 - INFO - __main__ - Step 20 Global step 20 Train loss 4.54 on epoch=9
03/01/2022 18:11:33 - INFO - __main__ - Step 30 Global step 30 Train loss 4.35 on epoch=14
03/01/2022 18:11:35 - INFO - __main__ - Step 40 Global step 40 Train loss 4.01 on epoch=19
03/01/2022 18:11:37 - INFO - __main__ - Step 50 Global step 50 Train loss 3.62 on epoch=24
03/01/2022 18:11:41 - INFO - __main__ - Global step 50 Train loss 4.31 EM 0.0 on epoch=24
03/01/2022 18:11:41 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 18:11:44 - INFO - __main__ - Step 60 Global step 60 Train loss 3.27 on epoch=29
03/01/2022 18:11:46 - INFO - __main__ - Step 70 Global step 70 Train loss 3.11 on epoch=34
03/01/2022 18:11:48 - INFO - __main__ - Step 80 Global step 80 Train loss 2.93 on epoch=39
03/01/2022 18:11:51 - INFO - __main__ - Step 90 Global step 90 Train loss 2.84 on epoch=44
03/01/2022 18:11:53 - INFO - __main__ - Step 100 Global step 100 Train loss 2.74 on epoch=49
03/01/2022 18:11:54 - INFO - __main__ - Global step 100 Train loss 2.98 EM 0.0 on epoch=49
03/01/2022 18:11:57 - INFO - __main__ - Step 110 Global step 110 Train loss 2.63 on epoch=54
03/01/2022 18:11:59 - INFO - __main__ - Step 120 Global step 120 Train loss 2.55 on epoch=59
03/01/2022 18:12:01 - INFO - __main__ - Step 130 Global step 130 Train loss 2.51 on epoch=64
03/01/2022 18:12:04 - INFO - __main__ - Step 140 Global step 140 Train loss 2.35 on epoch=69
03/01/2022 18:12:06 - INFO - __main__ - Step 150 Global step 150 Train loss 2.39 on epoch=74
03/01/2022 18:12:07 - INFO - __main__ - Global step 150 Train loss 2.49 EM 0.0 on epoch=74
03/01/2022 18:12:09 - INFO - __main__ - Step 160 Global step 160 Train loss 2.32 on epoch=79
03/01/2022 18:12:12 - INFO - __main__ - Step 170 Global step 170 Train loss 2.25 on epoch=84
03/01/2022 18:12:14 - INFO - __main__ - Step 180 Global step 180 Train loss 2.26 on epoch=89
03/01/2022 18:12:16 - INFO - __main__ - Step 190 Global step 190 Train loss 2.19 on epoch=94
03/01/2022 18:12:18 - INFO - __main__ - Step 200 Global step 200 Train loss 2.23 on epoch=99
03/01/2022 18:12:20 - INFO - __main__ - Global step 200 Train loss 2.25 EM 0.0 on epoch=99
03/01/2022 18:12:22 - INFO - __main__ - Step 210 Global step 210 Train loss 2.16 on epoch=104
03/01/2022 18:12:24 - INFO - __main__ - Step 220 Global step 220 Train loss 2.10 on epoch=109
03/01/2022 18:12:27 - INFO - __main__ - Step 230 Global step 230 Train loss 2.12 on epoch=114
03/01/2022 18:12:29 - INFO - __main__ - Step 240 Global step 240 Train loss 2.09 on epoch=119
03/01/2022 18:12:31 - INFO - __main__ - Step 250 Global step 250 Train loss 2.12 on epoch=124
03/01/2022 18:12:32 - INFO - __main__ - Global step 250 Train loss 2.12 EM 0.0 on epoch=124
03/01/2022 18:12:35 - INFO - __main__ - Step 260 Global step 260 Train loss 1.98 on epoch=129
03/01/2022 18:12:37 - INFO - __main__ - Step 270 Global step 270 Train loss 2.09 on epoch=134
03/01/2022 18:12:39 - INFO - __main__ - Step 280 Global step 280 Train loss 2.02 on epoch=139
03/01/2022 18:12:42 - INFO - __main__ - Step 290 Global step 290 Train loss 1.95 on epoch=144
03/01/2022 18:12:44 - INFO - __main__ - Step 300 Global step 300 Train loss 1.99 on epoch=149
03/01/2022 18:12:45 - INFO - __main__ - Global step 300 Train loss 2.01 EM 0.0 on epoch=149
03/01/2022 18:12:47 - INFO - __main__ - Step 310 Global step 310 Train loss 1.90 on epoch=154
03/01/2022 18:12:50 - INFO - __main__ - Step 320 Global step 320 Train loss 1.85 on epoch=159
03/01/2022 18:12:52 - INFO - __main__ - Step 330 Global step 330 Train loss 1.91 on epoch=164
03/01/2022 18:12:54 - INFO - __main__ - Step 340 Global step 340 Train loss 1.89 on epoch=169
03/01/2022 18:12:57 - INFO - __main__ - Step 350 Global step 350 Train loss 1.85 on epoch=174
03/01/2022 18:12:58 - INFO - __main__ - Global step 350 Train loss 1.88 EM 0.0 on epoch=174
03/01/2022 18:13:00 - INFO - __main__ - Step 360 Global step 360 Train loss 1.76 on epoch=179
03/01/2022 18:13:02 - INFO - __main__ - Step 370 Global step 370 Train loss 1.81 on epoch=184
03/01/2022 18:13:04 - INFO - __main__ - Step 380 Global step 380 Train loss 1.74 on epoch=189
03/01/2022 18:13:07 - INFO - __main__ - Step 390 Global step 390 Train loss 1.70 on epoch=194
03/01/2022 18:13:09 - INFO - __main__ - Step 400 Global step 400 Train loss 1.73 on epoch=199
03/01/2022 18:13:10 - INFO - __main__ - Global step 400 Train loss 1.75 EM 0.0 on epoch=199
03/01/2022 18:13:13 - INFO - __main__ - Step 410 Global step 410 Train loss 1.77 on epoch=204
03/01/2022 18:13:15 - INFO - __main__ - Step 420 Global step 420 Train loss 1.68 on epoch=209
03/01/2022 18:13:17 - INFO - __main__ - Step 430 Global step 430 Train loss 1.66 on epoch=214
03/01/2022 18:13:19 - INFO - __main__ - Step 440 Global step 440 Train loss 1.66 on epoch=219
03/01/2022 18:13:22 - INFO - __main__ - Step 450 Global step 450 Train loss 1.61 on epoch=224
03/01/2022 18:13:23 - INFO - __main__ - Global step 450 Train loss 1.68 EM 0.0 on epoch=224
03/01/2022 18:13:25 - INFO - __main__ - Step 460 Global step 460 Train loss 1.53 on epoch=229
03/01/2022 18:13:27 - INFO - __main__ - Step 470 Global step 470 Train loss 1.51 on epoch=234
03/01/2022 18:13:30 - INFO - __main__ - Step 480 Global step 480 Train loss 1.61 on epoch=239
03/01/2022 18:13:32 - INFO - __main__ - Step 490 Global step 490 Train loss 1.59 on epoch=244
03/01/2022 18:13:34 - INFO - __main__ - Step 500 Global step 500 Train loss 1.56 on epoch=249
03/01/2022 18:13:35 - INFO - __main__ - Global step 500 Train loss 1.56 EM 0.0 on epoch=249
03/01/2022 18:13:38 - INFO - __main__ - Step 510 Global step 510 Train loss 1.53 on epoch=254
03/01/2022 18:13:40 - INFO - __main__ - Step 520 Global step 520 Train loss 1.53 on epoch=259
03/01/2022 18:13:42 - INFO - __main__ - Step 530 Global step 530 Train loss 1.59 on epoch=264
03/01/2022 18:13:45 - INFO - __main__ - Step 540 Global step 540 Train loss 1.46 on epoch=269
03/01/2022 18:13:47 - INFO - __main__ - Step 550 Global step 550 Train loss 1.43 on epoch=274
03/01/2022 18:13:48 - INFO - __main__ - Global step 550 Train loss 1.51 EM 0.0 on epoch=274
03/01/2022 18:13:50 - INFO - __main__ - Step 560 Global step 560 Train loss 1.43 on epoch=279
03/01/2022 18:13:53 - INFO - __main__ - Step 570 Global step 570 Train loss 1.47 on epoch=284
03/01/2022 18:13:55 - INFO - __main__ - Step 580 Global step 580 Train loss 1.46 on epoch=289
03/01/2022 18:13:57 - INFO - __main__ - Step 590 Global step 590 Train loss 1.44 on epoch=294
03/01/2022 18:14:00 - INFO - __main__ - Step 600 Global step 600 Train loss 1.37 on epoch=299
03/01/2022 18:14:01 - INFO - __main__ - Global step 600 Train loss 1.43 EM 0.0 on epoch=299
03/01/2022 18:14:03 - INFO - __main__ - Step 610 Global step 610 Train loss 1.45 on epoch=304
03/01/2022 18:14:05 - INFO - __main__ - Step 620 Global step 620 Train loss 1.36 on epoch=309
03/01/2022 18:14:08 - INFO - __main__ - Step 630 Global step 630 Train loss 1.32 on epoch=314
03/01/2022 18:14:10 - INFO - __main__ - Step 640 Global step 640 Train loss 1.31 on epoch=319
03/01/2022 18:14:12 - INFO - __main__ - Step 650 Global step 650 Train loss 1.22 on epoch=324
03/01/2022 18:14:13 - INFO - __main__ - Global step 650 Train loss 1.33 EM 0.0 on epoch=324
03/01/2022 18:14:16 - INFO - __main__ - Step 660 Global step 660 Train loss 1.40 on epoch=329
03/01/2022 18:14:18 - INFO - __main__ - Step 670 Global step 670 Train loss 1.34 on epoch=334
03/01/2022 18:14:20 - INFO - __main__ - Step 680 Global step 680 Train loss 1.24 on epoch=339
03/01/2022 18:14:22 - INFO - __main__ - Step 690 Global step 690 Train loss 1.22 on epoch=344
03/01/2022 18:14:25 - INFO - __main__ - Step 700 Global step 700 Train loss 1.32 on epoch=349
03/01/2022 18:14:26 - INFO - __main__ - Global step 700 Train loss 1.31 EM 0.0 on epoch=349
03/01/2022 18:14:28 - INFO - __main__ - Step 710 Global step 710 Train loss 1.23 on epoch=354
03/01/2022 18:14:31 - INFO - __main__ - Step 720 Global step 720 Train loss 1.19 on epoch=359
03/01/2022 18:14:33 - INFO - __main__ - Step 730 Global step 730 Train loss 1.22 on epoch=364
03/01/2022 18:14:35 - INFO - __main__ - Step 740 Global step 740 Train loss 1.20 on epoch=369
03/01/2022 18:14:37 - INFO - __main__ - Step 750 Global step 750 Train loss 1.28 on epoch=374
03/01/2022 18:14:39 - INFO - __main__ - Global step 750 Train loss 1.22 EM 0.0 on epoch=374
03/01/2022 18:14:41 - INFO - __main__ - Step 760 Global step 760 Train loss 1.18 on epoch=379
03/01/2022 18:14:43 - INFO - __main__ - Step 770 Global step 770 Train loss 1.13 on epoch=384
03/01/2022 18:14:45 - INFO - __main__ - Step 780 Global step 780 Train loss 1.15 on epoch=389
03/01/2022 18:14:48 - INFO - __main__ - Step 790 Global step 790 Train loss 1.07 on epoch=394
03/01/2022 18:14:50 - INFO - __main__ - Step 800 Global step 800 Train loss 1.12 on epoch=399
03/01/2022 18:14:51 - INFO - __main__ - Global step 800 Train loss 1.13 EM 0.0 on epoch=399
03/01/2022 18:14:53 - INFO - __main__ - Step 810 Global step 810 Train loss 1.08 on epoch=404
03/01/2022 18:14:56 - INFO - __main__ - Step 820 Global step 820 Train loss 1.12 on epoch=409
03/01/2022 18:14:58 - INFO - __main__ - Step 830 Global step 830 Train loss 1.00 on epoch=414
03/01/2022 18:15:00 - INFO - __main__ - Step 840 Global step 840 Train loss 1.05 on epoch=419
03/01/2022 18:15:03 - INFO - __main__ - Step 850 Global step 850 Train loss 1.04 on epoch=424
03/01/2022 18:15:04 - INFO - __main__ - Global step 850 Train loss 1.06 EM 0.0 on epoch=424
03/01/2022 18:15:06 - INFO - __main__ - Step 860 Global step 860 Train loss 1.04 on epoch=429
03/01/2022 18:15:08 - INFO - __main__ - Step 870 Global step 870 Train loss 1.03 on epoch=434
03/01/2022 18:15:11 - INFO - __main__ - Step 880 Global step 880 Train loss 1.03 on epoch=439
03/01/2022 18:15:13 - INFO - __main__ - Step 890 Global step 890 Train loss 1.02 on epoch=444
03/01/2022 18:15:15 - INFO - __main__ - Step 900 Global step 900 Train loss 1.03 on epoch=449
03/01/2022 18:15:16 - INFO - __main__ - Global step 900 Train loss 1.03 EM 0.0 on epoch=449
03/01/2022 18:15:19 - INFO - __main__ - Step 910 Global step 910 Train loss 1.01 on epoch=454
03/01/2022 18:15:21 - INFO - __main__ - Step 920 Global step 920 Train loss 1.03 on epoch=459
03/01/2022 18:15:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.99 on epoch=464
03/01/2022 18:15:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.96 on epoch=469
03/01/2022 18:15:28 - INFO - __main__ - Step 950 Global step 950 Train loss 1.01 on epoch=474
03/01/2022 18:15:29 - INFO - __main__ - Global step 950 Train loss 1.00 EM 0.0 on epoch=474
03/01/2022 18:15:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.96 on epoch=479
03/01/2022 18:15:34 - INFO - __main__ - Step 970 Global step 970 Train loss 1.01 on epoch=484
03/01/2022 18:15:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.90 on epoch=489
03/01/2022 18:15:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.97 on epoch=494
03/01/2022 18:15:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.95 on epoch=499
03/01/2022 18:15:42 - INFO - __main__ - Global step 1000 Train loss 0.96 EM 0.0 on epoch=499
03/01/2022 18:15:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.89 on epoch=504
03/01/2022 18:15:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.89 on epoch=509
03/01/2022 18:15:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.91 on epoch=514
03/01/2022 18:15:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.82 on epoch=519
03/01/2022 18:15:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.81 on epoch=524
03/01/2022 18:15:55 - INFO - __main__ - Global step 1050 Train loss 0.87 EM 0.0 on epoch=524
03/01/2022 18:15:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.92 on epoch=529
03/01/2022 18:15:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.89 on epoch=534
03/01/2022 18:16:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.82 on epoch=539
03/01/2022 18:16:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.87 on epoch=544
03/01/2022 18:16:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.87 on epoch=549
03/01/2022 18:16:08 - INFO - __main__ - Global step 1100 Train loss 0.87 EM 0.0 on epoch=549
03/01/2022 18:16:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.79 on epoch=554
03/01/2022 18:16:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.83 on epoch=559
03/01/2022 18:16:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.85 on epoch=564
03/01/2022 18:16:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.77 on epoch=569
03/01/2022 18:16:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.86 on epoch=574
03/01/2022 18:16:20 - INFO - __main__ - Global step 1150 Train loss 0.82 EM 0.0 on epoch=574
03/01/2022 18:16:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.84 on epoch=579
03/01/2022 18:16:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.74 on epoch=584
03/01/2022 18:16:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.84 on epoch=589
03/01/2022 18:16:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.79 on epoch=594
03/01/2022 18:16:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.69 on epoch=599
03/01/2022 18:16:33 - INFO - __main__ - Global step 1200 Train loss 0.78 EM 0.0 on epoch=599
03/01/2022 18:16:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.74 on epoch=604
03/01/2022 18:16:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.74 on epoch=609
03/01/2022 18:16:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.76 on epoch=614
03/01/2022 18:16:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.69 on epoch=619
03/01/2022 18:16:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.75 on epoch=624
03/01/2022 18:16:46 - INFO - __main__ - Global step 1250 Train loss 0.73 EM 0.0 on epoch=624
03/01/2022 18:16:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.75 on epoch=629
03/01/2022 18:16:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.79 on epoch=634
03/01/2022 18:16:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.71 on epoch=639
03/01/2022 18:16:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.70 on epoch=644
03/01/2022 18:16:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.72 on epoch=649
03/01/2022 18:16:59 - INFO - __main__ - Global step 1300 Train loss 0.73 EM 0.0 on epoch=649
03/01/2022 18:17:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.72 on epoch=654
03/01/2022 18:17:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.75 on epoch=659
03/01/2022 18:17:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.70 on epoch=664
03/01/2022 18:17:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.67 on epoch=669
03/01/2022 18:17:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.65 on epoch=674
03/01/2022 18:17:11 - INFO - __main__ - Global step 1350 Train loss 0.70 EM 0.0 on epoch=674
03/01/2022 18:17:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.71 on epoch=679
03/01/2022 18:17:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.64 on epoch=684
03/01/2022 18:17:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.73 on epoch=689
03/01/2022 18:17:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.63 on epoch=694
03/01/2022 18:17:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.62 on epoch=699
03/01/2022 18:17:24 - INFO - __main__ - Global step 1400 Train loss 0.67 EM 0.0 on epoch=699
03/01/2022 18:17:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.68 on epoch=704
03/01/2022 18:17:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.69 on epoch=709
03/01/2022 18:17:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.63 on epoch=714
03/01/2022 18:17:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.70 on epoch=719
03/01/2022 18:17:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.61 on epoch=724
03/01/2022 18:17:37 - INFO - __main__ - Global step 1450 Train loss 0.66 EM 0.0 on epoch=724
03/01/2022 18:17:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.66 on epoch=729
03/01/2022 18:17:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.65 on epoch=734
03/01/2022 18:17:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.62 on epoch=739
03/01/2022 18:17:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.62 on epoch=744
03/01/2022 18:17:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.63 on epoch=749
03/01/2022 18:17:50 - INFO - __main__ - Global step 1500 Train loss 0.63 EM 0.0 on epoch=749
03/01/2022 18:17:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.55 on epoch=754
03/01/2022 18:17:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.59 on epoch=759
03/01/2022 18:17:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.60 on epoch=764
03/01/2022 18:17:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.54 on epoch=769
03/01/2022 18:18:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.57 on epoch=774
03/01/2022 18:18:02 - INFO - __main__ - Global step 1550 Train loss 0.57 EM 0.0 on epoch=774
03/01/2022 18:18:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.61 on epoch=779
03/01/2022 18:18:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=784
03/01/2022 18:18:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.58 on epoch=789
03/01/2022 18:18:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.57 on epoch=794
03/01/2022 18:18:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.58 on epoch=799
03/01/2022 18:18:15 - INFO - __main__ - Global step 1600 Train loss 0.58 EM 0.0 on epoch=799
03/01/2022 18:18:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.57 on epoch=804
03/01/2022 18:18:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.60 on epoch=809
03/01/2022 18:18:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.58 on epoch=814
03/01/2022 18:18:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.51 on epoch=819
03/01/2022 18:18:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.58 on epoch=824
03/01/2022 18:18:28 - INFO - __main__ - Global step 1650 Train loss 0.57 EM 0.0 on epoch=824
03/01/2022 18:18:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.50 on epoch=829
03/01/2022 18:18:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.55 on epoch=834
03/01/2022 18:18:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.45 on epoch=839
03/01/2022 18:18:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.58 on epoch=844
03/01/2022 18:18:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.50 on epoch=849
03/01/2022 18:18:40 - INFO - __main__ - Global step 1700 Train loss 0.52 EM 0.0 on epoch=849
03/01/2022 18:18:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.50 on epoch=854
03/01/2022 18:18:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.58 on epoch=859
03/01/2022 18:18:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=864
03/01/2022 18:18:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.49 on epoch=869
03/01/2022 18:18:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.51 on epoch=874
03/01/2022 18:18:53 - INFO - __main__ - Global step 1750 Train loss 0.51 EM 0.0 on epoch=874
03/01/2022 18:18:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=879
03/01/2022 18:18:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.52 on epoch=884
03/01/2022 18:18:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
03/01/2022 18:19:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=894
03/01/2022 18:19:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=899
03/01/2022 18:19:05 - INFO - __main__ - Global step 1800 Train loss 0.45 EM 0.0 on epoch=899
03/01/2022 18:19:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.48 on epoch=904
03/01/2022 18:19:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=909
03/01/2022 18:19:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.52 on epoch=914
03/01/2022 18:19:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.50 on epoch=919
03/01/2022 18:19:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.45 on epoch=924
03/01/2022 18:19:18 - INFO - __main__ - Global step 1850 Train loss 0.48 EM 0.0 on epoch=924
03/01/2022 18:19:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
03/01/2022 18:19:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.47 on epoch=934
03/01/2022 18:19:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=939
03/01/2022 18:19:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=944
03/01/2022 18:19:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.40 on epoch=949
03/01/2022 18:19:30 - INFO - __main__ - Global step 1900 Train loss 0.45 EM 0.0 on epoch=949
03/01/2022 18:19:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=954
03/01/2022 18:19:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=959
03/01/2022 18:19:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=964
03/01/2022 18:19:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=969
03/01/2022 18:19:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=974
03/01/2022 18:19:43 - INFO - __main__ - Global step 1950 Train loss 0.41 EM 0.0 on epoch=974
03/01/2022 18:19:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
03/01/2022 18:19:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=984
03/01/2022 18:19:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=989
03/01/2022 18:19:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=994
03/01/2022 18:19:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.43 on epoch=999
03/01/2022 18:19:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:19:55 - INFO - __main__ - Printing 3 examples
03/01/2022 18:19:55 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 18:19:55 - INFO - __main__ - ['kieren fallon']
03/01/2022 18:19:55 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 18:19:55 - INFO - __main__ - ['uranus']
03/01/2022 18:19:55 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 18:19:55 - INFO - __main__ - ['rob thomas']
03/01/2022 18:19:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 18:19:55 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:19:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:19:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:19:56 - INFO - __main__ - Printing 3 examples
03/01/2022 18:19:56 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 18:19:56 - INFO - __main__ - ['terence rattigan']
03/01/2022 18:19:56 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 18:19:56 - INFO - __main__ - ['casino royale']
03/01/2022 18:19:56 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 18:19:56 - INFO - __main__ - ['offenbach']
03/01/2022 18:19:56 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:19:56 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:19:56 - INFO - __main__ - Global step 2000 Train loss 0.43 EM 0.0 on epoch=999
03/01/2022 18:19:56 - INFO - __main__ - save last model!
03/01/2022 18:19:56 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:19:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 18:19:56 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 18:19:56 - INFO - __main__ - Printing 3 examples
03/01/2022 18:19:56 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 18:19:56 - INFO - __main__ - ['taming of the shrew']
03/01/2022 18:19:56 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 18:19:56 - INFO - __main__ - ['henry fonda']
03/01/2022 18:19:56 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 18:19:56 - INFO - __main__ - ['tchaikovsky']
03/01/2022 18:19:56 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:19:57 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:20:01 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 18:20:09 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 18:20:09 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(210, 3214246), (956, 507463), (177, 283247), (2600, 228530), (2320, 373103), (2867, 348102), (1756, 567748), (1056, 811824), (3677, 244767), (451, 1996105), (4656, 199279), (2365, 411961), (2118, 458638), (4865, 202967), (3714, 267548), (2401, 307744), (2330, 404909), (2602, 411193), (281, 3369363), (2021, 474295), (4180, 234955), (3604, 286058), (3074, 334210), (799, 1149745), (2942, 333386), (3102, 266483), (6112, 183777), (4526, 234296), (6478, 174299), (1720, 283907), (4, 2855733), (1237, 765840), (2853, 354788), (1738, 508620), (6440, 161231), (2893, 349540), (429, 2065410), (1164, 807916), (6748, 157449), (4838, 214821), (3568, 270815), (3452, 300052), (4891, 214175), (3640, 155110), (2298, 183293), (1268, 763020), (3449, 243414), (6878, 162543), (1389, 480591), (2350, 363461), (530, 1727807), (4735, 214273), (24, 46921718), (3913, 180488), (5858, 173605), (2172, 463811), (3354, 258314), (3961, 216648), (778, 1141371), (3472, 284740), (3377, 299929), (1425, 711960), (3475, 312556), (2180, 184483), (2567, 304604), (355, 230108), (3404, 288313), (4344, 232964), (3047, 316651), (1753, 168938), (901, 693591), (2107, 476561), (4457, 223114), (4036, 247461), (1860, 522151), (4239, 164202), (3320, 312204), (1536, 231332), (4170, 170840), (8, 234707885), (2896, 309135), (3168, 326177), (2018, 434867), (3916, 217282), (6144, 161446), (6551, 154998), (3510, 291560), (5111, 201364), (4764, 211573), (3258, 211425), (2283, 428223), (6848, 183388), (4505, 197810), (1586, 532514), (715, 971516), (1935, 328964), (5835, 181763), (3170, 281056), (1020, 892057)]
03/01/2022 18:20:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:20:10 - INFO - __main__ - Starting training!
03/01/2022 18:22:40 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_100_0.2_8_predictions.txt
03/01/2022 18:22:41 - INFO - __main__ - EM on test data: 0.0053
03/01/2022 18:22:41 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.2, bsz=8, dev_performance=0.0, test_performance=0.005257886830245368
03/01/2022 18:22:41 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.5, bsz=8 ...
03/01/2022 18:22:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:22:42 - INFO - __main__ - Printing 3 examples
03/01/2022 18:22:42 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 18:22:42 - INFO - __main__ - ['kieren fallon']
03/01/2022 18:22:42 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 18:22:42 - INFO - __main__ - ['uranus']
03/01/2022 18:22:42 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 18:22:42 - INFO - __main__ - ['rob thomas']
03/01/2022 18:22:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 18:22:42 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:22:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:22:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:22:42 - INFO - __main__ - Printing 3 examples
03/01/2022 18:22:42 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 18:22:42 - INFO - __main__ - ['terence rattigan']
03/01/2022 18:22:42 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 18:22:42 - INFO - __main__ - ['casino royale']
03/01/2022 18:22:42 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 18:22:42 - INFO - __main__ - ['offenbach']
03/01/2022 18:22:42 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:22:42 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:22:42 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:22:56 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 18:22:56 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(210, 3214246), (956, 507463), (177, 283247), (2600, 228530), (2320, 373103), (2867, 348102), (1756, 567748), (1056, 811824), (3677, 244767), (451, 1996105), (4656, 199279), (2365, 411961), (2118, 458638), (4865, 202967), (3714, 267548), (2401, 307744), (2330, 404909), (2602, 411193), (281, 3369363), (2021, 474295), (4180, 234955), (3604, 286058), (3074, 334210), (799, 1149745), (2942, 333386), (3102, 266483), (6112, 183777), (4526, 234296), (6478, 174299), (1720, 283907), (4, 2855733), (1237, 765840), (2853, 354788), (1738, 508620), (6440, 161231), (2893, 349540), (429, 2065410), (1164, 807916), (6748, 157449), (4838, 214821), (3568, 270815), (3452, 300052), (4891, 214175), (3640, 155110), (2298, 183293), (1268, 763020), (3449, 243414), (6878, 162543), (1389, 480591), (2350, 363461), (530, 1727807), (4735, 214273), (24, 46921718), (3913, 180488), (5858, 173605), (2172, 463811), (3354, 258314), (3961, 216648), (778, 1141371), (3472, 284740), (3377, 299929), (1425, 711960), (3475, 312556), (2180, 184483), (2567, 304604), (355, 230108), (3404, 288313), (4344, 232964), (3047, 316651), (1753, 168938), (901, 693591), (2107, 476561), (4457, 223114), (4036, 247461), (1860, 522151), (4239, 164202), (3320, 312204), (1536, 231332), (4170, 170840), (8, 234707885), (2896, 309135), (3168, 326177), (2018, 434867), (3916, 217282), (6144, 161446), (6551, 154998), (3510, 291560), (5111, 201364), (4764, 211573), (3258, 211425), (2283, 428223), (6848, 183388), (4505, 197810), (1586, 532514), (715, 971516), (1935, 328964), (5835, 181763), (3170, 281056), (1020, 892057)]
03/01/2022 18:22:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:22:57 - INFO - __main__ - Starting training!
03/01/2022 18:22:59 - INFO - __main__ - Step 10 Global step 10 Train loss 4.36 on epoch=4
03/01/2022 18:23:02 - INFO - __main__ - Step 20 Global step 20 Train loss 3.60 on epoch=9
03/01/2022 18:23:04 - INFO - __main__ - Step 30 Global step 30 Train loss 3.05 on epoch=14
03/01/2022 18:23:06 - INFO - __main__ - Step 40 Global step 40 Train loss 2.76 on epoch=19
03/01/2022 18:23:09 - INFO - __main__ - Step 50 Global step 50 Train loss 2.55 on epoch=24
03/01/2022 18:23:10 - INFO - __main__ - Global step 50 Train loss 3.26 EM 0.0 on epoch=24
03/01/2022 18:23:10 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 18:23:12 - INFO - __main__ - Step 60 Global step 60 Train loss 2.41 on epoch=29
03/01/2022 18:23:14 - INFO - __main__ - Step 70 Global step 70 Train loss 2.25 on epoch=34
03/01/2022 18:23:17 - INFO - __main__ - Step 80 Global step 80 Train loss 2.20 on epoch=39
03/01/2022 18:23:19 - INFO - __main__ - Step 90 Global step 90 Train loss 1.96 on epoch=44
03/01/2022 18:23:21 - INFO - __main__ - Step 100 Global step 100 Train loss 1.90 on epoch=49
03/01/2022 18:23:23 - INFO - __main__ - Global step 100 Train loss 2.14 EM 0.0 on epoch=49
03/01/2022 18:23:25 - INFO - __main__ - Step 110 Global step 110 Train loss 1.82 on epoch=54
03/01/2022 18:23:27 - INFO - __main__ - Step 120 Global step 120 Train loss 1.74 on epoch=59
03/01/2022 18:23:29 - INFO - __main__ - Step 130 Global step 130 Train loss 1.65 on epoch=64
03/01/2022 18:23:31 - INFO - __main__ - Step 140 Global step 140 Train loss 1.57 on epoch=69
03/01/2022 18:23:34 - INFO - __main__ - Step 150 Global step 150 Train loss 1.56 on epoch=74
03/01/2022 18:23:35 - INFO - __main__ - Global step 150 Train loss 1.67 EM 0.0 on epoch=74
03/01/2022 18:23:37 - INFO - __main__ - Step 160 Global step 160 Train loss 1.46 on epoch=79
03/01/2022 18:23:40 - INFO - __main__ - Step 170 Global step 170 Train loss 1.43 on epoch=84
03/01/2022 18:23:42 - INFO - __main__ - Step 180 Global step 180 Train loss 1.31 on epoch=89
03/01/2022 18:23:44 - INFO - __main__ - Step 190 Global step 190 Train loss 1.39 on epoch=94
03/01/2022 18:23:46 - INFO - __main__ - Step 200 Global step 200 Train loss 1.25 on epoch=99
03/01/2022 18:23:48 - INFO - __main__ - Global step 200 Train loss 1.37 EM 0.0 on epoch=99
03/01/2022 18:23:50 - INFO - __main__ - Step 210 Global step 210 Train loss 1.19 on epoch=104
03/01/2022 18:23:52 - INFO - __main__ - Step 220 Global step 220 Train loss 1.23 on epoch=109
03/01/2022 18:23:54 - INFO - __main__ - Step 230 Global step 230 Train loss 1.19 on epoch=114
03/01/2022 18:23:57 - INFO - __main__ - Step 240 Global step 240 Train loss 1.11 on epoch=119
03/01/2022 18:23:59 - INFO - __main__ - Step 250 Global step 250 Train loss 1.17 on epoch=124
03/01/2022 18:24:00 - INFO - __main__ - Global step 250 Train loss 1.18 EM 0.0 on epoch=124
03/01/2022 18:24:02 - INFO - __main__ - Step 260 Global step 260 Train loss 1.05 on epoch=129
03/01/2022 18:24:05 - INFO - __main__ - Step 270 Global step 270 Train loss 1.06 on epoch=134
03/01/2022 18:24:07 - INFO - __main__ - Step 280 Global step 280 Train loss 1.01 on epoch=139
03/01/2022 18:24:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.97 on epoch=144
03/01/2022 18:24:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.97 on epoch=149
03/01/2022 18:24:12 - INFO - __main__ - Global step 300 Train loss 1.01 EM 0.0 on epoch=149
03/01/2022 18:24:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.94 on epoch=154
03/01/2022 18:24:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.96 on epoch=159
03/01/2022 18:24:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.93 on epoch=164
03/01/2022 18:24:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.94 on epoch=169
03/01/2022 18:24:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.83 on epoch=174
03/01/2022 18:24:25 - INFO - __main__ - Global step 350 Train loss 0.92 EM 0.0 on epoch=174
03/01/2022 18:24:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.86 on epoch=179
03/01/2022 18:24:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.79 on epoch=184
03/01/2022 18:24:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.79 on epoch=189
03/01/2022 18:24:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.78 on epoch=194
03/01/2022 18:24:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.81 on epoch=199
03/01/2022 18:24:37 - INFO - __main__ - Global step 400 Train loss 0.81 EM 0.0 on epoch=199
03/01/2022 18:24:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.71 on epoch=204
03/01/2022 18:24:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.69 on epoch=209
03/01/2022 18:24:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.75 on epoch=214
03/01/2022 18:24:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.69 on epoch=219
03/01/2022 18:24:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.67 on epoch=224
03/01/2022 18:24:50 - INFO - __main__ - Global step 450 Train loss 0.70 EM 0.0 on epoch=224
03/01/2022 18:24:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.69 on epoch=229
03/01/2022 18:24:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.61 on epoch=234
03/01/2022 18:24:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.63 on epoch=239
03/01/2022 18:24:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.59 on epoch=244
03/01/2022 18:25:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.56 on epoch=249
03/01/2022 18:25:03 - INFO - __main__ - Global step 500 Train loss 0.62 EM 0.0 on epoch=249
03/01/2022 18:25:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.60 on epoch=254
03/01/2022 18:25:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.60 on epoch=259
03/01/2022 18:25:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=264
03/01/2022 18:25:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=269
03/01/2022 18:25:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.57 on epoch=274
03/01/2022 18:25:15 - INFO - __main__ - Global step 550 Train loss 0.59 EM 0.0 on epoch=274
03/01/2022 18:25:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.60 on epoch=279
03/01/2022 18:25:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=284
03/01/2022 18:25:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=289
03/01/2022 18:25:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=294
03/01/2022 18:25:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=299
03/01/2022 18:25:27 - INFO - __main__ - Global step 600 Train loss 0.52 EM 0.0 on epoch=299
03/01/2022 18:25:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=304
03/01/2022 18:25:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=309
03/01/2022 18:25:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=314
03/01/2022 18:25:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=319
03/01/2022 18:25:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=324
03/01/2022 18:25:40 - INFO - __main__ - Global step 650 Train loss 0.45 EM 0.0 on epoch=324
03/01/2022 18:25:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=329
03/01/2022 18:25:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.56 on epoch=334
03/01/2022 18:25:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=339
03/01/2022 18:25:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=344
03/01/2022 18:25:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=349
03/01/2022 18:25:52 - INFO - __main__ - Global step 700 Train loss 0.45 EM 0.0 on epoch=349
03/01/2022 18:25:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.35 on epoch=354
03/01/2022 18:25:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=359
03/01/2022 18:25:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=364
03/01/2022 18:26:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=369
03/01/2022 18:26:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=374
03/01/2022 18:26:05 - INFO - __main__ - Global step 750 Train loss 0.36 EM 0.0 on epoch=374
03/01/2022 18:26:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=379
03/01/2022 18:26:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=384
03/01/2022 18:26:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.34 on epoch=389
03/01/2022 18:26:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=394
03/01/2022 18:26:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=399
03/01/2022 18:26:17 - INFO - __main__ - Global step 800 Train loss 0.36 EM 0.0 on epoch=399
03/01/2022 18:26:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=404
03/01/2022 18:26:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.29 on epoch=409
03/01/2022 18:26:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.29 on epoch=414
03/01/2022 18:26:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
03/01/2022 18:26:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=424
03/01/2022 18:26:29 - INFO - __main__ - Global step 850 Train loss 0.28 EM 0.0 on epoch=424
03/01/2022 18:26:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=429
03/01/2022 18:26:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=434
03/01/2022 18:26:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=439
03/01/2022 18:26:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=444
03/01/2022 18:26:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.29 on epoch=449
03/01/2022 18:26:42 - INFO - __main__ - Global step 900 Train loss 0.27 EM 0.0 on epoch=449
03/01/2022 18:26:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
03/01/2022 18:26:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=459
03/01/2022 18:26:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.33 on epoch=464
03/01/2022 18:26:51 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=469
03/01/2022 18:26:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=474
03/01/2022 18:26:54 - INFO - __main__ - Global step 950 Train loss 0.25 EM 0.0 on epoch=474
03/01/2022 18:26:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=479
03/01/2022 18:26:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=484
03/01/2022 18:27:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=489
03/01/2022 18:27:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=494
03/01/2022 18:27:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=499
03/01/2022 18:27:07 - INFO - __main__ - Global step 1000 Train loss 0.22 EM 0.0 on epoch=499
03/01/2022 18:27:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=504
03/01/2022 18:27:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=509
03/01/2022 18:27:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=514
03/01/2022 18:27:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=519
03/01/2022 18:27:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=524
03/01/2022 18:27:19 - INFO - __main__ - Global step 1050 Train loss 0.21 EM 0.0 on epoch=524
03/01/2022 18:27:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=529
03/01/2022 18:27:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=534
03/01/2022 18:27:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=539
03/01/2022 18:27:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=544
03/01/2022 18:27:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.15 on epoch=549
03/01/2022 18:27:32 - INFO - __main__ - Global step 1100 Train loss 0.19 EM 0.0 on epoch=549
03/01/2022 18:27:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.16 on epoch=554
03/01/2022 18:27:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=559
03/01/2022 18:27:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=564
03/01/2022 18:27:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=569
03/01/2022 18:27:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=574
03/01/2022 18:27:44 - INFO - __main__ - Global step 1150 Train loss 0.17 EM 0.0 on epoch=574
03/01/2022 18:27:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.16 on epoch=579
03/01/2022 18:27:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=584
03/01/2022 18:27:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=589
03/01/2022 18:27:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=594
03/01/2022 18:27:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=599
03/01/2022 18:27:57 - INFO - __main__ - Global step 1200 Train loss 0.17 EM 0.0 on epoch=599
03/01/2022 18:27:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=604
03/01/2022 18:28:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=609
03/01/2022 18:28:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=614
03/01/2022 18:28:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=619
03/01/2022 18:28:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=624
03/01/2022 18:28:09 - INFO - __main__ - Global step 1250 Train loss 0.14 EM 0.0 on epoch=624
03/01/2022 18:28:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=629
03/01/2022 18:28:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=634
03/01/2022 18:28:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=639
03/01/2022 18:28:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=644
03/01/2022 18:28:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=649
03/01/2022 18:28:22 - INFO - __main__ - Global step 1300 Train loss 0.15 EM 0.0 on epoch=649
03/01/2022 18:28:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=654
03/01/2022 18:28:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=659
03/01/2022 18:28:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=664
03/01/2022 18:28:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=669
03/01/2022 18:28:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=674
03/01/2022 18:28:34 - INFO - __main__ - Global step 1350 Train loss 0.13 EM 0.0 on epoch=674
03/01/2022 18:28:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=679
03/01/2022 18:28:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=684
03/01/2022 18:28:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=689
03/01/2022 18:28:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=694
03/01/2022 18:28:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=699
03/01/2022 18:28:47 - INFO - __main__ - Global step 1400 Train loss 0.14 EM 0.0 on epoch=699
03/01/2022 18:28:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=704
03/01/2022 18:28:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=709
03/01/2022 18:28:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=714
03/01/2022 18:28:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=719
03/01/2022 18:28:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=724
03/01/2022 18:28:59 - INFO - __main__ - Global step 1450 Train loss 0.12 EM 0.0 on epoch=724
03/01/2022 18:29:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=729
03/01/2022 18:29:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.12 on epoch=734
03/01/2022 18:29:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=739
03/01/2022 18:29:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.10 on epoch=744
03/01/2022 18:29:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=749
03/01/2022 18:29:12 - INFO - __main__ - Global step 1500 Train loss 0.10 EM 0.0 on epoch=749
03/01/2022 18:29:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=754
03/01/2022 18:29:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=759
03/01/2022 18:29:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=764
03/01/2022 18:29:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=769
03/01/2022 18:29:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=774
03/01/2022 18:29:25 - INFO - __main__ - Global step 1550 Train loss 0.13 EM 0.0 on epoch=774
03/01/2022 18:29:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=779
03/01/2022 18:29:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.09 on epoch=784
03/01/2022 18:29:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=789
03/01/2022 18:29:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=794
03/01/2022 18:29:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.11 on epoch=799
03/01/2022 18:29:37 - INFO - __main__ - Global step 1600 Train loss 0.10 EM 0.0 on epoch=799
03/01/2022 18:29:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=804
03/01/2022 18:29:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=809
03/01/2022 18:29:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=814
03/01/2022 18:29:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=819
03/01/2022 18:29:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=824
03/01/2022 18:29:50 - INFO - __main__ - Global step 1650 Train loss 0.10 EM 0.0 on epoch=824
03/01/2022 18:29:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=829
03/01/2022 18:29:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=834
03/01/2022 18:29:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=839
03/01/2022 18:29:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=844
03/01/2022 18:30:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=849
03/01/2022 18:30:02 - INFO - __main__ - Global step 1700 Train loss 0.09 EM 0.0 on epoch=849
03/01/2022 18:30:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=854
03/01/2022 18:30:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=859
03/01/2022 18:30:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=864
03/01/2022 18:30:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=869
03/01/2022 18:30:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=874
03/01/2022 18:30:15 - INFO - __main__ - Global step 1750 Train loss 0.07 EM 0.0 on epoch=874
03/01/2022 18:30:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=879
03/01/2022 18:30:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=884
03/01/2022 18:30:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=889
03/01/2022 18:30:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=894
03/01/2022 18:30:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=899
03/01/2022 18:30:28 - INFO - __main__ - Global step 1800 Train loss 0.09 EM 0.0 on epoch=899
03/01/2022 18:30:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=904
03/01/2022 18:30:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=909
03/01/2022 18:30:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=914
03/01/2022 18:30:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=919
03/01/2022 18:30:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/01/2022 18:30:40 - INFO - __main__ - Global step 1850 Train loss 0.07 EM 0.0 on epoch=924
03/01/2022 18:30:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=929
03/01/2022 18:30:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=934
03/01/2022 18:30:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=939
03/01/2022 18:30:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=944
03/01/2022 18:30:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=949
03/01/2022 18:30:52 - INFO - __main__ - Global step 1900 Train loss 0.07 EM 0.0 on epoch=949
03/01/2022 18:30:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=954
03/01/2022 18:30:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=959
03/01/2022 18:30:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=964
03/01/2022 18:31:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=969
03/01/2022 18:31:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=974
03/01/2022 18:31:05 - INFO - __main__ - Global step 1950 Train loss 0.07 EM 0.0 on epoch=974
03/01/2022 18:31:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=979
03/01/2022 18:31:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=984
03/01/2022 18:31:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=989
03/01/2022 18:31:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=994
03/01/2022 18:31:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=999
03/01/2022 18:31:18 - INFO - __main__ - Global step 2000 Train loss 0.07 EM 0.0 on epoch=999
03/01/2022 18:31:18 - INFO - __main__ - save last model!
03/01/2022 18:31:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 18:31:18 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 18:31:18 - INFO - __main__ - Printing 3 examples
03/01/2022 18:31:18 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 18:31:18 - INFO - __main__ - ['taming of the shrew']
03/01/2022 18:31:18 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 18:31:18 - INFO - __main__ - ['henry fonda']
03/01/2022 18:31:18 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 18:31:18 - INFO - __main__ - ['tchaikovsky']
03/01/2022 18:31:18 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:31:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:31:19 - INFO - __main__ - Printing 3 examples
03/01/2022 18:31:19 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 18:31:19 - INFO - __main__ - ['kieren fallon']
03/01/2022 18:31:19 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 18:31:19 - INFO - __main__ - ['uranus']
03/01/2022 18:31:19 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 18:31:19 - INFO - __main__ - ['rob thomas']
03/01/2022 18:31:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 18:31:19 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:31:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:31:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:31:19 - INFO - __main__ - Printing 3 examples
03/01/2022 18:31:19 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 18:31:19 - INFO - __main__ - ['terence rattigan']
03/01/2022 18:31:19 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 18:31:19 - INFO - __main__ - ['casino royale']
03/01/2022 18:31:19 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 18:31:19 - INFO - __main__ - ['offenbach']
03/01/2022 18:31:19 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:31:19 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:31:19 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:31:20 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:31:24 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 18:31:31 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 18:31:31 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(2502, 393505), (3485, 303196), (2451, 408517), (2124, 458893), (727, 605486), (159, 3283944), (550, 1652637), (2943, 377167), (4577, 197081), (536, 1096599), (4800, 205648), (3374, 227031), (4959, 162645), (2020, 470862), (3016, 244028), (6059, 189253), (3944, 234304), (2651, 378692), (1638, 604699), (62, 15842384), (1130, 821238), (4783, 219874), (1890, 417949), (1929, 517016), (5885, 180884), (5519, 201464), (568, 1556089), (6068, 167828), (2545, 370902), (1310, 731782), (4964, 205248), (1443, 666960), (4999, 201462), (5908, 176655), (4066, 167278), (6883, 157698), (3596, 297061), (3747, 211292), (5364, 185276), (4984, 218955), (4170, 170840), (5020, 210030), (1776, 567810), (5258, 197449), (4949, 208735), (2848, 356857), (2595, 374340), (2349, 335938), (5783, 180158), (5245, 192501), (2287, 388098), (2224, 338862), (3959, 212293), (852, 1044002), (2923, 327246), (1458, 394015), (1963, 341745), (3521, 280332), (2606, 294174), (7609, 173097), (830, 1012157), (968, 645839), (1487, 592557), (2198, 398890), (4674, 244421), (1627, 587044), (2068, 427366), (724, 1234015), (4657, 226628), (4614, 230206), (3467, 284152), (3304, 172257), (3291, 200786), (4441, 226147), (651, 1269510), (2214, 442372), (3497, 223253), (4716, 239808), (6497, 160316), (227, 4080344), (7555, 162623), (3924, 246414), (4666, 195723), (66, 13829035), (3645, 265964), (3236, 314080), (3229, 239354), (4804, 223950), (6162, 173710), (6040, 171279), (6152, 155746), (2428, 415536), (5503, 191100), (2374, 417354), (2949, 340763), (4836, 214321), (3385, 191872), (313, 2745242), (2206, 240868)]
03/01/2022 18:31:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:31:32 - INFO - __main__ - Starting training!
03/01/2022 18:34:04 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_13_0.5_8_predictions.txt
03/01/2022 18:34:04 - INFO - __main__ - EM on test data: 0.0068
03/01/2022 18:34:05 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.5, bsz=8, dev_performance=0.0, test_performance=0.006760140210315473
03/01/2022 18:34:05 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.4, bsz=8 ...
03/01/2022 18:34:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:34:05 - INFO - __main__ - Printing 3 examples
03/01/2022 18:34:05 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 18:34:05 - INFO - __main__ - ['kieren fallon']
03/01/2022 18:34:05 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 18:34:05 - INFO - __main__ - ['uranus']
03/01/2022 18:34:05 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 18:34:05 - INFO - __main__ - ['rob thomas']
03/01/2022 18:34:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 18:34:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:34:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:34:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:34:05 - INFO - __main__ - Printing 3 examples
03/01/2022 18:34:05 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 18:34:05 - INFO - __main__ - ['terence rattigan']
03/01/2022 18:34:05 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 18:34:05 - INFO - __main__ - ['casino royale']
03/01/2022 18:34:05 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 18:34:05 - INFO - __main__ - ['offenbach']
03/01/2022 18:34:05 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:34:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:34:06 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:34:17 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 18:34:17 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(2502, 393505), (3485, 303196), (2451, 408517), (2124, 458893), (727, 605486), (159, 3283944), (550, 1652637), (2943, 377167), (4577, 197081), (536, 1096599), (4800, 205648), (3374, 227031), (4959, 162645), (2020, 470862), (3016, 244028), (6059, 189253), (3944, 234304), (2651, 378692), (1638, 604699), (62, 15842384), (1130, 821238), (4783, 219874), (1890, 417949), (1929, 517016), (5885, 180884), (5519, 201464), (568, 1556089), (6068, 167828), (2545, 370902), (1310, 731782), (4964, 205248), (1443, 666960), (4999, 201462), (5908, 176655), (4066, 167278), (6883, 157698), (3596, 297061), (3747, 211292), (5364, 185276), (4984, 218955), (4170, 170840), (5020, 210030), (1776, 567810), (5258, 197449), (4949, 208735), (2848, 356857), (2595, 374340), (2349, 335938), (5783, 180158), (5245, 192501), (2287, 388098), (2224, 338862), (3959, 212293), (852, 1044002), (2923, 327246), (1458, 394015), (1963, 341745), (3521, 280332), (2606, 294174), (7609, 173097), (830, 1012157), (968, 645839), (1487, 592557), (2198, 398890), (4674, 244421), (1627, 587044), (2068, 427366), (724, 1234015), (4657, 226628), (4614, 230206), (3467, 284152), (3304, 172257), (3291, 200786), (4441, 226147), (651, 1269510), (2214, 442372), (3497, 223253), (4716, 239808), (6497, 160316), (227, 4080344), (7555, 162623), (3924, 246414), (4666, 195723), (66, 13829035), (3645, 265964), (3236, 314080), (3229, 239354), (4804, 223950), (6162, 173710), (6040, 171279), (6152, 155746), (2428, 415536), (5503, 191100), (2374, 417354), (2949, 340763), (4836, 214321), (3385, 191872), (313, 2745242), (2206, 240868)]
03/01/2022 18:34:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:34:18 - INFO - __main__ - Starting training!
03/01/2022 18:34:23 - INFO - __main__ - Step 10 Global step 10 Train loss 4.47 on epoch=4
03/01/2022 18:34:25 - INFO - __main__ - Step 20 Global step 20 Train loss 3.74 on epoch=9
03/01/2022 18:34:28 - INFO - __main__ - Step 30 Global step 30 Train loss 3.32 on epoch=14
03/01/2022 18:34:30 - INFO - __main__ - Step 40 Global step 40 Train loss 2.96 on epoch=19
03/01/2022 18:34:32 - INFO - __main__ - Step 50 Global step 50 Train loss 2.70 on epoch=24
03/01/2022 18:34:34 - INFO - __main__ - Global step 50 Train loss 3.44 EM 0.0 on epoch=24
03/01/2022 18:34:34 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 18:34:37 - INFO - __main__ - Step 60 Global step 60 Train loss 2.55 on epoch=29
03/01/2022 18:34:39 - INFO - __main__ - Step 70 Global step 70 Train loss 2.38 on epoch=34
03/01/2022 18:34:41 - INFO - __main__ - Step 80 Global step 80 Train loss 2.27 on epoch=39
03/01/2022 18:34:43 - INFO - __main__ - Step 90 Global step 90 Train loss 2.08 on epoch=44
03/01/2022 18:34:45 - INFO - __main__ - Step 100 Global step 100 Train loss 2.03 on epoch=49
03/01/2022 18:34:47 - INFO - __main__ - Global step 100 Train loss 2.26 EM 0.0 on epoch=49
03/01/2022 18:34:49 - INFO - __main__ - Step 110 Global step 110 Train loss 1.92 on epoch=54
03/01/2022 18:34:52 - INFO - __main__ - Step 120 Global step 120 Train loss 1.96 on epoch=59
03/01/2022 18:34:54 - INFO - __main__ - Step 130 Global step 130 Train loss 1.80 on epoch=64
03/01/2022 18:34:56 - INFO - __main__ - Step 140 Global step 140 Train loss 1.77 on epoch=69
03/01/2022 18:34:59 - INFO - __main__ - Step 150 Global step 150 Train loss 1.71 on epoch=74
03/01/2022 18:35:00 - INFO - __main__ - Global step 150 Train loss 1.83 EM 0.0 on epoch=74
03/01/2022 18:35:02 - INFO - __main__ - Step 160 Global step 160 Train loss 1.60 on epoch=79
03/01/2022 18:35:04 - INFO - __main__ - Step 170 Global step 170 Train loss 1.56 on epoch=84
03/01/2022 18:35:06 - INFO - __main__ - Step 180 Global step 180 Train loss 1.48 on epoch=89
03/01/2022 18:35:09 - INFO - __main__ - Step 190 Global step 190 Train loss 1.45 on epoch=94
03/01/2022 18:35:11 - INFO - __main__ - Step 200 Global step 200 Train loss 1.39 on epoch=99
03/01/2022 18:35:12 - INFO - __main__ - Global step 200 Train loss 1.50 EM 0.0 on epoch=99
03/01/2022 18:35:15 - INFO - __main__ - Step 210 Global step 210 Train loss 1.38 on epoch=104
03/01/2022 18:35:17 - INFO - __main__ - Step 220 Global step 220 Train loss 1.29 on epoch=109
03/01/2022 18:35:19 - INFO - __main__ - Step 230 Global step 230 Train loss 1.25 on epoch=114
03/01/2022 18:35:21 - INFO - __main__ - Step 240 Global step 240 Train loss 1.33 on epoch=119
03/01/2022 18:35:24 - INFO - __main__ - Step 250 Global step 250 Train loss 1.34 on epoch=124
03/01/2022 18:35:25 - INFO - __main__ - Global step 250 Train loss 1.32 EM 0.0 on epoch=124
03/01/2022 18:35:27 - INFO - __main__ - Step 260 Global step 260 Train loss 1.29 on epoch=129
03/01/2022 18:35:29 - INFO - __main__ - Step 270 Global step 270 Train loss 1.29 on epoch=134
03/01/2022 18:35:32 - INFO - __main__ - Step 280 Global step 280 Train loss 1.16 on epoch=139
03/01/2022 18:35:34 - INFO - __main__ - Step 290 Global step 290 Train loss 1.21 on epoch=144
03/01/2022 18:35:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.97 on epoch=149
03/01/2022 18:35:37 - INFO - __main__ - Global step 300 Train loss 1.19 EM 0.0 on epoch=149
03/01/2022 18:35:40 - INFO - __main__ - Step 310 Global step 310 Train loss 1.07 on epoch=154
03/01/2022 18:35:42 - INFO - __main__ - Step 320 Global step 320 Train loss 1.05 on epoch=159
03/01/2022 18:35:44 - INFO - __main__ - Step 330 Global step 330 Train loss 1.03 on epoch=164
03/01/2022 18:35:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.95 on epoch=169
03/01/2022 18:35:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.93 on epoch=174
03/01/2022 18:35:50 - INFO - __main__ - Global step 350 Train loss 1.01 EM 0.0 on epoch=174
03/01/2022 18:35:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.95 on epoch=179
03/01/2022 18:35:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.98 on epoch=184
03/01/2022 18:35:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.91 on epoch=189
03/01/2022 18:35:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.82 on epoch=194
03/01/2022 18:36:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=199
03/01/2022 18:36:02 - INFO - __main__ - Global step 400 Train loss 0.90 EM 0.0 on epoch=199
03/01/2022 18:36:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.87 on epoch=204
03/01/2022 18:36:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.87 on epoch=209
03/01/2022 18:36:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.73 on epoch=214
03/01/2022 18:36:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.93 on epoch=219
03/01/2022 18:36:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.84 on epoch=224
03/01/2022 18:36:15 - INFO - __main__ - Global step 450 Train loss 0.85 EM 0.0 on epoch=224
03/01/2022 18:36:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.75 on epoch=229
03/01/2022 18:36:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.83 on epoch=234
03/01/2022 18:36:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.74 on epoch=239
03/01/2022 18:36:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.68 on epoch=244
03/01/2022 18:36:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.79 on epoch=249
03/01/2022 18:36:28 - INFO - __main__ - Global step 500 Train loss 0.76 EM 0.0 on epoch=249
03/01/2022 18:36:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.77 on epoch=254
03/01/2022 18:36:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.64 on epoch=259
03/01/2022 18:36:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.69 on epoch=264
03/01/2022 18:36:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.63 on epoch=269
03/01/2022 18:36:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.70 on epoch=274
03/01/2022 18:36:40 - INFO - __main__ - Global step 550 Train loss 0.69 EM 0.0 on epoch=274
03/01/2022 18:36:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.65 on epoch=279
03/01/2022 18:36:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.59 on epoch=284
03/01/2022 18:36:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.68 on epoch=289
03/01/2022 18:36:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.60 on epoch=294
03/01/2022 18:36:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=299
03/01/2022 18:36:53 - INFO - __main__ - Global step 600 Train loss 0.60 EM 0.0 on epoch=299
03/01/2022 18:36:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.64 on epoch=304
03/01/2022 18:36:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.53 on epoch=309
03/01/2022 18:36:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.62 on epoch=314
03/01/2022 18:37:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=319
03/01/2022 18:37:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=324
03/01/2022 18:37:05 - INFO - __main__ - Global step 650 Train loss 0.58 EM 0.0 on epoch=324
03/01/2022 18:37:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.55 on epoch=329
03/01/2022 18:37:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.53 on epoch=334
03/01/2022 18:37:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=339
03/01/2022 18:37:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.51 on epoch=344
03/01/2022 18:37:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=349
03/01/2022 18:37:18 - INFO - __main__ - Global step 700 Train loss 0.51 EM 0.0 on epoch=349
03/01/2022 18:37:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=354
03/01/2022 18:37:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=359
03/01/2022 18:37:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=364
03/01/2022 18:37:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=369
03/01/2022 18:37:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.50 on epoch=374
03/01/2022 18:37:30 - INFO - __main__ - Global step 750 Train loss 0.48 EM 0.0 on epoch=374
03/01/2022 18:37:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=379
03/01/2022 18:37:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=384
03/01/2022 18:37:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=389
03/01/2022 18:37:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=394
03/01/2022 18:37:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=399
03/01/2022 18:37:43 - INFO - __main__ - Global step 800 Train loss 0.43 EM 0.0 on epoch=399
03/01/2022 18:37:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=404
03/01/2022 18:37:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=409
03/01/2022 18:37:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=414
03/01/2022 18:37:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=419
03/01/2022 18:37:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=424
03/01/2022 18:37:56 - INFO - __main__ - Global step 850 Train loss 0.40 EM 0.0 on epoch=424
03/01/2022 18:37:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=429
03/01/2022 18:38:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=434
03/01/2022 18:38:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=439
03/01/2022 18:38:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=444
03/01/2022 18:38:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=449
03/01/2022 18:38:08 - INFO - __main__ - Global step 900 Train loss 0.37 EM 0.0 on epoch=449
03/01/2022 18:38:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=454
03/01/2022 18:38:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=459
03/01/2022 18:38:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=464
03/01/2022 18:38:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=469
03/01/2022 18:38:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=474
03/01/2022 18:38:21 - INFO - __main__ - Global step 950 Train loss 0.33 EM 0.0 on epoch=474
03/01/2022 18:38:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.30 on epoch=479
03/01/2022 18:38:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.28 on epoch=484
03/01/2022 18:38:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=489
03/01/2022 18:38:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=494
03/01/2022 18:38:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=499
03/01/2022 18:38:34 - INFO - __main__ - Global step 1000 Train loss 0.32 EM 0.0 on epoch=499
03/01/2022 18:38:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=504
03/01/2022 18:38:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.26 on epoch=509
03/01/2022 18:38:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=514
03/01/2022 18:38:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=519
03/01/2022 18:38:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=524
03/01/2022 18:38:47 - INFO - __main__ - Global step 1050 Train loss 0.29 EM 0.0 on epoch=524
03/01/2022 18:38:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.31 on epoch=529
03/01/2022 18:38:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.26 on epoch=534
03/01/2022 18:38:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=539
03/01/2022 18:38:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=544
03/01/2022 18:38:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=549
03/01/2022 18:38:59 - INFO - __main__ - Global step 1100 Train loss 0.26 EM 0.0 on epoch=549
03/01/2022 18:39:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=554
03/01/2022 18:39:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=559
03/01/2022 18:39:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.27 on epoch=564
03/01/2022 18:39:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.28 on epoch=569
03/01/2022 18:39:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=574
03/01/2022 18:39:12 - INFO - __main__ - Global step 1150 Train loss 0.26 EM 0.0 on epoch=574
03/01/2022 18:39:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=579
03/01/2022 18:39:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.24 on epoch=584
03/01/2022 18:39:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=589
03/01/2022 18:39:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=594
03/01/2022 18:39:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=599
03/01/2022 18:39:25 - INFO - __main__ - Global step 1200 Train loss 0.24 EM 0.0 on epoch=599
03/01/2022 18:39:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=604
03/01/2022 18:39:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=609
03/01/2022 18:39:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=614
03/01/2022 18:39:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.24 on epoch=619
03/01/2022 18:39:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=624
03/01/2022 18:39:37 - INFO - __main__ - Global step 1250 Train loss 0.21 EM 0.0 on epoch=624
03/01/2022 18:39:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=629
03/01/2022 18:39:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.18 on epoch=634
03/01/2022 18:39:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=639
03/01/2022 18:39:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=644
03/01/2022 18:39:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=649
03/01/2022 18:39:50 - INFO - __main__ - Global step 1300 Train loss 0.19 EM 0.0 on epoch=649
03/01/2022 18:39:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=654
03/01/2022 18:39:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=659
03/01/2022 18:39:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=664
03/01/2022 18:39:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=669
03/01/2022 18:40:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.17 on epoch=674
03/01/2022 18:40:03 - INFO - __main__ - Global step 1350 Train loss 0.18 EM 0.0 on epoch=674
03/01/2022 18:40:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=679
03/01/2022 18:40:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=684
03/01/2022 18:40:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=689
03/01/2022 18:40:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=694
03/01/2022 18:40:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=699
03/01/2022 18:40:16 - INFO - __main__ - Global step 1400 Train loss 0.17 EM 0.0 on epoch=699
03/01/2022 18:40:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=704
03/01/2022 18:40:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=709
03/01/2022 18:40:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=714
03/01/2022 18:40:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=719
03/01/2022 18:40:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=724
03/01/2022 18:40:28 - INFO - __main__ - Global step 1450 Train loss 0.16 EM 0.0 on epoch=724
03/01/2022 18:40:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=729
03/01/2022 18:40:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=734
03/01/2022 18:40:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=739
03/01/2022 18:40:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=744
03/01/2022 18:40:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=749
03/01/2022 18:40:41 - INFO - __main__ - Global step 1500 Train loss 0.17 EM 0.0 on epoch=749
03/01/2022 18:40:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=754
03/01/2022 18:40:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=759
03/01/2022 18:40:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=764
03/01/2022 18:40:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=769
03/01/2022 18:40:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=774
03/01/2022 18:40:54 - INFO - __main__ - Global step 1550 Train loss 0.12 EM 0.0 on epoch=774
03/01/2022 18:40:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=779
03/01/2022 18:40:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=784
03/01/2022 18:41:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=789
03/01/2022 18:41:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=794
03/01/2022 18:41:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=799
03/01/2022 18:41:06 - INFO - __main__ - Global step 1600 Train loss 0.14 EM 0.0 on epoch=799
03/01/2022 18:41:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=804
03/01/2022 18:41:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=809
03/01/2022 18:41:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=814
03/01/2022 18:41:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=819
03/01/2022 18:41:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=824
03/01/2022 18:41:18 - INFO - __main__ - Global step 1650 Train loss 0.14 EM 0.0 on epoch=824
03/01/2022 18:41:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=829
03/01/2022 18:41:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=834
03/01/2022 18:41:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=839
03/01/2022 18:41:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=844
03/01/2022 18:41:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=849
03/01/2022 18:41:31 - INFO - __main__ - Global step 1700 Train loss 0.11 EM 0.0 on epoch=849
03/01/2022 18:41:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=854
03/01/2022 18:41:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=859
03/01/2022 18:41:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=864
03/01/2022 18:41:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=869
03/01/2022 18:41:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=874
03/01/2022 18:41:43 - INFO - __main__ - Global step 1750 Train loss 0.12 EM 0.0 on epoch=874
03/01/2022 18:41:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.16 on epoch=879
03/01/2022 18:41:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=884
03/01/2022 18:41:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=889
03/01/2022 18:41:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=894
03/01/2022 18:41:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=899
03/01/2022 18:41:56 - INFO - __main__ - Global step 1800 Train loss 0.12 EM 0.0 on epoch=899
03/01/2022 18:41:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=904
03/01/2022 18:42:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=909
03/01/2022 18:42:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=914
03/01/2022 18:42:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=919
03/01/2022 18:42:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.11 on epoch=924
03/01/2022 18:42:08 - INFO - __main__ - Global step 1850 Train loss 0.10 EM 0.0 on epoch=924
03/01/2022 18:42:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=929
03/01/2022 18:42:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=934
03/01/2022 18:42:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=939
03/01/2022 18:42:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=944
03/01/2022 18:42:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=949
03/01/2022 18:42:21 - INFO - __main__ - Global step 1900 Train loss 0.09 EM 0.0 on epoch=949
03/01/2022 18:42:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=954
03/01/2022 18:42:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=959
03/01/2022 18:42:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=964
03/01/2022 18:42:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=969
03/01/2022 18:42:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=974
03/01/2022 18:42:33 - INFO - __main__ - Global step 1950 Train loss 0.08 EM 0.0 on epoch=974
03/01/2022 18:42:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=979
03/01/2022 18:42:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=984
03/01/2022 18:42:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=989
03/01/2022 18:42:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=994
03/01/2022 18:42:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=999
03/01/2022 18:42:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:42:45 - INFO - __main__ - Printing 3 examples
03/01/2022 18:42:45 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 18:42:45 - INFO - __main__ - ['kieren fallon']
03/01/2022 18:42:45 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 18:42:45 - INFO - __main__ - ['uranus']
03/01/2022 18:42:45 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 18:42:45 - INFO - __main__ - ['rob thomas']
03/01/2022 18:42:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 18:42:45 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:42:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:42:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:42:45 - INFO - __main__ - Printing 3 examples
03/01/2022 18:42:45 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 18:42:45 - INFO - __main__ - ['terence rattigan']
03/01/2022 18:42:45 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 18:42:45 - INFO - __main__ - ['casino royale']
03/01/2022 18:42:45 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 18:42:45 - INFO - __main__ - ['offenbach']
03/01/2022 18:42:45 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:42:45 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:42:45 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:42:45 - INFO - __main__ - Global step 2000 Train loss 0.09 EM 0.0 on epoch=999
03/01/2022 18:42:45 - INFO - __main__ - save last model!
03/01/2022 18:42:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 18:42:46 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 18:42:46 - INFO - __main__ - Printing 3 examples
03/01/2022 18:42:46 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 18:42:46 - INFO - __main__ - ['taming of the shrew']
03/01/2022 18:42:46 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 18:42:46 - INFO - __main__ - ['henry fonda']
03/01/2022 18:42:46 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 18:42:46 - INFO - __main__ - ['tchaikovsky']
03/01/2022 18:42:46 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:42:47 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:42:51 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 18:42:57 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 18:42:57 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(3666, 279856), (4279, 231910), (1376, 549699), (5099, 200518), (1480, 693138), (5956, 175083), (317, 2909017), (3859, 237931), (6537, 158593), (6422, 165641), (328, 2897449), (915, 980631), (6366, 217988), (1303, 658549), (5169, 202438), (1884, 502256), (568, 1556089), (1489, 355866), (4036, 247461), (2445, 287205), (2154, 432132), (5074, 206086), (3513, 287362), (2658, 279047), (2412, 334412), (2660, 222403), (2656, 367203), (882, 1004086), (4935, 199405), (115, 4101345), (5058, 173227), (593, 1511490), (3830, 267200), (2935, 409303), (726, 1159344), (464, 1931312), (262, 2562554), (2595, 374340), (2114, 458811), (231, 4022141), (1689, 588233), (2627, 389531), (1048, 702299), (5656, 197667), (1341, 767639), (4837, 165767), (2418, 420267), (5111, 201364), (2433, 361242), (3976, 254648), (1801, 537800), (3849, 155187), (6553, 154915), (1214, 767509), (1682, 553032), (3355, 302330), (1056, 811824), (5087, 161786), (196, 3104995), (2464, 301408), (7555, 162623), (3004, 249813), (4349, 236587), (1978, 460084), (448, 1064987), (5917, 158493), (2612, 380946), (1078, 844362), (2715, 311751), (5798, 156482), (1137, 734426), (6326, 165064), (456, 1871965), (3146, 269081), (1703, 175726), (4773, 218071), (3699, 253359), (746, 1156068), (5696, 184838), (3669, 275693), (165, 6245475), (3497, 223253), (4240, 190278), (1195, 825391), (400, 215271), (2938, 259506), (3095, 203292), (1433, 586613), (4400, 214423), (1995, 516280), (5706, 180057), (2953, 344008), (3396, 173723), (3557, 193090), (5002, 201395), (4365, 214420), (4936, 158408), (2855, 345487), (3008, 291537)]
03/01/2022 18:42:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:42:58 - INFO - __main__ - Starting training!
03/01/2022 18:45:32 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_13_0.4_8_predictions.txt
03/01/2022 18:45:32 - INFO - __main__ - EM on test data: 0.0028
03/01/2022 18:45:32 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.4, bsz=8, dev_performance=0.0, test_performance=0.002754131196795193
03/01/2022 18:45:32 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.3, bsz=8 ...
03/01/2022 18:45:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:45:33 - INFO - __main__ - Printing 3 examples
03/01/2022 18:45:33 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 18:45:33 - INFO - __main__ - ['kieren fallon']
03/01/2022 18:45:33 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 18:45:33 - INFO - __main__ - ['uranus']
03/01/2022 18:45:33 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 18:45:33 - INFO - __main__ - ['rob thomas']
03/01/2022 18:45:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 18:45:33 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:45:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:45:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:45:33 - INFO - __main__ - Printing 3 examples
03/01/2022 18:45:33 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 18:45:33 - INFO - __main__ - ['terence rattigan']
03/01/2022 18:45:33 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 18:45:33 - INFO - __main__ - ['casino royale']
03/01/2022 18:45:33 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 18:45:33 - INFO - __main__ - ['offenbach']
03/01/2022 18:45:33 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:45:33 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:45:33 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:45:45 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 18:45:45 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(3666, 279856), (4279, 231910), (1376, 549699), (5099, 200518), (1480, 693138), (5956, 175083), (317, 2909017), (3859, 237931), (6537, 158593), (6422, 165641), (328, 2897449), (915, 980631), (6366, 217988), (1303, 658549), (5169, 202438), (1884, 502256), (568, 1556089), (1489, 355866), (4036, 247461), (2445, 287205), (2154, 432132), (5074, 206086), (3513, 287362), (2658, 279047), (2412, 334412), (2660, 222403), (2656, 367203), (882, 1004086), (4935, 199405), (115, 4101345), (5058, 173227), (593, 1511490), (3830, 267200), (2935, 409303), (726, 1159344), (464, 1931312), (262, 2562554), (2595, 374340), (2114, 458811), (231, 4022141), (1689, 588233), (2627, 389531), (1048, 702299), (5656, 197667), (1341, 767639), (4837, 165767), (2418, 420267), (5111, 201364), (2433, 361242), (3976, 254648), (1801, 537800), (3849, 155187), (6553, 154915), (1214, 767509), (1682, 553032), (3355, 302330), (1056, 811824), (5087, 161786), (196, 3104995), (2464, 301408), (7555, 162623), (3004, 249813), (4349, 236587), (1978, 460084), (448, 1064987), (5917, 158493), (2612, 380946), (1078, 844362), (2715, 311751), (5798, 156482), (1137, 734426), (6326, 165064), (456, 1871965), (3146, 269081), (1703, 175726), (4773, 218071), (3699, 253359), (746, 1156068), (5696, 184838), (3669, 275693), (165, 6245475), (3497, 223253), (4240, 190278), (1195, 825391), (400, 215271), (2938, 259506), (3095, 203292), (1433, 586613), (4400, 214423), (1995, 516280), (5706, 180057), (2953, 344008), (3396, 173723), (3557, 193090), (5002, 201395), (4365, 214420), (4936, 158408), (2855, 345487), (3008, 291537)]
03/01/2022 18:45:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:45:46 - INFO - __main__ - Starting training!
03/01/2022 18:45:49 - INFO - __main__ - Step 10 Global step 10 Train loss 4.56 on epoch=4
03/01/2022 18:45:51 - INFO - __main__ - Step 20 Global step 20 Train loss 4.08 on epoch=9
03/01/2022 18:45:53 - INFO - __main__ - Step 30 Global step 30 Train loss 3.40 on epoch=14
03/01/2022 18:45:55 - INFO - __main__ - Step 40 Global step 40 Train loss 3.22 on epoch=19
03/01/2022 18:45:57 - INFO - __main__ - Step 50 Global step 50 Train loss 2.95 on epoch=24
03/01/2022 18:46:00 - INFO - __main__ - Global step 50 Train loss 3.64 EM 0.0 on epoch=24
03/01/2022 18:46:00 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 18:46:02 - INFO - __main__ - Step 60 Global step 60 Train loss 2.78 on epoch=29
03/01/2022 18:46:04 - INFO - __main__ - Step 70 Global step 70 Train loss 2.54 on epoch=34
03/01/2022 18:46:06 - INFO - __main__ - Step 80 Global step 80 Train loss 2.51 on epoch=39
03/01/2022 18:46:09 - INFO - __main__ - Step 90 Global step 90 Train loss 2.36 on epoch=44
03/01/2022 18:46:11 - INFO - __main__ - Step 100 Global step 100 Train loss 2.20 on epoch=49
03/01/2022 18:46:12 - INFO - __main__ - Global step 100 Train loss 2.48 EM 0.0 on epoch=49
03/01/2022 18:46:14 - INFO - __main__ - Step 110 Global step 110 Train loss 2.19 on epoch=54
03/01/2022 18:46:16 - INFO - __main__ - Step 120 Global step 120 Train loss 2.06 on epoch=59
03/01/2022 18:46:19 - INFO - __main__ - Step 130 Global step 130 Train loss 2.05 on epoch=64
03/01/2022 18:46:21 - INFO - __main__ - Step 140 Global step 140 Train loss 2.00 on epoch=69
03/01/2022 18:46:23 - INFO - __main__ - Step 150 Global step 150 Train loss 1.94 on epoch=74
03/01/2022 18:46:24 - INFO - __main__ - Global step 150 Train loss 2.05 EM 0.03125 on epoch=74
03/01/2022 18:46:24 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=74, global_step=150
03/01/2022 18:46:26 - INFO - __main__ - Step 160 Global step 160 Train loss 1.88 on epoch=79
03/01/2022 18:46:29 - INFO - __main__ - Step 170 Global step 170 Train loss 1.89 on epoch=84
03/01/2022 18:46:31 - INFO - __main__ - Step 180 Global step 180 Train loss 1.74 on epoch=89
03/01/2022 18:46:33 - INFO - __main__ - Step 190 Global step 190 Train loss 1.68 on epoch=94
03/01/2022 18:46:35 - INFO - __main__ - Step 200 Global step 200 Train loss 1.69 on epoch=99
03/01/2022 18:46:36 - INFO - __main__ - Global step 200 Train loss 1.78 EM 0.0 on epoch=99
03/01/2022 18:46:38 - INFO - __main__ - Step 210 Global step 210 Train loss 1.65 on epoch=104
03/01/2022 18:46:41 - INFO - __main__ - Step 220 Global step 220 Train loss 1.64 on epoch=109
03/01/2022 18:46:43 - INFO - __main__ - Step 230 Global step 230 Train loss 1.72 on epoch=114
03/01/2022 18:46:45 - INFO - __main__ - Step 240 Global step 240 Train loss 1.54 on epoch=119
03/01/2022 18:46:47 - INFO - __main__ - Step 250 Global step 250 Train loss 1.62 on epoch=124
03/01/2022 18:46:48 - INFO - __main__ - Global step 250 Train loss 1.63 EM 0.0 on epoch=124
03/01/2022 18:46:50 - INFO - __main__ - Step 260 Global step 260 Train loss 1.49 on epoch=129
03/01/2022 18:46:53 - INFO - __main__ - Step 270 Global step 270 Train loss 1.46 on epoch=134
03/01/2022 18:46:55 - INFO - __main__ - Step 280 Global step 280 Train loss 1.44 on epoch=139
03/01/2022 18:46:57 - INFO - __main__ - Step 290 Global step 290 Train loss 1.42 on epoch=144
03/01/2022 18:46:59 - INFO - __main__ - Step 300 Global step 300 Train loss 1.34 on epoch=149
03/01/2022 18:47:00 - INFO - __main__ - Global step 300 Train loss 1.43 EM 0.0 on epoch=149
03/01/2022 18:47:03 - INFO - __main__ - Step 310 Global step 310 Train loss 1.34 on epoch=154
03/01/2022 18:47:05 - INFO - __main__ - Step 320 Global step 320 Train loss 1.35 on epoch=159
03/01/2022 18:47:07 - INFO - __main__ - Step 330 Global step 330 Train loss 1.35 on epoch=164
03/01/2022 18:47:09 - INFO - __main__ - Step 340 Global step 340 Train loss 1.37 on epoch=169
03/01/2022 18:47:11 - INFO - __main__ - Step 350 Global step 350 Train loss 1.20 on epoch=174
03/01/2022 18:47:12 - INFO - __main__ - Global step 350 Train loss 1.32 EM 0.0 on epoch=174
03/01/2022 18:47:15 - INFO - __main__ - Step 360 Global step 360 Train loss 1.28 on epoch=179
03/01/2022 18:47:17 - INFO - __main__ - Step 370 Global step 370 Train loss 1.20 on epoch=184
03/01/2022 18:47:19 - INFO - __main__ - Step 380 Global step 380 Train loss 1.16 on epoch=189
03/01/2022 18:47:21 - INFO - __main__ - Step 390 Global step 390 Train loss 1.13 on epoch=194
03/01/2022 18:47:23 - INFO - __main__ - Step 400 Global step 400 Train loss 1.13 on epoch=199
03/01/2022 18:47:24 - INFO - __main__ - Global step 400 Train loss 1.18 EM 0.0 on epoch=199
03/01/2022 18:47:27 - INFO - __main__ - Step 410 Global step 410 Train loss 1.13 on epoch=204
03/01/2022 18:47:29 - INFO - __main__ - Step 420 Global step 420 Train loss 1.14 on epoch=209
03/01/2022 18:47:31 - INFO - __main__ - Step 430 Global step 430 Train loss 1.09 on epoch=214
03/01/2022 18:47:33 - INFO - __main__ - Step 440 Global step 440 Train loss 1.13 on epoch=219
03/01/2022 18:47:36 - INFO - __main__ - Step 450 Global step 450 Train loss 1.12 on epoch=224
03/01/2022 18:47:37 - INFO - __main__ - Global step 450 Train loss 1.12 EM 0.0 on epoch=224
03/01/2022 18:47:39 - INFO - __main__ - Step 460 Global step 460 Train loss 1.09 on epoch=229
03/01/2022 18:47:41 - INFO - __main__ - Step 470 Global step 470 Train loss 1.04 on epoch=234
03/01/2022 18:47:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.98 on epoch=239
03/01/2022 18:47:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.98 on epoch=244
03/01/2022 18:47:48 - INFO - __main__ - Step 500 Global step 500 Train loss 1.04 on epoch=249
03/01/2022 18:47:50 - INFO - __main__ - Global step 500 Train loss 1.03 EM 0.0 on epoch=249
03/01/2022 18:47:52 - INFO - __main__ - Step 510 Global step 510 Train loss 1.03 on epoch=254
03/01/2022 18:47:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.98 on epoch=259
03/01/2022 18:47:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.98 on epoch=264
03/01/2022 18:47:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.93 on epoch=269
03/01/2022 18:48:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.97 on epoch=274
03/01/2022 18:48:02 - INFO - __main__ - Global step 550 Train loss 0.98 EM 0.0 on epoch=274
03/01/2022 18:48:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.88 on epoch=279
03/01/2022 18:48:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.85 on epoch=284
03/01/2022 18:48:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.91 on epoch=289
03/01/2022 18:48:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.83 on epoch=294
03/01/2022 18:48:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.92 on epoch=299
03/01/2022 18:48:15 - INFO - __main__ - Global step 600 Train loss 0.88 EM 0.0 on epoch=299
03/01/2022 18:48:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.88 on epoch=304
03/01/2022 18:48:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.84 on epoch=309
03/01/2022 18:48:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.79 on epoch=314
03/01/2022 18:48:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.76 on epoch=319
03/01/2022 18:48:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.76 on epoch=324
03/01/2022 18:48:27 - INFO - __main__ - Global step 650 Train loss 0.81 EM 0.0 on epoch=324
03/01/2022 18:48:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.75 on epoch=329
03/01/2022 18:48:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.85 on epoch=334
03/01/2022 18:48:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.74 on epoch=339
03/01/2022 18:48:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.79 on epoch=344
03/01/2022 18:48:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.77 on epoch=349
03/01/2022 18:48:40 - INFO - __main__ - Global step 700 Train loss 0.78 EM 0.0 on epoch=349
03/01/2022 18:48:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.73 on epoch=354
03/01/2022 18:48:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.81 on epoch=359
03/01/2022 18:48:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.66 on epoch=364
03/01/2022 18:48:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.69 on epoch=369
03/01/2022 18:48:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.65 on epoch=374
03/01/2022 18:48:52 - INFO - __main__ - Global step 750 Train loss 0.71 EM 0.0 on epoch=374
03/01/2022 18:48:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.76 on epoch=379
03/01/2022 18:48:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.73 on epoch=384
03/01/2022 18:48:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.63 on epoch=389
03/01/2022 18:49:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.68 on epoch=394
03/01/2022 18:49:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.69 on epoch=399
03/01/2022 18:49:05 - INFO - __main__ - Global step 800 Train loss 0.70 EM 0.0 on epoch=399
03/01/2022 18:49:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.66 on epoch=404
03/01/2022 18:49:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.61 on epoch=409
03/01/2022 18:49:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.69 on epoch=414
03/01/2022 18:49:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.57 on epoch=419
03/01/2022 18:49:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.58 on epoch=424
03/01/2022 18:49:17 - INFO - __main__ - Global step 850 Train loss 0.62 EM 0.0 on epoch=424
03/01/2022 18:49:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.59 on epoch=429
03/01/2022 18:49:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.59 on epoch=434
03/01/2022 18:49:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.52 on epoch=439
03/01/2022 18:49:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.64 on epoch=444
03/01/2022 18:49:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.63 on epoch=449
03/01/2022 18:49:30 - INFO - __main__ - Global step 900 Train loss 0.59 EM 0.0 on epoch=449
03/01/2022 18:49:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.55 on epoch=454
03/01/2022 18:49:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.53 on epoch=459
03/01/2022 18:49:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.53 on epoch=464
03/01/2022 18:49:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.59 on epoch=469
03/01/2022 18:49:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.57 on epoch=474
03/01/2022 18:49:42 - INFO - __main__ - Global step 950 Train loss 0.56 EM 0.0 on epoch=474
03/01/2022 18:49:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.54 on epoch=479
03/01/2022 18:49:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=484
03/01/2022 18:49:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.56 on epoch=489
03/01/2022 18:49:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.56 on epoch=494
03/01/2022 18:49:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.53 on epoch=499
03/01/2022 18:49:55 - INFO - __main__ - Global step 1000 Train loss 0.53 EM 0.0 on epoch=499
03/01/2022 18:49:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.56 on epoch=504
03/01/2022 18:49:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=509
03/01/2022 18:50:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.51 on epoch=514
03/01/2022 18:50:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.51 on epoch=519
03/01/2022 18:50:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=524
03/01/2022 18:50:07 - INFO - __main__ - Global step 1050 Train loss 0.50 EM 0.0 on epoch=524
03/01/2022 18:50:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.51 on epoch=529
03/01/2022 18:50:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=534
03/01/2022 18:50:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.48 on epoch=539
03/01/2022 18:50:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
03/01/2022 18:50:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=549
03/01/2022 18:50:19 - INFO - __main__ - Global step 1100 Train loss 0.47 EM 0.0 on epoch=549
03/01/2022 18:50:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=554
03/01/2022 18:50:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=559
03/01/2022 18:50:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=564
03/01/2022 18:50:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=569
03/01/2022 18:50:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=574
03/01/2022 18:50:32 - INFO - __main__ - Global step 1150 Train loss 0.45 EM 0.0 on epoch=574
03/01/2022 18:50:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=579
03/01/2022 18:50:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=584
03/01/2022 18:50:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=589
03/01/2022 18:50:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=594
03/01/2022 18:50:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=599
03/01/2022 18:50:44 - INFO - __main__ - Global step 1200 Train loss 0.45 EM 0.0 on epoch=599
03/01/2022 18:50:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=604
03/01/2022 18:50:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=609
03/01/2022 18:50:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=614
03/01/2022 18:50:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.37 on epoch=619
03/01/2022 18:50:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=624
03/01/2022 18:50:57 - INFO - __main__ - Global step 1250 Train loss 0.38 EM 0.0 on epoch=624
03/01/2022 18:50:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=629
03/01/2022 18:51:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=634
03/01/2022 18:51:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=639
03/01/2022 18:51:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=644
03/01/2022 18:51:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=649
03/01/2022 18:51:10 - INFO - __main__ - Global step 1300 Train loss 0.37 EM 0.0 on epoch=649
03/01/2022 18:51:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=654
03/01/2022 18:51:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=659
03/01/2022 18:51:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
03/01/2022 18:51:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=669
03/01/2022 18:51:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=674
03/01/2022 18:51:22 - INFO - __main__ - Global step 1350 Train loss 0.36 EM 0.0 on epoch=674
03/01/2022 18:51:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=679
03/01/2022 18:51:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=684
03/01/2022 18:51:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.32 on epoch=689
03/01/2022 18:51:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=694
03/01/2022 18:51:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=699
03/01/2022 18:51:35 - INFO - __main__ - Global step 1400 Train loss 0.33 EM 0.0 on epoch=699
03/01/2022 18:51:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=704
03/01/2022 18:51:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=709
03/01/2022 18:51:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.29 on epoch=714
03/01/2022 18:51:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.31 on epoch=719
03/01/2022 18:51:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.28 on epoch=724
03/01/2022 18:51:48 - INFO - __main__ - Global step 1450 Train loss 0.30 EM 0.0 on epoch=724
03/01/2022 18:51:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=729
03/01/2022 18:51:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=734
03/01/2022 18:51:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=739
03/01/2022 18:51:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.30 on epoch=744
03/01/2022 18:51:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=749
03/01/2022 18:52:00 - INFO - __main__ - Global step 1500 Train loss 0.29 EM 0.0 on epoch=749
03/01/2022 18:52:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.31 on epoch=754
03/01/2022 18:52:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.30 on epoch=759
03/01/2022 18:52:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=764
03/01/2022 18:52:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=769
03/01/2022 18:52:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.26 on epoch=774
03/01/2022 18:52:13 - INFO - __main__ - Global step 1550 Train loss 0.27 EM 0.0 on epoch=774
03/01/2022 18:52:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=779
03/01/2022 18:52:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=784
03/01/2022 18:52:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=789
03/01/2022 18:52:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=794
03/01/2022 18:52:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.26 on epoch=799
03/01/2022 18:52:26 - INFO - __main__ - Global step 1600 Train loss 0.28 EM 0.0 on epoch=799
03/01/2022 18:52:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=804
03/01/2022 18:52:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=809
03/01/2022 18:52:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.29 on epoch=814
03/01/2022 18:52:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=819
03/01/2022 18:52:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.26 on epoch=824
03/01/2022 18:52:39 - INFO - __main__ - Global step 1650 Train loss 0.26 EM 0.0 on epoch=824
03/01/2022 18:52:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.26 on epoch=829
03/01/2022 18:52:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=834
03/01/2022 18:52:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=839
03/01/2022 18:52:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.31 on epoch=844
03/01/2022 18:52:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=849
03/01/2022 18:52:51 - INFO - __main__ - Global step 1700 Train loss 0.27 EM 0.0 on epoch=849
03/01/2022 18:52:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=854
03/01/2022 18:52:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=859
03/01/2022 18:52:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=864
03/01/2022 18:53:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=869
03/01/2022 18:53:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.24 on epoch=874
03/01/2022 18:53:04 - INFO - __main__ - Global step 1750 Train loss 0.25 EM 0.0 on epoch=874
03/01/2022 18:53:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.27 on epoch=879
03/01/2022 18:53:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=884
03/01/2022 18:53:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=889
03/01/2022 18:53:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.29 on epoch=894
03/01/2022 18:53:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=899
03/01/2022 18:53:16 - INFO - __main__ - Global step 1800 Train loss 0.26 EM 0.0 on epoch=899
03/01/2022 18:53:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=904
03/01/2022 18:53:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=909
03/01/2022 18:53:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.22 on epoch=914
03/01/2022 18:53:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.23 on epoch=919
03/01/2022 18:53:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.25 on epoch=924
03/01/2022 18:53:29 - INFO - __main__ - Global step 1850 Train loss 0.22 EM 0.0 on epoch=924
03/01/2022 18:53:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=929
03/01/2022 18:53:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=934
03/01/2022 18:53:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=939
03/01/2022 18:53:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.16 on epoch=944
03/01/2022 18:53:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=949
03/01/2022 18:53:42 - INFO - __main__ - Global step 1900 Train loss 0.19 EM 0.0 on epoch=949
03/01/2022 18:53:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=954
03/01/2022 18:53:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=959
03/01/2022 18:53:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=964
03/01/2022 18:53:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=969
03/01/2022 18:53:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=974
03/01/2022 18:53:55 - INFO - __main__ - Global step 1950 Train loss 0.21 EM 0.0 on epoch=974
03/01/2022 18:53:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=979
03/01/2022 18:53:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=984
03/01/2022 18:54:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=989
03/01/2022 18:54:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=994
03/01/2022 18:54:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=999
03/01/2022 18:54:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:54:07 - INFO - __main__ - Printing 3 examples
03/01/2022 18:54:07 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 18:54:07 - INFO - __main__ - ['kieren fallon']
03/01/2022 18:54:07 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 18:54:07 - INFO - __main__ - ['uranus']
03/01/2022 18:54:07 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 18:54:07 - INFO - __main__ - ['rob thomas']
03/01/2022 18:54:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 18:54:07 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:54:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:54:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:54:08 - INFO - __main__ - Printing 3 examples
03/01/2022 18:54:08 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 18:54:08 - INFO - __main__ - ['terence rattigan']
03/01/2022 18:54:08 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 18:54:08 - INFO - __main__ - ['casino royale']
03/01/2022 18:54:08 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 18:54:08 - INFO - __main__ - ['offenbach']
03/01/2022 18:54:08 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:54:08 - INFO - __main__ - Global step 2000 Train loss 0.18 EM 0.0 on epoch=999
03/01/2022 18:54:08 - INFO - __main__ - save last model!
03/01/2022 18:54:08 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:54:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 18:54:08 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:54:08 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 18:54:08 - INFO - __main__ - Printing 3 examples
03/01/2022 18:54:08 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 18:54:08 - INFO - __main__ - ['taming of the shrew']
03/01/2022 18:54:08 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 18:54:08 - INFO - __main__ - ['henry fonda']
03/01/2022 18:54:08 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 18:54:08 - INFO - __main__ - ['tchaikovsky']
03/01/2022 18:54:08 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:54:09 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:54:13 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 18:54:19 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 18:54:19 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(1074, 376286), (672, 948582), (3489, 333521), (6319, 208284), (2690, 378727), (4999, 201462), (4302, 163912), (4946, 154063), (4264, 247591), (3735, 279578), (286, 2755883), (6059, 189253), (3679, 289187), (1958, 502002), (4233, 191191), (2476, 431307), (3339, 264161), (2818, 358712), (3812, 276017), (2550, 332246), (6597, 157538), (1047, 336403), (5997, 168391), (96, 7894015), (5806, 181362), (2027, 478074), (793, 959415), (2480, 384879), (4374, 230866), (3351, 190868), (6096, 168646), (2144, 386724), (3519, 196508), (4992, 190172), (4493, 209311), (106, 4519798), (928, 894961), (1923, 318040), (2361, 413361), (4279, 231910), (900, 380561), (2466, 307035), (6330, 161028), (3939, 254546), (4505, 197810), (5847, 169158), (4974, 176708), (3742, 272355), (3643, 219730), (4895, 169946), (3087, 284156), (2977, 265914), (3683, 206457), (2772, 342861), (3413, 306732), (2721, 367851), (2431, 400094), (1327, 735060), (1602, 475410), (2733, 298680), (1228, 739170), (4132, 174353), (2023, 494514), (2503, 477739), (2299, 422736), (93, 192649), (2957, 336282), (6637, 158975), (1665, 179358), (4163, 157427), (1635, 329909), (1116, 852428), (2082, 471031), (3118, 315958), (2028, 404951), (4197, 260190), (956, 507463), (3505, 311150), (164, 5765797), (5956, 175083), (1305, 698278), (2879, 339182), (503, 1684486), (600, 1458762), (6854, 169166), (3202, 321048), (1048, 702299), (4621, 190154), (1087, 843077), (131, 6653166), (6292, 163131), (3412, 328424), (5201, 199644), (3769, 195111), (5001, 211928), (3879, 275863), (2016, 495776), (581, 1531361), (2912, 367106)]
03/01/2022 18:54:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:54:20 - INFO - __main__ - Starting training!
03/01/2022 18:56:54 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_13_0.3_8_predictions.txt
03/01/2022 18:56:54 - INFO - __main__ - EM on test data: 0.0040
03/01/2022 18:56:55 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.3, bsz=8, dev_performance=0.03125, test_performance=0.00400600901352028
03/01/2022 18:56:55 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.2, bsz=8 ...
03/01/2022 18:56:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:56:56 - INFO - __main__ - Printing 3 examples
03/01/2022 18:56:56 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 18:56:56 - INFO - __main__ - ['kieren fallon']
03/01/2022 18:56:56 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 18:56:56 - INFO - __main__ - ['uranus']
03/01/2022 18:56:56 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 18:56:56 - INFO - __main__ - ['rob thomas']
03/01/2022 18:56:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 18:56:56 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:56:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:56:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:56:56 - INFO - __main__ - Printing 3 examples
03/01/2022 18:56:56 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 18:56:56 - INFO - __main__ - ['terence rattigan']
03/01/2022 18:56:56 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 18:56:56 - INFO - __main__ - ['casino royale']
03/01/2022 18:56:56 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 18:56:56 - INFO - __main__ - ['offenbach']
03/01/2022 18:56:56 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:56:56 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:56:56 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:57:10 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 18:57:10 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(1074, 376286), (672, 948582), (3489, 333521), (6319, 208284), (2690, 378727), (4999, 201462), (4302, 163912), (4946, 154063), (4264, 247591), (3735, 279578), (286, 2755883), (6059, 189253), (3679, 289187), (1958, 502002), (4233, 191191), (2476, 431307), (3339, 264161), (2818, 358712), (3812, 276017), (2550, 332246), (6597, 157538), (1047, 336403), (5997, 168391), (96, 7894015), (5806, 181362), (2027, 478074), (793, 959415), (2480, 384879), (4374, 230866), (3351, 190868), (6096, 168646), (2144, 386724), (3519, 196508), (4992, 190172), (4493, 209311), (106, 4519798), (928, 894961), (1923, 318040), (2361, 413361), (4279, 231910), (900, 380561), (2466, 307035), (6330, 161028), (3939, 254546), (4505, 197810), (5847, 169158), (4974, 176708), (3742, 272355), (3643, 219730), (4895, 169946), (3087, 284156), (2977, 265914), (3683, 206457), (2772, 342861), (3413, 306732), (2721, 367851), (2431, 400094), (1327, 735060), (1602, 475410), (2733, 298680), (1228, 739170), (4132, 174353), (2023, 494514), (2503, 477739), (2299, 422736), (93, 192649), (2957, 336282), (6637, 158975), (1665, 179358), (4163, 157427), (1635, 329909), (1116, 852428), (2082, 471031), (3118, 315958), (2028, 404951), (4197, 260190), (956, 507463), (3505, 311150), (164, 5765797), (5956, 175083), (1305, 698278), (2879, 339182), (503, 1684486), (600, 1458762), (6854, 169166), (3202, 321048), (1048, 702299), (4621, 190154), (1087, 843077), (131, 6653166), (6292, 163131), (3412, 328424), (5201, 199644), (3769, 195111), (5001, 211928), (3879, 275863), (2016, 495776), (581, 1531361), (2912, 367106)]
03/01/2022 18:57:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:57:11 - INFO - __main__ - Starting training!
03/01/2022 18:57:13 - INFO - __main__ - Step 10 Global step 10 Train loss 4.63 on epoch=4
03/01/2022 18:57:15 - INFO - __main__ - Step 20 Global step 20 Train loss 4.38 on epoch=9
03/01/2022 18:57:18 - INFO - __main__ - Step 30 Global step 30 Train loss 4.06 on epoch=14
03/01/2022 18:57:20 - INFO - __main__ - Step 40 Global step 40 Train loss 3.71 on epoch=19
03/01/2022 18:57:22 - INFO - __main__ - Step 50 Global step 50 Train loss 3.40 on epoch=24
03/01/2022 18:57:27 - INFO - __main__ - Global step 50 Train loss 4.04 EM 0.0 on epoch=24
03/01/2022 18:57:27 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 18:57:30 - INFO - __main__ - Step 60 Global step 60 Train loss 3.33 on epoch=29
03/01/2022 18:57:32 - INFO - __main__ - Step 70 Global step 70 Train loss 3.07 on epoch=34
03/01/2022 18:57:34 - INFO - __main__ - Step 80 Global step 80 Train loss 2.96 on epoch=39
03/01/2022 18:57:36 - INFO - __main__ - Step 90 Global step 90 Train loss 2.81 on epoch=44
03/01/2022 18:57:39 - INFO - __main__ - Step 100 Global step 100 Train loss 2.74 on epoch=49
03/01/2022 18:57:40 - INFO - __main__ - Global step 100 Train loss 2.98 EM 0.0 on epoch=49
03/01/2022 18:57:42 - INFO - __main__ - Step 110 Global step 110 Train loss 2.65 on epoch=54
03/01/2022 18:57:44 - INFO - __main__ - Step 120 Global step 120 Train loss 2.59 on epoch=59
03/01/2022 18:57:47 - INFO - __main__ - Step 130 Global step 130 Train loss 2.46 on epoch=64
03/01/2022 18:57:49 - INFO - __main__ - Step 140 Global step 140 Train loss 2.42 on epoch=69
03/01/2022 18:57:51 - INFO - __main__ - Step 150 Global step 150 Train loss 2.42 on epoch=74
03/01/2022 18:57:53 - INFO - __main__ - Global step 150 Train loss 2.51 EM 0.0 on epoch=74
03/01/2022 18:57:55 - INFO - __main__ - Step 160 Global step 160 Train loss 2.32 on epoch=79
03/01/2022 18:57:57 - INFO - __main__ - Step 170 Global step 170 Train loss 2.17 on epoch=84
03/01/2022 18:57:59 - INFO - __main__ - Step 180 Global step 180 Train loss 2.20 on epoch=89
03/01/2022 18:58:02 - INFO - __main__ - Step 190 Global step 190 Train loss 2.10 on epoch=94
03/01/2022 18:58:04 - INFO - __main__ - Step 200 Global step 200 Train loss 2.13 on epoch=99
03/01/2022 18:58:05 - INFO - __main__ - Global step 200 Train loss 2.18 EM 0.0 on epoch=99
03/01/2022 18:58:07 - INFO - __main__ - Step 210 Global step 210 Train loss 2.08 on epoch=104
03/01/2022 18:58:10 - INFO - __main__ - Step 220 Global step 220 Train loss 2.04 on epoch=109
03/01/2022 18:58:12 - INFO - __main__ - Step 230 Global step 230 Train loss 2.00 on epoch=114
03/01/2022 18:58:14 - INFO - __main__ - Step 240 Global step 240 Train loss 1.99 on epoch=119
03/01/2022 18:58:16 - INFO - __main__ - Step 250 Global step 250 Train loss 1.94 on epoch=124
03/01/2022 18:58:18 - INFO - __main__ - Global step 250 Train loss 2.01 EM 0.0 on epoch=124
03/01/2022 18:58:20 - INFO - __main__ - Step 260 Global step 260 Train loss 1.93 on epoch=129
03/01/2022 18:58:22 - INFO - __main__ - Step 270 Global step 270 Train loss 1.97 on epoch=134
03/01/2022 18:58:24 - INFO - __main__ - Step 280 Global step 280 Train loss 1.78 on epoch=139
03/01/2022 18:58:27 - INFO - __main__ - Step 290 Global step 290 Train loss 1.75 on epoch=144
03/01/2022 18:58:29 - INFO - __main__ - Step 300 Global step 300 Train loss 1.84 on epoch=149
03/01/2022 18:58:30 - INFO - __main__ - Global step 300 Train loss 1.85 EM 0.0 on epoch=149
03/01/2022 18:58:33 - INFO - __main__ - Step 310 Global step 310 Train loss 1.74 on epoch=154
03/01/2022 18:58:35 - INFO - __main__ - Step 320 Global step 320 Train loss 1.63 on epoch=159
03/01/2022 18:58:37 - INFO - __main__ - Step 330 Global step 330 Train loss 1.73 on epoch=164
03/01/2022 18:58:39 - INFO - __main__ - Step 340 Global step 340 Train loss 1.75 on epoch=169
03/01/2022 18:58:41 - INFO - __main__ - Step 350 Global step 350 Train loss 1.75 on epoch=174
03/01/2022 18:58:43 - INFO - __main__ - Global step 350 Train loss 1.72 EM 0.0 on epoch=174
03/01/2022 18:58:45 - INFO - __main__ - Step 360 Global step 360 Train loss 1.64 on epoch=179
03/01/2022 18:58:47 - INFO - __main__ - Step 370 Global step 370 Train loss 1.64 on epoch=184
03/01/2022 18:58:50 - INFO - __main__ - Step 380 Global step 380 Train loss 1.57 on epoch=189
03/01/2022 18:58:52 - INFO - __main__ - Step 390 Global step 390 Train loss 1.59 on epoch=194
03/01/2022 18:58:54 - INFO - __main__ - Step 400 Global step 400 Train loss 1.57 on epoch=199
03/01/2022 18:58:55 - INFO - __main__ - Global step 400 Train loss 1.60 EM 0.0 on epoch=199
03/01/2022 18:58:58 - INFO - __main__ - Step 410 Global step 410 Train loss 1.55 on epoch=204
03/01/2022 18:59:00 - INFO - __main__ - Step 420 Global step 420 Train loss 1.52 on epoch=209
03/01/2022 18:59:02 - INFO - __main__ - Step 430 Global step 430 Train loss 1.55 on epoch=214
03/01/2022 18:59:04 - INFO - __main__ - Step 440 Global step 440 Train loss 1.47 on epoch=219
03/01/2022 18:59:07 - INFO - __main__ - Step 450 Global step 450 Train loss 1.48 on epoch=224
03/01/2022 18:59:08 - INFO - __main__ - Global step 450 Train loss 1.52 EM 0.0 on epoch=224
03/01/2022 18:59:10 - INFO - __main__ - Step 460 Global step 460 Train loss 1.47 on epoch=229
03/01/2022 18:59:12 - INFO - __main__ - Step 470 Global step 470 Train loss 1.42 on epoch=234
03/01/2022 18:59:15 - INFO - __main__ - Step 480 Global step 480 Train loss 1.47 on epoch=239
03/01/2022 18:59:17 - INFO - __main__ - Step 490 Global step 490 Train loss 1.34 on epoch=244
03/01/2022 18:59:19 - INFO - __main__ - Step 500 Global step 500 Train loss 1.45 on epoch=249
03/01/2022 18:59:20 - INFO - __main__ - Global step 500 Train loss 1.43 EM 0.0 on epoch=249
03/01/2022 18:59:23 - INFO - __main__ - Step 510 Global step 510 Train loss 1.31 on epoch=254
03/01/2022 18:59:25 - INFO - __main__ - Step 520 Global step 520 Train loss 1.24 on epoch=259
03/01/2022 18:59:27 - INFO - __main__ - Step 530 Global step 530 Train loss 1.29 on epoch=264
03/01/2022 18:59:29 - INFO - __main__ - Step 540 Global step 540 Train loss 1.23 on epoch=269
03/01/2022 18:59:32 - INFO - __main__ - Step 550 Global step 550 Train loss 1.27 on epoch=274
03/01/2022 18:59:33 - INFO - __main__ - Global step 550 Train loss 1.27 EM 0.0 on epoch=274
03/01/2022 18:59:35 - INFO - __main__ - Step 560 Global step 560 Train loss 1.33 on epoch=279
03/01/2022 18:59:38 - INFO - __main__ - Step 570 Global step 570 Train loss 1.26 on epoch=284
03/01/2022 18:59:40 - INFO - __main__ - Step 580 Global step 580 Train loss 1.23 on epoch=289
03/01/2022 18:59:42 - INFO - __main__ - Step 590 Global step 590 Train loss 1.16 on epoch=294
03/01/2022 18:59:44 - INFO - __main__ - Step 600 Global step 600 Train loss 1.20 on epoch=299
03/01/2022 18:59:46 - INFO - __main__ - Global step 600 Train loss 1.24 EM 0.0 on epoch=299
03/01/2022 18:59:48 - INFO - __main__ - Step 610 Global step 610 Train loss 1.20 on epoch=304
03/01/2022 18:59:50 - INFO - __main__ - Step 620 Global step 620 Train loss 1.19 on epoch=309
03/01/2022 18:59:52 - INFO - __main__ - Step 630 Global step 630 Train loss 1.20 on epoch=314
03/01/2022 18:59:55 - INFO - __main__ - Step 640 Global step 640 Train loss 1.12 on epoch=319
03/01/2022 18:59:57 - INFO - __main__ - Step 650 Global step 650 Train loss 1.11 on epoch=324
03/01/2022 18:59:58 - INFO - __main__ - Global step 650 Train loss 1.16 EM 0.0 on epoch=324
03/01/2022 19:00:00 - INFO - __main__ - Step 660 Global step 660 Train loss 1.13 on epoch=329
03/01/2022 19:00:03 - INFO - __main__ - Step 670 Global step 670 Train loss 1.09 on epoch=334
03/01/2022 19:00:05 - INFO - __main__ - Step 680 Global step 680 Train loss 1.07 on epoch=339
03/01/2022 19:00:07 - INFO - __main__ - Step 690 Global step 690 Train loss 1.07 on epoch=344
03/01/2022 19:00:10 - INFO - __main__ - Step 700 Global step 700 Train loss 1.13 on epoch=349
03/01/2022 19:00:11 - INFO - __main__ - Global step 700 Train loss 1.10 EM 0.0 on epoch=349
03/01/2022 19:00:13 - INFO - __main__ - Step 710 Global step 710 Train loss 1.15 on epoch=354
03/01/2022 19:00:16 - INFO - __main__ - Step 720 Global step 720 Train loss 1.06 on epoch=359
03/01/2022 19:00:18 - INFO - __main__ - Step 730 Global step 730 Train loss 1.04 on epoch=364
03/01/2022 19:00:20 - INFO - __main__ - Step 740 Global step 740 Train loss 1.04 on epoch=369
03/01/2022 19:00:23 - INFO - __main__ - Step 750 Global step 750 Train loss 1.02 on epoch=374
03/01/2022 19:00:24 - INFO - __main__ - Global step 750 Train loss 1.06 EM 0.0 on epoch=374
03/01/2022 19:00:26 - INFO - __main__ - Step 760 Global step 760 Train loss 1.00 on epoch=379
03/01/2022 19:00:28 - INFO - __main__ - Step 770 Global step 770 Train loss 1.01 on epoch=384
03/01/2022 19:00:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.99 on epoch=389
03/01/2022 19:00:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.97 on epoch=394
03/01/2022 19:00:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.91 on epoch=399
03/01/2022 19:00:36 - INFO - __main__ - Global step 800 Train loss 0.98 EM 0.0 on epoch=399
03/01/2022 19:00:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.98 on epoch=404
03/01/2022 19:00:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.98 on epoch=409
03/01/2022 19:00:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.97 on epoch=414
03/01/2022 19:00:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.93 on epoch=419
03/01/2022 19:00:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.92 on epoch=424
03/01/2022 19:00:49 - INFO - __main__ - Global step 850 Train loss 0.96 EM 0.0 on epoch=424
03/01/2022 19:00:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.88 on epoch=429
03/01/2022 19:00:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.91 on epoch=434
03/01/2022 19:00:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=439
03/01/2022 19:00:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.84 on epoch=444
03/01/2022 19:01:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.88 on epoch=449
03/01/2022 19:01:01 - INFO - __main__ - Global step 900 Train loss 0.87 EM 0.0 on epoch=449
03/01/2022 19:01:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.87 on epoch=454
03/01/2022 19:01:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.87 on epoch=459
03/01/2022 19:01:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.88 on epoch=464
03/01/2022 19:01:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.91 on epoch=469
03/01/2022 19:01:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.80 on epoch=474
03/01/2022 19:01:13 - INFO - __main__ - Global step 950 Train loss 0.87 EM 0.0 on epoch=474
03/01/2022 19:01:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.86 on epoch=479
03/01/2022 19:01:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.77 on epoch=484
03/01/2022 19:01:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.80 on epoch=489
03/01/2022 19:01:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.83 on epoch=494
03/01/2022 19:01:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.84 on epoch=499
03/01/2022 19:01:26 - INFO - __main__ - Global step 1000 Train loss 0.82 EM 0.0 on epoch=499
03/01/2022 19:01:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.77 on epoch=504
03/01/2022 19:01:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.78 on epoch=509
03/01/2022 19:01:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.80 on epoch=514
03/01/2022 19:01:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.76 on epoch=519
03/01/2022 19:01:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.72 on epoch=524
03/01/2022 19:01:38 - INFO - __main__ - Global step 1050 Train loss 0.77 EM 0.0 on epoch=524
03/01/2022 19:01:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.70 on epoch=529
03/01/2022 19:01:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.66 on epoch=534
03/01/2022 19:01:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.76 on epoch=539
03/01/2022 19:01:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.78 on epoch=544
03/01/2022 19:01:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.75 on epoch=549
03/01/2022 19:01:50 - INFO - __main__ - Global step 1100 Train loss 0.73 EM 0.0 on epoch=549
03/01/2022 19:01:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.68 on epoch=554
03/01/2022 19:01:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.70 on epoch=559
03/01/2022 19:01:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.71 on epoch=564
03/01/2022 19:01:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.65 on epoch=569
03/01/2022 19:02:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.69 on epoch=574
03/01/2022 19:02:02 - INFO - __main__ - Global step 1150 Train loss 0.69 EM 0.0 on epoch=574
03/01/2022 19:02:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.70 on epoch=579
03/01/2022 19:02:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.70 on epoch=584
03/01/2022 19:02:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.61 on epoch=589
03/01/2022 19:02:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.67 on epoch=594
03/01/2022 19:02:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.74 on epoch=599
03/01/2022 19:02:15 - INFO - __main__ - Global step 1200 Train loss 0.68 EM 0.0 on epoch=599
03/01/2022 19:02:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.64 on epoch=604
03/01/2022 19:02:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.61 on epoch=609
03/01/2022 19:02:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.66 on epoch=614
03/01/2022 19:02:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.64 on epoch=619
03/01/2022 19:02:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.67 on epoch=624
03/01/2022 19:02:27 - INFO - __main__ - Global step 1250 Train loss 0.64 EM 0.0 on epoch=624
03/01/2022 19:02:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.67 on epoch=629
03/01/2022 19:02:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.63 on epoch=634
03/01/2022 19:02:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.67 on epoch=639
03/01/2022 19:02:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.61 on epoch=644
03/01/2022 19:02:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.58 on epoch=649
03/01/2022 19:02:39 - INFO - __main__ - Global step 1300 Train loss 0.63 EM 0.0 on epoch=649
03/01/2022 19:02:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.60 on epoch=654
03/01/2022 19:02:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.55 on epoch=659
03/01/2022 19:02:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.59 on epoch=664
03/01/2022 19:02:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.60 on epoch=669
03/01/2022 19:02:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.55 on epoch=674
03/01/2022 19:02:52 - INFO - __main__ - Global step 1350 Train loss 0.58 EM 0.0 on epoch=674
03/01/2022 19:02:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.59 on epoch=679
03/01/2022 19:02:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.56 on epoch=684
03/01/2022 19:02:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.56 on epoch=689
03/01/2022 19:03:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.59 on epoch=694
03/01/2022 19:03:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.53 on epoch=699
03/01/2022 19:03:04 - INFO - __main__ - Global step 1400 Train loss 0.57 EM 0.0 on epoch=699
03/01/2022 19:03:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.56 on epoch=704
03/01/2022 19:03:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.60 on epoch=709
03/01/2022 19:03:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.53 on epoch=714
03/01/2022 19:03:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.54 on epoch=719
03/01/2022 19:03:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.54 on epoch=724
03/01/2022 19:03:16 - INFO - __main__ - Global step 1450 Train loss 0.55 EM 0.0 on epoch=724
03/01/2022 19:03:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.52 on epoch=729
03/01/2022 19:03:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.49 on epoch=734
03/01/2022 19:03:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.50 on epoch=739
03/01/2022 19:03:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.49 on epoch=744
03/01/2022 19:03:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.50 on epoch=749
03/01/2022 19:03:28 - INFO - __main__ - Global step 1500 Train loss 0.50 EM 0.0 on epoch=749
03/01/2022 19:03:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.51 on epoch=754
03/01/2022 19:03:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=759
03/01/2022 19:03:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.55 on epoch=764
03/01/2022 19:03:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.52 on epoch=769
03/01/2022 19:03:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.50 on epoch=774
03/01/2022 19:03:40 - INFO - __main__ - Global step 1550 Train loss 0.50 EM 0.0 on epoch=774
03/01/2022 19:03:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.49 on epoch=779
03/01/2022 19:03:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.42 on epoch=784
03/01/2022 19:03:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.49 on epoch=789
03/01/2022 19:03:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.53 on epoch=794
03/01/2022 19:03:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=799
03/01/2022 19:03:53 - INFO - __main__ - Global step 1600 Train loss 0.47 EM 0.0 on epoch=799
03/01/2022 19:03:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=804
03/01/2022 19:03:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.52 on epoch=809
03/01/2022 19:03:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.46 on epoch=814
03/01/2022 19:04:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
03/01/2022 19:04:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=824
03/01/2022 19:04:05 - INFO - __main__ - Global step 1650 Train loss 0.46 EM 0.0 on epoch=824
03/01/2022 19:04:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=829
03/01/2022 19:04:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=834
03/01/2022 19:04:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
03/01/2022 19:04:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=844
03/01/2022 19:04:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=849
03/01/2022 19:04:17 - INFO - __main__ - Global step 1700 Train loss 0.43 EM 0.0 on epoch=849
03/01/2022 19:04:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=854
03/01/2022 19:04:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=859
03/01/2022 19:04:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=864
03/01/2022 19:04:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=869
03/01/2022 19:04:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=874
03/01/2022 19:04:30 - INFO - __main__ - Global step 1750 Train loss 0.43 EM 0.0 on epoch=874
03/01/2022 19:04:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=879
03/01/2022 19:04:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=884
03/01/2022 19:04:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=889
03/01/2022 19:04:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=894
03/01/2022 19:04:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=899
03/01/2022 19:04:42 - INFO - __main__ - Global step 1800 Train loss 0.40 EM 0.0 on epoch=899
03/01/2022 19:04:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=904
03/01/2022 19:04:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=909
03/01/2022 19:04:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=914
03/01/2022 19:04:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=919
03/01/2022 19:04:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=924
03/01/2022 19:04:54 - INFO - __main__ - Global step 1850 Train loss 0.38 EM 0.0 on epoch=924
03/01/2022 19:04:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=929
03/01/2022 19:04:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=934
03/01/2022 19:05:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=939
03/01/2022 19:05:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.41 on epoch=944
03/01/2022 19:05:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
03/01/2022 19:05:07 - INFO - __main__ - Global step 1900 Train loss 0.39 EM 0.0 on epoch=949
03/01/2022 19:05:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=954
03/01/2022 19:05:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=959
03/01/2022 19:05:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
03/01/2022 19:05:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.38 on epoch=969
03/01/2022 19:05:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
03/01/2022 19:05:19 - INFO - __main__ - Global step 1950 Train loss 0.38 EM 0.0 on epoch=974
03/01/2022 19:05:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=979
03/01/2022 19:05:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=984
03/01/2022 19:05:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=989
03/01/2022 19:05:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=994
03/01/2022 19:05:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=999
03/01/2022 19:05:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:05:31 - INFO - __main__ - Printing 3 examples
03/01/2022 19:05:31 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 19:05:31 - INFO - __main__ - ['camille saint-saens']
03/01/2022 19:05:31 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 19:05:31 - INFO - __main__ - ['madness']
03/01/2022 19:05:31 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 19:05:31 - INFO - __main__ - ['genevieve']
03/01/2022 19:05:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 19:05:31 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:05:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:05:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:05:31 - INFO - __main__ - Printing 3 examples
03/01/2022 19:05:31 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 19:05:31 - INFO - __main__ - ['will hay']
03/01/2022 19:05:31 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 19:05:31 - INFO - __main__ - ['alan sugar']
03/01/2022 19:05:31 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 19:05:31 - INFO - __main__ - ['cleopatra']
03/01/2022 19:05:31 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:05:31 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:05:31 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:05:31 - INFO - __main__ - Global step 2000 Train loss 0.35 EM 0.0 on epoch=999
03/01/2022 19:05:31 - INFO - __main__ - save last model!
03/01/2022 19:05:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 19:05:31 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 19:05:31 - INFO - __main__ - Printing 3 examples
03/01/2022 19:05:31 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 19:05:31 - INFO - __main__ - ['taming of the shrew']
03/01/2022 19:05:31 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 19:05:31 - INFO - __main__ - ['henry fonda']
03/01/2022 19:05:31 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 19:05:31 - INFO - __main__ - ['tchaikovsky']
03/01/2022 19:05:31 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:05:33 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:05:37 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 19:05:45 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 19:05:45 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(5492, 195474), (1068, 771470), (733, 1214890), (5682, 233612), (5433, 189971), (845, 1086618), (5661, 162439), (590, 1481555), (1953, 589554), (1631, 598174), (6364, 166204), (2896, 309135), (918, 967989), (2332, 372930), (1418, 745074), (6659, 167523), (4626, 225869), (3640, 155110), (6597, 157538), (2460, 406809), (5734, 179839), (3924, 246414), (4737, 208385), (4578, 211218), (2000, 315151), (6461, 159321), (4926, 217922), (2029, 307697), (6465, 164367), (297, 1324121), (6650, 154117), (1042, 857631), (2200, 439737), (2272, 432795), (2930, 350772), (937, 1007700), (1169, 574433), (1748, 386930), (855, 522337), (5413, 169514), (783, 1075853), (1717, 576145), (34, 32364675), (4420, 230091), (6446, 158041), (4775, 199538), (2501, 321680), (379, 2412996), (2253, 398597), (3266, 325487), (3227, 330589), (4913, 206391), (771, 1130105), (2510, 395690), (6723, 158917), (1736, 460906), (4783, 219874), (1221, 767154), (5963, 171669), (2824, 408492), (1449, 598263), (2875, 349182), (1492, 630896), (770, 1128621), (647, 1332269), (1773, 547210), (3278, 305705), (2558, 157757), (4239, 164202), (3065, 325101), (5452, 212734), (1344, 712448), (4852, 199901), (2783, 308812), (3551, 232925), (2940, 341076), (2169, 187157), (2795, 173107), (3463, 189435), (3876, 213357), (866, 1018574), (6803, 157013), (453, 1935608), (5275, 216061), (3008, 291537), (6894, 155616), (970, 369184), (298, 3090275), (324, 909673), (2594, 403711), (4477, 162791), (6539, 159792), (237, 3927936), (3372, 344743), (6209, 162555), (464, 1931312), (3535, 517891), (5859, 199324), (6157, 180769)]
03/01/2022 19:05:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:05:46 - INFO - __main__ - Starting training!
03/01/2022 19:08:12 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_13_0.2_8_predictions.txt
03/01/2022 19:08:12 - INFO - __main__ - EM on test data: 0.0025
03/01/2022 19:08:13 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.2, bsz=8, dev_performance=0.0, test_performance=0.0025037556334501754
03/01/2022 19:08:13 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.5, bsz=8 ...
03/01/2022 19:08:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:08:14 - INFO - __main__ - Printing 3 examples
03/01/2022 19:08:14 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 19:08:14 - INFO - __main__ - ['camille saint-saens']
03/01/2022 19:08:14 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 19:08:14 - INFO - __main__ - ['madness']
03/01/2022 19:08:14 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 19:08:14 - INFO - __main__ - ['genevieve']
03/01/2022 19:08:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 19:08:14 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:08:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:08:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:08:14 - INFO - __main__ - Printing 3 examples
03/01/2022 19:08:14 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 19:08:14 - INFO - __main__ - ['will hay']
03/01/2022 19:08:14 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 19:08:14 - INFO - __main__ - ['alan sugar']
03/01/2022 19:08:14 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 19:08:14 - INFO - __main__ - ['cleopatra']
03/01/2022 19:08:14 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:08:14 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:08:14 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:08:27 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 19:08:27 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(5492, 195474), (1068, 771470), (733, 1214890), (5682, 233612), (5433, 189971), (845, 1086618), (5661, 162439), (590, 1481555), (1953, 589554), (1631, 598174), (6364, 166204), (2896, 309135), (918, 967989), (2332, 372930), (1418, 745074), (6659, 167523), (4626, 225869), (3640, 155110), (6597, 157538), (2460, 406809), (5734, 179839), (3924, 246414), (4737, 208385), (4578, 211218), (2000, 315151), (6461, 159321), (4926, 217922), (2029, 307697), (6465, 164367), (297, 1324121), (6650, 154117), (1042, 857631), (2200, 439737), (2272, 432795), (2930, 350772), (937, 1007700), (1169, 574433), (1748, 386930), (855, 522337), (5413, 169514), (783, 1075853), (1717, 576145), (34, 32364675), (4420, 230091), (6446, 158041), (4775, 199538), (2501, 321680), (379, 2412996), (2253, 398597), (3266, 325487), (3227, 330589), (4913, 206391), (771, 1130105), (2510, 395690), (6723, 158917), (1736, 460906), (4783, 219874), (1221, 767154), (5963, 171669), (2824, 408492), (1449, 598263), (2875, 349182), (1492, 630896), (770, 1128621), (647, 1332269), (1773, 547210), (3278, 305705), (2558, 157757), (4239, 164202), (3065, 325101), (5452, 212734), (1344, 712448), (4852, 199901), (2783, 308812), (3551, 232925), (2940, 341076), (2169, 187157), (2795, 173107), (3463, 189435), (3876, 213357), (866, 1018574), (6803, 157013), (453, 1935608), (5275, 216061), (3008, 291537), (6894, 155616), (970, 369184), (298, 3090275), (324, 909673), (2594, 403711), (4477, 162791), (6539, 159792), (237, 3927936), (3372, 344743), (6209, 162555), (464, 1931312), (3535, 517891), (5859, 199324), (6157, 180769)]
03/01/2022 19:08:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:08:28 - INFO - __main__ - Starting training!
03/01/2022 19:08:31 - INFO - __main__ - Step 10 Global step 10 Train loss 5.48 on epoch=4
03/01/2022 19:08:33 - INFO - __main__ - Step 20 Global step 20 Train loss 4.27 on epoch=9
03/01/2022 19:08:35 - INFO - __main__ - Step 30 Global step 30 Train loss 3.66 on epoch=14
03/01/2022 19:08:38 - INFO - __main__ - Step 40 Global step 40 Train loss 3.32 on epoch=19
03/01/2022 19:08:40 - INFO - __main__ - Step 50 Global step 50 Train loss 3.11 on epoch=24
03/01/2022 19:08:41 - INFO - __main__ - Global step 50 Train loss 3.97 EM 0.0 on epoch=24
03/01/2022 19:08:41 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 19:08:44 - INFO - __main__ - Step 60 Global step 60 Train loss 2.86 on epoch=29
03/01/2022 19:08:46 - INFO - __main__ - Step 70 Global step 70 Train loss 2.70 on epoch=34
03/01/2022 19:08:48 - INFO - __main__ - Step 80 Global step 80 Train loss 2.63 on epoch=39
03/01/2022 19:08:51 - INFO - __main__ - Step 90 Global step 90 Train loss 2.51 on epoch=44
03/01/2022 19:08:53 - INFO - __main__ - Step 100 Global step 100 Train loss 2.47 on epoch=49
03/01/2022 19:08:54 - INFO - __main__ - Global step 100 Train loss 2.63 EM 0.0 on epoch=49
03/01/2022 19:08:56 - INFO - __main__ - Step 110 Global step 110 Train loss 2.34 on epoch=54
03/01/2022 19:08:58 - INFO - __main__ - Step 120 Global step 120 Train loss 2.25 on epoch=59
03/01/2022 19:09:00 - INFO - __main__ - Step 130 Global step 130 Train loss 2.23 on epoch=64
03/01/2022 19:09:03 - INFO - __main__ - Step 140 Global step 140 Train loss 2.22 on epoch=69
03/01/2022 19:09:05 - INFO - __main__ - Step 150 Global step 150 Train loss 2.17 on epoch=74
03/01/2022 19:09:06 - INFO - __main__ - Global step 150 Train loss 2.24 EM 0.0 on epoch=74
03/01/2022 19:09:08 - INFO - __main__ - Step 160 Global step 160 Train loss 2.20 on epoch=79
03/01/2022 19:09:10 - INFO - __main__ - Step 170 Global step 170 Train loss 2.03 on epoch=84
03/01/2022 19:09:12 - INFO - __main__ - Step 180 Global step 180 Train loss 2.03 on epoch=89
03/01/2022 19:09:14 - INFO - __main__ - Step 190 Global step 190 Train loss 2.02 on epoch=94
03/01/2022 19:09:17 - INFO - __main__ - Step 200 Global step 200 Train loss 1.86 on epoch=99
03/01/2022 19:09:18 - INFO - __main__ - Global step 200 Train loss 2.03 EM 0.0 on epoch=99
03/01/2022 19:09:20 - INFO - __main__ - Step 210 Global step 210 Train loss 1.80 on epoch=104
03/01/2022 19:09:22 - INFO - __main__ - Step 220 Global step 220 Train loss 1.75 on epoch=109
03/01/2022 19:09:25 - INFO - __main__ - Step 230 Global step 230 Train loss 1.78 on epoch=114
03/01/2022 19:09:27 - INFO - __main__ - Step 240 Global step 240 Train loss 1.64 on epoch=119
03/01/2022 19:09:29 - INFO - __main__ - Step 250 Global step 250 Train loss 1.60 on epoch=124
03/01/2022 19:09:30 - INFO - __main__ - Global step 250 Train loss 1.71 EM 0.0 on epoch=124
03/01/2022 19:09:32 - INFO - __main__ - Step 260 Global step 260 Train loss 1.45 on epoch=129
03/01/2022 19:09:34 - INFO - __main__ - Step 270 Global step 270 Train loss 1.48 on epoch=134
03/01/2022 19:09:37 - INFO - __main__ - Step 280 Global step 280 Train loss 1.51 on epoch=139
03/01/2022 19:09:39 - INFO - __main__ - Step 290 Global step 290 Train loss 1.44 on epoch=144
03/01/2022 19:09:41 - INFO - __main__ - Step 300 Global step 300 Train loss 1.37 on epoch=149
03/01/2022 19:09:42 - INFO - __main__ - Global step 300 Train loss 1.45 EM 0.0 on epoch=149
03/01/2022 19:09:44 - INFO - __main__ - Step 310 Global step 310 Train loss 1.44 on epoch=154
03/01/2022 19:09:47 - INFO - __main__ - Step 320 Global step 320 Train loss 1.34 on epoch=159
03/01/2022 19:09:49 - INFO - __main__ - Step 330 Global step 330 Train loss 1.30 on epoch=164
03/01/2022 19:09:51 - INFO - __main__ - Step 340 Global step 340 Train loss 1.25 on epoch=169
03/01/2022 19:09:53 - INFO - __main__ - Step 350 Global step 350 Train loss 1.31 on epoch=174
03/01/2022 19:09:54 - INFO - __main__ - Global step 350 Train loss 1.33 EM 0.0 on epoch=174
03/01/2022 19:09:57 - INFO - __main__ - Step 360 Global step 360 Train loss 1.32 on epoch=179
03/01/2022 19:09:59 - INFO - __main__ - Step 370 Global step 370 Train loss 1.15 on epoch=184
03/01/2022 19:10:01 - INFO - __main__ - Step 380 Global step 380 Train loss 1.27 on epoch=189
03/01/2022 19:10:03 - INFO - __main__ - Step 390 Global step 390 Train loss 1.19 on epoch=194
03/01/2022 19:10:05 - INFO - __main__ - Step 400 Global step 400 Train loss 1.13 on epoch=199
03/01/2022 19:10:06 - INFO - __main__ - Global step 400 Train loss 1.21 EM 0.0 on epoch=199
03/01/2022 19:10:09 - INFO - __main__ - Step 410 Global step 410 Train loss 1.03 on epoch=204
03/01/2022 19:10:11 - INFO - __main__ - Step 420 Global step 420 Train loss 1.07 on epoch=209
03/01/2022 19:10:13 - INFO - __main__ - Step 430 Global step 430 Train loss 1.06 on epoch=214
03/01/2022 19:10:15 - INFO - __main__ - Step 440 Global step 440 Train loss 1.08 on epoch=219
03/01/2022 19:10:17 - INFO - __main__ - Step 450 Global step 450 Train loss 1.03 on epoch=224
03/01/2022 19:10:19 - INFO - __main__ - Global step 450 Train loss 1.05 EM 0.0 on epoch=224
03/01/2022 19:10:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.93 on epoch=229
03/01/2022 19:10:23 - INFO - __main__ - Step 470 Global step 470 Train loss 1.05 on epoch=234
03/01/2022 19:10:25 - INFO - __main__ - Step 480 Global step 480 Train loss 1.00 on epoch=239
03/01/2022 19:10:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.91 on epoch=244
03/01/2022 19:10:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.92 on epoch=249
03/01/2022 19:10:31 - INFO - __main__ - Global step 500 Train loss 0.96 EM 0.0 on epoch=249
03/01/2022 19:10:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.84 on epoch=254
03/01/2022 19:10:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.91 on epoch=259
03/01/2022 19:10:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.89 on epoch=264
03/01/2022 19:10:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=269
03/01/2022 19:10:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.81 on epoch=274
03/01/2022 19:10:43 - INFO - __main__ - Global step 550 Train loss 0.85 EM 0.0 on epoch=274
03/01/2022 19:10:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.85 on epoch=279
03/01/2022 19:10:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.79 on epoch=284
03/01/2022 19:10:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.88 on epoch=289
03/01/2022 19:10:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.75 on epoch=294
03/01/2022 19:10:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.80 on epoch=299
03/01/2022 19:10:55 - INFO - __main__ - Global step 600 Train loss 0.81 EM 0.0 on epoch=299
03/01/2022 19:10:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.74 on epoch=304
03/01/2022 19:10:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=309
03/01/2022 19:11:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.71 on epoch=314
03/01/2022 19:11:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.70 on epoch=319
03/01/2022 19:11:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.71 on epoch=324
03/01/2022 19:11:07 - INFO - __main__ - Global step 650 Train loss 0.74 EM 0.0 on epoch=324
03/01/2022 19:11:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.73 on epoch=329
03/01/2022 19:11:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.69 on epoch=334
03/01/2022 19:11:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.66 on epoch=339
03/01/2022 19:11:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.64 on epoch=344
03/01/2022 19:11:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.59 on epoch=349
03/01/2022 19:11:19 - INFO - __main__ - Global step 700 Train loss 0.66 EM 0.0 on epoch=349
03/01/2022 19:11:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.58 on epoch=354
03/01/2022 19:11:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.61 on epoch=359
03/01/2022 19:11:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=364
03/01/2022 19:11:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.60 on epoch=369
03/01/2022 19:11:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.62 on epoch=374
03/01/2022 19:11:32 - INFO - __main__ - Global step 750 Train loss 0.58 EM 0.0 on epoch=374
03/01/2022 19:11:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.59 on epoch=379
03/01/2022 19:11:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.57 on epoch=384
03/01/2022 19:11:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.59 on epoch=389
03/01/2022 19:11:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=394
03/01/2022 19:11:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=399
03/01/2022 19:11:44 - INFO - __main__ - Global step 800 Train loss 0.55 EM 0.0 on epoch=399
03/01/2022 19:11:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=404
03/01/2022 19:11:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.52 on epoch=409
03/01/2022 19:11:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.52 on epoch=414
03/01/2022 19:11:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=419
03/01/2022 19:11:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=424
03/01/2022 19:11:56 - INFO - __main__ - Global step 850 Train loss 0.48 EM 0.0 on epoch=424
03/01/2022 19:11:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=429
03/01/2022 19:12:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=434
03/01/2022 19:12:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=439
03/01/2022 19:12:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=444
03/01/2022 19:12:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=449
03/01/2022 19:12:08 - INFO - __main__ - Global step 900 Train loss 0.46 EM 0.0 on epoch=449
03/01/2022 19:12:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=454
03/01/2022 19:12:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=459
03/01/2022 19:12:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=464
03/01/2022 19:12:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=469
03/01/2022 19:12:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=474
03/01/2022 19:12:20 - INFO - __main__ - Global step 950 Train loss 0.39 EM 0.0 on epoch=474
03/01/2022 19:12:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=479
03/01/2022 19:12:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=484
03/01/2022 19:12:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.32 on epoch=489
03/01/2022 19:12:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=494
03/01/2022 19:12:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=499
03/01/2022 19:12:33 - INFO - __main__ - Global step 1000 Train loss 0.37 EM 0.0 on epoch=499
03/01/2022 19:12:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=504
03/01/2022 19:12:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=509
03/01/2022 19:12:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=514
03/01/2022 19:12:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=519
03/01/2022 19:12:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=524
03/01/2022 19:12:45 - INFO - __main__ - Global step 1050 Train loss 0.33 EM 0.0 on epoch=524
03/01/2022 19:12:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=529
03/01/2022 19:12:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.31 on epoch=534
03/01/2022 19:12:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=539
03/01/2022 19:12:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=544
03/01/2022 19:12:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=549
03/01/2022 19:12:58 - INFO - __main__ - Global step 1100 Train loss 0.35 EM 0.0 on epoch=549
03/01/2022 19:13:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.28 on epoch=554
03/01/2022 19:13:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.26 on epoch=559
03/01/2022 19:13:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=564
03/01/2022 19:13:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=569
03/01/2022 19:13:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=574
03/01/2022 19:13:11 - INFO - __main__ - Global step 1150 Train loss 0.27 EM 0.0 on epoch=574
03/01/2022 19:13:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=579
03/01/2022 19:13:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=584
03/01/2022 19:13:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=589
03/01/2022 19:13:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=594
03/01/2022 19:13:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.24 on epoch=599
03/01/2022 19:13:23 - INFO - __main__ - Global step 1200 Train loss 0.26 EM 0.0 on epoch=599
03/01/2022 19:13:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=604
03/01/2022 19:13:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=609
03/01/2022 19:13:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=614
03/01/2022 19:13:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=619
03/01/2022 19:13:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.27 on epoch=624
03/01/2022 19:13:36 - INFO - __main__ - Global step 1250 Train loss 0.24 EM 0.0 on epoch=624
03/01/2022 19:13:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=629
03/01/2022 19:13:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=634
03/01/2022 19:13:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.29 on epoch=639
03/01/2022 19:13:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=644
03/01/2022 19:13:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.29 on epoch=649
03/01/2022 19:13:49 - INFO - __main__ - Global step 1300 Train loss 0.26 EM 0.0 on epoch=649
03/01/2022 19:13:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=654
03/01/2022 19:13:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=659
03/01/2022 19:13:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=664
03/01/2022 19:13:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=669
03/01/2022 19:14:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=674
03/01/2022 19:14:02 - INFO - __main__ - Global step 1350 Train loss 0.19 EM 0.0 on epoch=674
03/01/2022 19:14:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=679
03/01/2022 19:14:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=684
03/01/2022 19:14:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=689
03/01/2022 19:14:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=694
03/01/2022 19:14:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.15 on epoch=699
03/01/2022 19:14:14 - INFO - __main__ - Global step 1400 Train loss 0.19 EM 0.0 on epoch=699
03/01/2022 19:14:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=704
03/01/2022 19:14:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=709
03/01/2022 19:14:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=714
03/01/2022 19:14:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=719
03/01/2022 19:14:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=724
03/01/2022 19:14:27 - INFO - __main__ - Global step 1450 Train loss 0.20 EM 0.0 on epoch=724
03/01/2022 19:14:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=729
03/01/2022 19:14:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=734
03/01/2022 19:14:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=739
03/01/2022 19:14:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.12 on epoch=744
03/01/2022 19:14:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=749
03/01/2022 19:14:40 - INFO - __main__ - Global step 1500 Train loss 0.17 EM 0.0 on epoch=749
03/01/2022 19:14:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=754
03/01/2022 19:14:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=759
03/01/2022 19:14:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=764
03/01/2022 19:14:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=769
03/01/2022 19:14:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=774
03/01/2022 19:14:53 - INFO - __main__ - Global step 1550 Train loss 0.15 EM 0.0 on epoch=774
03/01/2022 19:14:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=779
03/01/2022 19:14:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=784
03/01/2022 19:15:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=789
03/01/2022 19:15:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=794
03/01/2022 19:15:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=799
03/01/2022 19:15:06 - INFO - __main__ - Global step 1600 Train loss 0.16 EM 0.0 on epoch=799
03/01/2022 19:15:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=804
03/01/2022 19:15:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=809
03/01/2022 19:15:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=814
03/01/2022 19:15:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=819
03/01/2022 19:15:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=824
03/01/2022 19:15:19 - INFO - __main__ - Global step 1650 Train loss 0.14 EM 0.0 on epoch=824
03/01/2022 19:15:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=829
03/01/2022 19:15:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=834
03/01/2022 19:15:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=839
03/01/2022 19:15:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=844
03/01/2022 19:15:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=849
03/01/2022 19:15:32 - INFO - __main__ - Global step 1700 Train loss 0.12 EM 0.0 on epoch=849
03/01/2022 19:15:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=854
03/01/2022 19:15:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=859
03/01/2022 19:15:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=864
03/01/2022 19:15:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=869
03/01/2022 19:15:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.16 on epoch=874
03/01/2022 19:15:45 - INFO - __main__ - Global step 1750 Train loss 0.13 EM 0.0 on epoch=874
03/01/2022 19:15:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=879
03/01/2022 19:15:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=884
03/01/2022 19:15:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=889
03/01/2022 19:15:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=894
03/01/2022 19:15:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=899
03/01/2022 19:15:58 - INFO - __main__ - Global step 1800 Train loss 0.10 EM 0.0 on epoch=899
03/01/2022 19:16:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=904
03/01/2022 19:16:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=909
03/01/2022 19:16:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=914
03/01/2022 19:16:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=919
03/01/2022 19:16:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/01/2022 19:16:11 - INFO - __main__ - Global step 1850 Train loss 0.11 EM 0.0 on epoch=924
03/01/2022 19:16:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=929
03/01/2022 19:16:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=934
03/01/2022 19:16:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=939
03/01/2022 19:16:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=944
03/01/2022 19:16:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=949
03/01/2022 19:16:24 - INFO - __main__ - Global step 1900 Train loss 0.10 EM 0.0 on epoch=949
03/01/2022 19:16:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=954
03/01/2022 19:16:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=959
03/01/2022 19:16:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=964
03/01/2022 19:16:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=969
03/01/2022 19:16:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=974
03/01/2022 19:16:37 - INFO - __main__ - Global step 1950 Train loss 0.11 EM 0.0 on epoch=974
03/01/2022 19:16:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=979
03/01/2022 19:16:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=984
03/01/2022 19:16:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=989
03/01/2022 19:16:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=994
03/01/2022 19:16:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=999
03/01/2022 19:16:50 - INFO - __main__ - Global step 2000 Train loss 0.11 EM 0.0 on epoch=999
03/01/2022 19:16:50 - INFO - __main__ - save last model!
03/01/2022 19:16:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 19:16:50 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 19:16:51 - INFO - __main__ - Printing 3 examples
03/01/2022 19:16:51 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 19:16:51 - INFO - __main__ - ['taming of the shrew']
03/01/2022 19:16:51 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 19:16:51 - INFO - __main__ - ['henry fonda']
03/01/2022 19:16:51 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 19:16:51 - INFO - __main__ - ['tchaikovsky']
03/01/2022 19:16:51 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:16:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:16:51 - INFO - __main__ - Printing 3 examples
03/01/2022 19:16:51 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 19:16:51 - INFO - __main__ - ['camille saint-saens']
03/01/2022 19:16:51 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 19:16:51 - INFO - __main__ - ['madness']
03/01/2022 19:16:51 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 19:16:51 - INFO - __main__ - ['genevieve']
03/01/2022 19:16:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 19:16:51 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:16:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:16:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:16:51 - INFO - __main__ - Printing 3 examples
03/01/2022 19:16:51 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 19:16:51 - INFO - __main__ - ['will hay']
03/01/2022 19:16:51 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 19:16:51 - INFO - __main__ - ['alan sugar']
03/01/2022 19:16:51 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 19:16:51 - INFO - __main__ - ['cleopatra']
03/01/2022 19:16:51 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:16:51 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:16:51 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:16:52 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:16:56 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 19:17:02 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 19:17:02 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(3258, 211425), (3213, 334916), (1909, 502837), (8857, 160161), (4836, 214321), (5433, 189971), (962, 911154), (4497, 199538), (1712, 268688), (355, 230108), (2271, 282308), (2560, 292543), (1090, 829527), (1212, 563029), (3426, 284907), (2600, 228530), (5100, 188863), (362, 325366), (4454, 233931), (5535, 170020), (424, 2133697), (3541, 206040), (1155, 940173), (2043, 299538), (2711, 367153), (3410, 290749), (1277, 761840), (4351, 231959), (4660, 182019), (16, 86594496), (2562, 172369), (5531, 202427), (4427, 227401), (531, 1463009), (1959, 488127), (6058, 180821), (3884, 259889), (4747, 187821), (5105, 211448), (577, 1526764), (2024, 448261), (2891, 347678), (6176, 170373), (526, 676575), (1744, 325562), (4183, 213298), (5308, 192463), (1310, 731782), (233, 2752414), (6865, 154321), (2409, 387991), (1429, 570527), (1618, 302520), (5642, 169703), (102, 5915536), (6195, 169481), (4890, 230195), (1039, 892616), (2451, 408517), (1293, 773200), (4327, 202585), (1248, 751368), (1705, 584257), (5563, 187825), (3381, 321221), (5918, 183921), (2786, 340142), (4285, 226300), (5872, 193577), (5250, 198049), (2692, 378744), (4659, 205143), (4173, 169588), (1469, 619260), (4321, 244144), (1921, 479667), (5352, 156643), (5286, 184140), (1472, 652757), (427, 1146202), (1047, 336403), (4463, 226544), (2403, 276759), (5209, 184306), (2787, 347383), (34, 32364675), (3033, 329937), (3275, 314100), (873, 159860), (6005, 161456), (4593, 190444), (1587, 599851), (3581, 209865), (4857, 173501), (4440, 192534), (2693, 271357), (2668, 283036), (5413, 169514), (6025, 171968)]
03/01/2022 19:17:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:17:03 - INFO - __main__ - Starting training!
03/01/2022 19:19:44 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_21_0.5_8_predictions.txt
03/01/2022 19:19:44 - INFO - __main__ - EM on test data: 0.0053
03/01/2022 19:19:45 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.5, bsz=8, dev_performance=0.0, test_performance=0.005257886830245368
03/01/2022 19:19:45 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.4, bsz=8 ...
03/01/2022 19:19:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:19:45 - INFO - __main__ - Printing 3 examples
03/01/2022 19:19:45 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 19:19:45 - INFO - __main__ - ['camille saint-saens']
03/01/2022 19:19:45 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 19:19:45 - INFO - __main__ - ['madness']
03/01/2022 19:19:45 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 19:19:45 - INFO - __main__ - ['genevieve']
03/01/2022 19:19:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 19:19:45 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:19:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:19:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:19:45 - INFO - __main__ - Printing 3 examples
03/01/2022 19:19:45 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 19:19:45 - INFO - __main__ - ['will hay']
03/01/2022 19:19:45 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 19:19:45 - INFO - __main__ - ['alan sugar']
03/01/2022 19:19:45 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 19:19:45 - INFO - __main__ - ['cleopatra']
03/01/2022 19:19:45 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:19:45 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:19:46 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:19:59 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 19:19:59 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(3258, 211425), (3213, 334916), (1909, 502837), (8857, 160161), (4836, 214321), (5433, 189971), (962, 911154), (4497, 199538), (1712, 268688), (355, 230108), (2271, 282308), (2560, 292543), (1090, 829527), (1212, 563029), (3426, 284907), (2600, 228530), (5100, 188863), (362, 325366), (4454, 233931), (5535, 170020), (424, 2133697), (3541, 206040), (1155, 940173), (2043, 299538), (2711, 367153), (3410, 290749), (1277, 761840), (4351, 231959), (4660, 182019), (16, 86594496), (2562, 172369), (5531, 202427), (4427, 227401), (531, 1463009), (1959, 488127), (6058, 180821), (3884, 259889), (4747, 187821), (5105, 211448), (577, 1526764), (2024, 448261), (2891, 347678), (6176, 170373), (526, 676575), (1744, 325562), (4183, 213298), (5308, 192463), (1310, 731782), (233, 2752414), (6865, 154321), (2409, 387991), (1429, 570527), (1618, 302520), (5642, 169703), (102, 5915536), (6195, 169481), (4890, 230195), (1039, 892616), (2451, 408517), (1293, 773200), (4327, 202585), (1248, 751368), (1705, 584257), (5563, 187825), (3381, 321221), (5918, 183921), (2786, 340142), (4285, 226300), (5872, 193577), (5250, 198049), (2692, 378744), (4659, 205143), (4173, 169588), (1469, 619260), (4321, 244144), (1921, 479667), (5352, 156643), (5286, 184140), (1472, 652757), (427, 1146202), (1047, 336403), (4463, 226544), (2403, 276759), (5209, 184306), (2787, 347383), (34, 32364675), (3033, 329937), (3275, 314100), (873, 159860), (6005, 161456), (4593, 190444), (1587, 599851), (3581, 209865), (4857, 173501), (4440, 192534), (2693, 271357), (2668, 283036), (5413, 169514), (6025, 171968)]
03/01/2022 19:20:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:20:00 - INFO - __main__ - Starting training!
03/01/2022 19:20:03 - INFO - __main__ - Step 10 Global step 10 Train loss 5.44 on epoch=4
03/01/2022 19:20:05 - INFO - __main__ - Step 20 Global step 20 Train loss 4.36 on epoch=9
03/01/2022 19:20:07 - INFO - __main__ - Step 30 Global step 30 Train loss 3.66 on epoch=14
03/01/2022 19:20:10 - INFO - __main__ - Step 40 Global step 40 Train loss 3.34 on epoch=19
03/01/2022 19:20:12 - INFO - __main__ - Step 50 Global step 50 Train loss 3.14 on epoch=24
03/01/2022 19:20:13 - INFO - __main__ - Global step 50 Train loss 3.99 EM 0.0 on epoch=24
03/01/2022 19:20:13 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 19:20:15 - INFO - __main__ - Step 60 Global step 60 Train loss 2.96 on epoch=29
03/01/2022 19:20:18 - INFO - __main__ - Step 70 Global step 70 Train loss 2.81 on epoch=34
03/01/2022 19:20:20 - INFO - __main__ - Step 80 Global step 80 Train loss 2.73 on epoch=39
03/01/2022 19:20:22 - INFO - __main__ - Step 90 Global step 90 Train loss 2.58 on epoch=44
03/01/2022 19:20:24 - INFO - __main__ - Step 100 Global step 100 Train loss 2.38 on epoch=49
03/01/2022 19:20:25 - INFO - __main__ - Global step 100 Train loss 2.69 EM 0.0 on epoch=49
03/01/2022 19:20:28 - INFO - __main__ - Step 110 Global step 110 Train loss 2.35 on epoch=54
03/01/2022 19:20:30 - INFO - __main__ - Step 120 Global step 120 Train loss 2.30 on epoch=59
03/01/2022 19:20:32 - INFO - __main__ - Step 130 Global step 130 Train loss 2.19 on epoch=64
03/01/2022 19:20:34 - INFO - __main__ - Step 140 Global step 140 Train loss 2.10 on epoch=69
03/01/2022 19:20:37 - INFO - __main__ - Step 150 Global step 150 Train loss 2.09 on epoch=74
03/01/2022 19:20:38 - INFO - __main__ - Global step 150 Train loss 2.21 EM 0.0 on epoch=74
03/01/2022 19:20:40 - INFO - __main__ - Step 160 Global step 160 Train loss 1.92 on epoch=79
03/01/2022 19:20:42 - INFO - __main__ - Step 170 Global step 170 Train loss 1.82 on epoch=84
03/01/2022 19:20:45 - INFO - __main__ - Step 180 Global step 180 Train loss 1.89 on epoch=89
03/01/2022 19:20:47 - INFO - __main__ - Step 190 Global step 190 Train loss 1.77 on epoch=94
03/01/2022 19:20:49 - INFO - __main__ - Step 200 Global step 200 Train loss 1.70 on epoch=99
03/01/2022 19:20:50 - INFO - __main__ - Global step 200 Train loss 1.82 EM 0.0 on epoch=99
03/01/2022 19:20:53 - INFO - __main__ - Step 210 Global step 210 Train loss 1.69 on epoch=104
03/01/2022 19:20:55 - INFO - __main__ - Step 220 Global step 220 Train loss 1.61 on epoch=109
03/01/2022 19:20:57 - INFO - __main__ - Step 230 Global step 230 Train loss 1.58 on epoch=114
03/01/2022 19:21:00 - INFO - __main__ - Step 240 Global step 240 Train loss 1.56 on epoch=119
03/01/2022 19:21:02 - INFO - __main__ - Step 250 Global step 250 Train loss 1.45 on epoch=124
03/01/2022 19:21:03 - INFO - __main__ - Global step 250 Train loss 1.58 EM 0.0 on epoch=124
03/01/2022 19:21:05 - INFO - __main__ - Step 260 Global step 260 Train loss 1.39 on epoch=129
03/01/2022 19:21:07 - INFO - __main__ - Step 270 Global step 270 Train loss 1.44 on epoch=134
03/01/2022 19:21:10 - INFO - __main__ - Step 280 Global step 280 Train loss 1.25 on epoch=139
03/01/2022 19:21:12 - INFO - __main__ - Step 290 Global step 290 Train loss 1.34 on epoch=144
03/01/2022 19:21:14 - INFO - __main__ - Step 300 Global step 300 Train loss 1.28 on epoch=149
03/01/2022 19:21:15 - INFO - __main__ - Global step 300 Train loss 1.34 EM 0.0 on epoch=149
03/01/2022 19:21:17 - INFO - __main__ - Step 310 Global step 310 Train loss 1.26 on epoch=154
03/01/2022 19:21:20 - INFO - __main__ - Step 320 Global step 320 Train loss 1.26 on epoch=159
03/01/2022 19:21:22 - INFO - __main__ - Step 330 Global step 330 Train loss 1.25 on epoch=164
03/01/2022 19:21:24 - INFO - __main__ - Step 340 Global step 340 Train loss 1.18 on epoch=169
03/01/2022 19:21:26 - INFO - __main__ - Step 350 Global step 350 Train loss 1.19 on epoch=174
03/01/2022 19:21:27 - INFO - __main__ - Global step 350 Train loss 1.23 EM 0.0 on epoch=174
03/01/2022 19:21:30 - INFO - __main__ - Step 360 Global step 360 Train loss 1.26 on epoch=179
03/01/2022 19:21:32 - INFO - __main__ - Step 370 Global step 370 Train loss 1.15 on epoch=184
03/01/2022 19:21:34 - INFO - __main__ - Step 380 Global step 380 Train loss 1.20 on epoch=189
03/01/2022 19:21:36 - INFO - __main__ - Step 390 Global step 390 Train loss 1.22 on epoch=194
03/01/2022 19:21:39 - INFO - __main__ - Step 400 Global step 400 Train loss 1.13 on epoch=199
03/01/2022 19:21:40 - INFO - __main__ - Global step 400 Train loss 1.19 EM 0.0 on epoch=199
03/01/2022 19:21:42 - INFO - __main__ - Step 410 Global step 410 Train loss 1.14 on epoch=204
03/01/2022 19:21:44 - INFO - __main__ - Step 420 Global step 420 Train loss 1.10 on epoch=209
03/01/2022 19:21:46 - INFO - __main__ - Step 430 Global step 430 Train loss 1.10 on epoch=214
03/01/2022 19:21:49 - INFO - __main__ - Step 440 Global step 440 Train loss 1.03 on epoch=219
03/01/2022 19:21:51 - INFO - __main__ - Step 450 Global step 450 Train loss 1.04 on epoch=224
03/01/2022 19:21:52 - INFO - __main__ - Global step 450 Train loss 1.08 EM 0.0 on epoch=224
03/01/2022 19:21:54 - INFO - __main__ - Step 460 Global step 460 Train loss 1.05 on epoch=229
03/01/2022 19:21:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.95 on epoch=234
03/01/2022 19:21:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.99 on epoch=239
03/01/2022 19:22:01 - INFO - __main__ - Step 490 Global step 490 Train loss 1.00 on epoch=244
03/01/2022 19:22:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.89 on epoch=249
03/01/2022 19:22:05 - INFO - __main__ - Global step 500 Train loss 0.97 EM 0.0 on epoch=249
03/01/2022 19:22:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.97 on epoch=254
03/01/2022 19:22:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.91 on epoch=259
03/01/2022 19:22:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.93 on epoch=264
03/01/2022 19:22:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.91 on epoch=269
03/01/2022 19:22:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.88 on epoch=274
03/01/2022 19:22:17 - INFO - __main__ - Global step 550 Train loss 0.92 EM 0.0 on epoch=274
03/01/2022 19:22:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.80 on epoch=279
03/01/2022 19:22:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.76 on epoch=284
03/01/2022 19:22:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.81 on epoch=289
03/01/2022 19:22:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.83 on epoch=294
03/01/2022 19:22:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.81 on epoch=299
03/01/2022 19:22:29 - INFO - __main__ - Global step 600 Train loss 0.80 EM 0.0 on epoch=299
03/01/2022 19:22:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.81 on epoch=304
03/01/2022 19:22:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.79 on epoch=309
03/01/2022 19:22:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.69 on epoch=314
03/01/2022 19:22:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.73 on epoch=319
03/01/2022 19:22:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.80 on epoch=324
03/01/2022 19:22:42 - INFO - __main__ - Global step 650 Train loss 0.76 EM 0.0 on epoch=324
03/01/2022 19:22:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.70 on epoch=329
03/01/2022 19:22:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.78 on epoch=334
03/01/2022 19:22:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.61 on epoch=339
03/01/2022 19:22:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.71 on epoch=344
03/01/2022 19:22:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.69 on epoch=349
03/01/2022 19:22:54 - INFO - __main__ - Global step 700 Train loss 0.70 EM 0.0 on epoch=349
03/01/2022 19:22:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.72 on epoch=354
03/01/2022 19:22:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=359
03/01/2022 19:23:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.65 on epoch=364
03/01/2022 19:23:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.75 on epoch=369
03/01/2022 19:23:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.61 on epoch=374
03/01/2022 19:23:07 - INFO - __main__ - Global step 750 Train loss 0.66 EM 0.0 on epoch=374
03/01/2022 19:23:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.61 on epoch=379
03/01/2022 19:23:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.58 on epoch=384
03/01/2022 19:23:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.52 on epoch=389
03/01/2022 19:23:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.58 on epoch=394
03/01/2022 19:23:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=399
03/01/2022 19:23:19 - INFO - __main__ - Global step 800 Train loss 0.58 EM 0.0 on epoch=399
03/01/2022 19:23:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.56 on epoch=404
03/01/2022 19:23:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=409
03/01/2022 19:23:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.56 on epoch=414
03/01/2022 19:23:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.53 on epoch=419
03/01/2022 19:23:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=424
03/01/2022 19:23:32 - INFO - __main__ - Global step 850 Train loss 0.52 EM 0.0 on epoch=424
03/01/2022 19:23:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=429
03/01/2022 19:23:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.54 on epoch=434
03/01/2022 19:23:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.53 on epoch=439
03/01/2022 19:23:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.54 on epoch=444
03/01/2022 19:23:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=449
03/01/2022 19:23:44 - INFO - __main__ - Global step 900 Train loss 0.53 EM 0.0 on epoch=449
03/01/2022 19:23:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=454
03/01/2022 19:23:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=459
03/01/2022 19:23:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=464
03/01/2022 19:23:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=469
03/01/2022 19:23:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=474
03/01/2022 19:23:57 - INFO - __main__ - Global step 950 Train loss 0.43 EM 0.0 on epoch=474
03/01/2022 19:23:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=479
03/01/2022 19:24:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=484
03/01/2022 19:24:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=489
03/01/2022 19:24:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=494
03/01/2022 19:24:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=499
03/01/2022 19:24:09 - INFO - __main__ - Global step 1000 Train loss 0.41 EM 0.0 on epoch=499
03/01/2022 19:24:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=504
03/01/2022 19:24:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=509
03/01/2022 19:24:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=514
03/01/2022 19:24:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=519
03/01/2022 19:24:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=524
03/01/2022 19:24:21 - INFO - __main__ - Global step 1050 Train loss 0.41 EM 0.0 on epoch=524
03/01/2022 19:24:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=529
03/01/2022 19:24:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.32 on epoch=534
03/01/2022 19:24:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=539
03/01/2022 19:24:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=544
03/01/2022 19:24:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=549
03/01/2022 19:24:34 - INFO - __main__ - Global step 1100 Train loss 0.34 EM 0.0 on epoch=549
03/01/2022 19:24:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=554
03/01/2022 19:24:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=559
03/01/2022 19:24:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=564
03/01/2022 19:24:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=569
03/01/2022 19:24:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=574
03/01/2022 19:24:47 - INFO - __main__ - Global step 1150 Train loss 0.35 EM 0.0 on epoch=574
03/01/2022 19:24:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.29 on epoch=579
03/01/2022 19:24:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=584
03/01/2022 19:24:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.26 on epoch=589
03/01/2022 19:24:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=594
03/01/2022 19:24:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=599
03/01/2022 19:24:59 - INFO - __main__ - Global step 1200 Train loss 0.30 EM 0.0 on epoch=599
03/01/2022 19:25:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.29 on epoch=604
03/01/2022 19:25:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=609
03/01/2022 19:25:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=614
03/01/2022 19:25:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=619
03/01/2022 19:25:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=624
03/01/2022 19:25:12 - INFO - __main__ - Global step 1250 Train loss 0.28 EM 0.0 on epoch=624
03/01/2022 19:25:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=629
03/01/2022 19:25:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=634
03/01/2022 19:25:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.30 on epoch=639
03/01/2022 19:25:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=644
03/01/2022 19:25:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=649
03/01/2022 19:25:24 - INFO - __main__ - Global step 1300 Train loss 0.26 EM 0.0 on epoch=649
03/01/2022 19:25:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=654
03/01/2022 19:25:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.27 on epoch=659
03/01/2022 19:25:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=664
03/01/2022 19:25:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=669
03/01/2022 19:25:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=674
03/01/2022 19:25:37 - INFO - __main__ - Global step 1350 Train loss 0.24 EM 0.0 on epoch=674
03/01/2022 19:25:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=679
03/01/2022 19:25:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=684
03/01/2022 19:25:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=689
03/01/2022 19:25:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=694
03/01/2022 19:25:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=699
03/01/2022 19:25:50 - INFO - __main__ - Global step 1400 Train loss 0.22 EM 0.0 on epoch=699
03/01/2022 19:25:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=704
03/01/2022 19:25:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=709
03/01/2022 19:25:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=714
03/01/2022 19:25:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=719
03/01/2022 19:26:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=724
03/01/2022 19:26:02 - INFO - __main__ - Global step 1450 Train loss 0.22 EM 0.0 on epoch=724
03/01/2022 19:26:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=729
03/01/2022 19:26:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.24 on epoch=734
03/01/2022 19:26:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=739
03/01/2022 19:26:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.26 on epoch=744
03/01/2022 19:26:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=749
03/01/2022 19:26:15 - INFO - __main__ - Global step 1500 Train loss 0.22 EM 0.0 on epoch=749
03/01/2022 19:26:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=754
03/01/2022 19:26:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=759
03/01/2022 19:26:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=764
03/01/2022 19:26:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=769
03/01/2022 19:26:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=774
03/01/2022 19:26:28 - INFO - __main__ - Global step 1550 Train loss 0.20 EM 0.0 on epoch=774
03/01/2022 19:26:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=779
03/01/2022 19:26:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=784
03/01/2022 19:26:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=789
03/01/2022 19:26:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.17 on epoch=794
03/01/2022 19:26:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=799
03/01/2022 19:26:40 - INFO - __main__ - Global step 1600 Train loss 0.17 EM 0.0 on epoch=799
03/01/2022 19:26:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.16 on epoch=804
03/01/2022 19:26:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=809
03/01/2022 19:26:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=814
03/01/2022 19:26:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=819
03/01/2022 19:26:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=824
03/01/2022 19:26:53 - INFO - __main__ - Global step 1650 Train loss 0.15 EM 0.0 on epoch=824
03/01/2022 19:26:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=829
03/01/2022 19:26:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=834
03/01/2022 19:27:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=839
03/01/2022 19:27:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=844
03/01/2022 19:27:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=849
03/01/2022 19:27:06 - INFO - __main__ - Global step 1700 Train loss 0.15 EM 0.0 on epoch=849
03/01/2022 19:27:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=854
03/01/2022 19:27:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.17 on epoch=859
03/01/2022 19:27:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=864
03/01/2022 19:27:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=869
03/01/2022 19:27:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=874
03/01/2022 19:27:18 - INFO - __main__ - Global step 1750 Train loss 0.14 EM 0.0 on epoch=874
03/01/2022 19:27:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=879
03/01/2022 19:27:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=884
03/01/2022 19:27:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=889
03/01/2022 19:27:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=894
03/01/2022 19:27:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=899
03/01/2022 19:27:31 - INFO - __main__ - Global step 1800 Train loss 0.14 EM 0.0 on epoch=899
03/01/2022 19:27:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=904
03/01/2022 19:27:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=909
03/01/2022 19:27:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=914
03/01/2022 19:27:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=919
03/01/2022 19:27:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/01/2022 19:27:44 - INFO - __main__ - Global step 1850 Train loss 0.12 EM 0.0 on epoch=924
03/01/2022 19:27:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.17 on epoch=929
03/01/2022 19:27:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=934
03/01/2022 19:27:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=939
03/01/2022 19:27:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.15 on epoch=944
03/01/2022 19:27:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=949
03/01/2022 19:27:56 - INFO - __main__ - Global step 1900 Train loss 0.14 EM 0.0 on epoch=949
03/01/2022 19:27:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=954
03/01/2022 19:28:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=959
03/01/2022 19:28:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.15 on epoch=964
03/01/2022 19:28:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=969
03/01/2022 19:28:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=974
03/01/2022 19:28:09 - INFO - __main__ - Global step 1950 Train loss 0.13 EM 0.0 on epoch=974
03/01/2022 19:28:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=979
03/01/2022 19:28:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=984
03/01/2022 19:28:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=989
03/01/2022 19:28:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=994
03/01/2022 19:28:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=999
03/01/2022 19:28:22 - INFO - __main__ - Global step 2000 Train loss 0.11 EM 0.0 on epoch=999
03/01/2022 19:28:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:28:22 - INFO - __main__ - Printing 3 examples
03/01/2022 19:28:22 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 19:28:22 - INFO - __main__ - ['camille saint-saens']
03/01/2022 19:28:22 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 19:28:22 - INFO - __main__ - ['madness']
03/01/2022 19:28:22 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 19:28:22 - INFO - __main__ - ['genevieve']
03/01/2022 19:28:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 19:28:22 - INFO - __main__ - save last model!
03/01/2022 19:28:22 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:28:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 19:28:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:28:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:28:22 - INFO - __main__ - Printing 3 examples
03/01/2022 19:28:22 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 19:28:22 - INFO - __main__ - ['will hay']
03/01/2022 19:28:22 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 19:28:22 - INFO - __main__ - ['alan sugar']
03/01/2022 19:28:22 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 19:28:22 - INFO - __main__ - ['cleopatra']
03/01/2022 19:28:22 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:28:22 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 19:28:22 - INFO - __main__ - Printing 3 examples
03/01/2022 19:28:22 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 19:28:22 - INFO - __main__ - ['taming of the shrew']
03/01/2022 19:28:22 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 19:28:22 - INFO - __main__ - ['henry fonda']
03/01/2022 19:28:22 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 19:28:22 - INFO - __main__ - ['tchaikovsky']
03/01/2022 19:28:22 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:28:22 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:28:22 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:28:23 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:28:27 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 19:28:33 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 19:28:33 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(4269, 248302), (4800, 205648), (3565, 292071), (1941, 484885), (1808, 389120), (6473, 163516), (4674, 244421), (2681, 395660), (4879, 229125), (502, 1761063), (3151, 296309), (4225, 197872), (4109, 245907), (4346, 242679), (1663, 588937), (4271, 188919), (441, 2112542), (1048, 702299), (5459, 186372), (4777, 158874), (3616, 283413), (1107, 853041), (4877, 212046), (553, 856375), (5721, 178984), (6061, 205009), (107, 5404644), (1188, 621816), (3563, 229598), (687, 582989), (863, 1077354), (2740, 199631), (2909, 350152), (2766, 278103), (4119, 235907), (828, 1018934), (2652, 286006), (4174, 175228), (4237, 246742), (3347, 297641), (4212, 191151), (6161, 172774), (420, 2224081), (762, 1164855), (2948, 393735), (3214, 325872), (2028, 404951), (990, 919957), (5028, 214929), (1101, 860462), (1116, 852428), (4378, 210800), (1396, 545283), (3305, 313410), (313, 2745242), (541, 1667482), (3566, 290067), (643, 1424557), (3834, 319344), (3759, 261834), (3698, 250226), (3184, 216978), (1566, 615576), (1845, 378247), (5908, 176655), (4501, 228152), (6326, 165064), (603, 506191), (1842, 531497), (776, 518408), (893, 1024942), (6672, 154162), (3677, 244767), (2326, 385363), (5638, 188290), (6193, 160491), (1361, 625400), (2491, 397483), (4794, 202669), (2669, 364149), (2781, 203418), (2488, 362236), (99, 8503604), (5718, 201254), (3231, 325809), (5154, 171205), (1486, 571578), (829, 1087334), (4929, 211594), (3434, 270859), (4305, 159538), (3164, 313795), (4685, 220739), (2507, 369601), (3757, 205092), (2917, 311362), (2124, 458893), (3479, 243656), (5558, 193177)]
03/01/2022 19:28:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:28:34 - INFO - __main__ - Starting training!
03/01/2022 19:31:00 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_21_0.4_8_predictions.txt
03/01/2022 19:31:00 - INFO - __main__ - EM on test data: 0.0035
03/01/2022 19:31:00 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.4, bsz=8, dev_performance=0.0, test_performance=0.0035052578868302454
03/01/2022 19:31:00 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.3, bsz=8 ...
03/01/2022 19:31:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:31:01 - INFO - __main__ - Printing 3 examples
03/01/2022 19:31:01 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 19:31:01 - INFO - __main__ - ['camille saint-saens']
03/01/2022 19:31:01 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 19:31:01 - INFO - __main__ - ['madness']
03/01/2022 19:31:01 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 19:31:01 - INFO - __main__ - ['genevieve']
03/01/2022 19:31:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 19:31:01 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:31:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:31:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:31:01 - INFO - __main__ - Printing 3 examples
03/01/2022 19:31:01 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 19:31:01 - INFO - __main__ - ['will hay']
03/01/2022 19:31:01 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 19:31:01 - INFO - __main__ - ['alan sugar']
03/01/2022 19:31:01 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 19:31:01 - INFO - __main__ - ['cleopatra']
03/01/2022 19:31:01 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:31:01 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:31:01 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:31:15 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 19:31:15 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(4269, 248302), (4800, 205648), (3565, 292071), (1941, 484885), (1808, 389120), (6473, 163516), (4674, 244421), (2681, 395660), (4879, 229125), (502, 1761063), (3151, 296309), (4225, 197872), (4109, 245907), (4346, 242679), (1663, 588937), (4271, 188919), (441, 2112542), (1048, 702299), (5459, 186372), (4777, 158874), (3616, 283413), (1107, 853041), (4877, 212046), (553, 856375), (5721, 178984), (6061, 205009), (107, 5404644), (1188, 621816), (3563, 229598), (687, 582989), (863, 1077354), (2740, 199631), (2909, 350152), (2766, 278103), (4119, 235907), (828, 1018934), (2652, 286006), (4174, 175228), (4237, 246742), (3347, 297641), (4212, 191151), (6161, 172774), (420, 2224081), (762, 1164855), (2948, 393735), (3214, 325872), (2028, 404951), (990, 919957), (5028, 214929), (1101, 860462), (1116, 852428), (4378, 210800), (1396, 545283), (3305, 313410), (313, 2745242), (541, 1667482), (3566, 290067), (643, 1424557), (3834, 319344), (3759, 261834), (3698, 250226), (3184, 216978), (1566, 615576), (1845, 378247), (5908, 176655), (4501, 228152), (6326, 165064), (603, 506191), (1842, 531497), (776, 518408), (893, 1024942), (6672, 154162), (3677, 244767), (2326, 385363), (5638, 188290), (6193, 160491), (1361, 625400), (2491, 397483), (4794, 202669), (2669, 364149), (2781, 203418), (2488, 362236), (99, 8503604), (5718, 201254), (3231, 325809), (5154, 171205), (1486, 571578), (829, 1087334), (4929, 211594), (3434, 270859), (4305, 159538), (3164, 313795), (4685, 220739), (2507, 369601), (3757, 205092), (2917, 311362), (2124, 458893), (3479, 243656), (5558, 193177)]
03/01/2022 19:31:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:31:16 - INFO - __main__ - Starting training!
03/01/2022 19:31:20 - INFO - __main__ - Step 10 Global step 10 Train loss 5.71 on epoch=4
03/01/2022 19:31:22 - INFO - __main__ - Step 20 Global step 20 Train loss 5.11 on epoch=9
03/01/2022 19:31:24 - INFO - __main__ - Step 30 Global step 30 Train loss 4.39 on epoch=14
03/01/2022 19:31:26 - INFO - __main__ - Step 40 Global step 40 Train loss 3.90 on epoch=19
03/01/2022 19:31:29 - INFO - __main__ - Step 50 Global step 50 Train loss 3.55 on epoch=24
03/01/2022 19:31:30 - INFO - __main__ - Global step 50 Train loss 4.53 EM 0.0 on epoch=24
03/01/2022 19:31:30 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 19:31:32 - INFO - __main__ - Step 60 Global step 60 Train loss 3.31 on epoch=29
03/01/2022 19:31:34 - INFO - __main__ - Step 70 Global step 70 Train loss 3.20 on epoch=34
03/01/2022 19:31:37 - INFO - __main__ - Step 80 Global step 80 Train loss 3.05 on epoch=39
03/01/2022 19:31:39 - INFO - __main__ - Step 90 Global step 90 Train loss 2.95 on epoch=44
03/01/2022 19:31:41 - INFO - __main__ - Step 100 Global step 100 Train loss 2.83 on epoch=49
03/01/2022 19:31:42 - INFO - __main__ - Global step 100 Train loss 3.07 EM 0.0 on epoch=49
03/01/2022 19:31:44 - INFO - __main__ - Step 110 Global step 110 Train loss 2.75 on epoch=54
03/01/2022 19:31:47 - INFO - __main__ - Step 120 Global step 120 Train loss 2.69 on epoch=59
03/01/2022 19:31:49 - INFO - __main__ - Step 130 Global step 130 Train loss 2.57 on epoch=64
03/01/2022 19:31:51 - INFO - __main__ - Step 140 Global step 140 Train loss 2.46 on epoch=69
03/01/2022 19:31:53 - INFO - __main__ - Step 150 Global step 150 Train loss 2.49 on epoch=74
03/01/2022 19:31:55 - INFO - __main__ - Global step 150 Train loss 2.59 EM 0.0 on epoch=74
03/01/2022 19:31:57 - INFO - __main__ - Step 160 Global step 160 Train loss 2.34 on epoch=79
03/01/2022 19:31:59 - INFO - __main__ - Step 170 Global step 170 Train loss 2.25 on epoch=84
03/01/2022 19:32:01 - INFO - __main__ - Step 180 Global step 180 Train loss 2.29 on epoch=89
03/01/2022 19:32:04 - INFO - __main__ - Step 190 Global step 190 Train loss 2.21 on epoch=94
03/01/2022 19:32:06 - INFO - __main__ - Step 200 Global step 200 Train loss 2.13 on epoch=99
03/01/2022 19:32:07 - INFO - __main__ - Global step 200 Train loss 2.24 EM 0.0 on epoch=99
03/01/2022 19:32:09 - INFO - __main__ - Step 210 Global step 210 Train loss 2.11 on epoch=104
03/01/2022 19:32:12 - INFO - __main__ - Step 220 Global step 220 Train loss 2.03 on epoch=109
03/01/2022 19:32:14 - INFO - __main__ - Step 230 Global step 230 Train loss 1.92 on epoch=114
03/01/2022 19:32:16 - INFO - __main__ - Step 240 Global step 240 Train loss 1.97 on epoch=119
03/01/2022 19:32:18 - INFO - __main__ - Step 250 Global step 250 Train loss 1.89 on epoch=124
03/01/2022 19:32:19 - INFO - __main__ - Global step 250 Train loss 1.98 EM 0.0 on epoch=124
03/01/2022 19:32:21 - INFO - __main__ - Step 260 Global step 260 Train loss 1.90 on epoch=129
03/01/2022 19:32:24 - INFO - __main__ - Step 270 Global step 270 Train loss 1.89 on epoch=134
03/01/2022 19:32:26 - INFO - __main__ - Step 280 Global step 280 Train loss 1.83 on epoch=139
03/01/2022 19:32:28 - INFO - __main__ - Step 290 Global step 290 Train loss 1.86 on epoch=144
03/01/2022 19:32:30 - INFO - __main__ - Step 300 Global step 300 Train loss 1.82 on epoch=149
03/01/2022 19:32:32 - INFO - __main__ - Global step 300 Train loss 1.86 EM 0.0 on epoch=149
03/01/2022 19:32:34 - INFO - __main__ - Step 310 Global step 310 Train loss 1.73 on epoch=154
03/01/2022 19:32:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.73 on epoch=159
03/01/2022 19:32:38 - INFO - __main__ - Step 330 Global step 330 Train loss 1.67 on epoch=164
03/01/2022 19:32:41 - INFO - __main__ - Step 340 Global step 340 Train loss 1.66 on epoch=169
03/01/2022 19:32:43 - INFO - __main__ - Step 350 Global step 350 Train loss 1.59 on epoch=174
03/01/2022 19:32:44 - INFO - __main__ - Global step 350 Train loss 1.67 EM 0.0 on epoch=174
03/01/2022 19:32:46 - INFO - __main__ - Step 360 Global step 360 Train loss 1.52 on epoch=179
03/01/2022 19:32:49 - INFO - __main__ - Step 370 Global step 370 Train loss 1.55 on epoch=184
03/01/2022 19:32:51 - INFO - __main__ - Step 380 Global step 380 Train loss 1.57 on epoch=189
03/01/2022 19:32:53 - INFO - __main__ - Step 390 Global step 390 Train loss 1.44 on epoch=194
03/01/2022 19:32:55 - INFO - __main__ - Step 400 Global step 400 Train loss 1.48 on epoch=199
03/01/2022 19:32:57 - INFO - __main__ - Global step 400 Train loss 1.51 EM 0.0 on epoch=199
03/01/2022 19:32:59 - INFO - __main__ - Step 410 Global step 410 Train loss 1.46 on epoch=204
03/01/2022 19:33:01 - INFO - __main__ - Step 420 Global step 420 Train loss 1.48 on epoch=209
03/01/2022 19:33:03 - INFO - __main__ - Step 430 Global step 430 Train loss 1.39 on epoch=214
03/01/2022 19:33:06 - INFO - __main__ - Step 440 Global step 440 Train loss 1.43 on epoch=219
03/01/2022 19:33:08 - INFO - __main__ - Step 450 Global step 450 Train loss 1.29 on epoch=224
03/01/2022 19:33:09 - INFO - __main__ - Global step 450 Train loss 1.41 EM 0.0 on epoch=224
03/01/2022 19:33:11 - INFO - __main__ - Step 460 Global step 460 Train loss 1.37 on epoch=229
03/01/2022 19:33:13 - INFO - __main__ - Step 470 Global step 470 Train loss 1.29 on epoch=234
03/01/2022 19:33:16 - INFO - __main__ - Step 480 Global step 480 Train loss 1.32 on epoch=239
03/01/2022 19:33:18 - INFO - __main__ - Step 490 Global step 490 Train loss 1.38 on epoch=244
03/01/2022 19:33:20 - INFO - __main__ - Step 500 Global step 500 Train loss 1.25 on epoch=249
03/01/2022 19:33:21 - INFO - __main__ - Global step 500 Train loss 1.32 EM 0.0 on epoch=249
03/01/2022 19:33:23 - INFO - __main__ - Step 510 Global step 510 Train loss 1.28 on epoch=254
03/01/2022 19:33:26 - INFO - __main__ - Step 520 Global step 520 Train loss 1.17 on epoch=259
03/01/2022 19:33:28 - INFO - __main__ - Step 530 Global step 530 Train loss 1.27 on epoch=264
03/01/2022 19:33:30 - INFO - __main__ - Step 540 Global step 540 Train loss 1.19 on epoch=269
03/01/2022 19:33:33 - INFO - __main__ - Step 550 Global step 550 Train loss 1.13 on epoch=274
03/01/2022 19:33:34 - INFO - __main__ - Global step 550 Train loss 1.21 EM 0.0 on epoch=274
03/01/2022 19:33:36 - INFO - __main__ - Step 560 Global step 560 Train loss 1.16 on epoch=279
03/01/2022 19:33:38 - INFO - __main__ - Step 570 Global step 570 Train loss 1.22 on epoch=284
03/01/2022 19:33:40 - INFO - __main__ - Step 580 Global step 580 Train loss 1.17 on epoch=289
03/01/2022 19:33:43 - INFO - __main__ - Step 590 Global step 590 Train loss 1.03 on epoch=294
03/01/2022 19:33:45 - INFO - __main__ - Step 600 Global step 600 Train loss 1.16 on epoch=299
03/01/2022 19:33:46 - INFO - __main__ - Global step 600 Train loss 1.15 EM 0.0 on epoch=299
03/01/2022 19:33:48 - INFO - __main__ - Step 610 Global step 610 Train loss 1.06 on epoch=304
03/01/2022 19:33:51 - INFO - __main__ - Step 620 Global step 620 Train loss 1.10 on epoch=309
03/01/2022 19:33:53 - INFO - __main__ - Step 630 Global step 630 Train loss 1.05 on epoch=314
03/01/2022 19:33:55 - INFO - __main__ - Step 640 Global step 640 Train loss 1.04 on epoch=319
03/01/2022 19:33:57 - INFO - __main__ - Step 650 Global step 650 Train loss 1.03 on epoch=324
03/01/2022 19:33:58 - INFO - __main__ - Global step 650 Train loss 1.06 EM 0.0 on epoch=324
03/01/2022 19:34:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.98 on epoch=329
03/01/2022 19:34:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.94 on epoch=334
03/01/2022 19:34:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.93 on epoch=339
03/01/2022 19:34:08 - INFO - __main__ - Step 690 Global step 690 Train loss 1.04 on epoch=344
03/01/2022 19:34:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.96 on epoch=349
03/01/2022 19:34:11 - INFO - __main__ - Global step 700 Train loss 0.97 EM 0.0 on epoch=349
03/01/2022 19:34:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.94 on epoch=354
03/01/2022 19:34:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.87 on epoch=359
03/01/2022 19:34:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.95 on epoch=364
03/01/2022 19:34:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.93 on epoch=369
03/01/2022 19:34:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.93 on epoch=374
03/01/2022 19:34:24 - INFO - __main__ - Global step 750 Train loss 0.92 EM 0.0 on epoch=374
03/01/2022 19:34:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.87 on epoch=379
03/01/2022 19:34:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.92 on epoch=384
03/01/2022 19:34:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.86 on epoch=389
03/01/2022 19:34:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.83 on epoch=394
03/01/2022 19:34:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.91 on epoch=399
03/01/2022 19:34:36 - INFO - __main__ - Global step 800 Train loss 0.88 EM 0.0 on epoch=399
03/01/2022 19:34:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.81 on epoch=404
03/01/2022 19:34:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.84 on epoch=409
03/01/2022 19:34:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.80 on epoch=414
03/01/2022 19:34:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.77 on epoch=419
03/01/2022 19:34:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.79 on epoch=424
03/01/2022 19:34:49 - INFO - __main__ - Global step 850 Train loss 0.80 EM 0.0 on epoch=424
03/01/2022 19:34:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.80 on epoch=429
03/01/2022 19:34:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.68 on epoch=434
03/01/2022 19:34:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.66 on epoch=439
03/01/2022 19:34:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.72 on epoch=444
03/01/2022 19:35:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.72 on epoch=449
03/01/2022 19:35:02 - INFO - __main__ - Global step 900 Train loss 0.72 EM 0.0 on epoch=449
03/01/2022 19:35:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.77 on epoch=454
03/01/2022 19:35:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.67 on epoch=459
03/01/2022 19:35:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.67 on epoch=464
03/01/2022 19:35:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.74 on epoch=469
03/01/2022 19:35:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.70 on epoch=474
03/01/2022 19:35:15 - INFO - __main__ - Global step 950 Train loss 0.71 EM 0.0 on epoch=474
03/01/2022 19:35:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.64 on epoch=479
03/01/2022 19:35:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.66 on epoch=484
03/01/2022 19:35:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.71 on epoch=489
03/01/2022 19:35:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.70 on epoch=494
03/01/2022 19:35:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.64 on epoch=499
03/01/2022 19:35:27 - INFO - __main__ - Global step 1000 Train loss 0.67 EM 0.0 on epoch=499
03/01/2022 19:35:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.69 on epoch=504
03/01/2022 19:35:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.63 on epoch=509
03/01/2022 19:35:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.58 on epoch=514
03/01/2022 19:35:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.52 on epoch=519
03/01/2022 19:35:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.53 on epoch=524
03/01/2022 19:35:40 - INFO - __main__ - Global step 1050 Train loss 0.59 EM 0.0 on epoch=524
03/01/2022 19:35:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.54 on epoch=529
03/01/2022 19:35:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.60 on epoch=534
03/01/2022 19:35:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.50 on epoch=539
03/01/2022 19:35:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.57 on epoch=544
03/01/2022 19:35:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.53 on epoch=549
03/01/2022 19:35:53 - INFO - __main__ - Global step 1100 Train loss 0.55 EM 0.0 on epoch=549
03/01/2022 19:35:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.54 on epoch=554
03/01/2022 19:35:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=559
03/01/2022 19:36:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.54 on epoch=564
03/01/2022 19:36:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=569
03/01/2022 19:36:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.49 on epoch=574
03/01/2022 19:36:05 - INFO - __main__ - Global step 1150 Train loss 0.50 EM 0.0 on epoch=574
03/01/2022 19:36:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=579
03/01/2022 19:36:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.52 on epoch=584
03/01/2022 19:36:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=589
03/01/2022 19:36:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=594
03/01/2022 19:36:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.52 on epoch=599
03/01/2022 19:36:18 - INFO - __main__ - Global step 1200 Train loss 0.50 EM 0.0 on epoch=599
03/01/2022 19:36:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=604
03/01/2022 19:36:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=609
03/01/2022 19:36:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=614
03/01/2022 19:36:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=619
03/01/2022 19:36:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=624
03/01/2022 19:36:31 - INFO - __main__ - Global step 1250 Train loss 0.41 EM 0.0 on epoch=624
03/01/2022 19:36:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=629
03/01/2022 19:36:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.48 on epoch=634
03/01/2022 19:36:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=639
03/01/2022 19:36:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=644
03/01/2022 19:36:42 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=649
03/01/2022 19:36:44 - INFO - __main__ - Global step 1300 Train loss 0.41 EM 0.0 on epoch=649
03/01/2022 19:36:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=654
03/01/2022 19:36:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=659
03/01/2022 19:36:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=664
03/01/2022 19:36:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=669
03/01/2022 19:36:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=674
03/01/2022 19:36:57 - INFO - __main__ - Global step 1350 Train loss 0.37 EM 0.0 on epoch=674
03/01/2022 19:36:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=679
03/01/2022 19:37:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=684
03/01/2022 19:37:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.32 on epoch=689
03/01/2022 19:37:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=694
03/01/2022 19:37:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=699
03/01/2022 19:37:09 - INFO - __main__ - Global step 1400 Train loss 0.33 EM 0.0 on epoch=699
03/01/2022 19:37:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=704
03/01/2022 19:37:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=709
03/01/2022 19:37:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.34 on epoch=714
03/01/2022 19:37:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.31 on epoch=719
03/01/2022 19:37:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=724
03/01/2022 19:37:22 - INFO - __main__ - Global step 1450 Train loss 0.33 EM 0.0 on epoch=724
03/01/2022 19:37:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=729
03/01/2022 19:37:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.31 on epoch=734
03/01/2022 19:37:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=739
03/01/2022 19:37:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=744
03/01/2022 19:37:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=749
03/01/2022 19:37:35 - INFO - __main__ - Global step 1500 Train loss 0.31 EM 0.0 on epoch=749
03/01/2022 19:37:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=754
03/01/2022 19:37:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=759
03/01/2022 19:37:42 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.28 on epoch=764
03/01/2022 19:37:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=769
03/01/2022 19:37:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=774
03/01/2022 19:37:47 - INFO - __main__ - Global step 1550 Train loss 0.28 EM 0.0 on epoch=774
03/01/2022 19:37:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=779
03/01/2022 19:37:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=784
03/01/2022 19:37:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.30 on epoch=789
03/01/2022 19:37:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=794
03/01/2022 19:37:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=799
03/01/2022 19:38:00 - INFO - __main__ - Global step 1600 Train loss 0.27 EM 0.0 on epoch=799
03/01/2022 19:38:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=804
03/01/2022 19:38:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=809
03/01/2022 19:38:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.29 on epoch=814
03/01/2022 19:38:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=819
03/01/2022 19:38:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.26 on epoch=824
03/01/2022 19:38:13 - INFO - __main__ - Global step 1650 Train loss 0.26 EM 0.0 on epoch=824
03/01/2022 19:38:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=829
03/01/2022 19:38:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.28 on epoch=834
03/01/2022 19:38:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=839
03/01/2022 19:38:22 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=844
03/01/2022 19:38:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=849
03/01/2022 19:38:25 - INFO - __main__ - Global step 1700 Train loss 0.23 EM 0.0 on epoch=849
03/01/2022 19:38:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=854
03/01/2022 19:38:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.20 on epoch=859
03/01/2022 19:38:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=864
03/01/2022 19:38:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=869
03/01/2022 19:38:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=874
03/01/2022 19:38:38 - INFO - __main__ - Global step 1750 Train loss 0.22 EM 0.0 on epoch=874
03/01/2022 19:38:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=879
03/01/2022 19:38:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=884
03/01/2022 19:38:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=889
03/01/2022 19:38:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=894
03/01/2022 19:38:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.25 on epoch=899
03/01/2022 19:38:51 - INFO - __main__ - Global step 1800 Train loss 0.21 EM 0.0 on epoch=899
03/01/2022 19:38:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=904
03/01/2022 19:38:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=909
03/01/2022 19:38:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=914
03/01/2022 19:39:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=919
03/01/2022 19:39:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=924
03/01/2022 19:39:03 - INFO - __main__ - Global step 1850 Train loss 0.21 EM 0.0 on epoch=924
03/01/2022 19:39:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=929
03/01/2022 19:39:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=934
03/01/2022 19:39:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.16 on epoch=939
03/01/2022 19:39:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=944
03/01/2022 19:39:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=949
03/01/2022 19:39:16 - INFO - __main__ - Global step 1900 Train loss 0.20 EM 0.0 on epoch=949
03/01/2022 19:39:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=954
03/01/2022 19:39:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=959
03/01/2022 19:39:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=964
03/01/2022 19:39:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=969
03/01/2022 19:39:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=974
03/01/2022 19:39:29 - INFO - __main__ - Global step 1950 Train loss 0.18 EM 0.0 on epoch=974
03/01/2022 19:39:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.24 on epoch=979
03/01/2022 19:39:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=984
03/01/2022 19:39:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.26 on epoch=989
03/01/2022 19:39:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=994
03/01/2022 19:39:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.19 on epoch=999
03/01/2022 19:39:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:39:41 - INFO - __main__ - Printing 3 examples
03/01/2022 19:39:41 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 19:39:41 - INFO - __main__ - ['camille saint-saens']
03/01/2022 19:39:41 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 19:39:41 - INFO - __main__ - ['madness']
03/01/2022 19:39:41 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 19:39:41 - INFO - __main__ - ['genevieve']
03/01/2022 19:39:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 19:39:41 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:39:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:39:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:39:41 - INFO - __main__ - Printing 3 examples
03/01/2022 19:39:41 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 19:39:41 - INFO - __main__ - ['will hay']
03/01/2022 19:39:41 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 19:39:41 - INFO - __main__ - ['alan sugar']
03/01/2022 19:39:41 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 19:39:41 - INFO - __main__ - ['cleopatra']
03/01/2022 19:39:41 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:39:41 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:39:41 - INFO - __main__ - Global step 2000 Train loss 0.25 EM 0.0 on epoch=999
03/01/2022 19:39:41 - INFO - __main__ - save last model!
03/01/2022 19:39:41 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:39:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 19:39:41 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 19:39:41 - INFO - __main__ - Printing 3 examples
03/01/2022 19:39:41 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 19:39:41 - INFO - __main__ - ['taming of the shrew']
03/01/2022 19:39:41 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 19:39:41 - INFO - __main__ - ['henry fonda']
03/01/2022 19:39:41 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 19:39:41 - INFO - __main__ - ['tchaikovsky']
03/01/2022 19:39:41 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:39:43 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:39:47 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 19:39:55 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 19:39:55 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(1130, 821238), (2505, 391017), (2260, 245889), (6412, 163495), (4205, 261446), (6219, 162939), (3217, 316619), (3119, 317140), (261, 3686702), (5120, 236973), (2851, 338896), (71, 9677051), (6326, 165064), (301, 1599306), (3882, 155103), (5676, 188429), (3248, 326955), (2134, 401363), (6997, 154928), (4199, 265295), (2620, 417765), (1389, 480591), (2756, 369138), (1502, 652657), (1040, 867513), (308, 1959584), (3116, 304867), (4158, 211917), (383, 2368094), (7228, 161576), (3889, 257596), (1691, 674454), (493, 921866), (2808, 317238), (2338, 287842), (4285, 226300), (1848, 517255), (2051, 455798), (1202, 672342), (5058, 173227), (3721, 256780), (6643, 176713), (4514, 186655), (2736, 340951), (1719, 551197), (2604, 359999), (2280, 193726), (3579, 277730), (1108, 766167), (3114, 200024), (809, 1073970), (1435, 636297), (2460, 406809), (5542, 236029), (6357, 167884), (3985, 256868), (983, 921089), (4083, 237800), (140, 5830325), (3001, 351252), (5545, 174101), (1427, 715870), (3541, 206040), (3701, 254712), (3026, 353651), (6881, 160252), (4294, 245995), (3221, 252473), (1037, 808814), (2657, 395294), (5030, 198426), (226, 3154509), (6398, 166527), (3438, 286245), (6733, 154076), (2351, 414849), (691, 1251430), (5163, 201510), (3068, 311224), (4456, 230159), (1295, 756208), (1024, 639303), (442, 1688276), (418, 2112116), (2777, 308767), (3180, 189715), (1323, 759069), (1015, 867172), (2948, 393735), (5589, 173942), (1303, 658549), (4033, 240888), (3845, 206498), (3783, 253073), (4871, 173063), (4531, 223921), (6297, 159697), (1656, 582760), (4657, 226628)]
03/01/2022 19:39:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:39:56 - INFO - __main__ - Starting training!
03/01/2022 19:42:22 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_21_0.3_8_predictions.txt
03/01/2022 19:42:22 - INFO - __main__ - EM on test data: 0.0048
03/01/2022 19:42:23 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.3, bsz=8, dev_performance=0.0, test_performance=0.004757135703555333
03/01/2022 19:42:23 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.2, bsz=8 ...
03/01/2022 19:42:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:42:24 - INFO - __main__ - Printing 3 examples
03/01/2022 19:42:24 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 19:42:24 - INFO - __main__ - ['camille saint-saens']
03/01/2022 19:42:24 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 19:42:24 - INFO - __main__ - ['madness']
03/01/2022 19:42:24 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 19:42:24 - INFO - __main__ - ['genevieve']
03/01/2022 19:42:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 19:42:24 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:42:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:42:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:42:24 - INFO - __main__ - Printing 3 examples
03/01/2022 19:42:24 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 19:42:24 - INFO - __main__ - ['will hay']
03/01/2022 19:42:24 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 19:42:24 - INFO - __main__ - ['alan sugar']
03/01/2022 19:42:24 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 19:42:24 - INFO - __main__ - ['cleopatra']
03/01/2022 19:42:24 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:42:24 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:42:24 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:42:37 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 19:42:37 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(1130, 821238), (2505, 391017), (2260, 245889), (6412, 163495), (4205, 261446), (6219, 162939), (3217, 316619), (3119, 317140), (261, 3686702), (5120, 236973), (2851, 338896), (71, 9677051), (6326, 165064), (301, 1599306), (3882, 155103), (5676, 188429), (3248, 326955), (2134, 401363), (6997, 154928), (4199, 265295), (2620, 417765), (1389, 480591), (2756, 369138), (1502, 652657), (1040, 867513), (308, 1959584), (3116, 304867), (4158, 211917), (383, 2368094), (7228, 161576), (3889, 257596), (1691, 674454), (493, 921866), (2808, 317238), (2338, 287842), (4285, 226300), (1848, 517255), (2051, 455798), (1202, 672342), (5058, 173227), (3721, 256780), (6643, 176713), (4514, 186655), (2736, 340951), (1719, 551197), (2604, 359999), (2280, 193726), (3579, 277730), (1108, 766167), (3114, 200024), (809, 1073970), (1435, 636297), (2460, 406809), (5542, 236029), (6357, 167884), (3985, 256868), (983, 921089), (4083, 237800), (140, 5830325), (3001, 351252), (5545, 174101), (1427, 715870), (3541, 206040), (3701, 254712), (3026, 353651), (6881, 160252), (4294, 245995), (3221, 252473), (1037, 808814), (2657, 395294), (5030, 198426), (226, 3154509), (6398, 166527), (3438, 286245), (6733, 154076), (2351, 414849), (691, 1251430), (5163, 201510), (3068, 311224), (4456, 230159), (1295, 756208), (1024, 639303), (442, 1688276), (418, 2112116), (2777, 308767), (3180, 189715), (1323, 759069), (1015, 867172), (2948, 393735), (5589, 173942), (1303, 658549), (4033, 240888), (3845, 206498), (3783, 253073), (4871, 173063), (4531, 223921), (6297, 159697), (1656, 582760), (4657, 226628)]
03/01/2022 19:42:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:42:38 - INFO - __main__ - Starting training!
03/01/2022 19:42:41 - INFO - __main__ - Step 10 Global step 10 Train loss 5.87 on epoch=4
03/01/2022 19:42:43 - INFO - __main__ - Step 20 Global step 20 Train loss 5.30 on epoch=9
03/01/2022 19:42:45 - INFO - __main__ - Step 30 Global step 30 Train loss 4.90 on epoch=14
03/01/2022 19:42:48 - INFO - __main__ - Step 40 Global step 40 Train loss 4.47 on epoch=19
03/01/2022 19:42:50 - INFO - __main__ - Step 50 Global step 50 Train loss 4.12 on epoch=24
03/01/2022 19:42:52 - INFO - __main__ - Global step 50 Train loss 4.93 EM 0.0 on epoch=24
03/01/2022 19:42:52 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 19:42:55 - INFO - __main__ - Step 60 Global step 60 Train loss 3.81 on epoch=29
03/01/2022 19:42:57 - INFO - __main__ - Step 70 Global step 70 Train loss 3.56 on epoch=34
03/01/2022 19:42:59 - INFO - __main__ - Step 80 Global step 80 Train loss 3.26 on epoch=39
03/01/2022 19:43:01 - INFO - __main__ - Step 90 Global step 90 Train loss 3.14 on epoch=44
03/01/2022 19:43:04 - INFO - __main__ - Step 100 Global step 100 Train loss 3.00 on epoch=49
03/01/2022 19:43:05 - INFO - __main__ - Global step 100 Train loss 3.36 EM 0.0 on epoch=49
03/01/2022 19:43:07 - INFO - __main__ - Step 110 Global step 110 Train loss 3.00 on epoch=54
03/01/2022 19:43:09 - INFO - __main__ - Step 120 Global step 120 Train loss 2.83 on epoch=59
03/01/2022 19:43:12 - INFO - __main__ - Step 130 Global step 130 Train loss 2.82 on epoch=64
03/01/2022 19:43:14 - INFO - __main__ - Step 140 Global step 140 Train loss 2.76 on epoch=69
03/01/2022 19:43:16 - INFO - __main__ - Step 150 Global step 150 Train loss 2.66 on epoch=74
03/01/2022 19:43:17 - INFO - __main__ - Global step 150 Train loss 2.82 EM 0.0 on epoch=74
03/01/2022 19:43:20 - INFO - __main__ - Step 160 Global step 160 Train loss 2.62 on epoch=79
03/01/2022 19:43:22 - INFO - __main__ - Step 170 Global step 170 Train loss 2.57 on epoch=84
03/01/2022 19:43:24 - INFO - __main__ - Step 180 Global step 180 Train loss 2.49 on epoch=89
03/01/2022 19:43:26 - INFO - __main__ - Step 190 Global step 190 Train loss 2.51 on epoch=94
03/01/2022 19:43:29 - INFO - __main__ - Step 200 Global step 200 Train loss 2.45 on epoch=99
03/01/2022 19:43:30 - INFO - __main__ - Global step 200 Train loss 2.53 EM 0.0 on epoch=99
03/01/2022 19:43:32 - INFO - __main__ - Step 210 Global step 210 Train loss 2.40 on epoch=104
03/01/2022 19:43:34 - INFO - __main__ - Step 220 Global step 220 Train loss 2.43 on epoch=109
03/01/2022 19:43:37 - INFO - __main__ - Step 230 Global step 230 Train loss 2.38 on epoch=114
03/01/2022 19:43:39 - INFO - __main__ - Step 240 Global step 240 Train loss 2.18 on epoch=119
03/01/2022 19:43:41 - INFO - __main__ - Step 250 Global step 250 Train loss 2.21 on epoch=124
03/01/2022 19:43:42 - INFO - __main__ - Global step 250 Train loss 2.32 EM 0.0 on epoch=124
03/01/2022 19:43:44 - INFO - __main__ - Step 260 Global step 260 Train loss 2.26 on epoch=129
03/01/2022 19:43:47 - INFO - __main__ - Step 270 Global step 270 Train loss 2.11 on epoch=134
03/01/2022 19:43:49 - INFO - __main__ - Step 280 Global step 280 Train loss 2.07 on epoch=139
03/01/2022 19:43:51 - INFO - __main__ - Step 290 Global step 290 Train loss 1.93 on epoch=144
03/01/2022 19:43:53 - INFO - __main__ - Step 300 Global step 300 Train loss 1.98 on epoch=149
03/01/2022 19:43:55 - INFO - __main__ - Global step 300 Train loss 2.07 EM 0.0 on epoch=149
03/01/2022 19:43:57 - INFO - __main__ - Step 310 Global step 310 Train loss 1.98 on epoch=154
03/01/2022 19:43:59 - INFO - __main__ - Step 320 Global step 320 Train loss 1.91 on epoch=159
03/01/2022 19:44:02 - INFO - __main__ - Step 330 Global step 330 Train loss 1.94 on epoch=164
03/01/2022 19:44:04 - INFO - __main__ - Step 340 Global step 340 Train loss 1.89 on epoch=169
03/01/2022 19:44:06 - INFO - __main__ - Step 350 Global step 350 Train loss 1.83 on epoch=174
03/01/2022 19:44:07 - INFO - __main__ - Global step 350 Train loss 1.91 EM 0.0 on epoch=174
03/01/2022 19:44:09 - INFO - __main__ - Step 360 Global step 360 Train loss 1.83 on epoch=179
03/01/2022 19:44:11 - INFO - __main__ - Step 370 Global step 370 Train loss 1.75 on epoch=184
03/01/2022 19:44:14 - INFO - __main__ - Step 380 Global step 380 Train loss 1.82 on epoch=189
03/01/2022 19:44:16 - INFO - __main__ - Step 390 Global step 390 Train loss 1.70 on epoch=194
03/01/2022 19:44:18 - INFO - __main__ - Step 400 Global step 400 Train loss 1.74 on epoch=199
03/01/2022 19:44:19 - INFO - __main__ - Global step 400 Train loss 1.77 EM 0.0 on epoch=199
03/01/2022 19:44:22 - INFO - __main__ - Step 410 Global step 410 Train loss 1.76 on epoch=204
03/01/2022 19:44:24 - INFO - __main__ - Step 420 Global step 420 Train loss 1.67 on epoch=209
03/01/2022 19:44:26 - INFO - __main__ - Step 430 Global step 430 Train loss 1.66 on epoch=214
03/01/2022 19:44:29 - INFO - __main__ - Step 440 Global step 440 Train loss 1.63 on epoch=219
03/01/2022 19:44:31 - INFO - __main__ - Step 450 Global step 450 Train loss 1.57 on epoch=224
03/01/2022 19:44:32 - INFO - __main__ - Global step 450 Train loss 1.66 EM 0.0 on epoch=224
03/01/2022 19:44:34 - INFO - __main__ - Step 460 Global step 460 Train loss 1.60 on epoch=229
03/01/2022 19:44:36 - INFO - __main__ - Step 470 Global step 470 Train loss 1.63 on epoch=234
03/01/2022 19:44:38 - INFO - __main__ - Step 480 Global step 480 Train loss 1.54 on epoch=239
03/01/2022 19:44:41 - INFO - __main__ - Step 490 Global step 490 Train loss 1.56 on epoch=244
03/01/2022 19:44:43 - INFO - __main__ - Step 500 Global step 500 Train loss 1.58 on epoch=249
03/01/2022 19:44:44 - INFO - __main__ - Global step 500 Train loss 1.58 EM 0.0 on epoch=249
03/01/2022 19:44:46 - INFO - __main__ - Step 510 Global step 510 Train loss 1.48 on epoch=254
03/01/2022 19:44:48 - INFO - __main__ - Step 520 Global step 520 Train loss 1.53 on epoch=259
03/01/2022 19:44:51 - INFO - __main__ - Step 530 Global step 530 Train loss 1.42 on epoch=264
03/01/2022 19:44:53 - INFO - __main__ - Step 540 Global step 540 Train loss 1.39 on epoch=269
03/01/2022 19:44:55 - INFO - __main__ - Step 550 Global step 550 Train loss 1.42 on epoch=274
03/01/2022 19:44:56 - INFO - __main__ - Global step 550 Train loss 1.45 EM 0.0 on epoch=274
03/01/2022 19:44:59 - INFO - __main__ - Step 560 Global step 560 Train loss 1.37 on epoch=279
03/01/2022 19:45:01 - INFO - __main__ - Step 570 Global step 570 Train loss 1.42 on epoch=284
03/01/2022 19:45:03 - INFO - __main__ - Step 580 Global step 580 Train loss 1.24 on epoch=289
03/01/2022 19:45:05 - INFO - __main__ - Step 590 Global step 590 Train loss 1.33 on epoch=294
03/01/2022 19:45:07 - INFO - __main__ - Step 600 Global step 600 Train loss 1.27 on epoch=299
03/01/2022 19:45:08 - INFO - __main__ - Global step 600 Train loss 1.33 EM 0.0 on epoch=299
03/01/2022 19:45:11 - INFO - __main__ - Step 610 Global step 610 Train loss 1.31 on epoch=304
03/01/2022 19:45:13 - INFO - __main__ - Step 620 Global step 620 Train loss 1.27 on epoch=309
03/01/2022 19:45:15 - INFO - __main__ - Step 630 Global step 630 Train loss 1.29 on epoch=314
03/01/2022 19:45:17 - INFO - __main__ - Step 640 Global step 640 Train loss 1.35 on epoch=319
03/01/2022 19:45:20 - INFO - __main__ - Step 650 Global step 650 Train loss 1.23 on epoch=324
03/01/2022 19:45:21 - INFO - __main__ - Global step 650 Train loss 1.29 EM 0.0 on epoch=324
03/01/2022 19:45:23 - INFO - __main__ - Step 660 Global step 660 Train loss 1.23 on epoch=329
03/01/2022 19:45:25 - INFO - __main__ - Step 670 Global step 670 Train loss 1.22 on epoch=334
03/01/2022 19:45:28 - INFO - __main__ - Step 680 Global step 680 Train loss 1.14 on epoch=339
03/01/2022 19:45:30 - INFO - __main__ - Step 690 Global step 690 Train loss 1.09 on epoch=344
03/01/2022 19:45:32 - INFO - __main__ - Step 700 Global step 700 Train loss 1.18 on epoch=349
03/01/2022 19:45:33 - INFO - __main__ - Global step 700 Train loss 1.17 EM 0.0 on epoch=349
03/01/2022 19:45:35 - INFO - __main__ - Step 710 Global step 710 Train loss 1.14 on epoch=354
03/01/2022 19:45:38 - INFO - __main__ - Step 720 Global step 720 Train loss 1.12 on epoch=359
03/01/2022 19:45:40 - INFO - __main__ - Step 730 Global step 730 Train loss 1.13 on epoch=364
03/01/2022 19:45:42 - INFO - __main__ - Step 740 Global step 740 Train loss 1.07 on epoch=369
03/01/2022 19:45:45 - INFO - __main__ - Step 750 Global step 750 Train loss 1.13 on epoch=374
03/01/2022 19:45:46 - INFO - __main__ - Global step 750 Train loss 1.12 EM 0.0 on epoch=374
03/01/2022 19:45:48 - INFO - __main__ - Step 760 Global step 760 Train loss 1.04 on epoch=379
03/01/2022 19:45:50 - INFO - __main__ - Step 770 Global step 770 Train loss 1.04 on epoch=384
03/01/2022 19:45:52 - INFO - __main__ - Step 780 Global step 780 Train loss 1.08 on epoch=389
03/01/2022 19:45:55 - INFO - __main__ - Step 790 Global step 790 Train loss 1.08 on epoch=394
03/01/2022 19:45:57 - INFO - __main__ - Step 800 Global step 800 Train loss 1.11 on epoch=399
03/01/2022 19:45:58 - INFO - __main__ - Global step 800 Train loss 1.07 EM 0.0 on epoch=399
03/01/2022 19:46:00 - INFO - __main__ - Step 810 Global step 810 Train loss 1.05 on epoch=404
03/01/2022 19:46:03 - INFO - __main__ - Step 820 Global step 820 Train loss 1.01 on epoch=409
03/01/2022 19:46:05 - INFO - __main__ - Step 830 Global step 830 Train loss 1.02 on epoch=414
03/01/2022 19:46:07 - INFO - __main__ - Step 840 Global step 840 Train loss 1.01 on epoch=419
03/01/2022 19:46:09 - INFO - __main__ - Step 850 Global step 850 Train loss 1.07 on epoch=424
03/01/2022 19:46:11 - INFO - __main__ - Global step 850 Train loss 1.03 EM 0.0 on epoch=424
03/01/2022 19:46:13 - INFO - __main__ - Step 860 Global step 860 Train loss 1.03 on epoch=429
03/01/2022 19:46:15 - INFO - __main__ - Step 870 Global step 870 Train loss 1.03 on epoch=434
03/01/2022 19:46:17 - INFO - __main__ - Step 880 Global step 880 Train loss 1.03 on epoch=439
03/01/2022 19:46:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.96 on epoch=444
03/01/2022 19:46:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.95 on epoch=449
03/01/2022 19:46:23 - INFO - __main__ - Global step 900 Train loss 1.00 EM 0.0 on epoch=449
03/01/2022 19:46:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.98 on epoch=454
03/01/2022 19:46:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.88 on epoch=459
03/01/2022 19:46:30 - INFO - __main__ - Step 930 Global step 930 Train loss 1.00 on epoch=464
03/01/2022 19:46:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.91 on epoch=469
03/01/2022 19:46:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.94 on epoch=474
03/01/2022 19:46:35 - INFO - __main__ - Global step 950 Train loss 0.94 EM 0.0 on epoch=474
03/01/2022 19:46:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.94 on epoch=479
03/01/2022 19:46:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.86 on epoch=484
03/01/2022 19:46:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.93 on epoch=489
03/01/2022 19:46:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.91 on epoch=494
03/01/2022 19:46:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.94 on epoch=499
03/01/2022 19:46:48 - INFO - __main__ - Global step 1000 Train loss 0.92 EM 0.0 on epoch=499
03/01/2022 19:46:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.90 on epoch=504
03/01/2022 19:46:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.91 on epoch=509
03/01/2022 19:46:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.88 on epoch=514
03/01/2022 19:46:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.86 on epoch=519
03/01/2022 19:46:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.87 on epoch=524
03/01/2022 19:47:01 - INFO - __main__ - Global step 1050 Train loss 0.88 EM 0.0 on epoch=524
03/01/2022 19:47:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.92 on epoch=529
03/01/2022 19:47:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.79 on epoch=534
03/01/2022 19:47:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.83 on epoch=539
03/01/2022 19:47:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.86 on epoch=544
03/01/2022 19:47:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.79 on epoch=549
03/01/2022 19:47:13 - INFO - __main__ - Global step 1100 Train loss 0.84 EM 0.0 on epoch=549
03/01/2022 19:47:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.84 on epoch=554
03/01/2022 19:47:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.82 on epoch=559
03/01/2022 19:47:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.83 on epoch=564
03/01/2022 19:47:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.75 on epoch=569
03/01/2022 19:47:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.75 on epoch=574
03/01/2022 19:47:26 - INFO - __main__ - Global step 1150 Train loss 0.80 EM 0.0 on epoch=574
03/01/2022 19:47:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.79 on epoch=579
03/01/2022 19:47:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.72 on epoch=584
03/01/2022 19:47:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.76 on epoch=589
03/01/2022 19:47:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.76 on epoch=594
03/01/2022 19:47:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.69 on epoch=599
03/01/2022 19:47:39 - INFO - __main__ - Global step 1200 Train loss 0.74 EM 0.0 on epoch=599
03/01/2022 19:47:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.71 on epoch=604
03/01/2022 19:47:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.68 on epoch=609
03/01/2022 19:47:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.69 on epoch=614
03/01/2022 19:47:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.74 on epoch=619
03/01/2022 19:47:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.66 on epoch=624
03/01/2022 19:47:51 - INFO - __main__ - Global step 1250 Train loss 0.69 EM 0.0 on epoch=624
03/01/2022 19:47:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.68 on epoch=629
03/01/2022 19:47:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.64 on epoch=634
03/01/2022 19:47:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.70 on epoch=639
03/01/2022 19:48:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.66 on epoch=644
03/01/2022 19:48:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.64 on epoch=649
03/01/2022 19:48:04 - INFO - __main__ - Global step 1300 Train loss 0.66 EM 0.0 on epoch=649
03/01/2022 19:48:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.70 on epoch=654
03/01/2022 19:48:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.64 on epoch=659
03/01/2022 19:48:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.66 on epoch=664
03/01/2022 19:48:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.68 on epoch=669
03/01/2022 19:48:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.64 on epoch=674
03/01/2022 19:48:16 - INFO - __main__ - Global step 1350 Train loss 0.67 EM 0.0 on epoch=674
03/01/2022 19:48:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.68 on epoch=679
03/01/2022 19:48:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.67 on epoch=684
03/01/2022 19:48:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.64 on epoch=689
03/01/2022 19:48:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.63 on epoch=694
03/01/2022 19:48:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.61 on epoch=699
03/01/2022 19:48:29 - INFO - __main__ - Global step 1400 Train loss 0.65 EM 0.0 on epoch=699
03/01/2022 19:48:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.61 on epoch=704
03/01/2022 19:48:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.63 on epoch=709
03/01/2022 19:48:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.65 on epoch=714
03/01/2022 19:48:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.55 on epoch=719
03/01/2022 19:48:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.55 on epoch=724
03/01/2022 19:48:42 - INFO - __main__ - Global step 1450 Train loss 0.60 EM 0.0 on epoch=724
03/01/2022 19:48:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.60 on epoch=729
03/01/2022 19:48:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.58 on epoch=734
03/01/2022 19:48:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.53 on epoch=739
03/01/2022 19:48:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.61 on epoch=744
03/01/2022 19:48:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.59 on epoch=749
03/01/2022 19:48:54 - INFO - __main__ - Global step 1500 Train loss 0.58 EM 0.0 on epoch=749
03/01/2022 19:48:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.51 on epoch=754
03/01/2022 19:48:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.48 on epoch=759
03/01/2022 19:49:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.46 on epoch=764
03/01/2022 19:49:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.57 on epoch=769
03/01/2022 19:49:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.63 on epoch=774
03/01/2022 19:49:07 - INFO - __main__ - Global step 1550 Train loss 0.53 EM 0.0 on epoch=774
03/01/2022 19:49:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.54 on epoch=779
03/01/2022 19:49:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.49 on epoch=784
03/01/2022 19:49:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.56 on epoch=789
03/01/2022 19:49:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.56 on epoch=794
03/01/2022 19:49:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.50 on epoch=799
03/01/2022 19:49:19 - INFO - __main__ - Global step 1600 Train loss 0.53 EM 0.0 on epoch=799
03/01/2022 19:49:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=804
03/01/2022 19:49:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.53 on epoch=809
03/01/2022 19:49:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.51 on epoch=814
03/01/2022 19:49:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.53 on epoch=819
03/01/2022 19:49:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.58 on epoch=824
03/01/2022 19:49:32 - INFO - __main__ - Global step 1650 Train loss 0.53 EM 0.0 on epoch=824
03/01/2022 19:49:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.51 on epoch=829
03/01/2022 19:49:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.47 on epoch=834
03/01/2022 19:49:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=839
03/01/2022 19:49:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.53 on epoch=844
03/01/2022 19:49:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=849
03/01/2022 19:49:45 - INFO - __main__ - Global step 1700 Train loss 0.48 EM 0.0 on epoch=849
03/01/2022 19:49:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.49 on epoch=854
03/01/2022 19:49:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.52 on epoch=859
03/01/2022 19:49:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.52 on epoch=864
03/01/2022 19:49:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.49 on epoch=869
03/01/2022 19:49:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.46 on epoch=874
03/01/2022 19:49:58 - INFO - __main__ - Global step 1750 Train loss 0.49 EM 0.0 on epoch=874
03/01/2022 19:50:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=879
03/01/2022 19:50:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.53 on epoch=884
03/01/2022 19:50:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
03/01/2022 19:50:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=894
03/01/2022 19:50:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=899
03/01/2022 19:50:10 - INFO - __main__ - Global step 1800 Train loss 0.47 EM 0.0 on epoch=899
03/01/2022 19:50:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=904
03/01/2022 19:50:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.49 on epoch=909
03/01/2022 19:50:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=914
03/01/2022 19:50:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=919
03/01/2022 19:50:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=924
03/01/2022 19:50:23 - INFO - __main__ - Global step 1850 Train loss 0.42 EM 0.0 on epoch=924
03/01/2022 19:50:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=929
03/01/2022 19:50:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=934
03/01/2022 19:50:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=939
03/01/2022 19:50:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=944
03/01/2022 19:50:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=949
03/01/2022 19:50:36 - INFO - __main__ - Global step 1900 Train loss 0.43 EM 0.0 on epoch=949
03/01/2022 19:50:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.42 on epoch=954
03/01/2022 19:50:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=959
03/01/2022 19:50:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=964
03/01/2022 19:50:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=969
03/01/2022 19:50:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=974
03/01/2022 19:50:49 - INFO - __main__ - Global step 1950 Train loss 0.40 EM 0.0 on epoch=974
03/01/2022 19:50:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=979
03/01/2022 19:50:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=984
03/01/2022 19:50:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=989
03/01/2022 19:50:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
03/01/2022 19:51:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=999
03/01/2022 19:51:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:51:01 - INFO - __main__ - Printing 3 examples
03/01/2022 19:51:01 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 19:51:01 - INFO - __main__ - ['francois mitterrand']
03/01/2022 19:51:01 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 19:51:01 - INFO - __main__ - ['james callaghan']
03/01/2022 19:51:01 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 19:51:01 - INFO - __main__ - ['aberdeen']
03/01/2022 19:51:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 19:51:01 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:51:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:51:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:51:01 - INFO - __main__ - Printing 3 examples
03/01/2022 19:51:01 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 19:51:01 - INFO - __main__ - ['tulisa']
03/01/2022 19:51:01 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 19:51:01 - INFO - __main__ - ['calgary']
03/01/2022 19:51:01 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 19:51:01 - INFO - __main__ - ['jeff bridges']
03/01/2022 19:51:01 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:51:01 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:51:01 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:51:01 - INFO - __main__ - Global step 2000 Train loss 0.39 EM 0.0 on epoch=999
03/01/2022 19:51:01 - INFO - __main__ - save last model!
03/01/2022 19:51:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 19:51:01 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 19:51:01 - INFO - __main__ - Printing 3 examples
03/01/2022 19:51:01 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 19:51:01 - INFO - __main__ - ['taming of the shrew']
03/01/2022 19:51:01 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 19:51:01 - INFO - __main__ - ['henry fonda']
03/01/2022 19:51:01 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 19:51:01 - INFO - __main__ - ['tchaikovsky']
03/01/2022 19:51:01 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:51:03 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:51:07 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 19:51:15 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 19:51:15 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(1096, 857963), (4738, 190584), (4049, 230165), (1793, 332226), (208, 2410751), (3699, 253359), (2204, 421709), (5094, 210937), (4813, 210454), (1976, 392476), (4110, 259070), (1053, 857202), (4273, 255574), (6171, 171690), (3084, 261725), (661, 1317687), (4284, 235257), (2728, 337315), (2019, 481757), (1308, 726600), (3541, 206040), (977, 908393), (2476, 431307), (4749, 157283), (241, 3838559), (705, 1548305), (3827, 282190), (2182, 379209), (1380, 703116), (2960, 165540), (2003, 445063), (726, 1159344), (5894, 169006), (1673, 440939), (6562, 159935), (1696, 423464), (2541, 395945), (2652, 286006), (1638, 604699), (6523, 157539), (37, 31055239), (2349, 335938), (1592, 619246), (1317, 681720), (4871, 173063), (2756, 369138), (1027, 519620), (1267, 795407), (298, 3090275), (1371, 673811), (168, 5553682), (3165, 262039), (1757, 390014), (7344, 159238), (2158, 290395), (174, 5233753), (927, 520773), (2859, 349629), (342, 1503107), (1275, 695113), (1919, 222828), (5458, 178957), (1128, 773025), (731, 1250799), (4162, 196307), (4849, 209310), (4187, 259804), (6546, 182629), (6114, 158081), (1172, 802296), (3273, 207524), (5776, 185572), (1572, 414616), (499, 1766405), (6166, 180142), (2715, 311751), (1907, 204437), (283, 2553263), (534, 1311231), (5351, 195051), (3431, 233202), (4676, 219209), (1200, 812133), (5216, 190231), (5182, 210925), (663, 1092211), (874, 1011784), (3106, 291674), (4556, 154640), (1161, 600418), (772, 1209132), (1135, 702740), (2306, 338371), (8857, 160161), (3772, 170716), (3627, 289143), (4234, 246090), (7216, 157191), (3465, 176815)]
03/01/2022 19:51:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:51:16 - INFO - __main__ - Starting training!
03/01/2022 19:53:46 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_21_0.2_8_predictions.txt
03/01/2022 19:53:46 - INFO - __main__ - EM on test data: 0.0025
03/01/2022 19:53:48 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.2, bsz=8, dev_performance=0.0, test_performance=0.0025037556334501754
03/01/2022 19:53:48 - INFO - __main__ - Running ... prefix=freebase_qa_32_42, lr=0.5, bsz=8 ...
03/01/2022 19:53:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:53:49 - INFO - __main__ - Printing 3 examples
03/01/2022 19:53:49 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 19:53:49 - INFO - __main__ - ['francois mitterrand']
03/01/2022 19:53:49 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 19:53:49 - INFO - __main__ - ['james callaghan']
03/01/2022 19:53:49 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 19:53:49 - INFO - __main__ - ['aberdeen']
03/01/2022 19:53:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 19:53:49 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:53:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:53:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:53:49 - INFO - __main__ - Printing 3 examples
03/01/2022 19:53:49 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 19:53:49 - INFO - __main__ - ['tulisa']
03/01/2022 19:53:49 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 19:53:49 - INFO - __main__ - ['calgary']
03/01/2022 19:53:49 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 19:53:49 - INFO - __main__ - ['jeff bridges']
03/01/2022 19:53:49 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:53:49 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:53:49 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:54:04 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 19:54:04 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(1096, 857963), (4738, 190584), (4049, 230165), (1793, 332226), (208, 2410751), (3699, 253359), (2204, 421709), (5094, 210937), (4813, 210454), (1976, 392476), (4110, 259070), (1053, 857202), (4273, 255574), (6171, 171690), (3084, 261725), (661, 1317687), (4284, 235257), (2728, 337315), (2019, 481757), (1308, 726600), (3541, 206040), (977, 908393), (2476, 431307), (4749, 157283), (241, 3838559), (705, 1548305), (3827, 282190), (2182, 379209), (1380, 703116), (2960, 165540), (2003, 445063), (726, 1159344), (5894, 169006), (1673, 440939), (6562, 159935), (1696, 423464), (2541, 395945), (2652, 286006), (1638, 604699), (6523, 157539), (37, 31055239), (2349, 335938), (1592, 619246), (1317, 681720), (4871, 173063), (2756, 369138), (1027, 519620), (1267, 795407), (298, 3090275), (1371, 673811), (168, 5553682), (3165, 262039), (1757, 390014), (7344, 159238), (2158, 290395), (174, 5233753), (927, 520773), (2859, 349629), (342, 1503107), (1275, 695113), (1919, 222828), (5458, 178957), (1128, 773025), (731, 1250799), (4162, 196307), (4849, 209310), (4187, 259804), (6546, 182629), (6114, 158081), (1172, 802296), (3273, 207524), (5776, 185572), (1572, 414616), (499, 1766405), (6166, 180142), (2715, 311751), (1907, 204437), (283, 2553263), (534, 1311231), (5351, 195051), (3431, 233202), (4676, 219209), (1200, 812133), (5216, 190231), (5182, 210925), (663, 1092211), (874, 1011784), (3106, 291674), (4556, 154640), (1161, 600418), (772, 1209132), (1135, 702740), (2306, 338371), (8857, 160161), (3772, 170716), (3627, 289143), (4234, 246090), (7216, 157191), (3465, 176815)]
03/01/2022 19:54:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:54:04 - INFO - __main__ - Starting training!
03/01/2022 19:54:07 - INFO - __main__ - Step 10 Global step 10 Train loss 4.94 on epoch=4
03/01/2022 19:54:09 - INFO - __main__ - Step 20 Global step 20 Train loss 3.90 on epoch=9
03/01/2022 19:54:11 - INFO - __main__ - Step 30 Global step 30 Train loss 3.37 on epoch=14
03/01/2022 19:54:14 - INFO - __main__ - Step 40 Global step 40 Train loss 3.12 on epoch=19
03/01/2022 19:54:16 - INFO - __main__ - Step 50 Global step 50 Train loss 2.90 on epoch=24
03/01/2022 19:54:17 - INFO - __main__ - Global step 50 Train loss 3.64 EM 0.0 on epoch=24
03/01/2022 19:54:17 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 19:54:20 - INFO - __main__ - Step 60 Global step 60 Train loss 2.88 on epoch=29
03/01/2022 19:54:22 - INFO - __main__ - Step 70 Global step 70 Train loss 2.70 on epoch=34
03/01/2022 19:54:24 - INFO - __main__ - Step 80 Global step 80 Train loss 2.73 on epoch=39
03/01/2022 19:54:26 - INFO - __main__ - Step 90 Global step 90 Train loss 2.51 on epoch=44
03/01/2022 19:54:29 - INFO - __main__ - Step 100 Global step 100 Train loss 2.43 on epoch=49
03/01/2022 19:54:30 - INFO - __main__ - Global step 100 Train loss 2.65 EM 0.0 on epoch=49
03/01/2022 19:54:32 - INFO - __main__ - Step 110 Global step 110 Train loss 2.35 on epoch=54
03/01/2022 19:54:34 - INFO - __main__ - Step 120 Global step 120 Train loss 2.22 on epoch=59
03/01/2022 19:54:37 - INFO - __main__ - Step 130 Global step 130 Train loss 2.23 on epoch=64
03/01/2022 19:54:39 - INFO - __main__ - Step 140 Global step 140 Train loss 2.12 on epoch=69
03/01/2022 19:54:41 - INFO - __main__ - Step 150 Global step 150 Train loss 2.07 on epoch=74
03/01/2022 19:54:42 - INFO - __main__ - Global step 150 Train loss 2.20 EM 0.0 on epoch=74
03/01/2022 19:54:45 - INFO - __main__ - Step 160 Global step 160 Train loss 2.02 on epoch=79
03/01/2022 19:54:47 - INFO - __main__ - Step 170 Global step 170 Train loss 1.83 on epoch=84
03/01/2022 19:54:49 - INFO - __main__ - Step 180 Global step 180 Train loss 1.87 on epoch=89
03/01/2022 19:54:52 - INFO - __main__ - Step 190 Global step 190 Train loss 1.72 on epoch=94
03/01/2022 19:54:54 - INFO - __main__ - Step 200 Global step 200 Train loss 1.68 on epoch=99
03/01/2022 19:54:55 - INFO - __main__ - Global step 200 Train loss 1.83 EM 0.0 on epoch=99
03/01/2022 19:54:57 - INFO - __main__ - Step 210 Global step 210 Train loss 1.62 on epoch=104
03/01/2022 19:55:00 - INFO - __main__ - Step 220 Global step 220 Train loss 1.55 on epoch=109
03/01/2022 19:55:02 - INFO - __main__ - Step 230 Global step 230 Train loss 1.49 on epoch=114
03/01/2022 19:55:04 - INFO - __main__ - Step 240 Global step 240 Train loss 1.46 on epoch=119
03/01/2022 19:55:06 - INFO - __main__ - Step 250 Global step 250 Train loss 1.49 on epoch=124
03/01/2022 19:55:08 - INFO - __main__ - Global step 250 Train loss 1.52 EM 0.0 on epoch=124
03/01/2022 19:55:10 - INFO - __main__ - Step 260 Global step 260 Train loss 1.37 on epoch=129
03/01/2022 19:55:12 - INFO - __main__ - Step 270 Global step 270 Train loss 1.30 on epoch=134
03/01/2022 19:55:14 - INFO - __main__ - Step 280 Global step 280 Train loss 1.35 on epoch=139
03/01/2022 19:55:17 - INFO - __main__ - Step 290 Global step 290 Train loss 1.23 on epoch=144
03/01/2022 19:55:19 - INFO - __main__ - Step 300 Global step 300 Train loss 1.22 on epoch=149
03/01/2022 19:55:20 - INFO - __main__ - Global step 300 Train loss 1.30 EM 0.0 on epoch=149
03/01/2022 19:55:23 - INFO - __main__ - Step 310 Global step 310 Train loss 1.16 on epoch=154
03/01/2022 19:55:25 - INFO - __main__ - Step 320 Global step 320 Train loss 1.24 on epoch=159
03/01/2022 19:55:27 - INFO - __main__ - Step 330 Global step 330 Train loss 1.16 on epoch=164
03/01/2022 19:55:29 - INFO - __main__ - Step 340 Global step 340 Train loss 1.18 on epoch=169
03/01/2022 19:55:32 - INFO - __main__ - Step 350 Global step 350 Train loss 1.12 on epoch=174
03/01/2022 19:55:33 - INFO - __main__ - Global step 350 Train loss 1.17 EM 0.0 on epoch=174
03/01/2022 19:55:35 - INFO - __main__ - Step 360 Global step 360 Train loss 1.02 on epoch=179
03/01/2022 19:55:37 - INFO - __main__ - Step 370 Global step 370 Train loss 1.13 on epoch=184
03/01/2022 19:55:40 - INFO - __main__ - Step 380 Global step 380 Train loss 1.05 on epoch=189
03/01/2022 19:55:42 - INFO - __main__ - Step 390 Global step 390 Train loss 1.03 on epoch=194
03/01/2022 19:55:44 - INFO - __main__ - Step 400 Global step 400 Train loss 1.04 on epoch=199
03/01/2022 19:55:45 - INFO - __main__ - Global step 400 Train loss 1.05 EM 0.0 on epoch=199
03/01/2022 19:55:48 - INFO - __main__ - Step 410 Global step 410 Train loss 1.00 on epoch=204
03/01/2022 19:55:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.94 on epoch=209
03/01/2022 19:55:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.99 on epoch=214
03/01/2022 19:55:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.94 on epoch=219
03/01/2022 19:55:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.91 on epoch=224
03/01/2022 19:55:58 - INFO - __main__ - Global step 450 Train loss 0.95 EM 0.0 on epoch=224
03/01/2022 19:56:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.79 on epoch=229
03/01/2022 19:56:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.92 on epoch=234
03/01/2022 19:56:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.82 on epoch=239
03/01/2022 19:56:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.91 on epoch=244
03/01/2022 19:56:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.85 on epoch=249
03/01/2022 19:56:10 - INFO - __main__ - Global step 500 Train loss 0.86 EM 0.0 on epoch=249
03/01/2022 19:56:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.80 on epoch=254
03/01/2022 19:56:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.77 on epoch=259
03/01/2022 19:56:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.79 on epoch=264
03/01/2022 19:56:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.74 on epoch=269
03/01/2022 19:56:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.76 on epoch=274
03/01/2022 19:56:23 - INFO - __main__ - Global step 550 Train loss 0.77 EM 0.0 on epoch=274
03/01/2022 19:56:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.67 on epoch=279
03/01/2022 19:56:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.73 on epoch=284
03/01/2022 19:56:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.68 on epoch=289
03/01/2022 19:56:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.71 on epoch=294
03/01/2022 19:56:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.66 on epoch=299
03/01/2022 19:56:36 - INFO - __main__ - Global step 600 Train loss 0.69 EM 0.0 on epoch=299
03/01/2022 19:56:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.72 on epoch=304
03/01/2022 19:56:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.63 on epoch=309
03/01/2022 19:56:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.67 on epoch=314
03/01/2022 19:56:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.57 on epoch=319
03/01/2022 19:56:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.56 on epoch=324
03/01/2022 19:56:48 - INFO - __main__ - Global step 650 Train loss 0.63 EM 0.0 on epoch=324
03/01/2022 19:56:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.63 on epoch=329
03/01/2022 19:56:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=334
03/01/2022 19:56:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.55 on epoch=339
03/01/2022 19:56:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.56 on epoch=344
03/01/2022 19:57:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=349
03/01/2022 19:57:01 - INFO - __main__ - Global step 700 Train loss 0.56 EM 0.0 on epoch=349
03/01/2022 19:57:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=354
03/01/2022 19:57:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=359
03/01/2022 19:57:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=364
03/01/2022 19:57:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.60 on epoch=369
03/01/2022 19:57:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=374
03/01/2022 19:57:14 - INFO - __main__ - Global step 750 Train loss 0.51 EM 0.0 on epoch=374
03/01/2022 19:57:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=379
03/01/2022 19:57:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=384
03/01/2022 19:57:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.55 on epoch=389
03/01/2022 19:57:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=394
03/01/2022 19:57:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=399
03/01/2022 19:57:27 - INFO - __main__ - Global step 800 Train loss 0.48 EM 0.0 on epoch=399
03/01/2022 19:57:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=404
03/01/2022 19:57:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=409
03/01/2022 19:57:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=414
03/01/2022 19:57:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.51 on epoch=419
03/01/2022 19:57:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=424
03/01/2022 19:57:39 - INFO - __main__ - Global step 850 Train loss 0.45 EM 0.0 on epoch=424
03/01/2022 19:57:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=429
03/01/2022 19:57:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=434
03/01/2022 19:57:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=439
03/01/2022 19:57:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=444
03/01/2022 19:57:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=449
03/01/2022 19:57:52 - INFO - __main__ - Global step 900 Train loss 0.38 EM 0.0 on epoch=449
03/01/2022 19:57:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=454
03/01/2022 19:57:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=459
03/01/2022 19:57:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=464
03/01/2022 19:58:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=469
03/01/2022 19:58:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=474
03/01/2022 19:58:05 - INFO - __main__ - Global step 950 Train loss 0.35 EM 0.0 on epoch=474
03/01/2022 19:58:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.31 on epoch=479
03/01/2022 19:58:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=484
03/01/2022 19:58:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=489
03/01/2022 19:58:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.27 on epoch=494
03/01/2022 19:58:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=499
03/01/2022 19:58:18 - INFO - __main__ - Global step 1000 Train loss 0.32 EM 0.0 on epoch=499
03/01/2022 19:58:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=504
03/01/2022 19:58:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=509
03/01/2022 19:58:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.33 on epoch=514
03/01/2022 19:58:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=519
03/01/2022 19:58:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.31 on epoch=524
03/01/2022 19:58:31 - INFO - __main__ - Global step 1050 Train loss 0.32 EM 0.0 on epoch=524
03/01/2022 19:58:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=529
03/01/2022 19:58:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=534
03/01/2022 19:58:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=539
03/01/2022 19:58:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=544
03/01/2022 19:58:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=549
03/01/2022 19:58:44 - INFO - __main__ - Global step 1100 Train loss 0.30 EM 0.0 on epoch=549
03/01/2022 19:58:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=554
03/01/2022 19:58:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=559
03/01/2022 19:58:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=564
03/01/2022 19:58:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=569
03/01/2022 19:58:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=574
03/01/2022 19:58:56 - INFO - __main__ - Global step 1150 Train loss 0.26 EM 0.0 on epoch=574
03/01/2022 19:58:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=579
03/01/2022 19:59:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=584
03/01/2022 19:59:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=589
03/01/2022 19:59:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=594
03/01/2022 19:59:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=599
03/01/2022 19:59:09 - INFO - __main__ - Global step 1200 Train loss 0.21 EM 0.0 on epoch=599
03/01/2022 19:59:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.23 on epoch=604
03/01/2022 19:59:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=609
03/01/2022 19:59:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=614
03/01/2022 19:59:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=619
03/01/2022 19:59:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=624
03/01/2022 19:59:22 - INFO - __main__ - Global step 1250 Train loss 0.21 EM 0.0 on epoch=624
03/01/2022 19:59:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=629
03/01/2022 19:59:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=634
03/01/2022 19:59:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=639
03/01/2022 19:59:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=644
03/01/2022 19:59:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=649
03/01/2022 19:59:34 - INFO - __main__ - Global step 1300 Train loss 0.21 EM 0.0 on epoch=649
03/01/2022 19:59:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=654
03/01/2022 19:59:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=659
03/01/2022 19:59:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=664
03/01/2022 19:59:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=669
03/01/2022 19:59:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=674
03/01/2022 19:59:47 - INFO - __main__ - Global step 1350 Train loss 0.20 EM 0.0 on epoch=674
03/01/2022 19:59:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=679
03/01/2022 19:59:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=684
03/01/2022 19:59:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=689
03/01/2022 19:59:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=694
03/01/2022 19:59:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=699
03/01/2022 20:00:00 - INFO - __main__ - Global step 1400 Train loss 0.19 EM 0.0 on epoch=699
03/01/2022 20:00:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=704
03/01/2022 20:00:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=709
03/01/2022 20:00:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=714
03/01/2022 20:00:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=719
03/01/2022 20:00:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=724
03/01/2022 20:00:13 - INFO - __main__ - Global step 1450 Train loss 0.18 EM 0.0 on epoch=724
03/01/2022 20:00:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=729
03/01/2022 20:00:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=734
03/01/2022 20:00:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=739
03/01/2022 20:00:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=744
03/01/2022 20:00:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=749
03/01/2022 20:00:25 - INFO - __main__ - Global step 1500 Train loss 0.17 EM 0.0 on epoch=749
03/01/2022 20:00:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=754
03/01/2022 20:00:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=759
03/01/2022 20:00:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=764
03/01/2022 20:00:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=769
03/01/2022 20:00:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=774
03/01/2022 20:00:38 - INFO - __main__ - Global step 1550 Train loss 0.16 EM 0.0 on epoch=774
03/01/2022 20:00:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=779
03/01/2022 20:00:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=784
03/01/2022 20:00:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.15 on epoch=789
03/01/2022 20:00:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=794
03/01/2022 20:00:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.22 on epoch=799
03/01/2022 20:00:51 - INFO - __main__ - Global step 1600 Train loss 0.15 EM 0.0 on epoch=799
03/01/2022 20:00:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=804
03/01/2022 20:00:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=809
03/01/2022 20:00:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=814
03/01/2022 20:01:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=819
03/01/2022 20:01:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=824
03/01/2022 20:01:04 - INFO - __main__ - Global step 1650 Train loss 0.16 EM 0.0 on epoch=824
03/01/2022 20:01:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=829
03/01/2022 20:01:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=834
03/01/2022 20:01:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=839
03/01/2022 20:01:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=844
03/01/2022 20:01:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=849
03/01/2022 20:01:17 - INFO - __main__ - Global step 1700 Train loss 0.12 EM 0.0 on epoch=849
03/01/2022 20:01:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=854
03/01/2022 20:01:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.10 on epoch=859
03/01/2022 20:01:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=864
03/01/2022 20:01:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=869
03/01/2022 20:01:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=874
03/01/2022 20:01:29 - INFO - __main__ - Global step 1750 Train loss 0.12 EM 0.0 on epoch=874
03/01/2022 20:01:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=879
03/01/2022 20:01:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=884
03/01/2022 20:01:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=889
03/01/2022 20:01:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=894
03/01/2022 20:01:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=899
03/01/2022 20:01:42 - INFO - __main__ - Global step 1800 Train loss 0.14 EM 0.0 on epoch=899
03/01/2022 20:01:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=904
03/01/2022 20:01:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=909
03/01/2022 20:01:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=914
03/01/2022 20:01:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=919
03/01/2022 20:01:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/01/2022 20:01:55 - INFO - __main__ - Global step 1850 Train loss 0.13 EM 0.0 on epoch=924
03/01/2022 20:01:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=929
03/01/2022 20:01:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=934
03/01/2022 20:02:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=939
03/01/2022 20:02:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=944
03/01/2022 20:02:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=949
03/01/2022 20:02:08 - INFO - __main__ - Global step 1900 Train loss 0.11 EM 0.0 on epoch=949
03/01/2022 20:02:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=954
03/01/2022 20:02:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=959
03/01/2022 20:02:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=964
03/01/2022 20:02:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=969
03/01/2022 20:02:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=974
03/01/2022 20:02:20 - INFO - __main__ - Global step 1950 Train loss 0.10 EM 0.0 on epoch=974
03/01/2022 20:02:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.11 on epoch=979
03/01/2022 20:02:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=984
03/01/2022 20:02:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=989
03/01/2022 20:02:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=994
03/01/2022 20:02:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=999
03/01/2022 20:02:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:02:33 - INFO - __main__ - Printing 3 examples
03/01/2022 20:02:33 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 20:02:33 - INFO - __main__ - ['francois mitterrand']
03/01/2022 20:02:33 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 20:02:33 - INFO - __main__ - ['james callaghan']
03/01/2022 20:02:33 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 20:02:33 - INFO - __main__ - ['aberdeen']
03/01/2022 20:02:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 20:02:33 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:02:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:02:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:02:33 - INFO - __main__ - Printing 3 examples
03/01/2022 20:02:33 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 20:02:33 - INFO - __main__ - ['tulisa']
03/01/2022 20:02:33 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 20:02:33 - INFO - __main__ - ['calgary']
03/01/2022 20:02:33 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 20:02:33 - INFO - __main__ - ['jeff bridges']
03/01/2022 20:02:33 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:02:33 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:02:33 - INFO - __main__ - Global step 2000 Train loss 0.10 EM 0.0 on epoch=999
03/01/2022 20:02:33 - INFO - __main__ - save last model!
03/01/2022 20:02:33 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:02:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 20:02:33 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 20:02:33 - INFO - __main__ - Printing 3 examples
03/01/2022 20:02:33 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 20:02:33 - INFO - __main__ - ['taming of the shrew']
03/01/2022 20:02:33 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 20:02:33 - INFO - __main__ - ['henry fonda']
03/01/2022 20:02:33 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 20:02:33 - INFO - __main__ - ['tchaikovsky']
03/01/2022 20:02:33 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:02:35 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:02:39 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 20:02:45 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 20:02:45 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(1813, 317619), (3845, 206498), (5633, 185013), (3553, 155391), (3808, 217101), (1042, 857631), (1341, 767639), (5810, 169570), (2610, 426074), (3083, 217219), (2494, 207618), (2508, 403980), (3973, 227298), (3711, 277625), (5256, 206528), (4394, 234859), (3268, 226138), (1097, 881855), (3594, 300832), (4680, 218826), (2881, 363181), (3159, 250529), (1646, 587152), (5092, 197494), (751, 1191325), (962, 911154), (973, 965632), (1039, 892616), (4898, 216287), (1049, 871051), (3352, 251355), (1488, 681027), (6088, 168191), (389, 1329969), (1584, 159932), (6963, 166421), (3794, 166375), (2534, 284947), (1127, 724151), (4879, 229125), (2961, 264523), (4845, 221351), (596, 1528627), (3231, 325809), (5798, 156482), (2517, 300897), (3509, 266051), (1273, 833860), (5562, 162019), (3835, 184385), (2334, 430503), (1707, 582614), (3965, 193632), (1886, 409261), (1178, 811299), (3776, 266993), (4904, 168205), (4134, 255300), (1260, 767060), (3074, 334210), (7137, 163092), (2383, 406446), (5146, 217691), (5693, 166587), (4819, 168038), (310, 2990534), (6363, 154270), (3011, 346978), (313, 2745242), (1965, 505323), (2983, 340155), (2826, 304796), (3864, 276837), (3312, 266535), (63, 13320350), (1909, 502837), (1591, 624673), (6603, 165014), (4262, 233607), (761, 1146699), (1003, 631823), (331, 2589436), (1246, 907731), (5763, 176930), (2329, 425780), (4295, 249014), (4336, 223332), (3719, 275802), (1249, 573784), (3693, 253676), (3447, 301910), (3566, 290067), (2788, 164681), (6631, 157532), (909, 970948), (592, 1501058), (1645, 581504), (564, 1570044), (885, 1006302)]
03/01/2022 20:02:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:02:46 - INFO - __main__ - Starting training!
03/01/2022 20:05:16 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_42_0.5_8_predictions.txt
03/01/2022 20:05:16 - INFO - __main__ - EM on test data: 0.0065
03/01/2022 20:05:18 - INFO - __main__ - prefix=freebase_qa_32_42, lr=0.5, bsz=8, dev_performance=0.0, test_performance=0.006509764646970456
03/01/2022 20:05:18 - INFO - __main__ - Running ... prefix=freebase_qa_32_42, lr=0.4, bsz=8 ...
03/01/2022 20:05:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:05:18 - INFO - __main__ - Printing 3 examples
03/01/2022 20:05:18 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 20:05:18 - INFO - __main__ - ['francois mitterrand']
03/01/2022 20:05:18 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 20:05:18 - INFO - __main__ - ['james callaghan']
03/01/2022 20:05:18 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 20:05:18 - INFO - __main__ - ['aberdeen']
03/01/2022 20:05:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 20:05:18 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:05:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:05:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:05:18 - INFO - __main__ - Printing 3 examples
03/01/2022 20:05:18 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 20:05:18 - INFO - __main__ - ['tulisa']
03/01/2022 20:05:18 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 20:05:18 - INFO - __main__ - ['calgary']
03/01/2022 20:05:18 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 20:05:18 - INFO - __main__ - ['jeff bridges']
03/01/2022 20:05:18 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:05:18 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:05:19 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:05:32 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 20:05:32 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(1813, 317619), (3845, 206498), (5633, 185013), (3553, 155391), (3808, 217101), (1042, 857631), (1341, 767639), (5810, 169570), (2610, 426074), (3083, 217219), (2494, 207618), (2508, 403980), (3973, 227298), (3711, 277625), (5256, 206528), (4394, 234859), (3268, 226138), (1097, 881855), (3594, 300832), (4680, 218826), (2881, 363181), (3159, 250529), (1646, 587152), (5092, 197494), (751, 1191325), (962, 911154), (973, 965632), (1039, 892616), (4898, 216287), (1049, 871051), (3352, 251355), (1488, 681027), (6088, 168191), (389, 1329969), (1584, 159932), (6963, 166421), (3794, 166375), (2534, 284947), (1127, 724151), (4879, 229125), (2961, 264523), (4845, 221351), (596, 1528627), (3231, 325809), (5798, 156482), (2517, 300897), (3509, 266051), (1273, 833860), (5562, 162019), (3835, 184385), (2334, 430503), (1707, 582614), (3965, 193632), (1886, 409261), (1178, 811299), (3776, 266993), (4904, 168205), (4134, 255300), (1260, 767060), (3074, 334210), (7137, 163092), (2383, 406446), (5146, 217691), (5693, 166587), (4819, 168038), (310, 2990534), (6363, 154270), (3011, 346978), (313, 2745242), (1965, 505323), (2983, 340155), (2826, 304796), (3864, 276837), (3312, 266535), (63, 13320350), (1909, 502837), (1591, 624673), (6603, 165014), (4262, 233607), (761, 1146699), (1003, 631823), (331, 2589436), (1246, 907731), (5763, 176930), (2329, 425780), (4295, 249014), (4336, 223332), (3719, 275802), (1249, 573784), (3693, 253676), (3447, 301910), (3566, 290067), (2788, 164681), (6631, 157532), (909, 970948), (592, 1501058), (1645, 581504), (564, 1570044), (885, 1006302)]
03/01/2022 20:05:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:05:33 - INFO - __main__ - Starting training!
03/01/2022 20:05:36 - INFO - __main__ - Step 10 Global step 10 Train loss 4.90 on epoch=4
03/01/2022 20:05:38 - INFO - __main__ - Step 20 Global step 20 Train loss 4.20 on epoch=9
03/01/2022 20:05:40 - INFO - __main__ - Step 30 Global step 30 Train loss 3.64 on epoch=14
03/01/2022 20:05:43 - INFO - __main__ - Step 40 Global step 40 Train loss 3.39 on epoch=19
03/01/2022 20:05:45 - INFO - __main__ - Step 50 Global step 50 Train loss 3.15 on epoch=24
03/01/2022 20:05:46 - INFO - __main__ - Global step 50 Train loss 3.86 EM 0.0 on epoch=24
03/01/2022 20:05:46 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 20:05:48 - INFO - __main__ - Step 60 Global step 60 Train loss 2.99 on epoch=29
03/01/2022 20:05:51 - INFO - __main__ - Step 70 Global step 70 Train loss 2.91 on epoch=34
03/01/2022 20:05:53 - INFO - __main__ - Step 80 Global step 80 Train loss 2.87 on epoch=39
03/01/2022 20:05:55 - INFO - __main__ - Step 90 Global step 90 Train loss 2.71 on epoch=44
03/01/2022 20:05:58 - INFO - __main__ - Step 100 Global step 100 Train loss 2.70 on epoch=49
03/01/2022 20:05:59 - INFO - __main__ - Global step 100 Train loss 2.84 EM 0.0 on epoch=49
03/01/2022 20:06:01 - INFO - __main__ - Step 110 Global step 110 Train loss 2.62 on epoch=54
03/01/2022 20:06:03 - INFO - __main__ - Step 120 Global step 120 Train loss 2.59 on epoch=59
03/01/2022 20:06:06 - INFO - __main__ - Step 130 Global step 130 Train loss 2.40 on epoch=64
03/01/2022 20:06:08 - INFO - __main__ - Step 140 Global step 140 Train loss 2.39 on epoch=69
03/01/2022 20:06:10 - INFO - __main__ - Step 150 Global step 150 Train loss 2.38 on epoch=74
03/01/2022 20:06:12 - INFO - __main__ - Global step 150 Train loss 2.47 EM 0.0 on epoch=74
03/01/2022 20:06:14 - INFO - __main__ - Step 160 Global step 160 Train loss 2.31 on epoch=79
03/01/2022 20:06:16 - INFO - __main__ - Step 170 Global step 170 Train loss 2.18 on epoch=84
03/01/2022 20:06:18 - INFO - __main__ - Step 180 Global step 180 Train loss 2.22 on epoch=89
03/01/2022 20:06:21 - INFO - __main__ - Step 190 Global step 190 Train loss 2.07 on epoch=94
03/01/2022 20:06:23 - INFO - __main__ - Step 200 Global step 200 Train loss 2.07 on epoch=99
03/01/2022 20:06:24 - INFO - __main__ - Global step 200 Train loss 2.17 EM 0.0 on epoch=99
03/01/2022 20:06:27 - INFO - __main__ - Step 210 Global step 210 Train loss 2.01 on epoch=104
03/01/2022 20:06:29 - INFO - __main__ - Step 220 Global step 220 Train loss 1.95 on epoch=109
03/01/2022 20:06:31 - INFO - __main__ - Step 230 Global step 230 Train loss 1.86 on epoch=114
03/01/2022 20:06:34 - INFO - __main__ - Step 240 Global step 240 Train loss 1.93 on epoch=119
03/01/2022 20:06:36 - INFO - __main__ - Step 250 Global step 250 Train loss 1.79 on epoch=124
03/01/2022 20:06:37 - INFO - __main__ - Global step 250 Train loss 1.91 EM 0.0 on epoch=124
03/01/2022 20:06:40 - INFO - __main__ - Step 260 Global step 260 Train loss 1.76 on epoch=129
03/01/2022 20:06:42 - INFO - __main__ - Step 270 Global step 270 Train loss 1.76 on epoch=134
03/01/2022 20:06:44 - INFO - __main__ - Step 280 Global step 280 Train loss 1.79 on epoch=139
03/01/2022 20:06:46 - INFO - __main__ - Step 290 Global step 290 Train loss 1.70 on epoch=144
03/01/2022 20:06:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.65 on epoch=149
03/01/2022 20:06:50 - INFO - __main__ - Global step 300 Train loss 1.73 EM 0.0 on epoch=149
03/01/2022 20:06:52 - INFO - __main__ - Step 310 Global step 310 Train loss 1.71 on epoch=154
03/01/2022 20:06:55 - INFO - __main__ - Step 320 Global step 320 Train loss 1.56 on epoch=159
03/01/2022 20:06:57 - INFO - __main__ - Step 330 Global step 330 Train loss 1.57 on epoch=164
03/01/2022 20:06:59 - INFO - __main__ - Step 340 Global step 340 Train loss 1.55 on epoch=169
03/01/2022 20:07:01 - INFO - __main__ - Step 350 Global step 350 Train loss 1.50 on epoch=174
03/01/2022 20:07:03 - INFO - __main__ - Global step 350 Train loss 1.58 EM 0.0 on epoch=174
03/01/2022 20:07:05 - INFO - __main__ - Step 360 Global step 360 Train loss 1.45 on epoch=179
03/01/2022 20:07:07 - INFO - __main__ - Step 370 Global step 370 Train loss 1.37 on epoch=184
03/01/2022 20:07:10 - INFO - __main__ - Step 380 Global step 380 Train loss 1.41 on epoch=189
03/01/2022 20:07:12 - INFO - __main__ - Step 390 Global step 390 Train loss 1.42 on epoch=194
03/01/2022 20:07:14 - INFO - __main__ - Step 400 Global step 400 Train loss 1.34 on epoch=199
03/01/2022 20:07:15 - INFO - __main__ - Global step 400 Train loss 1.40 EM 0.0 on epoch=199
03/01/2022 20:07:17 - INFO - __main__ - Step 410 Global step 410 Train loss 1.39 on epoch=204
03/01/2022 20:07:20 - INFO - __main__ - Step 420 Global step 420 Train loss 1.37 on epoch=209
03/01/2022 20:07:22 - INFO - __main__ - Step 430 Global step 430 Train loss 1.23 on epoch=214
03/01/2022 20:07:24 - INFO - __main__ - Step 440 Global step 440 Train loss 1.26 on epoch=219
03/01/2022 20:07:27 - INFO - __main__ - Step 450 Global step 450 Train loss 1.23 on epoch=224
03/01/2022 20:07:28 - INFO - __main__ - Global step 450 Train loss 1.30 EM 0.0 on epoch=224
03/01/2022 20:07:30 - INFO - __main__ - Step 460 Global step 460 Train loss 1.26 on epoch=229
03/01/2022 20:07:32 - INFO - __main__ - Step 470 Global step 470 Train loss 1.23 on epoch=234
03/01/2022 20:07:35 - INFO - __main__ - Step 480 Global step 480 Train loss 1.15 on epoch=239
03/01/2022 20:07:37 - INFO - __main__ - Step 490 Global step 490 Train loss 1.16 on epoch=244
03/01/2022 20:07:39 - INFO - __main__ - Step 500 Global step 500 Train loss 1.22 on epoch=249
03/01/2022 20:07:40 - INFO - __main__ - Global step 500 Train loss 1.21 EM 0.0 on epoch=249
03/01/2022 20:07:43 - INFO - __main__ - Step 510 Global step 510 Train loss 1.08 on epoch=254
03/01/2022 20:07:45 - INFO - __main__ - Step 520 Global step 520 Train loss 1.19 on epoch=259
03/01/2022 20:07:47 - INFO - __main__ - Step 530 Global step 530 Train loss 1.17 on epoch=264
03/01/2022 20:07:49 - INFO - __main__ - Step 540 Global step 540 Train loss 1.04 on epoch=269
03/01/2022 20:07:52 - INFO - __main__ - Step 550 Global step 550 Train loss 1.09 on epoch=274
03/01/2022 20:07:53 - INFO - __main__ - Global step 550 Train loss 1.11 EM 0.0 on epoch=274
03/01/2022 20:07:55 - INFO - __main__ - Step 560 Global step 560 Train loss 1.10 on epoch=279
03/01/2022 20:07:58 - INFO - __main__ - Step 570 Global step 570 Train loss 1.03 on epoch=284
03/01/2022 20:08:00 - INFO - __main__ - Step 580 Global step 580 Train loss 1.02 on epoch=289
03/01/2022 20:08:02 - INFO - __main__ - Step 590 Global step 590 Train loss 1.01 on epoch=294
03/01/2022 20:08:04 - INFO - __main__ - Step 600 Global step 600 Train loss 1.04 on epoch=299
03/01/2022 20:08:06 - INFO - __main__ - Global step 600 Train loss 1.04 EM 0.0 on epoch=299
03/01/2022 20:08:08 - INFO - __main__ - Step 610 Global step 610 Train loss 1.03 on epoch=304
03/01/2022 20:08:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.97 on epoch=309
03/01/2022 20:08:12 - INFO - __main__ - Step 630 Global step 630 Train loss 1.00 on epoch=314
03/01/2022 20:08:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.87 on epoch=319
03/01/2022 20:08:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.95 on epoch=324
03/01/2022 20:08:18 - INFO - __main__ - Global step 650 Train loss 0.96 EM 0.0 on epoch=324
03/01/2022 20:08:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.94 on epoch=329
03/01/2022 20:08:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.87 on epoch=334
03/01/2022 20:08:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.80 on epoch=339
03/01/2022 20:08:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.83 on epoch=344
03/01/2022 20:08:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.80 on epoch=349
03/01/2022 20:08:31 - INFO - __main__ - Global step 700 Train loss 0.85 EM 0.0 on epoch=349
03/01/2022 20:08:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.80 on epoch=354
03/01/2022 20:08:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.75 on epoch=359
03/01/2022 20:08:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.74 on epoch=364
03/01/2022 20:08:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.80 on epoch=369
03/01/2022 20:08:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.74 on epoch=374
03/01/2022 20:08:43 - INFO - __main__ - Global step 750 Train loss 0.77 EM 0.0 on epoch=374
03/01/2022 20:08:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.75 on epoch=379
03/01/2022 20:08:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.75 on epoch=384
03/01/2022 20:08:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.66 on epoch=389
03/01/2022 20:08:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.65 on epoch=394
03/01/2022 20:08:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.68 on epoch=399
03/01/2022 20:08:56 - INFO - __main__ - Global step 800 Train loss 0.70 EM 0.0 on epoch=399
03/01/2022 20:08:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.74 on epoch=404
03/01/2022 20:09:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.65 on epoch=409
03/01/2022 20:09:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.69 on epoch=414
03/01/2022 20:09:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.65 on epoch=419
03/01/2022 20:09:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.73 on epoch=424
03/01/2022 20:09:09 - INFO - __main__ - Global step 850 Train loss 0.69 EM 0.0 on epoch=424
03/01/2022 20:09:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.58 on epoch=429
03/01/2022 20:09:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.58 on epoch=434
03/01/2022 20:09:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.61 on epoch=439
03/01/2022 20:09:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.62 on epoch=444
03/01/2022 20:09:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=449
03/01/2022 20:09:22 - INFO - __main__ - Global step 900 Train loss 0.58 EM 0.0 on epoch=449
03/01/2022 20:09:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.55 on epoch=454
03/01/2022 20:09:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.61 on epoch=459
03/01/2022 20:09:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.64 on epoch=464
03/01/2022 20:09:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.69 on epoch=469
03/01/2022 20:09:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.51 on epoch=474
03/01/2022 20:09:34 - INFO - __main__ - Global step 950 Train loss 0.60 EM 0.0 on epoch=474
03/01/2022 20:09:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.56 on epoch=479
03/01/2022 20:09:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.62 on epoch=484
03/01/2022 20:09:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.53 on epoch=489
03/01/2022 20:09:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=494
03/01/2022 20:09:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.55 on epoch=499
03/01/2022 20:09:47 - INFO - __main__ - Global step 1000 Train loss 0.55 EM 0.0 on epoch=499
03/01/2022 20:09:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.50 on epoch=504
03/01/2022 20:09:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.53 on epoch=509
03/01/2022 20:09:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.54 on epoch=514
03/01/2022 20:09:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.50 on epoch=519
03/01/2022 20:09:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.51 on epoch=524
03/01/2022 20:10:00 - INFO - __main__ - Global step 1050 Train loss 0.52 EM 0.0 on epoch=524
03/01/2022 20:10:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.54 on epoch=529
03/01/2022 20:10:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=534
03/01/2022 20:10:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.48 on epoch=539
03/01/2022 20:10:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
03/01/2022 20:10:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=549
03/01/2022 20:10:12 - INFO - __main__ - Global step 1100 Train loss 0.48 EM 0.0 on epoch=549
03/01/2022 20:10:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=554
03/01/2022 20:10:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=559
03/01/2022 20:10:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=564
03/01/2022 20:10:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=569
03/01/2022 20:10:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=574
03/01/2022 20:10:25 - INFO - __main__ - Global step 1150 Train loss 0.41 EM 0.0 on epoch=574
03/01/2022 20:10:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=579
03/01/2022 20:10:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.52 on epoch=584
03/01/2022 20:10:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=589
03/01/2022 20:10:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=594
03/01/2022 20:10:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=599
03/01/2022 20:10:38 - INFO - __main__ - Global step 1200 Train loss 0.42 EM 0.0 on epoch=599
03/01/2022 20:10:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=604
03/01/2022 20:10:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=609
03/01/2022 20:10:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=614
03/01/2022 20:10:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=619
03/01/2022 20:10:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=624
03/01/2022 20:10:50 - INFO - __main__ - Global step 1250 Train loss 0.38 EM 0.0 on epoch=624
03/01/2022 20:10:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=629
03/01/2022 20:10:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=634
03/01/2022 20:10:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=639
03/01/2022 20:11:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=644
03/01/2022 20:11:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=649
03/01/2022 20:11:03 - INFO - __main__ - Global step 1300 Train loss 0.32 EM 0.0 on epoch=649
03/01/2022 20:11:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=654
03/01/2022 20:11:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=659
03/01/2022 20:11:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.29 on epoch=664
03/01/2022 20:11:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=669
03/01/2022 20:11:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=674
03/01/2022 20:11:16 - INFO - __main__ - Global step 1350 Train loss 0.30 EM 0.0 on epoch=674
03/01/2022 20:11:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=679
03/01/2022 20:11:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.28 on epoch=684
03/01/2022 20:11:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=689
03/01/2022 20:11:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=694
03/01/2022 20:11:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=699
03/01/2022 20:11:28 - INFO - __main__ - Global step 1400 Train loss 0.31 EM 0.0 on epoch=699
03/01/2022 20:11:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.33 on epoch=704
03/01/2022 20:11:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=709
03/01/2022 20:11:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=714
03/01/2022 20:11:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=719
03/01/2022 20:11:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=724
03/01/2022 20:11:41 - INFO - __main__ - Global step 1450 Train loss 0.31 EM 0.0 on epoch=724
03/01/2022 20:11:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.29 on epoch=729
03/01/2022 20:11:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=734
03/01/2022 20:11:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=739
03/01/2022 20:11:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=744
03/01/2022 20:11:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.27 on epoch=749
03/01/2022 20:11:54 - INFO - __main__ - Global step 1500 Train loss 0.29 EM 0.0 on epoch=749
03/01/2022 20:11:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=754
03/01/2022 20:11:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=759
03/01/2022 20:12:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=764
03/01/2022 20:12:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.21 on epoch=769
03/01/2022 20:12:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=774
03/01/2022 20:12:07 - INFO - __main__ - Global step 1550 Train loss 0.24 EM 0.0 on epoch=774
03/01/2022 20:12:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=779
03/01/2022 20:12:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=784
03/01/2022 20:12:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=789
03/01/2022 20:12:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=794
03/01/2022 20:12:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=799
03/01/2022 20:12:19 - INFO - __main__ - Global step 1600 Train loss 0.24 EM 0.0 on epoch=799
03/01/2022 20:12:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=804
03/01/2022 20:12:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=809
03/01/2022 20:12:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=814
03/01/2022 20:12:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=819
03/01/2022 20:12:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=824
03/01/2022 20:12:32 - INFO - __main__ - Global step 1650 Train loss 0.24 EM 0.0 on epoch=824
03/01/2022 20:12:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=829
03/01/2022 20:12:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=834
03/01/2022 20:12:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=839
03/01/2022 20:12:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=844
03/01/2022 20:12:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=849
03/01/2022 20:12:45 - INFO - __main__ - Global step 1700 Train loss 0.23 EM 0.0 on epoch=849
03/01/2022 20:12:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=854
03/01/2022 20:12:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=859
03/01/2022 20:12:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=864
03/01/2022 20:12:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=869
03/01/2022 20:12:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=874
03/01/2022 20:12:57 - INFO - __main__ - Global step 1750 Train loss 0.20 EM 0.0 on epoch=874
03/01/2022 20:13:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.15 on epoch=879
03/01/2022 20:13:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=884
03/01/2022 20:13:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=889
03/01/2022 20:13:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=894
03/01/2022 20:13:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=899
03/01/2022 20:13:10 - INFO - __main__ - Global step 1800 Train loss 0.18 EM 0.0 on epoch=899
03/01/2022 20:13:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.18 on epoch=904
03/01/2022 20:13:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.21 on epoch=909
03/01/2022 20:13:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=914
03/01/2022 20:13:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=919
03/01/2022 20:13:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=924
03/01/2022 20:13:23 - INFO - __main__ - Global step 1850 Train loss 0.18 EM 0.0 on epoch=924
03/01/2022 20:13:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=929
03/01/2022 20:13:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=934
03/01/2022 20:13:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=939
03/01/2022 20:13:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.15 on epoch=944
03/01/2022 20:13:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=949
03/01/2022 20:13:36 - INFO - __main__ - Global step 1900 Train loss 0.17 EM 0.0 on epoch=949
03/01/2022 20:13:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=954
03/01/2022 20:13:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=959
03/01/2022 20:13:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=964
03/01/2022 20:13:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.15 on epoch=969
03/01/2022 20:13:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=974
03/01/2022 20:13:48 - INFO - __main__ - Global step 1950 Train loss 0.16 EM 0.0 on epoch=974
03/01/2022 20:13:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=979
03/01/2022 20:13:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=984
03/01/2022 20:13:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=989
03/01/2022 20:13:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=994
03/01/2022 20:14:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=999
03/01/2022 20:14:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:14:01 - INFO - __main__ - Printing 3 examples
03/01/2022 20:14:01 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 20:14:01 - INFO - __main__ - ['francois mitterrand']
03/01/2022 20:14:01 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 20:14:01 - INFO - __main__ - ['james callaghan']
03/01/2022 20:14:01 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 20:14:01 - INFO - __main__ - ['aberdeen']
03/01/2022 20:14:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 20:14:01 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:14:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:14:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:14:01 - INFO - __main__ - Printing 3 examples
03/01/2022 20:14:01 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 20:14:01 - INFO - __main__ - ['tulisa']
03/01/2022 20:14:01 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 20:14:01 - INFO - __main__ - ['calgary']
03/01/2022 20:14:01 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 20:14:01 - INFO - __main__ - ['jeff bridges']
03/01/2022 20:14:01 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:14:01 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:14:01 - INFO - __main__ - Global step 2000 Train loss 0.16 EM 0.0 on epoch=999
03/01/2022 20:14:01 - INFO - __main__ - save last model!
03/01/2022 20:14:01 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:14:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 20:14:01 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 20:14:01 - INFO - __main__ - Printing 3 examples
03/01/2022 20:14:01 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 20:14:01 - INFO - __main__ - ['taming of the shrew']
03/01/2022 20:14:01 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 20:14:01 - INFO - __main__ - ['henry fonda']
03/01/2022 20:14:01 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 20:14:01 - INFO - __main__ - ['tchaikovsky']
03/01/2022 20:14:01 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:14:03 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:14:06 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 20:14:15 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 20:14:15 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(2997, 341195), (3891, 211094), (4703, 220765), (3385, 191872), (6997, 154928), (5105, 211448), (5050, 227151), (1962, 340756), (2317, 424876), (5741, 183016), (1397, 772752), (2693, 271357), (3916, 217282), (749, 787508), (3977, 329839), (6888, 158291), (5324, 191543), (5650, 178209), (3786, 305013), (503, 1684486), (2428, 415536), (3205, 237602), (2434, 156656), (83, 1452381), (84, 10288303), (2125, 446994), (3298, 311752), (1445, 432249), (1501, 650320), (2478, 361778), (3376, 323744), (3438, 286245), (1269, 737181), (94, 9989518), (5727, 188354), (5008, 216986), (1879, 517099), (1384, 683777), (4335, 246709), (5620, 175844), (2134, 401363), (5446, 187212), (5199, 167310), (3883, 264299), (844, 1112441), (2255, 236375), (482, 1812745), (4903, 214208), (215, 4221120), (3095, 203292), (584, 1039586), (3073, 300611), (7555, 162623), (4112, 217671), (6337, 163252), (4048, 231896), (3063, 224634), (1108, 766167), (4724, 231722), (234, 3920679), (3046, 289396), (1906, 526589), (4947, 202998), (2744, 259869), (1155, 940173), (5069, 213802), (1162, 824379), (900, 380561), (4417, 214340), (5523, 159171), (4454, 233931), (5899, 179457), (739, 1029836), (5120, 236973), (2518, 303433), (4002, 277079), (2361, 413361), (1859, 282618), (1123, 537015), (816, 1059326), (6116, 183563), (1068, 771470), (3465, 176815), (4407, 183682), (2013, 472815), (4217, 267655), (3102, 266483), (1623, 603619), (2654, 392174), (953, 840707), (935, 612200), (2790, 361504), (1647, 462933), (1873, 523188), (6526, 155946), (1653, 586845), (757, 1071575), (1952, 512051), (3637, 189415)]
03/01/2022 20:14:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:14:16 - INFO - __main__ - Starting training!
03/01/2022 20:16:42 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_42_0.4_8_predictions.txt
03/01/2022 20:16:42 - INFO - __main__ - EM on test data: 0.0043
03/01/2022 20:16:42 - INFO - __main__ - prefix=freebase_qa_32_42, lr=0.4, bsz=8, dev_performance=0.0, test_performance=0.004256384576865298
03/01/2022 20:16:42 - INFO - __main__ - Running ... prefix=freebase_qa_32_42, lr=0.3, bsz=8 ...
03/01/2022 20:16:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:16:43 - INFO - __main__ - Printing 3 examples
03/01/2022 20:16:43 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 20:16:43 - INFO - __main__ - ['francois mitterrand']
03/01/2022 20:16:43 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 20:16:43 - INFO - __main__ - ['james callaghan']
03/01/2022 20:16:43 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 20:16:43 - INFO - __main__ - ['aberdeen']
03/01/2022 20:16:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 20:16:43 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:16:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:16:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:16:43 - INFO - __main__ - Printing 3 examples
03/01/2022 20:16:43 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 20:16:43 - INFO - __main__ - ['tulisa']
03/01/2022 20:16:43 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 20:16:43 - INFO - __main__ - ['calgary']
03/01/2022 20:16:43 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 20:16:43 - INFO - __main__ - ['jeff bridges']
03/01/2022 20:16:43 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:16:43 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:16:43 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:16:55 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 20:16:55 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(2997, 341195), (3891, 211094), (4703, 220765), (3385, 191872), (6997, 154928), (5105, 211448), (5050, 227151), (1962, 340756), (2317, 424876), (5741, 183016), (1397, 772752), (2693, 271357), (3916, 217282), (749, 787508), (3977, 329839), (6888, 158291), (5324, 191543), (5650, 178209), (3786, 305013), (503, 1684486), (2428, 415536), (3205, 237602), (2434, 156656), (83, 1452381), (84, 10288303), (2125, 446994), (3298, 311752), (1445, 432249), (1501, 650320), (2478, 361778), (3376, 323744), (3438, 286245), (1269, 737181), (94, 9989518), (5727, 188354), (5008, 216986), (1879, 517099), (1384, 683777), (4335, 246709), (5620, 175844), (2134, 401363), (5446, 187212), (5199, 167310), (3883, 264299), (844, 1112441), (2255, 236375), (482, 1812745), (4903, 214208), (215, 4221120), (3095, 203292), (584, 1039586), (3073, 300611), (7555, 162623), (4112, 217671), (6337, 163252), (4048, 231896), (3063, 224634), (1108, 766167), (4724, 231722), (234, 3920679), (3046, 289396), (1906, 526589), (4947, 202998), (2744, 259869), (1155, 940173), (5069, 213802), (1162, 824379), (900, 380561), (4417, 214340), (5523, 159171), (4454, 233931), (5899, 179457), (739, 1029836), (5120, 236973), (2518, 303433), (4002, 277079), (2361, 413361), (1859, 282618), (1123, 537015), (816, 1059326), (6116, 183563), (1068, 771470), (3465, 176815), (4407, 183682), (2013, 472815), (4217, 267655), (3102, 266483), (1623, 603619), (2654, 392174), (953, 840707), (935, 612200), (2790, 361504), (1647, 462933), (1873, 523188), (6526, 155946), (1653, 586845), (757, 1071575), (1952, 512051), (3637, 189415)]
03/01/2022 20:16:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:16:56 - INFO - __main__ - Starting training!
03/01/2022 20:16:58 - INFO - __main__ - Step 10 Global step 10 Train loss 5.09 on epoch=4
03/01/2022 20:17:01 - INFO - __main__ - Step 20 Global step 20 Train loss 4.57 on epoch=9
03/01/2022 20:17:03 - INFO - __main__ - Step 30 Global step 30 Train loss 4.05 on epoch=14
03/01/2022 20:17:05 - INFO - __main__ - Step 40 Global step 40 Train loss 3.61 on epoch=19
03/01/2022 20:17:07 - INFO - __main__ - Step 50 Global step 50 Train loss 3.38 on epoch=24
03/01/2022 20:17:09 - INFO - __main__ - Global step 50 Train loss 4.14 EM 0.0 on epoch=24
03/01/2022 20:17:09 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 20:17:11 - INFO - __main__ - Step 60 Global step 60 Train loss 3.22 on epoch=29
03/01/2022 20:17:13 - INFO - __main__ - Step 70 Global step 70 Train loss 3.10 on epoch=34
03/01/2022 20:17:16 - INFO - __main__ - Step 80 Global step 80 Train loss 3.03 on epoch=39
03/01/2022 20:17:18 - INFO - __main__ - Step 90 Global step 90 Train loss 2.96 on epoch=44
03/01/2022 20:17:20 - INFO - __main__ - Step 100 Global step 100 Train loss 2.89 on epoch=49
03/01/2022 20:17:21 - INFO - __main__ - Global step 100 Train loss 3.04 EM 0.0 on epoch=49
03/01/2022 20:17:24 - INFO - __main__ - Step 110 Global step 110 Train loss 2.81 on epoch=54
03/01/2022 20:17:26 - INFO - __main__ - Step 120 Global step 120 Train loss 2.73 on epoch=59
03/01/2022 20:17:28 - INFO - __main__ - Step 130 Global step 130 Train loss 2.68 on epoch=64
03/01/2022 20:17:31 - INFO - __main__ - Step 140 Global step 140 Train loss 2.61 on epoch=69
03/01/2022 20:17:33 - INFO - __main__ - Step 150 Global step 150 Train loss 2.66 on epoch=74
03/01/2022 20:17:34 - INFO - __main__ - Global step 150 Train loss 2.70 EM 0.0 on epoch=74
03/01/2022 20:17:36 - INFO - __main__ - Step 160 Global step 160 Train loss 2.59 on epoch=79
03/01/2022 20:17:39 - INFO - __main__ - Step 170 Global step 170 Train loss 2.48 on epoch=84
03/01/2022 20:17:41 - INFO - __main__ - Step 180 Global step 180 Train loss 2.45 on epoch=89
03/01/2022 20:17:43 - INFO - __main__ - Step 190 Global step 190 Train loss 2.43 on epoch=94
03/01/2022 20:17:46 - INFO - __main__ - Step 200 Global step 200 Train loss 2.30 on epoch=99
03/01/2022 20:17:47 - INFO - __main__ - Global step 200 Train loss 2.45 EM 0.0 on epoch=99
03/01/2022 20:17:49 - INFO - __main__ - Step 210 Global step 210 Train loss 2.26 on epoch=104
03/01/2022 20:17:51 - INFO - __main__ - Step 220 Global step 220 Train loss 2.21 on epoch=109
03/01/2022 20:17:54 - INFO - __main__ - Step 230 Global step 230 Train loss 2.18 on epoch=114
03/01/2022 20:17:56 - INFO - __main__ - Step 240 Global step 240 Train loss 2.05 on epoch=119
03/01/2022 20:17:58 - INFO - __main__ - Step 250 Global step 250 Train loss 2.03 on epoch=124
03/01/2022 20:17:59 - INFO - __main__ - Global step 250 Train loss 2.15 EM 0.0 on epoch=124
03/01/2022 20:18:02 - INFO - __main__ - Step 260 Global step 260 Train loss 2.02 on epoch=129
03/01/2022 20:18:04 - INFO - __main__ - Step 270 Global step 270 Train loss 1.98 on epoch=134
03/01/2022 20:18:06 - INFO - __main__ - Step 280 Global step 280 Train loss 1.85 on epoch=139
03/01/2022 20:18:08 - INFO - __main__ - Step 290 Global step 290 Train loss 1.98 on epoch=144
03/01/2022 20:18:11 - INFO - __main__ - Step 300 Global step 300 Train loss 1.84 on epoch=149
03/01/2022 20:18:12 - INFO - __main__ - Global step 300 Train loss 1.93 EM 0.0 on epoch=149
03/01/2022 20:18:14 - INFO - __main__ - Step 310 Global step 310 Train loss 1.80 on epoch=154
03/01/2022 20:18:16 - INFO - __main__ - Step 320 Global step 320 Train loss 1.80 on epoch=159
03/01/2022 20:18:19 - INFO - __main__ - Step 330 Global step 330 Train loss 1.70 on epoch=164
03/01/2022 20:18:21 - INFO - __main__ - Step 340 Global step 340 Train loss 1.73 on epoch=169
03/01/2022 20:18:23 - INFO - __main__ - Step 350 Global step 350 Train loss 1.69 on epoch=174
03/01/2022 20:18:24 - INFO - __main__ - Global step 350 Train loss 1.74 EM 0.0 on epoch=174
03/01/2022 20:18:27 - INFO - __main__ - Step 360 Global step 360 Train loss 1.62 on epoch=179
03/01/2022 20:18:29 - INFO - __main__ - Step 370 Global step 370 Train loss 1.64 on epoch=184
03/01/2022 20:18:31 - INFO - __main__ - Step 380 Global step 380 Train loss 1.57 on epoch=189
03/01/2022 20:18:34 - INFO - __main__ - Step 390 Global step 390 Train loss 1.53 on epoch=194
03/01/2022 20:18:36 - INFO - __main__ - Step 400 Global step 400 Train loss 1.48 on epoch=199
03/01/2022 20:18:37 - INFO - __main__ - Global step 400 Train loss 1.57 EM 0.0 on epoch=199
03/01/2022 20:18:39 - INFO - __main__ - Step 410 Global step 410 Train loss 1.52 on epoch=204
03/01/2022 20:18:42 - INFO - __main__ - Step 420 Global step 420 Train loss 1.43 on epoch=209
03/01/2022 20:18:44 - INFO - __main__ - Step 430 Global step 430 Train loss 1.51 on epoch=214
03/01/2022 20:18:46 - INFO - __main__ - Step 440 Global step 440 Train loss 1.48 on epoch=219
03/01/2022 20:18:49 - INFO - __main__ - Step 450 Global step 450 Train loss 1.47 on epoch=224
03/01/2022 20:18:50 - INFO - __main__ - Global step 450 Train loss 1.48 EM 0.0 on epoch=224
03/01/2022 20:18:52 - INFO - __main__ - Step 460 Global step 460 Train loss 1.38 on epoch=229
03/01/2022 20:18:54 - INFO - __main__ - Step 470 Global step 470 Train loss 1.41 on epoch=234
03/01/2022 20:18:56 - INFO - __main__ - Step 480 Global step 480 Train loss 1.39 on epoch=239
03/01/2022 20:18:59 - INFO - __main__ - Step 490 Global step 490 Train loss 1.38 on epoch=244
03/01/2022 20:19:01 - INFO - __main__ - Step 500 Global step 500 Train loss 1.33 on epoch=249
03/01/2022 20:19:02 - INFO - __main__ - Global step 500 Train loss 1.38 EM 0.0 on epoch=249
03/01/2022 20:19:05 - INFO - __main__ - Step 510 Global step 510 Train loss 1.35 on epoch=254
03/01/2022 20:19:07 - INFO - __main__ - Step 520 Global step 520 Train loss 1.28 on epoch=259
03/01/2022 20:19:09 - INFO - __main__ - Step 530 Global step 530 Train loss 1.27 on epoch=264
03/01/2022 20:19:11 - INFO - __main__ - Step 540 Global step 540 Train loss 1.20 on epoch=269
03/01/2022 20:19:14 - INFO - __main__ - Step 550 Global step 550 Train loss 1.27 on epoch=274
03/01/2022 20:19:15 - INFO - __main__ - Global step 550 Train loss 1.27 EM 0.0 on epoch=274
03/01/2022 20:19:17 - INFO - __main__ - Step 560 Global step 560 Train loss 1.20 on epoch=279
03/01/2022 20:19:19 - INFO - __main__ - Step 570 Global step 570 Train loss 1.23 on epoch=284
03/01/2022 20:19:22 - INFO - __main__ - Step 580 Global step 580 Train loss 1.18 on epoch=289
03/01/2022 20:19:24 - INFO - __main__ - Step 590 Global step 590 Train loss 1.21 on epoch=294
03/01/2022 20:19:26 - INFO - __main__ - Step 600 Global step 600 Train loss 1.15 on epoch=299
03/01/2022 20:19:27 - INFO - __main__ - Global step 600 Train loss 1.19 EM 0.0 on epoch=299
03/01/2022 20:19:30 - INFO - __main__ - Step 610 Global step 610 Train loss 1.09 on epoch=304
03/01/2022 20:19:32 - INFO - __main__ - Step 620 Global step 620 Train loss 1.08 on epoch=309
03/01/2022 20:19:34 - INFO - __main__ - Step 630 Global step 630 Train loss 1.08 on epoch=314
03/01/2022 20:19:36 - INFO - __main__ - Step 640 Global step 640 Train loss 1.13 on epoch=319
03/01/2022 20:19:39 - INFO - __main__ - Step 650 Global step 650 Train loss 1.10 on epoch=324
03/01/2022 20:19:40 - INFO - __main__ - Global step 650 Train loss 1.10 EM 0.0 on epoch=324
03/01/2022 20:19:42 - INFO - __main__ - Step 660 Global step 660 Train loss 1.06 on epoch=329
03/01/2022 20:19:45 - INFO - __main__ - Step 670 Global step 670 Train loss 1.11 on epoch=334
03/01/2022 20:19:47 - INFO - __main__ - Step 680 Global step 680 Train loss 1.08 on epoch=339
03/01/2022 20:19:49 - INFO - __main__ - Step 690 Global step 690 Train loss 1.10 on epoch=344
03/01/2022 20:19:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.91 on epoch=349
03/01/2022 20:19:53 - INFO - __main__ - Global step 700 Train loss 1.05 EM 0.0 on epoch=349
03/01/2022 20:19:55 - INFO - __main__ - Step 710 Global step 710 Train loss 1.02 on epoch=354
03/01/2022 20:19:57 - INFO - __main__ - Step 720 Global step 720 Train loss 1.04 on epoch=359
03/01/2022 20:19:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.98 on epoch=364
03/01/2022 20:20:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.99 on epoch=369
03/01/2022 20:20:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.92 on epoch=374
03/01/2022 20:20:05 - INFO - __main__ - Global step 750 Train loss 0.99 EM 0.0 on epoch=374
03/01/2022 20:20:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.96 on epoch=379
03/01/2022 20:20:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.99 on epoch=384
03/01/2022 20:20:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.96 on epoch=389
03/01/2022 20:20:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.90 on epoch=394
03/01/2022 20:20:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.97 on epoch=399
03/01/2022 20:20:18 - INFO - __main__ - Global step 800 Train loss 0.96 EM 0.0 on epoch=399
03/01/2022 20:20:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.90 on epoch=404
03/01/2022 20:20:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.91 on epoch=409
03/01/2022 20:20:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.89 on epoch=414
03/01/2022 20:20:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.89 on epoch=419
03/01/2022 20:20:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=424
03/01/2022 20:20:31 - INFO - __main__ - Global step 850 Train loss 0.88 EM 0.0 on epoch=424
03/01/2022 20:20:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.91 on epoch=429
03/01/2022 20:20:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.96 on epoch=434
03/01/2022 20:20:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.90 on epoch=439
03/01/2022 20:20:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.87 on epoch=444
03/01/2022 20:20:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.80 on epoch=449
03/01/2022 20:20:43 - INFO - __main__ - Global step 900 Train loss 0.89 EM 0.0 on epoch=449
03/01/2022 20:20:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.78 on epoch=454
03/01/2022 20:20:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.81 on epoch=459
03/01/2022 20:20:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.84 on epoch=464
03/01/2022 20:20:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.80 on epoch=469
03/01/2022 20:20:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.81 on epoch=474
03/01/2022 20:20:56 - INFO - __main__ - Global step 950 Train loss 0.81 EM 0.0 on epoch=474
03/01/2022 20:20:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.83 on epoch=479
03/01/2022 20:21:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.73 on epoch=484
03/01/2022 20:21:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.81 on epoch=489
03/01/2022 20:21:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.77 on epoch=494
03/01/2022 20:21:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.77 on epoch=499
03/01/2022 20:21:09 - INFO - __main__ - Global step 1000 Train loss 0.78 EM 0.0 on epoch=499
03/01/2022 20:21:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.76 on epoch=504
03/01/2022 20:21:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.74 on epoch=509
03/01/2022 20:21:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.69 on epoch=514
03/01/2022 20:21:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.62 on epoch=519
03/01/2022 20:21:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.67 on epoch=524
03/01/2022 20:21:21 - INFO - __main__ - Global step 1050 Train loss 0.70 EM 0.0 on epoch=524
03/01/2022 20:21:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.70 on epoch=529
03/01/2022 20:21:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.68 on epoch=534
03/01/2022 20:21:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.68 on epoch=539
03/01/2022 20:21:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.66 on epoch=544
03/01/2022 20:21:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.68 on epoch=549
03/01/2022 20:21:34 - INFO - __main__ - Global step 1100 Train loss 0.68 EM 0.0 on epoch=549
03/01/2022 20:21:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.64 on epoch=554
03/01/2022 20:21:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.57 on epoch=559
03/01/2022 20:21:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.54 on epoch=564
03/01/2022 20:21:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.58 on epoch=569
03/01/2022 20:21:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.59 on epoch=574
03/01/2022 20:21:47 - INFO - __main__ - Global step 1150 Train loss 0.58 EM 0.0 on epoch=574
03/01/2022 20:21:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.55 on epoch=579
03/01/2022 20:21:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.59 on epoch=584
03/01/2022 20:21:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=589
03/01/2022 20:21:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.55 on epoch=594
03/01/2022 20:21:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.60 on epoch=599
03/01/2022 20:21:59 - INFO - __main__ - Global step 1200 Train loss 0.56 EM 0.0 on epoch=599
03/01/2022 20:22:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=604
03/01/2022 20:22:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=609
03/01/2022 20:22:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.54 on epoch=614
03/01/2022 20:22:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.54 on epoch=619
03/01/2022 20:22:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.51 on epoch=624
03/01/2022 20:22:12 - INFO - __main__ - Global step 1250 Train loss 0.52 EM 0.0 on epoch=624
03/01/2022 20:22:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=629
03/01/2022 20:22:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=634
03/01/2022 20:22:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=639
03/01/2022 20:22:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.46 on epoch=644
03/01/2022 20:22:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=649
03/01/2022 20:22:25 - INFO - __main__ - Global step 1300 Train loss 0.46 EM 0.0 on epoch=649
03/01/2022 20:22:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=654
03/01/2022 20:22:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=659
03/01/2022 20:22:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=664
03/01/2022 20:22:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=669
03/01/2022 20:22:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=674
03/01/2022 20:22:37 - INFO - __main__ - Global step 1350 Train loss 0.43 EM 0.0 on epoch=674
03/01/2022 20:22:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=679
03/01/2022 20:22:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=684
03/01/2022 20:22:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=689
03/01/2022 20:22:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=694
03/01/2022 20:22:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=699
03/01/2022 20:22:50 - INFO - __main__ - Global step 1400 Train loss 0.40 EM 0.0 on epoch=699
03/01/2022 20:22:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=704
03/01/2022 20:22:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
03/01/2022 20:22:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=714
03/01/2022 20:22:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=719
03/01/2022 20:23:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.45 on epoch=724
03/01/2022 20:23:03 - INFO - __main__ - Global step 1450 Train loss 0.40 EM 0.0 on epoch=724
03/01/2022 20:23:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=729
03/01/2022 20:23:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=734
03/01/2022 20:23:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=739
03/01/2022 20:23:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=744
03/01/2022 20:23:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.32 on epoch=749
03/01/2022 20:23:16 - INFO - __main__ - Global step 1500 Train loss 0.35 EM 0.0 on epoch=749
03/01/2022 20:23:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=754
03/01/2022 20:23:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=759
03/01/2022 20:23:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.33 on epoch=764
03/01/2022 20:23:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=769
03/01/2022 20:23:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=774
03/01/2022 20:23:28 - INFO - __main__ - Global step 1550 Train loss 0.34 EM 0.0 on epoch=774
03/01/2022 20:23:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.29 on epoch=779
03/01/2022 20:23:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=784
03/01/2022 20:23:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=789
03/01/2022 20:23:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=794
03/01/2022 20:23:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=799
03/01/2022 20:23:41 - INFO - __main__ - Global step 1600 Train loss 0.29 EM 0.0 on epoch=799
03/01/2022 20:23:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=804
03/01/2022 20:23:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=809
03/01/2022 20:23:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.29 on epoch=814
03/01/2022 20:23:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=819
03/01/2022 20:23:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=824
03/01/2022 20:23:54 - INFO - __main__ - Global step 1650 Train loss 0.28 EM 0.0 on epoch=824
03/01/2022 20:23:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=829
03/01/2022 20:23:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=834
03/01/2022 20:24:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=839
03/01/2022 20:24:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=844
03/01/2022 20:24:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.27 on epoch=849
03/01/2022 20:24:07 - INFO - __main__ - Global step 1700 Train loss 0.27 EM 0.0 on epoch=849
03/01/2022 20:24:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.26 on epoch=854
03/01/2022 20:24:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=859
03/01/2022 20:24:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=864
03/01/2022 20:24:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.29 on epoch=869
03/01/2022 20:24:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.27 on epoch=874
03/01/2022 20:24:20 - INFO - __main__ - Global step 1750 Train loss 0.27 EM 0.0 on epoch=874
03/01/2022 20:24:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=879
03/01/2022 20:24:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=884
03/01/2022 20:24:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
03/01/2022 20:24:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=894
03/01/2022 20:24:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=899
03/01/2022 20:24:32 - INFO - __main__ - Global step 1800 Train loss 0.26 EM 0.0 on epoch=899
03/01/2022 20:24:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=904
03/01/2022 20:24:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=909
03/01/2022 20:24:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=914
03/01/2022 20:24:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=919
03/01/2022 20:24:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=924
03/01/2022 20:24:45 - INFO - __main__ - Global step 1850 Train loss 0.21 EM 0.0 on epoch=924
03/01/2022 20:24:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.27 on epoch=929
03/01/2022 20:24:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=934
03/01/2022 20:24:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.21 on epoch=939
03/01/2022 20:24:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=944
03/01/2022 20:24:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=949
03/01/2022 20:24:58 - INFO - __main__ - Global step 1900 Train loss 0.22 EM 0.0 on epoch=949
03/01/2022 20:25:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=954
03/01/2022 20:25:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=959
03/01/2022 20:25:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=964
03/01/2022 20:25:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=969
03/01/2022 20:25:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=974
03/01/2022 20:25:11 - INFO - __main__ - Global step 1950 Train loss 0.20 EM 0.0 on epoch=974
03/01/2022 20:25:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=979
03/01/2022 20:25:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=984
03/01/2022 20:25:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=989
03/01/2022 20:25:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=994
03/01/2022 20:25:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=999
03/01/2022 20:25:23 - INFO - __main__ - Global step 2000 Train loss 0.20 EM 0.0 on epoch=999
03/01/2022 20:25:23 - INFO - __main__ - save last model!
03/01/2022 20:25:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:25:23 - INFO - __main__ - Printing 3 examples
03/01/2022 20:25:23 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 20:25:23 - INFO - __main__ - ['francois mitterrand']
03/01/2022 20:25:23 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 20:25:23 - INFO - __main__ - ['james callaghan']
03/01/2022 20:25:23 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 20:25:23 - INFO - __main__ - ['aberdeen']
03/01/2022 20:25:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 20:25:23 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:25:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 20:25:23 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 20:25:23 - INFO - __main__ - Printing 3 examples
03/01/2022 20:25:23 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 20:25:23 - INFO - __main__ - ['taming of the shrew']
03/01/2022 20:25:23 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 20:25:23 - INFO - __main__ - ['henry fonda']
03/01/2022 20:25:23 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 20:25:23 - INFO - __main__ - ['tchaikovsky']
03/01/2022 20:25:23 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:25:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:25:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:25:23 - INFO - __main__ - Printing 3 examples
03/01/2022 20:25:23 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 20:25:23 - INFO - __main__ - ['tulisa']
03/01/2022 20:25:23 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 20:25:23 - INFO - __main__ - ['calgary']
03/01/2022 20:25:23 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 20:25:23 - INFO - __main__ - ['jeff bridges']
03/01/2022 20:25:23 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:25:23 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:25:23 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:25:25 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:25:29 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 20:25:37 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 20:25:37 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(5086, 201716), (2701, 165860), (7228, 161576), (4782, 209172), (4369, 165583), (3567, 290752), (2487, 298264), (1475, 592201), (361, 212358), (634, 1165089), (5294, 199801), (3258, 211425), (3345, 310573), (2928, 338683), (5307, 158689), (382, 1423765), (3907, 265870), (3835, 184385), (797, 1085506), (3485, 303196), (5169, 202438), (3273, 207524), (428, 2074871), (460, 1411651), (4014, 254677), (3186, 326052), (838, 1056754), (983, 921089), (5130, 157985), (5461, 184577), (4375, 243717), (5071, 202122), (4009, 161333), (5894, 169006), (456, 1871965), (5095, 208743), (6224, 163528), (2005, 485196), (3547, 292304), (5140, 155426), (4241, 196978), (5528, 186850), (1487, 592557), (626, 1288292), (4737, 208385), (1009, 834880), (2773, 272911), (888, 977080), (5795, 185326), (1223, 524358), (455, 1972093), (2755, 373233), (3876, 213357), (4864, 212633), (5268, 179444), (4511, 179764), (6279, 154104), (1704, 569790), (1063, 258418), (4072, 251271), (3033, 329937), (4105, 241736), (1625, 230019), (2899, 278147), (6381, 157040), (556, 1506310), (3586, 281438), (671, 1208120), (2534, 284947), (992, 883589), (5961, 167603), (4307, 242447), (3022, 328805), (1708, 367245), (6246, 155887), (1616, 595713), (2271, 282308), (837, 1006426), (4624, 160645), (1513, 633559), (3097, 267345), (2518, 303433), (4263, 238773), (1601, 685019), (2558, 157757), (715, 971516), (3058, 304132), (4066, 167278), (4131, 248468), (3487, 286058), (1357, 690767), (5655, 178526), (1020, 892057), (3847, 221046), (5688, 184003), (2744, 259869), (122, 4321801), (2216, 259152), (3116, 304867)]
03/01/2022 20:25:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:25:38 - INFO - __main__ - Starting training!
03/01/2022 20:28:02 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_42_0.3_8_predictions.txt
03/01/2022 20:28:02 - INFO - __main__ - EM on test data: 0.0035
03/01/2022 20:28:03 - INFO - __main__ - prefix=freebase_qa_32_42, lr=0.3, bsz=8, dev_performance=0.0, test_performance=0.0035052578868302454
03/01/2022 20:28:03 - INFO - __main__ - Running ... prefix=freebase_qa_32_42, lr=0.2, bsz=8 ...
03/01/2022 20:28:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:28:04 - INFO - __main__ - Printing 3 examples
03/01/2022 20:28:04 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 20:28:04 - INFO - __main__ - ['francois mitterrand']
03/01/2022 20:28:04 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 20:28:04 - INFO - __main__ - ['james callaghan']
03/01/2022 20:28:04 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 20:28:04 - INFO - __main__ - ['aberdeen']
03/01/2022 20:28:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 20:28:04 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:28:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:28:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:28:04 - INFO - __main__ - Printing 3 examples
03/01/2022 20:28:04 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 20:28:04 - INFO - __main__ - ['tulisa']
03/01/2022 20:28:04 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 20:28:04 - INFO - __main__ - ['calgary']
03/01/2022 20:28:04 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 20:28:04 - INFO - __main__ - ['jeff bridges']
03/01/2022 20:28:04 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:28:04 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:28:04 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:28:15 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 20:28:15 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(5086, 201716), (2701, 165860), (7228, 161576), (4782, 209172), (4369, 165583), (3567, 290752), (2487, 298264), (1475, 592201), (361, 212358), (634, 1165089), (5294, 199801), (3258, 211425), (3345, 310573), (2928, 338683), (5307, 158689), (382, 1423765), (3907, 265870), (3835, 184385), (797, 1085506), (3485, 303196), (5169, 202438), (3273, 207524), (428, 2074871), (460, 1411651), (4014, 254677), (3186, 326052), (838, 1056754), (983, 921089), (5130, 157985), (5461, 184577), (4375, 243717), (5071, 202122), (4009, 161333), (5894, 169006), (456, 1871965), (5095, 208743), (6224, 163528), (2005, 485196), (3547, 292304), (5140, 155426), (4241, 196978), (5528, 186850), (1487, 592557), (626, 1288292), (4737, 208385), (1009, 834880), (2773, 272911), (888, 977080), (5795, 185326), (1223, 524358), (455, 1972093), (2755, 373233), (3876, 213357), (4864, 212633), (5268, 179444), (4511, 179764), (6279, 154104), (1704, 569790), (1063, 258418), (4072, 251271), (3033, 329937), (4105, 241736), (1625, 230019), (2899, 278147), (6381, 157040), (556, 1506310), (3586, 281438), (671, 1208120), (2534, 284947), (992, 883589), (5961, 167603), (4307, 242447), (3022, 328805), (1708, 367245), (6246, 155887), (1616, 595713), (2271, 282308), (837, 1006426), (4624, 160645), (1513, 633559), (3097, 267345), (2518, 303433), (4263, 238773), (1601, 685019), (2558, 157757), (715, 971516), (3058, 304132), (4066, 167278), (4131, 248468), (3487, 286058), (1357, 690767), (5655, 178526), (1020, 892057), (3847, 221046), (5688, 184003), (2744, 259869), (122, 4321801), (2216, 259152), (3116, 304867)]
03/01/2022 20:28:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:28:16 - INFO - __main__ - Starting training!
03/01/2022 20:28:21 - INFO - __main__ - Step 10 Global step 10 Train loss 5.10 on epoch=4
03/01/2022 20:28:23 - INFO - __main__ - Step 20 Global step 20 Train loss 4.69 on epoch=9
03/01/2022 20:28:25 - INFO - __main__ - Step 30 Global step 30 Train loss 4.31 on epoch=14
03/01/2022 20:28:28 - INFO - __main__ - Step 40 Global step 40 Train loss 3.88 on epoch=19
03/01/2022 20:28:30 - INFO - __main__ - Step 50 Global step 50 Train loss 3.63 on epoch=24
03/01/2022 20:28:31 - INFO - __main__ - Global step 50 Train loss 4.32 EM 0.0 on epoch=24
03/01/2022 20:28:31 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 20:28:33 - INFO - __main__ - Step 60 Global step 60 Train loss 3.46 on epoch=29
03/01/2022 20:28:36 - INFO - __main__ - Step 70 Global step 70 Train loss 3.41 on epoch=34
03/01/2022 20:28:38 - INFO - __main__ - Step 80 Global step 80 Train loss 3.20 on epoch=39
03/01/2022 20:28:40 - INFO - __main__ - Step 90 Global step 90 Train loss 3.14 on epoch=44
03/01/2022 20:28:43 - INFO - __main__ - Step 100 Global step 100 Train loss 3.01 on epoch=49
03/01/2022 20:28:44 - INFO - __main__ - Global step 100 Train loss 3.24 EM 0.0 on epoch=49
03/01/2022 20:28:46 - INFO - __main__ - Step 110 Global step 110 Train loss 2.99 on epoch=54
03/01/2022 20:28:48 - INFO - __main__ - Step 120 Global step 120 Train loss 2.95 on epoch=59
03/01/2022 20:28:51 - INFO - __main__ - Step 130 Global step 130 Train loss 2.90 on epoch=64
03/01/2022 20:28:53 - INFO - __main__ - Step 140 Global step 140 Train loss 2.74 on epoch=69
03/01/2022 20:28:55 - INFO - __main__ - Step 150 Global step 150 Train loss 2.77 on epoch=74
03/01/2022 20:28:57 - INFO - __main__ - Global step 150 Train loss 2.87 EM 0.0 on epoch=74
03/01/2022 20:28:59 - INFO - __main__ - Step 160 Global step 160 Train loss 2.68 on epoch=79
03/01/2022 20:29:01 - INFO - __main__ - Step 170 Global step 170 Train loss 2.62 on epoch=84
03/01/2022 20:29:03 - INFO - __main__ - Step 180 Global step 180 Train loss 2.70 on epoch=89
03/01/2022 20:29:06 - INFO - __main__ - Step 190 Global step 190 Train loss 2.64 on epoch=94
03/01/2022 20:29:08 - INFO - __main__ - Step 200 Global step 200 Train loss 2.52 on epoch=99
03/01/2022 20:29:09 - INFO - __main__ - Global step 200 Train loss 2.63 EM 0.0 on epoch=99
03/01/2022 20:29:11 - INFO - __main__ - Step 210 Global step 210 Train loss 2.62 on epoch=104
03/01/2022 20:29:14 - INFO - __main__ - Step 220 Global step 220 Train loss 2.48 on epoch=109
03/01/2022 20:29:16 - INFO - __main__ - Step 230 Global step 230 Train loss 2.47 on epoch=114
03/01/2022 20:29:18 - INFO - __main__ - Step 240 Global step 240 Train loss 2.42 on epoch=119
03/01/2022 20:29:21 - INFO - __main__ - Step 250 Global step 250 Train loss 2.39 on epoch=124
03/01/2022 20:29:22 - INFO - __main__ - Global step 250 Train loss 2.47 EM 0.0 on epoch=124
03/01/2022 20:29:24 - INFO - __main__ - Step 260 Global step 260 Train loss 2.41 on epoch=129
03/01/2022 20:29:26 - INFO - __main__ - Step 270 Global step 270 Train loss 2.41 on epoch=134
03/01/2022 20:29:29 - INFO - __main__ - Step 280 Global step 280 Train loss 2.33 on epoch=139
03/01/2022 20:29:31 - INFO - __main__ - Step 290 Global step 290 Train loss 2.29 on epoch=144
03/01/2022 20:29:33 - INFO - __main__ - Step 300 Global step 300 Train loss 2.28 on epoch=149
03/01/2022 20:29:34 - INFO - __main__ - Global step 300 Train loss 2.34 EM 0.0 on epoch=149
03/01/2022 20:29:37 - INFO - __main__ - Step 310 Global step 310 Train loss 2.17 on epoch=154
03/01/2022 20:29:39 - INFO - __main__ - Step 320 Global step 320 Train loss 2.21 on epoch=159
03/01/2022 20:29:41 - INFO - __main__ - Step 330 Global step 330 Train loss 2.23 on epoch=164
03/01/2022 20:29:43 - INFO - __main__ - Step 340 Global step 340 Train loss 2.21 on epoch=169
03/01/2022 20:29:46 - INFO - __main__ - Step 350 Global step 350 Train loss 2.09 on epoch=174
03/01/2022 20:29:47 - INFO - __main__ - Global step 350 Train loss 2.18 EM 0.0 on epoch=174
03/01/2022 20:29:49 - INFO - __main__ - Step 360 Global step 360 Train loss 2.10 on epoch=179
03/01/2022 20:29:51 - INFO - __main__ - Step 370 Global step 370 Train loss 2.02 on epoch=184
03/01/2022 20:29:54 - INFO - __main__ - Step 380 Global step 380 Train loss 2.04 on epoch=189
03/01/2022 20:29:56 - INFO - __main__ - Step 390 Global step 390 Train loss 2.00 on epoch=194
03/01/2022 20:29:58 - INFO - __main__ - Step 400 Global step 400 Train loss 2.08 on epoch=199
03/01/2022 20:29:59 - INFO - __main__ - Global step 400 Train loss 2.05 EM 0.0 on epoch=199
03/01/2022 20:30:02 - INFO - __main__ - Step 410 Global step 410 Train loss 1.96 on epoch=204
03/01/2022 20:30:04 - INFO - __main__ - Step 420 Global step 420 Train loss 1.89 on epoch=209
03/01/2022 20:30:06 - INFO - __main__ - Step 430 Global step 430 Train loss 1.92 on epoch=214
03/01/2022 20:30:09 - INFO - __main__ - Step 440 Global step 440 Train loss 1.86 on epoch=219
03/01/2022 20:30:11 - INFO - __main__ - Step 450 Global step 450 Train loss 1.89 on epoch=224
03/01/2022 20:30:13 - INFO - __main__ - Global step 450 Train loss 1.90 EM 0.0 on epoch=224
03/01/2022 20:30:15 - INFO - __main__ - Step 460 Global step 460 Train loss 1.82 on epoch=229
03/01/2022 20:30:17 - INFO - __main__ - Step 470 Global step 470 Train loss 1.75 on epoch=234
03/01/2022 20:30:19 - INFO - __main__ - Step 480 Global step 480 Train loss 1.80 on epoch=239
03/01/2022 20:30:22 - INFO - __main__ - Step 490 Global step 490 Train loss 1.77 on epoch=244
03/01/2022 20:30:24 - INFO - __main__ - Step 500 Global step 500 Train loss 1.80 on epoch=249
03/01/2022 20:30:25 - INFO - __main__ - Global step 500 Train loss 1.79 EM 0.0 on epoch=249
03/01/2022 20:30:27 - INFO - __main__ - Step 510 Global step 510 Train loss 1.70 on epoch=254
03/01/2022 20:30:30 - INFO - __main__ - Step 520 Global step 520 Train loss 1.68 on epoch=259
03/01/2022 20:30:32 - INFO - __main__ - Step 530 Global step 530 Train loss 1.69 on epoch=264
03/01/2022 20:30:34 - INFO - __main__ - Step 540 Global step 540 Train loss 1.68 on epoch=269
03/01/2022 20:30:37 - INFO - __main__ - Step 550 Global step 550 Train loss 1.74 on epoch=274
03/01/2022 20:30:38 - INFO - __main__ - Global step 550 Train loss 1.70 EM 0.0 on epoch=274
03/01/2022 20:30:40 - INFO - __main__ - Step 560 Global step 560 Train loss 1.69 on epoch=279
03/01/2022 20:30:42 - INFO - __main__ - Step 570 Global step 570 Train loss 1.63 on epoch=284
03/01/2022 20:30:45 - INFO - __main__ - Step 580 Global step 580 Train loss 1.65 on epoch=289
03/01/2022 20:30:47 - INFO - __main__ - Step 590 Global step 590 Train loss 1.62 on epoch=294
03/01/2022 20:30:49 - INFO - __main__ - Step 600 Global step 600 Train loss 1.61 on epoch=299
03/01/2022 20:30:51 - INFO - __main__ - Global step 600 Train loss 1.64 EM 0.0 on epoch=299
03/01/2022 20:30:53 - INFO - __main__ - Step 610 Global step 610 Train loss 1.58 on epoch=304
03/01/2022 20:30:55 - INFO - __main__ - Step 620 Global step 620 Train loss 1.61 on epoch=309
03/01/2022 20:30:57 - INFO - __main__ - Step 630 Global step 630 Train loss 1.52 on epoch=314
03/01/2022 20:31:00 - INFO - __main__ - Step 640 Global step 640 Train loss 1.52 on epoch=319
03/01/2022 20:31:02 - INFO - __main__ - Step 650 Global step 650 Train loss 1.50 on epoch=324
03/01/2022 20:31:03 - INFO - __main__ - Global step 650 Train loss 1.55 EM 0.0 on epoch=324
03/01/2022 20:31:05 - INFO - __main__ - Step 660 Global step 660 Train loss 1.52 on epoch=329
03/01/2022 20:31:08 - INFO - __main__ - Step 670 Global step 670 Train loss 1.54 on epoch=334
03/01/2022 20:31:10 - INFO - __main__ - Step 680 Global step 680 Train loss 1.52 on epoch=339
03/01/2022 20:31:12 - INFO - __main__ - Step 690 Global step 690 Train loss 1.48 on epoch=344
03/01/2022 20:31:15 - INFO - __main__ - Step 700 Global step 700 Train loss 1.49 on epoch=349
03/01/2022 20:31:16 - INFO - __main__ - Global step 700 Train loss 1.51 EM 0.0 on epoch=349
03/01/2022 20:31:18 - INFO - __main__ - Step 710 Global step 710 Train loss 1.40 on epoch=354
03/01/2022 20:31:20 - INFO - __main__ - Step 720 Global step 720 Train loss 1.44 on epoch=359
03/01/2022 20:31:23 - INFO - __main__ - Step 730 Global step 730 Train loss 1.41 on epoch=364
03/01/2022 20:31:25 - INFO - __main__ - Step 740 Global step 740 Train loss 1.37 on epoch=369
03/01/2022 20:31:27 - INFO - __main__ - Step 750 Global step 750 Train loss 1.46 on epoch=374
03/01/2022 20:31:28 - INFO - __main__ - Global step 750 Train loss 1.42 EM 0.0 on epoch=374
03/01/2022 20:31:31 - INFO - __main__ - Step 760 Global step 760 Train loss 1.37 on epoch=379
03/01/2022 20:31:33 - INFO - __main__ - Step 770 Global step 770 Train loss 1.35 on epoch=384
03/01/2022 20:31:35 - INFO - __main__ - Step 780 Global step 780 Train loss 1.38 on epoch=389
03/01/2022 20:31:37 - INFO - __main__ - Step 790 Global step 790 Train loss 1.30 on epoch=394
03/01/2022 20:31:40 - INFO - __main__ - Step 800 Global step 800 Train loss 1.41 on epoch=399
03/01/2022 20:31:41 - INFO - __main__ - Global step 800 Train loss 1.36 EM 0.0 on epoch=399
03/01/2022 20:31:43 - INFO - __main__ - Step 810 Global step 810 Train loss 1.38 on epoch=404
03/01/2022 20:31:45 - INFO - __main__ - Step 820 Global step 820 Train loss 1.32 on epoch=409
03/01/2022 20:31:48 - INFO - __main__ - Step 830 Global step 830 Train loss 1.29 on epoch=414
03/01/2022 20:31:50 - INFO - __main__ - Step 840 Global step 840 Train loss 1.29 on epoch=419
03/01/2022 20:31:52 - INFO - __main__ - Step 850 Global step 850 Train loss 1.25 on epoch=424
03/01/2022 20:31:54 - INFO - __main__ - Global step 850 Train loss 1.31 EM 0.0 on epoch=424
03/01/2022 20:31:56 - INFO - __main__ - Step 860 Global step 860 Train loss 1.38 on epoch=429
03/01/2022 20:31:58 - INFO - __main__ - Step 870 Global step 870 Train loss 1.24 on epoch=434
03/01/2022 20:32:01 - INFO - __main__ - Step 880 Global step 880 Train loss 1.16 on epoch=439
03/01/2022 20:32:03 - INFO - __main__ - Step 890 Global step 890 Train loss 1.26 on epoch=444
03/01/2022 20:32:05 - INFO - __main__ - Step 900 Global step 900 Train loss 1.19 on epoch=449
03/01/2022 20:32:06 - INFO - __main__ - Global step 900 Train loss 1.25 EM 0.0 on epoch=449
03/01/2022 20:32:09 - INFO - __main__ - Step 910 Global step 910 Train loss 1.22 on epoch=454
03/01/2022 20:32:11 - INFO - __main__ - Step 920 Global step 920 Train loss 1.24 on epoch=459
03/01/2022 20:32:13 - INFO - __main__ - Step 930 Global step 930 Train loss 1.14 on epoch=464
03/01/2022 20:32:15 - INFO - __main__ - Step 940 Global step 940 Train loss 1.06 on epoch=469
03/01/2022 20:32:18 - INFO - __main__ - Step 950 Global step 950 Train loss 1.07 on epoch=474
03/01/2022 20:32:19 - INFO - __main__ - Global step 950 Train loss 1.15 EM 0.0 on epoch=474
03/01/2022 20:32:21 - INFO - __main__ - Step 960 Global step 960 Train loss 1.14 on epoch=479
03/01/2022 20:32:23 - INFO - __main__ - Step 970 Global step 970 Train loss 1.14 on epoch=484
03/01/2022 20:32:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.99 on epoch=489
03/01/2022 20:32:28 - INFO - __main__ - Step 990 Global step 990 Train loss 1.14 on epoch=494
03/01/2022 20:32:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.07 on epoch=499
03/01/2022 20:32:32 - INFO - __main__ - Global step 1000 Train loss 1.10 EM 0.0 on epoch=499
03/01/2022 20:32:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.15 on epoch=504
03/01/2022 20:32:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.00 on epoch=509
03/01/2022 20:32:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.05 on epoch=514
03/01/2022 20:32:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.07 on epoch=519
03/01/2022 20:32:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.05 on epoch=524
03/01/2022 20:32:44 - INFO - __main__ - Global step 1050 Train loss 1.06 EM 0.0 on epoch=524
03/01/2022 20:32:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.95 on epoch=529
03/01/2022 20:32:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.98 on epoch=534
03/01/2022 20:32:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.93 on epoch=539
03/01/2022 20:32:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.91 on epoch=544
03/01/2022 20:32:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.97 on epoch=549
03/01/2022 20:32:56 - INFO - __main__ - Global step 1100 Train loss 0.95 EM 0.0 on epoch=549
03/01/2022 20:32:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.86 on epoch=554
03/01/2022 20:33:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.93 on epoch=559
03/01/2022 20:33:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.96 on epoch=564
03/01/2022 20:33:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.89 on epoch=569
03/01/2022 20:33:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.86 on epoch=574
03/01/2022 20:33:09 - INFO - __main__ - Global step 1150 Train loss 0.90 EM 0.0 on epoch=574
03/01/2022 20:33:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.96 on epoch=579
03/01/2022 20:33:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.84 on epoch=584
03/01/2022 20:33:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.86 on epoch=589
03/01/2022 20:33:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.80 on epoch=594
03/01/2022 20:33:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=599
03/01/2022 20:33:22 - INFO - __main__ - Global step 1200 Train loss 0.86 EM 0.0 on epoch=599
03/01/2022 20:33:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.82 on epoch=604
03/01/2022 20:33:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=609
03/01/2022 20:33:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.84 on epoch=614
03/01/2022 20:33:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.75 on epoch=619
03/01/2022 20:33:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.81 on epoch=624
03/01/2022 20:33:34 - INFO - __main__ - Global step 1250 Train loss 0.81 EM 0.0 on epoch=624
03/01/2022 20:33:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.80 on epoch=629
03/01/2022 20:33:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.70 on epoch=634
03/01/2022 20:33:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.72 on epoch=639
03/01/2022 20:33:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.76 on epoch=644
03/01/2022 20:33:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.78 on epoch=649
03/01/2022 20:33:47 - INFO - __main__ - Global step 1300 Train loss 0.75 EM 0.0 on epoch=649
03/01/2022 20:33:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.79 on epoch=654
03/01/2022 20:33:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.80 on epoch=659
03/01/2022 20:33:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.68 on epoch=664
03/01/2022 20:33:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.81 on epoch=669
03/01/2022 20:33:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.73 on epoch=674
03/01/2022 20:33:59 - INFO - __main__ - Global step 1350 Train loss 0.76 EM 0.0 on epoch=674
03/01/2022 20:34:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.77 on epoch=679
03/01/2022 20:34:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.67 on epoch=684
03/01/2022 20:34:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.76 on epoch=689
03/01/2022 20:34:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.72 on epoch=694
03/01/2022 20:34:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.72 on epoch=699
03/01/2022 20:34:12 - INFO - __main__ - Global step 1400 Train loss 0.73 EM 0.0 on epoch=699
03/01/2022 20:34:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.70 on epoch=704
03/01/2022 20:34:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.66 on epoch=709
03/01/2022 20:34:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.66 on epoch=714
03/01/2022 20:34:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.64 on epoch=719
03/01/2022 20:34:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.65 on epoch=724
03/01/2022 20:34:24 - INFO - __main__ - Global step 1450 Train loss 0.66 EM 0.0 on epoch=724
03/01/2022 20:34:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.68 on epoch=729
03/01/2022 20:34:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.59 on epoch=734
03/01/2022 20:34:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.64 on epoch=739
03/01/2022 20:34:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.76 on epoch=744
03/01/2022 20:34:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.62 on epoch=749
03/01/2022 20:34:36 - INFO - __main__ - Global step 1500 Train loss 0.66 EM 0.0 on epoch=749
03/01/2022 20:34:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.63 on epoch=754
03/01/2022 20:34:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.67 on epoch=759
03/01/2022 20:34:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.59 on epoch=764
03/01/2022 20:34:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.57 on epoch=769
03/01/2022 20:34:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.67 on epoch=774
03/01/2022 20:34:49 - INFO - __main__ - Global step 1550 Train loss 0.63 EM 0.0 on epoch=774
03/01/2022 20:34:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.61 on epoch=779
03/01/2022 20:34:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.62 on epoch=784
03/01/2022 20:34:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.58 on epoch=789
03/01/2022 20:34:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.54 on epoch=794
03/01/2022 20:35:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.64 on epoch=799
03/01/2022 20:35:01 - INFO - __main__ - Global step 1600 Train loss 0.60 EM 0.0 on epoch=799
03/01/2022 20:35:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.64 on epoch=804
03/01/2022 20:35:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.53 on epoch=809
03/01/2022 20:35:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.54 on epoch=814
03/01/2022 20:35:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.57 on epoch=819
03/01/2022 20:35:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.54 on epoch=824
03/01/2022 20:35:14 - INFO - __main__ - Global step 1650 Train loss 0.56 EM 0.0 on epoch=824
03/01/2022 20:35:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.57 on epoch=829
03/01/2022 20:35:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.60 on epoch=834
03/01/2022 20:35:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.50 on epoch=839
03/01/2022 20:35:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=844
03/01/2022 20:35:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.54 on epoch=849
03/01/2022 20:35:27 - INFO - __main__ - Global step 1700 Train loss 0.54 EM 0.0 on epoch=849
03/01/2022 20:35:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.50 on epoch=854
03/01/2022 20:35:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.59 on epoch=859
03/01/2022 20:35:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.50 on epoch=864
03/01/2022 20:35:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.60 on epoch=869
03/01/2022 20:35:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.60 on epoch=874
03/01/2022 20:35:39 - INFO - __main__ - Global step 1750 Train loss 0.56 EM 0.0 on epoch=874
03/01/2022 20:35:41 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=879
03/01/2022 20:35:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.53 on epoch=884
03/01/2022 20:35:46 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.51 on epoch=889
03/01/2022 20:35:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.53 on epoch=894
03/01/2022 20:35:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=899
03/01/2022 20:35:52 - INFO - __main__ - Global step 1800 Train loss 0.50 EM 0.0 on epoch=899
03/01/2022 20:35:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.56 on epoch=904
03/01/2022 20:35:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.48 on epoch=909
03/01/2022 20:35:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.56 on epoch=914
03/01/2022 20:36:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.47 on epoch=919
03/01/2022 20:36:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.49 on epoch=924
03/01/2022 20:36:04 - INFO - __main__ - Global step 1850 Train loss 0.51 EM 0.0 on epoch=924
03/01/2022 20:36:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=929
03/01/2022 20:36:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.52 on epoch=934
03/01/2022 20:36:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.56 on epoch=939
03/01/2022 20:36:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=944
03/01/2022 20:36:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=949
03/01/2022 20:36:17 - INFO - __main__ - Global step 1900 Train loss 0.47 EM 0.0 on epoch=949
03/01/2022 20:36:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=954
03/01/2022 20:36:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.51 on epoch=959
03/01/2022 20:36:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=964
03/01/2022 20:36:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=969
03/01/2022 20:36:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.45 on epoch=974
03/01/2022 20:36:30 - INFO - __main__ - Global step 1950 Train loss 0.45 EM 0.0 on epoch=974
03/01/2022 20:36:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.47 on epoch=979
03/01/2022 20:36:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.52 on epoch=984
03/01/2022 20:36:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.49 on epoch=989
03/01/2022 20:36:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=994
03/01/2022 20:36:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=999
03/01/2022 20:36:42 - INFO - __main__ - Global step 2000 Train loss 0.48 EM 0.0 on epoch=999
03/01/2022 20:36:42 - INFO - __main__ - save last model!
03/01/2022 20:36:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:36:42 - INFO - __main__ - Printing 3 examples
03/01/2022 20:36:42 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 20:36:42 - INFO - __main__ - ['ok computer']
03/01/2022 20:36:42 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 20:36:42 - INFO - __main__ - ['thursday']
03/01/2022 20:36:42 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 20:36:42 - INFO - __main__ - ['sigourney weaver']
03/01/2022 20:36:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 20:36:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 20:36:42 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:36:42 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 20:36:42 - INFO - __main__ - Printing 3 examples
03/01/2022 20:36:42 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 20:36:42 - INFO - __main__ - ['taming of the shrew']
03/01/2022 20:36:42 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 20:36:42 - INFO - __main__ - ['henry fonda']
03/01/2022 20:36:42 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 20:36:42 - INFO - __main__ - ['tchaikovsky']
03/01/2022 20:36:42 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:36:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:36:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:36:42 - INFO - __main__ - Printing 3 examples
03/01/2022 20:36:42 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 20:36:42 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 20:36:42 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 20:36:42 - INFO - __main__ - ['daphne du maurier']
03/01/2022 20:36:42 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 20:36:42 - INFO - __main__ - ['back to the future']
03/01/2022 20:36:42 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:36:42 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:36:42 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:36:44 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:36:48 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 20:36:54 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 20:36:54 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(1865, 505831), (2321, 427255), (3814, 275126), (3534, 191669), (2081, 476380), (1981, 405777), (50, 649230), (1702, 582322), (711, 1051315), (3217, 316619), (1084, 848613), (3574, 184323), (6060, 173161), (5272, 178396), (435, 2072263), (3040, 279538), (526, 676575), (6578, 157228), (2318, 248450), (1722, 583639), (1355, 559231), (1871, 504548), (1783, 548358), (4188, 213409), (502, 1761063), (4879, 229125), (3904, 258285), (2677, 390904), (5781, 210606), (3315, 327974), (4782, 209172), (2493, 394839), (5849, 175119), (2287, 388098), (3134, 304431), (5281, 199800), (2250, 472469), (4245, 255196), (4542, 163807), (3537, 211928), (5169, 202438), (3127, 331271), (3826, 245516), (5640, 186094), (5055, 178040), (836, 224036), (1326, 560217), (1626, 458443), (6468, 155450), (968, 645839), (2507, 369601), (585, 1508230), (5330, 194338), (4073, 252987), (6168, 168840), (1087, 843077), (5017, 181544), (1244, 709702), (3060, 327552), (915, 980631), (1749, 563947), (575, 340830), (5285, 209039), (6026, 182603), (727, 605486), (3597, 213529), (1246, 907731), (2471, 412585), (4796, 207811), (3545, 267617), (315, 2942959), (4657, 226628), (582, 1484200), (4326, 236656), (6178, 186518), (4226, 250683), (2416, 392857), (4193, 242583), (885, 1006302), (4093, 250163), (2496, 408535), (337, 2812930), (3635, 295690), (1096, 857963), (3387, 280917), (1568, 614370), (1485, 648438), (429, 2065410), (3168, 326177), (2226, 198030), (1406, 645374), (4888, 209272), (10, 16409736), (791, 1024488), (223, 4176310), (2790, 361504), (1575, 424541), (1514, 597937), (6198, 170564)]
03/01/2022 20:36:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:36:55 - INFO - __main__ - Starting training!
03/01/2022 20:39:17 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_42_0.2_8_predictions.txt
03/01/2022 20:39:17 - INFO - __main__ - EM on test data: 0.0038
03/01/2022 20:39:18 - INFO - __main__ - prefix=freebase_qa_32_42, lr=0.2, bsz=8, dev_performance=0.0, test_performance=0.003755633450175263
03/01/2022 20:39:18 - INFO - __main__ - Running ... prefix=freebase_qa_32_87, lr=0.5, bsz=8 ...
03/01/2022 20:39:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:39:19 - INFO - __main__ - Printing 3 examples
03/01/2022 20:39:19 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 20:39:19 - INFO - __main__ - ['ok computer']
03/01/2022 20:39:19 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 20:39:19 - INFO - __main__ - ['thursday']
03/01/2022 20:39:19 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 20:39:19 - INFO - __main__ - ['sigourney weaver']
03/01/2022 20:39:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 20:39:19 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:39:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:39:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:39:19 - INFO - __main__ - Printing 3 examples
03/01/2022 20:39:19 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 20:39:19 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 20:39:19 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 20:39:19 - INFO - __main__ - ['daphne du maurier']
03/01/2022 20:39:19 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 20:39:19 - INFO - __main__ - ['back to the future']
03/01/2022 20:39:19 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:39:19 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:39:19 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:39:31 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 20:39:31 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(1865, 505831), (2321, 427255), (3814, 275126), (3534, 191669), (2081, 476380), (1981, 405777), (50, 649230), (1702, 582322), (711, 1051315), (3217, 316619), (1084, 848613), (3574, 184323), (6060, 173161), (5272, 178396), (435, 2072263), (3040, 279538), (526, 676575), (6578, 157228), (2318, 248450), (1722, 583639), (1355, 559231), (1871, 504548), (1783, 548358), (4188, 213409), (502, 1761063), (4879, 229125), (3904, 258285), (2677, 390904), (5781, 210606), (3315, 327974), (4782, 209172), (2493, 394839), (5849, 175119), (2287, 388098), (3134, 304431), (5281, 199800), (2250, 472469), (4245, 255196), (4542, 163807), (3537, 211928), (5169, 202438), (3127, 331271), (3826, 245516), (5640, 186094), (5055, 178040), (836, 224036), (1326, 560217), (1626, 458443), (6468, 155450), (968, 645839), (2507, 369601), (585, 1508230), (5330, 194338), (4073, 252987), (6168, 168840), (1087, 843077), (5017, 181544), (1244, 709702), (3060, 327552), (915, 980631), (1749, 563947), (575, 340830), (5285, 209039), (6026, 182603), (727, 605486), (3597, 213529), (1246, 907731), (2471, 412585), (4796, 207811), (3545, 267617), (315, 2942959), (4657, 226628), (582, 1484200), (4326, 236656), (6178, 186518), (4226, 250683), (2416, 392857), (4193, 242583), (885, 1006302), (4093, 250163), (2496, 408535), (337, 2812930), (3635, 295690), (1096, 857963), (3387, 280917), (1568, 614370), (1485, 648438), (429, 2065410), (3168, 326177), (2226, 198030), (1406, 645374), (4888, 209272), (10, 16409736), (791, 1024488), (223, 4176310), (2790, 361504), (1575, 424541), (1514, 597937), (6198, 170564)]
03/01/2022 20:39:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:39:32 - INFO - __main__ - Starting training!
03/01/2022 20:39:34 - INFO - __main__ - Step 10 Global step 10 Train loss 4.75 on epoch=4
03/01/2022 20:39:37 - INFO - __main__ - Step 20 Global step 20 Train loss 4.02 on epoch=9
03/01/2022 20:39:39 - INFO - __main__ - Step 30 Global step 30 Train loss 3.54 on epoch=14
03/01/2022 20:39:41 - INFO - __main__ - Step 40 Global step 40 Train loss 3.22 on epoch=19
03/01/2022 20:39:43 - INFO - __main__ - Step 50 Global step 50 Train loss 3.14 on epoch=24
03/01/2022 20:39:45 - INFO - __main__ - Global step 50 Train loss 3.73 EM 0.0 on epoch=24
03/01/2022 20:39:45 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 20:39:47 - INFO - __main__ - Step 60 Global step 60 Train loss 2.88 on epoch=29
03/01/2022 20:39:49 - INFO - __main__ - Step 70 Global step 70 Train loss 2.71 on epoch=34
03/01/2022 20:39:51 - INFO - __main__ - Step 80 Global step 80 Train loss 2.73 on epoch=39
03/01/2022 20:39:53 - INFO - __main__ - Step 90 Global step 90 Train loss 2.59 on epoch=44
03/01/2022 20:39:56 - INFO - __main__ - Step 100 Global step 100 Train loss 2.52 on epoch=49
03/01/2022 20:39:57 - INFO - __main__ - Global step 100 Train loss 2.69 EM 0.0 on epoch=49
03/01/2022 20:39:59 - INFO - __main__ - Step 110 Global step 110 Train loss 2.24 on epoch=54
03/01/2022 20:40:02 - INFO - __main__ - Step 120 Global step 120 Train loss 2.28 on epoch=59
03/01/2022 20:40:04 - INFO - __main__ - Step 130 Global step 130 Train loss 2.19 on epoch=64
03/01/2022 20:40:06 - INFO - __main__ - Step 140 Global step 140 Train loss 2.10 on epoch=69
03/01/2022 20:40:08 - INFO - __main__ - Step 150 Global step 150 Train loss 2.04 on epoch=74
03/01/2022 20:40:09 - INFO - __main__ - Global step 150 Train loss 2.17 EM 0.0 on epoch=74
03/01/2022 20:40:12 - INFO - __main__ - Step 160 Global step 160 Train loss 2.02 on epoch=79
03/01/2022 20:40:14 - INFO - __main__ - Step 170 Global step 170 Train loss 1.93 on epoch=84
03/01/2022 20:40:16 - INFO - __main__ - Step 180 Global step 180 Train loss 1.82 on epoch=89
03/01/2022 20:40:18 - INFO - __main__ - Step 190 Global step 190 Train loss 1.88 on epoch=94
03/01/2022 20:40:20 - INFO - __main__ - Step 200 Global step 200 Train loss 1.75 on epoch=99
03/01/2022 20:40:22 - INFO - __main__ - Global step 200 Train loss 1.88 EM 0.0 on epoch=99
03/01/2022 20:40:24 - INFO - __main__ - Step 210 Global step 210 Train loss 1.73 on epoch=104
03/01/2022 20:40:26 - INFO - __main__ - Step 220 Global step 220 Train loss 1.69 on epoch=109
03/01/2022 20:40:28 - INFO - __main__ - Step 230 Global step 230 Train loss 1.66 on epoch=114
03/01/2022 20:40:30 - INFO - __main__ - Step 240 Global step 240 Train loss 1.58 on epoch=119
03/01/2022 20:40:33 - INFO - __main__ - Step 250 Global step 250 Train loss 1.54 on epoch=124
03/01/2022 20:40:34 - INFO - __main__ - Global step 250 Train loss 1.64 EM 0.0 on epoch=124
03/01/2022 20:40:36 - INFO - __main__ - Step 260 Global step 260 Train loss 1.51 on epoch=129
03/01/2022 20:40:39 - INFO - __main__ - Step 270 Global step 270 Train loss 1.42 on epoch=134
03/01/2022 20:40:41 - INFO - __main__ - Step 280 Global step 280 Train loss 1.43 on epoch=139
03/01/2022 20:40:43 - INFO - __main__ - Step 290 Global step 290 Train loss 1.35 on epoch=144
03/01/2022 20:40:45 - INFO - __main__ - Step 300 Global step 300 Train loss 1.33 on epoch=149
03/01/2022 20:40:47 - INFO - __main__ - Global step 300 Train loss 1.41 EM 0.0 on epoch=149
03/01/2022 20:40:49 - INFO - __main__ - Step 310 Global step 310 Train loss 1.31 on epoch=154
03/01/2022 20:40:51 - INFO - __main__ - Step 320 Global step 320 Train loss 1.18 on epoch=159
03/01/2022 20:40:53 - INFO - __main__ - Step 330 Global step 330 Train loss 1.25 on epoch=164
03/01/2022 20:40:55 - INFO - __main__ - Step 340 Global step 340 Train loss 1.20 on epoch=169
03/01/2022 20:40:57 - INFO - __main__ - Step 350 Global step 350 Train loss 1.17 on epoch=174
03/01/2022 20:40:59 - INFO - __main__ - Global step 350 Train loss 1.22 EM 0.0 on epoch=174
03/01/2022 20:41:01 - INFO - __main__ - Step 360 Global step 360 Train loss 1.15 on epoch=179
03/01/2022 20:41:03 - INFO - __main__ - Step 370 Global step 370 Train loss 1.19 on epoch=184
03/01/2022 20:41:05 - INFO - __main__ - Step 380 Global step 380 Train loss 1.09 on epoch=189
03/01/2022 20:41:07 - INFO - __main__ - Step 390 Global step 390 Train loss 1.09 on epoch=194
03/01/2022 20:41:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.98 on epoch=199
03/01/2022 20:41:11 - INFO - __main__ - Global step 400 Train loss 1.10 EM 0.0 on epoch=199
03/01/2022 20:41:13 - INFO - __main__ - Step 410 Global step 410 Train loss 1.01 on epoch=204
03/01/2022 20:41:15 - INFO - __main__ - Step 420 Global step 420 Train loss 1.03 on epoch=209
03/01/2022 20:41:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.95 on epoch=214
03/01/2022 20:41:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.90 on epoch=219
03/01/2022 20:41:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.85 on epoch=224
03/01/2022 20:41:23 - INFO - __main__ - Global step 450 Train loss 0.95 EM 0.0 on epoch=224
03/01/2022 20:41:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.87 on epoch=229
03/01/2022 20:41:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.82 on epoch=234
03/01/2022 20:41:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.85 on epoch=239
03/01/2022 20:41:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.87 on epoch=244
03/01/2022 20:41:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.91 on epoch=249
03/01/2022 20:41:36 - INFO - __main__ - Global step 500 Train loss 0.86 EM 0.0 on epoch=249
03/01/2022 20:41:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.72 on epoch=254
03/01/2022 20:41:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.81 on epoch=259
03/01/2022 20:41:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.70 on epoch=264
03/01/2022 20:41:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.69 on epoch=269
03/01/2022 20:41:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.76 on epoch=274
03/01/2022 20:41:49 - INFO - __main__ - Global step 550 Train loss 0.74 EM 0.0 on epoch=274
03/01/2022 20:41:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.65 on epoch=279
03/01/2022 20:41:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.67 on epoch=284
03/01/2022 20:41:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.68 on epoch=289
03/01/2022 20:41:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.68 on epoch=294
03/01/2022 20:42:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.67 on epoch=299
03/01/2022 20:42:01 - INFO - __main__ - Global step 600 Train loss 0.67 EM 0.0 on epoch=299
03/01/2022 20:42:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.65 on epoch=304
03/01/2022 20:42:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.67 on epoch=309
03/01/2022 20:42:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.64 on epoch=314
03/01/2022 20:42:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.63 on epoch=319
03/01/2022 20:42:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.64 on epoch=324
03/01/2022 20:42:14 - INFO - __main__ - Global step 650 Train loss 0.64 EM 0.0 on epoch=324
03/01/2022 20:42:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.65 on epoch=329
03/01/2022 20:42:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.62 on epoch=334
03/01/2022 20:42:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.63 on epoch=339
03/01/2022 20:42:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.58 on epoch=344
03/01/2022 20:42:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=349
03/01/2022 20:42:26 - INFO - __main__ - Global step 700 Train loss 0.59 EM 0.0 on epoch=349
03/01/2022 20:42:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.53 on epoch=354
03/01/2022 20:42:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=359
03/01/2022 20:42:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=364
03/01/2022 20:42:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.53 on epoch=369
03/01/2022 20:42:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=374
03/01/2022 20:42:38 - INFO - __main__ - Global step 750 Train loss 0.53 EM 0.0 on epoch=374
03/01/2022 20:42:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.48 on epoch=379
03/01/2022 20:42:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.49 on epoch=384
03/01/2022 20:42:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=389
03/01/2022 20:42:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=394
03/01/2022 20:42:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=399
03/01/2022 20:42:51 - INFO - __main__ - Global step 800 Train loss 0.45 EM 0.0 on epoch=399
03/01/2022 20:42:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=404
03/01/2022 20:42:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=409
03/01/2022 20:42:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=414
03/01/2022 20:43:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=419
03/01/2022 20:43:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=424
03/01/2022 20:43:03 - INFO - __main__ - Global step 850 Train loss 0.43 EM 0.0 on epoch=424
03/01/2022 20:43:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=429
03/01/2022 20:43:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=434
03/01/2022 20:43:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=439
03/01/2022 20:43:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=444
03/01/2022 20:43:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=449
03/01/2022 20:43:15 - INFO - __main__ - Global step 900 Train loss 0.40 EM 0.0 on epoch=449
03/01/2022 20:43:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=454
03/01/2022 20:43:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=459
03/01/2022 20:43:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=464
03/01/2022 20:43:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=469
03/01/2022 20:43:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.31 on epoch=474
03/01/2022 20:43:27 - INFO - __main__ - Global step 950 Train loss 0.36 EM 0.0 on epoch=474
03/01/2022 20:43:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=479
03/01/2022 20:43:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=484
03/01/2022 20:43:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=489
03/01/2022 20:43:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=494
03/01/2022 20:43:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=499
03/01/2022 20:43:40 - INFO - __main__ - Global step 1000 Train loss 0.38 EM 0.0 on epoch=499
03/01/2022 20:43:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=504
03/01/2022 20:43:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=509
03/01/2022 20:43:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=514
03/01/2022 20:43:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=519
03/01/2022 20:43:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=524
03/01/2022 20:43:52 - INFO - __main__ - Global step 1050 Train loss 0.28 EM 0.0 on epoch=524
03/01/2022 20:43:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=529
03/01/2022 20:43:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=534
03/01/2022 20:43:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=539
03/01/2022 20:44:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=544
03/01/2022 20:44:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=549
03/01/2022 20:44:04 - INFO - __main__ - Global step 1100 Train loss 0.29 EM 0.0 on epoch=549
03/01/2022 20:44:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=554
03/01/2022 20:44:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.26 on epoch=559
03/01/2022 20:44:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=564
03/01/2022 20:44:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=569
03/01/2022 20:44:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=574
03/01/2022 20:44:17 - INFO - __main__ - Global step 1150 Train loss 0.25 EM 0.0 on epoch=574
03/01/2022 20:44:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=579
03/01/2022 20:44:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=584
03/01/2022 20:44:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.28 on epoch=589
03/01/2022 20:44:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=594
03/01/2022 20:44:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.26 on epoch=599
03/01/2022 20:44:29 - INFO - __main__ - Global step 1200 Train loss 0.23 EM 0.0 on epoch=599
03/01/2022 20:44:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=604
03/01/2022 20:44:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=609
03/01/2022 20:44:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=614
03/01/2022 20:44:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=619
03/01/2022 20:44:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=624
03/01/2022 20:44:42 - INFO - __main__ - Global step 1250 Train loss 0.24 EM 0.0 on epoch=624
03/01/2022 20:44:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=629
03/01/2022 20:44:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=634
03/01/2022 20:44:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=639
03/01/2022 20:44:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=644
03/01/2022 20:44:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=649
03/01/2022 20:44:54 - INFO - __main__ - Global step 1300 Train loss 0.20 EM 0.0 on epoch=649
03/01/2022 20:44:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=654
03/01/2022 20:44:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=659
03/01/2022 20:45:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=664
03/01/2022 20:45:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=669
03/01/2022 20:45:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=674
03/01/2022 20:45:07 - INFO - __main__ - Global step 1350 Train loss 0.19 EM 0.0 on epoch=674
03/01/2022 20:45:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=679
03/01/2022 20:45:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=684
03/01/2022 20:45:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=689
03/01/2022 20:45:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=694
03/01/2022 20:45:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=699
03/01/2022 20:45:19 - INFO - __main__ - Global step 1400 Train loss 0.19 EM 0.0 on epoch=699
03/01/2022 20:45:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=704
03/01/2022 20:45:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=709
03/01/2022 20:45:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=714
03/01/2022 20:45:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=719
03/01/2022 20:45:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=724
03/01/2022 20:45:32 - INFO - __main__ - Global step 1450 Train loss 0.16 EM 0.0 on epoch=724
03/01/2022 20:45:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=729
03/01/2022 20:45:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=734
03/01/2022 20:45:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=739
03/01/2022 20:45:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=744
03/01/2022 20:45:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.15 on epoch=749
03/01/2022 20:45:44 - INFO - __main__ - Global step 1500 Train loss 0.16 EM 0.0 on epoch=749
03/01/2022 20:45:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=754
03/01/2022 20:45:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=759
03/01/2022 20:45:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=764
03/01/2022 20:45:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=769
03/01/2022 20:45:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=774
03/01/2022 20:45:56 - INFO - __main__ - Global step 1550 Train loss 0.16 EM 0.0 on epoch=774
03/01/2022 20:45:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=779
03/01/2022 20:46:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=784
03/01/2022 20:46:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=789
03/01/2022 20:46:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=794
03/01/2022 20:46:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=799
03/01/2022 20:46:09 - INFO - __main__ - Global step 1600 Train loss 0.16 EM 0.0 on epoch=799
03/01/2022 20:46:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=804
03/01/2022 20:46:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=809
03/01/2022 20:46:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=814
03/01/2022 20:46:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=819
03/01/2022 20:46:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=824
03/01/2022 20:46:22 - INFO - __main__ - Global step 1650 Train loss 0.13 EM 0.0 on epoch=824
03/01/2022 20:46:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=829
03/01/2022 20:46:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=834
03/01/2022 20:46:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=839
03/01/2022 20:46:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=844
03/01/2022 20:46:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=849
03/01/2022 20:46:34 - INFO - __main__ - Global step 1700 Train loss 0.12 EM 0.0 on epoch=849
03/01/2022 20:46:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=854
03/01/2022 20:46:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=859
03/01/2022 20:46:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=864
03/01/2022 20:46:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=869
03/01/2022 20:46:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=874
03/01/2022 20:46:47 - INFO - __main__ - Global step 1750 Train loss 0.13 EM 0.0 on epoch=874
03/01/2022 20:46:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=879
03/01/2022 20:46:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=884
03/01/2022 20:46:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=889
03/01/2022 20:46:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=894
03/01/2022 20:46:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=899
03/01/2022 20:46:59 - INFO - __main__ - Global step 1800 Train loss 0.11 EM 0.0 on epoch=899
03/01/2022 20:47:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=904
03/01/2022 20:47:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=909
03/01/2022 20:47:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=914
03/01/2022 20:47:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=919
03/01/2022 20:47:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=924
03/01/2022 20:47:12 - INFO - __main__ - Global step 1850 Train loss 0.11 EM 0.0 on epoch=924
03/01/2022 20:47:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=929
03/01/2022 20:47:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=934
03/01/2022 20:47:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=939
03/01/2022 20:47:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=944
03/01/2022 20:47:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=949
03/01/2022 20:47:24 - INFO - __main__ - Global step 1900 Train loss 0.13 EM 0.0 on epoch=949
03/01/2022 20:47:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=954
03/01/2022 20:47:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=959
03/01/2022 20:47:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=964
03/01/2022 20:47:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=969
03/01/2022 20:47:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=974
03/01/2022 20:47:37 - INFO - __main__ - Global step 1950 Train loss 0.11 EM 0.0 on epoch=974
03/01/2022 20:47:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=979
03/01/2022 20:47:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=984
03/01/2022 20:47:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=989
03/01/2022 20:47:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=994
03/01/2022 20:47:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=999
03/01/2022 20:47:49 - INFO - __main__ - Global step 2000 Train loss 0.10 EM 0.0 on epoch=999
03/01/2022 20:47:49 - INFO - __main__ - save last model!
03/01/2022 20:47:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 20:47:49 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 20:47:49 - INFO - __main__ - Printing 3 examples
03/01/2022 20:47:49 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 20:47:49 - INFO - __main__ - ['taming of the shrew']
03/01/2022 20:47:49 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 20:47:49 - INFO - __main__ - ['henry fonda']
03/01/2022 20:47:49 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 20:47:49 - INFO - __main__ - ['tchaikovsky']
03/01/2022 20:47:49 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:47:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:47:49 - INFO - __main__ - Printing 3 examples
03/01/2022 20:47:49 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 20:47:49 - INFO - __main__ - ['ok computer']
03/01/2022 20:47:49 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 20:47:49 - INFO - __main__ - ['thursday']
03/01/2022 20:47:49 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 20:47:49 - INFO - __main__ - ['sigourney weaver']
03/01/2022 20:47:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 20:47:49 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:47:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:47:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:47:50 - INFO - __main__ - Printing 3 examples
03/01/2022 20:47:50 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 20:47:50 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 20:47:50 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 20:47:50 - INFO - __main__ - ['daphne du maurier']
03/01/2022 20:47:50 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 20:47:50 - INFO - __main__ - ['back to the future']
03/01/2022 20:47:50 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:47:50 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:47:50 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:47:51 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:47:55 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 20:48:01 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 20:48:01 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(6865, 154321), (4323, 177841), (3628, 221135), (1187, 785998), (3266, 325487), (2667, 390774), (3388, 312116), (1364, 727815), (571, 1622848), (2285, 389634), (4336, 223332), (5136, 204994), (709, 1264316), (749, 787508), (5658, 187484), (4163, 157427), (6244, 174311), (186, 4844172), (5598, 181777), (6613, 162836), (2419, 382488), (1473, 623867), (3131, 321577), (4112, 217671), (18, 50033297), (3866, 265039), (2641, 386604), (6625, 163763), (3318, 225546), (1759, 488564), (793, 959415), (6063, 179543), (3989, 260528), (740, 1183405), (5458, 178957), (6086, 171829), (3835, 184385), (3574, 184323), (5225, 169030), (231, 4022141), (2415, 239286), (5235, 199452), (391, 1765194), (4805, 242141), (2147, 250994), (2675, 364936), (89, 4855953), (3886, 262677), (766, 1186765), (3399, 271540), (2069, 384965), (1093, 832892), (7836, 160922), (2668, 283036), (1443, 666960), (530, 1727807), (4199, 265295), (6855, 171299), (2766, 278103), (776, 518408), (4999, 201462), (5382, 195164), (1669, 500832), (1281, 662981), (700, 1256519), (5242, 204577), (423, 2128181), (3834, 319344), (2066, 452267), (483, 1751244), (2118, 458638), (481, 1838055), (2262, 298634), (3476, 303459), (5835, 181763), (4568, 235140), (5164, 172215), (4917, 197668), (2992, 365452), (416, 2166899), (2027, 478074), (3511, 327136), (3946, 262708), (3616, 283413), (2983, 340155), (1306, 712364), (3450, 255182), (4858, 213667), (2883, 234030), (361, 212358), (4043, 242414), (3615, 277585), (1942, 489032), (5455, 188626), (5530, 188311), (4273, 255574), (6126, 180684), (2671, 348278), (931, 995491)]
03/01/2022 20:48:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:48:02 - INFO - __main__ - Starting training!
03/01/2022 20:50:41 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_87_0.5_8_predictions.txt
03/01/2022 20:50:41 - INFO - __main__ - EM on test data: 0.0085
03/01/2022 20:50:42 - INFO - __main__ - prefix=freebase_qa_32_87, lr=0.5, bsz=8, dev_performance=0.0, test_performance=0.008512769153730596
03/01/2022 20:50:42 - INFO - __main__ - Running ... prefix=freebase_qa_32_87, lr=0.4, bsz=8 ...
03/01/2022 20:50:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:50:43 - INFO - __main__ - Printing 3 examples
03/01/2022 20:50:43 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 20:50:43 - INFO - __main__ - ['ok computer']
03/01/2022 20:50:43 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 20:50:43 - INFO - __main__ - ['thursday']
03/01/2022 20:50:43 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 20:50:43 - INFO - __main__ - ['sigourney weaver']
03/01/2022 20:50:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 20:50:43 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:50:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:50:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:50:43 - INFO - __main__ - Printing 3 examples
03/01/2022 20:50:43 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 20:50:43 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 20:50:43 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 20:50:43 - INFO - __main__ - ['daphne du maurier']
03/01/2022 20:50:43 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 20:50:43 - INFO - __main__ - ['back to the future']
03/01/2022 20:50:43 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:50:43 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:50:43 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:50:55 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 20:50:55 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(6865, 154321), (4323, 177841), (3628, 221135), (1187, 785998), (3266, 325487), (2667, 390774), (3388, 312116), (1364, 727815), (571, 1622848), (2285, 389634), (4336, 223332), (5136, 204994), (709, 1264316), (749, 787508), (5658, 187484), (4163, 157427), (6244, 174311), (186, 4844172), (5598, 181777), (6613, 162836), (2419, 382488), (1473, 623867), (3131, 321577), (4112, 217671), (18, 50033297), (3866, 265039), (2641, 386604), (6625, 163763), (3318, 225546), (1759, 488564), (793, 959415), (6063, 179543), (3989, 260528), (740, 1183405), (5458, 178957), (6086, 171829), (3835, 184385), (3574, 184323), (5225, 169030), (231, 4022141), (2415, 239286), (5235, 199452), (391, 1765194), (4805, 242141), (2147, 250994), (2675, 364936), (89, 4855953), (3886, 262677), (766, 1186765), (3399, 271540), (2069, 384965), (1093, 832892), (7836, 160922), (2668, 283036), (1443, 666960), (530, 1727807), (4199, 265295), (6855, 171299), (2766, 278103), (776, 518408), (4999, 201462), (5382, 195164), (1669, 500832), (1281, 662981), (700, 1256519), (5242, 204577), (423, 2128181), (3834, 319344), (2066, 452267), (483, 1751244), (2118, 458638), (481, 1838055), (2262, 298634), (3476, 303459), (5835, 181763), (4568, 235140), (5164, 172215), (4917, 197668), (2992, 365452), (416, 2166899), (2027, 478074), (3511, 327136), (3946, 262708), (3616, 283413), (2983, 340155), (1306, 712364), (3450, 255182), (4858, 213667), (2883, 234030), (361, 212358), (4043, 242414), (3615, 277585), (1942, 489032), (5455, 188626), (5530, 188311), (4273, 255574), (6126, 180684), (2671, 348278), (931, 995491)]
03/01/2022 20:50:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:50:56 - INFO - __main__ - Starting training!
03/01/2022 20:50:59 - INFO - __main__ - Step 10 Global step 10 Train loss 4.85 on epoch=4
03/01/2022 20:51:01 - INFO - __main__ - Step 20 Global step 20 Train loss 4.38 on epoch=9
03/01/2022 20:51:03 - INFO - __main__ - Step 30 Global step 30 Train loss 3.91 on epoch=14
03/01/2022 20:51:05 - INFO - __main__ - Step 40 Global step 40 Train loss 3.56 on epoch=19
03/01/2022 20:51:07 - INFO - __main__ - Step 50 Global step 50 Train loss 3.24 on epoch=24
03/01/2022 20:51:09 - INFO - __main__ - Global step 50 Train loss 3.99 EM 0.0 on epoch=24
03/01/2022 20:51:09 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 20:51:11 - INFO - __main__ - Step 60 Global step 60 Train loss 3.07 on epoch=29
03/01/2022 20:51:14 - INFO - __main__ - Step 70 Global step 70 Train loss 2.94 on epoch=34
03/01/2022 20:51:16 - INFO - __main__ - Step 80 Global step 80 Train loss 2.79 on epoch=39
03/01/2022 20:51:18 - INFO - __main__ - Step 90 Global step 90 Train loss 2.69 on epoch=44
03/01/2022 20:51:20 - INFO - __main__ - Step 100 Global step 100 Train loss 2.60 on epoch=49
03/01/2022 20:51:22 - INFO - __main__ - Global step 100 Train loss 2.82 EM 0.0 on epoch=49
03/01/2022 20:51:24 - INFO - __main__ - Step 110 Global step 110 Train loss 2.52 on epoch=54
03/01/2022 20:51:26 - INFO - __main__ - Step 120 Global step 120 Train loss 2.37 on epoch=59
03/01/2022 20:51:28 - INFO - __main__ - Step 130 Global step 130 Train loss 2.28 on epoch=64
03/01/2022 20:51:31 - INFO - __main__ - Step 140 Global step 140 Train loss 2.22 on epoch=69
03/01/2022 20:51:33 - INFO - __main__ - Step 150 Global step 150 Train loss 2.13 on epoch=74
03/01/2022 20:51:35 - INFO - __main__ - Global step 150 Train loss 2.30 EM 0.0 on epoch=74
03/01/2022 20:51:37 - INFO - __main__ - Step 160 Global step 160 Train loss 1.98 on epoch=79
03/01/2022 20:51:40 - INFO - __main__ - Step 170 Global step 170 Train loss 1.86 on epoch=84
03/01/2022 20:51:42 - INFO - __main__ - Step 180 Global step 180 Train loss 1.86 on epoch=89
03/01/2022 20:51:44 - INFO - __main__ - Step 190 Global step 190 Train loss 1.75 on epoch=94
03/01/2022 20:51:46 - INFO - __main__ - Step 200 Global step 200 Train loss 1.72 on epoch=99
03/01/2022 20:51:48 - INFO - __main__ - Global step 200 Train loss 1.84 EM 0.0 on epoch=99
03/01/2022 20:51:50 - INFO - __main__ - Step 210 Global step 210 Train loss 1.65 on epoch=104
03/01/2022 20:51:52 - INFO - __main__ - Step 220 Global step 220 Train loss 1.50 on epoch=109
03/01/2022 20:51:54 - INFO - __main__ - Step 230 Global step 230 Train loss 1.46 on epoch=114
03/01/2022 20:51:57 - INFO - __main__ - Step 240 Global step 240 Train loss 1.47 on epoch=119
03/01/2022 20:51:59 - INFO - __main__ - Step 250 Global step 250 Train loss 1.34 on epoch=124
03/01/2022 20:52:00 - INFO - __main__ - Global step 250 Train loss 1.48 EM 0.0 on epoch=124
03/01/2022 20:52:02 - INFO - __main__ - Step 260 Global step 260 Train loss 1.34 on epoch=129
03/01/2022 20:52:05 - INFO - __main__ - Step 270 Global step 270 Train loss 1.32 on epoch=134
03/01/2022 20:52:07 - INFO - __main__ - Step 280 Global step 280 Train loss 1.24 on epoch=139
03/01/2022 20:52:09 - INFO - __main__ - Step 290 Global step 290 Train loss 1.16 on epoch=144
03/01/2022 20:52:11 - INFO - __main__ - Step 300 Global step 300 Train loss 1.18 on epoch=149
03/01/2022 20:52:13 - INFO - __main__ - Global step 300 Train loss 1.25 EM 0.0 on epoch=149
03/01/2022 20:52:15 - INFO - __main__ - Step 310 Global step 310 Train loss 1.17 on epoch=154
03/01/2022 20:52:17 - INFO - __main__ - Step 320 Global step 320 Train loss 1.12 on epoch=159
03/01/2022 20:52:19 - INFO - __main__ - Step 330 Global step 330 Train loss 1.12 on epoch=164
03/01/2022 20:52:21 - INFO - __main__ - Step 340 Global step 340 Train loss 1.11 on epoch=169
03/01/2022 20:52:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.98 on epoch=174
03/01/2022 20:52:25 - INFO - __main__ - Global step 350 Train loss 1.10 EM 0.0 on epoch=174
03/01/2022 20:52:27 - INFO - __main__ - Step 360 Global step 360 Train loss 1.03 on epoch=179
03/01/2022 20:52:29 - INFO - __main__ - Step 370 Global step 370 Train loss 1.04 on epoch=184
03/01/2022 20:52:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.99 on epoch=189
03/01/2022 20:52:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.92 on epoch=194
03/01/2022 20:52:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=199
03/01/2022 20:52:37 - INFO - __main__ - Global step 400 Train loss 0.97 EM 0.0 on epoch=199
03/01/2022 20:52:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.86 on epoch=204
03/01/2022 20:52:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.92 on epoch=209
03/01/2022 20:52:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.87 on epoch=214
03/01/2022 20:52:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.84 on epoch=219
03/01/2022 20:52:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.84 on epoch=224
03/01/2022 20:52:49 - INFO - __main__ - Global step 450 Train loss 0.86 EM 0.0 on epoch=224
03/01/2022 20:52:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.81 on epoch=229
03/01/2022 20:52:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.77 on epoch=234
03/01/2022 20:52:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.74 on epoch=239
03/01/2022 20:52:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.75 on epoch=244
03/01/2022 20:53:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.67 on epoch=249
03/01/2022 20:53:01 - INFO - __main__ - Global step 500 Train loss 0.75 EM 0.0 on epoch=249
03/01/2022 20:53:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.74 on epoch=254
03/01/2022 20:53:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.63 on epoch=259
03/01/2022 20:53:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.70 on epoch=264
03/01/2022 20:53:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.68 on epoch=269
03/01/2022 20:53:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.64 on epoch=274
03/01/2022 20:53:14 - INFO - __main__ - Global step 550 Train loss 0.68 EM 0.0 on epoch=274
03/01/2022 20:53:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.61 on epoch=279
03/01/2022 20:53:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=284
03/01/2022 20:53:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.60 on epoch=289
03/01/2022 20:53:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.60 on epoch=294
03/01/2022 20:53:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.66 on epoch=299
03/01/2022 20:53:26 - INFO - __main__ - Global step 600 Train loss 0.61 EM 0.0 on epoch=299
03/01/2022 20:53:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.68 on epoch=304
03/01/2022 20:53:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.50 on epoch=309
03/01/2022 20:53:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.58 on epoch=314
03/01/2022 20:53:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.62 on epoch=319
03/01/2022 20:53:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.57 on epoch=324
03/01/2022 20:53:38 - INFO - __main__ - Global step 650 Train loss 0.59 EM 0.0 on epoch=324
03/01/2022 20:53:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=329
03/01/2022 20:53:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=334
03/01/2022 20:53:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.55 on epoch=339
03/01/2022 20:53:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.53 on epoch=344
03/01/2022 20:53:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.57 on epoch=349
03/01/2022 20:53:50 - INFO - __main__ - Global step 700 Train loss 0.56 EM 0.0 on epoch=349
03/01/2022 20:53:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.57 on epoch=354
03/01/2022 20:53:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=359
03/01/2022 20:53:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=364
03/01/2022 20:53:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=369
03/01/2022 20:54:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=374
03/01/2022 20:54:02 - INFO - __main__ - Global step 750 Train loss 0.53 EM 0.0 on epoch=374
03/01/2022 20:54:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=379
03/01/2022 20:54:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=384
03/01/2022 20:54:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=389
03/01/2022 20:54:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=394
03/01/2022 20:54:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=399
03/01/2022 20:54:14 - INFO - __main__ - Global step 800 Train loss 0.45 EM 0.0 on epoch=399
03/01/2022 20:54:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=404
03/01/2022 20:54:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=409
03/01/2022 20:54:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=414
03/01/2022 20:54:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=419
03/01/2022 20:54:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=424
03/01/2022 20:54:27 - INFO - __main__ - Global step 850 Train loss 0.42 EM 0.0 on epoch=424
03/01/2022 20:54:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=429
03/01/2022 20:54:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=434
03/01/2022 20:54:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=439
03/01/2022 20:54:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
03/01/2022 20:54:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=449
03/01/2022 20:54:39 - INFO - __main__ - Global step 900 Train loss 0.38 EM 0.0 on epoch=449
03/01/2022 20:54:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=454
03/01/2022 20:54:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=459
03/01/2022 20:54:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=464
03/01/2022 20:54:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=469
03/01/2022 20:54:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=474
03/01/2022 20:54:52 - INFO - __main__ - Global step 950 Train loss 0.37 EM 0.0 on epoch=474
03/01/2022 20:54:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=479
03/01/2022 20:54:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=484
03/01/2022 20:54:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=489
03/01/2022 20:55:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=494
03/01/2022 20:55:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=499
03/01/2022 20:55:04 - INFO - __main__ - Global step 1000 Train loss 0.35 EM 0.0 on epoch=499
03/01/2022 20:55:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=504
03/01/2022 20:55:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=509
03/01/2022 20:55:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=514
03/01/2022 20:55:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.30 on epoch=519
03/01/2022 20:55:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=524
03/01/2022 20:55:16 - INFO - __main__ - Global step 1050 Train loss 0.30 EM 0.0 on epoch=524
03/01/2022 20:55:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=529
03/01/2022 20:55:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=534
03/01/2022 20:55:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=539
03/01/2022 20:55:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=544
03/01/2022 20:55:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=549
03/01/2022 20:55:29 - INFO - __main__ - Global step 1100 Train loss 0.33 EM 0.0 on epoch=549
03/01/2022 20:55:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.30 on epoch=554
03/01/2022 20:55:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=559
03/01/2022 20:55:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.27 on epoch=564
03/01/2022 20:55:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=569
03/01/2022 20:55:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=574
03/01/2022 20:55:41 - INFO - __main__ - Global step 1150 Train loss 0.28 EM 0.0 on epoch=574
03/01/2022 20:55:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=579
03/01/2022 20:55:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=584
03/01/2022 20:55:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=589
03/01/2022 20:55:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=594
03/01/2022 20:55:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.24 on epoch=599
03/01/2022 20:55:54 - INFO - __main__ - Global step 1200 Train loss 0.27 EM 0.0 on epoch=599
03/01/2022 20:55:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=604
03/01/2022 20:55:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=609
03/01/2022 20:56:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=614
03/01/2022 20:56:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=619
03/01/2022 20:56:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=624
03/01/2022 20:56:06 - INFO - __main__ - Global step 1250 Train loss 0.24 EM 0.0 on epoch=624
03/01/2022 20:56:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=629
03/01/2022 20:56:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=634
03/01/2022 20:56:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=639
03/01/2022 20:56:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=644
03/01/2022 20:56:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=649
03/01/2022 20:56:18 - INFO - __main__ - Global step 1300 Train loss 0.25 EM 0.0 on epoch=649
03/01/2022 20:56:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=654
03/01/2022 20:56:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=659
03/01/2022 20:56:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=664
03/01/2022 20:56:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=669
03/01/2022 20:56:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=674
03/01/2022 20:56:31 - INFO - __main__ - Global step 1350 Train loss 0.21 EM 0.0 on epoch=674
03/01/2022 20:56:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.24 on epoch=679
03/01/2022 20:56:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=684
03/01/2022 20:56:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=689
03/01/2022 20:56:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=694
03/01/2022 20:56:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.28 on epoch=699
03/01/2022 20:56:43 - INFO - __main__ - Global step 1400 Train loss 0.23 EM 0.0 on epoch=699
03/01/2022 20:56:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=704
03/01/2022 20:56:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=709
03/01/2022 20:56:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=714
03/01/2022 20:56:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=719
03/01/2022 20:56:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=724
03/01/2022 20:56:56 - INFO - __main__ - Global step 1450 Train loss 0.20 EM 0.0 on epoch=724
03/01/2022 20:56:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=729
03/01/2022 20:57:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=734
03/01/2022 20:57:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=739
03/01/2022 20:57:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.30 on epoch=744
03/01/2022 20:57:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=749
03/01/2022 20:57:08 - INFO - __main__ - Global step 1500 Train loss 0.26 EM 0.0 on epoch=749
03/01/2022 20:57:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=754
03/01/2022 20:57:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=759
03/01/2022 20:57:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=764
03/01/2022 20:57:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=769
03/01/2022 20:57:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=774
03/01/2022 20:57:21 - INFO - __main__ - Global step 1550 Train loss 0.29 EM 0.0 on epoch=774
03/01/2022 20:57:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=779
03/01/2022 20:57:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=784
03/01/2022 20:57:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=789
03/01/2022 20:57:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=794
03/01/2022 20:57:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=799
03/01/2022 20:57:33 - INFO - __main__ - Global step 1600 Train loss 0.24 EM 0.0 on epoch=799
03/01/2022 20:57:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=804
03/01/2022 20:57:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=809
03/01/2022 20:57:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=814
03/01/2022 20:57:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=819
03/01/2022 20:57:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=824
03/01/2022 20:57:46 - INFO - __main__ - Global step 1650 Train loss 0.19 EM 0.0 on epoch=824
03/01/2022 20:57:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=829
03/01/2022 20:57:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=834
03/01/2022 20:57:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=839
03/01/2022 20:57:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=844
03/01/2022 20:57:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=849
03/01/2022 20:57:59 - INFO - __main__ - Global step 1700 Train loss 0.18 EM 0.0 on epoch=849
03/01/2022 20:58:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=854
03/01/2022 20:58:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.20 on epoch=859
03/01/2022 20:58:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=864
03/01/2022 20:58:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=869
03/01/2022 20:58:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=874
03/01/2022 20:58:11 - INFO - __main__ - Global step 1750 Train loss 0.19 EM 0.0 on epoch=874
03/01/2022 20:58:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.17 on epoch=879
03/01/2022 20:58:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=884
03/01/2022 20:58:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.19 on epoch=889
03/01/2022 20:58:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=894
03/01/2022 20:58:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=899
03/01/2022 20:58:24 - INFO - __main__ - Global step 1800 Train loss 0.17 EM 0.0 on epoch=899
03/01/2022 20:58:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=904
03/01/2022 20:58:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=909
03/01/2022 20:58:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=914
03/01/2022 20:58:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=919
03/01/2022 20:58:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=924
03/01/2022 20:58:36 - INFO - __main__ - Global step 1850 Train loss 0.16 EM 0.0 on epoch=924
03/01/2022 20:58:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=929
03/01/2022 20:58:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=934
03/01/2022 20:58:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=939
03/01/2022 20:58:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=944
03/01/2022 20:58:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.15 on epoch=949
03/01/2022 20:58:49 - INFO - __main__ - Global step 1900 Train loss 0.15 EM 0.0 on epoch=949
03/01/2022 20:58:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.15 on epoch=954
03/01/2022 20:58:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=959
03/01/2022 20:58:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=964
03/01/2022 20:58:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=969
03/01/2022 20:59:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=974
03/01/2022 20:59:02 - INFO - __main__ - Global step 1950 Train loss 0.14 EM 0.0 on epoch=974
03/01/2022 20:59:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=979
03/01/2022 20:59:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.15 on epoch=984
03/01/2022 20:59:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=989
03/01/2022 20:59:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=994
03/01/2022 20:59:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=999
03/01/2022 20:59:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:59:14 - INFO - __main__ - Printing 3 examples
03/01/2022 20:59:14 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 20:59:14 - INFO - __main__ - ['ok computer']
03/01/2022 20:59:14 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 20:59:14 - INFO - __main__ - ['thursday']
03/01/2022 20:59:14 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 20:59:14 - INFO - __main__ - ['sigourney weaver']
03/01/2022 20:59:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 20:59:15 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:59:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:59:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:59:15 - INFO - __main__ - Printing 3 examples
03/01/2022 20:59:15 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 20:59:15 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 20:59:15 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 20:59:15 - INFO - __main__ - ['daphne du maurier']
03/01/2022 20:59:15 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 20:59:15 - INFO - __main__ - ['back to the future']
03/01/2022 20:59:15 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:59:15 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:59:15 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:59:15 - INFO - __main__ - Global step 2000 Train loss 0.14 EM 0.0 on epoch=999
03/01/2022 20:59:15 - INFO - __main__ - save last model!
03/01/2022 20:59:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 20:59:15 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 20:59:15 - INFO - __main__ - Printing 3 examples
03/01/2022 20:59:15 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 20:59:15 - INFO - __main__ - ['taming of the shrew']
03/01/2022 20:59:15 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 20:59:15 - INFO - __main__ - ['henry fonda']
03/01/2022 20:59:15 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 20:59:15 - INFO - __main__ - ['tchaikovsky']
03/01/2022 20:59:15 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:59:16 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:59:20 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 20:59:28 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 20:59:28 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(3350, 220687), (893, 1024942), (5008, 216986), (3569, 155945), (1895, 505183), (6448, 171477), (3241, 318391), (929, 292427), (1087, 843077), (697, 1020385), (3240, 286466), (2247, 316743), (3160, 306799), (5020, 210030), (4688, 155891), (4117, 219390), (1585, 544408), (4944, 210752), (4061, 266094), (4431, 209722), (482, 1812745), (3620, 265063), (5097, 214762), (2224, 338862), (569, 1402094), (783, 1075853), (4357, 231267), (3841, 258710), (2528, 182651), (1731, 565272), (283, 2553263), (1562, 628829), (6743, 154852), (4183, 213298), (405, 2285563), (1048, 702299), (748, 1191082), (2090, 392646), (4006, 256579), (3806, 259517), (4364, 244427), (4569, 164817), (376, 2422082), (5140, 155426), (1659, 586057), (4023, 208736), (3750, 253148), (3668, 251835), (821, 1056303), (5473, 163136), (1485, 648438), (5236, 177258), (3899, 255931), (4358, 235267), (3492, 299542), (3232, 334015), (2591, 370033), (1293, 773200), (306, 3068322), (1737, 490367), (5564, 187660), (5897, 180972), (1954, 242093), (3794, 166375), (1038, 749777), (2691, 354421), (2292, 355522), (3657, 209918), (2372, 427055), (3366, 327251), (5341, 190097), (2086, 457719), (1349, 721864), (1303, 658549), (845, 1086618), (4833, 207472), (1676, 524121), (6029, 210280), (942, 942590), (2366, 294304), (3598, 269003), (694, 1218279), (6109, 170031), (6098, 172422), (2985, 322411), (2846, 312704), (1230, 573495), (1549, 521331), (3860, 259925), (5853, 183928), (2935, 409303), (1310, 731782), (2098, 455459), (1679, 644199), (2605, 391810), (3637, 189415), (310, 2990534), (4197, 260190), (4416, 169042)]
03/01/2022 20:59:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:59:29 - INFO - __main__ - Starting training!
03/01/2022 21:02:17 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_87_0.4_8_predictions.txt
03/01/2022 21:02:17 - INFO - __main__ - EM on test data: 0.0070
03/01/2022 21:02:18 - INFO - __main__ - prefix=freebase_qa_32_87, lr=0.4, bsz=8, dev_performance=0.0, test_performance=0.007010515773660491
03/01/2022 21:02:18 - INFO - __main__ - Running ... prefix=freebase_qa_32_87, lr=0.3, bsz=8 ...
03/01/2022 21:02:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:02:19 - INFO - __main__ - Printing 3 examples
03/01/2022 21:02:19 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 21:02:19 - INFO - __main__ - ['ok computer']
03/01/2022 21:02:19 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 21:02:19 - INFO - __main__ - ['thursday']
03/01/2022 21:02:19 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 21:02:19 - INFO - __main__ - ['sigourney weaver']
03/01/2022 21:02:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 21:02:19 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:02:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:02:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:02:19 - INFO - __main__ - Printing 3 examples
03/01/2022 21:02:19 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 21:02:19 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 21:02:19 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 21:02:19 - INFO - __main__ - ['daphne du maurier']
03/01/2022 21:02:19 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 21:02:19 - INFO - __main__ - ['back to the future']
03/01/2022 21:02:19 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:02:19 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:02:19 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:02:31 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 21:02:31 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(3350, 220687), (893, 1024942), (5008, 216986), (3569, 155945), (1895, 505183), (6448, 171477), (3241, 318391), (929, 292427), (1087, 843077), (697, 1020385), (3240, 286466), (2247, 316743), (3160, 306799), (5020, 210030), (4688, 155891), (4117, 219390), (1585, 544408), (4944, 210752), (4061, 266094), (4431, 209722), (482, 1812745), (3620, 265063), (5097, 214762), (2224, 338862), (569, 1402094), (783, 1075853), (4357, 231267), (3841, 258710), (2528, 182651), (1731, 565272), (283, 2553263), (1562, 628829), (6743, 154852), (4183, 213298), (405, 2285563), (1048, 702299), (748, 1191082), (2090, 392646), (4006, 256579), (3806, 259517), (4364, 244427), (4569, 164817), (376, 2422082), (5140, 155426), (1659, 586057), (4023, 208736), (3750, 253148), (3668, 251835), (821, 1056303), (5473, 163136), (1485, 648438), (5236, 177258), (3899, 255931), (4358, 235267), (3492, 299542), (3232, 334015), (2591, 370033), (1293, 773200), (306, 3068322), (1737, 490367), (5564, 187660), (5897, 180972), (1954, 242093), (3794, 166375), (1038, 749777), (2691, 354421), (2292, 355522), (3657, 209918), (2372, 427055), (3366, 327251), (5341, 190097), (2086, 457719), (1349, 721864), (1303, 658549), (845, 1086618), (4833, 207472), (1676, 524121), (6029, 210280), (942, 942590), (2366, 294304), (3598, 269003), (694, 1218279), (6109, 170031), (6098, 172422), (2985, 322411), (2846, 312704), (1230, 573495), (1549, 521331), (3860, 259925), (5853, 183928), (2935, 409303), (1310, 731782), (2098, 455459), (1679, 644199), (2605, 391810), (3637, 189415), (310, 2990534), (4197, 260190), (4416, 169042)]
03/01/2022 21:02:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:02:32 - INFO - __main__ - Starting training!
03/01/2022 21:02:35 - INFO - __main__ - Step 10 Global step 10 Train loss 4.86 on epoch=4
03/01/2022 21:02:37 - INFO - __main__ - Step 20 Global step 20 Train loss 4.33 on epoch=9
03/01/2022 21:02:39 - INFO - __main__ - Step 30 Global step 30 Train loss 3.81 on epoch=14
03/01/2022 21:02:41 - INFO - __main__ - Step 40 Global step 40 Train loss 3.44 on epoch=19
03/01/2022 21:02:43 - INFO - __main__ - Step 50 Global step 50 Train loss 3.20 on epoch=24
03/01/2022 21:02:45 - INFO - __main__ - Global step 50 Train loss 3.93 EM 0.0 on epoch=24
03/01/2022 21:02:45 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 21:02:47 - INFO - __main__ - Step 60 Global step 60 Train loss 3.12 on epoch=29
03/01/2022 21:02:50 - INFO - __main__ - Step 70 Global step 70 Train loss 3.03 on epoch=34
03/01/2022 21:02:52 - INFO - __main__ - Step 80 Global step 80 Train loss 2.91 on epoch=39
03/01/2022 21:02:54 - INFO - __main__ - Step 90 Global step 90 Train loss 2.81 on epoch=44
03/01/2022 21:02:56 - INFO - __main__ - Step 100 Global step 100 Train loss 2.71 on epoch=49
03/01/2022 21:02:58 - INFO - __main__ - Global step 100 Train loss 2.92 EM 0.0 on epoch=49
03/01/2022 21:03:00 - INFO - __main__ - Step 110 Global step 110 Train loss 2.62 on epoch=54
03/01/2022 21:03:02 - INFO - __main__ - Step 120 Global step 120 Train loss 2.48 on epoch=59
03/01/2022 21:03:04 - INFO - __main__ - Step 130 Global step 130 Train loss 2.47 on epoch=64
03/01/2022 21:03:07 - INFO - __main__ - Step 140 Global step 140 Train loss 2.49 on epoch=69
03/01/2022 21:03:09 - INFO - __main__ - Step 150 Global step 150 Train loss 2.36 on epoch=74
03/01/2022 21:03:10 - INFO - __main__ - Global step 150 Train loss 2.48 EM 0.0 on epoch=74
03/01/2022 21:03:12 - INFO - __main__ - Step 160 Global step 160 Train loss 2.25 on epoch=79
03/01/2022 21:03:15 - INFO - __main__ - Step 170 Global step 170 Train loss 2.27 on epoch=84
03/01/2022 21:03:17 - INFO - __main__ - Step 180 Global step 180 Train loss 2.17 on epoch=89
03/01/2022 21:03:19 - INFO - __main__ - Step 190 Global step 190 Train loss 2.09 on epoch=94
03/01/2022 21:03:21 - INFO - __main__ - Step 200 Global step 200 Train loss 2.07 on epoch=99
03/01/2022 21:03:23 - INFO - __main__ - Global step 200 Train loss 2.17 EM 0.0 on epoch=99
03/01/2022 21:03:25 - INFO - __main__ - Step 210 Global step 210 Train loss 2.03 on epoch=104
03/01/2022 21:03:27 - INFO - __main__ - Step 220 Global step 220 Train loss 1.98 on epoch=109
03/01/2022 21:03:30 - INFO - __main__ - Step 230 Global step 230 Train loss 1.94 on epoch=114
03/01/2022 21:03:32 - INFO - __main__ - Step 240 Global step 240 Train loss 1.82 on epoch=119
03/01/2022 21:03:34 - INFO - __main__ - Step 250 Global step 250 Train loss 1.91 on epoch=124
03/01/2022 21:03:35 - INFO - __main__ - Global step 250 Train loss 1.94 EM 0.0 on epoch=124
03/01/2022 21:03:38 - INFO - __main__ - Step 260 Global step 260 Train loss 1.83 on epoch=129
03/01/2022 21:03:40 - INFO - __main__ - Step 270 Global step 270 Train loss 1.81 on epoch=134
03/01/2022 21:03:42 - INFO - __main__ - Step 280 Global step 280 Train loss 1.79 on epoch=139
03/01/2022 21:03:44 - INFO - __main__ - Step 290 Global step 290 Train loss 1.69 on epoch=144
03/01/2022 21:03:47 - INFO - __main__ - Step 300 Global step 300 Train loss 1.62 on epoch=149
03/01/2022 21:03:48 - INFO - __main__ - Global step 300 Train loss 1.75 EM 0.0 on epoch=149
03/01/2022 21:03:50 - INFO - __main__ - Step 310 Global step 310 Train loss 1.59 on epoch=154
03/01/2022 21:03:52 - INFO - __main__ - Step 320 Global step 320 Train loss 1.61 on epoch=159
03/01/2022 21:03:55 - INFO - __main__ - Step 330 Global step 330 Train loss 1.61 on epoch=164
03/01/2022 21:03:57 - INFO - __main__ - Step 340 Global step 340 Train loss 1.54 on epoch=169
03/01/2022 21:03:59 - INFO - __main__ - Step 350 Global step 350 Train loss 1.53 on epoch=174
03/01/2022 21:04:01 - INFO - __main__ - Global step 350 Train loss 1.58 EM 0.0 on epoch=174
03/01/2022 21:04:03 - INFO - __main__ - Step 360 Global step 360 Train loss 1.50 on epoch=179
03/01/2022 21:04:05 - INFO - __main__ - Step 370 Global step 370 Train loss 1.49 on epoch=184
03/01/2022 21:04:07 - INFO - __main__ - Step 380 Global step 380 Train loss 1.42 on epoch=189
03/01/2022 21:04:10 - INFO - __main__ - Step 390 Global step 390 Train loss 1.38 on epoch=194
03/01/2022 21:04:12 - INFO - __main__ - Step 400 Global step 400 Train loss 1.38 on epoch=199
03/01/2022 21:04:13 - INFO - __main__ - Global step 400 Train loss 1.44 EM 0.0 on epoch=199
03/01/2022 21:04:16 - INFO - __main__ - Step 410 Global step 410 Train loss 1.41 on epoch=204
03/01/2022 21:04:18 - INFO - __main__ - Step 420 Global step 420 Train loss 1.44 on epoch=209
03/01/2022 21:04:20 - INFO - __main__ - Step 430 Global step 430 Train loss 1.31 on epoch=214
03/01/2022 21:04:22 - INFO - __main__ - Step 440 Global step 440 Train loss 1.24 on epoch=219
03/01/2022 21:04:24 - INFO - __main__ - Step 450 Global step 450 Train loss 1.30 on epoch=224
03/01/2022 21:04:26 - INFO - __main__ - Global step 450 Train loss 1.34 EM 0.0 on epoch=224
03/01/2022 21:04:28 - INFO - __main__ - Step 460 Global step 460 Train loss 1.25 on epoch=229
03/01/2022 21:04:30 - INFO - __main__ - Step 470 Global step 470 Train loss 1.29 on epoch=234
03/01/2022 21:04:32 - INFO - __main__ - Step 480 Global step 480 Train loss 1.20 on epoch=239
03/01/2022 21:04:35 - INFO - __main__ - Step 490 Global step 490 Train loss 1.25 on epoch=244
03/01/2022 21:04:37 - INFO - __main__ - Step 500 Global step 500 Train loss 1.28 on epoch=249
03/01/2022 21:04:38 - INFO - __main__ - Global step 500 Train loss 1.25 EM 0.0 on epoch=249
03/01/2022 21:04:40 - INFO - __main__ - Step 510 Global step 510 Train loss 1.15 on epoch=254
03/01/2022 21:04:42 - INFO - __main__ - Step 520 Global step 520 Train loss 1.23 on epoch=259
03/01/2022 21:04:45 - INFO - __main__ - Step 530 Global step 530 Train loss 1.09 on epoch=264
03/01/2022 21:04:47 - INFO - __main__ - Step 540 Global step 540 Train loss 1.06 on epoch=269
03/01/2022 21:04:49 - INFO - __main__ - Step 550 Global step 550 Train loss 1.10 on epoch=274
03/01/2022 21:04:50 - INFO - __main__ - Global step 550 Train loss 1.12 EM 0.0 on epoch=274
03/01/2022 21:04:52 - INFO - __main__ - Step 560 Global step 560 Train loss 1.12 on epoch=279
03/01/2022 21:04:55 - INFO - __main__ - Step 570 Global step 570 Train loss 1.05 on epoch=284
03/01/2022 21:04:57 - INFO - __main__ - Step 580 Global step 580 Train loss 1.03 on epoch=289
03/01/2022 21:04:59 - INFO - __main__ - Step 590 Global step 590 Train loss 1.02 on epoch=294
03/01/2022 21:05:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.96 on epoch=299
03/01/2022 21:05:03 - INFO - __main__ - Global step 600 Train loss 1.04 EM 0.0 on epoch=299
03/01/2022 21:05:05 - INFO - __main__ - Step 610 Global step 610 Train loss 1.11 on epoch=304
03/01/2022 21:05:07 - INFO - __main__ - Step 620 Global step 620 Train loss 1.03 on epoch=309
03/01/2022 21:05:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.92 on epoch=314
03/01/2022 21:05:11 - INFO - __main__ - Step 640 Global step 640 Train loss 1.02 on epoch=319
03/01/2022 21:05:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.98 on epoch=324
03/01/2022 21:05:15 - INFO - __main__ - Global step 650 Train loss 1.01 EM 0.0 on epoch=324
03/01/2022 21:05:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.94 on epoch=329
03/01/2022 21:05:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.97 on epoch=334
03/01/2022 21:05:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.91 on epoch=339
03/01/2022 21:05:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.92 on epoch=344
03/01/2022 21:05:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.85 on epoch=349
03/01/2022 21:05:27 - INFO - __main__ - Global step 700 Train loss 0.92 EM 0.0 on epoch=349
03/01/2022 21:05:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.92 on epoch=354
03/01/2022 21:05:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.86 on epoch=359
03/01/2022 21:05:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.85 on epoch=364
03/01/2022 21:05:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.92 on epoch=369
03/01/2022 21:05:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.83 on epoch=374
03/01/2022 21:05:39 - INFO - __main__ - Global step 750 Train loss 0.88 EM 0.0 on epoch=374
03/01/2022 21:05:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.85 on epoch=379
03/01/2022 21:05:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.82 on epoch=384
03/01/2022 21:05:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.84 on epoch=389
03/01/2022 21:05:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.85 on epoch=394
03/01/2022 21:05:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.75 on epoch=399
03/01/2022 21:05:51 - INFO - __main__ - Global step 800 Train loss 0.82 EM 0.0 on epoch=399
03/01/2022 21:05:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.75 on epoch=404
03/01/2022 21:05:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.78 on epoch=409
03/01/2022 21:05:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.78 on epoch=414
03/01/2022 21:06:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.79 on epoch=419
03/01/2022 21:06:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.77 on epoch=424
03/01/2022 21:06:03 - INFO - __main__ - Global step 850 Train loss 0.77 EM 0.0 on epoch=424
03/01/2022 21:06:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.72 on epoch=429
03/01/2022 21:06:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.71 on epoch=434
03/01/2022 21:06:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.80 on epoch=439
03/01/2022 21:06:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.70 on epoch=444
03/01/2022 21:06:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.78 on epoch=449
03/01/2022 21:06:15 - INFO - __main__ - Global step 900 Train loss 0.74 EM 0.0 on epoch=449
03/01/2022 21:06:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.67 on epoch=454
03/01/2022 21:06:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.76 on epoch=459
03/01/2022 21:06:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.63 on epoch=464
03/01/2022 21:06:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.63 on epoch=469
03/01/2022 21:06:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.67 on epoch=474
03/01/2022 21:06:27 - INFO - __main__ - Global step 950 Train loss 0.67 EM 0.0 on epoch=474
03/01/2022 21:06:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.68 on epoch=479
03/01/2022 21:06:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.62 on epoch=484
03/01/2022 21:06:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.64 on epoch=489
03/01/2022 21:06:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.65 on epoch=494
03/01/2022 21:06:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.66 on epoch=499
03/01/2022 21:06:40 - INFO - __main__ - Global step 1000 Train loss 0.65 EM 0.0 on epoch=499
03/01/2022 21:06:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.64 on epoch=504
03/01/2022 21:06:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.59 on epoch=509
03/01/2022 21:06:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.56 on epoch=514
03/01/2022 21:06:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.59 on epoch=519
03/01/2022 21:06:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.60 on epoch=524
03/01/2022 21:06:52 - INFO - __main__ - Global step 1050 Train loss 0.60 EM 0.0 on epoch=524
03/01/2022 21:06:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.60 on epoch=529
03/01/2022 21:06:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.59 on epoch=534
03/01/2022 21:06:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.59 on epoch=539
03/01/2022 21:07:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.57 on epoch=544
03/01/2022 21:07:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.60 on epoch=549
03/01/2022 21:07:04 - INFO - __main__ - Global step 1100 Train loss 0.59 EM 0.0 on epoch=549
03/01/2022 21:07:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.57 on epoch=554
03/01/2022 21:07:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.61 on epoch=559
03/01/2022 21:07:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.53 on epoch=564
03/01/2022 21:07:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.59 on epoch=569
03/01/2022 21:07:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.55 on epoch=574
03/01/2022 21:07:16 - INFO - __main__ - Global step 1150 Train loss 0.57 EM 0.0 on epoch=574
03/01/2022 21:07:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=579
03/01/2022 21:07:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=584
03/01/2022 21:07:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.48 on epoch=589
03/01/2022 21:07:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=594
03/01/2022 21:07:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=599
03/01/2022 21:07:28 - INFO - __main__ - Global step 1200 Train loss 0.48 EM 0.0 on epoch=599
03/01/2022 21:07:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=604
03/01/2022 21:07:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=609
03/01/2022 21:07:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=614
03/01/2022 21:07:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=619
03/01/2022 21:07:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.49 on epoch=624
03/01/2022 21:07:40 - INFO - __main__ - Global step 1250 Train loss 0.47 EM 0.0 on epoch=624
03/01/2022 21:07:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=629
03/01/2022 21:07:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=634
03/01/2022 21:07:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=639
03/01/2022 21:07:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=644
03/01/2022 21:07:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.49 on epoch=649
03/01/2022 21:07:52 - INFO - __main__ - Global step 1300 Train loss 0.43 EM 0.0 on epoch=649
03/01/2022 21:07:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=654
03/01/2022 21:07:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=659
03/01/2022 21:07:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.50 on epoch=664
03/01/2022 21:08:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=669
03/01/2022 21:08:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=674
03/01/2022 21:08:04 - INFO - __main__ - Global step 1350 Train loss 0.41 EM 0.0 on epoch=674
03/01/2022 21:08:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=679
03/01/2022 21:08:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=684
03/01/2022 21:08:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=689
03/01/2022 21:08:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=694
03/01/2022 21:08:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=699
03/01/2022 21:08:16 - INFO - __main__ - Global step 1400 Train loss 0.38 EM 0.0 on epoch=699
03/01/2022 21:08:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=704
03/01/2022 21:08:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
03/01/2022 21:08:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=714
03/01/2022 21:08:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=719
03/01/2022 21:08:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=724
03/01/2022 21:08:28 - INFO - __main__ - Global step 1450 Train loss 0.39 EM 0.0 on epoch=724
03/01/2022 21:08:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=729
03/01/2022 21:08:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=734
03/01/2022 21:08:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=739
03/01/2022 21:08:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=744
03/01/2022 21:08:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.32 on epoch=749
03/01/2022 21:08:40 - INFO - __main__ - Global step 1500 Train loss 0.34 EM 0.0 on epoch=749
03/01/2022 21:08:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=754
03/01/2022 21:08:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=759
03/01/2022 21:08:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=764
03/01/2022 21:08:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=769
03/01/2022 21:08:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=774
03/01/2022 21:08:52 - INFO - __main__ - Global step 1550 Train loss 0.34 EM 0.0 on epoch=774
03/01/2022 21:08:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.27 on epoch=779
03/01/2022 21:08:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=784
03/01/2022 21:08:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.28 on epoch=789
03/01/2022 21:09:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=794
03/01/2022 21:09:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=799
03/01/2022 21:09:05 - INFO - __main__ - Global step 1600 Train loss 0.31 EM 0.0 on epoch=799
03/01/2022 21:09:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.28 on epoch=804
03/01/2022 21:09:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=809
03/01/2022 21:09:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=814
03/01/2022 21:09:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=819
03/01/2022 21:09:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=824
03/01/2022 21:09:17 - INFO - __main__ - Global step 1650 Train loss 0.27 EM 0.0 on epoch=824
03/01/2022 21:09:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=829
03/01/2022 21:09:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=834
03/01/2022 21:09:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.26 on epoch=839
03/01/2022 21:09:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=844
03/01/2022 21:09:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=849
03/01/2022 21:09:30 - INFO - __main__ - Global step 1700 Train loss 0.27 EM 0.0 on epoch=849
03/01/2022 21:09:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.27 on epoch=854
03/01/2022 21:09:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.28 on epoch=859
03/01/2022 21:09:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.28 on epoch=864
03/01/2022 21:09:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.27 on epoch=869
03/01/2022 21:09:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=874
03/01/2022 21:09:43 - INFO - __main__ - Global step 1750 Train loss 0.26 EM 0.0 on epoch=874
03/01/2022 21:09:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=879
03/01/2022 21:09:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.25 on epoch=884
03/01/2022 21:09:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=889
03/01/2022 21:09:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=894
03/01/2022 21:09:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.25 on epoch=899
03/01/2022 21:09:56 - INFO - __main__ - Global step 1800 Train loss 0.24 EM 0.0 on epoch=899
03/01/2022 21:09:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=904
03/01/2022 21:10:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.24 on epoch=909
03/01/2022 21:10:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=914
03/01/2022 21:10:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=919
03/01/2022 21:10:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=924
03/01/2022 21:10:08 - INFO - __main__ - Global step 1850 Train loss 0.22 EM 0.0 on epoch=924
03/01/2022 21:10:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=929
03/01/2022 21:10:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.25 on epoch=934
03/01/2022 21:10:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.21 on epoch=939
03/01/2022 21:10:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=944
03/01/2022 21:10:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=949
03/01/2022 21:10:21 - INFO - __main__ - Global step 1900 Train loss 0.23 EM 0.0 on epoch=949
03/01/2022 21:10:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=954
03/01/2022 21:10:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.23 on epoch=959
03/01/2022 21:10:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=964
03/01/2022 21:10:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=969
03/01/2022 21:10:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=974
03/01/2022 21:10:34 - INFO - __main__ - Global step 1950 Train loss 0.21 EM 0.0 on epoch=974
03/01/2022 21:10:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.24 on epoch=979
03/01/2022 21:10:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=984
03/01/2022 21:10:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=989
03/01/2022 21:10:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=994
03/01/2022 21:10:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=999
03/01/2022 21:10:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:10:46 - INFO - __main__ - Printing 3 examples
03/01/2022 21:10:46 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 21:10:46 - INFO - __main__ - ['ok computer']
03/01/2022 21:10:46 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 21:10:46 - INFO - __main__ - ['thursday']
03/01/2022 21:10:46 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 21:10:46 - INFO - __main__ - ['sigourney weaver']
03/01/2022 21:10:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 21:10:46 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:10:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:10:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:10:46 - INFO - __main__ - Printing 3 examples
03/01/2022 21:10:46 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 21:10:46 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 21:10:46 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 21:10:46 - INFO - __main__ - ['daphne du maurier']
03/01/2022 21:10:46 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 21:10:46 - INFO - __main__ - ['back to the future']
03/01/2022 21:10:46 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:10:46 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:10:46 - INFO - __main__ - Global step 2000 Train loss 0.20 EM 0.0 on epoch=999
03/01/2022 21:10:46 - INFO - __main__ - save last model!
03/01/2022 21:10:46 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:10:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 21:10:46 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 21:10:46 - INFO - __main__ - Printing 3 examples
03/01/2022 21:10:46 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 21:10:46 - INFO - __main__ - ['taming of the shrew']
03/01/2022 21:10:46 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 21:10:46 - INFO - __main__ - ['henry fonda']
03/01/2022 21:10:46 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 21:10:46 - INFO - __main__ - ['tchaikovsky']
03/01/2022 21:10:46 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:10:48 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:10:52 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 21:10:58 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 21:10:58 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(6261, 163483), (4632, 254637), (5099, 200518), (6349, 169524), (1329, 682367), (917, 952119), (772, 1209132), (2567, 304604), (4284, 235257), (3769, 195111), (74, 179054), (4879, 229125), (4142, 229917), (6316, 162881), (295, 1089836), (1232, 706951), (2628, 297222), (797, 1085506), (4949, 208735), (3291, 200786), (5623, 181105), (4050, 271976), (1438, 688034), (5810, 169570), (6476, 159449), (1937, 501940), (1253, 684766), (64, 215681), (5640, 186094), (617, 1442206), (1338, 712614), (5749, 180863), (1433, 586613), (2783, 308812), (1790, 550995), (883, 560479), (3516, 291319), (2213, 406980), (3854, 270252), (5624, 180780), (2128, 328887), (893, 1024942), (2724, 369166), (2096, 465293), (5642, 169703), (2851, 338896), (1051, 707945), (3345, 310573), (5726, 175237), (1020, 892057), (2121, 186176), (1583, 535374), (6537, 158593), (6440, 161231), (1722, 583639), (2088, 528184), (4540, 155419), (3665, 278672), (4772, 165920), (377, 1870362), (334, 2807606), (937, 1007700), (503, 1684486), (7444, 162492), (2377, 350752), (2191, 433246), (6516, 163489), (3318, 225546), (272, 2641523), (4633, 187899), (5782, 171468), (2931, 323269), (2917, 311362), (5191, 193394), (2767, 377111), (1536, 231332), (3234, 315011), (5256, 206528), (826, 1095264), (507, 1331640), (1486, 571578), (4696, 212488), (3943, 226298), (93, 192649), (5327, 201469), (2186, 447650), (3209, 276527), (6551, 154998), (1473, 623867), (3594, 300832), (3361, 294678), (1956, 270967), (4394, 234859), (1368, 677011), (3699, 253359), (5124, 181868), (3794, 166375), (1060, 823306), (3070, 294809)]
03/01/2022 21:10:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:10:59 - INFO - __main__ - Starting training!
03/01/2022 21:13:42 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_87_0.3_8_predictions.txt
03/01/2022 21:13:42 - INFO - __main__ - EM on test data: 0.0080
03/01/2022 21:13:43 - INFO - __main__ - prefix=freebase_qa_32_87, lr=0.3, bsz=8, dev_performance=0.0, test_performance=0.00801201802704056
03/01/2022 21:13:43 - INFO - __main__ - Running ... prefix=freebase_qa_32_87, lr=0.2, bsz=8 ...
03/01/2022 21:13:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:13:43 - INFO - __main__ - Printing 3 examples
03/01/2022 21:13:43 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 21:13:43 - INFO - __main__ - ['ok computer']
03/01/2022 21:13:43 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 21:13:43 - INFO - __main__ - ['thursday']
03/01/2022 21:13:43 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 21:13:43 - INFO - __main__ - ['sigourney weaver']
03/01/2022 21:13:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 21:13:43 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:13:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:13:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:13:43 - INFO - __main__ - Printing 3 examples
03/01/2022 21:13:43 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 21:13:43 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 21:13:43 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 21:13:43 - INFO - __main__ - ['daphne du maurier']
03/01/2022 21:13:43 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 21:13:43 - INFO - __main__ - ['back to the future']
03/01/2022 21:13:43 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:13:43 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:13:44 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:13:58 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 21:13:58 - INFO - __main__ - task name: freebase_qa
initialize from c4
[(6261, 163483), (4632, 254637), (5099, 200518), (6349, 169524), (1329, 682367), (917, 952119), (772, 1209132), (2567, 304604), (4284, 235257), (3769, 195111), (74, 179054), (4879, 229125), (4142, 229917), (6316, 162881), (295, 1089836), (1232, 706951), (2628, 297222), (797, 1085506), (4949, 208735), (3291, 200786), (5623, 181105), (4050, 271976), (1438, 688034), (5810, 169570), (6476, 159449), (1937, 501940), (1253, 684766), (64, 215681), (5640, 186094), (617, 1442206), (1338, 712614), (5749, 180863), (1433, 586613), (2783, 308812), (1790, 550995), (883, 560479), (3516, 291319), (2213, 406980), (3854, 270252), (5624, 180780), (2128, 328887), (893, 1024942), (2724, 369166), (2096, 465293), (5642, 169703), (2851, 338896), (1051, 707945), (3345, 310573), (5726, 175237), (1020, 892057), (2121, 186176), (1583, 535374), (6537, 158593), (6440, 161231), (1722, 583639), (2088, 528184), (4540, 155419), (3665, 278672), (4772, 165920), (377, 1870362), (334, 2807606), (937, 1007700), (503, 1684486), (7444, 162492), (2377, 350752), (2191, 433246), (6516, 163489), (3318, 225546), (272, 2641523), (4633, 187899), (5782, 171468), (2931, 323269), (2917, 311362), (5191, 193394), (2767, 377111), (1536, 231332), (3234, 315011), (5256, 206528), (826, 1095264), (507, 1331640), (1486, 571578), (4696, 212488), (3943, 226298), (93, 192649), (5327, 201469), (2186, 447650), (3209, 276527), (6551, 154998), (1473, 623867), (3594, 300832), (3361, 294678), (1956, 270967), (4394, 234859), (1368, 677011), (3699, 253359), (5124, 181868), (3794, 166375), (1060, 823306), (3070, 294809)]
03/01/2022 21:13:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:13:59 - INFO - __main__ - Starting training!
03/01/2022 21:14:01 - INFO - __main__ - Step 10 Global step 10 Train loss 4.84 on epoch=4
03/01/2022 21:14:03 - INFO - __main__ - Step 20 Global step 20 Train loss 4.41 on epoch=9
03/01/2022 21:14:05 - INFO - __main__ - Step 30 Global step 30 Train loss 4.11 on epoch=14
03/01/2022 21:14:08 - INFO - __main__ - Step 40 Global step 40 Train loss 3.81 on epoch=19
03/01/2022 21:14:10 - INFO - __main__ - Step 50 Global step 50 Train loss 3.56 on epoch=24
03/01/2022 21:14:14 - INFO - __main__ - Global step 50 Train loss 4.15 EM 0.0 on epoch=24
03/01/2022 21:14:14 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 21:14:16 - INFO - __main__ - Step 60 Global step 60 Train loss 3.39 on epoch=29
03/01/2022 21:14:18 - INFO - __main__ - Step 70 Global step 70 Train loss 3.34 on epoch=34
03/01/2022 21:14:20 - INFO - __main__ - Step 80 Global step 80 Train loss 3.32 on epoch=39
03/01/2022 21:14:23 - INFO - __main__ - Step 90 Global step 90 Train loss 3.23 on epoch=44
03/01/2022 21:14:25 - INFO - __main__ - Step 100 Global step 100 Train loss 3.08 on epoch=49
03/01/2022 21:14:26 - INFO - __main__ - Global step 100 Train loss 3.27 EM 0.0 on epoch=49
03/01/2022 21:14:28 - INFO - __main__ - Step 110 Global step 110 Train loss 2.92 on epoch=54
03/01/2022 21:14:31 - INFO - __main__ - Step 120 Global step 120 Train loss 2.91 on epoch=59
03/01/2022 21:14:33 - INFO - __main__ - Step 130 Global step 130 Train loss 2.83 on epoch=64
03/01/2022 21:14:35 - INFO - __main__ - Step 140 Global step 140 Train loss 2.85 on epoch=69
03/01/2022 21:14:37 - INFO - __main__ - Step 150 Global step 150 Train loss 2.74 on epoch=74
03/01/2022 21:14:39 - INFO - __main__ - Global step 150 Train loss 2.85 EM 0.0 on epoch=74
03/01/2022 21:14:41 - INFO - __main__ - Step 160 Global step 160 Train loss 2.58 on epoch=79
03/01/2022 21:14:43 - INFO - __main__ - Step 170 Global step 170 Train loss 2.62 on epoch=84
03/01/2022 21:14:45 - INFO - __main__ - Step 180 Global step 180 Train loss 2.51 on epoch=89
03/01/2022 21:14:47 - INFO - __main__ - Step 190 Global step 190 Train loss 2.51 on epoch=94
03/01/2022 21:14:49 - INFO - __main__ - Step 200 Global step 200 Train loss 2.48 on epoch=99
03/01/2022 21:14:51 - INFO - __main__ - Global step 200 Train loss 2.54 EM 0.0 on epoch=99
03/01/2022 21:14:53 - INFO - __main__ - Step 210 Global step 210 Train loss 2.46 on epoch=104
03/01/2022 21:14:55 - INFO - __main__ - Step 220 Global step 220 Train loss 2.38 on epoch=109
03/01/2022 21:14:57 - INFO - __main__ - Step 230 Global step 230 Train loss 2.29 on epoch=114
03/01/2022 21:15:00 - INFO - __main__ - Step 240 Global step 240 Train loss 2.20 on epoch=119
03/01/2022 21:15:02 - INFO - __main__ - Step 250 Global step 250 Train loss 2.28 on epoch=124
03/01/2022 21:15:03 - INFO - __main__ - Global step 250 Train loss 2.32 EM 0.0 on epoch=124
03/01/2022 21:15:05 - INFO - __main__ - Step 260 Global step 260 Train loss 2.20 on epoch=129
03/01/2022 21:15:08 - INFO - __main__ - Step 270 Global step 270 Train loss 2.11 on epoch=134
03/01/2022 21:15:10 - INFO - __main__ - Step 280 Global step 280 Train loss 2.19 on epoch=139
03/01/2022 21:15:12 - INFO - __main__ - Step 290 Global step 290 Train loss 2.12 on epoch=144
03/01/2022 21:15:14 - INFO - __main__ - Step 300 Global step 300 Train loss 2.01 on epoch=149
03/01/2022 21:15:17 - INFO - __main__ - Global step 300 Train loss 2.12 EM 0.0 on epoch=149
03/01/2022 21:15:19 - INFO - __main__ - Step 310 Global step 310 Train loss 2.05 on epoch=154
03/01/2022 21:15:21 - INFO - __main__ - Step 320 Global step 320 Train loss 1.90 on epoch=159
03/01/2022 21:15:23 - INFO - __main__ - Step 330 Global step 330 Train loss 1.88 on epoch=164
03/01/2022 21:15:25 - INFO - __main__ - Step 340 Global step 340 Train loss 1.90 on epoch=169
03/01/2022 21:15:28 - INFO - __main__ - Step 350 Global step 350 Train loss 1.87 on epoch=174
03/01/2022 21:15:30 - INFO - __main__ - Global step 350 Train loss 1.92 EM 0.0 on epoch=174
03/01/2022 21:15:32 - INFO - __main__ - Step 360 Global step 360 Train loss 1.99 on epoch=179
03/01/2022 21:15:35 - INFO - __main__ - Step 370 Global step 370 Train loss 1.84 on epoch=184
03/01/2022 21:15:37 - INFO - __main__ - Step 380 Global step 380 Train loss 1.80 on epoch=189
03/01/2022 21:15:39 - INFO - __main__ - Step 390 Global step 390 Train loss 1.71 on epoch=194
03/01/2022 21:15:41 - INFO - __main__ - Step 400 Global step 400 Train loss 1.69 on epoch=199
03/01/2022 21:15:44 - INFO - __main__ - Global step 400 Train loss 1.81 EM 0.0 on epoch=199
03/01/2022 21:15:46 - INFO - __main__ - Step 410 Global step 410 Train loss 1.66 on epoch=204
03/01/2022 21:15:48 - INFO - __main__ - Step 420 Global step 420 Train loss 1.64 on epoch=209
03/01/2022 21:15:50 - INFO - __main__ - Step 430 Global step 430 Train loss 1.60 on epoch=214
03/01/2022 21:15:53 - INFO - __main__ - Step 440 Global step 440 Train loss 1.63 on epoch=219
03/01/2022 21:15:55 - INFO - __main__ - Step 450 Global step 450 Train loss 1.52 on epoch=224
03/01/2022 21:15:56 - INFO - __main__ - Global step 450 Train loss 1.61 EM 0.0 on epoch=224
03/01/2022 21:15:58 - INFO - __main__ - Step 460 Global step 460 Train loss 1.52 on epoch=229
03/01/2022 21:16:00 - INFO - __main__ - Step 470 Global step 470 Train loss 1.58 on epoch=234
03/01/2022 21:16:02 - INFO - __main__ - Step 480 Global step 480 Train loss 1.49 on epoch=239
03/01/2022 21:16:05 - INFO - __main__ - Step 490 Global step 490 Train loss 1.56 on epoch=244
03/01/2022 21:16:07 - INFO - __main__ - Step 500 Global step 500 Train loss 1.49 on epoch=249
03/01/2022 21:16:08 - INFO - __main__ - Global step 500 Train loss 1.53 EM 0.0 on epoch=249
03/01/2022 21:16:10 - INFO - __main__ - Step 510 Global step 510 Train loss 1.58 on epoch=254
03/01/2022 21:16:12 - INFO - __main__ - Step 520 Global step 520 Train loss 1.46 on epoch=259
03/01/2022 21:16:15 - INFO - __main__ - Step 530 Global step 530 Train loss 1.36 on epoch=264
03/01/2022 21:16:17 - INFO - __main__ - Step 540 Global step 540 Train loss 1.43 on epoch=269
03/01/2022 21:16:19 - INFO - __main__ - Step 550 Global step 550 Train loss 1.40 on epoch=274
03/01/2022 21:16:21 - INFO - __main__ - Global step 550 Train loss 1.45 EM 0.0 on epoch=274
03/01/2022 21:16:23 - INFO - __main__ - Step 560 Global step 560 Train loss 1.44 on epoch=279
03/01/2022 21:16:25 - INFO - __main__ - Step 570 Global step 570 Train loss 1.35 on epoch=284
03/01/2022 21:16:27 - INFO - __main__ - Step 580 Global step 580 Train loss 1.36 on epoch=289
03/01/2022 21:16:29 - INFO - __main__ - Step 590 Global step 590 Train loss 1.33 on epoch=294
03/01/2022 21:16:32 - INFO - __main__ - Step 600 Global step 600 Train loss 1.35 on epoch=299
03/01/2022 21:16:33 - INFO - __main__ - Global step 600 Train loss 1.37 EM 0.0 on epoch=299
03/01/2022 21:16:35 - INFO - __main__ - Step 610 Global step 610 Train loss 1.34 on epoch=304
03/01/2022 21:16:38 - INFO - __main__ - Step 620 Global step 620 Train loss 1.29 on epoch=309
03/01/2022 21:16:40 - INFO - __main__ - Step 630 Global step 630 Train loss 1.26 on epoch=314
03/01/2022 21:16:42 - INFO - __main__ - Step 640 Global step 640 Train loss 1.22 on epoch=319
03/01/2022 21:16:44 - INFO - __main__ - Step 650 Global step 650 Train loss 1.20 on epoch=324
03/01/2022 21:16:45 - INFO - __main__ - Global step 650 Train loss 1.26 EM 0.0 on epoch=324
03/01/2022 21:16:48 - INFO - __main__ - Step 660 Global step 660 Train loss 1.26 on epoch=329
03/01/2022 21:16:50 - INFO - __main__ - Step 670 Global step 670 Train loss 1.17 on epoch=334
03/01/2022 21:16:52 - INFO - __main__ - Step 680 Global step 680 Train loss 1.14 on epoch=339
03/01/2022 21:16:54 - INFO - __main__ - Step 690 Global step 690 Train loss 1.19 on epoch=344
03/01/2022 21:16:57 - INFO - __main__ - Step 700 Global step 700 Train loss 1.12 on epoch=349
03/01/2022 21:16:58 - INFO - __main__ - Global step 700 Train loss 1.18 EM 0.0 on epoch=349
03/01/2022 21:17:00 - INFO - __main__ - Step 710 Global step 710 Train loss 1.23 on epoch=354
03/01/2022 21:17:02 - INFO - __main__ - Step 720 Global step 720 Train loss 1.11 on epoch=359
03/01/2022 21:17:04 - INFO - __main__ - Step 730 Global step 730 Train loss 1.05 on epoch=364
03/01/2022 21:17:07 - INFO - __main__ - Step 740 Global step 740 Train loss 1.14 on epoch=369
03/01/2022 21:17:09 - INFO - __main__ - Step 750 Global step 750 Train loss 1.13 on epoch=374
03/01/2022 21:17:10 - INFO - __main__ - Global step 750 Train loss 1.13 EM 0.0 on epoch=374
03/01/2022 21:17:12 - INFO - __main__ - Step 760 Global step 760 Train loss 1.07 on epoch=379
03/01/2022 21:17:14 - INFO - __main__ - Step 770 Global step 770 Train loss 1.08 on epoch=384
03/01/2022 21:17:17 - INFO - __main__ - Step 780 Global step 780 Train loss 1.07 on epoch=389
03/01/2022 21:17:19 - INFO - __main__ - Step 790 Global step 790 Train loss 1.13 on epoch=394
03/01/2022 21:17:21 - INFO - __main__ - Step 800 Global step 800 Train loss 1.11 on epoch=399
03/01/2022 21:17:22 - INFO - __main__ - Global step 800 Train loss 1.09 EM 0.0 on epoch=399
03/01/2022 21:17:24 - INFO - __main__ - Step 810 Global step 810 Train loss 1.03 on epoch=404
03/01/2022 21:17:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.93 on epoch=409
03/01/2022 21:17:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.89 on epoch=414
03/01/2022 21:17:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.92 on epoch=419
03/01/2022 21:17:33 - INFO - __main__ - Step 850 Global step 850 Train loss 1.00 on epoch=424
03/01/2022 21:17:35 - INFO - __main__ - Global step 850 Train loss 0.96 EM 0.0 on epoch=424
03/01/2022 21:17:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.94 on epoch=429
03/01/2022 21:17:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.97 on epoch=434
03/01/2022 21:17:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=439
03/01/2022 21:17:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.98 on epoch=444
03/01/2022 21:17:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.90 on epoch=449
03/01/2022 21:17:48 - INFO - __main__ - Global step 900 Train loss 0.92 EM 0.0 on epoch=449
03/01/2022 21:17:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.91 on epoch=454
03/01/2022 21:17:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.90 on epoch=459
03/01/2022 21:17:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.91 on epoch=464
03/01/2022 21:17:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.85 on epoch=469
03/01/2022 21:17:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.86 on epoch=474
03/01/2022 21:18:00 - INFO - __main__ - Global step 950 Train loss 0.89 EM 0.0 on epoch=474
03/01/2022 21:18:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.78 on epoch=479
03/01/2022 21:18:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.86 on epoch=484
03/01/2022 21:18:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.89 on epoch=489
03/01/2022 21:18:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.78 on epoch=494
03/01/2022 21:18:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.80 on epoch=499
03/01/2022 21:18:13 - INFO - __main__ - Global step 1000 Train loss 0.82 EM 0.0 on epoch=499
03/01/2022 21:18:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.79 on epoch=504
03/01/2022 21:18:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.81 on epoch=509
03/01/2022 21:18:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.78 on epoch=514
03/01/2022 21:18:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.76 on epoch=519
03/01/2022 21:18:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.73 on epoch=524
03/01/2022 21:18:26 - INFO - __main__ - Global step 1050 Train loss 0.78 EM 0.0 on epoch=524
03/01/2022 21:18:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.83 on epoch=529
03/01/2022 21:18:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.72 on epoch=534
03/01/2022 21:18:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.70 on epoch=539
03/01/2022 21:18:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.73 on epoch=544
03/01/2022 21:18:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.75 on epoch=549
03/01/2022 21:18:38 - INFO - __main__ - Global step 1100 Train loss 0.75 EM 0.0 on epoch=549
03/01/2022 21:18:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.75 on epoch=554
03/01/2022 21:18:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.75 on epoch=559
03/01/2022 21:18:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.77 on epoch=564
03/01/2022 21:18:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.69 on epoch=569
03/01/2022 21:18:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.74 on epoch=574
03/01/2022 21:18:51 - INFO - __main__ - Global step 1150 Train loss 0.74 EM 0.0 on epoch=574
03/01/2022 21:18:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.70 on epoch=579
03/01/2022 21:18:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.65 on epoch=584
03/01/2022 21:18:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.68 on epoch=589
03/01/2022 21:19:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.66 on epoch=594
03/01/2022 21:19:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.69 on epoch=599
03/01/2022 21:19:04 - INFO - __main__ - Global step 1200 Train loss 0.68 EM 0.0 on epoch=599
03/01/2022 21:19:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.57 on epoch=604
03/01/2022 21:19:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.68 on epoch=609
03/01/2022 21:19:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.65 on epoch=614
03/01/2022 21:19:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.62 on epoch=619
03/01/2022 21:19:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.66 on epoch=624
03/01/2022 21:19:16 - INFO - __main__ - Global step 1250 Train loss 0.64 EM 0.0 on epoch=624
03/01/2022 21:19:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=629
03/01/2022 21:19:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.67 on epoch=634
03/01/2022 21:19:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.57 on epoch=639
03/01/2022 21:19:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.60 on epoch=644
03/01/2022 21:19:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.66 on epoch=649
03/01/2022 21:19:29 - INFO - __main__ - Global step 1300 Train loss 0.61 EM 0.0 on epoch=649
03/01/2022 21:19:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.62 on epoch=654
03/01/2022 21:19:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.60 on epoch=659
03/01/2022 21:19:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.57 on epoch=664
03/01/2022 21:19:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.61 on epoch=669
03/01/2022 21:19:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.56 on epoch=674
03/01/2022 21:19:42 - INFO - __main__ - Global step 1350 Train loss 0.59 EM 0.0 on epoch=674
03/01/2022 21:19:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.61 on epoch=679
03/01/2022 21:19:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.57 on epoch=684
03/01/2022 21:19:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.56 on epoch=689
03/01/2022 21:19:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.60 on epoch=694
03/01/2022 21:19:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.56 on epoch=699
03/01/2022 21:19:54 - INFO - __main__ - Global step 1400 Train loss 0.58 EM 0.0 on epoch=699
03/01/2022 21:19:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.49 on epoch=704
03/01/2022 21:19:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.57 on epoch=709
03/01/2022 21:20:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.54 on epoch=714
03/01/2022 21:20:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.57 on epoch=719
03/01/2022 21:20:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.50 on epoch=724
03/01/2022 21:20:07 - INFO - __main__ - Global step 1450 Train loss 0.53 EM 0.0 on epoch=724
03/01/2022 21:20:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.58 on epoch=729
03/01/2022 21:20:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.55 on epoch=734
03/01/2022 21:20:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.62 on epoch=739
03/01/2022 21:20:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.50 on epoch=744
03/01/2022 21:20:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.58 on epoch=749
03/01/2022 21:20:19 - INFO - __main__ - Global step 1500 Train loss 0.57 EM 0.0 on epoch=749
03/01/2022 21:20:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.54 on epoch=754
03/01/2022 21:20:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.53 on epoch=759
03/01/2022 21:20:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.56 on epoch=764
03/01/2022 21:20:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.61 on epoch=769
03/01/2022 21:20:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.56 on epoch=774
03/01/2022 21:20:32 - INFO - __main__ - Global step 1550 Train loss 0.56 EM 0.0 on epoch=774
03/01/2022 21:20:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=779
03/01/2022 21:20:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.53 on epoch=784
03/01/2022 21:20:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.48 on epoch=789
03/01/2022 21:20:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.56 on epoch=794
03/01/2022 21:20:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.50 on epoch=799
03/01/2022 21:20:45 - INFO - __main__ - Global step 1600 Train loss 0.52 EM 0.0 on epoch=799
03/01/2022 21:20:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.52 on epoch=804
03/01/2022 21:20:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.56 on epoch=809
03/01/2022 21:20:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.56 on epoch=814
03/01/2022 21:20:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
03/01/2022 21:20:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.54 on epoch=824
03/01/2022 21:20:57 - INFO - __main__ - Global step 1650 Train loss 0.52 EM 0.0 on epoch=824
03/01/2022 21:20:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=829
03/01/2022 21:21:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.48 on epoch=834
03/01/2022 21:21:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.50 on epoch=839
03/01/2022 21:21:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=844
03/01/2022 21:21:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=849
03/01/2022 21:21:10 - INFO - __main__ - Global step 1700 Train loss 0.46 EM 0.0 on epoch=849
03/01/2022 21:21:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.44 on epoch=854
03/01/2022 21:21:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=859
03/01/2022 21:21:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.48 on epoch=864
03/01/2022 21:21:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=869
03/01/2022 21:21:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.50 on epoch=874
03/01/2022 21:21:23 - INFO - __main__ - Global step 1750 Train loss 0.46 EM 0.0 on epoch=874
03/01/2022 21:21:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.62 on epoch=879
03/01/2022 21:21:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.61 on epoch=884
03/01/2022 21:21:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.53 on epoch=889
03/01/2022 21:21:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.56 on epoch=894
03/01/2022 21:21:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.51 on epoch=899
03/01/2022 21:21:35 - INFO - __main__ - Global step 1800 Train loss 0.56 EM 0.0 on epoch=899
03/01/2022 21:21:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.55 on epoch=904
03/01/2022 21:21:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.53 on epoch=909
03/01/2022 21:21:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=914
03/01/2022 21:21:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.53 on epoch=919
03/01/2022 21:21:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.50 on epoch=924
03/01/2022 21:21:48 - INFO - __main__ - Global step 1850 Train loss 0.50 EM 0.0 on epoch=924
03/01/2022 21:21:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
03/01/2022 21:21:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=934
03/01/2022 21:21:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.50 on epoch=939
03/01/2022 21:21:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=944
03/01/2022 21:21:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=949
03/01/2022 21:22:00 - INFO - __main__ - Global step 1900 Train loss 0.45 EM 0.0 on epoch=949
03/01/2022 21:22:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=954
03/01/2022 21:22:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=959
03/01/2022 21:22:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.44 on epoch=964
03/01/2022 21:22:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=969
03/01/2022 21:22:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.48 on epoch=974
03/01/2022 21:22:13 - INFO - __main__ - Global step 1950 Train loss 0.44 EM 0.0 on epoch=974
03/01/2022 21:22:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.39 on epoch=979
03/01/2022 21:22:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
03/01/2022 21:22:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=989
03/01/2022 21:22:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=994
03/01/2022 21:22:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.47 on epoch=999
03/01/2022 21:22:26 - INFO - __main__ - Global step 2000 Train loss 0.41 EM 0.0 on epoch=999
03/01/2022 21:22:26 - INFO - __main__ - save last model!
03/01/2022 21:22:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 21:22:26 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 21:22:26 - INFO - __main__ - Printing 3 examples
03/01/2022 21:22:26 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 21:22:26 - INFO - __main__ - ['taming of the shrew']
03/01/2022 21:22:26 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 21:22:26 - INFO - __main__ - ['henry fonda']
03/01/2022 21:22:26 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 21:22:26 - INFO - __main__ - ['tchaikovsky']
03/01/2022 21:22:26 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:22:27 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:22:31 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 21:25:12 - INFO - __main__ - Saved prediction in models/T5-large/singletask-freebase_qa/freebase_qa_32_87_0.2_8_predictions.txt
03/01/2022 21:25:12 - INFO - __main__ - EM on test data: 0.0130
03/01/2022 21:25:12 - INFO - __main__ - prefix=freebase_qa_32_87, lr=0.2, bsz=8, dev_performance=0.0, test_performance=0.013019529293940912
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00033593177795410156 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "2371", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 13639, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "2372", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 13639, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 13639, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (2495): No such process
Task: glue-qnli, Checkpoint: None, Identifier: T5-large
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_hps_singletask_ddp_prompt.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29544
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_zayrcugy/none_ndmdnnz7
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29544
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zayrcugy/none_ndmdnnz7/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zayrcugy/none_ndmdnnz7/attempt_0/1/error.json
03/01/2022 21:25:17 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 21:25:17 - INFO - __main__ - models/T5-large/singletask-glue-qnli
Output directory () already exists and is not empty.
03/01/2022 21:25:17 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 21:25:17 - INFO - __main__ - models/T5-large/singletask-glue-qnli
03/01/2022 21:25:19 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 21:25:19 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 21:25:19 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/01/2022 21:25:19 - INFO - __main__ - args.device: cuda:0
03/01/2022 21:25:19 - INFO - __main__ - Using 2 gpus
03/01/2022 21:25:19 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/01/2022 21:25:19 - INFO - __main__ - args.device: cuda:1
03/01/2022 21:25:19 - INFO - __main__ - Using 2 gpus
03/01/2022 21:25:19 - INFO - __main__ - Fine-tuning the following samples: ['glue-qnli_16_100', 'glue-qnli_16_13', 'glue-qnli_16_21', 'glue-qnli_16_42', 'glue-qnli_16_87']
03/01/2022 21:25:19 - INFO - __main__ - Fine-tuning the following samples: ['glue-qnli_16_100', 'glue-qnli_16_13', 'glue-qnli_16_21', 'glue-qnli_16_42', 'glue-qnli_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/01/2022 21:25:25 - INFO - __main__ - Running ... prefix=glue-qnli_16_100, lr=0.5, bsz=8 ...
03/01/2022 21:25:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:25:26 - INFO - __main__ - Printing 3 examples
03/01/2022 21:25:26 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 21:25:26 - INFO - __main__ - ['entailment']
03/01/2022 21:25:26 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 21:25:26 - INFO - __main__ - ['entailment']
03/01/2022 21:25:26 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 21:25:26 - INFO - __main__ - ['entailment']
03/01/2022 21:25:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 21:25:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:25:26 - INFO - __main__ - Printing 3 examples
03/01/2022 21:25:26 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 21:25:26 - INFO - __main__ - ['entailment']
03/01/2022 21:25:26 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 21:25:26 - INFO - __main__ - ['entailment']
03/01/2022 21:25:26 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 21:25:26 - INFO - __main__ - ['entailment']
03/01/2022 21:25:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 21:25:26 - INFO - __main__ - Tokenizing Output ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 21:25:26 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:25:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:25:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:25:26 - INFO - __main__ - Printing 3 examples
03/01/2022 21:25:26 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 21:25:26 - INFO - __main__ - ['entailment']
03/01/2022 21:25:26 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 21:25:26 - INFO - __main__ - ['entailment']
03/01/2022 21:25:26 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 21:25:26 - INFO - __main__ - ['entailment']
03/01/2022 21:25:26 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:25:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:25:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:25:26 - INFO - __main__ - Printing 3 examples
03/01/2022 21:25:26 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 21:25:26 - INFO - __main__ - ['entailment']
03/01/2022 21:25:26 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 21:25:26 - INFO - __main__ - ['entailment']
03/01/2022 21:25:26 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 21:25:26 - INFO - __main__ - ['entailment']
03/01/2022 21:25:26 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:25:26 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:25:26 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:25:26 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:25:26 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:25:41 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 21:25:41 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446), (3702, 272314), (6894, 155616), (3001, 351252), (499, 1766405), (3731, 203441), (6478, 174299), (1287, 709613), (4382, 247856), (851, 1026262), (5946, 169484), (1409, 319874), (3527, 259912), (6373, 162161), (1640, 475640), (801, 1149556), (357, 1674058), (2402, 404464), (2536, 323855), (887, 1021213), (2386, 396013), (1103, 849279), (4237, 246742), (3032, 335423), (2922, 206677), (3889, 257596), (1683, 558497), (4149, 253554), (3634, 263727), (1955, 437347), (2891, 347678), (770, 1128621), (6298, 154205), (1904, 528496), (5956, 175083), (2943, 377167), (1220, 555589), (5103, 202299), (4313, 246924), (2734, 344156), (5735, 168028), (2404, 417090), (3626, 290198), (605, 1425064), (2324, 403572), (235, 2453612), (2733, 298680), (4166, 233368), (2422, 345969), (676, 1204693), (2272, 432795), (4569, 164817), (3285, 299538), (2136, 430918), (5166, 187635), (2040, 237038), (5037, 203882), (1556, 625505), (2836, 350538), (2079, 374647)]
03/01/2022 21:25:41 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 21:25:41 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446), (3702, 272314), (6894, 155616), (3001, 351252), (499, 1766405), (3731, 203441), (6478, 174299), (1287, 709613), (4382, 247856), (851, 1026262), (5946, 169484), (1409, 319874), (3527, 259912), (6373, 162161), (1640, 475640), (801, 1149556), (357, 1674058), (2402, 404464), (2536, 323855), (887, 1021213), (2386, 396013), (1103, 849279), (4237, 246742), (3032, 335423), (2922, 206677), (3889, 257596), (1683, 558497), (4149, 253554), (3634, 263727), (1955, 437347), (2891, 347678), (770, 1128621), (6298, 154205), (1904, 528496), (5956, 175083), (2943, 377167), (1220, 555589), (5103, 202299), (4313, 246924), (2734, 344156), (5735, 168028), (2404, 417090), (3626, 290198), (605, 1425064), (2324, 403572), (235, 2453612), (2733, 298680), (4166, 233368), (2422, 345969), (676, 1204693), (2272, 432795), (4569, 164817), (3285, 299538), (2136, 430918), (5166, 187635), (2040, 237038), (5037, 203882), (1556, 625505), (2836, 350538), (2079, 374647)]
03/01/2022 21:25:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:25:42 - INFO - __main__ - Starting training!
03/01/2022 21:25:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:25:42 - INFO - __main__ - Starting training!
03/01/2022 21:25:45 - INFO - __main__ - Step 10 Global step 10 Train loss 4.25 on epoch=4
03/01/2022 21:25:47 - INFO - __main__ - Step 20 Global step 20 Train loss 1.15 on epoch=9
03/01/2022 21:25:49 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=14
03/01/2022 21:25:51 - INFO - __main__ - Step 40 Global step 40 Train loss 0.34 on epoch=19
03/01/2022 21:25:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.25 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/01/2022 21:25:55 - INFO - __main__ - Global step 50 Train loss 1.30 ACC 0.59375 on epoch=24
03/01/2022 21:25:55 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.59375 on epoch=24, global_step=50
03/01/2022 21:25:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.26 on epoch=29
03/01/2022 21:25:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.23 on epoch=34
03/01/2022 21:26:01 - INFO - __main__ - Step 80 Global step 80 Train loss 0.21 on epoch=39
03/01/2022 21:26:03 - INFO - __main__ - Step 90 Global step 90 Train loss 0.17 on epoch=44
03/01/2022 21:26:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.20 on epoch=49
03/01/2022 21:26:07 - INFO - __main__ - Global step 100 Train loss 0.21 ACC 0.5 on epoch=49
03/01/2022 21:26:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.19 on epoch=54
03/01/2022 21:26:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.17 on epoch=59
03/01/2022 21:26:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.17 on epoch=64
03/01/2022 21:26:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.15 on epoch=69
03/01/2022 21:26:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.16 on epoch=74
03/01/2022 21:26:19 - INFO - __main__ - Global step 150 Train loss 0.17 ACC 0.5 on epoch=74
03/01/2022 21:26:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.15 on epoch=79
03/01/2022 21:26:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.15 on epoch=84
03/01/2022 21:26:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.14 on epoch=89
03/01/2022 21:26:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.15 on epoch=94
03/01/2022 21:26:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.14 on epoch=99
03/01/2022 21:26:31 - INFO - __main__ - Global step 200 Train loss 0.15 ACC 0.53125 on epoch=99
03/01/2022 21:26:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.13 on epoch=104
03/01/2022 21:26:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.17 on epoch=109
03/01/2022 21:26:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.15 on epoch=114
03/01/2022 21:26:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.15 on epoch=119
03/01/2022 21:26:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.14 on epoch=124
03/01/2022 21:26:43 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.5 on epoch=124
03/01/2022 21:26:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.15 on epoch=129
03/01/2022 21:26:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.12 on epoch=134
03/01/2022 21:26:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.14 on epoch=139
03/01/2022 21:26:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.14 on epoch=144
03/01/2022 21:26:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.11 on epoch=149
03/01/2022 21:26:55 - INFO - __main__ - Global step 300 Train loss 0.13 ACC 0.75 on epoch=149
03/01/2022 21:26:55 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.75 on epoch=149, global_step=300
03/01/2022 21:26:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.13 on epoch=154
03/01/2022 21:26:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.11 on epoch=159
03/01/2022 21:27:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.12 on epoch=164
03/01/2022 21:27:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
03/01/2022 21:27:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.12 on epoch=174
03/01/2022 21:27:07 - INFO - __main__ - Global step 350 Train loss 0.12 ACC 0.625 on epoch=174
03/01/2022 21:27:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=179
03/01/2022 21:27:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.11 on epoch=184
03/01/2022 21:27:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.10 on epoch=189
03/01/2022 21:27:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.11 on epoch=194
03/01/2022 21:27:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.13 on epoch=199
03/01/2022 21:27:19 - INFO - __main__ - Global step 400 Train loss 0.12 ACC 0.5 on epoch=199
03/01/2022 21:27:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=204
03/01/2022 21:27:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.11 on epoch=209
03/01/2022 21:27:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
03/01/2022 21:27:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=219
03/01/2022 21:27:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.12 on epoch=224
03/01/2022 21:27:32 - INFO - __main__ - Global step 450 Train loss 0.11 ACC 0.59375 on epoch=224
03/01/2022 21:27:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=229
03/01/2022 21:27:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=234
03/01/2022 21:27:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.08 on epoch=239
03/01/2022 21:27:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
03/01/2022 21:27:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.08 on epoch=249
03/01/2022 21:27:44 - INFO - __main__ - Global step 500 Train loss 0.08 ACC 0.53125 on epoch=249
03/01/2022 21:27:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
03/01/2022 21:27:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
03/01/2022 21:27:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
03/01/2022 21:27:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
03/01/2022 21:27:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
03/01/2022 21:27:58 - INFO - __main__ - Global step 550 Train loss 0.06 ACC 0.625 on epoch=274
03/01/2022 21:28:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
03/01/2022 21:28:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
03/01/2022 21:28:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.03 on epoch=289
03/01/2022 21:28:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
03/01/2022 21:28:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
03/01/2022 21:28:10 - INFO - __main__ - Global step 600 Train loss 0.04 ACC 0.6875 on epoch=299
03/01/2022 21:28:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
03/01/2022 21:28:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/01/2022 21:28:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/01/2022 21:28:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
03/01/2022 21:28:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
03/01/2022 21:28:22 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.5625 on epoch=324
03/01/2022 21:28:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
03/01/2022 21:28:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
03/01/2022 21:28:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
03/01/2022 21:28:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/01/2022 21:28:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/01/2022 21:28:34 - INFO - __main__ - Global step 700 Train loss 0.02 ACC 0.71875 on epoch=349
03/01/2022 21:28:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/01/2022 21:28:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
03/01/2022 21:28:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/01/2022 21:28:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/01/2022 21:28:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/01/2022 21:28:46 - INFO - __main__ - Global step 750 Train loss 0.00 ACC 0.65625 on epoch=374
03/01/2022 21:28:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/01/2022 21:28:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/01/2022 21:28:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/01/2022 21:28:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/01/2022 21:28:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/01/2022 21:28:58 - INFO - __main__ - Global step 800 Train loss 0.00 ACC 0.6875 on epoch=399
03/01/2022 21:29:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
03/01/2022 21:29:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 21:29:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/01/2022 21:29:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/01/2022 21:29:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/01/2022 21:29:10 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.5625 on epoch=424
03/01/2022 21:29:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
03/01/2022 21:29:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 21:29:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 21:29:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/01/2022 21:29:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/01/2022 21:29:23 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.71875 on epoch=449
03/01/2022 21:29:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 21:29:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 21:29:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 21:29:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 21:29:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 21:29:35 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.6875 on epoch=474
03/01/2022 21:29:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/01/2022 21:29:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 21:29:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 21:29:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 21:29:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 21:29:48 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.65625 on epoch=499
03/01/2022 21:29:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 21:29:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 21:29:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 21:29:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 21:29:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 21:30:00 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.6875 on epoch=524
03/01/2022 21:30:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 21:30:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 21:30:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 21:30:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 21:30:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 21:30:12 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.6875 on epoch=549
03/01/2022 21:30:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 21:30:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 21:30:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 21:30:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 21:30:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 21:30:25 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.6875 on epoch=574
03/01/2022 21:30:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 21:30:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 21:30:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 21:30:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 21:30:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 21:30:37 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.6875 on epoch=599
03/01/2022 21:30:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 21:30:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 21:30:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 21:30:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 21:30:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 21:30:50 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.625 on epoch=624
03/01/2022 21:30:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 21:30:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 21:30:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 21:30:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 21:31:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 21:31:02 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.71875 on epoch=649
03/01/2022 21:31:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 21:31:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 21:31:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 21:31:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 21:31:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 21:31:14 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.65625 on epoch=674
03/01/2022 21:31:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 21:31:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 21:31:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 21:31:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 21:31:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 21:31:27 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.65625 on epoch=699
03/01/2022 21:31:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 21:31:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 21:31:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 21:31:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/01/2022 21:31:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 21:31:39 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.71875 on epoch=724
03/01/2022 21:31:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 21:31:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 21:31:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 21:31:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 21:31:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 21:31:52 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.6875 on epoch=749
03/01/2022 21:31:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 21:31:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 21:31:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 21:32:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 21:32:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 21:32:04 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.65625 on epoch=774
03/01/2022 21:32:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 21:32:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 21:32:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 21:32:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 21:32:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 21:32:16 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.65625 on epoch=799
03/01/2022 21:32:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 21:32:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 21:32:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 21:32:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 21:32:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 21:32:29 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.65625 on epoch=824
03/01/2022 21:32:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 21:32:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 21:32:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 21:32:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 21:32:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 21:32:41 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.75 on epoch=849
03/01/2022 21:32:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/01/2022 21:32:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 21:32:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 21:32:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 21:32:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 21:32:53 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.59375 on epoch=874
03/01/2022 21:32:56 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 21:32:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 21:33:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 21:33:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 21:33:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 21:33:06 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.59375 on epoch=899
03/01/2022 21:33:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 21:33:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 21:33:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 21:33:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 21:33:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 21:33:18 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.71875 on epoch=924
03/01/2022 21:33:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 21:33:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 21:33:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 21:33:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 21:33:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 21:33:30 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.65625 on epoch=949
03/01/2022 21:33:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 21:33:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 21:33:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 21:33:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 21:33:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 21:33:43 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.71875 on epoch=974
03/01/2022 21:33:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 21:33:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 21:33:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 21:33:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 21:33:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 21:33:55 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.6875 on epoch=999
03/01/2022 21:33:55 - INFO - __main__ - save last model!
03/01/2022 21:33:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 21:33:55 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 21:33:55 - INFO - __main__ - Printing 3 examples
03/01/2022 21:33:55 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 21:33:55 - INFO - __main__ - ['entailment']
03/01/2022 21:33:55 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 21:33:55 - INFO - __main__ - ['not_entailment']
03/01/2022 21:33:55 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 21:33:55 - INFO - __main__ - ['not_entailment']
03/01/2022 21:33:55 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:33:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:33:55 - INFO - __main__ - Printing 3 examples
03/01/2022 21:33:55 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 21:33:55 - INFO - __main__ - ['entailment']
03/01/2022 21:33:55 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 21:33:55 - INFO - __main__ - ['entailment']
03/01/2022 21:33:55 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 21:33:55 - INFO - __main__ - ['entailment']
03/01/2022 21:33:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 21:33:55 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:33:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:33:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:33:55 - INFO - __main__ - Printing 3 examples
03/01/2022 21:33:55 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 21:33:55 - INFO - __main__ - ['entailment']
03/01/2022 21:33:55 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 21:33:55 - INFO - __main__ - ['entailment']
03/01/2022 21:33:55 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 21:33:55 - INFO - __main__ - ['entailment']
03/01/2022 21:33:55 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:33:55 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:33:55 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:33:58 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:34:04 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 21:34:10 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 21:34:10 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(6579, 166537), (5880, 173333), (2252, 353035), (6881, 160252), (4773, 218071), (6404, 160401), (3371, 234531), (4187, 259804), (1925, 417452), (968, 645839), (2729, 183765), (5538, 189511), (987, 914447), (144, 1635131), (1079, 796417), (1653, 586845), (1731, 565272), (3424, 221936), (5680, 157495), (731, 1250799), (4298, 243616), (3238, 245783), (6415, 157560), (2406, 200208), (5775, 177401), (2721, 367851), (6309, 169174), (135, 7037199), (1214, 767509), (5962, 174066), (2867, 348102), (3209, 276527), (1189, 784250), (891, 319194), (5015, 214805), (1701, 571183), (3845, 206498), (26, 28035103), (2809, 352164), (5121, 187048), (1892, 508575), (3574, 184323), (996, 821524), (3263, 314166), (6363, 154270), (90, 459181), (4226, 250683), (1751, 560750), (5058, 173227), (5205, 176939), (7, 158399866), (6008, 156882), (3474, 290209), (3351, 190868), (225, 4092648), (1142, 782542), (154, 259007), (3278, 305705), (2637, 387504), (532, 1388213), (2597, 385302), (6237, 164778), (851, 1026262), (793, 959415), (2719, 191820), (788, 1153138), (4209, 175746), (1393, 703212), (1438, 688034), (5340, 197066), (3995, 170269), (1376, 549699), (2909, 350152), (5773, 177903), (6551, 154998), (4799, 221150), (2335, 431458), (6082, 173261), (1824, 455362), (3242, 301888), (4627, 234647), (3797, 250903), (4027, 213331), (6464, 181012), (5210, 207673), (1137, 734426), (2765, 373166), (2451, 408517), (708, 1240422), (3108, 278320), (243, 3854645), (4830, 159383), (5630, 169060), (2134, 401363), (4490, 159360), (2342, 415802), (80, 11392202), (758, 1131386), (652, 1360888)]
03/01/2022 21:34:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:34:11 - INFO - __main__ - Starting training!
03/01/2022 21:37:08 - INFO - __main__ - Saved prediction in models/T5-large/singletask-glue-qnli/glue-qnli_16_100_0.5_8_predictions.txt
03/01/2022 21:37:08 - INFO - __main__ - ACC on test data: 0.5665
03/01/2022 21:37:08 - INFO - __main__ - prefix=glue-qnli_16_100, lr=0.5, bsz=8, dev_performance=0.75, test_performance=0.5665385319421563
03/01/2022 21:37:08 - INFO - __main__ - Running ... prefix=glue-qnli_16_100, lr=0.4, bsz=8 ...
03/01/2022 21:37:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:37:09 - INFO - __main__ - Printing 3 examples
03/01/2022 21:37:09 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 21:37:09 - INFO - __main__ - ['entailment']
03/01/2022 21:37:09 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 21:37:09 - INFO - __main__ - ['entailment']
03/01/2022 21:37:09 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 21:37:09 - INFO - __main__ - ['entailment']
03/01/2022 21:37:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 21:37:09 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:37:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:37:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:37:09 - INFO - __main__ - Printing 3 examples
03/01/2022 21:37:09 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 21:37:09 - INFO - __main__ - ['entailment']
03/01/2022 21:37:09 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 21:37:09 - INFO - __main__ - ['entailment']
03/01/2022 21:37:09 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 21:37:09 - INFO - __main__ - ['entailment']
03/01/2022 21:37:09 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:37:09 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:37:09 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:37:24 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 21:37:24 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(6579, 166537), (5880, 173333), (2252, 353035), (6881, 160252), (4773, 218071), (6404, 160401), (3371, 234531), (4187, 259804), (1925, 417452), (968, 645839), (2729, 183765), (5538, 189511), (987, 914447), (144, 1635131), (1079, 796417), (1653, 586845), (1731, 565272), (3424, 221936), (5680, 157495), (731, 1250799), (4298, 243616), (3238, 245783), (6415, 157560), (2406, 200208), (5775, 177401), (2721, 367851), (6309, 169174), (135, 7037199), (1214, 767509), (5962, 174066), (2867, 348102), (3209, 276527), (1189, 784250), (891, 319194), (5015, 214805), (1701, 571183), (3845, 206498), (26, 28035103), (2809, 352164), (5121, 187048), (1892, 508575), (3574, 184323), (996, 821524), (3263, 314166), (6363, 154270), (90, 459181), (4226, 250683), (1751, 560750), (5058, 173227), (5205, 176939), (7, 158399866), (6008, 156882), (3474, 290209), (3351, 190868), (225, 4092648), (1142, 782542), (154, 259007), (3278, 305705), (2637, 387504), (532, 1388213), (2597, 385302), (6237, 164778), (851, 1026262), (793, 959415), (2719, 191820), (788, 1153138), (4209, 175746), (1393, 703212), (1438, 688034), (5340, 197066), (3995, 170269), (1376, 549699), (2909, 350152), (5773, 177903), (6551, 154998), (4799, 221150), (2335, 431458), (6082, 173261), (1824, 455362), (3242, 301888), (4627, 234647), (3797, 250903), (4027, 213331), (6464, 181012), (5210, 207673), (1137, 734426), (2765, 373166), (2451, 408517), (708, 1240422), (3108, 278320), (243, 3854645), (4830, 159383), (5630, 169060), (2134, 401363), (4490, 159360), (2342, 415802), (80, 11392202), (758, 1131386), (652, 1360888)]
03/01/2022 21:37:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:37:25 - INFO - __main__ - Starting training!
03/01/2022 21:37:27 - INFO - __main__ - Step 10 Global step 10 Train loss 4.56 on epoch=4
03/01/2022 21:37:30 - INFO - __main__ - Step 20 Global step 20 Train loss 1.25 on epoch=9
03/01/2022 21:37:32 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=14
03/01/2022 21:37:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.31 on epoch=19
03/01/2022 21:37:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.25 on epoch=24
03/01/2022 21:37:37 - INFO - __main__ - Global step 50 Train loss 1.38 ACC 0.65625 on epoch=24
03/01/2022 21:37:37 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.65625 on epoch=24, global_step=50
03/01/2022 21:37:39 - INFO - __main__ - Step 60 Global step 60 Train loss 0.23 on epoch=29
03/01/2022 21:37:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.19 on epoch=34
03/01/2022 21:37:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.19 on epoch=39
03/01/2022 21:37:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.16 on epoch=44
03/01/2022 21:37:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.17 on epoch=49
03/01/2022 21:37:49 - INFO - __main__ - Global step 100 Train loss 0.19 ACC 0.46875 on epoch=49
03/01/2022 21:37:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.15 on epoch=54
03/01/2022 21:37:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.16 on epoch=59
03/01/2022 21:37:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.16 on epoch=64
03/01/2022 21:37:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.14 on epoch=69
03/01/2022 21:38:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.17 on epoch=74
03/01/2022 21:38:02 - INFO - __main__ - Global step 150 Train loss 0.16 ACC 0.78125 on epoch=74
03/01/2022 21:38:02 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.78125 on epoch=74, global_step=150
03/01/2022 21:38:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.15 on epoch=79
03/01/2022 21:38:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.14 on epoch=84
03/01/2022 21:38:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.14 on epoch=89
03/01/2022 21:38:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.14 on epoch=94
03/01/2022 21:38:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.16 on epoch=99
03/01/2022 21:38:14 - INFO - __main__ - Global step 200 Train loss 0.15 ACC 0.78125 on epoch=99
03/01/2022 21:38:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.17 on epoch=104
03/01/2022 21:38:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.14 on epoch=109
03/01/2022 21:38:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.14 on epoch=114
03/01/2022 21:38:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.12 on epoch=119
03/01/2022 21:38:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.16 on epoch=124
03/01/2022 21:38:26 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.5 on epoch=124
03/01/2022 21:38:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.15 on epoch=129
03/01/2022 21:38:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.14 on epoch=134
03/01/2022 21:38:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.14 on epoch=139
03/01/2022 21:38:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.15 on epoch=144
03/01/2022 21:38:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.14 on epoch=149
03/01/2022 21:38:38 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.5 on epoch=149
03/01/2022 21:38:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
03/01/2022 21:38:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.12 on epoch=159
03/01/2022 21:38:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.14 on epoch=164
03/01/2022 21:38:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
03/01/2022 21:38:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.14 on epoch=174
03/01/2022 21:38:51 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.5 on epoch=174
03/01/2022 21:38:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=179
03/01/2022 21:38:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
03/01/2022 21:38:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.14 on epoch=189
03/01/2022 21:39:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.12 on epoch=194
03/01/2022 21:39:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.15 on epoch=199
03/01/2022 21:39:03 - INFO - __main__ - Global step 400 Train loss 0.13 ACC 0.5 on epoch=199
03/01/2022 21:39:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.12 on epoch=204
03/01/2022 21:39:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.11 on epoch=209
03/01/2022 21:39:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.13 on epoch=214
03/01/2022 21:39:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
03/01/2022 21:39:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.11 on epoch=224
03/01/2022 21:39:15 - INFO - __main__ - Global step 450 Train loss 0.12 ACC 0.625 on epoch=224
03/01/2022 21:39:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
03/01/2022 21:39:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.14 on epoch=234
03/01/2022 21:39:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
03/01/2022 21:39:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=244
03/01/2022 21:39:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.07 on epoch=249
03/01/2022 21:39:27 - INFO - __main__ - Global step 500 Train loss 0.11 ACC 0.6875 on epoch=249
03/01/2022 21:39:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
03/01/2022 21:39:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=259
03/01/2022 21:39:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
03/01/2022 21:39:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
03/01/2022 21:39:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.05 on epoch=274
03/01/2022 21:39:39 - INFO - __main__ - Global step 550 Train loss 0.09 ACC 0.65625 on epoch=274
03/01/2022 21:39:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.10 on epoch=279
03/01/2022 21:39:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=284
03/01/2022 21:39:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
03/01/2022 21:39:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=294
03/01/2022 21:39:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
03/01/2022 21:39:51 - INFO - __main__ - Global step 600 Train loss 0.07 ACC 0.5625 on epoch=299
03/01/2022 21:39:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
03/01/2022 21:39:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=309
03/01/2022 21:39:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.06 on epoch=314
03/01/2022 21:40:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
03/01/2022 21:40:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
03/01/2022 21:40:03 - INFO - __main__ - Global step 650 Train loss 0.05 ACC 0.65625 on epoch=324
03/01/2022 21:40:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
03/01/2022 21:40:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
03/01/2022 21:40:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.06 on epoch=339
03/01/2022 21:40:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
03/01/2022 21:40:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
03/01/2022 21:40:16 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.59375 on epoch=349
03/01/2022 21:40:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
03/01/2022 21:40:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
03/01/2022 21:40:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/01/2022 21:40:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
03/01/2022 21:40:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/01/2022 21:40:28 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.625 on epoch=374
03/01/2022 21:40:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/01/2022 21:40:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
03/01/2022 21:40:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
03/01/2022 21:40:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/01/2022 21:40:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/01/2022 21:40:40 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.625 on epoch=399
03/01/2022 21:40:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/01/2022 21:40:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 21:40:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/01/2022 21:40:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/01/2022 21:40:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
03/01/2022 21:40:52 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.65625 on epoch=424
03/01/2022 21:40:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 21:40:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/01/2022 21:40:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
03/01/2022 21:41:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/01/2022 21:41:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/01/2022 21:41:04 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.5625 on epoch=449
03/01/2022 21:41:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 21:41:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 21:41:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 21:41:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 21:41:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 21:41:16 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.625 on epoch=474
03/01/2022 21:41:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 21:41:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 21:41:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 21:41:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 21:41:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 21:41:28 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.625 on epoch=499
03/01/2022 21:41:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 21:41:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 21:41:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 21:41:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 21:41:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 21:41:40 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.625 on epoch=524
03/01/2022 21:41:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 21:41:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 21:41:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 21:41:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 21:41:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 21:41:52 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.5625 on epoch=549
03/01/2022 21:41:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 21:41:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/01/2022 21:41:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 21:42:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 21:42:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 21:42:04 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.6875 on epoch=574
03/01/2022 21:42:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 21:42:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/01/2022 21:42:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 21:42:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 21:42:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 21:42:16 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.59375 on epoch=599
03/01/2022 21:42:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 21:42:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/01/2022 21:42:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 21:42:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/01/2022 21:42:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 21:42:28 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.65625 on epoch=624
03/01/2022 21:42:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 21:42:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 21:42:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 21:42:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 21:42:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 21:42:41 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.625 on epoch=649
03/01/2022 21:42:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 21:42:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 21:42:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 21:42:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 21:42:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 21:42:53 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.65625 on epoch=674
03/01/2022 21:42:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 21:42:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 21:42:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 21:43:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 21:43:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 21:43:05 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.59375 on epoch=699
03/01/2022 21:43:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 21:43:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 21:43:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 21:43:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 21:43:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 21:43:17 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.625 on epoch=724
03/01/2022 21:43:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 21:43:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 21:43:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 21:43:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 21:43:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 21:43:29 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.65625 on epoch=749
03/01/2022 21:43:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 21:43:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 21:43:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 21:43:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 21:43:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 21:43:41 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.59375 on epoch=774
03/01/2022 21:43:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 21:43:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 21:43:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 21:43:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 21:43:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 21:43:53 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.65625 on epoch=799
03/01/2022 21:43:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 21:43:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 21:44:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 21:44:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 21:44:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 21:44:05 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.59375 on epoch=824
03/01/2022 21:44:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/01/2022 21:44:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
03/01/2022 21:44:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 21:44:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 21:44:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 21:44:17 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.65625 on epoch=849
03/01/2022 21:44:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 21:44:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 21:44:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/01/2022 21:44:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
03/01/2022 21:44:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 21:44:29 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.59375 on epoch=874
03/01/2022 21:44:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 21:44:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 21:44:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 21:44:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 21:44:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 21:44:42 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.5625 on epoch=899
03/01/2022 21:44:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 21:44:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 21:44:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 21:44:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 21:44:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 21:44:54 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.65625 on epoch=924
03/01/2022 21:44:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 21:44:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 21:45:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 21:45:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 21:45:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 21:45:06 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.625 on epoch=949
03/01/2022 21:45:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 21:45:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 21:45:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 21:45:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 21:45:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 21:45:18 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.75 on epoch=974
03/01/2022 21:45:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 21:45:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 21:45:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 21:45:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 21:45:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 21:45:30 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.71875 on epoch=999
03/01/2022 21:45:30 - INFO - __main__ - save last model!
03/01/2022 21:45:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 21:45:30 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 21:45:30 - INFO - __main__ - Printing 3 examples
03/01/2022 21:45:30 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 21:45:30 - INFO - __main__ - ['entailment']
03/01/2022 21:45:30 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 21:45:30 - INFO - __main__ - ['not_entailment']
03/01/2022 21:45:30 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 21:45:30 - INFO - __main__ - ['not_entailment']
03/01/2022 21:45:30 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:45:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:45:31 - INFO - __main__ - Printing 3 examples
03/01/2022 21:45:31 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 21:45:31 - INFO - __main__ - ['entailment']
03/01/2022 21:45:31 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 21:45:31 - INFO - __main__ - ['entailment']
03/01/2022 21:45:31 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 21:45:31 - INFO - __main__ - ['entailment']
03/01/2022 21:45:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 21:45:31 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:45:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:45:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:45:31 - INFO - __main__ - Printing 3 examples
03/01/2022 21:45:31 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 21:45:31 - INFO - __main__ - ['entailment']
03/01/2022 21:45:31 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 21:45:31 - INFO - __main__ - ['entailment']
03/01/2022 21:45:31 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 21:45:31 - INFO - __main__ - ['entailment']
03/01/2022 21:45:31 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:45:31 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:45:31 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:45:33 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:45:39 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 21:45:44 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 21:45:44 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(2594, 403711), (738, 1187017), (73, 2525281), (3583, 284375), (773, 1132610), (2099, 182268), (1757, 390014), (3023, 334230), (7161, 192247), (2231, 427234), (6082, 173261), (1461, 672164), (3573, 163617), (6209, 162555), (5259, 197913), (2697, 380850), (4225, 197872), (2319, 230472), (1994, 479967), (993, 886924), (1069, 868644), (4404, 216707), (4001, 264258), (4585, 221032), (4434, 228933), (5229, 200589), (629, 1463788), (1101, 860462), (661, 1317687), (3147, 232857), (2132, 277175), (1102, 798392), (2542, 371822), (2503, 477739), (2036, 480551), (5480, 174200), (4952, 209263), (397, 637384), (4682, 222111), (2030, 497571), (5234, 202177), (2665, 369855), (757, 1071575), (4370, 210982), (4895, 169946), (1085, 862457), (705, 1548305), (4508, 172897), (167, 5559739), (1112, 892303), (2193, 392132), (1396, 545283), (4374, 230866), (3385, 191872), (5816, 194070), (2358, 427447), (4191, 233374), (646, 1377071), (1790, 550995), (4264, 247591), (22, 38484130), (2415, 239286), (2392, 349952), (4190, 205854), (2833, 327549), (5032, 221170), (6353, 168404), (5324, 191543), (1518, 582487), (1894, 481117), (3228, 315556), (2265, 421208), (636, 1378362), (6548, 161733), (6098, 172422), (696, 1306625), (3452, 300052), (717, 1408825), (573, 1561543), (4562, 160268), (4277, 195942), (5640, 186094), (2551, 176659), (1249, 573784), (631, 1412370), (4905, 184294), (877, 1019947), (1758, 490726), (767, 1164520), (2558, 157757), (518, 1170336), (1534, 393440), (3774, 232302), (1269, 737181), (6699, 164021), (2888, 375431), (6496, 161755), (4973, 157902), (348, 1943901)]
03/01/2022 21:45:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:45:45 - INFO - __main__ - Starting training!
03/01/2022 21:48:28 - INFO - __main__ - Saved prediction in models/T5-large/singletask-glue-qnli/glue-qnli_16_100_0.4_8_predictions.txt
03/01/2022 21:48:28 - INFO - __main__ - ACC on test data: 0.5230
03/01/2022 21:48:28 - INFO - __main__ - prefix=glue-qnli_16_100, lr=0.4, bsz=8, dev_performance=0.78125, test_performance=0.5229727256086399
03/01/2022 21:48:28 - INFO - __main__ - Running ... prefix=glue-qnli_16_100, lr=0.3, bsz=8 ...
03/01/2022 21:48:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:48:29 - INFO - __main__ - Printing 3 examples
03/01/2022 21:48:29 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 21:48:29 - INFO - __main__ - ['entailment']
03/01/2022 21:48:29 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 21:48:29 - INFO - __main__ - ['entailment']
03/01/2022 21:48:29 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 21:48:29 - INFO - __main__ - ['entailment']
03/01/2022 21:48:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 21:48:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:48:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:48:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:48:29 - INFO - __main__ - Printing 3 examples
03/01/2022 21:48:29 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 21:48:29 - INFO - __main__ - ['entailment']
03/01/2022 21:48:29 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 21:48:29 - INFO - __main__ - ['entailment']
03/01/2022 21:48:29 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 21:48:29 - INFO - __main__ - ['entailment']
03/01/2022 21:48:29 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:48:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:48:29 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:48:44 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 21:48:44 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(2594, 403711), (738, 1187017), (73, 2525281), (3583, 284375), (773, 1132610), (2099, 182268), (1757, 390014), (3023, 334230), (7161, 192247), (2231, 427234), (6082, 173261), (1461, 672164), (3573, 163617), (6209, 162555), (5259, 197913), (2697, 380850), (4225, 197872), (2319, 230472), (1994, 479967), (993, 886924), (1069, 868644), (4404, 216707), (4001, 264258), (4585, 221032), (4434, 228933), (5229, 200589), (629, 1463788), (1101, 860462), (661, 1317687), (3147, 232857), (2132, 277175), (1102, 798392), (2542, 371822), (2503, 477739), (2036, 480551), (5480, 174200), (4952, 209263), (397, 637384), (4682, 222111), (2030, 497571), (5234, 202177), (2665, 369855), (757, 1071575), (4370, 210982), (4895, 169946), (1085, 862457), (705, 1548305), (4508, 172897), (167, 5559739), (1112, 892303), (2193, 392132), (1396, 545283), (4374, 230866), (3385, 191872), (5816, 194070), (2358, 427447), (4191, 233374), (646, 1377071), (1790, 550995), (4264, 247591), (22, 38484130), (2415, 239286), (2392, 349952), (4190, 205854), (2833, 327549), (5032, 221170), (6353, 168404), (5324, 191543), (1518, 582487), (1894, 481117), (3228, 315556), (2265, 421208), (636, 1378362), (6548, 161733), (6098, 172422), (696, 1306625), (3452, 300052), (717, 1408825), (573, 1561543), (4562, 160268), (4277, 195942), (5640, 186094), (2551, 176659), (1249, 573784), (631, 1412370), (4905, 184294), (877, 1019947), (1758, 490726), (767, 1164520), (2558, 157757), (518, 1170336), (1534, 393440), (3774, 232302), (1269, 737181), (6699, 164021), (2888, 375431), (6496, 161755), (4973, 157902), (348, 1943901)]
03/01/2022 21:48:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:48:45 - INFO - __main__ - Starting training!
03/01/2022 21:48:47 - INFO - __main__ - Step 10 Global step 10 Train loss 4.41 on epoch=4
03/01/2022 21:48:50 - INFO - __main__ - Step 20 Global step 20 Train loss 1.60 on epoch=9
03/01/2022 21:48:52 - INFO - __main__ - Step 30 Global step 30 Train loss 0.69 on epoch=14
03/01/2022 21:48:54 - INFO - __main__ - Step 40 Global step 40 Train loss 0.46 on epoch=19
03/01/2022 21:48:56 - INFO - __main__ - Step 50 Global step 50 Train loss 0.38 on epoch=24
03/01/2022 21:48:57 - INFO - __main__ - Global step 50 Train loss 1.51 ACC 0.53125 on epoch=24
03/01/2022 21:48:57 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.53125 on epoch=24, global_step=50
03/01/2022 21:49:00 - INFO - __main__ - Step 60 Global step 60 Train loss 0.31 on epoch=29
03/01/2022 21:49:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.28 on epoch=34
03/01/2022 21:49:04 - INFO - __main__ - Step 80 Global step 80 Train loss 0.25 on epoch=39
03/01/2022 21:49:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.24 on epoch=44
03/01/2022 21:49:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.18 on epoch=49
03/01/2022 21:49:10 - INFO - __main__ - Global step 100 Train loss 0.25 ACC 0.65625 on epoch=49
03/01/2022 21:49:10 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.65625 on epoch=49, global_step=100
03/01/2022 21:49:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.16 on epoch=54
03/01/2022 21:49:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.18 on epoch=59
03/01/2022 21:49:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.18 on epoch=64
03/01/2022 21:49:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.15 on epoch=69
03/01/2022 21:49:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.16 on epoch=74
03/01/2022 21:49:22 - INFO - __main__ - Global step 150 Train loss 0.17 ACC 0.53125 on epoch=74
03/01/2022 21:49:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.16 on epoch=79
03/01/2022 21:49:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.16 on epoch=84
03/01/2022 21:49:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.16 on epoch=89
03/01/2022 21:49:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.16 on epoch=94
03/01/2022 21:49:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.17 on epoch=99
03/01/2022 21:49:34 - INFO - __main__ - Global step 200 Train loss 0.16 ACC 0.6875 on epoch=99
03/01/2022 21:49:34 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=99, global_step=200
03/01/2022 21:49:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.19 on epoch=104
03/01/2022 21:49:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.13 on epoch=109
03/01/2022 21:49:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.18 on epoch=114
03/01/2022 21:49:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.14 on epoch=119
03/01/2022 21:49:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.13 on epoch=124
03/01/2022 21:49:47 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.75 on epoch=124
03/01/2022 21:49:47 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.75 on epoch=124, global_step=250
03/01/2022 21:49:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.15 on epoch=129
03/01/2022 21:49:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.13 on epoch=134
03/01/2022 21:49:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.17 on epoch=139
03/01/2022 21:49:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.14 on epoch=144
03/01/2022 21:49:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.15 on epoch=149
03/01/2022 21:50:00 - INFO - __main__ - Global step 300 Train loss 0.15 ACC 0.65625 on epoch=149
03/01/2022 21:50:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
03/01/2022 21:50:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.13 on epoch=159
03/01/2022 21:50:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.13 on epoch=164
03/01/2022 21:50:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.13 on epoch=169
03/01/2022 21:50:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.13 on epoch=174
03/01/2022 21:50:12 - INFO - __main__ - Global step 350 Train loss 0.13 ACC 0.625 on epoch=174
03/01/2022 21:50:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.11 on epoch=179
03/01/2022 21:50:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.13 on epoch=184
03/01/2022 21:50:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.13 on epoch=189
03/01/2022 21:50:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.12 on epoch=194
03/01/2022 21:50:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=199
03/01/2022 21:50:24 - INFO - __main__ - Global step 400 Train loss 0.12 ACC 0.53125 on epoch=199
03/01/2022 21:50:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.12 on epoch=204
03/01/2022 21:50:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.08 on epoch=209
03/01/2022 21:50:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.11 on epoch=214
03/01/2022 21:50:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
03/01/2022 21:50:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.05 on epoch=224
03/01/2022 21:50:37 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.5625 on epoch=224
03/01/2022 21:50:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
03/01/2022 21:50:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.05 on epoch=234
03/01/2022 21:50:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.05 on epoch=239
03/01/2022 21:50:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
03/01/2022 21:50:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.02 on epoch=249
03/01/2022 21:50:49 - INFO - __main__ - Global step 500 Train loss 0.05 ACC 0.6875 on epoch=249
03/01/2022 21:50:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.04 on epoch=254
03/01/2022 21:50:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
03/01/2022 21:50:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.03 on epoch=264
03/01/2022 21:50:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.04 on epoch=269
03/01/2022 21:51:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
03/01/2022 21:51:01 - INFO - __main__ - Global step 550 Train loss 0.04 ACC 0.65625 on epoch=274
03/01/2022 21:51:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/01/2022 21:51:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
03/01/2022 21:51:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
03/01/2022 21:51:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=294
03/01/2022 21:51:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
03/01/2022 21:51:13 - INFO - __main__ - Global step 600 Train loss 0.02 ACC 0.59375 on epoch=299
03/01/2022 21:51:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
03/01/2022 21:51:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/01/2022 21:51:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
03/01/2022 21:51:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
03/01/2022 21:51:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/01/2022 21:51:26 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.65625 on epoch=324
03/01/2022 21:51:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/01/2022 21:51:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
03/01/2022 21:51:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/01/2022 21:51:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/01/2022 21:51:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
03/01/2022 21:51:38 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.625 on epoch=349
03/01/2022 21:51:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/01/2022 21:51:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
03/01/2022 21:51:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/01/2022 21:51:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
03/01/2022 21:51:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
03/01/2022 21:51:50 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.71875 on epoch=374
03/01/2022 21:51:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/01/2022 21:51:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/01/2022 21:51:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/01/2022 21:51:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/01/2022 21:52:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/01/2022 21:52:03 - INFO - __main__ - Global step 800 Train loss 0.00 ACC 0.6875 on epoch=399
03/01/2022 21:52:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/01/2022 21:52:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 21:52:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
03/01/2022 21:52:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/01/2022 21:52:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/01/2022 21:52:15 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.6875 on epoch=424
03/01/2022 21:52:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 21:52:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 21:52:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 21:52:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/01/2022 21:52:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/01/2022 21:52:27 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.75 on epoch=449
03/01/2022 21:52:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 21:52:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 21:52:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 21:52:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 21:52:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 21:52:39 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.625 on epoch=474
03/01/2022 21:52:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 21:52:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
03/01/2022 21:52:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 21:52:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 21:52:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/01/2022 21:52:52 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.71875 on epoch=499
03/01/2022 21:52:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
03/01/2022 21:52:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 21:52:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 21:53:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 21:53:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 21:53:05 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.75 on epoch=524
03/01/2022 21:53:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 21:53:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/01/2022 21:53:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 21:53:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 21:53:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 21:53:17 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.71875 on epoch=549
03/01/2022 21:53:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 21:53:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 21:53:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 21:53:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 21:53:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 21:53:29 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.71875 on epoch=574
03/01/2022 21:53:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 21:53:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 21:53:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 21:53:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 21:53:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 21:53:42 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.75 on epoch=599
03/01/2022 21:53:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 21:53:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 21:53:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 21:53:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 21:53:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 21:53:54 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.71875 on epoch=624
03/01/2022 21:53:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 21:53:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 21:54:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 21:54:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 21:54:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 21:54:07 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.65625 on epoch=649
03/01/2022 21:54:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 21:54:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 21:54:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 21:54:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
03/01/2022 21:54:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 21:54:19 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.71875 on epoch=674
03/01/2022 21:54:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 21:54:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/01/2022 21:54:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 21:54:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 21:54:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 21:54:31 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.75 on epoch=699
03/01/2022 21:54:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 21:54:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 21:54:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 21:54:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 21:54:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 21:54:44 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.71875 on epoch=724
03/01/2022 21:54:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 21:54:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 21:54:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 21:54:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 21:54:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 21:54:56 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.71875 on epoch=749
03/01/2022 21:54:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/01/2022 21:55:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 21:55:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 21:55:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 21:55:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 21:55:08 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.71875 on epoch=774
03/01/2022 21:55:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 21:55:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 21:55:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 21:55:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 21:55:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 21:55:20 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.65625 on epoch=799
03/01/2022 21:55:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 21:55:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 21:55:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
03/01/2022 21:55:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 21:55:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 21:55:33 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.6875 on epoch=824
03/01/2022 21:55:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 21:55:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 21:55:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 21:55:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 21:55:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 21:55:45 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.71875 on epoch=849
03/01/2022 21:55:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 21:55:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 21:55:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 21:55:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 21:55:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 21:55:58 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.6875 on epoch=874
03/01/2022 21:56:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 21:56:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 21:56:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 21:56:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 21:56:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 21:56:10 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.6875 on epoch=899
03/01/2022 21:56:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 21:56:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 21:56:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 21:56:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 21:56:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 21:56:22 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.6875 on epoch=924
03/01/2022 21:56:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 21:56:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 21:56:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 21:56:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 21:56:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 21:56:35 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.65625 on epoch=949
03/01/2022 21:56:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 21:56:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 21:56:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 21:56:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 21:56:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 21:56:47 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.65625 on epoch=974
03/01/2022 21:56:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 21:56:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 21:56:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 21:56:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 21:56:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 21:56:59 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.65625 on epoch=999
03/01/2022 21:56:59 - INFO - __main__ - save last model!
03/01/2022 21:56:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 21:57:00 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 21:57:00 - INFO - __main__ - Printing 3 examples
03/01/2022 21:57:00 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 21:57:00 - INFO - __main__ - ['entailment']
03/01/2022 21:57:00 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 21:57:00 - INFO - __main__ - ['not_entailment']
03/01/2022 21:57:00 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 21:57:00 - INFO - __main__ - ['not_entailment']
03/01/2022 21:57:00 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:57:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:57:00 - INFO - __main__ - Printing 3 examples
03/01/2022 21:57:00 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 21:57:00 - INFO - __main__ - ['entailment']
03/01/2022 21:57:00 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 21:57:00 - INFO - __main__ - ['entailment']
03/01/2022 21:57:00 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 21:57:00 - INFO - __main__ - ['entailment']
03/01/2022 21:57:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 21:57:00 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:57:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:57:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:57:00 - INFO - __main__ - Printing 3 examples
03/01/2022 21:57:00 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 21:57:00 - INFO - __main__ - ['entailment']
03/01/2022 21:57:00 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 21:57:00 - INFO - __main__ - ['entailment']
03/01/2022 21:57:00 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 21:57:00 - INFO - __main__ - ['entailment']
03/01/2022 21:57:00 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:57:00 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:57:00 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:57:02 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:57:08 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 21:57:14 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 21:57:14 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(907, 999819), (4328, 223698), (7511, 160346), (5954, 165280), (4511, 179764), (3531, 297895), (2852, 355056), (1714, 447470), (3237, 299613), (2554, 389221), (1462, 349360), (5120, 236973), (1511, 676741), (2846, 312704), (4848, 204967), (3155, 298023), (976, 1102538), (99, 8503604), (4345, 204361), (5077, 166084), (1077, 853741), (930, 1093140), (5712, 173941), (2441, 429114), (5696, 184838), (2392, 349952), (1363, 669374), (2856, 268051), (554, 1156528), (2131, 378214), (3105, 254084), (3315, 327974), (1735, 571995), (4183, 213298), (6331, 171986), (3243, 311061), (5189, 177490), (91, 9930046), (4320, 168819), (2577, 313511), (1273, 833860), (1466, 664308), (3001, 351252), (955, 764542), (1170, 817452), (6309, 169174), (986, 581568), (2381, 341017), (3127, 331271), (7594, 155434), (2456, 433793), (3716, 273699), (2024, 448261), (2411, 351989), (5529, 185095), (3351, 190868), (2717, 368013), (580, 1542140), (1019, 901860), (4585, 221032), (2887, 336279), (514, 1732295), (42, 24506569), (3827, 282190), (1368, 677011), (2835, 354320), (1751, 560750), (4764, 211573), (6349, 169524), (4554, 218517), (6817, 166736), (103, 8154869), (1142, 782542), (809, 1073970), (1651, 598254), (5782, 171468), (616, 2195072), (4145, 254483), (6957, 160765), (6212, 169413), (1519, 600961), (4894, 217353), (944, 693547), (124, 1842230), (2122, 305087), (3931, 257603), (453, 1935608), (4016, 262231), (1677, 434886), (2591, 370033), (953, 840707), (1022, 264436), (4487, 166978), (4374, 230866), (1499, 582683), (2605, 391810), (1675, 558878), (948, 512142), (4647, 228104)]
03/01/2022 21:57:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:57:15 - INFO - __main__ - Starting training!
03/01/2022 22:00:08 - INFO - __main__ - Saved prediction in models/T5-large/singletask-glue-qnli/glue-qnli_16_100_0.3_8_predictions.txt
03/01/2022 22:00:08 - INFO - __main__ - ACC on test data: 0.5204
03/01/2022 22:00:08 - INFO - __main__ - prefix=glue-qnli_16_100, lr=0.3, bsz=8, dev_performance=0.75, test_performance=0.5204100311184331
03/01/2022 22:00:08 - INFO - __main__ - Running ... prefix=glue-qnli_16_100, lr=0.2, bsz=8 ...
03/01/2022 22:00:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:00:09 - INFO - __main__ - Printing 3 examples
03/01/2022 22:00:09 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 22:00:09 - INFO - __main__ - ['entailment']
03/01/2022 22:00:09 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 22:00:09 - INFO - __main__ - ['entailment']
03/01/2022 22:00:09 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 22:00:09 - INFO - __main__ - ['entailment']
03/01/2022 22:00:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 22:00:09 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:00:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:00:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:00:09 - INFO - __main__ - Printing 3 examples
03/01/2022 22:00:09 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 22:00:09 - INFO - __main__ - ['entailment']
03/01/2022 22:00:09 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 22:00:09 - INFO - __main__ - ['entailment']
03/01/2022 22:00:09 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 22:00:09 - INFO - __main__ - ['entailment']
03/01/2022 22:00:09 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:00:09 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:00:09 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:00:24 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 22:00:24 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(907, 999819), (4328, 223698), (7511, 160346), (5954, 165280), (4511, 179764), (3531, 297895), (2852, 355056), (1714, 447470), (3237, 299613), (2554, 389221), (1462, 349360), (5120, 236973), (1511, 676741), (2846, 312704), (4848, 204967), (3155, 298023), (976, 1102538), (99, 8503604), (4345, 204361), (5077, 166084), (1077, 853741), (930, 1093140), (5712, 173941), (2441, 429114), (5696, 184838), (2392, 349952), (1363, 669374), (2856, 268051), (554, 1156528), (2131, 378214), (3105, 254084), (3315, 327974), (1735, 571995), (4183, 213298), (6331, 171986), (3243, 311061), (5189, 177490), (91, 9930046), (4320, 168819), (2577, 313511), (1273, 833860), (1466, 664308), (3001, 351252), (955, 764542), (1170, 817452), (6309, 169174), (986, 581568), (2381, 341017), (3127, 331271), (7594, 155434), (2456, 433793), (3716, 273699), (2024, 448261), (2411, 351989), (5529, 185095), (3351, 190868), (2717, 368013), (580, 1542140), (1019, 901860), (4585, 221032), (2887, 336279), (514, 1732295), (42, 24506569), (3827, 282190), (1368, 677011), (2835, 354320), (1751, 560750), (4764, 211573), (6349, 169524), (4554, 218517), (6817, 166736), (103, 8154869), (1142, 782542), (809, 1073970), (1651, 598254), (5782, 171468), (616, 2195072), (4145, 254483), (6957, 160765), (6212, 169413), (1519, 600961), (4894, 217353), (944, 693547), (124, 1842230), (2122, 305087), (3931, 257603), (453, 1935608), (4016, 262231), (1677, 434886), (2591, 370033), (953, 840707), (1022, 264436), (4487, 166978), (4374, 230866), (1499, 582683), (2605, 391810), (1675, 558878), (948, 512142), (4647, 228104)]
03/01/2022 22:00:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:00:25 - INFO - __main__ - Starting training!
03/01/2022 22:00:27 - INFO - __main__ - Step 10 Global step 10 Train loss 5.47 on epoch=4
03/01/2022 22:00:30 - INFO - __main__ - Step 20 Global step 20 Train loss 3.03 on epoch=9
03/01/2022 22:00:32 - INFO - __main__ - Step 30 Global step 30 Train loss 1.49 on epoch=14
03/01/2022 22:00:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.94 on epoch=19
03/01/2022 22:00:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.66 on epoch=24
03/01/2022 22:00:38 - INFO - __main__ - Global step 50 Train loss 2.32 ACC 0.5 on epoch=24
03/01/2022 22:00:38 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 22:00:40 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=29
03/01/2022 22:00:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=34
03/01/2022 22:00:45 - INFO - __main__ - Step 80 Global step 80 Train loss 0.41 on epoch=39
03/01/2022 22:00:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.33 on epoch=44
03/01/2022 22:00:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.27 on epoch=49
03/01/2022 22:00:51 - INFO - __main__ - Global step 100 Train loss 0.39 ACC 0.5 on epoch=49
03/01/2022 22:00:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.21 on epoch=54
03/01/2022 22:00:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.22 on epoch=59
03/01/2022 22:00:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.21 on epoch=64
03/01/2022 22:01:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.21 on epoch=69
03/01/2022 22:01:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.23 on epoch=74
03/01/2022 22:01:03 - INFO - __main__ - Global step 150 Train loss 0.22 ACC 0.53125 on epoch=74
03/01/2022 22:01:03 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=74, global_step=150
03/01/2022 22:01:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.24 on epoch=79
03/01/2022 22:01:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.17 on epoch=84
03/01/2022 22:01:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.20 on epoch=89
03/01/2022 22:01:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.19 on epoch=94
03/01/2022 22:01:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.20 on epoch=99
03/01/2022 22:01:15 - INFO - __main__ - Global step 200 Train loss 0.20 ACC 0.5625 on epoch=99
03/01/2022 22:01:15 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=99, global_step=200
03/01/2022 22:01:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.17 on epoch=104
03/01/2022 22:01:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.12 on epoch=109
03/01/2022 22:01:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.16 on epoch=114
03/01/2022 22:01:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.17 on epoch=119
03/01/2022 22:01:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.19 on epoch=124
03/01/2022 22:01:28 - INFO - __main__ - Global step 250 Train loss 0.16 ACC 0.53125 on epoch=124
03/01/2022 22:01:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.17 on epoch=129
03/01/2022 22:01:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.16 on epoch=134
03/01/2022 22:01:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.16 on epoch=139
03/01/2022 22:01:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.17 on epoch=144
03/01/2022 22:01:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.15 on epoch=149
03/01/2022 22:01:40 - INFO - __main__ - Global step 300 Train loss 0.16 ACC 0.53125 on epoch=149
03/01/2022 22:01:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.15 on epoch=154
03/01/2022 22:01:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.16 on epoch=159
03/01/2022 22:01:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.14 on epoch=164
03/01/2022 22:01:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
03/01/2022 22:01:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.15 on epoch=174
03/01/2022 22:01:52 - INFO - __main__ - Global step 350 Train loss 0.15 ACC 0.5 on epoch=174
03/01/2022 22:01:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.16 on epoch=179
03/01/2022 22:01:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.16 on epoch=184
03/01/2022 22:01:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.17 on epoch=189
03/01/2022 22:02:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.14 on epoch=194
03/01/2022 22:02:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.15 on epoch=199
03/01/2022 22:02:04 - INFO - __main__ - Global step 400 Train loss 0.16 ACC 0.59375 on epoch=199
03/01/2022 22:02:05 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=199, global_step=400
03/01/2022 22:02:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.12 on epoch=204
03/01/2022 22:02:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.14 on epoch=209
03/01/2022 22:02:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.14 on epoch=214
03/01/2022 22:02:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.15 on epoch=219
03/01/2022 22:02:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=224
03/01/2022 22:02:17 - INFO - __main__ - Global step 450 Train loss 0.14 ACC 0.5 on epoch=224
03/01/2022 22:02:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.12 on epoch=229
03/01/2022 22:02:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.14 on epoch=234
03/01/2022 22:02:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.12 on epoch=239
03/01/2022 22:02:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=244
03/01/2022 22:02:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=249
03/01/2022 22:02:29 - INFO - __main__ - Global step 500 Train loss 0.13 ACC 0.5 on epoch=249
03/01/2022 22:02:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.13 on epoch=254
03/01/2022 22:02:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.12 on epoch=259
03/01/2022 22:02:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.16 on epoch=264
03/01/2022 22:02:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=269
03/01/2022 22:02:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.14 on epoch=274
03/01/2022 22:02:41 - INFO - __main__ - Global step 550 Train loss 0.14 ACC 0.5 on epoch=274
03/01/2022 22:02:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=279
03/01/2022 22:02:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=284
03/01/2022 22:02:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=289
03/01/2022 22:02:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=294
03/01/2022 22:02:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=299
03/01/2022 22:02:54 - INFO - __main__ - Global step 600 Train loss 0.13 ACC 0.5 on epoch=299
03/01/2022 22:02:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=304
03/01/2022 22:02:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=309
03/01/2022 22:03:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.14 on epoch=314
03/01/2022 22:03:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=319
03/01/2022 22:03:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=324
03/01/2022 22:03:06 - INFO - __main__ - Global step 650 Train loss 0.13 ACC 0.5 on epoch=324
03/01/2022 22:03:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=329
03/01/2022 22:03:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
03/01/2022 22:03:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.11 on epoch=339
03/01/2022 22:03:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
03/01/2022 22:03:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=349
03/01/2022 22:03:18 - INFO - __main__ - Global step 700 Train loss 0.12 ACC 0.5 on epoch=349
03/01/2022 22:03:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=354
03/01/2022 22:03:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=359
03/01/2022 22:03:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=364
03/01/2022 22:03:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=369
03/01/2022 22:03:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=374
03/01/2022 22:03:30 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.5 on epoch=374
03/01/2022 22:03:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
03/01/2022 22:03:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=384
03/01/2022 22:03:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=389
03/01/2022 22:03:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=394
03/01/2022 22:03:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=399
03/01/2022 22:03:43 - INFO - __main__ - Global step 800 Train loss 0.09 ACC 0.5625 on epoch=399
03/01/2022 22:03:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=404
03/01/2022 22:03:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=409
03/01/2022 22:03:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=414
03/01/2022 22:03:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=419
03/01/2022 22:03:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=424
03/01/2022 22:03:55 - INFO - __main__ - Global step 850 Train loss 0.09 ACC 0.65625 on epoch=424
03/01/2022 22:03:55 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=424, global_step=850
03/01/2022 22:03:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
03/01/2022 22:03:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=434
03/01/2022 22:04:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=439
03/01/2022 22:04:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
03/01/2022 22:04:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=449
03/01/2022 22:04:08 - INFO - __main__ - Global step 900 Train loss 0.07 ACC 0.65625 on epoch=449
03/01/2022 22:04:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
03/01/2022 22:04:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=459
03/01/2022 22:04:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
03/01/2022 22:04:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=469
03/01/2022 22:04:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
03/01/2022 22:04:20 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.6875 on epoch=474
03/01/2022 22:04:20 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=474, global_step=950
03/01/2022 22:04:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
03/01/2022 22:04:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
03/01/2022 22:04:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
03/01/2022 22:04:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
03/01/2022 22:04:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
03/01/2022 22:04:32 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.65625 on epoch=499
03/01/2022 22:04:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
03/01/2022 22:04:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
03/01/2022 22:04:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
03/01/2022 22:04:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
03/01/2022 22:04:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
03/01/2022 22:04:44 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.65625 on epoch=524
03/01/2022 22:04:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
03/01/2022 22:04:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
03/01/2022 22:04:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
03/01/2022 22:04:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/01/2022 22:04:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=549
03/01/2022 22:04:57 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.6875 on epoch=549
03/01/2022 22:04:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
03/01/2022 22:05:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/01/2022 22:05:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
03/01/2022 22:05:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
03/01/2022 22:05:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/01/2022 22:05:09 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.625 on epoch=574
03/01/2022 22:05:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/01/2022 22:05:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
03/01/2022 22:05:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
03/01/2022 22:05:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/01/2022 22:05:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
03/01/2022 22:05:21 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.625 on epoch=599
03/01/2022 22:05:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
03/01/2022 22:05:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
03/01/2022 22:05:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
03/01/2022 22:05:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/01/2022 22:05:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/01/2022 22:05:34 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.6875 on epoch=624
03/01/2022 22:05:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/01/2022 22:05:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
03/01/2022 22:05:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
03/01/2022 22:05:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/01/2022 22:05:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/01/2022 22:05:46 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.625 on epoch=649
03/01/2022 22:05:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
03/01/2022 22:05:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 22:05:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
03/01/2022 22:05:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
03/01/2022 22:05:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/01/2022 22:05:58 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.625 on epoch=674
03/01/2022 22:06:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/01/2022 22:06:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 22:06:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/01/2022 22:06:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 22:06:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/01/2022 22:06:11 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.65625 on epoch=699
03/01/2022 22:06:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
03/01/2022 22:06:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/01/2022 22:06:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 22:06:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/01/2022 22:06:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 22:06:23 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.75 on epoch=724
03/01/2022 22:06:23 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.75 on epoch=724, global_step=1450
03/01/2022 22:06:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 22:06:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 22:06:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
03/01/2022 22:06:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
03/01/2022 22:06:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 22:06:35 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.75 on epoch=749
03/01/2022 22:06:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
03/01/2022 22:06:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/01/2022 22:06:42 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 22:06:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 22:06:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
03/01/2022 22:06:48 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.71875 on epoch=774
03/01/2022 22:06:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 22:06:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 22:06:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 22:06:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 22:06:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 22:07:00 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.71875 on epoch=799
03/01/2022 22:07:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 22:07:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 22:07:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 22:07:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/01/2022 22:07:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/01/2022 22:07:12 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.6875 on epoch=824
03/01/2022 22:07:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 22:07:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 22:07:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 22:07:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 22:07:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 22:07:25 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.6875 on epoch=849
03/01/2022 22:07:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/01/2022 22:07:29 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 22:07:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 22:07:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 22:07:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
03/01/2022 22:07:37 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.625 on epoch=874
03/01/2022 22:07:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
03/01/2022 22:07:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 22:07:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 22:07:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
03/01/2022 22:07:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 22:07:50 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.6875 on epoch=899
03/01/2022 22:07:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 22:07:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/01/2022 22:07:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 22:07:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 22:08:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 22:08:02 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.625 on epoch=924
03/01/2022 22:08:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/01/2022 22:08:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 22:08:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 22:08:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 22:08:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 22:08:14 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.6875 on epoch=949
03/01/2022 22:08:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 22:08:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 22:08:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 22:08:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 22:08:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 22:08:27 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.59375 on epoch=974
03/01/2022 22:08:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 22:08:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 22:08:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 22:08:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 22:08:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 22:08:39 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.65625 on epoch=999
03/01/2022 22:08:39 - INFO - __main__ - save last model!
03/01/2022 22:08:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 22:08:39 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 22:08:39 - INFO - __main__ - Printing 3 examples
03/01/2022 22:08:39 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 22:08:39 - INFO - __main__ - ['entailment']
03/01/2022 22:08:39 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 22:08:39 - INFO - __main__ - ['not_entailment']
03/01/2022 22:08:39 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 22:08:39 - INFO - __main__ - ['not_entailment']
03/01/2022 22:08:39 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:08:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:08:39 - INFO - __main__ - Printing 3 examples
03/01/2022 22:08:39 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 22:08:39 - INFO - __main__ - ['entailment']
03/01/2022 22:08:39 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 22:08:39 - INFO - __main__ - ['entailment']
03/01/2022 22:08:39 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 22:08:39 - INFO - __main__ - ['entailment']
03/01/2022 22:08:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 22:08:39 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:08:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:08:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:08:39 - INFO - __main__ - Printing 3 examples
03/01/2022 22:08:39 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 22:08:39 - INFO - __main__ - ['entailment']
03/01/2022 22:08:39 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 22:08:39 - INFO - __main__ - ['entailment']
03/01/2022 22:08:39 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 22:08:39 - INFO - __main__ - ['entailment']
03/01/2022 22:08:39 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:08:39 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:08:39 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:08:42 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:08:47 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 22:08:53 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 22:08:53 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(210, 3214246), (956, 507463), (177, 283247), (2600, 228530), (2320, 373103), (2867, 348102), (1756, 567748), (1056, 811824), (3677, 244767), (451, 1996105), (4656, 199279), (2365, 411961), (2118, 458638), (4865, 202967), (3714, 267548), (2401, 307744), (2330, 404909), (2602, 411193), (281, 3369363), (2021, 474295), (4180, 234955), (3604, 286058), (3074, 334210), (799, 1149745), (2942, 333386), (3102, 266483), (6112, 183777), (4526, 234296), (6478, 174299), (1720, 283907), (4, 2855733), (1237, 765840), (2853, 354788), (1738, 508620), (6440, 161231), (2893, 349540), (429, 2065410), (1164, 807916), (6748, 157449), (4838, 214821), (3568, 270815), (3452, 300052), (4891, 214175), (3640, 155110), (2298, 183293), (1268, 763020), (3449, 243414), (6878, 162543), (1389, 480591), (2350, 363461), (530, 1727807), (4735, 214273), (24, 46921718), (3913, 180488), (5858, 173605), (2172, 463811), (3354, 258314), (3961, 216648), (778, 1141371), (3472, 284740), (3377, 299929), (1425, 711960), (3475, 312556), (2180, 184483), (2567, 304604), (355, 230108), (3404, 288313), (4344, 232964), (3047, 316651), (1753, 168938), (901, 693591), (2107, 476561), (4457, 223114), (4036, 247461), (1860, 522151), (4239, 164202), (3320, 312204), (1536, 231332), (4170, 170840), (8, 234707885), (2896, 309135), (3168, 326177), (2018, 434867), (3916, 217282), (6144, 161446), (6551, 154998), (3510, 291560), (5111, 201364), (4764, 211573), (3258, 211425), (2283, 428223), (6848, 183388), (4505, 197810), (1586, 532514), (715, 971516), (1935, 328964), (5835, 181763), (3170, 281056), (1020, 892057)]
03/01/2022 22:08:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:08:54 - INFO - __main__ - Starting training!
03/01/2022 22:12:04 - INFO - __main__ - Saved prediction in models/T5-large/singletask-glue-qnli/glue-qnli_16_100_0.2_8_predictions.txt
03/01/2022 22:12:04 - INFO - __main__ - ACC on test data: 0.5329
03/01/2022 22:12:04 - INFO - __main__ - prefix=glue-qnli_16_100, lr=0.2, bsz=8, dev_performance=0.75, test_performance=0.5328574043565807
03/01/2022 22:12:04 - INFO - __main__ - Running ... prefix=glue-qnli_16_13, lr=0.5, bsz=8 ...
03/01/2022 22:12:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:12:05 - INFO - __main__ - Printing 3 examples
03/01/2022 22:12:05 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 22:12:05 - INFO - __main__ - ['entailment']
03/01/2022 22:12:05 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 22:12:05 - INFO - __main__ - ['entailment']
03/01/2022 22:12:05 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 22:12:05 - INFO - __main__ - ['entailment']
03/01/2022 22:12:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 22:12:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:12:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:12:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:12:05 - INFO - __main__ - Printing 3 examples
03/01/2022 22:12:05 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 22:12:05 - INFO - __main__ - ['entailment']
03/01/2022 22:12:05 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 22:12:05 - INFO - __main__ - ['entailment']
03/01/2022 22:12:05 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 22:12:05 - INFO - __main__ - ['entailment']
03/01/2022 22:12:05 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:12:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:12:05 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:12:19 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 22:12:19 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(210, 3214246), (956, 507463), (177, 283247), (2600, 228530), (2320, 373103), (2867, 348102), (1756, 567748), (1056, 811824), (3677, 244767), (451, 1996105), (4656, 199279), (2365, 411961), (2118, 458638), (4865, 202967), (3714, 267548), (2401, 307744), (2330, 404909), (2602, 411193), (281, 3369363), (2021, 474295), (4180, 234955), (3604, 286058), (3074, 334210), (799, 1149745), (2942, 333386), (3102, 266483), (6112, 183777), (4526, 234296), (6478, 174299), (1720, 283907), (4, 2855733), (1237, 765840), (2853, 354788), (1738, 508620), (6440, 161231), (2893, 349540), (429, 2065410), (1164, 807916), (6748, 157449), (4838, 214821), (3568, 270815), (3452, 300052), (4891, 214175), (3640, 155110), (2298, 183293), (1268, 763020), (3449, 243414), (6878, 162543), (1389, 480591), (2350, 363461), (530, 1727807), (4735, 214273), (24, 46921718), (3913, 180488), (5858, 173605), (2172, 463811), (3354, 258314), (3961, 216648), (778, 1141371), (3472, 284740), (3377, 299929), (1425, 711960), (3475, 312556), (2180, 184483), (2567, 304604), (355, 230108), (3404, 288313), (4344, 232964), (3047, 316651), (1753, 168938), (901, 693591), (2107, 476561), (4457, 223114), (4036, 247461), (1860, 522151), (4239, 164202), (3320, 312204), (1536, 231332), (4170, 170840), (8, 234707885), (2896, 309135), (3168, 326177), (2018, 434867), (3916, 217282), (6144, 161446), (6551, 154998), (3510, 291560), (5111, 201364), (4764, 211573), (3258, 211425), (2283, 428223), (6848, 183388), (4505, 197810), (1586, 532514), (715, 971516), (1935, 328964), (5835, 181763), (3170, 281056), (1020, 892057)]
03/01/2022 22:12:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:12:20 - INFO - __main__ - Starting training!
03/01/2022 22:12:24 - INFO - __main__ - Step 10 Global step 10 Train loss 3.85 on epoch=4
03/01/2022 22:12:26 - INFO - __main__ - Step 20 Global step 20 Train loss 1.06 on epoch=9
03/01/2022 22:12:28 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=14
03/01/2022 22:12:31 - INFO - __main__ - Step 40 Global step 40 Train loss 0.29 on epoch=19
03/01/2022 22:12:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.24 on epoch=24
03/01/2022 22:12:34 - INFO - __main__ - Global step 50 Train loss 1.18 ACC 0.5 on epoch=24
03/01/2022 22:12:34 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 22:12:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.20 on epoch=29
03/01/2022 22:12:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.19 on epoch=34
03/01/2022 22:12:41 - INFO - __main__ - Step 80 Global step 80 Train loss 0.19 on epoch=39
03/01/2022 22:12:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.19 on epoch=44
03/01/2022 22:12:46 - INFO - __main__ - Step 100 Global step 100 Train loss 0.17 on epoch=49
03/01/2022 22:12:47 - INFO - __main__ - Global step 100 Train loss 0.19 ACC 0.5 on epoch=49
03/01/2022 22:12:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.20 on epoch=54
03/01/2022 22:12:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.19 on epoch=59
03/01/2022 22:12:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.16 on epoch=64
03/01/2022 22:12:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.18 on epoch=69
03/01/2022 22:12:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.18 on epoch=74
03/01/2022 22:13:00 - INFO - __main__ - Global step 150 Train loss 0.18 ACC 0.6875 on epoch=74
03/01/2022 22:13:00 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.6875 on epoch=74, global_step=150
03/01/2022 22:13:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.17 on epoch=79
03/01/2022 22:13:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.16 on epoch=84
03/01/2022 22:13:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.15 on epoch=89
03/01/2022 22:13:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.14 on epoch=94
03/01/2022 22:13:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.15 on epoch=99
03/01/2022 22:13:13 - INFO - __main__ - Global step 200 Train loss 0.15 ACC 0.5 on epoch=99
03/01/2022 22:13:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.16 on epoch=104
03/01/2022 22:13:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.17 on epoch=109
03/01/2022 22:13:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.15 on epoch=114
03/01/2022 22:13:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.12 on epoch=119
03/01/2022 22:13:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.14 on epoch=124
03/01/2022 22:13:26 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.53125 on epoch=124
03/01/2022 22:13:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.13 on epoch=129
03/01/2022 22:13:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.14 on epoch=134
03/01/2022 22:13:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.13 on epoch=139
03/01/2022 22:13:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.13 on epoch=144
03/01/2022 22:13:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.15 on epoch=149
03/01/2022 22:13:39 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.53125 on epoch=149
03/01/2022 22:13:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.15 on epoch=154
03/01/2022 22:13:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.13 on epoch=159
03/01/2022 22:13:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.13 on epoch=164
03/01/2022 22:13:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
03/01/2022 22:13:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.13 on epoch=174
03/01/2022 22:13:52 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.5 on epoch=174
03/01/2022 22:13:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=179
03/01/2022 22:13:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.13 on epoch=184
03/01/2022 22:14:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.13 on epoch=189
03/01/2022 22:14:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.12 on epoch=194
03/01/2022 22:14:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.12 on epoch=199
03/01/2022 22:14:05 - INFO - __main__ - Global step 400 Train loss 0.13 ACC 0.5625 on epoch=199
03/01/2022 22:14:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=204
03/01/2022 22:14:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.10 on epoch=209
03/01/2022 22:14:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.10 on epoch=214
03/01/2022 22:14:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.12 on epoch=219
03/01/2022 22:14:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.11 on epoch=224
03/01/2022 22:14:18 - INFO - __main__ - Global step 450 Train loss 0.11 ACC 0.6875 on epoch=224
03/01/2022 22:14:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.12 on epoch=229
03/01/2022 22:14:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=234
03/01/2022 22:14:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=239
03/01/2022 22:14:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
03/01/2022 22:14:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.11 on epoch=249
03/01/2022 22:14:31 - INFO - __main__ - Global step 500 Train loss 0.11 ACC 0.5625 on epoch=249
03/01/2022 22:14:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.11 on epoch=254
03/01/2022 22:14:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=259
03/01/2022 22:14:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=264
03/01/2022 22:14:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=269
03/01/2022 22:14:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.05 on epoch=274
03/01/2022 22:14:44 - INFO - __main__ - Global step 550 Train loss 0.09 ACC 0.59375 on epoch=274
03/01/2022 22:14:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
03/01/2022 22:14:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.11 on epoch=284
03/01/2022 22:14:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
03/01/2022 22:14:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=294
03/01/2022 22:14:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
03/01/2022 22:14:57 - INFO - __main__ - Global step 600 Train loss 0.07 ACC 0.59375 on epoch=299
03/01/2022 22:15:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
03/01/2022 22:15:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=309
03/01/2022 22:15:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
03/01/2022 22:15:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
03/01/2022 22:15:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
03/01/2022 22:15:10 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.5625 on epoch=324
03/01/2022 22:15:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.07 on epoch=329
03/01/2022 22:15:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
03/01/2022 22:15:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
03/01/2022 22:15:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
03/01/2022 22:15:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
03/01/2022 22:15:23 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.5625 on epoch=349
03/01/2022 22:15:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
03/01/2022 22:15:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
03/01/2022 22:15:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
03/01/2022 22:15:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
03/01/2022 22:15:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/01/2022 22:15:36 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.53125 on epoch=374
03/01/2022 22:15:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
03/01/2022 22:15:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/01/2022 22:15:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
03/01/2022 22:15:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/01/2022 22:15:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
03/01/2022 22:15:49 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.6875 on epoch=399
03/01/2022 22:15:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/01/2022 22:15:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
03/01/2022 22:15:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
03/01/2022 22:15:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
03/01/2022 22:16:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/01/2022 22:16:02 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.625 on epoch=424
03/01/2022 22:16:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 22:16:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/01/2022 22:16:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/01/2022 22:16:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/01/2022 22:16:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/01/2022 22:16:15 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.6875 on epoch=449
03/01/2022 22:16:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
03/01/2022 22:16:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
03/01/2022 22:16:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
03/01/2022 22:16:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
03/01/2022 22:16:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
03/01/2022 22:16:28 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.59375 on epoch=474
03/01/2022 22:16:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 22:16:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
03/01/2022 22:16:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 22:16:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 22:16:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 22:16:41 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.5625 on epoch=499
03/01/2022 22:16:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/01/2022 22:16:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
03/01/2022 22:16:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 22:16:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
03/01/2022 22:16:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 22:16:54 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.59375 on epoch=524
03/01/2022 22:16:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 22:16:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
03/01/2022 22:17:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 22:17:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 22:17:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/01/2022 22:17:07 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.75 on epoch=549
03/01/2022 22:17:07 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.75 on epoch=549, global_step=1100
03/01/2022 22:17:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 22:17:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 22:17:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 22:17:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 22:17:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
03/01/2022 22:17:20 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.65625 on epoch=574
03/01/2022 22:17:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/01/2022 22:17:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/01/2022 22:17:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 22:17:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/01/2022 22:17:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 22:17:32 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.71875 on epoch=599
03/01/2022 22:17:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
03/01/2022 22:17:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/01/2022 22:17:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 22:17:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 22:17:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 22:17:45 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.6875 on epoch=624
03/01/2022 22:17:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 22:17:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 22:17:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 22:17:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 22:17:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 22:17:58 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.59375 on epoch=649
03/01/2022 22:18:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/01/2022 22:18:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 22:18:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 22:18:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 22:18:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 22:18:10 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.6875 on epoch=674
03/01/2022 22:18:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 22:18:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 22:18:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 22:18:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 22:18:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 22:18:23 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.625 on epoch=699
03/01/2022 22:18:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 22:18:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 22:18:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 22:18:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 22:18:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 22:18:36 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.59375 on epoch=724
03/01/2022 22:18:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 22:18:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 22:18:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 22:18:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 22:18:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 22:18:48 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.59375 on epoch=749
03/01/2022 22:18:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 22:18:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 22:18:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 22:18:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 22:19:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 22:19:01 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.59375 on epoch=774
03/01/2022 22:19:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 22:19:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 22:19:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 22:19:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 22:19:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 22:19:14 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.5625 on epoch=799
03/01/2022 22:19:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 22:19:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 22:19:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 22:19:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
03/01/2022 22:19:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 22:19:27 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.65625 on epoch=824
03/01/2022 22:19:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 22:19:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 22:19:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 22:19:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 22:19:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 22:19:39 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.71875 on epoch=849
03/01/2022 22:19:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 22:19:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 22:19:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 22:19:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 22:19:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 22:19:52 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.5 on epoch=874
03/01/2022 22:19:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 22:19:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/01/2022 22:19:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 22:20:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 22:20:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 22:20:05 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.5625 on epoch=899
03/01/2022 22:20:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
03/01/2022 22:20:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 22:20:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 22:20:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 22:20:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 22:20:17 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.625 on epoch=924
03/01/2022 22:20:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 22:20:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 22:20:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 22:20:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 22:20:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 22:20:30 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.59375 on epoch=949
03/01/2022 22:20:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 22:20:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 22:20:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 22:20:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 22:20:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 22:20:43 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.59375 on epoch=974
03/01/2022 22:20:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
03/01/2022 22:20:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 22:20:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 22:20:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 22:20:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 22:20:56 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.59375 on epoch=999
03/01/2022 22:20:56 - INFO - __main__ - save last model!
03/01/2022 22:20:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 22:20:56 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 22:20:56 - INFO - __main__ - Printing 3 examples
03/01/2022 22:20:56 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 22:20:56 - INFO - __main__ - ['entailment']
03/01/2022 22:20:56 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 22:20:56 - INFO - __main__ - ['not_entailment']
03/01/2022 22:20:56 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 22:20:56 - INFO - __main__ - ['not_entailment']
03/01/2022 22:20:56 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:20:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:20:56 - INFO - __main__ - Printing 3 examples
03/01/2022 22:20:56 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 22:20:56 - INFO - __main__ - ['entailment']
03/01/2022 22:20:56 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 22:20:56 - INFO - __main__ - ['entailment']
03/01/2022 22:20:56 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 22:20:56 - INFO - __main__ - ['entailment']
03/01/2022 22:20:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 22:20:56 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:20:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:20:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:20:56 - INFO - __main__ - Printing 3 examples
03/01/2022 22:20:56 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 22:20:56 - INFO - __main__ - ['entailment']
03/01/2022 22:20:56 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 22:20:56 - INFO - __main__ - ['entailment']
03/01/2022 22:20:56 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 22:20:56 - INFO - __main__ - ['entailment']
03/01/2022 22:20:56 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:20:56 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:20:56 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:20:59 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:21:04 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 22:21:10 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 22:21:10 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(2502, 393505), (3485, 303196), (2451, 408517), (2124, 458893), (727, 605486), (159, 3283944), (550, 1652637), (2943, 377167), (4577, 197081), (536, 1096599), (4800, 205648), (3374, 227031), (4959, 162645), (2020, 470862), (3016, 244028), (6059, 189253), (3944, 234304), (2651, 378692), (1638, 604699), (62, 15842384), (1130, 821238), (4783, 219874), (1890, 417949), (1929, 517016), (5885, 180884), (5519, 201464), (568, 1556089), (6068, 167828), (2545, 370902), (1310, 731782), (4964, 205248), (1443, 666960), (4999, 201462), (5908, 176655), (4066, 167278), (6883, 157698), (3596, 297061), (3747, 211292), (5364, 185276), (4984, 218955), (4170, 170840), (5020, 210030), (1776, 567810), (5258, 197449), (4949, 208735), (2848, 356857), (2595, 374340), (2349, 335938), (5783, 180158), (5245, 192501), (2287, 388098), (2224, 338862), (3959, 212293), (852, 1044002), (2923, 327246), (1458, 394015), (1963, 341745), (3521, 280332), (2606, 294174), (7609, 173097), (830, 1012157), (968, 645839), (1487, 592557), (2198, 398890), (4674, 244421), (1627, 587044), (2068, 427366), (724, 1234015), (4657, 226628), (4614, 230206), (3467, 284152), (3304, 172257), (3291, 200786), (4441, 226147), (651, 1269510), (2214, 442372), (3497, 223253), (4716, 239808), (6497, 160316), (227, 4080344), (7555, 162623), (3924, 246414), (4666, 195723), (66, 13829035), (3645, 265964), (3236, 314080), (3229, 239354), (4804, 223950), (6162, 173710), (6040, 171279), (6152, 155746), (2428, 415536), (5503, 191100), (2374, 417354), (2949, 340763), (4836, 214321), (3385, 191872), (313, 2745242), (2206, 240868)]
03/01/2022 22:21:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:21:11 - INFO - __main__ - Starting training!
03/01/2022 22:24:31 - INFO - __main__ - Saved prediction in models/T5-large/singletask-glue-qnli/glue-qnli_16_13_0.5_8_predictions.txt
03/01/2022 22:24:31 - INFO - __main__ - ACC on test data: 0.5581
03/01/2022 22:24:32 - INFO - __main__ - prefix=glue-qnli_16_13, lr=0.5, bsz=8, dev_performance=0.75, test_performance=0.5581182500457624
03/01/2022 22:24:32 - INFO - __main__ - Running ... prefix=glue-qnli_16_13, lr=0.4, bsz=8 ...
03/01/2022 22:24:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:24:32 - INFO - __main__ - Printing 3 examples
03/01/2022 22:24:32 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 22:24:32 - INFO - __main__ - ['entailment']
03/01/2022 22:24:32 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 22:24:32 - INFO - __main__ - ['entailment']
03/01/2022 22:24:32 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 22:24:32 - INFO - __main__ - ['entailment']
03/01/2022 22:24:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 22:24:32 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:24:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:24:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:24:33 - INFO - __main__ - Printing 3 examples
03/01/2022 22:24:33 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 22:24:33 - INFO - __main__ - ['entailment']
03/01/2022 22:24:33 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 22:24:33 - INFO - __main__ - ['entailment']
03/01/2022 22:24:33 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 22:24:33 - INFO - __main__ - ['entailment']
03/01/2022 22:24:33 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:24:33 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:24:33 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:24:47 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 22:24:47 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(2502, 393505), (3485, 303196), (2451, 408517), (2124, 458893), (727, 605486), (159, 3283944), (550, 1652637), (2943, 377167), (4577, 197081), (536, 1096599), (4800, 205648), (3374, 227031), (4959, 162645), (2020, 470862), (3016, 244028), (6059, 189253), (3944, 234304), (2651, 378692), (1638, 604699), (62, 15842384), (1130, 821238), (4783, 219874), (1890, 417949), (1929, 517016), (5885, 180884), (5519, 201464), (568, 1556089), (6068, 167828), (2545, 370902), (1310, 731782), (4964, 205248), (1443, 666960), (4999, 201462), (5908, 176655), (4066, 167278), (6883, 157698), (3596, 297061), (3747, 211292), (5364, 185276), (4984, 218955), (4170, 170840), (5020, 210030), (1776, 567810), (5258, 197449), (4949, 208735), (2848, 356857), (2595, 374340), (2349, 335938), (5783, 180158), (5245, 192501), (2287, 388098), (2224, 338862), (3959, 212293), (852, 1044002), (2923, 327246), (1458, 394015), (1963, 341745), (3521, 280332), (2606, 294174), (7609, 173097), (830, 1012157), (968, 645839), (1487, 592557), (2198, 398890), (4674, 244421), (1627, 587044), (2068, 427366), (724, 1234015), (4657, 226628), (4614, 230206), (3467, 284152), (3304, 172257), (3291, 200786), (4441, 226147), (651, 1269510), (2214, 442372), (3497, 223253), (4716, 239808), (6497, 160316), (227, 4080344), (7555, 162623), (3924, 246414), (4666, 195723), (66, 13829035), (3645, 265964), (3236, 314080), (3229, 239354), (4804, 223950), (6162, 173710), (6040, 171279), (6152, 155746), (2428, 415536), (5503, 191100), (2374, 417354), (2949, 340763), (4836, 214321), (3385, 191872), (313, 2745242), (2206, 240868)]
03/01/2022 22:24:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:24:48 - INFO - __main__ - Starting training!
03/01/2022 22:24:54 - INFO - __main__ - Step 10 Global step 10 Train loss 4.57 on epoch=4
03/01/2022 22:24:56 - INFO - __main__ - Step 20 Global step 20 Train loss 1.47 on epoch=9
03/01/2022 22:24:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=14
03/01/2022 22:25:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.38 on epoch=19
03/01/2022 22:25:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.28 on epoch=24
03/01/2022 22:25:04 - INFO - __main__ - Global step 50 Train loss 1.45 ACC 0.5 on epoch=24
03/01/2022 22:25:04 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 22:25:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.27 on epoch=29
03/01/2022 22:25:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.22 on epoch=34
03/01/2022 22:25:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.25 on epoch=39
03/01/2022 22:25:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.17 on epoch=44
03/01/2022 22:25:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.22 on epoch=49
03/01/2022 22:25:17 - INFO - __main__ - Global step 100 Train loss 0.22 ACC 0.5 on epoch=49
03/01/2022 22:25:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.15 on epoch=54
03/01/2022 22:25:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.18 on epoch=59
03/01/2022 22:25:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.17 on epoch=64
03/01/2022 22:25:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.17 on epoch=69
03/01/2022 22:25:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.16 on epoch=74
03/01/2022 22:25:30 - INFO - __main__ - Global step 150 Train loss 0.17 ACC 0.5 on epoch=74
03/01/2022 22:25:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.17 on epoch=79
03/01/2022 22:25:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.16 on epoch=84
03/01/2022 22:25:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.17 on epoch=89
03/01/2022 22:25:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.15 on epoch=94
03/01/2022 22:25:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.14 on epoch=99
03/01/2022 22:25:43 - INFO - __main__ - Global step 200 Train loss 0.16 ACC 0.5 on epoch=99
03/01/2022 22:25:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.15 on epoch=104
03/01/2022 22:25:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.16 on epoch=109
03/01/2022 22:25:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.17 on epoch=114
03/01/2022 22:25:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.15 on epoch=119
03/01/2022 22:25:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.15 on epoch=124
03/01/2022 22:25:55 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.5 on epoch=124
03/01/2022 22:25:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.16 on epoch=129
03/01/2022 22:26:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.13 on epoch=134
03/01/2022 22:26:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.15 on epoch=139
03/01/2022 22:26:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.14 on epoch=144
03/01/2022 22:26:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.13 on epoch=149
03/01/2022 22:26:08 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.53125 on epoch=149
03/01/2022 22:26:08 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=149, global_step=300
03/01/2022 22:26:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
03/01/2022 22:26:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.13 on epoch=159
03/01/2022 22:26:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.13 on epoch=164
03/01/2022 22:26:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.13 on epoch=169
03/01/2022 22:26:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.14 on epoch=174
03/01/2022 22:26:21 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.53125 on epoch=174
03/01/2022 22:26:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
03/01/2022 22:26:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
03/01/2022 22:26:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.14 on epoch=189
03/01/2022 22:26:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.14 on epoch=194
03/01/2022 22:26:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.12 on epoch=199
03/01/2022 22:26:34 - INFO - __main__ - Global step 400 Train loss 0.13 ACC 0.5 on epoch=199
03/01/2022 22:26:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.14 on epoch=204
03/01/2022 22:26:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.14 on epoch=209
03/01/2022 22:26:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.11 on epoch=214
03/01/2022 22:26:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
03/01/2022 22:26:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.10 on epoch=224
03/01/2022 22:26:46 - INFO - __main__ - Global step 450 Train loss 0.12 ACC 0.625 on epoch=224
03/01/2022 22:26:46 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.625 on epoch=224, global_step=450
03/01/2022 22:26:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.11 on epoch=229
03/01/2022 22:26:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.10 on epoch=234
03/01/2022 22:26:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.08 on epoch=239
03/01/2022 22:26:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.10 on epoch=244
03/01/2022 22:26:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=249
03/01/2022 22:26:59 - INFO - __main__ - Global step 500 Train loss 0.10 ACC 0.53125 on epoch=249
03/01/2022 22:27:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=254
03/01/2022 22:27:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=259
03/01/2022 22:27:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=264
03/01/2022 22:27:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
03/01/2022 22:27:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
03/01/2022 22:27:12 - INFO - __main__ - Global step 550 Train loss 0.08 ACC 0.5625 on epoch=274
03/01/2022 22:27:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=279
03/01/2022 22:27:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=284
03/01/2022 22:27:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
03/01/2022 22:27:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
03/01/2022 22:27:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
03/01/2022 22:27:25 - INFO - __main__ - Global step 600 Train loss 0.06 ACC 0.625 on epoch=299
03/01/2022 22:27:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=304
03/01/2022 22:27:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
03/01/2022 22:27:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
03/01/2022 22:27:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
03/01/2022 22:27:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
03/01/2022 22:27:37 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.6875 on epoch=324
03/01/2022 22:27:37 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.6875 on epoch=324, global_step=650
03/01/2022 22:27:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
03/01/2022 22:27:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/01/2022 22:27:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
03/01/2022 22:27:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
03/01/2022 22:27:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
03/01/2022 22:27:50 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.59375 on epoch=349
03/01/2022 22:27:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=354
03/01/2022 22:27:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
03/01/2022 22:27:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
03/01/2022 22:28:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
03/01/2022 22:28:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/01/2022 22:28:03 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.53125 on epoch=374
03/01/2022 22:28:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
03/01/2022 22:28:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
03/01/2022 22:28:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
03/01/2022 22:28:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/01/2022 22:28:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/01/2022 22:28:16 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.5625 on epoch=399
03/01/2022 22:28:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
03/01/2022 22:28:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
03/01/2022 22:28:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
03/01/2022 22:28:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/01/2022 22:28:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/01/2022 22:28:29 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.65625 on epoch=424
03/01/2022 22:28:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
03/01/2022 22:28:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/01/2022 22:28:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/01/2022 22:28:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
03/01/2022 22:28:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/01/2022 22:28:41 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.59375 on epoch=449
03/01/2022 22:28:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 22:28:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
03/01/2022 22:28:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/01/2022 22:28:51 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
03/01/2022 22:28:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 22:28:54 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.625 on epoch=474
03/01/2022 22:28:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 22:28:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 22:29:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 22:29:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
03/01/2022 22:29:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
03/01/2022 22:29:07 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.65625 on epoch=499
03/01/2022 22:29:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 22:29:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/01/2022 22:29:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/01/2022 22:29:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 22:29:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
03/01/2022 22:29:20 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.59375 on epoch=524
03/01/2022 22:29:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 22:29:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/01/2022 22:29:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 22:29:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 22:29:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/01/2022 22:29:33 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.53125 on epoch=549
03/01/2022 22:29:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
03/01/2022 22:29:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 22:29:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 22:29:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
03/01/2022 22:29:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 22:29:46 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.53125 on epoch=574
03/01/2022 22:29:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
03/01/2022 22:29:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/01/2022 22:29:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 22:29:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 22:29:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 22:30:00 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.5625 on epoch=599
03/01/2022 22:30:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/01/2022 22:30:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 22:30:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 22:30:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 22:30:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 22:30:13 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.5625 on epoch=624
03/01/2022 22:30:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 22:30:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 22:30:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 22:30:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 22:30:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
03/01/2022 22:30:26 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.65625 on epoch=649
03/01/2022 22:30:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 22:30:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 22:30:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 22:30:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 22:30:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 22:30:39 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.59375 on epoch=674
03/01/2022 22:30:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 22:30:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 22:30:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 22:30:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 22:30:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 22:30:52 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.5625 on epoch=699
03/01/2022 22:30:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 22:30:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 22:30:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 22:31:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 22:31:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 22:31:05 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.5625 on epoch=724
03/01/2022 22:31:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 22:31:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 22:31:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 22:31:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/01/2022 22:31:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 22:31:18 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.53125 on epoch=749
03/01/2022 22:31:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 22:31:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 22:31:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 22:31:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 22:31:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=774
03/01/2022 22:31:31 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.53125 on epoch=774
03/01/2022 22:31:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 22:31:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 22:31:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 22:31:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 22:31:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 22:31:44 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.59375 on epoch=799
03/01/2022 22:31:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 22:31:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 22:31:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 22:31:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 22:31:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 22:31:57 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.5625 on epoch=824
03/01/2022 22:31:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 22:32:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/01/2022 22:32:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 22:32:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=844
03/01/2022 22:32:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/01/2022 22:32:10 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.625 on epoch=849
03/01/2022 22:32:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 22:32:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 22:32:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 22:32:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 22:32:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 22:32:23 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.53125 on epoch=874
03/01/2022 22:32:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 22:32:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 22:32:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 22:32:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 22:32:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 22:32:36 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.5 on epoch=899
03/01/2022 22:32:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
03/01/2022 22:32:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 22:32:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 22:32:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 22:32:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 22:32:49 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.53125 on epoch=924
03/01/2022 22:32:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 22:32:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 22:32:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 22:32:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 22:33:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 22:33:02 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.625 on epoch=949
03/01/2022 22:33:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/01/2022 22:33:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 22:33:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 22:33:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 22:33:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 22:33:15 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.46875 on epoch=974
03/01/2022 22:33:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 22:33:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 22:33:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
03/01/2022 22:33:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
03/01/2022 22:33:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 22:33:28 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.5625 on epoch=999
03/01/2022 22:33:28 - INFO - __main__ - save last model!
03/01/2022 22:33:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 22:33:28 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 22:33:28 - INFO - __main__ - Printing 3 examples
03/01/2022 22:33:28 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 22:33:28 - INFO - __main__ - ['entailment']
03/01/2022 22:33:28 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 22:33:28 - INFO - __main__ - ['not_entailment']
03/01/2022 22:33:28 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 22:33:28 - INFO - __main__ - ['not_entailment']
03/01/2022 22:33:28 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:33:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:33:28 - INFO - __main__ - Printing 3 examples
03/01/2022 22:33:28 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 22:33:28 - INFO - __main__ - ['entailment']
03/01/2022 22:33:28 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 22:33:28 - INFO - __main__ - ['entailment']
03/01/2022 22:33:28 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 22:33:28 - INFO - __main__ - ['entailment']
03/01/2022 22:33:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 22:33:28 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:33:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:33:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:33:28 - INFO - __main__ - Printing 3 examples
03/01/2022 22:33:28 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 22:33:28 - INFO - __main__ - ['entailment']
03/01/2022 22:33:28 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 22:33:28 - INFO - __main__ - ['entailment']
03/01/2022 22:33:28 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 22:33:28 - INFO - __main__ - ['entailment']
03/01/2022 22:33:28 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:33:28 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:33:28 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:33:31 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:33:36 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 22:33:41 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 22:33:41 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(3666, 279856), (4279, 231910), (1376, 549699), (5099, 200518), (1480, 693138), (5956, 175083), (317, 2909017), (3859, 237931), (6537, 158593), (6422, 165641), (328, 2897449), (915, 980631), (6366, 217988), (1303, 658549), (5169, 202438), (1884, 502256), (568, 1556089), (1489, 355866), (4036, 247461), (2445, 287205), (2154, 432132), (5074, 206086), (3513, 287362), (2658, 279047), (2412, 334412), (2660, 222403), (2656, 367203), (882, 1004086), (4935, 199405), (115, 4101345), (5058, 173227), (593, 1511490), (3830, 267200), (2935, 409303), (726, 1159344), (464, 1931312), (262, 2562554), (2595, 374340), (2114, 458811), (231, 4022141), (1689, 588233), (2627, 389531), (1048, 702299), (5656, 197667), (1341, 767639), (4837, 165767), (2418, 420267), (5111, 201364), (2433, 361242), (3976, 254648), (1801, 537800), (3849, 155187), (6553, 154915), (1214, 767509), (1682, 553032), (3355, 302330), (1056, 811824), (5087, 161786), (196, 3104995), (2464, 301408), (7555, 162623), (3004, 249813), (4349, 236587), (1978, 460084), (448, 1064987), (5917, 158493), (2612, 380946), (1078, 844362), (2715, 311751), (5798, 156482), (1137, 734426), (6326, 165064), (456, 1871965), (3146, 269081), (1703, 175726), (4773, 218071), (3699, 253359), (746, 1156068), (5696, 184838), (3669, 275693), (165, 6245475), (3497, 223253), (4240, 190278), (1195, 825391), (400, 215271), (2938, 259506), (3095, 203292), (1433, 586613), (4400, 214423), (1995, 516280), (5706, 180057), (2953, 344008), (3396, 173723), (3557, 193090), (5002, 201395), (4365, 214420), (4936, 158408), (2855, 345487), (3008, 291537)]
03/01/2022 22:33:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:33:42 - INFO - __main__ - Starting training!
03/01/2022 22:36:25 - INFO - __main__ - Saved prediction in models/T5-large/singletask-glue-qnli/glue-qnli_16_13_0.4_8_predictions.txt
03/01/2022 22:36:25 - INFO - __main__ - ACC on test data: 0.5561
03/01/2022 22:36:26 - INFO - __main__ - prefix=glue-qnli_16_13, lr=0.4, bsz=8, dev_performance=0.6875, test_performance=0.5561047043748856
03/01/2022 22:36:26 - INFO - __main__ - Running ... prefix=glue-qnli_16_13, lr=0.3, bsz=8 ...
03/01/2022 22:36:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:36:27 - INFO - __main__ - Printing 3 examples
03/01/2022 22:36:27 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 22:36:27 - INFO - __main__ - ['entailment']
03/01/2022 22:36:27 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 22:36:27 - INFO - __main__ - ['entailment']
03/01/2022 22:36:27 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 22:36:27 - INFO - __main__ - ['entailment']
03/01/2022 22:36:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 22:36:27 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:36:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:36:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:36:27 - INFO - __main__ - Printing 3 examples
03/01/2022 22:36:27 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 22:36:27 - INFO - __main__ - ['entailment']
03/01/2022 22:36:27 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 22:36:27 - INFO - __main__ - ['entailment']
03/01/2022 22:36:27 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 22:36:27 - INFO - __main__ - ['entailment']
03/01/2022 22:36:27 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:36:27 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:36:27 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:36:39 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 22:36:39 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(3666, 279856), (4279, 231910), (1376, 549699), (5099, 200518), (1480, 693138), (5956, 175083), (317, 2909017), (3859, 237931), (6537, 158593), (6422, 165641), (328, 2897449), (915, 980631), (6366, 217988), (1303, 658549), (5169, 202438), (1884, 502256), (568, 1556089), (1489, 355866), (4036, 247461), (2445, 287205), (2154, 432132), (5074, 206086), (3513, 287362), (2658, 279047), (2412, 334412), (2660, 222403), (2656, 367203), (882, 1004086), (4935, 199405), (115, 4101345), (5058, 173227), (593, 1511490), (3830, 267200), (2935, 409303), (726, 1159344), (464, 1931312), (262, 2562554), (2595, 374340), (2114, 458811), (231, 4022141), (1689, 588233), (2627, 389531), (1048, 702299), (5656, 197667), (1341, 767639), (4837, 165767), (2418, 420267), (5111, 201364), (2433, 361242), (3976, 254648), (1801, 537800), (3849, 155187), (6553, 154915), (1214, 767509), (1682, 553032), (3355, 302330), (1056, 811824), (5087, 161786), (196, 3104995), (2464, 301408), (7555, 162623), (3004, 249813), (4349, 236587), (1978, 460084), (448, 1064987), (5917, 158493), (2612, 380946), (1078, 844362), (2715, 311751), (5798, 156482), (1137, 734426), (6326, 165064), (456, 1871965), (3146, 269081), (1703, 175726), (4773, 218071), (3699, 253359), (746, 1156068), (5696, 184838), (3669, 275693), (165, 6245475), (3497, 223253), (4240, 190278), (1195, 825391), (400, 215271), (2938, 259506), (3095, 203292), (1433, 586613), (4400, 214423), (1995, 516280), (5706, 180057), (2953, 344008), (3396, 173723), (3557, 193090), (5002, 201395), (4365, 214420), (4936, 158408), (2855, 345487), (3008, 291537)]
03/01/2022 22:36:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:36:40 - INFO - __main__ - Starting training!
03/01/2022 22:36:42 - INFO - __main__ - Step 10 Global step 10 Train loss 4.38 on epoch=4
03/01/2022 22:36:45 - INFO - __main__ - Step 20 Global step 20 Train loss 1.53 on epoch=9
03/01/2022 22:36:47 - INFO - __main__ - Step 30 Global step 30 Train loss 0.73 on epoch=14
03/01/2022 22:36:49 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=19
03/01/2022 22:36:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.34 on epoch=24
03/01/2022 22:36:53 - INFO - __main__ - Global step 50 Train loss 1.50 ACC 0.46875 on epoch=24
03/01/2022 22:36:53 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.46875 on epoch=24, global_step=50
03/01/2022 22:36:55 - INFO - __main__ - Step 60 Global step 60 Train loss 0.29 on epoch=29
03/01/2022 22:36:57 - INFO - __main__ - Step 70 Global step 70 Train loss 0.25 on epoch=34
03/01/2022 22:37:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.29 on epoch=39
03/01/2022 22:37:02 - INFO - __main__ - Step 90 Global step 90 Train loss 0.21 on epoch=44
03/01/2022 22:37:04 - INFO - __main__ - Step 100 Global step 100 Train loss 0.22 on epoch=49
03/01/2022 22:37:05 - INFO - __main__ - Global step 100 Train loss 0.25 ACC 0.5 on epoch=49
03/01/2022 22:37:05 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=49, global_step=100
03/01/2022 22:37:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.17 on epoch=54
03/01/2022 22:37:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.19 on epoch=59
03/01/2022 22:37:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.24 on epoch=64
03/01/2022 22:37:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.15 on epoch=69
03/01/2022 22:37:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.19 on epoch=74
03/01/2022 22:37:18 - INFO - __main__ - Global step 150 Train loss 0.19 ACC 0.5 on epoch=74
03/01/2022 22:37:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.19 on epoch=79
03/01/2022 22:37:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.18 on epoch=84
03/01/2022 22:37:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.17 on epoch=89
03/01/2022 22:37:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.18 on epoch=94
03/01/2022 22:37:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.18 on epoch=99
03/01/2022 22:37:30 - INFO - __main__ - Global step 200 Train loss 0.18 ACC 0.5 on epoch=99
03/01/2022 22:37:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.16 on epoch=104
03/01/2022 22:37:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.18 on epoch=109
03/01/2022 22:37:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.17 on epoch=114
03/01/2022 22:37:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.16 on epoch=119
03/01/2022 22:37:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.17 on epoch=124
03/01/2022 22:37:43 - INFO - __main__ - Global step 250 Train loss 0.17 ACC 0.53125 on epoch=124
03/01/2022 22:37:43 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=124, global_step=250
03/01/2022 22:37:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.18 on epoch=129
03/01/2022 22:37:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
03/01/2022 22:37:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.15 on epoch=139
03/01/2022 22:37:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.14 on epoch=144
03/01/2022 22:37:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.15 on epoch=149
03/01/2022 22:37:56 - INFO - __main__ - Global step 300 Train loss 0.15 ACC 0.5 on epoch=149
03/01/2022 22:37:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.15 on epoch=154
03/01/2022 22:38:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.14 on epoch=159
03/01/2022 22:38:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.16 on epoch=164
03/01/2022 22:38:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.15 on epoch=169
03/01/2022 22:38:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.14 on epoch=174
03/01/2022 22:38:08 - INFO - __main__ - Global step 350 Train loss 0.15 ACC 0.5625 on epoch=174
03/01/2022 22:38:08 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=174, global_step=350
03/01/2022 22:38:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.15 on epoch=179
03/01/2022 22:38:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
03/01/2022 22:38:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.14 on epoch=189
03/01/2022 22:38:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.15 on epoch=194
03/01/2022 22:38:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.13 on epoch=199
03/01/2022 22:38:21 - INFO - __main__ - Global step 400 Train loss 0.14 ACC 0.53125 on epoch=199
03/01/2022 22:38:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.15 on epoch=204
03/01/2022 22:38:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.13 on epoch=209
03/01/2022 22:38:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.14 on epoch=214
03/01/2022 22:38:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.13 on epoch=219
03/01/2022 22:38:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.15 on epoch=224
03/01/2022 22:38:33 - INFO - __main__ - Global step 450 Train loss 0.14 ACC 0.5 on epoch=224
03/01/2022 22:38:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=229
03/01/2022 22:38:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=234
03/01/2022 22:38:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=239
03/01/2022 22:38:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=244
03/01/2022 22:38:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=249
03/01/2022 22:38:46 - INFO - __main__ - Global step 500 Train loss 0.14 ACC 0.53125 on epoch=249
03/01/2022 22:38:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.13 on epoch=254
03/01/2022 22:38:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=259
03/01/2022 22:38:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=264
03/01/2022 22:38:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=269
03/01/2022 22:38:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=274
03/01/2022 22:38:58 - INFO - __main__ - Global step 550 Train loss 0.12 ACC 0.53125 on epoch=274
03/01/2022 22:39:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=279
03/01/2022 22:39:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=284
03/01/2022 22:39:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.12 on epoch=289
03/01/2022 22:39:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=294
03/01/2022 22:39:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=299
03/01/2022 22:39:11 - INFO - __main__ - Global step 600 Train loss 0.13 ACC 0.59375 on epoch=299
03/01/2022 22:39:11 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=299, global_step=600
03/01/2022 22:39:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=304
03/01/2022 22:39:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=309
03/01/2022 22:39:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=314
03/01/2022 22:39:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=319
03/01/2022 22:39:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=324
03/01/2022 22:39:24 - INFO - __main__ - Global step 650 Train loss 0.12 ACC 0.5625 on epoch=324
03/01/2022 22:39:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=329
03/01/2022 22:39:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=334
03/01/2022 22:39:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=339
03/01/2022 22:39:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
03/01/2022 22:39:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.10 on epoch=349
03/01/2022 22:39:36 - INFO - __main__ - Global step 700 Train loss 0.10 ACC 0.5625 on epoch=349
03/01/2022 22:39:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=354
03/01/2022 22:39:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=359
03/01/2022 22:39:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=364
03/01/2022 22:39:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
03/01/2022 22:39:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=374
03/01/2022 22:39:49 - INFO - __main__ - Global step 750 Train loss 0.10 ACC 0.59375 on epoch=374
03/01/2022 22:39:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
03/01/2022 22:39:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=384
03/01/2022 22:39:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=389
03/01/2022 22:39:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=394
03/01/2022 22:40:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=399
03/01/2022 22:40:02 - INFO - __main__ - Global step 800 Train loss 0.10 ACC 0.5625 on epoch=399
03/01/2022 22:40:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=404
03/01/2022 22:40:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=409
03/01/2022 22:40:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=414
03/01/2022 22:40:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=419
03/01/2022 22:40:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=424
03/01/2022 22:40:14 - INFO - __main__ - Global step 850 Train loss 0.09 ACC 0.71875 on epoch=424
03/01/2022 22:40:15 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.71875 on epoch=424, global_step=850
03/01/2022 22:40:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
03/01/2022 22:40:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=434
03/01/2022 22:40:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=439
03/01/2022 22:40:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=444
03/01/2022 22:40:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
03/01/2022 22:40:27 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.65625 on epoch=449
03/01/2022 22:40:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
03/01/2022 22:40:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=459
03/01/2022 22:40:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
03/01/2022 22:40:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
03/01/2022 22:40:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
03/01/2022 22:40:40 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.625 on epoch=474
03/01/2022 22:40:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/01/2022 22:40:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
03/01/2022 22:40:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
03/01/2022 22:40:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
03/01/2022 22:40:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/01/2022 22:40:52 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.59375 on epoch=499
03/01/2022 22:40:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/01/2022 22:40:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
03/01/2022 22:40:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 22:41:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 22:41:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
03/01/2022 22:41:05 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.59375 on epoch=524
03/01/2022 22:41:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/01/2022 22:41:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/01/2022 22:41:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 22:41:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
03/01/2022 22:41:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 22:41:18 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.59375 on epoch=549
03/01/2022 22:41:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
03/01/2022 22:41:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
03/01/2022 22:41:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 22:41:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
03/01/2022 22:41:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 22:41:30 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.59375 on epoch=574
03/01/2022 22:41:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/01/2022 22:41:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 22:41:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 22:41:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 22:41:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 22:41:43 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.5625 on epoch=599
03/01/2022 22:41:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/01/2022 22:41:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/01/2022 22:41:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 22:41:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 22:41:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 22:41:55 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.53125 on epoch=624
03/01/2022 22:41:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 22:42:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
03/01/2022 22:42:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 22:42:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 22:42:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 22:42:08 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.625 on epoch=649
03/01/2022 22:42:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 22:42:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 22:42:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 22:42:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
03/01/2022 22:42:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 22:42:21 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.65625 on epoch=674
03/01/2022 22:42:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 22:42:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 22:42:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 22:42:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 22:42:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 22:42:33 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.625 on epoch=699
03/01/2022 22:42:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 22:42:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 22:42:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
03/01/2022 22:42:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 22:42:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/01/2022 22:42:46 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.5625 on epoch=724
03/01/2022 22:42:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 22:42:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 22:42:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 22:42:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 22:42:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 22:42:59 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.625 on epoch=749
03/01/2022 22:43:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 22:43:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 22:43:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 22:43:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/01/2022 22:43:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/01/2022 22:43:11 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.65625 on epoch=774
03/01/2022 22:43:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 22:43:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 22:43:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/01/2022 22:43:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 22:43:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 22:43:24 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.59375 on epoch=799
03/01/2022 22:43:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 22:43:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 22:43:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 22:43:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 22:43:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 22:43:37 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.625 on epoch=824
03/01/2022 22:43:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/01/2022 22:43:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 22:43:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 22:43:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 22:43:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 22:43:49 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.625 on epoch=849
03/01/2022 22:43:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 22:43:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/01/2022 22:43:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 22:43:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 22:44:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 22:44:02 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.65625 on epoch=874
03/01/2022 22:44:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 22:44:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 22:44:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 22:44:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 22:44:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/01/2022 22:44:15 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.65625 on epoch=899
03/01/2022 22:44:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
03/01/2022 22:44:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 22:44:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 22:44:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 22:44:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/01/2022 22:44:27 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.625 on epoch=924
03/01/2022 22:44:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 22:44:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 22:44:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 22:44:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 22:44:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 22:44:40 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.65625 on epoch=949
03/01/2022 22:44:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 22:44:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 22:44:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 22:44:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 22:44:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 22:44:52 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.65625 on epoch=974
03/01/2022 22:44:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 22:44:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 22:44:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 22:45:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 22:45:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 22:45:05 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.625 on epoch=999
03/01/2022 22:45:05 - INFO - __main__ - save last model!
03/01/2022 22:45:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 22:45:05 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 22:45:05 - INFO - __main__ - Printing 3 examples
03/01/2022 22:45:05 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 22:45:05 - INFO - __main__ - ['entailment']
03/01/2022 22:45:05 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 22:45:05 - INFO - __main__ - ['not_entailment']
03/01/2022 22:45:05 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 22:45:05 - INFO - __main__ - ['not_entailment']
03/01/2022 22:45:05 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:45:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:45:05 - INFO - __main__ - Printing 3 examples
03/01/2022 22:45:05 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 22:45:05 - INFO - __main__ - ['entailment']
03/01/2022 22:45:05 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 22:45:05 - INFO - __main__ - ['entailment']
03/01/2022 22:45:05 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 22:45:05 - INFO - __main__ - ['entailment']
03/01/2022 22:45:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 22:45:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:45:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:45:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:45:05 - INFO - __main__ - Printing 3 examples
03/01/2022 22:45:05 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 22:45:05 - INFO - __main__ - ['entailment']
03/01/2022 22:45:05 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 22:45:05 - INFO - __main__ - ['entailment']
03/01/2022 22:45:05 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 22:45:05 - INFO - __main__ - ['entailment']
03/01/2022 22:45:05 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:45:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:45:05 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:45:08 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:45:13 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 22:45:18 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 22:45:18 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(1074, 376286), (672, 948582), (3489, 333521), (6319, 208284), (2690, 378727), (4999, 201462), (4302, 163912), (4946, 154063), (4264, 247591), (3735, 279578), (286, 2755883), (6059, 189253), (3679, 289187), (1958, 502002), (4233, 191191), (2476, 431307), (3339, 264161), (2818, 358712), (3812, 276017), (2550, 332246), (6597, 157538), (1047, 336403), (5997, 168391), (96, 7894015), (5806, 181362), (2027, 478074), (793, 959415), (2480, 384879), (4374, 230866), (3351, 190868), (6096, 168646), (2144, 386724), (3519, 196508), (4992, 190172), (4493, 209311), (106, 4519798), (928, 894961), (1923, 318040), (2361, 413361), (4279, 231910), (900, 380561), (2466, 307035), (6330, 161028), (3939, 254546), (4505, 197810), (5847, 169158), (4974, 176708), (3742, 272355), (3643, 219730), (4895, 169946), (3087, 284156), (2977, 265914), (3683, 206457), (2772, 342861), (3413, 306732), (2721, 367851), (2431, 400094), (1327, 735060), (1602, 475410), (2733, 298680), (1228, 739170), (4132, 174353), (2023, 494514), (2503, 477739), (2299, 422736), (93, 192649), (2957, 336282), (6637, 158975), (1665, 179358), (4163, 157427), (1635, 329909), (1116, 852428), (2082, 471031), (3118, 315958), (2028, 404951), (4197, 260190), (956, 507463), (3505, 311150), (164, 5765797), (5956, 175083), (1305, 698278), (2879, 339182), (503, 1684486), (600, 1458762), (6854, 169166), (3202, 321048), (1048, 702299), (4621, 190154), (1087, 843077), (131, 6653166), (6292, 163131), (3412, 328424), (5201, 199644), (3769, 195111), (5001, 211928), (3879, 275863), (2016, 495776), (581, 1531361), (2912, 367106)]
03/01/2022 22:45:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:45:19 - INFO - __main__ - Starting training!
03/01/2022 22:48:19 - INFO - __main__ - Saved prediction in models/T5-large/singletask-glue-qnli/glue-qnli_16_13_0.3_8_predictions.txt
03/01/2022 22:48:19 - INFO - __main__ - ACC on test data: 0.5629
03/01/2022 22:48:20 - INFO - __main__ - prefix=glue-qnli_16_13, lr=0.3, bsz=8, dev_performance=0.71875, test_performance=0.5628775398132894
03/01/2022 22:48:20 - INFO - __main__ - Running ... prefix=glue-qnli_16_13, lr=0.2, bsz=8 ...
03/01/2022 22:48:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:48:21 - INFO - __main__ - Printing 3 examples
03/01/2022 22:48:21 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 22:48:21 - INFO - __main__ - ['entailment']
03/01/2022 22:48:21 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 22:48:21 - INFO - __main__ - ['entailment']
03/01/2022 22:48:21 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 22:48:21 - INFO - __main__ - ['entailment']
03/01/2022 22:48:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 22:48:21 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:48:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:48:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:48:21 - INFO - __main__ - Printing 3 examples
03/01/2022 22:48:21 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 22:48:21 - INFO - __main__ - ['entailment']
03/01/2022 22:48:21 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 22:48:21 - INFO - __main__ - ['entailment']
03/01/2022 22:48:21 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 22:48:21 - INFO - __main__ - ['entailment']
03/01/2022 22:48:21 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:48:21 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:48:21 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:48:33 - INFO - __main__ - try to initialize prompt embeddings
03/01/2022 22:48:33 - INFO - __main__ - task name: glue-qnli
initialize from c4
[(1074, 376286), (672, 948582), (3489, 333521), (6319, 208284), (2690, 378727), (4999, 201462), (4302, 163912), (4946, 154063), (4264, 247591), (3735, 279578), (286, 2755883), (6059, 189253), (3679, 289187), (1958, 502002), (4233, 191191), (2476, 431307), (3339, 264161), (2818, 358712), (3812, 276017), (2550, 332246), (6597, 157538), (1047, 336403), (5997, 168391), (96, 7894015), (5806, 181362), (2027, 478074), (793, 959415), (2480, 384879), (4374, 230866), (3351, 190868), (6096, 168646), (2144, 386724), (3519, 196508), (4992, 190172), (4493, 209311), (106, 4519798), (928, 894961), (1923, 318040), (2361, 413361), (4279, 231910), (900, 380561), (2466, 307035), (6330, 161028), (3939, 254546), (4505, 197810), (5847, 169158), (4974, 176708), (3742, 272355), (3643, 219730), (4895, 169946), (3087, 284156), (2977, 265914), (3683, 206457), (2772, 342861), (3413, 306732), (2721, 367851), (2431, 400094), (1327, 735060), (1602, 475410), (2733, 298680), (1228, 739170), (4132, 174353), (2023, 494514), (2503, 477739), (2299, 422736), (93, 192649), (2957, 336282), (6637, 158975), (1665, 179358), (4163, 157427), (1635, 329909), (1116, 852428), (2082, 471031), (3118, 315958), (2028, 404951), (4197, 260190), (956, 507463), (3505, 311150), (164, 5765797), (5956, 175083), (1305, 698278), (2879, 339182), (503, 1684486), (600, 1458762), (6854, 169166), (3202, 321048), (1048, 702299), (4621, 190154), (1087, 843077), (131, 6653166), (6292, 163131), (3412, 328424), (5201, 199644), (3769, 195111), (5001, 211928), (3879, 275863), (2016, 495776), (581, 1531361), (2912, 367106)]
03/01/2022 22:48:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:48:34 - INFO - __main__ - Starting training!
03/01/2022 22:48:37 - INFO - __main__ - Step 10 Global step 10 Train loss 5.11 on epoch=4
03/01/2022 22:48:39 - INFO - __main__ - Step 20 Global step 20 Train loss 2.96 on epoch=9
03/01/2022 22:48:41 - INFO - __main__ - Step 30 Global step 30 Train loss 1.40 on epoch=14
03/01/2022 22:48:44 - INFO - __main__ - Step 40 Global step 40 Train loss 0.78 on epoch=19
03/01/2022 22:48:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=24
03/01/2022 22:48:47 - INFO - __main__ - Global step 50 Train loss 2.15 ACC 0.5 on epoch=24
03/01/2022 22:48:47 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 22:48:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=29
03/01/2022 22:48:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.33 on epoch=34
03/01/2022 22:48:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.30 on epoch=39
03/01/2022 22:48:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.26 on epoch=44
03/01/2022 22:48:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.24 on epoch=49
03/01/2022 22:49:00 - INFO - __main__ - Global step 100 Train loss 0.31 ACC 0.5 on epoch=49
03/01/2022 22:49:02 - INFO - __main__ - Step 110 Global step 110 Train loss 0.20 on epoch=54
03/01/2022 22:49:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.22 on epoch=59
03/01/2022 22:49:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.18 on epoch=64
03/01/2022 22:49:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.20 on epoch=69
03/01/2022 22:49:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.19 on epoch=74
03/01/2022 22:49:12 - INFO - __main__ - Global step 150 Train loss 0.20 ACC 0.5 on epoch=74
03/01/2022 22:49:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.19 on epoch=79
03/01/2022 22:49:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.19 on epoch=84
03/01/2022 22:49:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.16 on epoch=89
03/01/2022 22:49:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.20 on epoch=94
03/01/2022 22:49:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.16 on epoch=99
03/01/2022 22:49:25 - INFO - __main__ - Global step 200 Train loss 0.18 ACC 0.5 on epoch=99
03/01/2022 22:49:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.16 on epoch=104
03/01/2022 22:49:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.17 on epoch=109
03/01/2022 22:49:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.16 on epoch=114
03/01/2022 22:49:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.17 on epoch=119
03/01/2022 22:49:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.16 on epoch=124
03/01/2022 22:49:37 - INFO - __main__ - Global step 250 Train loss 0.17 ACC 0.5 on epoch=124
03/01/2022 22:49:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.13 on epoch=129
03/01/2022 22:49:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.14 on epoch=134
03/01/2022 22:49:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.17 on epoch=139
03/01/2022 22:49:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.13 on epoch=144
03/01/2022 22:49:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.14 on epoch=149
03/01/2022 22:49:50 - INFO - __main__ - Global step 300 Train loss 0.15 ACC 0.5 on epoch=149
03/01/2022 22:49:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.16 on epoch=154
03/01/2022 22:49:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.19 on epoch=159
03/01/2022 22:49:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.16 on epoch=164
03/01/2022 22:49:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.16 on epoch=169
03/01/2022 22:50:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.15 on epoch=174
03/01/2022 22:50:03 - INFO - __main__ - Global step 350 Train loss 0.16 ACC 0.5 on epoch=174
03/01/2022 22:50:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.15 on epoch=179
03/01/2022 22:50:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.15 on epoch=184
03/01/2022 22:50:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.13 on epoch=189
03/01/2022 22:50:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.14 on epoch=194
03/01/2022 22:50:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.14 on epoch=199
03/01/2022 22:50:15 - INFO - __main__ - Global step 400 Train loss 0.14 ACC 0.5 on epoch=199
03/01/2022 22:50:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.18 on epoch=204
03/01/2022 22:50:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.14 on epoch=209
03/01/2022 22:50:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=214
03/01/2022 22:50:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.16 on epoch=219
03/01/2022 22:50:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.15 on epoch=224
03/01/2022 22:50:28 - INFO - __main__ - Global step 450 Train loss 0.16 ACC 0.5 on epoch=224
03/01/2022 22:50:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.14 on epoch=229
03/01/2022 22:50:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=234
03/01/2022 22:50:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=239
03/01/2022 22:50:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=244
03/01/2022 22:50:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=249
03/01/2022 22:50:40 - INFO - __main__ - Global step 500 Train loss 0.14 ACC 0.5 on epoch=249
03/01/2022 22:50:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.12 on epoch=254
03/01/2022 22:50:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=259
03/01/2022 22:50:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=264
03/01/2022 22:50:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.12 on epoch=269
03/01/2022 22:50:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.12 on epoch=274
03/01/2022 22:50:53 - INFO - __main__ - Global step 550 Train loss 0.13 ACC 0.53125 on epoch=274
03/01/2022 22:50:53 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=274, global_step=550
03/01/2022 22:50:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.13 on epoch=279
03/01/2022 22:50:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=284
03/01/2022 22:51:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=289
03/01/2022 22:51:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=294
03/01/2022 22:51:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.13 on epoch=299
03/01/2022 22:51:06 - INFO - __main__ - Global step 600 Train loss 0.13 ACC 0.5 on epoch=299
03/01/2022 22:51:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=304
03/01/2022 22:51:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=309
03/01/2022 22:51:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=314
03/01/2022 22:51:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=319
03/01/2022 22:51:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=324
03/01/2022 22:51:18 - INFO - __main__ - Global step 650 Train loss 0.13 ACC 0.5 on epoch=324
03/01/2022 22:51:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=329
03/01/2022 22:51:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=334
03/01/2022 22:51:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=339
03/01/2022 22:51:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=344
03/01/2022 22:51:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=349
03/01/2022 22:51:31 - INFO - __main__ - Global step 700 Train loss 0.13 ACC 0.5 on epoch=349
03/01/2022 22:51:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=354
03/01/2022 22:51:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=359
03/01/2022 22:51:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=364
03/01/2022 22:51:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.12 on epoch=369
03/01/2022 22:51:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=374
03/01/2022 22:51:44 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.5 on epoch=374
03/01/2022 22:51:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
03/01/2022 22:51:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=384
03/01/2022 22:51:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=389
03/01/2022 22:51:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=394
03/01/2022 22:51:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=399
03/01/2022 22:51:56 - INFO - __main__ - Global step 800 Train loss 0.11 ACC 0.53125 on epoch=399
03/01/2022 22:51:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=404
03/01/2022 22:52:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=409
03/01/2022 22:52:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=414
03/01/2022 22:52:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=419
03/01/2022 22:52:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=424
03/01/2022 22:52:09 - INFO - __main__ - Global step 850 Train loss 0.10 ACC 0.5 on epoch=424
03/01/2022 22:52:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=429
03/01/2022 22:52:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=434
03/01/2022 22:52:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=439
03/01/2022 22:52:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=444
03/01/2022 22:52:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=449
03/01/2022 22:52:21 - INFO - __main__ - Global step 900 Train loss 0.07 ACC 0.5 on epoch=449
03/01/2022 22:52:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=454
03/01/2022 22:52:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=459
03/01/2022 22:52:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=464
03/01/2022 22:52:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=469
03/01/2022 22:52:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=474
03/01/2022 22:52:34 - INFO - __main__ - Global step 950 Train loss 0.08 ACC 0.53125 on epoch=474
03/01/2022 22:52:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
03/01/2022 22:52:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
03/01/2022 22:52:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
03/01/2022 22:52:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
03/01/2022 22:52:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=499
03/01/2022 22:52:46 - INFO - __main__ - Global step 1000 Train loss 0.06 ACC 0.5 on epoch=499
03/01/2022 22:52:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
03/01/2022 22:52:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=509
03/01/2022 22:52:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
03/01/2022 22:52:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
03/01/2022 22:52:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
03/01/2022 22:52:59 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.5625 on epoch=524
03/01/2022 22:52:59 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=524, global_step=1050
03/01/2022 22:53:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=529
03/01/2022 22:53:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
03/01/2022 22:53:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=539
03/01/2022 22:53:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
03/01/2022 22:53:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
03/01/2022 22:53:11 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.53125 on epoch=549
03/01/2022 22:53:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=554
03/01/2022 22:53:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=559
03/01/2022 22:53:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=564
03/01/2022 22:53:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
03/01/2022 22:53:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
03/01/2022 22:53:24 - INFO - __main__ - Global step 1150 Train loss 0.05 ACC 0.5 on epoch=574
03/01/2022 22:53:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
03/01/2022 22:53:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
03/01/2022 22:53:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
03/01/2022 22:53:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
03/01/2022 22:53:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
03/01/2022 22:53:36 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.5625 on epoch=599
03/01/2022 22:53:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=604
03/01/2022 22:53:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
03/01/2022 22:53:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
03/01/2022 22:53:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
03/01/2022 22:53:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
03/01/2022 22:53:49 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.59375 on epoch=624
03/01/2022 22:53:49 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=624, global_step=1250
03/01/2022 22:53:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
03/01/2022 22:53:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
03/01/2022 22:53:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
03/01/2022 22:53:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
03/01/2022 22:54:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/01/2022 22:54:02 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.5625 on epoch=649
03/01/2022 22:54:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/01/2022 22:54:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
03/01/2022 22:54:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
03/01/2022 22:54:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
03/01/2022 22:54:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/01/2022 22:54:14 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.5625 on epoch=674
03/01/2022 22:54:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/01/2022 22:54:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/01/2022 22:54:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
03/01/2022 22:54:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/01/2022 22:54:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
03/01/2022 22:54:27 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.5625 on epoch=699
03/01/2022 22:54:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
03/01/2022 22:54:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
03/01/2022 22:54:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/01/2022 22:54:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
03/01/2022 22:54:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/01/2022 22:54:39 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.59375 on epoch=724
03/01/2022 22:54:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
03/01/2022 22:54:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/01/2022 22:54:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
03/01/2022 22:54:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=744
03/01/2022 22:54:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
03/01/2022 22:54:52 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.5625 on epoch=749
03/01/2022 22:54:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/01/2022 22:54:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/01/2022 22:54:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/01/2022 22:55:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/01/2022 22:55:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/01/2022 22:55:04 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.59375 on epoch=774
03/01/2022 22:55:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 22:55:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
03/01/2022 22:55:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/01/2022 22:55:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/01/2022 22:55:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 22:55:17 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.5625 on epoch=799
03/01/2022 22:55:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
03/01/2022 22:55:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=809
03/01/2022 22:55:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
03/01/2022 22:55:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/01/2022 22:55:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/01/2022 22:55:30 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.625 on epoch=824
03/01/2022 22:55:30 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=824, global_step=1650
03/01/2022 22:55:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
03/01/2022 22:55:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 22:55:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/01/2022 22:55:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/01/2022 22:55:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
03/01/2022 22:55:42 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.5625 on epoch=849
03/01/2022 22:55:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
03/01/2022 22:55:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/01/2022 22:55:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/01/2022 22:55:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 22:55:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 22:55:55 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.5625 on epoch=874
03/01/2022 22:55:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
03/01/2022 22:55:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
03/01/2022 22:56:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
03/01/2022 22:56:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 22:56:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 22:56:07 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.5625 on epoch=899
03/01/2022 22:56:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
03/01/2022 22:56:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/01/2022 22:56:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 22:56:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
03/01/2022 22:56:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/01/2022 22:56:20 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.53125 on epoch=924
03/01/2022 22:56:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/01/2022 22:56:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 22:56:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 22:56:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
03/01/2022 22:56:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 22:56:32 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.5625 on epoch=949
03/01/2022 22:56:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
03/01/2022 22:56:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 22:56:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 22:56:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 22:56:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 22:56:45 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.5625 on epoch=974
03/01/2022 22:56:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 22:56:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
03/01/2022 22:56:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
03/01/2022 22:56:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 22:56:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
03/01/2022 22:56:58 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.53125 on epoch=999
03/01/2022 22:56:58 - INFO - __main__ - save last model!
03/01/2022 22:56:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 22:56:58 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 22:56:58 - INFO - __main__ - Printing 3 examples
03/01/2022 22:56:58 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 22:56:58 - INFO - __main__ - ['entailment']
03/01/2022 22:56:58 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 22:56:58 - INFO - __main__ - ['not_entailment']
03/01/2022 22:56:58 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 22:56:58 - INFO - __main__ - ['not_entailment']
03/01/2022 22:56:58 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:57:01 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:57:06 - INFO - __main__ - Loaded 5463 examples from test data
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 210, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1552, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 2503) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29544
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zayrcugy/none_ndmdnnz7/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zayrcugy/none_ndmdnnz7/attempt_1/1/error.json
Output directory () already exists and is not empty.
03/01/2022 22:57:24 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 22:57:24 - INFO - __main__ - models/T5-large/singletask-glue-qnli
03/01/2022 22:57:24 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 22:57:24 - INFO - __main__ - models/T5-large/singletask-glue-qnli
03/01/2022 22:57:25 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 22:57:25 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 22:57:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:27:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:27:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:27:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:27:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2605) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29544
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zayrcugy/none_ndmdnnz7/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zayrcugy/none_ndmdnnz7/attempt_2/1/error.json
Output directory () already exists and is not empty.
03/01/2022 23:27:31 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 23:27:31 - INFO - __main__ - models/T5-large/singletask-glue-qnli
03/01/2022 23:27:31 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 23:27:31 - INFO - __main__ - models/T5-large/singletask-glue-qnli
03/01/2022 23:27:32 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 23:27:32 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 23:27:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:57:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:57:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:57:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:57:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:57:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:57:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
    main()
      File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 156, in main
main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2653) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29544
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zayrcugy/none_ndmdnnz7/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zayrcugy/none_ndmdnnz7/attempt_3/1/error.json
Output directory () already exists and is not empty.
03/01/2022 23:57:38 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 23:57:38 - INFO - __main__ - models/T5-large/singletask-glue-qnli
03/01/2022 23:57:38 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/01/2022 23:57:38 - INFO - __main__ - models/T5-large/singletask-glue-qnli
03/01/2022 23:57:38 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 23:57:38 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 23:57:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:27:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:27:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:27:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:27:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:27:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:27:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:27:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 229, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 156, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit/tune_hps_singletask_ddp_prompt.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2701) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003337860107421875 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "2701", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 10947, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "2702", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 10947, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 10947, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2701 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
********************************************
  tune_hps_singletask_ddp_prompt.py FAILED  
============================================
Root Cause:
[0]:
  time: 2022-03-02_00:27:43
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2701)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
============================================
Other Failures:
[1]:
  time: 2022-03-02_00:27:43
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2702)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
********************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (2790): No such process
Task: hatexplain, Checkpoint: None, Identifier: T5-large
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : tune_hps_singletask_ddp_prompt.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29544
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_45ncv0d0/none_zex5cbne
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29544
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_45ncv0d0/none_zex5cbne/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_45ncv0d0/none_zex5cbne/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/02/2022 00:27:46 - INFO - __main__ - Namespace(task_dir='data/hatexplain/', task_name='hatexplain', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-hatexplain', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/02/2022 00:27:46 - INFO - __main__ - models/T5-large/singletask-hatexplain
03/02/2022 00:27:46 - INFO - __main__ - Namespace(task_dir='data/hatexplain/', task_name='hatexplain', identifier='T5-large', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large/singletask-hatexplain', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/02/2022 00:27:46 - INFO - __main__ - models/T5-large/singletask-hatexplain
03/02/2022 00:27:48 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/02/2022 00:27:48 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/02/2022 00:27:48 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/02/2022 00:27:48 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/02/2022 00:27:48 - INFO - __main__ - args.device: cuda:0
03/02/2022 00:27:48 - INFO - __main__ - Using 2 gpus
03/02/2022 00:27:48 - INFO - __main__ - args.device: cuda:1
03/02/2022 00:27:48 - INFO - __main__ - Using 2 gpus
03/02/2022 00:27:48 - INFO - __main__ - Fine-tuning the following samples: ['hatexplain_16_100', 'hatexplain_16_13', 'hatexplain_16_21', 'hatexplain_16_42', 'hatexplain_16_87']
03/02/2022 00:27:48 - INFO - __main__ - Fine-tuning the following samples: ['hatexplain_16_100', 'hatexplain_16_13', 'hatexplain_16_21', 'hatexplain_16_42', 'hatexplain_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/02/2022 00:27:55 - INFO - __main__ - Running ... prefix=hatexplain_16_100, lr=0.5, bsz=8 ...
03/02/2022 00:27:56 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:27:56 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:27:56 - INFO - __main__ - Printing 3 examples
03/02/2022 00:27:56 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 00:27:56 - INFO - __main__ - Printing 3 examples
03/02/2022 00:27:56 - INFO - __main__ - ['offensive']
03/02/2022 00:27:56 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 00:27:56 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 00:27:56 - INFO - __main__ - ['offensive']
03/02/2022 00:27:56 - INFO - __main__ - ['offensive']
03/02/2022 00:27:56 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 00:27:56 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 00:27:56 - INFO - __main__ - ['offensive']
03/02/2022 00:27:56 - INFO - __main__ - ['offensive']
03/02/2022 00:27:56 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 00:27:56 - INFO - __main__ - ['offensive']
03/02/2022 00:27:56 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:27:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 00:27:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:27:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:27:56 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 00:27:56 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:27:56 - INFO - __main__ - Printing 3 examples
03/02/2022 00:27:56 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 00:27:56 - INFO - __main__ - ['offensive']
03/02/2022 00:27:56 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 00:27:56 - INFO - __main__ - ['offensive']
03/02/2022 00:27:56 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 00:27:56 - INFO - __main__ - ['offensive']
03/02/2022 00:27:56 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:27:56 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 00:27:56 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:27:56 - INFO - __main__ - Printing 3 examples
03/02/2022 00:27:56 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 00:27:56 - INFO - __main__ - ['offensive']
03/02/2022 00:27:56 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 00:27:56 - INFO - __main__ - ['offensive']
03/02/2022 00:27:56 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 00:27:56 - INFO - __main__ - ['offensive']
03/02/2022 00:27:56 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:27:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:27:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:27:56 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 00:27:56 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 00:28:11 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 00:28:11 - INFO - __main__ - task name: hatexplain
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446), (3702, 272314), (6894, 155616), (3001, 351252), (499, 1766405), (3731, 203441), (6478, 174299), (1287, 709613), (4382, 247856), (851, 1026262), (5946, 169484), (1409, 319874), (3527, 259912), (6373, 162161), (1640, 475640), (801, 1149556), (357, 1674058), (2402, 404464), (2536, 323855), (887, 1021213), (2386, 396013), (1103, 849279), (4237, 246742), (3032, 335423), (2922, 206677), (3889, 257596), (1683, 558497), (4149, 253554), (3634, 263727), (1955, 437347), (2891, 347678), (770, 1128621), (6298, 154205), (1904, 528496), (5956, 175083), (2943, 377167), (1220, 555589), (5103, 202299), (4313, 246924), (2734, 344156), (5735, 168028), (2404, 417090), (3626, 290198), (605, 1425064), (2324, 403572), (235, 2453612), (2733, 298680), (4166, 233368), (2422, 345969), (676, 1204693), (2272, 432795), (4569, 164817), (3285, 299538), (2136, 430918), (5166, 187635), (2040, 237038), (5037, 203882), (1556, 625505), (2836, 350538), (2079, 374647)]
03/02/2022 00:28:11 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 00:28:11 - INFO - __main__ - task name: hatexplain
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446), (3702, 272314), (6894, 155616), (3001, 351252), (499, 1766405), (3731, 203441), (6478, 174299), (1287, 709613), (4382, 247856), (851, 1026262), (5946, 169484), (1409, 319874), (3527, 259912), (6373, 162161), (1640, 475640), (801, 1149556), (357, 1674058), (2402, 404464), (2536, 323855), (887, 1021213), (2386, 396013), (1103, 849279), (4237, 246742), (3032, 335423), (2922, 206677), (3889, 257596), (1683, 558497), (4149, 253554), (3634, 263727), (1955, 437347), (2891, 347678), (770, 1128621), (6298, 154205), (1904, 528496), (5956, 175083), (2943, 377167), (1220, 555589), (5103, 202299), (4313, 246924), (2734, 344156), (5735, 168028), (2404, 417090), (3626, 290198), (605, 1425064), (2324, 403572), (235, 2453612), (2733, 298680), (4166, 233368), (2422, 345969), (676, 1204693), (2272, 432795), (4569, 164817), (3285, 299538), (2136, 430918), (5166, 187635), (2040, 237038), (5037, 203882), (1556, 625505), (2836, 350538), (2079, 374647)]
03/02/2022 00:28:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 00:28:12 - INFO - __main__ - Starting training!
03/02/2022 00:28:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 00:28:12 - INFO - __main__ - Starting training!
03/02/2022 00:28:14 - INFO - __main__ - Step 10 Global step 10 Train loss 6.78 on epoch=3
03/02/2022 00:28:17 - INFO - __main__ - Step 20 Global step 20 Train loss 2.66 on epoch=6
03/02/2022 00:28:19 - INFO - __main__ - Step 30 Global step 30 Train loss 1.04 on epoch=9
03/02/2022 00:28:21 - INFO - __main__ - Step 40 Global step 40 Train loss 0.74 on epoch=13
03/02/2022 00:28:23 - INFO - __main__ - Step 50 Global step 50 Train loss 0.63 on epoch=16
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/02/2022 00:28:24 - INFO - __main__ - Global step 50 Train loss 2.37 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 00:28:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 00:28:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.62 on epoch=19
03/02/2022 00:28:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.60 on epoch=23
03/02/2022 00:28:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=26
03/02/2022 00:28:33 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=29
03/02/2022 00:28:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=33
03/02/2022 00:28:36 - INFO - __main__ - Global step 100 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 00:28:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
03/02/2022 00:28:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=39
03/02/2022 00:28:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=43
03/02/2022 00:28:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=46
03/02/2022 00:28:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=49
03/02/2022 00:28:48 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.27869212201009436 on epoch=49
03/02/2022 00:28:48 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.27869212201009436 on epoch=49, global_step=150
03/02/2022 00:28:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
03/02/2022 00:28:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
03/02/2022 00:28:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=59
03/02/2022 00:28:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
03/02/2022 00:28:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=66
03/02/2022 00:29:00 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.2433862433862434 on epoch=66
03/02/2022 00:29:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=69
03/02/2022 00:29:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=73
03/02/2022 00:29:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=76
03/02/2022 00:29:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=79
03/02/2022 00:29:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=83
03/02/2022 00:29:11 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=83
03/02/2022 00:29:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=86
03/02/2022 00:29:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
03/02/2022 00:29:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
03/02/2022 00:29:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=96
03/02/2022 00:29:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=99
03/02/2022 00:29:24 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.2618181818181818 on epoch=99
03/02/2022 00:29:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=103
03/02/2022 00:29:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=106
03/02/2022 00:29:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
03/02/2022 00:29:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
03/02/2022 00:29:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=116
03/02/2022 00:29:35 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.15204678362573099 on epoch=116
03/02/2022 00:29:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=119
03/02/2022 00:29:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=123
03/02/2022 00:29:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=126
03/02/2022 00:29:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=129
03/02/2022 00:29:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=133
03/02/2022 00:29:47 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.19122257053291536 on epoch=133
03/02/2022 00:29:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=136
03/02/2022 00:29:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=139
03/02/2022 00:29:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=143
03/02/2022 00:29:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=146
03/02/2022 00:29:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=149
03/02/2022 00:29:59 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.2574224021592443 on epoch=149
03/02/2022 00:30:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=153
03/02/2022 00:30:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=156
03/02/2022 00:30:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=159
03/02/2022 00:30:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=163
03/02/2022 00:30:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=166
03/02/2022 00:30:11 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.23615819209039554 on epoch=166
03/02/2022 00:30:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=169
03/02/2022 00:30:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=173
03/02/2022 00:30:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=176
03/02/2022 00:30:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=179
03/02/2022 00:30:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.32 on epoch=183
03/02/2022 00:30:23 - INFO - __main__ - Global step 550 Train loss 0.32 Classification-F1 0.23615819209039554 on epoch=183
03/02/2022 00:30:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=186
03/02/2022 00:30:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=189
03/02/2022 00:30:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=193
03/02/2022 00:30:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.33 on epoch=196
03/02/2022 00:30:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=199
03/02/2022 00:30:36 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.32536301006889246 on epoch=199
03/02/2022 00:30:36 - INFO - __main__ - Saving model with best Classification-F1: 0.27869212201009436 -> 0.32536301006889246 on epoch=199, global_step=600
03/02/2022 00:30:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.28 on epoch=203
03/02/2022 00:30:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=206
03/02/2022 00:30:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=209
03/02/2022 00:30:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=213
03/02/2022 00:30:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=216
03/02/2022 00:30:48 - INFO - __main__ - Global step 650 Train loss 0.28 Classification-F1 0.27422544495715223 on epoch=216
03/02/2022 00:30:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=219
03/02/2022 00:30:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=223
03/02/2022 00:30:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=226
03/02/2022 00:30:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=229
03/02/2022 00:30:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=233
03/02/2022 00:31:00 - INFO - __main__ - Global step 700 Train loss 0.27 Classification-F1 0.2523719165085389 on epoch=233
03/02/2022 00:31:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=236
03/02/2022 00:31:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=239
03/02/2022 00:31:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=243
03/02/2022 00:31:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=246
03/02/2022 00:31:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=249
03/02/2022 00:31:12 - INFO - __main__ - Global step 750 Train loss 0.19 Classification-F1 0.198184667624396 on epoch=249
03/02/2022 00:31:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=253
03/02/2022 00:31:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=256
03/02/2022 00:31:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=259
03/02/2022 00:31:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=263
03/02/2022 00:31:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=266
03/02/2022 00:31:25 - INFO - __main__ - Global step 800 Train loss 0.14 Classification-F1 0.18481481481481482 on epoch=266
03/02/2022 00:31:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=269
03/02/2022 00:31:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.13 on epoch=273
03/02/2022 00:31:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=276
03/02/2022 00:31:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=279
03/02/2022 00:31:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=283
03/02/2022 00:31:37 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.1511111111111111 on epoch=283
03/02/2022 00:31:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=286
03/02/2022 00:31:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=289
03/02/2022 00:31:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=293
03/02/2022 00:31:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=296
03/02/2022 00:31:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=299
03/02/2022 00:31:50 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.1782276422764228 on epoch=299
03/02/2022 00:31:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=303
03/02/2022 00:31:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=306
03/02/2022 00:31:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=309
03/02/2022 00:31:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=313
03/02/2022 00:32:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=316
03/02/2022 00:32:02 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.13118765292678336 on epoch=316
03/02/2022 00:32:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=319
03/02/2022 00:32:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=323
03/02/2022 00:32:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=326
03/02/2022 00:32:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=329
03/02/2022 00:32:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=333
03/02/2022 00:32:15 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.16666666666666666 on epoch=333
03/02/2022 00:32:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
03/02/2022 00:32:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=339
03/02/2022 00:32:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=343
03/02/2022 00:32:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
03/02/2022 00:32:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=349
03/02/2022 00:32:27 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.23580952380952377 on epoch=349
03/02/2022 00:32:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=353
03/02/2022 00:32:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
03/02/2022 00:32:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=359
03/02/2022 00:32:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=363
03/02/2022 00:32:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
03/02/2022 00:32:39 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.1405895691609977 on epoch=366
03/02/2022 00:32:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
03/02/2022 00:32:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
03/02/2022 00:32:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=376
03/02/2022 00:32:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
03/02/2022 00:32:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=383
03/02/2022 00:32:52 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.14183150183150184 on epoch=383
03/02/2022 00:32:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
03/02/2022 00:32:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=389
03/02/2022 00:32:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
03/02/2022 00:33:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
03/02/2022 00:33:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=399
03/02/2022 00:33:05 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.1031946651259088 on epoch=399
03/02/2022 00:33:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
03/02/2022 00:33:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=406
03/02/2022 00:33:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
03/02/2022 00:33:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=413
03/02/2022 00:33:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
03/02/2022 00:33:17 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.23850174216027872 on epoch=416
03/02/2022 00:33:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
03/02/2022 00:33:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
03/02/2022 00:33:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
03/02/2022 00:33:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=429
03/02/2022 00:33:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
03/02/2022 00:33:29 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.24948240165631475 on epoch=433
03/02/2022 00:33:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
03/02/2022 00:33:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
03/02/2022 00:33:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
03/02/2022 00:33:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
03/02/2022 00:33:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
03/02/2022 00:33:42 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.12153593532903877 on epoch=449
03/02/2022 00:33:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
03/02/2022 00:33:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
03/02/2022 00:33:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=459
03/02/2022 00:33:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
03/02/2022 00:33:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
03/02/2022 00:33:54 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.14270333064161955 on epoch=466
03/02/2022 00:33:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=469
03/02/2022 00:33:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
03/02/2022 00:34:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
03/02/2022 00:34:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=479
03/02/2022 00:34:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=483
03/02/2022 00:34:07 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.08983863090143282 on epoch=483
03/02/2022 00:34:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
03/02/2022 00:34:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
03/02/2022 00:34:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
03/02/2022 00:34:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=496
03/02/2022 00:34:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
03/02/2022 00:34:19 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.18354515050167225 on epoch=499
03/02/2022 00:34:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
03/02/2022 00:34:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
03/02/2022 00:34:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
03/02/2022 00:34:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
03/02/2022 00:34:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
03/02/2022 00:34:31 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.19350233100233102 on epoch=516
03/02/2022 00:34:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=519
03/02/2022 00:34:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
03/02/2022 00:34:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
03/02/2022 00:34:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
03/02/2022 00:34:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
03/02/2022 00:34:44 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.2387719298245614 on epoch=533
03/02/2022 00:34:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
03/02/2022 00:34:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
03/02/2022 00:34:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
03/02/2022 00:34:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
03/02/2022 00:34:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
03/02/2022 00:34:56 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.1819710987078523 on epoch=549
03/02/2022 00:34:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
03/02/2022 00:35:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
03/02/2022 00:35:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
03/02/2022 00:35:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
03/02/2022 00:35:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
03/02/2022 00:35:09 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.19961181689856825 on epoch=566
03/02/2022 00:35:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
03/02/2022 00:35:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=573
03/02/2022 00:35:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
03/02/2022 00:35:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
03/02/2022 00:35:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
03/02/2022 00:35:21 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.1939153439153439 on epoch=583
03/02/2022 00:35:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 00:35:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/02/2022 00:35:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
03/02/2022 00:35:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/02/2022 00:35:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
03/02/2022 00:35:34 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.34936143039591316 on epoch=599
03/02/2022 00:35:34 - INFO - __main__ - Saving model with best Classification-F1: 0.32536301006889246 -> 0.34936143039591316 on epoch=599, global_step=1800
03/02/2022 00:35:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 00:35:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=606
03/02/2022 00:35:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
03/02/2022 00:35:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
03/02/2022 00:35:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
03/02/2022 00:35:47 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.36446886446886445 on epoch=616
03/02/2022 00:35:47 - INFO - __main__ - Saving model with best Classification-F1: 0.34936143039591316 -> 0.36446886446886445 on epoch=616, global_step=1850
03/02/2022 00:35:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
03/02/2022 00:35:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
03/02/2022 00:35:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
03/02/2022 00:35:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 00:35:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 00:35:59 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.2673466735966736 on epoch=633
03/02/2022 00:36:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
03/02/2022 00:36:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 00:36:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 00:36:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 00:36:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
03/02/2022 00:36:12 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.21721611721611725 on epoch=649
03/02/2022 00:36:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 00:36:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
03/02/2022 00:36:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
03/02/2022 00:36:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=663
03/02/2022 00:36:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 00:36:24 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.17284848484848486 on epoch=666
03/02/2022 00:36:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 00:36:29 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=673
03/02/2022 00:36:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
03/02/2022 00:36:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
03/02/2022 00:36:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 00:36:37 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.3904761904761904 on epoch=683
03/02/2022 00:36:37 - INFO - __main__ - Saving model with best Classification-F1: 0.36446886446886445 -> 0.3904761904761904 on epoch=683, global_step=2050
03/02/2022 00:36:39 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 00:36:41 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=689
03/02/2022 00:36:43 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 00:36:46 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 00:36:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 00:36:49 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.25132275132275134 on epoch=699
03/02/2022 00:36:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 00:36:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 00:36:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/02/2022 00:36:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 00:37:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=716
03/02/2022 00:37:01 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.35497835497835495 on epoch=716
03/02/2022 00:37:03 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
03/02/2022 00:37:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 00:37:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=726
03/02/2022 00:37:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 00:37:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 00:37:14 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.2830026455026455 on epoch=733
03/02/2022 00:37:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 00:37:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 00:37:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=743
03/02/2022 00:37:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 00:37:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 00:37:26 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.3148496458841286 on epoch=749
03/02/2022 00:37:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 00:37:31 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=756
03/02/2022 00:37:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=759
03/02/2022 00:37:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 00:37:37 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 00:37:39 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.2066302755957928 on epoch=766
03/02/2022 00:37:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 00:37:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 00:37:46 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 00:37:48 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 00:37:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 00:37:51 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.2207007007007007 on epoch=783
03/02/2022 00:37:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=786
03/02/2022 00:37:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 00:37:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 00:38:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 00:38:03 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 00:38:04 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.288497150997151 on epoch=799
03/02/2022 00:38:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 00:38:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 00:38:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 00:38:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 00:38:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/02/2022 00:38:16 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.1574823811665917 on epoch=816
03/02/2022 00:38:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 00:38:21 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
03/02/2022 00:38:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 00:38:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=829
03/02/2022 00:38:28 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 00:38:29 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.19081787777439949 on epoch=833
03/02/2022 00:38:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 00:38:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 00:38:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 00:38:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
03/02/2022 00:38:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 00:38:41 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.28583333333333333 on epoch=849
03/02/2022 00:38:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/02/2022 00:38:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 00:38:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 00:38:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 00:38:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 00:38:54 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.42795840175150524 on epoch=866
03/02/2022 00:38:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3904761904761904 -> 0.42795840175150524 on epoch=866, global_step=2600
03/02/2022 00:38:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 00:38:58 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 00:39:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 00:39:03 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
03/02/2022 00:39:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 00:39:06 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.4023199023199024 on epoch=883
03/02/2022 00:39:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 00:39:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 00:39:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 00:39:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 00:39:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=899
03/02/2022 00:39:19 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.317948717948718 on epoch=899
03/02/2022 00:39:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 00:39:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 00:39:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 00:39:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 00:39:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 00:39:31 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.2042407367744699 on epoch=916
03/02/2022 00:39:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
03/02/2022 00:39:36 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 00:39:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 00:39:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=929
03/02/2022 00:39:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 00:39:44 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.12325428194993414 on epoch=933
03/02/2022 00:39:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 00:39:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 00:39:51 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 00:39:53 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 00:39:55 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=949
03/02/2022 00:39:56 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.13648947951273532 on epoch=949
03/02/2022 00:39:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=953
03/02/2022 00:40:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
03/02/2022 00:40:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
03/02/2022 00:40:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 00:40:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 00:40:09 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.2901785714285714 on epoch=966
03/02/2022 00:40:11 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=969
03/02/2022 00:40:13 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 00:40:15 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 00:40:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 00:40:20 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 00:40:21 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.12061224489795917 on epoch=983
03/02/2022 00:40:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=986
03/02/2022 00:40:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 00:40:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 00:40:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 00:40:32 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 00:40:34 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:40:34 - INFO - __main__ - Printing 3 examples
03/02/2022 00:40:34 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 00:40:34 - INFO - __main__ - ['offensive']
03/02/2022 00:40:34 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 00:40:34 - INFO - __main__ - ['offensive']
03/02/2022 00:40:34 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 00:40:34 - INFO - __main__ - ['offensive']
03/02/2022 00:40:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 00:40:34 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:40:34 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.22612126245847175 on epoch=999
03/02/2022 00:40:34 - INFO - __main__ - save last model!
03/02/2022 00:40:34 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 00:40:34 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:40:34 - INFO - __main__ - Printing 3 examples
03/02/2022 00:40:34 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 00:40:34 - INFO - __main__ - ['offensive']
03/02/2022 00:40:34 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 00:40:34 - INFO - __main__ - ['offensive']
03/02/2022 00:40:34 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 00:40:34 - INFO - __main__ - ['offensive']
03/02/2022 00:40:34 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:40:34 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:40:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 00:40:34 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 00:40:34 - INFO - __main__ - Printing 3 examples
03/02/2022 00:40:34 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 00:40:34 - INFO - __main__ - ['normal']
03/02/2022 00:40:34 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 00:40:34 - INFO - __main__ - ['normal']
03/02/2022 00:40:34 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 00:40:34 - INFO - __main__ - ['normal']
03/02/2022 00:40:34 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 00:40:34 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 00:40:35 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:40:36 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 00:40:48 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 00:40:48 - INFO - __main__ - task name: hatexplain
initialize from c4
[(6579, 166537), (5880, 173333), (2252, 353035), (6881, 160252), (4773, 218071), (6404, 160401), (3371, 234531), (4187, 259804), (1925, 417452), (968, 645839), (2729, 183765), (5538, 189511), (987, 914447), (144, 1635131), (1079, 796417), (1653, 586845), (1731, 565272), (3424, 221936), (5680, 157495), (731, 1250799), (4298, 243616), (3238, 245783), (6415, 157560), (2406, 200208), (5775, 177401), (2721, 367851), (6309, 169174), (135, 7037199), (1214, 767509), (5962, 174066), (2867, 348102), (3209, 276527), (1189, 784250), (891, 319194), (5015, 214805), (1701, 571183), (3845, 206498), (26, 28035103), (2809, 352164), (5121, 187048), (1892, 508575), (3574, 184323), (996, 821524), (3263, 314166), (6363, 154270), (90, 459181), (4226, 250683), (1751, 560750), (5058, 173227), (5205, 176939), (7, 158399866), (6008, 156882), (3474, 290209), (3351, 190868), (225, 4092648), (1142, 782542), (154, 259007), (3278, 305705), (2637, 387504), (532, 1388213), (2597, 385302), (6237, 164778), (851, 1026262), (793, 959415), (2719, 191820), (788, 1153138), (4209, 175746), (1393, 703212), (1438, 688034), (5340, 197066), (3995, 170269), (1376, 549699), (2909, 350152), (5773, 177903), (6551, 154998), (4799, 221150), (2335, 431458), (6082, 173261), (1824, 455362), (3242, 301888), (4627, 234647), (3797, 250903), (4027, 213331), (6464, 181012), (5210, 207673), (1137, 734426), (2765, 373166), (2451, 408517), (708, 1240422), (3108, 278320), (243, 3854645), (4830, 159383), (5630, 169060), (2134, 401363), (4490, 159360), (2342, 415802), (80, 11392202), (758, 1131386), (652, 1360888)]
03/02/2022 00:40:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 00:40:50 - INFO - __main__ - Starting training!
03/02/2022 00:41:25 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_100_0.5_8_predictions.txt
03/02/2022 00:41:25 - INFO - __main__ - Classification-F1 on test data: 0.0880
03/02/2022 00:41:25 - INFO - __main__ - prefix=hatexplain_16_100, lr=0.5, bsz=8, dev_performance=0.42795840175150524, test_performance=0.08804613840910898
03/02/2022 00:41:25 - INFO - __main__ - Running ... prefix=hatexplain_16_100, lr=0.4, bsz=8 ...
03/02/2022 00:41:26 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:41:26 - INFO - __main__ - Printing 3 examples
03/02/2022 00:41:26 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 00:41:26 - INFO - __main__ - ['offensive']
03/02/2022 00:41:26 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 00:41:26 - INFO - __main__ - ['offensive']
03/02/2022 00:41:26 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 00:41:26 - INFO - __main__ - ['offensive']
03/02/2022 00:41:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 00:41:26 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:41:26 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 00:41:26 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:41:26 - INFO - __main__ - Printing 3 examples
03/02/2022 00:41:26 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 00:41:26 - INFO - __main__ - ['offensive']
03/02/2022 00:41:26 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 00:41:26 - INFO - __main__ - ['offensive']
03/02/2022 00:41:26 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 00:41:26 - INFO - __main__ - ['offensive']
03/02/2022 00:41:26 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:41:26 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:41:26 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 00:41:40 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 00:41:40 - INFO - __main__ - task name: hatexplain
initialize from c4
[(6579, 166537), (5880, 173333), (2252, 353035), (6881, 160252), (4773, 218071), (6404, 160401), (3371, 234531), (4187, 259804), (1925, 417452), (968, 645839), (2729, 183765), (5538, 189511), (987, 914447), (144, 1635131), (1079, 796417), (1653, 586845), (1731, 565272), (3424, 221936), (5680, 157495), (731, 1250799), (4298, 243616), (3238, 245783), (6415, 157560), (2406, 200208), (5775, 177401), (2721, 367851), (6309, 169174), (135, 7037199), (1214, 767509), (5962, 174066), (2867, 348102), (3209, 276527), (1189, 784250), (891, 319194), (5015, 214805), (1701, 571183), (3845, 206498), (26, 28035103), (2809, 352164), (5121, 187048), (1892, 508575), (3574, 184323), (996, 821524), (3263, 314166), (6363, 154270), (90, 459181), (4226, 250683), (1751, 560750), (5058, 173227), (5205, 176939), (7, 158399866), (6008, 156882), (3474, 290209), (3351, 190868), (225, 4092648), (1142, 782542), (154, 259007), (3278, 305705), (2637, 387504), (532, 1388213), (2597, 385302), (6237, 164778), (851, 1026262), (793, 959415), (2719, 191820), (788, 1153138), (4209, 175746), (1393, 703212), (1438, 688034), (5340, 197066), (3995, 170269), (1376, 549699), (2909, 350152), (5773, 177903), (6551, 154998), (4799, 221150), (2335, 431458), (6082, 173261), (1824, 455362), (3242, 301888), (4627, 234647), (3797, 250903), (4027, 213331), (6464, 181012), (5210, 207673), (1137, 734426), (2765, 373166), (2451, 408517), (708, 1240422), (3108, 278320), (243, 3854645), (4830, 159383), (5630, 169060), (2134, 401363), (4490, 159360), (2342, 415802), (80, 11392202), (758, 1131386), (652, 1360888)]
03/02/2022 00:41:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 00:41:41 - INFO - __main__ - Starting training!
03/02/2022 00:41:43 - INFO - __main__ - Step 10 Global step 10 Train loss 6.58 on epoch=3
03/02/2022 00:41:46 - INFO - __main__ - Step 20 Global step 20 Train loss 3.03 on epoch=6
03/02/2022 00:41:48 - INFO - __main__ - Step 30 Global step 30 Train loss 1.43 on epoch=9
03/02/2022 00:41:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.84 on epoch=13
03/02/2022 00:41:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.62 on epoch=16
03/02/2022 00:41:53 - INFO - __main__ - Global step 50 Train loss 2.50 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 00:41:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 00:41:55 - INFO - __main__ - Step 60 Global step 60 Train loss 0.68 on epoch=19
03/02/2022 00:41:58 - INFO - __main__ - Step 70 Global step 70 Train loss 0.57 on epoch=23
03/02/2022 00:42:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=26
03/02/2022 00:42:02 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=29
03/02/2022 00:42:04 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=33
03/02/2022 00:42:05 - INFO - __main__ - Global step 100 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 00:42:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=36
03/02/2022 00:42:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=39
03/02/2022 00:42:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=43
03/02/2022 00:42:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=46
03/02/2022 00:42:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=49
03/02/2022 00:42:17 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.19999999999999998 on epoch=49
03/02/2022 00:42:17 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.19999999999999998 on epoch=49, global_step=150
03/02/2022 00:42:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.56 on epoch=53
03/02/2022 00:42:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=56
03/02/2022 00:42:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
03/02/2022 00:42:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=63
03/02/2022 00:42:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=66
03/02/2022 00:42:30 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.24305555555555558 on epoch=66
03/02/2022 00:42:30 - INFO - __main__ - Saving model with best Classification-F1: 0.19999999999999998 -> 0.24305555555555558 on epoch=66, global_step=200
03/02/2022 00:42:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=69
03/02/2022 00:42:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=73
03/02/2022 00:42:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=76
03/02/2022 00:42:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
03/02/2022 00:42:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
03/02/2022 00:42:42 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.20908004778972522 on epoch=83
03/02/2022 00:42:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=86
03/02/2022 00:42:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
03/02/2022 00:42:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=93
03/02/2022 00:42:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=96
03/02/2022 00:42:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=99
03/02/2022 00:42:54 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.3432679738562092 on epoch=99
03/02/2022 00:42:54 - INFO - __main__ - Saving model with best Classification-F1: 0.24305555555555558 -> 0.3432679738562092 on epoch=99, global_step=300
03/02/2022 00:42:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=103
03/02/2022 00:42:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=106
03/02/2022 00:43:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=109
03/02/2022 00:43:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=113
03/02/2022 00:43:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=116
03/02/2022 00:43:06 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.2871689026487788 on epoch=116
03/02/2022 00:43:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=119
03/02/2022 00:43:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=123
03/02/2022 00:43:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=126
03/02/2022 00:43:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
03/02/2022 00:43:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=133
03/02/2022 00:43:18 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.2148148148148148 on epoch=133
03/02/2022 00:43:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=136
03/02/2022 00:43:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=139
03/02/2022 00:43:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=143
03/02/2022 00:43:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.34 on epoch=146
03/02/2022 00:43:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=149
03/02/2022 00:43:31 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.27652464494569756 on epoch=149
03/02/2022 00:43:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.31 on epoch=153
03/02/2022 00:43:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=156
03/02/2022 00:43:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=159
03/02/2022 00:43:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=163
03/02/2022 00:43:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=166
03/02/2022 00:43:43 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.2578467101731785 on epoch=166
03/02/2022 00:43:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=169
03/02/2022 00:43:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.28 on epoch=173
03/02/2022 00:43:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=176
03/02/2022 00:43:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=179
03/02/2022 00:43:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=183
03/02/2022 00:43:55 - INFO - __main__ - Global step 550 Train loss 0.26 Classification-F1 0.291616096713548 on epoch=183
03/02/2022 00:43:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=186
03/02/2022 00:43:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=189
03/02/2022 00:44:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=193
03/02/2022 00:44:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=196
03/02/2022 00:44:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=199
03/02/2022 00:44:07 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.21392461197339246 on epoch=199
03/02/2022 00:44:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=203
03/02/2022 00:44:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=206
03/02/2022 00:44:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=209
03/02/2022 00:44:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=213
03/02/2022 00:44:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=216
03/02/2022 00:44:19 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.16016830294530154 on epoch=216
03/02/2022 00:44:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=219
03/02/2022 00:44:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=223
03/02/2022 00:44:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=226
03/02/2022 00:44:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=229
03/02/2022 00:44:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=233
03/02/2022 00:44:31 - INFO - __main__ - Global step 700 Train loss 0.11 Classification-F1 0.19888517279821627 on epoch=233
03/02/2022 00:44:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=236
03/02/2022 00:44:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.13 on epoch=239
03/02/2022 00:44:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=243
03/02/2022 00:44:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=246
03/02/2022 00:44:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=249
03/02/2022 00:44:44 - INFO - __main__ - Global step 750 Train loss 0.08 Classification-F1 0.21485696222538325 on epoch=249
03/02/2022 00:44:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=253
03/02/2022 00:44:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=256
03/02/2022 00:44:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=259
03/02/2022 00:44:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=263
03/02/2022 00:44:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=266
03/02/2022 00:44:56 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.17198879551820725 on epoch=266
03/02/2022 00:44:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=269
03/02/2022 00:45:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=273
03/02/2022 00:45:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=276
03/02/2022 00:45:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=279
03/02/2022 00:45:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=283
03/02/2022 00:45:08 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.12594268476621417 on epoch=283
03/02/2022 00:45:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=286
03/02/2022 00:45:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=289
03/02/2022 00:45:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=293
03/02/2022 00:45:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
03/02/2022 00:45:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=299
03/02/2022 00:45:20 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.12586847088474526 on epoch=299
03/02/2022 00:45:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=303
03/02/2022 00:45:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=306
03/02/2022 00:45:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=309
03/02/2022 00:45:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=313
03/02/2022 00:45:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=316
03/02/2022 00:45:32 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.16473684210526315 on epoch=316
03/02/2022 00:45:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=319
03/02/2022 00:45:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=323
03/02/2022 00:45:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
03/02/2022 00:45:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=329
03/02/2022 00:45:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=333
03/02/2022 00:45:44 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.14486486486486486 on epoch=333
03/02/2022 00:45:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
03/02/2022 00:45:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=339
03/02/2022 00:45:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=343
03/02/2022 00:45:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
03/02/2022 00:45:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=349
03/02/2022 00:45:56 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.1576374889478338 on epoch=349
03/02/2022 00:45:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=353
03/02/2022 00:46:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=356
03/02/2022 00:46:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=359
03/02/2022 00:46:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=363
03/02/2022 00:46:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
03/02/2022 00:46:08 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.12094347732645604 on epoch=366
03/02/2022 00:46:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
03/02/2022 00:46:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
03/02/2022 00:46:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=376
03/02/2022 00:46:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
03/02/2022 00:46:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=383
03/02/2022 00:46:20 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.19375415282392025 on epoch=383
03/02/2022 00:46:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=386
03/02/2022 00:46:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
03/02/2022 00:46:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
03/02/2022 00:46:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
03/02/2022 00:46:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
03/02/2022 00:46:32 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.15422222222222223 on epoch=399
03/02/2022 00:46:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=403
03/02/2022 00:46:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
03/02/2022 00:46:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=409
03/02/2022 00:46:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
03/02/2022 00:46:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
03/02/2022 00:46:44 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.19685324422166525 on epoch=416
03/02/2022 00:46:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=419
03/02/2022 00:46:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
03/02/2022 00:46:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
03/02/2022 00:46:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
03/02/2022 00:46:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
03/02/2022 00:46:56 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.12423235679049632 on epoch=433
03/02/2022 00:46:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
03/02/2022 00:47:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
03/02/2022 00:47:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
03/02/2022 00:47:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
03/02/2022 00:47:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
03/02/2022 00:47:09 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.1377955377955378 on epoch=449
03/02/2022 00:47:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=453
03/02/2022 00:47:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
03/02/2022 00:47:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
03/02/2022 00:47:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=463
03/02/2022 00:47:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
03/02/2022 00:47:21 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.15626333174875978 on epoch=466
03/02/2022 00:47:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
03/02/2022 00:47:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
03/02/2022 00:47:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
03/02/2022 00:47:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=479
03/02/2022 00:47:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
03/02/2022 00:47:33 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.1360544217687075 on epoch=483
03/02/2022 00:47:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
03/02/2022 00:47:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
03/02/2022 00:47:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
03/02/2022 00:47:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
03/02/2022 00:47:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=499
03/02/2022 00:47:45 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.23323886639676114 on epoch=499
03/02/2022 00:47:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
03/02/2022 00:47:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
03/02/2022 00:47:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
03/02/2022 00:47:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
03/02/2022 00:47:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=516
03/02/2022 00:47:57 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.21210144927536234 on epoch=516
03/02/2022 00:47:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
03/02/2022 00:48:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
03/02/2022 00:48:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
03/02/2022 00:48:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
03/02/2022 00:48:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
03/02/2022 00:48:09 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.1425954997383569 on epoch=533
03/02/2022 00:48:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
03/02/2022 00:48:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
03/02/2022 00:48:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
03/02/2022 00:48:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
03/02/2022 00:48:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
03/02/2022 00:48:21 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.0880656610470275 on epoch=549
03/02/2022 00:48:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
03/02/2022 00:48:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
03/02/2022 00:48:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
03/02/2022 00:48:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
03/02/2022 00:48:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
03/02/2022 00:48:34 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.15092361055093975 on epoch=566
03/02/2022 00:48:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
03/02/2022 00:48:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
03/02/2022 00:48:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
03/02/2022 00:48:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
03/02/2022 00:48:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
03/02/2022 00:48:46 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.10998536530451422 on epoch=583
03/02/2022 00:48:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
03/02/2022 00:48:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
03/02/2022 00:48:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
03/02/2022 00:48:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
03/02/2022 00:48:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
03/02/2022 00:48:58 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.09920634920634921 on epoch=599
03/02/2022 00:49:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 00:49:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 00:49:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
03/02/2022 00:49:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
03/02/2022 00:49:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
03/02/2022 00:49:10 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.09559386973180076 on epoch=616
03/02/2022 00:49:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
03/02/2022 00:49:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
03/02/2022 00:49:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
03/02/2022 00:49:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
03/02/2022 00:49:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
03/02/2022 00:49:22 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.1818181818181818 on epoch=633
03/02/2022 00:49:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
03/02/2022 00:49:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 00:49:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 00:49:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 00:49:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
03/02/2022 00:49:34 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.22605128205128205 on epoch=649
03/02/2022 00:49:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 00:49:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=656
03/02/2022 00:49:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
03/02/2022 00:49:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
03/02/2022 00:49:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 00:49:47 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.13190383365821962 on epoch=666
03/02/2022 00:49:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 00:49:51 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 00:49:54 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 00:49:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 00:49:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 00:49:59 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.14415086828879933 on epoch=683
03/02/2022 00:50:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
03/02/2022 00:50:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/02/2022 00:50:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 00:50:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 00:50:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
03/02/2022 00:50:11 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.10476608187134505 on epoch=699
03/02/2022 00:50:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 00:50:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=706
03/02/2022 00:50:18 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/02/2022 00:50:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
03/02/2022 00:50:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 00:50:24 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.16063492063492063 on epoch=716
03/02/2022 00:50:26 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
03/02/2022 00:50:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
03/02/2022 00:50:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
03/02/2022 00:50:32 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
03/02/2022 00:50:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 00:50:36 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.08673913043478261 on epoch=733
03/02/2022 00:50:38 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
03/02/2022 00:50:40 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
03/02/2022 00:50:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=743
03/02/2022 00:50:45 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=746
03/02/2022 00:50:47 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 00:50:48 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.21000000000000002 on epoch=749
03/02/2022 00:50:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 00:50:53 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=756
03/02/2022 00:50:55 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
03/02/2022 00:50:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 00:51:00 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
03/02/2022 00:51:01 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.15579248658318426 on epoch=766
03/02/2022 00:51:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 00:51:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 00:51:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 00:51:10 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 00:51:12 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 00:51:13 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.10607315389924085 on epoch=783
03/02/2022 00:51:16 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 00:51:18 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
03/02/2022 00:51:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 00:51:23 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
03/02/2022 00:51:25 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=799
03/02/2022 00:51:26 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.2704307187065808 on epoch=799
03/02/2022 00:51:28 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 00:51:31 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 00:51:33 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 00:51:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 00:51:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 00:51:39 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.14642901813633522 on epoch=816
03/02/2022 00:51:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=819
03/02/2022 00:51:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 00:51:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 00:51:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 00:51:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 00:51:51 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.2068040654997177 on epoch=833
03/02/2022 00:51:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 00:51:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 00:51:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 00:52:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 00:52:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
03/02/2022 00:52:04 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.15405831363278172 on epoch=849
03/02/2022 00:52:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
03/02/2022 00:52:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 00:52:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 00:52:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 00:52:15 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 00:52:16 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.26315618503118504 on epoch=866
03/02/2022 00:52:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 00:52:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 00:52:23 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
03/02/2022 00:52:25 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 00:52:27 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 00:52:29 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.16263736263736264 on epoch=883
03/02/2022 00:52:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 00:52:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 00:52:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 00:52:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 00:52:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 00:52:41 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.25866402116402115 on epoch=899
03/02/2022 00:52:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 00:52:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 00:52:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 00:52:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
03/02/2022 00:52:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 00:52:54 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.2163095238095238 on epoch=916
03/02/2022 00:52:56 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 00:52:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 00:53:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 00:53:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 00:53:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 00:53:06 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.2530943830410376 on epoch=933
03/02/2022 00:53:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 00:53:11 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 00:53:13 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
03/02/2022 00:53:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 00:53:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 00:53:19 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.1233974358974359 on epoch=949
03/02/2022 00:53:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
03/02/2022 00:53:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 00:53:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 00:53:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 00:53:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 00:53:31 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.1992615227909346 on epoch=966
03/02/2022 00:53:34 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 00:53:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=973
03/02/2022 00:53:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 00:53:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 00:53:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
03/02/2022 00:53:44 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.2900813008130081 on epoch=983
03/02/2022 00:53:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 00:53:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 00:53:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
03/02/2022 00:53:53 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 00:53:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 00:53:57 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:53:57 - INFO - __main__ - Printing 3 examples
03/02/2022 00:53:57 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 00:53:57 - INFO - __main__ - ['offensive']
03/02/2022 00:53:57 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 00:53:57 - INFO - __main__ - ['offensive']
03/02/2022 00:53:57 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 00:53:57 - INFO - __main__ - ['offensive']
03/02/2022 00:53:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 00:53:57 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:53:57 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.2870915032679739 on epoch=999
03/02/2022 00:53:57 - INFO - __main__ - save last model!
03/02/2022 00:53:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 00:53:57 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 00:53:57 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:53:57 - INFO - __main__ - Printing 3 examples
03/02/2022 00:53:57 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 00:53:57 - INFO - __main__ - ['offensive']
03/02/2022 00:53:57 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 00:53:57 - INFO - __main__ - ['offensive']
03/02/2022 00:53:57 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 00:53:57 - INFO - __main__ - ['offensive']
03/02/2022 00:53:57 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:53:57 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 00:53:57 - INFO - __main__ - Printing 3 examples
03/02/2022 00:53:57 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 00:53:57 - INFO - __main__ - ['normal']
03/02/2022 00:53:57 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 00:53:57 - INFO - __main__ - ['normal']
03/02/2022 00:53:57 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 00:53:57 - INFO - __main__ - ['normal']
03/02/2022 00:53:57 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 00:53:57 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:53:57 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 00:53:58 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:53:59 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 00:54:10 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 00:54:10 - INFO - __main__ - task name: hatexplain
initialize from c4
[(2594, 403711), (738, 1187017), (73, 2525281), (3583, 284375), (773, 1132610), (2099, 182268), (1757, 390014), (3023, 334230), (7161, 192247), (2231, 427234), (6082, 173261), (1461, 672164), (3573, 163617), (6209, 162555), (5259, 197913), (2697, 380850), (4225, 197872), (2319, 230472), (1994, 479967), (993, 886924), (1069, 868644), (4404, 216707), (4001, 264258), (4585, 221032), (4434, 228933), (5229, 200589), (629, 1463788), (1101, 860462), (661, 1317687), (3147, 232857), (2132, 277175), (1102, 798392), (2542, 371822), (2503, 477739), (2036, 480551), (5480, 174200), (4952, 209263), (397, 637384), (4682, 222111), (2030, 497571), (5234, 202177), (2665, 369855), (757, 1071575), (4370, 210982), (4895, 169946), (1085, 862457), (705, 1548305), (4508, 172897), (167, 5559739), (1112, 892303), (2193, 392132), (1396, 545283), (4374, 230866), (3385, 191872), (5816, 194070), (2358, 427447), (4191, 233374), (646, 1377071), (1790, 550995), (4264, 247591), (22, 38484130), (2415, 239286), (2392, 349952), (4190, 205854), (2833, 327549), (5032, 221170), (6353, 168404), (5324, 191543), (1518, 582487), (1894, 481117), (3228, 315556), (2265, 421208), (636, 1378362), (6548, 161733), (6098, 172422), (696, 1306625), (3452, 300052), (717, 1408825), (573, 1561543), (4562, 160268), (4277, 195942), (5640, 186094), (2551, 176659), (1249, 573784), (631, 1412370), (4905, 184294), (877, 1019947), (1758, 490726), (767, 1164520), (2558, 157757), (518, 1170336), (1534, 393440), (3774, 232302), (1269, 737181), (6699, 164021), (2888, 375431), (6496, 161755), (4973, 157902), (348, 1943901)]
03/02/2022 00:54:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 00:54:11 - INFO - __main__ - Starting training!
03/02/2022 00:54:49 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_100_0.4_8_predictions.txt
03/02/2022 00:54:49 - INFO - __main__ - Classification-F1 on test data: 0.1035
03/02/2022 00:54:49 - INFO - __main__ - prefix=hatexplain_16_100, lr=0.4, bsz=8, dev_performance=0.3432679738562092, test_performance=0.10347748442291857
03/02/2022 00:54:49 - INFO - __main__ - Running ... prefix=hatexplain_16_100, lr=0.3, bsz=8 ...
03/02/2022 00:54:50 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:54:50 - INFO - __main__ - Printing 3 examples
03/02/2022 00:54:50 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 00:54:50 - INFO - __main__ - ['offensive']
03/02/2022 00:54:50 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 00:54:50 - INFO - __main__ - ['offensive']
03/02/2022 00:54:50 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 00:54:50 - INFO - __main__ - ['offensive']
03/02/2022 00:54:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 00:54:50 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:54:50 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 00:54:50 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:54:50 - INFO - __main__ - Printing 3 examples
03/02/2022 00:54:50 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 00:54:50 - INFO - __main__ - ['offensive']
03/02/2022 00:54:50 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 00:54:50 - INFO - __main__ - ['offensive']
03/02/2022 00:54:50 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 00:54:50 - INFO - __main__ - ['offensive']
03/02/2022 00:54:50 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:54:50 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:54:50 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 00:55:04 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 00:55:04 - INFO - __main__ - task name: hatexplain
initialize from c4
[(2594, 403711), (738, 1187017), (73, 2525281), (3583, 284375), (773, 1132610), (2099, 182268), (1757, 390014), (3023, 334230), (7161, 192247), (2231, 427234), (6082, 173261), (1461, 672164), (3573, 163617), (6209, 162555), (5259, 197913), (2697, 380850), (4225, 197872), (2319, 230472), (1994, 479967), (993, 886924), (1069, 868644), (4404, 216707), (4001, 264258), (4585, 221032), (4434, 228933), (5229, 200589), (629, 1463788), (1101, 860462), (661, 1317687), (3147, 232857), (2132, 277175), (1102, 798392), (2542, 371822), (2503, 477739), (2036, 480551), (5480, 174200), (4952, 209263), (397, 637384), (4682, 222111), (2030, 497571), (5234, 202177), (2665, 369855), (757, 1071575), (4370, 210982), (4895, 169946), (1085, 862457), (705, 1548305), (4508, 172897), (167, 5559739), (1112, 892303), (2193, 392132), (1396, 545283), (4374, 230866), (3385, 191872), (5816, 194070), (2358, 427447), (4191, 233374), (646, 1377071), (1790, 550995), (4264, 247591), (22, 38484130), (2415, 239286), (2392, 349952), (4190, 205854), (2833, 327549), (5032, 221170), (6353, 168404), (5324, 191543), (1518, 582487), (1894, 481117), (3228, 315556), (2265, 421208), (636, 1378362), (6548, 161733), (6098, 172422), (696, 1306625), (3452, 300052), (717, 1408825), (573, 1561543), (4562, 160268), (4277, 195942), (5640, 186094), (2551, 176659), (1249, 573784), (631, 1412370), (4905, 184294), (877, 1019947), (1758, 490726), (767, 1164520), (2558, 157757), (518, 1170336), (1534, 393440), (3774, 232302), (1269, 737181), (6699, 164021), (2888, 375431), (6496, 161755), (4973, 157902), (348, 1943901)]
03/02/2022 00:55:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 00:55:05 - INFO - __main__ - Starting training!
03/02/2022 00:55:08 - INFO - __main__ - Step 10 Global step 10 Train loss 7.06 on epoch=3
03/02/2022 00:55:10 - INFO - __main__ - Step 20 Global step 20 Train loss 4.06 on epoch=6
03/02/2022 00:55:12 - INFO - __main__ - Step 30 Global step 30 Train loss 1.86 on epoch=9
03/02/2022 00:55:15 - INFO - __main__ - Step 40 Global step 40 Train loss 1.21 on epoch=13
03/02/2022 00:55:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.89 on epoch=16
03/02/2022 00:55:18 - INFO - __main__ - Global step 50 Train loss 3.02 Classification-F1 0.20123755716976058 on epoch=16
03/02/2022 00:55:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.20123755716976058 on epoch=16, global_step=50
03/02/2022 00:55:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.74 on epoch=19
03/02/2022 00:55:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.63 on epoch=23
03/02/2022 00:55:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.56 on epoch=26
03/02/2022 00:55:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=29
03/02/2022 00:55:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=33
03/02/2022 00:55:30 - INFO - __main__ - Global step 100 Train loss 0.62 Classification-F1 0.17486338797814208 on epoch=33
03/02/2022 00:55:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=36
03/02/2022 00:55:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=39
03/02/2022 00:55:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=43
03/02/2022 00:55:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=46
03/02/2022 00:55:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
03/02/2022 00:55:43 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.2774327122153209 on epoch=49
03/02/2022 00:55:43 - INFO - __main__ - Saving model with best Classification-F1: 0.20123755716976058 -> 0.2774327122153209 on epoch=49, global_step=150
03/02/2022 00:55:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=53
03/02/2022 00:55:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=56
03/02/2022 00:55:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=59
03/02/2022 00:55:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=63
03/02/2022 00:55:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=66
03/02/2022 00:55:55 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=66
03/02/2022 00:55:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=69
03/02/2022 00:56:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=73
03/02/2022 00:56:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=76
03/02/2022 00:56:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=79
03/02/2022 00:56:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=83
03/02/2022 00:56:08 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16823406478578895 on epoch=83
03/02/2022 00:56:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=86
03/02/2022 00:56:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=89
03/02/2022 00:56:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=93
03/02/2022 00:56:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=96
03/02/2022 00:56:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=99
03/02/2022 00:56:20 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.22962497381102032 on epoch=99
03/02/2022 00:56:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=103
03/02/2022 00:56:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
03/02/2022 00:56:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
03/02/2022 00:56:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=113
03/02/2022 00:56:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=116
03/02/2022 00:56:33 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.27963150207841814 on epoch=116
03/02/2022 00:56:33 - INFO - __main__ - Saving model with best Classification-F1: 0.2774327122153209 -> 0.27963150207841814 on epoch=116, global_step=350
03/02/2022 00:56:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=119
03/02/2022 00:56:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=123
03/02/2022 00:56:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=126
03/02/2022 00:56:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=129
03/02/2022 00:56:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=133
03/02/2022 00:56:46 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.19999999999999998 on epoch=133
03/02/2022 00:56:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=136
03/02/2022 00:56:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=139
03/02/2022 00:56:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=143
03/02/2022 00:56:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=146
03/02/2022 00:56:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=149
03/02/2022 00:56:58 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.22456922162804516 on epoch=149
03/02/2022 00:57:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=153
03/02/2022 00:57:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=156
03/02/2022 00:57:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=159
03/02/2022 00:57:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=163
03/02/2022 00:57:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=166
03/02/2022 00:57:11 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.2915876027830487 on epoch=166
03/02/2022 00:57:11 - INFO - __main__ - Saving model with best Classification-F1: 0.27963150207841814 -> 0.2915876027830487 on epoch=166, global_step=500
03/02/2022 00:57:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=169
03/02/2022 00:57:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=173
03/02/2022 00:57:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=176
03/02/2022 00:57:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=179
03/02/2022 00:57:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=183
03/02/2022 00:57:23 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.18666666666666668 on epoch=183
03/02/2022 00:57:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=186
03/02/2022 00:57:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=189
03/02/2022 00:57:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=193
03/02/2022 00:57:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=196
03/02/2022 00:57:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=199
03/02/2022 00:57:36 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.23347427766032416 on epoch=199
03/02/2022 00:57:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=203
03/02/2022 00:57:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=206
03/02/2022 00:57:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=209
03/02/2022 00:57:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=213
03/02/2022 00:57:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=216
03/02/2022 00:57:48 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.1481481481481481 on epoch=216
03/02/2022 00:57:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=219
03/02/2022 00:57:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=223
03/02/2022 00:57:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=226
03/02/2022 00:57:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=229
03/02/2022 00:58:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=233
03/02/2022 00:58:01 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.2771804062126643 on epoch=233
03/02/2022 00:58:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=236
03/02/2022 00:58:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=239
03/02/2022 00:58:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=243
03/02/2022 00:58:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=246
03/02/2022 00:58:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=249
03/02/2022 00:58:14 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.2481203007518797 on epoch=249
03/02/2022 00:58:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=253
03/02/2022 00:58:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=256
03/02/2022 00:58:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=259
03/02/2022 00:58:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.29 on epoch=263
03/02/2022 00:58:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=266
03/02/2022 00:58:26 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.22626262626262625 on epoch=266
03/02/2022 00:58:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=269
03/02/2022 00:58:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=273
03/02/2022 00:58:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=276
03/02/2022 00:58:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=279
03/02/2022 00:58:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=283
03/02/2022 00:58:39 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.19272819730485635 on epoch=283
03/02/2022 00:58:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=286
03/02/2022 00:58:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=289
03/02/2022 00:58:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=293
03/02/2022 00:58:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=296
03/02/2022 00:58:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=299
03/02/2022 00:58:51 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.16677935542032907 on epoch=299
03/02/2022 00:58:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=303
03/02/2022 00:58:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=306
03/02/2022 00:58:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=309
03/02/2022 00:59:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=313
03/02/2022 00:59:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=316
03/02/2022 00:59:04 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.24195575666163904 on epoch=316
03/02/2022 00:59:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=319
03/02/2022 00:59:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=323
03/02/2022 00:59:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=326
03/02/2022 00:59:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=329
03/02/2022 00:59:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=333
03/02/2022 00:59:16 - INFO - __main__ - Global step 1000 Train loss 0.18 Classification-F1 0.3631578947368421 on epoch=333
03/02/2022 00:59:16 - INFO - __main__ - Saving model with best Classification-F1: 0.2915876027830487 -> 0.3631578947368421 on epoch=333, global_step=1000
03/02/2022 00:59:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=336
03/02/2022 00:59:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=339
03/02/2022 00:59:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=343
03/02/2022 00:59:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=346
03/02/2022 00:59:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=349
03/02/2022 00:59:29 - INFO - __main__ - Global step 1050 Train loss 0.17 Classification-F1 0.21212121212121213 on epoch=349
03/02/2022 00:59:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=353
03/02/2022 00:59:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=356
03/02/2022 00:59:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=359
03/02/2022 00:59:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=363
03/02/2022 00:59:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=366
03/02/2022 00:59:41 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.32808476926123986 on epoch=366
03/02/2022 00:59:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=369
03/02/2022 00:59:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=373
03/02/2022 00:59:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=376
03/02/2022 00:59:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=379
03/02/2022 00:59:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=383
03/02/2022 00:59:54 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.21236470296620671 on epoch=383
03/02/2022 00:59:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=386
03/02/2022 00:59:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=389
03/02/2022 01:00:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=393
03/02/2022 01:00:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=396
03/02/2022 01:00:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=399
03/02/2022 01:00:06 - INFO - __main__ - Global step 1200 Train loss 0.10 Classification-F1 0.17813229474840528 on epoch=399
03/02/2022 01:00:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=403
03/02/2022 01:00:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=406
03/02/2022 01:00:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=409
03/02/2022 01:00:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=413
03/02/2022 01:00:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=416
03/02/2022 01:00:18 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.23663003663003662 on epoch=416
03/02/2022 01:00:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=419
03/02/2022 01:00:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=423
03/02/2022 01:00:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=426
03/02/2022 01:00:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=429
03/02/2022 01:00:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=433
03/02/2022 01:00:30 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.17099957099957097 on epoch=433
03/02/2022 01:00:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=436
03/02/2022 01:00:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.11 on epoch=439
03/02/2022 01:00:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=443
03/02/2022 01:00:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=446
03/02/2022 01:00:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=449
03/02/2022 01:00:43 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.24096749811035525 on epoch=449
03/02/2022 01:00:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=453
03/02/2022 01:00:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=456
03/02/2022 01:00:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=459
03/02/2022 01:00:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=463
03/02/2022 01:00:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
03/02/2022 01:00:55 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.31684981684981683 on epoch=466
03/02/2022 01:00:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
03/02/2022 01:00:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=473
03/02/2022 01:01:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=476
03/02/2022 01:01:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=479
03/02/2022 01:01:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
03/02/2022 01:01:07 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.19254709254709254 on epoch=483
03/02/2022 01:01:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=486
03/02/2022 01:01:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
03/02/2022 01:01:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
03/02/2022 01:01:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
03/02/2022 01:01:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
03/02/2022 01:01:19 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.41269841269841273 on epoch=499
03/02/2022 01:01:19 - INFO - __main__ - Saving model with best Classification-F1: 0.3631578947368421 -> 0.41269841269841273 on epoch=499, global_step=1500
03/02/2022 01:01:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
03/02/2022 01:01:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
03/02/2022 01:01:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
03/02/2022 01:01:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
03/02/2022 01:01:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
03/02/2022 01:01:32 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.2534482758620689 on epoch=516
03/02/2022 01:01:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
03/02/2022 01:01:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=523
03/02/2022 01:01:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
03/02/2022 01:01:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=529
03/02/2022 01:01:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
03/02/2022 01:01:44 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.33625541125541125 on epoch=533
03/02/2022 01:01:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=536
03/02/2022 01:01:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=539
03/02/2022 01:01:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
03/02/2022 01:01:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
03/02/2022 01:01:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
03/02/2022 01:01:57 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.29778739778739777 on epoch=549
03/02/2022 01:01:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
03/02/2022 01:02:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=556
03/02/2022 01:02:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
03/02/2022 01:02:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
03/02/2022 01:02:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
03/02/2022 01:02:09 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.21504576659038901 on epoch=566
03/02/2022 01:02:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
03/02/2022 01:02:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
03/02/2022 01:02:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
03/02/2022 01:02:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=579
03/02/2022 01:02:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
03/02/2022 01:02:21 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.3450460829493088 on epoch=583
03/02/2022 01:02:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
03/02/2022 01:02:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
03/02/2022 01:02:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
03/02/2022 01:02:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=596
03/02/2022 01:02:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
03/02/2022 01:02:33 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.36467617349970294 on epoch=599
03/02/2022 01:02:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
03/02/2022 01:02:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
03/02/2022 01:02:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
03/02/2022 01:02:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/02/2022 01:02:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
03/02/2022 01:02:46 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.18156761412575362 on epoch=616
03/02/2022 01:02:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
03/02/2022 01:02:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
03/02/2022 01:02:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=626
03/02/2022 01:02:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
03/02/2022 01:02:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=633
03/02/2022 01:02:58 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.15009652509652507 on epoch=633
03/02/2022 01:03:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
03/02/2022 01:03:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
03/02/2022 01:03:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
03/02/2022 01:03:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
03/02/2022 01:03:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
03/02/2022 01:03:10 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.30923792486583185 on epoch=649
03/02/2022 01:03:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
03/02/2022 01:03:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
03/02/2022 01:03:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
03/02/2022 01:03:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
03/02/2022 01:03:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 01:03:23 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.24029593594810983 on epoch=666
03/02/2022 01:03:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 01:03:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
03/02/2022 01:03:29 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
03/02/2022 01:03:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=679
03/02/2022 01:03:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=683
03/02/2022 01:03:35 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.32772881880024735 on epoch=683
03/02/2022 01:03:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
03/02/2022 01:03:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
03/02/2022 01:03:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
03/02/2022 01:03:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 01:03:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 01:03:48 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.32594086021505375 on epoch=699
03/02/2022 01:03:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 01:03:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
03/02/2022 01:03:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
03/02/2022 01:03:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 01:03:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=716
03/02/2022 01:04:02 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.21402724563644102 on epoch=716
03/02/2022 01:04:04 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/02/2022 01:04:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
03/02/2022 01:04:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
03/02/2022 01:04:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
03/02/2022 01:04:13 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
03/02/2022 01:04:14 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.22003878474466712 on epoch=733
03/02/2022 01:04:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
03/02/2022 01:04:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
03/02/2022 01:04:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 01:04:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 01:04:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
03/02/2022 01:04:26 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.26620387285348923 on epoch=749
03/02/2022 01:04:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
03/02/2022 01:04:31 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
03/02/2022 01:04:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=759
03/02/2022 01:04:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 01:04:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 01:04:39 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.21668323266304762 on epoch=766
03/02/2022 01:04:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
03/02/2022 01:04:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 01:04:46 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
03/02/2022 01:04:48 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 01:04:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 01:04:57 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.2454906204906205 on epoch=783
03/02/2022 01:05:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
03/02/2022 01:05:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 01:05:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 01:05:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
03/02/2022 01:05:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 01:05:11 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.19692307692307692 on epoch=799
03/02/2022 01:05:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 01:05:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
03/02/2022 01:05:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=809
03/02/2022 01:05:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
03/02/2022 01:05:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 01:05:25 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.2675438596491228 on epoch=816
03/02/2022 01:05:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 01:05:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 01:05:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 01:05:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 01:05:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 01:05:38 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.4342676009342676 on epoch=833
03/02/2022 01:05:38 - INFO - __main__ - Saving model with best Classification-F1: 0.41269841269841273 -> 0.4342676009342676 on epoch=833, global_step=2500
03/02/2022 01:05:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=836
03/02/2022 01:05:43 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
03/02/2022 01:05:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 01:05:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
03/02/2022 01:05:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
03/02/2022 01:05:53 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.39804639804639813 on epoch=849
03/02/2022 01:05:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
03/02/2022 01:05:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 01:06:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
03/02/2022 01:06:02 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
03/02/2022 01:06:04 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 01:06:07 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.30547504025764893 on epoch=866
03/02/2022 01:06:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=869
03/02/2022 01:06:11 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
03/02/2022 01:06:14 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 01:06:16 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 01:06:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 01:06:21 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.38865731969180245 on epoch=883
03/02/2022 01:06:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 01:06:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 01:06:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 01:06:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 01:06:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
03/02/2022 01:06:35 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.31269830201178483 on epoch=899
03/02/2022 01:06:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 01:06:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 01:06:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 01:06:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
03/02/2022 01:06:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 01:06:49 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.2296276908983838 on epoch=916
03/02/2022 01:06:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=919
03/02/2022 01:06:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 01:06:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 01:06:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 01:07:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 01:07:03 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.22058823529411767 on epoch=933
03/02/2022 01:07:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 01:07:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=939
03/02/2022 01:07:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 01:07:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=946
03/02/2022 01:07:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 01:07:16 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.16990863542587684 on epoch=949
03/02/2022 01:07:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
03/02/2022 01:07:21 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 01:07:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 01:07:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 01:07:27 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 01:07:31 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.1775793650793651 on epoch=966
03/02/2022 01:07:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=969
03/02/2022 01:07:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
03/02/2022 01:07:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 01:07:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
03/02/2022 01:07:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 01:07:43 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.18961247086247088 on epoch=983
03/02/2022 01:07:45 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=986
03/02/2022 01:07:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
03/02/2022 01:07:50 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 01:07:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
03/02/2022 01:07:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 01:07:55 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:07:55 - INFO - __main__ - Printing 3 examples
03/02/2022 01:07:55 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 01:07:55 - INFO - __main__ - ['offensive']
03/02/2022 01:07:55 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 01:07:55 - INFO - __main__ - ['offensive']
03/02/2022 01:07:55 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 01:07:55 - INFO - __main__ - ['offensive']
03/02/2022 01:07:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 01:07:55 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:07:55 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:07:55 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:07:55 - INFO - __main__ - Printing 3 examples
03/02/2022 01:07:55 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 01:07:55 - INFO - __main__ - ['offensive']
03/02/2022 01:07:55 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 01:07:55 - INFO - __main__ - ['offensive']
03/02/2022 01:07:55 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 01:07:55 - INFO - __main__ - ['offensive']
03/02/2022 01:07:55 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:07:55 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:07:55 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:07:57 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.2142623716153128 on epoch=999
03/02/2022 01:07:57 - INFO - __main__ - save last model!
03/02/2022 01:07:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 01:07:57 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 01:07:57 - INFO - __main__ - Printing 3 examples
03/02/2022 01:07:57 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 01:07:57 - INFO - __main__ - ['normal']
03/02/2022 01:07:57 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 01:07:57 - INFO - __main__ - ['normal']
03/02/2022 01:07:57 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 01:07:57 - INFO - __main__ - ['normal']
03/02/2022 01:07:57 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:07:58 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:08:00 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 01:08:08 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 01:08:08 - INFO - __main__ - task name: hatexplain
initialize from c4
[(907, 999819), (4328, 223698), (7511, 160346), (5954, 165280), (4511, 179764), (3531, 297895), (2852, 355056), (1714, 447470), (3237, 299613), (2554, 389221), (1462, 349360), (5120, 236973), (1511, 676741), (2846, 312704), (4848, 204967), (3155, 298023), (976, 1102538), (99, 8503604), (4345, 204361), (5077, 166084), (1077, 853741), (930, 1093140), (5712, 173941), (2441, 429114), (5696, 184838), (2392, 349952), (1363, 669374), (2856, 268051), (554, 1156528), (2131, 378214), (3105, 254084), (3315, 327974), (1735, 571995), (4183, 213298), (6331, 171986), (3243, 311061), (5189, 177490), (91, 9930046), (4320, 168819), (2577, 313511), (1273, 833860), (1466, 664308), (3001, 351252), (955, 764542), (1170, 817452), (6309, 169174), (986, 581568), (2381, 341017), (3127, 331271), (7594, 155434), (2456, 433793), (3716, 273699), (2024, 448261), (2411, 351989), (5529, 185095), (3351, 190868), (2717, 368013), (580, 1542140), (1019, 901860), (4585, 221032), (2887, 336279), (514, 1732295), (42, 24506569), (3827, 282190), (1368, 677011), (2835, 354320), (1751, 560750), (4764, 211573), (6349, 169524), (4554, 218517), (6817, 166736), (103, 8154869), (1142, 782542), (809, 1073970), (1651, 598254), (5782, 171468), (616, 2195072), (4145, 254483), (6957, 160765), (6212, 169413), (1519, 600961), (4894, 217353), (944, 693547), (124, 1842230), (2122, 305087), (3931, 257603), (453, 1935608), (4016, 262231), (1677, 434886), (2591, 370033), (953, 840707), (1022, 264436), (4487, 166978), (4374, 230866), (1499, 582683), (2605, 391810), (1675, 558878), (948, 512142), (4647, 228104)]
03/02/2022 01:08:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:08:09 - INFO - __main__ - Starting training!
03/02/2022 01:09:28 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_100_0.3_8_predictions.txt
03/02/2022 01:09:28 - INFO - __main__ - Classification-F1 on test data: 0.0830
03/02/2022 01:09:29 - INFO - __main__ - prefix=hatexplain_16_100, lr=0.3, bsz=8, dev_performance=0.4342676009342676, test_performance=0.08300483825697628
03/02/2022 01:09:29 - INFO - __main__ - Running ... prefix=hatexplain_16_100, lr=0.2, bsz=8 ...
03/02/2022 01:09:30 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:09:30 - INFO - __main__ - Printing 3 examples
03/02/2022 01:09:30 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 01:09:30 - INFO - __main__ - ['offensive']
03/02/2022 01:09:30 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 01:09:30 - INFO - __main__ - ['offensive']
03/02/2022 01:09:30 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 01:09:30 - INFO - __main__ - ['offensive']
03/02/2022 01:09:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:09:30 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:09:30 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:09:30 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:09:30 - INFO - __main__ - Printing 3 examples
03/02/2022 01:09:30 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 01:09:30 - INFO - __main__ - ['offensive']
03/02/2022 01:09:30 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 01:09:30 - INFO - __main__ - ['offensive']
03/02/2022 01:09:30 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 01:09:30 - INFO - __main__ - ['offensive']
03/02/2022 01:09:30 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:09:30 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:09:30 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:09:44 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 01:09:44 - INFO - __main__ - task name: hatexplain
initialize from c4
[(907, 999819), (4328, 223698), (7511, 160346), (5954, 165280), (4511, 179764), (3531, 297895), (2852, 355056), (1714, 447470), (3237, 299613), (2554, 389221), (1462, 349360), (5120, 236973), (1511, 676741), (2846, 312704), (4848, 204967), (3155, 298023), (976, 1102538), (99, 8503604), (4345, 204361), (5077, 166084), (1077, 853741), (930, 1093140), (5712, 173941), (2441, 429114), (5696, 184838), (2392, 349952), (1363, 669374), (2856, 268051), (554, 1156528), (2131, 378214), (3105, 254084), (3315, 327974), (1735, 571995), (4183, 213298), (6331, 171986), (3243, 311061), (5189, 177490), (91, 9930046), (4320, 168819), (2577, 313511), (1273, 833860), (1466, 664308), (3001, 351252), (955, 764542), (1170, 817452), (6309, 169174), (986, 581568), (2381, 341017), (3127, 331271), (7594, 155434), (2456, 433793), (3716, 273699), (2024, 448261), (2411, 351989), (5529, 185095), (3351, 190868), (2717, 368013), (580, 1542140), (1019, 901860), (4585, 221032), (2887, 336279), (514, 1732295), (42, 24506569), (3827, 282190), (1368, 677011), (2835, 354320), (1751, 560750), (4764, 211573), (6349, 169524), (4554, 218517), (6817, 166736), (103, 8154869), (1142, 782542), (809, 1073970), (1651, 598254), (5782, 171468), (616, 2195072), (4145, 254483), (6957, 160765), (6212, 169413), (1519, 600961), (4894, 217353), (944, 693547), (124, 1842230), (2122, 305087), (3931, 257603), (453, 1935608), (4016, 262231), (1677, 434886), (2591, 370033), (953, 840707), (1022, 264436), (4487, 166978), (4374, 230866), (1499, 582683), (2605, 391810), (1675, 558878), (948, 512142), (4647, 228104)]
03/02/2022 01:09:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:09:44 - INFO - __main__ - Starting training!
03/02/2022 01:09:47 - INFO - __main__ - Step 10 Global step 10 Train loss 7.06 on epoch=3
03/02/2022 01:09:49 - INFO - __main__ - Step 20 Global step 20 Train loss 4.94 on epoch=6
03/02/2022 01:09:51 - INFO - __main__ - Step 30 Global step 30 Train loss 3.11 on epoch=9
03/02/2022 01:09:54 - INFO - __main__ - Step 40 Global step 40 Train loss 1.91 on epoch=13
03/02/2022 01:09:56 - INFO - __main__ - Step 50 Global step 50 Train loss 1.10 on epoch=16
03/02/2022 01:09:57 - INFO - __main__ - Global step 50 Train loss 3.63 Classification-F1 0.23298358891579232 on epoch=16
03/02/2022 01:09:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.23298358891579232 on epoch=16, global_step=50
03/02/2022 01:09:59 - INFO - __main__ - Step 60 Global step 60 Train loss 0.97 on epoch=19
03/02/2022 01:10:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.80 on epoch=23
03/02/2022 01:10:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.65 on epoch=26
03/02/2022 01:10:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.64 on epoch=29
03/02/2022 01:10:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.62 on epoch=33
03/02/2022 01:10:09 - INFO - __main__ - Global step 100 Train loss 0.74 Classification-F1 0.23333333333333336 on epoch=33
03/02/2022 01:10:09 - INFO - __main__ - Saving model with best Classification-F1: 0.23298358891579232 -> 0.23333333333333336 on epoch=33, global_step=100
03/02/2022 01:10:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.63 on epoch=36
03/02/2022 01:10:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.65 on epoch=39
03/02/2022 01:10:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=43
03/02/2022 01:10:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.60 on epoch=46
03/02/2022 01:10:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=49
03/02/2022 01:10:21 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.16091954022988506 on epoch=49
03/02/2022 01:10:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=53
03/02/2022 01:10:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=56
03/02/2022 01:10:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=59
03/02/2022 01:10:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=63
03/02/2022 01:10:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=66
03/02/2022 01:10:33 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=66
03/02/2022 01:10:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=69
03/02/2022 01:10:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.58 on epoch=73
03/02/2022 01:10:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=76
03/02/2022 01:10:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=79
03/02/2022 01:10:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
03/02/2022 01:10:45 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.15555555555555556 on epoch=83
03/02/2022 01:10:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=86
03/02/2022 01:10:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=89
03/02/2022 01:10:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=93
03/02/2022 01:10:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=96
03/02/2022 01:10:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=99
03/02/2022 01:10:57 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.17204301075268816 on epoch=99
03/02/2022 01:10:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=103
03/02/2022 01:11:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=106
03/02/2022 01:11:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=109
03/02/2022 01:11:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=113
03/02/2022 01:11:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=116
03/02/2022 01:11:09 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.2099511072763877 on epoch=116
03/02/2022 01:11:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=119
03/02/2022 01:11:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=123
03/02/2022 01:11:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=126
03/02/2022 01:11:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=129
03/02/2022 01:11:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=133
03/02/2022 01:11:21 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=133
03/02/2022 01:11:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=136
03/02/2022 01:11:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=139
03/02/2022 01:11:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=143
03/02/2022 01:11:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=146
03/02/2022 01:11:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=149
03/02/2022 01:11:33 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.2085278555866791 on epoch=149
03/02/2022 01:11:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=153
03/02/2022 01:11:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=156
03/02/2022 01:11:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=159
03/02/2022 01:11:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=163
03/02/2022 01:11:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=166
03/02/2022 01:11:45 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.15873015873015875 on epoch=166
03/02/2022 01:11:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=169
03/02/2022 01:11:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=173
03/02/2022 01:11:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=176
03/02/2022 01:11:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=179
03/02/2022 01:11:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=183
03/02/2022 01:11:57 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.28218243819266836 on epoch=183
03/02/2022 01:11:57 - INFO - __main__ - Saving model with best Classification-F1: 0.23333333333333336 -> 0.28218243819266836 on epoch=183, global_step=550
03/02/2022 01:11:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=186
03/02/2022 01:12:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=189
03/02/2022 01:12:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=193
03/02/2022 01:12:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=196
03/02/2022 01:12:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=199
03/02/2022 01:12:09 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=199
03/02/2022 01:12:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=203
03/02/2022 01:12:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=206
03/02/2022 01:12:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=209
03/02/2022 01:12:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=213
03/02/2022 01:12:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=216
03/02/2022 01:12:21 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.24928681149879306 on epoch=216
03/02/2022 01:12:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=219
03/02/2022 01:12:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=223
03/02/2022 01:12:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=226
03/02/2022 01:12:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=229
03/02/2022 01:12:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=233
03/02/2022 01:12:33 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.2630579297245964 on epoch=233
03/02/2022 01:12:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=236
03/02/2022 01:12:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=239
03/02/2022 01:12:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=243
03/02/2022 01:12:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=246
03/02/2022 01:12:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.30 on epoch=249
03/02/2022 01:12:45 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.15873015873015875 on epoch=249
03/02/2022 01:12:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=253
03/02/2022 01:12:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=256
03/02/2022 01:12:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.34 on epoch=259
03/02/2022 01:12:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=263
03/02/2022 01:12:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=266
03/02/2022 01:12:57 - INFO - __main__ - Global step 800 Train loss 0.31 Classification-F1 0.17361111111111108 on epoch=266
03/02/2022 01:12:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=269
03/02/2022 01:13:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=273
03/02/2022 01:13:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=276
03/02/2022 01:13:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=279
03/02/2022 01:13:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=283
03/02/2022 01:13:09 - INFO - __main__ - Global step 850 Train loss 0.30 Classification-F1 0.22146676852559202 on epoch=283
03/02/2022 01:13:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=286
03/02/2022 01:13:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=289
03/02/2022 01:13:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=293
03/02/2022 01:13:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.30 on epoch=296
03/02/2022 01:13:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=299
03/02/2022 01:13:21 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.20620689655172414 on epoch=299
03/02/2022 01:13:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=303
03/02/2022 01:13:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=306
03/02/2022 01:13:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=309
03/02/2022 01:13:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=313
03/02/2022 01:13:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=316
03/02/2022 01:13:33 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.2147562582345191 on epoch=316
03/02/2022 01:13:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=319
03/02/2022 01:13:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=323
03/02/2022 01:13:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=326
03/02/2022 01:13:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.30 on epoch=329
03/02/2022 01:13:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=333
03/02/2022 01:13:45 - INFO - __main__ - Global step 1000 Train loss 0.25 Classification-F1 0.1360062893081761 on epoch=333
03/02/2022 01:13:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=336
03/02/2022 01:13:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=339
03/02/2022 01:13:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=343
03/02/2022 01:13:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=346
03/02/2022 01:13:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=349
03/02/2022 01:13:58 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.1953703703703704 on epoch=349
03/02/2022 01:14:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=353
03/02/2022 01:14:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=356
03/02/2022 01:14:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=359
03/02/2022 01:14:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=363
03/02/2022 01:14:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.15 on epoch=366
03/02/2022 01:14:10 - INFO - __main__ - Global step 1100 Train loss 0.17 Classification-F1 0.14991771513510643 on epoch=366
03/02/2022 01:14:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=369
03/02/2022 01:14:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=373
03/02/2022 01:14:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=376
03/02/2022 01:14:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=379
03/02/2022 01:14:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.15 on epoch=383
03/02/2022 01:14:23 - INFO - __main__ - Global step 1150 Train loss 0.16 Classification-F1 0.13123067408781694 on epoch=383
03/02/2022 01:14:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=386
03/02/2022 01:14:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=389
03/02/2022 01:14:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=393
03/02/2022 01:14:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=396
03/02/2022 01:14:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=399
03/02/2022 01:14:35 - INFO - __main__ - Global step 1200 Train loss 0.14 Classification-F1 0.11224489795918369 on epoch=399
03/02/2022 01:14:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=403
03/02/2022 01:14:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=406
03/02/2022 01:14:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=409
03/02/2022 01:14:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=413
03/02/2022 01:14:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=416
03/02/2022 01:14:47 - INFO - __main__ - Global step 1250 Train loss 0.13 Classification-F1 0.14995288733342305 on epoch=416
03/02/2022 01:14:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=419
03/02/2022 01:14:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=423
03/02/2022 01:14:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.09 on epoch=426
03/02/2022 01:14:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=429
03/02/2022 01:14:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=433
03/02/2022 01:15:00 - INFO - __main__ - Global step 1300 Train loss 0.11 Classification-F1 0.09629028379028377 on epoch=433
03/02/2022 01:15:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=436
03/02/2022 01:15:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.11 on epoch=439
03/02/2022 01:15:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=443
03/02/2022 01:15:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=446
03/02/2022 01:15:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=449
03/02/2022 01:15:12 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.23963133640552997 on epoch=449
03/02/2022 01:15:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=453
03/02/2022 01:15:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=456
03/02/2022 01:15:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=459
03/02/2022 01:15:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=463
03/02/2022 01:15:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=466
03/02/2022 01:15:25 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.10540730070884348 on epoch=466
03/02/2022 01:15:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.10 on epoch=469
03/02/2022 01:15:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=473
03/02/2022 01:15:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=476
03/02/2022 01:15:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=479
03/02/2022 01:15:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=483
03/02/2022 01:15:37 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.15195029624753126 on epoch=483
03/02/2022 01:15:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=486
03/02/2022 01:15:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=489
03/02/2022 01:15:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=493
03/02/2022 01:15:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=496
03/02/2022 01:15:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=499
03/02/2022 01:15:49 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.1604037267080745 on epoch=499
03/02/2022 01:15:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=503
03/02/2022 01:15:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=506
03/02/2022 01:15:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=509
03/02/2022 01:15:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=513
03/02/2022 01:16:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=516
03/02/2022 01:16:02 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.1711217561950732 on epoch=516
03/02/2022 01:16:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
03/02/2022 01:16:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=523
03/02/2022 01:16:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=526
03/02/2022 01:16:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=529
03/02/2022 01:16:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=533
03/02/2022 01:16:14 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.1600586346502862 on epoch=533
03/02/2022 01:16:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=536
03/02/2022 01:16:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=539
03/02/2022 01:16:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=543
03/02/2022 01:16:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=546
03/02/2022 01:16:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=549
03/02/2022 01:16:27 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.1902454780361757 on epoch=549
03/02/2022 01:16:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
03/02/2022 01:16:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=556
03/02/2022 01:16:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=559
03/02/2022 01:16:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
03/02/2022 01:16:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=566
03/02/2022 01:16:39 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.15886062861869313 on epoch=566
03/02/2022 01:16:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=569
03/02/2022 01:16:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=573
03/02/2022 01:16:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
03/02/2022 01:16:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=579
03/02/2022 01:16:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
03/02/2022 01:16:52 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.17569444444444446 on epoch=583
03/02/2022 01:16:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=586
03/02/2022 01:16:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=589
03/02/2022 01:16:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=593
03/02/2022 01:17:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
03/02/2022 01:17:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=599
03/02/2022 01:17:04 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.12196459135643989 on epoch=599
03/02/2022 01:17:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=603
03/02/2022 01:17:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
03/02/2022 01:17:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=609
03/02/2022 01:17:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=613
03/02/2022 01:17:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=616
03/02/2022 01:17:16 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.24615384615384617 on epoch=616
03/02/2022 01:17:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
03/02/2022 01:17:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=623
03/02/2022 01:17:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
03/02/2022 01:17:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
03/02/2022 01:17:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=633
03/02/2022 01:17:28 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.3306841817186645 on epoch=633
03/02/2022 01:17:28 - INFO - __main__ - Saving model with best Classification-F1: 0.28218243819266836 -> 0.3306841817186645 on epoch=633, global_step=1900
03/02/2022 01:17:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=636
03/02/2022 01:17:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
03/02/2022 01:17:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
03/02/2022 01:17:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=646
03/02/2022 01:17:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
03/02/2022 01:17:41 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.252403156384505 on epoch=649
03/02/2022 01:17:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=653
03/02/2022 01:17:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=656
03/02/2022 01:17:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
03/02/2022 01:17:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
03/02/2022 01:17:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=666
03/02/2022 01:17:53 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.2652676715176715 on epoch=666
03/02/2022 01:17:55 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
03/02/2022 01:17:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
03/02/2022 01:18:00 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
03/02/2022 01:18:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=679
03/02/2022 01:18:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=683
03/02/2022 01:18:05 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.1279985618139122 on epoch=683
03/02/2022 01:18:08 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
03/02/2022 01:18:10 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=689
03/02/2022 01:18:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=693
03/02/2022 01:18:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/02/2022 01:18:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
03/02/2022 01:18:18 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.26343101343101344 on epoch=699
03/02/2022 01:18:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=703
03/02/2022 01:18:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=706
03/02/2022 01:18:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
03/02/2022 01:18:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 01:18:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
03/02/2022 01:18:30 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.16056626300528737 on epoch=716
03/02/2022 01:18:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
03/02/2022 01:18:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
03/02/2022 01:18:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
03/02/2022 01:18:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
03/02/2022 01:18:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=733
03/02/2022 01:18:42 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.16969696969696968 on epoch=733
03/02/2022 01:18:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=736
03/02/2022 01:18:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
03/02/2022 01:18:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
03/02/2022 01:18:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
03/02/2022 01:18:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=749
03/02/2022 01:18:54 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.15696011441283966 on epoch=749
03/02/2022 01:18:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=753
03/02/2022 01:18:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
03/02/2022 01:19:01 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
03/02/2022 01:19:03 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
03/02/2022 01:19:05 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
03/02/2022 01:19:07 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.11462585034013606 on epoch=766
03/02/2022 01:19:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
03/02/2022 01:19:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
03/02/2022 01:19:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=776
03/02/2022 01:19:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=779
03/02/2022 01:19:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
03/02/2022 01:19:19 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.10074074074074074 on epoch=783
03/02/2022 01:19:21 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 01:19:23 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=789
03/02/2022 01:19:26 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
03/02/2022 01:19:28 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=796
03/02/2022 01:19:30 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
03/02/2022 01:19:31 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.09322580645161291 on epoch=799
03/02/2022 01:19:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
03/02/2022 01:19:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 01:19:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
03/02/2022 01:19:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 01:19:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 01:19:44 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.13065217391304348 on epoch=816
03/02/2022 01:19:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
03/02/2022 01:19:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
03/02/2022 01:19:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
03/02/2022 01:19:53 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
03/02/2022 01:19:55 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 01:19:56 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.1301404853128991 on epoch=833
03/02/2022 01:19:58 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 01:20:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
03/02/2022 01:20:03 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
03/02/2022 01:20:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=846
03/02/2022 01:20:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
03/02/2022 01:20:08 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.18258823529411763 on epoch=849
03/02/2022 01:20:10 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=853
03/02/2022 01:20:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
03/02/2022 01:20:15 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 01:20:17 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 01:20:19 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
03/02/2022 01:20:20 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.11428571428571428 on epoch=866
03/02/2022 01:20:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=869
03/02/2022 01:20:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
03/02/2022 01:20:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
03/02/2022 01:20:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 01:20:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=883
03/02/2022 01:20:33 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.12731481481481483 on epoch=883
03/02/2022 01:20:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
03/02/2022 01:20:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.07 on epoch=889
03/02/2022 01:20:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
03/02/2022 01:20:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=896
03/02/2022 01:20:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 01:20:45 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.09821428571428571 on epoch=899
03/02/2022 01:20:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=903
03/02/2022 01:20:50 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
03/02/2022 01:20:52 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 01:20:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
03/02/2022 01:20:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
03/02/2022 01:20:57 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.19629629629629627 on epoch=916
03/02/2022 01:21:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
03/02/2022 01:21:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 01:21:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 01:21:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 01:21:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 01:21:10 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.13077030033370413 on epoch=933
03/02/2022 01:21:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 01:21:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
03/02/2022 01:21:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=943
03/02/2022 01:21:18 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
03/02/2022 01:21:21 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
03/02/2022 01:21:22 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.24606060606060604 on epoch=949
03/02/2022 01:21:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 01:21:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 01:21:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
03/02/2022 01:21:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
03/02/2022 01:21:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
03/02/2022 01:21:34 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.37967948717948724 on epoch=966
03/02/2022 01:21:34 - INFO - __main__ - Saving model with best Classification-F1: 0.3306841817186645 -> 0.37967948717948724 on epoch=966, global_step=2900
03/02/2022 01:21:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
03/02/2022 01:21:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
03/02/2022 01:21:41 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 01:21:43 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
03/02/2022 01:21:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
03/02/2022 01:21:47 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.19173109243697478 on epoch=983
03/02/2022 01:21:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 01:21:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=989
03/02/2022 01:21:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
03/02/2022 01:21:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
03/02/2022 01:21:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=999
03/02/2022 01:21:59 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:21:59 - INFO - __main__ - Printing 3 examples
03/02/2022 01:21:59 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:21:59 - INFO - __main__ - ['hatespeech']
03/02/2022 01:21:59 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:21:59 - INFO - __main__ - ['hatespeech']
03/02/2022 01:21:59 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:21:59 - INFO - __main__ - ['hatespeech']
03/02/2022 01:21:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 01:21:59 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.3181665828724652 on epoch=999
03/02/2022 01:21:59 - INFO - __main__ - save last model!
03/02/2022 01:21:59 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:21:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 01:21:59 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 01:21:59 - INFO - __main__ - Printing 3 examples
03/02/2022 01:21:59 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 01:21:59 - INFO - __main__ - ['normal']
03/02/2022 01:21:59 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 01:21:59 - INFO - __main__ - ['normal']
03/02/2022 01:21:59 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 01:21:59 - INFO - __main__ - ['normal']
03/02/2022 01:21:59 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:21:59 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:21:59 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:21:59 - INFO - __main__ - Printing 3 examples
03/02/2022 01:21:59 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:21:59 - INFO - __main__ - ['hatespeech']
03/02/2022 01:21:59 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:21:59 - INFO - __main__ - ['hatespeech']
03/02/2022 01:21:59 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:21:59 - INFO - __main__ - ['hatespeech']
03/02/2022 01:21:59 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:21:59 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:21:59 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:22:00 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:22:02 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 01:22:12 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 01:22:12 - INFO - __main__ - task name: hatexplain
initialize from c4
[(210, 3214246), (956, 507463), (177, 283247), (2600, 228530), (2320, 373103), (2867, 348102), (1756, 567748), (1056, 811824), (3677, 244767), (451, 1996105), (4656, 199279), (2365, 411961), (2118, 458638), (4865, 202967), (3714, 267548), (2401, 307744), (2330, 404909), (2602, 411193), (281, 3369363), (2021, 474295), (4180, 234955), (3604, 286058), (3074, 334210), (799, 1149745), (2942, 333386), (3102, 266483), (6112, 183777), (4526, 234296), (6478, 174299), (1720, 283907), (4, 2855733), (1237, 765840), (2853, 354788), (1738, 508620), (6440, 161231), (2893, 349540), (429, 2065410), (1164, 807916), (6748, 157449), (4838, 214821), (3568, 270815), (3452, 300052), (4891, 214175), (3640, 155110), (2298, 183293), (1268, 763020), (3449, 243414), (6878, 162543), (1389, 480591), (2350, 363461), (530, 1727807), (4735, 214273), (24, 46921718), (3913, 180488), (5858, 173605), (2172, 463811), (3354, 258314), (3961, 216648), (778, 1141371), (3472, 284740), (3377, 299929), (1425, 711960), (3475, 312556), (2180, 184483), (2567, 304604), (355, 230108), (3404, 288313), (4344, 232964), (3047, 316651), (1753, 168938), (901, 693591), (2107, 476561), (4457, 223114), (4036, 247461), (1860, 522151), (4239, 164202), (3320, 312204), (1536, 231332), (4170, 170840), (8, 234707885), (2896, 309135), (3168, 326177), (2018, 434867), (3916, 217282), (6144, 161446), (6551, 154998), (3510, 291560), (5111, 201364), (4764, 211573), (3258, 211425), (2283, 428223), (6848, 183388), (4505, 197810), (1586, 532514), (715, 971516), (1935, 328964), (5835, 181763), (3170, 281056), (1020, 892057)]
03/02/2022 01:22:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:22:13 - INFO - __main__ - Starting training!
03/02/2022 01:22:52 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_100_0.2_8_predictions.txt
03/02/2022 01:22:52 - INFO - __main__ - Classification-F1 on test data: 0.1133
03/02/2022 01:22:53 - INFO - __main__ - prefix=hatexplain_16_100, lr=0.2, bsz=8, dev_performance=0.37967948717948724, test_performance=0.11327528030475188
03/02/2022 01:22:53 - INFO - __main__ - Running ... prefix=hatexplain_16_13, lr=0.5, bsz=8 ...
03/02/2022 01:22:54 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:22:54 - INFO - __main__ - Printing 3 examples
03/02/2022 01:22:54 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:22:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:22:54 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:22:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:22:54 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:22:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:22:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:22:54 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:22:54 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:22:54 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:22:54 - INFO - __main__ - Printing 3 examples
03/02/2022 01:22:54 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:22:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:22:54 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:22:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:22:54 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:22:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:22:54 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:22:54 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:22:54 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:23:06 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 01:23:06 - INFO - __main__ - task name: hatexplain
initialize from c4
[(210, 3214246), (956, 507463), (177, 283247), (2600, 228530), (2320, 373103), (2867, 348102), (1756, 567748), (1056, 811824), (3677, 244767), (451, 1996105), (4656, 199279), (2365, 411961), (2118, 458638), (4865, 202967), (3714, 267548), (2401, 307744), (2330, 404909), (2602, 411193), (281, 3369363), (2021, 474295), (4180, 234955), (3604, 286058), (3074, 334210), (799, 1149745), (2942, 333386), (3102, 266483), (6112, 183777), (4526, 234296), (6478, 174299), (1720, 283907), (4, 2855733), (1237, 765840), (2853, 354788), (1738, 508620), (6440, 161231), (2893, 349540), (429, 2065410), (1164, 807916), (6748, 157449), (4838, 214821), (3568, 270815), (3452, 300052), (4891, 214175), (3640, 155110), (2298, 183293), (1268, 763020), (3449, 243414), (6878, 162543), (1389, 480591), (2350, 363461), (530, 1727807), (4735, 214273), (24, 46921718), (3913, 180488), (5858, 173605), (2172, 463811), (3354, 258314), (3961, 216648), (778, 1141371), (3472, 284740), (3377, 299929), (1425, 711960), (3475, 312556), (2180, 184483), (2567, 304604), (355, 230108), (3404, 288313), (4344, 232964), (3047, 316651), (1753, 168938), (901, 693591), (2107, 476561), (4457, 223114), (4036, 247461), (1860, 522151), (4239, 164202), (3320, 312204), (1536, 231332), (4170, 170840), (8, 234707885), (2896, 309135), (3168, 326177), (2018, 434867), (3916, 217282), (6144, 161446), (6551, 154998), (3510, 291560), (5111, 201364), (4764, 211573), (3258, 211425), (2283, 428223), (6848, 183388), (4505, 197810), (1586, 532514), (715, 971516), (1935, 328964), (5835, 181763), (3170, 281056), (1020, 892057)]
03/02/2022 01:23:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:23:07 - INFO - __main__ - Starting training!
03/02/2022 01:23:09 - INFO - __main__ - Step 10 Global step 10 Train loss 5.56 on epoch=3
03/02/2022 01:23:12 - INFO - __main__ - Step 20 Global step 20 Train loss 2.05 on epoch=6
03/02/2022 01:23:14 - INFO - __main__ - Step 30 Global step 30 Train loss 0.97 on epoch=9
03/02/2022 01:23:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.70 on epoch=13
03/02/2022 01:23:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.76 on epoch=16
03/02/2022 01:23:20 - INFO - __main__ - Global step 50 Train loss 2.01 Classification-F1 0.27079218784287906 on epoch=16
03/02/2022 01:23:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.27079218784287906 on epoch=16, global_step=50
03/02/2022 01:23:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.64 on epoch=19
03/02/2022 01:23:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.72 on epoch=23
03/02/2022 01:23:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.62 on epoch=26
03/02/2022 01:23:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=29
03/02/2022 01:23:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=33
03/02/2022 01:23:32 - INFO - __main__ - Global step 100 Train loss 0.63 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 01:23:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=36
03/02/2022 01:23:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=39
03/02/2022 01:23:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=43
03/02/2022 01:23:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=46
03/02/2022 01:23:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=49
03/02/2022 01:23:44 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.2333333333333333 on epoch=49
03/02/2022 01:23:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=53
03/02/2022 01:23:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.55 on epoch=56
03/02/2022 01:23:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=59
03/02/2022 01:23:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=63
03/02/2022 01:23:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=66
03/02/2022 01:23:57 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.30575516693163757 on epoch=66
03/02/2022 01:23:57 - INFO - __main__ - Saving model with best Classification-F1: 0.27079218784287906 -> 0.30575516693163757 on epoch=66, global_step=200
03/02/2022 01:23:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=69
03/02/2022 01:24:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=73
03/02/2022 01:24:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=76
03/02/2022 01:24:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=79
03/02/2022 01:24:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.52 on epoch=83
03/02/2022 01:24:09 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.1481481481481481 on epoch=83
03/02/2022 01:24:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=86
03/02/2022 01:24:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
03/02/2022 01:24:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
03/02/2022 01:24:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=96
03/02/2022 01:24:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=99
03/02/2022 01:24:21 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.31974637681159424 on epoch=99
03/02/2022 01:24:21 - INFO - __main__ - Saving model with best Classification-F1: 0.30575516693163757 -> 0.31974637681159424 on epoch=99, global_step=300
03/02/2022 01:24:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=103
03/02/2022 01:24:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=106
03/02/2022 01:24:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
03/02/2022 01:24:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=113
03/02/2022 01:24:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
03/02/2022 01:24:33 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.3127917833800187 on epoch=116
03/02/2022 01:24:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=119
03/02/2022 01:24:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=123
03/02/2022 01:24:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=126
03/02/2022 01:24:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=129
03/02/2022 01:24:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=133
03/02/2022 01:24:45 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=133
03/02/2022 01:24:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=136
03/02/2022 01:24:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=139
03/02/2022 01:24:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=143
03/02/2022 01:24:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=146
03/02/2022 01:24:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=149
03/02/2022 01:24:57 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.12391713747645952 on epoch=149
03/02/2022 01:24:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=153
03/02/2022 01:25:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=156
03/02/2022 01:25:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=159
03/02/2022 01:25:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=163
03/02/2022 01:25:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=166
03/02/2022 01:25:09 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.15666666666666665 on epoch=166
03/02/2022 01:25:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=169
03/02/2022 01:25:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=173
03/02/2022 01:25:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.35 on epoch=176
03/02/2022 01:25:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=179
03/02/2022 01:25:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.35 on epoch=183
03/02/2022 01:25:21 - INFO - __main__ - Global step 550 Train loss 0.33 Classification-F1 0.19071146245059292 on epoch=183
03/02/2022 01:25:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=186
03/02/2022 01:25:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=189
03/02/2022 01:25:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=193
03/02/2022 01:25:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.33 on epoch=196
03/02/2022 01:25:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=199
03/02/2022 01:25:33 - INFO - __main__ - Global step 600 Train loss 0.28 Classification-F1 0.24988648090815274 on epoch=199
03/02/2022 01:25:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=203
03/02/2022 01:25:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=206
03/02/2022 01:25:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=209
03/02/2022 01:25:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=213
03/02/2022 01:25:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=216
03/02/2022 01:25:45 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.17915510329941411 on epoch=216
03/02/2022 01:25:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=219
03/02/2022 01:25:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=223
03/02/2022 01:25:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=226
03/02/2022 01:25:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=229
03/02/2022 01:25:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=233
03/02/2022 01:25:57 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.17346420176608857 on epoch=233
03/02/2022 01:26:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=236
03/02/2022 01:26:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=239
03/02/2022 01:26:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=243
03/02/2022 01:26:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=246
03/02/2022 01:26:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=249
03/02/2022 01:26:09 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.4074584658890067 on epoch=249
03/02/2022 01:26:10 - INFO - __main__ - Saving model with best Classification-F1: 0.31974637681159424 -> 0.4074584658890067 on epoch=249, global_step=750
03/02/2022 01:26:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=253
03/02/2022 01:26:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=256
03/02/2022 01:26:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=259
03/02/2022 01:26:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=263
03/02/2022 01:26:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=266
03/02/2022 01:26:22 - INFO - __main__ - Global step 800 Train loss 0.13 Classification-F1 0.4064039408866995 on epoch=266
03/02/2022 01:26:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=269
03/02/2022 01:26:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=273
03/02/2022 01:26:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=276
03/02/2022 01:26:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=279
03/02/2022 01:26:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=283
03/02/2022 01:26:34 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.3984674329501916 on epoch=283
03/02/2022 01:26:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=286
03/02/2022 01:26:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=289
03/02/2022 01:26:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=293
03/02/2022 01:26:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=296
03/02/2022 01:26:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=299
03/02/2022 01:26:46 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.41870617110799435 on epoch=299
03/02/2022 01:26:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4074584658890067 -> 0.41870617110799435 on epoch=299, global_step=900
03/02/2022 01:26:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=303
03/02/2022 01:26:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
03/02/2022 01:26:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=309
03/02/2022 01:26:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=313
03/02/2022 01:26:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=316
03/02/2022 01:26:58 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.27575757575757576 on epoch=316
03/02/2022 01:27:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=319
03/02/2022 01:27:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=323
03/02/2022 01:27:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=326
03/02/2022 01:27:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=329
03/02/2022 01:27:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=333
03/02/2022 01:27:10 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.328 on epoch=333
03/02/2022 01:27:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=336
03/02/2022 01:27:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=339
03/02/2022 01:27:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
03/02/2022 01:27:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=346
03/02/2022 01:27:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=349
03/02/2022 01:27:22 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.2198116169544741 on epoch=349
03/02/2022 01:27:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=353
03/02/2022 01:27:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=356
03/02/2022 01:27:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=359
03/02/2022 01:27:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=363
03/02/2022 01:27:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=366
03/02/2022 01:27:35 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.2088936627282492 on epoch=366
03/02/2022 01:27:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
03/02/2022 01:27:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
03/02/2022 01:27:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=376
03/02/2022 01:27:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
03/02/2022 01:27:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
03/02/2022 01:27:47 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.12047138047138047 on epoch=383
03/02/2022 01:27:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=386
03/02/2022 01:27:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
03/02/2022 01:27:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=393
03/02/2022 01:27:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=396
03/02/2022 01:27:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=399
03/02/2022 01:27:59 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.3475648323845667 on epoch=399
03/02/2022 01:28:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=403
03/02/2022 01:28:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
03/02/2022 01:28:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=409
03/02/2022 01:28:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=413
03/02/2022 01:28:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
03/02/2022 01:28:11 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.2932539422888176 on epoch=416
03/02/2022 01:28:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
03/02/2022 01:28:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=423
03/02/2022 01:28:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=426
03/02/2022 01:28:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
03/02/2022 01:28:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=433
03/02/2022 01:28:24 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.3684944684944685 on epoch=433
03/02/2022 01:28:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
03/02/2022 01:28:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=439
03/02/2022 01:28:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=443
03/02/2022 01:28:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
03/02/2022 01:28:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
03/02/2022 01:28:37 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.39464285714285713 on epoch=449
03/02/2022 01:28:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=453
03/02/2022 01:28:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
03/02/2022 01:28:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=459
03/02/2022 01:28:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=463
03/02/2022 01:28:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=466
03/02/2022 01:28:49 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.4336996336996337 on epoch=466
03/02/2022 01:28:49 - INFO - __main__ - Saving model with best Classification-F1: 0.41870617110799435 -> 0.4336996336996337 on epoch=466, global_step=1400
03/02/2022 01:28:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
03/02/2022 01:28:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
03/02/2022 01:28:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
03/02/2022 01:28:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
03/02/2022 01:29:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
03/02/2022 01:29:02 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.5529983860487538 on epoch=483
03/02/2022 01:29:02 - INFO - __main__ - Saving model with best Classification-F1: 0.4336996336996337 -> 0.5529983860487538 on epoch=483, global_step=1450
03/02/2022 01:29:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
03/02/2022 01:29:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=489
03/02/2022 01:29:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
03/02/2022 01:29:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=496
03/02/2022 01:29:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
03/02/2022 01:29:14 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.314034034034034 on epoch=499
03/02/2022 01:29:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
03/02/2022 01:29:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
03/02/2022 01:29:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
03/02/2022 01:29:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=513
03/02/2022 01:29:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
03/02/2022 01:29:27 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.3204500066569032 on epoch=516
03/02/2022 01:29:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
03/02/2022 01:29:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
03/02/2022 01:29:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
03/02/2022 01:29:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
03/02/2022 01:29:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
03/02/2022 01:29:39 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.3987689393939394 on epoch=533
03/02/2022 01:29:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
03/02/2022 01:29:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
03/02/2022 01:29:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=543
03/02/2022 01:29:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
03/02/2022 01:29:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=549
03/02/2022 01:29:52 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.40177638453500525 on epoch=549
03/02/2022 01:29:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
03/02/2022 01:29:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
03/02/2022 01:29:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
03/02/2022 01:30:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
03/02/2022 01:30:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
03/02/2022 01:30:04 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5713524699325916 on epoch=566
03/02/2022 01:30:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5529983860487538 -> 0.5713524699325916 on epoch=566, global_step=1700
03/02/2022 01:30:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
03/02/2022 01:30:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
03/02/2022 01:30:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=576
03/02/2022 01:30:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
03/02/2022 01:30:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
03/02/2022 01:30:17 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5320402298850574 on epoch=583
03/02/2022 01:30:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
03/02/2022 01:30:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/02/2022 01:30:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
03/02/2022 01:30:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=596
03/02/2022 01:30:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
03/02/2022 01:30:29 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.42837697692507126 on epoch=599
03/02/2022 01:30:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 01:30:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 01:30:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
03/02/2022 01:30:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/02/2022 01:30:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
03/02/2022 01:30:42 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5530991821314402 on epoch=616
03/02/2022 01:30:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
03/02/2022 01:30:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
03/02/2022 01:30:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
03/02/2022 01:30:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 01:30:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
03/02/2022 01:30:54 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5119347634560617 on epoch=633
03/02/2022 01:30:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
03/02/2022 01:30:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
03/02/2022 01:31:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
03/02/2022 01:31:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 01:31:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
03/02/2022 01:31:07 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.3410830185023733 on epoch=649
03/02/2022 01:31:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
03/02/2022 01:31:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
03/02/2022 01:31:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
03/02/2022 01:31:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
03/02/2022 01:31:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 01:31:19 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5767195767195767 on epoch=666
03/02/2022 01:31:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5713524699325916 -> 0.5767195767195767 on epoch=666, global_step=2000
03/02/2022 01:31:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 01:31:24 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 01:31:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 01:31:28 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 01:31:31 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 01:31:32 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.3068548387096774 on epoch=683
03/02/2022 01:31:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 01:31:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=689
03/02/2022 01:31:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
03/02/2022 01:31:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/02/2022 01:31:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
03/02/2022 01:31:44 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.5119347634560617 on epoch=699
03/02/2022 01:31:47 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 01:31:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 01:31:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=709
03/02/2022 01:31:53 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 01:31:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 01:31:57 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.37582846003898635 on epoch=716
03/02/2022 01:31:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=719
03/02/2022 01:32:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 01:32:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 01:32:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 01:32:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 01:32:09 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.4138836179633144 on epoch=733
03/02/2022 01:32:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
03/02/2022 01:32:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 01:32:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 01:32:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
03/02/2022 01:32:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 01:32:22 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.5035311173242208 on epoch=749
03/02/2022 01:32:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 01:32:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=756
03/02/2022 01:32:28 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
03/02/2022 01:32:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 01:32:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 01:32:34 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.4932536222858804 on epoch=766
03/02/2022 01:32:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 01:32:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 01:32:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 01:32:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 01:32:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 01:32:47 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.3663469017371014 on epoch=783
03/02/2022 01:32:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 01:32:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 01:32:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 01:32:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
03/02/2022 01:32:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 01:32:59 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.5474747474747476 on epoch=799
03/02/2022 01:33:01 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 01:33:04 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
03/02/2022 01:33:06 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 01:33:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 01:33:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 01:33:12 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.5348446147296722 on epoch=816
03/02/2022 01:33:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 01:33:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 01:33:18 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
03/02/2022 01:33:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 01:33:23 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 01:33:24 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.547140522875817 on epoch=833
03/02/2022 01:33:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
03/02/2022 01:33:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
03/02/2022 01:33:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 01:33:33 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
03/02/2022 01:33:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 01:33:36 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.5759018759018759 on epoch=849
03/02/2022 01:33:39 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
03/02/2022 01:33:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
03/02/2022 01:33:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 01:33:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 01:33:48 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 01:33:49 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.5713524699325916 on epoch=866
03/02/2022 01:33:51 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 01:33:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 01:33:56 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 01:33:58 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 01:34:00 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 01:34:01 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.47971781305114636 on epoch=883
03/02/2022 01:34:04 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 01:34:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 01:34:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 01:34:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 01:34:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 01:34:14 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.4744582043343653 on epoch=899
03/02/2022 01:34:16 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 01:34:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 01:34:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 01:34:22 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 01:34:24 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 01:34:26 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.3896408469579202 on epoch=916
03/02/2022 01:34:28 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 01:34:30 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 01:34:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
03/02/2022 01:34:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 01:34:37 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 01:34:38 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.49757727652464495 on epoch=933
03/02/2022 01:34:40 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 01:34:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 01:34:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 01:34:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=946
03/02/2022 01:34:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 01:34:50 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.48517846339987897 on epoch=949
03/02/2022 01:34:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 01:34:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
03/02/2022 01:34:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 01:34:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 01:35:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 01:35:02 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.41981721981721976 on epoch=966
03/02/2022 01:35:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 01:35:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 01:35:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 01:35:11 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 01:35:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 01:35:14 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.2832983193277311 on epoch=983
03/02/2022 01:35:16 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 01:35:19 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=989
03/02/2022 01:35:21 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 01:35:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 01:35:25 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 01:35:26 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:35:26 - INFO - __main__ - Printing 3 examples
03/02/2022 01:35:26 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:35:26 - INFO - __main__ - ['hatespeech']
03/02/2022 01:35:26 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:35:26 - INFO - __main__ - ['hatespeech']
03/02/2022 01:35:26 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:35:26 - INFO - __main__ - ['hatespeech']
03/02/2022 01:35:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 01:35:26 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:35:26 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.4827712609970674 on epoch=999
03/02/2022 01:35:26 - INFO - __main__ - save last model!
03/02/2022 01:35:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 01:35:26 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:35:26 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:35:26 - INFO - __main__ - Printing 3 examples
03/02/2022 01:35:26 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:35:26 - INFO - __main__ - ['hatespeech']
03/02/2022 01:35:26 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:35:26 - INFO - __main__ - ['hatespeech']
03/02/2022 01:35:26 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:35:26 - INFO - __main__ - ['hatespeech']
03/02/2022 01:35:26 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:35:26 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 01:35:26 - INFO - __main__ - Printing 3 examples
03/02/2022 01:35:26 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 01:35:26 - INFO - __main__ - ['normal']
03/02/2022 01:35:26 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 01:35:26 - INFO - __main__ - ['normal']
03/02/2022 01:35:26 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 01:35:26 - INFO - __main__ - ['normal']
03/02/2022 01:35:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:35:26 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:35:26 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:35:27 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:35:29 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 01:35:41 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 01:35:41 - INFO - __main__ - task name: hatexplain
initialize from c4
[(2502, 393505), (3485, 303196), (2451, 408517), (2124, 458893), (727, 605486), (159, 3283944), (550, 1652637), (2943, 377167), (4577, 197081), (536, 1096599), (4800, 205648), (3374, 227031), (4959, 162645), (2020, 470862), (3016, 244028), (6059, 189253), (3944, 234304), (2651, 378692), (1638, 604699), (62, 15842384), (1130, 821238), (4783, 219874), (1890, 417949), (1929, 517016), (5885, 180884), (5519, 201464), (568, 1556089), (6068, 167828), (2545, 370902), (1310, 731782), (4964, 205248), (1443, 666960), (4999, 201462), (5908, 176655), (4066, 167278), (6883, 157698), (3596, 297061), (3747, 211292), (5364, 185276), (4984, 218955), (4170, 170840), (5020, 210030), (1776, 567810), (5258, 197449), (4949, 208735), (2848, 356857), (2595, 374340), (2349, 335938), (5783, 180158), (5245, 192501), (2287, 388098), (2224, 338862), (3959, 212293), (852, 1044002), (2923, 327246), (1458, 394015), (1963, 341745), (3521, 280332), (2606, 294174), (7609, 173097), (830, 1012157), (968, 645839), (1487, 592557), (2198, 398890), (4674, 244421), (1627, 587044), (2068, 427366), (724, 1234015), (4657, 226628), (4614, 230206), (3467, 284152), (3304, 172257), (3291, 200786), (4441, 226147), (651, 1269510), (2214, 442372), (3497, 223253), (4716, 239808), (6497, 160316), (227, 4080344), (7555, 162623), (3924, 246414), (4666, 195723), (66, 13829035), (3645, 265964), (3236, 314080), (3229, 239354), (4804, 223950), (6162, 173710), (6040, 171279), (6152, 155746), (2428, 415536), (5503, 191100), (2374, 417354), (2949, 340763), (4836, 214321), (3385, 191872), (313, 2745242), (2206, 240868)]
03/02/2022 01:35:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:35:42 - INFO - __main__ - Starting training!
03/02/2022 01:36:18 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_13_0.5_8_predictions.txt
03/02/2022 01:36:18 - INFO - __main__ - Classification-F1 on test data: 0.2100
03/02/2022 01:36:19 - INFO - __main__ - prefix=hatexplain_16_13, lr=0.5, bsz=8, dev_performance=0.5767195767195767, test_performance=0.2100192701714713
03/02/2022 01:36:19 - INFO - __main__ - Running ... prefix=hatexplain_16_13, lr=0.4, bsz=8 ...
03/02/2022 01:36:19 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:36:19 - INFO - __main__ - Printing 3 examples
03/02/2022 01:36:19 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:36:19 - INFO - __main__ - ['hatespeech']
03/02/2022 01:36:19 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:36:19 - INFO - __main__ - ['hatespeech']
03/02/2022 01:36:19 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:36:19 - INFO - __main__ - ['hatespeech']
03/02/2022 01:36:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:36:19 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:36:20 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:36:20 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:36:20 - INFO - __main__ - Printing 3 examples
03/02/2022 01:36:20 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:36:20 - INFO - __main__ - ['hatespeech']
03/02/2022 01:36:20 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:36:20 - INFO - __main__ - ['hatespeech']
03/02/2022 01:36:20 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:36:20 - INFO - __main__ - ['hatespeech']
03/02/2022 01:36:20 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:36:20 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:36:20 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:36:33 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 01:36:33 - INFO - __main__ - task name: hatexplain
initialize from c4
[(2502, 393505), (3485, 303196), (2451, 408517), (2124, 458893), (727, 605486), (159, 3283944), (550, 1652637), (2943, 377167), (4577, 197081), (536, 1096599), (4800, 205648), (3374, 227031), (4959, 162645), (2020, 470862), (3016, 244028), (6059, 189253), (3944, 234304), (2651, 378692), (1638, 604699), (62, 15842384), (1130, 821238), (4783, 219874), (1890, 417949), (1929, 517016), (5885, 180884), (5519, 201464), (568, 1556089), (6068, 167828), (2545, 370902), (1310, 731782), (4964, 205248), (1443, 666960), (4999, 201462), (5908, 176655), (4066, 167278), (6883, 157698), (3596, 297061), (3747, 211292), (5364, 185276), (4984, 218955), (4170, 170840), (5020, 210030), (1776, 567810), (5258, 197449), (4949, 208735), (2848, 356857), (2595, 374340), (2349, 335938), (5783, 180158), (5245, 192501), (2287, 388098), (2224, 338862), (3959, 212293), (852, 1044002), (2923, 327246), (1458, 394015), (1963, 341745), (3521, 280332), (2606, 294174), (7609, 173097), (830, 1012157), (968, 645839), (1487, 592557), (2198, 398890), (4674, 244421), (1627, 587044), (2068, 427366), (724, 1234015), (4657, 226628), (4614, 230206), (3467, 284152), (3304, 172257), (3291, 200786), (4441, 226147), (651, 1269510), (2214, 442372), (3497, 223253), (4716, 239808), (6497, 160316), (227, 4080344), (7555, 162623), (3924, 246414), (4666, 195723), (66, 13829035), (3645, 265964), (3236, 314080), (3229, 239354), (4804, 223950), (6162, 173710), (6040, 171279), (6152, 155746), (2428, 415536), (5503, 191100), (2374, 417354), (2949, 340763), (4836, 214321), (3385, 191872), (313, 2745242), (2206, 240868)]
03/02/2022 01:36:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:36:34 - INFO - __main__ - Starting training!
03/02/2022 01:36:37 - INFO - __main__ - Step 10 Global step 10 Train loss 6.16 on epoch=3
03/02/2022 01:36:39 - INFO - __main__ - Step 20 Global step 20 Train loss 2.29 on epoch=6
03/02/2022 01:36:41 - INFO - __main__ - Step 30 Global step 30 Train loss 1.03 on epoch=9
03/02/2022 01:36:43 - INFO - __main__ - Step 40 Global step 40 Train loss 0.77 on epoch=13
03/02/2022 01:36:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.80 on epoch=16
03/02/2022 01:36:46 - INFO - __main__ - Global step 50 Train loss 2.21 Classification-F1 0.18809318377911996 on epoch=16
03/02/2022 01:36:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18809318377911996 on epoch=16, global_step=50
03/02/2022 01:36:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.65 on epoch=19
03/02/2022 01:36:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=23
03/02/2022 01:36:53 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=26
03/02/2022 01:36:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=29
03/02/2022 01:36:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=33
03/02/2022 01:36:59 - INFO - __main__ - Global step 100 Train loss 0.59 Classification-F1 0.24970882832518052 on epoch=33
03/02/2022 01:36:59 - INFO - __main__ - Saving model with best Classification-F1: 0.18809318377911996 -> 0.24970882832518052 on epoch=33, global_step=100
03/02/2022 01:37:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=36
03/02/2022 01:37:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=39
03/02/2022 01:37:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=43
03/02/2022 01:37:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=46
03/02/2022 01:37:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=49
03/02/2022 01:37:11 - INFO - __main__ - Global step 150 Train loss 0.55 Classification-F1 0.17204301075268816 on epoch=49
03/02/2022 01:37:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=53
03/02/2022 01:37:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=56
03/02/2022 01:37:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
03/02/2022 01:37:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.55 on epoch=63
03/02/2022 01:37:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=66
03/02/2022 01:37:23 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.21001779811848462 on epoch=66
03/02/2022 01:37:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.55 on epoch=69
03/02/2022 01:37:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=73
03/02/2022 01:37:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=76
03/02/2022 01:37:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=79
03/02/2022 01:37:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=83
03/02/2022 01:37:35 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.2480818414322251 on epoch=83
03/02/2022 01:37:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=86
03/02/2022 01:37:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=89
03/02/2022 01:37:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=93
03/02/2022 01:37:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=96
03/02/2022 01:37:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=99
03/02/2022 01:37:47 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.10491803278688525 on epoch=99
03/02/2022 01:37:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=103
03/02/2022 01:37:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=106
03/02/2022 01:37:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=109
03/02/2022 01:37:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.54 on epoch=113
03/02/2022 01:37:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=116
03/02/2022 01:37:59 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.19004250151791138 on epoch=116
03/02/2022 01:38:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=119
03/02/2022 01:38:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=123
03/02/2022 01:38:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=126
03/02/2022 01:38:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=129
03/02/2022 01:38:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=133
03/02/2022 01:38:11 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.12698412698412698 on epoch=133
03/02/2022 01:38:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=136
03/02/2022 01:38:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=139
03/02/2022 01:38:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=143
03/02/2022 01:38:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=146
03/02/2022 01:38:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=149
03/02/2022 01:38:23 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.19111111111111112 on epoch=149
03/02/2022 01:38:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=153
03/02/2022 01:38:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=156
03/02/2022 01:38:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=159
03/02/2022 01:38:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=163
03/02/2022 01:38:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=166
03/02/2022 01:38:35 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.16092592592592594 on epoch=166
03/02/2022 01:38:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.33 on epoch=169
03/02/2022 01:38:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=173
03/02/2022 01:38:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=176
03/02/2022 01:38:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.31 on epoch=179
03/02/2022 01:38:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=183
03/02/2022 01:38:48 - INFO - __main__ - Global step 550 Train loss 0.33 Classification-F1 0.1517094017094017 on epoch=183
03/02/2022 01:38:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=186
03/02/2022 01:38:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=189
03/02/2022 01:38:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.34 on epoch=193
03/02/2022 01:38:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.29 on epoch=196
03/02/2022 01:38:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=199
03/02/2022 01:39:00 - INFO - __main__ - Global step 600 Train loss 0.30 Classification-F1 0.2919480519480519 on epoch=199
03/02/2022 01:39:00 - INFO - __main__ - Saving model with best Classification-F1: 0.24970882832518052 -> 0.2919480519480519 on epoch=199, global_step=600
03/02/2022 01:39:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=203
03/02/2022 01:39:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=206
03/02/2022 01:39:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=209
03/02/2022 01:39:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=213
03/02/2022 01:39:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=216
03/02/2022 01:39:13 - INFO - __main__ - Global step 650 Train loss 0.29 Classification-F1 0.27412008281573497 on epoch=216
03/02/2022 01:39:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=219
03/02/2022 01:39:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=223
03/02/2022 01:39:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=226
03/02/2022 01:39:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=229
03/02/2022 01:39:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=233
03/02/2022 01:39:25 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.21802325581395346 on epoch=233
03/02/2022 01:39:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=236
03/02/2022 01:39:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=239
03/02/2022 01:39:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=243
03/02/2022 01:39:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=246
03/02/2022 01:39:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=249
03/02/2022 01:39:37 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.2857142857142857 on epoch=249
03/02/2022 01:39:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=253
03/02/2022 01:39:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=256
03/02/2022 01:39:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=259
03/02/2022 01:39:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=263
03/02/2022 01:39:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=266
03/02/2022 01:39:50 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.32833763996554693 on epoch=266
03/02/2022 01:39:50 - INFO - __main__ - Saving model with best Classification-F1: 0.2919480519480519 -> 0.32833763996554693 on epoch=266, global_step=800
03/02/2022 01:39:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.13 on epoch=269
03/02/2022 01:39:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=273
03/02/2022 01:39:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=276
03/02/2022 01:39:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.13 on epoch=279
03/02/2022 01:40:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=283
03/02/2022 01:40:02 - INFO - __main__ - Global step 850 Train loss 0.14 Classification-F1 0.6328671328671328 on epoch=283
03/02/2022 01:40:02 - INFO - __main__ - Saving model with best Classification-F1: 0.32833763996554693 -> 0.6328671328671328 on epoch=283, global_step=850
03/02/2022 01:40:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=286
03/02/2022 01:40:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=289
03/02/2022 01:40:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=293
03/02/2022 01:40:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=296
03/02/2022 01:40:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=299
03/02/2022 01:40:14 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.27997807532691255 on epoch=299
03/02/2022 01:40:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=303
03/02/2022 01:40:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=306
03/02/2022 01:40:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=309
03/02/2022 01:40:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=313
03/02/2022 01:40:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=316
03/02/2022 01:40:26 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.3265421618562535 on epoch=316
03/02/2022 01:40:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=319
03/02/2022 01:40:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=323
03/02/2022 01:40:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=326
03/02/2022 01:40:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=329
03/02/2022 01:40:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=333
03/02/2022 01:40:38 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.2235903631252468 on epoch=333
03/02/2022 01:40:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=336
03/02/2022 01:40:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=339
03/02/2022 01:40:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=343
03/02/2022 01:40:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
03/02/2022 01:40:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=349
03/02/2022 01:40:49 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.20818713450292395 on epoch=349
03/02/2022 01:40:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=353
03/02/2022 01:40:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=356
03/02/2022 01:40:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=359
03/02/2022 01:40:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=363
03/02/2022 01:41:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=366
03/02/2022 01:41:01 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.33909090909090905 on epoch=366
03/02/2022 01:41:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=369
03/02/2022 01:41:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=373
03/02/2022 01:41:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=376
03/02/2022 01:41:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
03/02/2022 01:41:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=383
03/02/2022 01:41:13 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.34850420168067225 on epoch=383
03/02/2022 01:41:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=386
03/02/2022 01:41:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
03/02/2022 01:41:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=393
03/02/2022 01:41:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=396
03/02/2022 01:41:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=399
03/02/2022 01:41:25 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.24699188867104904 on epoch=399
03/02/2022 01:41:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
03/02/2022 01:41:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=406
03/02/2022 01:41:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=409
03/02/2022 01:41:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=413
03/02/2022 01:41:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=416
03/02/2022 01:41:37 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.24494392479873425 on epoch=416
03/02/2022 01:41:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
03/02/2022 01:41:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=423
03/02/2022 01:41:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
03/02/2022 01:41:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=429
03/02/2022 01:41:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=433
03/02/2022 01:41:49 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.2222222222222222 on epoch=433
03/02/2022 01:41:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
03/02/2022 01:41:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=439
03/02/2022 01:41:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
03/02/2022 01:41:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
03/02/2022 01:42:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=449
03/02/2022 01:42:01 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.2303637948799239 on epoch=449
03/02/2022 01:42:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
03/02/2022 01:42:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=456
03/02/2022 01:42:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
03/02/2022 01:42:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=463
03/02/2022 01:42:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
03/02/2022 01:42:13 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.3419895045531718 on epoch=466
03/02/2022 01:42:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=469
03/02/2022 01:42:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
03/02/2022 01:42:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=476
03/02/2022 01:42:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
03/02/2022 01:42:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=483
03/02/2022 01:42:25 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.374251012145749 on epoch=483
03/02/2022 01:42:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=486
03/02/2022 01:42:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
03/02/2022 01:42:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
03/02/2022 01:42:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
03/02/2022 01:42:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
03/02/2022 01:42:37 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.34843205574912894 on epoch=499
03/02/2022 01:42:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
03/02/2022 01:42:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
03/02/2022 01:42:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
03/02/2022 01:42:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
03/02/2022 01:42:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
03/02/2022 01:42:49 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.4019620582120582 on epoch=516
03/02/2022 01:42:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
03/02/2022 01:42:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
03/02/2022 01:42:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
03/02/2022 01:42:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=529
03/02/2022 01:43:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
03/02/2022 01:43:02 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.3982456140350877 on epoch=533
03/02/2022 01:43:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
03/02/2022 01:43:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
03/02/2022 01:43:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=543
03/02/2022 01:43:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
03/02/2022 01:43:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
03/02/2022 01:43:13 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.3808333333333333 on epoch=549
03/02/2022 01:43:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
03/02/2022 01:43:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=556
03/02/2022 01:43:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
03/02/2022 01:43:22 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
03/02/2022 01:43:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=566
03/02/2022 01:43:26 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.2945054945054945 on epoch=566
03/02/2022 01:43:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
03/02/2022 01:43:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
03/02/2022 01:43:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
03/02/2022 01:43:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
03/02/2022 01:43:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
03/02/2022 01:43:38 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.2936849816849817 on epoch=583
03/02/2022 01:43:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
03/02/2022 01:43:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
03/02/2022 01:43:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=593
03/02/2022 01:43:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
03/02/2022 01:43:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
03/02/2022 01:43:50 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.47293706293706295 on epoch=599
03/02/2022 01:43:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=603
03/02/2022 01:43:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 01:43:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
03/02/2022 01:43:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
03/02/2022 01:44:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
03/02/2022 01:44:02 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.49074933687002653 on epoch=616
03/02/2022 01:44:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
03/02/2022 01:44:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
03/02/2022 01:44:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
03/02/2022 01:44:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
03/02/2022 01:44:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
03/02/2022 01:44:14 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.4147050276082534 on epoch=633
03/02/2022 01:44:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
03/02/2022 01:44:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=639
03/02/2022 01:44:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 01:44:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 01:44:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=649
03/02/2022 01:44:26 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.35320512820512817 on epoch=649
03/02/2022 01:44:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
03/02/2022 01:44:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
03/02/2022 01:44:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
03/02/2022 01:44:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
03/02/2022 01:44:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
03/02/2022 01:44:38 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.4456233421750663 on epoch=666
03/02/2022 01:44:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
03/02/2022 01:44:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 01:44:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 01:44:47 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 01:44:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
03/02/2022 01:44:50 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.2437803430790808 on epoch=683
03/02/2022 01:44:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 01:44:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/02/2022 01:44:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 01:44:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
03/02/2022 01:45:01 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
03/02/2022 01:45:03 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.36588451805843103 on epoch=699
03/02/2022 01:45:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 01:45:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
03/02/2022 01:45:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
03/02/2022 01:45:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 01:45:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
03/02/2022 01:45:15 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.3216880341880342 on epoch=716
03/02/2022 01:45:17 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
03/02/2022 01:45:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=723
03/02/2022 01:45:21 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 01:45:23 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 01:45:26 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 01:45:27 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.36813186813186816 on epoch=733
03/02/2022 01:45:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 01:45:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 01:45:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 01:45:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
03/02/2022 01:45:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 01:45:39 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.3687584345479082 on epoch=749
03/02/2022 01:45:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
03/02/2022 01:45:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
03/02/2022 01:45:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 01:45:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 01:45:50 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 01:45:51 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.3216880341880342 on epoch=766
03/02/2022 01:45:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 01:45:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=773
03/02/2022 01:45:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 01:46:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
03/02/2022 01:46:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=783
03/02/2022 01:46:03 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.1972658920027341 on epoch=783
03/02/2022 01:46:06 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
03/02/2022 01:46:08 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 01:46:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 01:46:12 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
03/02/2022 01:46:14 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 01:46:16 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.3521645021645022 on epoch=799
03/02/2022 01:46:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 01:46:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 01:46:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 01:46:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 01:46:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=816
03/02/2022 01:46:28 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.3380808080808081 on epoch=816
03/02/2022 01:46:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
03/02/2022 01:46:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
03/02/2022 01:46:34 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 01:46:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 01:46:39 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 01:46:40 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.2692307692307692 on epoch=833
03/02/2022 01:46:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=836
03/02/2022 01:46:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
03/02/2022 01:46:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 01:46:49 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 01:46:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 01:46:52 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.2814285714285714 on epoch=849
03/02/2022 01:46:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/02/2022 01:46:56 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 01:46:59 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 01:47:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 01:47:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 01:47:04 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.2909655172413793 on epoch=866
03/02/2022 01:47:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 01:47:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
03/02/2022 01:47:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 01:47:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 01:47:15 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 01:47:17 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.43214285714285716 on epoch=883
03/02/2022 01:47:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 01:47:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
03/02/2022 01:47:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=893
03/02/2022 01:47:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 01:47:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=899
03/02/2022 01:47:29 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.23880837359098225 on epoch=899
03/02/2022 01:47:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=903
03/02/2022 01:47:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 01:47:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=909
03/02/2022 01:47:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 01:47:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=916
03/02/2022 01:47:41 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.2083333333333333 on epoch=916
03/02/2022 01:47:43 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 01:47:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 01:47:47 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 01:47:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 01:47:52 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 01:47:53 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.202082941213376 on epoch=933
03/02/2022 01:47:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 01:47:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 01:48:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 01:48:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 01:48:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
03/02/2022 01:48:05 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.24817656531211518 on epoch=949
03/02/2022 01:48:07 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 01:48:10 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 01:48:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 01:48:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 01:48:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 01:48:17 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.2806304164990675 on epoch=966
03/02/2022 01:48:20 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 01:48:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
03/02/2022 01:48:24 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 01:48:26 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 01:48:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 01:48:30 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.27883597883597877 on epoch=983
03/02/2022 01:48:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 01:48:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 01:48:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 01:48:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
03/02/2022 01:48:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 01:48:42 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.20163170163170163 on epoch=999
03/02/2022 01:48:42 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:48:42 - INFO - __main__ - Printing 3 examples
03/02/2022 01:48:42 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:48:42 - INFO - __main__ - ['hatespeech']
03/02/2022 01:48:42 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:48:42 - INFO - __main__ - ['hatespeech']
03/02/2022 01:48:42 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:48:42 - INFO - __main__ - ['hatespeech']
03/02/2022 01:48:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 01:48:42 - INFO - __main__ - save last model!
03/02/2022 01:48:42 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:48:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 01:48:42 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 01:48:42 - INFO - __main__ - Printing 3 examples
03/02/2022 01:48:42 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 01:48:42 - INFO - __main__ - ['normal']
03/02/2022 01:48:42 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 01:48:42 - INFO - __main__ - ['normal']
03/02/2022 01:48:42 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 01:48:42 - INFO - __main__ - ['normal']
03/02/2022 01:48:42 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:48:42 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:48:42 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:48:42 - INFO - __main__ - Printing 3 examples
03/02/2022 01:48:42 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:48:42 - INFO - __main__ - ['hatespeech']
03/02/2022 01:48:42 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:48:42 - INFO - __main__ - ['hatespeech']
03/02/2022 01:48:42 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:48:42 - INFO - __main__ - ['hatespeech']
03/02/2022 01:48:42 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:48:42 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:48:42 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:48:43 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:48:44 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 01:48:56 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 01:48:56 - INFO - __main__ - task name: hatexplain
initialize from c4
[(3666, 279856), (4279, 231910), (1376, 549699), (5099, 200518), (1480, 693138), (5956, 175083), (317, 2909017), (3859, 237931), (6537, 158593), (6422, 165641), (328, 2897449), (915, 980631), (6366, 217988), (1303, 658549), (5169, 202438), (1884, 502256), (568, 1556089), (1489, 355866), (4036, 247461), (2445, 287205), (2154, 432132), (5074, 206086), (3513, 287362), (2658, 279047), (2412, 334412), (2660, 222403), (2656, 367203), (882, 1004086), (4935, 199405), (115, 4101345), (5058, 173227), (593, 1511490), (3830, 267200), (2935, 409303), (726, 1159344), (464, 1931312), (262, 2562554), (2595, 374340), (2114, 458811), (231, 4022141), (1689, 588233), (2627, 389531), (1048, 702299), (5656, 197667), (1341, 767639), (4837, 165767), (2418, 420267), (5111, 201364), (2433, 361242), (3976, 254648), (1801, 537800), (3849, 155187), (6553, 154915), (1214, 767509), (1682, 553032), (3355, 302330), (1056, 811824), (5087, 161786), (196, 3104995), (2464, 301408), (7555, 162623), (3004, 249813), (4349, 236587), (1978, 460084), (448, 1064987), (5917, 158493), (2612, 380946), (1078, 844362), (2715, 311751), (5798, 156482), (1137, 734426), (6326, 165064), (456, 1871965), (3146, 269081), (1703, 175726), (4773, 218071), (3699, 253359), (746, 1156068), (5696, 184838), (3669, 275693), (165, 6245475), (3497, 223253), (4240, 190278), (1195, 825391), (400, 215271), (2938, 259506), (3095, 203292), (1433, 586613), (4400, 214423), (1995, 516280), (5706, 180057), (2953, 344008), (3396, 173723), (3557, 193090), (5002, 201395), (4365, 214420), (4936, 158408), (2855, 345487), (3008, 291537)]
03/02/2022 01:48:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:48:57 - INFO - __main__ - Starting training!
03/02/2022 01:49:32 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_13_0.4_8_predictions.txt
03/02/2022 01:49:32 - INFO - __main__ - Classification-F1 on test data: 0.1865
03/02/2022 01:49:33 - INFO - __main__ - prefix=hatexplain_16_13, lr=0.4, bsz=8, dev_performance=0.6328671328671328, test_performance=0.18653052867657594
03/02/2022 01:49:33 - INFO - __main__ - Running ... prefix=hatexplain_16_13, lr=0.3, bsz=8 ...
03/02/2022 01:49:34 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:49:34 - INFO - __main__ - Printing 3 examples
03/02/2022 01:49:34 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:49:34 - INFO - __main__ - ['hatespeech']
03/02/2022 01:49:34 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:49:34 - INFO - __main__ - ['hatespeech']
03/02/2022 01:49:34 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:49:34 - INFO - __main__ - ['hatespeech']
03/02/2022 01:49:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:49:34 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:49:34 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:49:34 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:49:34 - INFO - __main__ - Printing 3 examples
03/02/2022 01:49:34 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:49:34 - INFO - __main__ - ['hatespeech']
03/02/2022 01:49:34 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:49:34 - INFO - __main__ - ['hatespeech']
03/02/2022 01:49:34 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:49:34 - INFO - __main__ - ['hatespeech']
03/02/2022 01:49:34 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:49:34 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:49:34 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:49:48 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 01:49:48 - INFO - __main__ - task name: hatexplain
initialize from c4
[(3666, 279856), (4279, 231910), (1376, 549699), (5099, 200518), (1480, 693138), (5956, 175083), (317, 2909017), (3859, 237931), (6537, 158593), (6422, 165641), (328, 2897449), (915, 980631), (6366, 217988), (1303, 658549), (5169, 202438), (1884, 502256), (568, 1556089), (1489, 355866), (4036, 247461), (2445, 287205), (2154, 432132), (5074, 206086), (3513, 287362), (2658, 279047), (2412, 334412), (2660, 222403), (2656, 367203), (882, 1004086), (4935, 199405), (115, 4101345), (5058, 173227), (593, 1511490), (3830, 267200), (2935, 409303), (726, 1159344), (464, 1931312), (262, 2562554), (2595, 374340), (2114, 458811), (231, 4022141), (1689, 588233), (2627, 389531), (1048, 702299), (5656, 197667), (1341, 767639), (4837, 165767), (2418, 420267), (5111, 201364), (2433, 361242), (3976, 254648), (1801, 537800), (3849, 155187), (6553, 154915), (1214, 767509), (1682, 553032), (3355, 302330), (1056, 811824), (5087, 161786), (196, 3104995), (2464, 301408), (7555, 162623), (3004, 249813), (4349, 236587), (1978, 460084), (448, 1064987), (5917, 158493), (2612, 380946), (1078, 844362), (2715, 311751), (5798, 156482), (1137, 734426), (6326, 165064), (456, 1871965), (3146, 269081), (1703, 175726), (4773, 218071), (3699, 253359), (746, 1156068), (5696, 184838), (3669, 275693), (165, 6245475), (3497, 223253), (4240, 190278), (1195, 825391), (400, 215271), (2938, 259506), (3095, 203292), (1433, 586613), (4400, 214423), (1995, 516280), (5706, 180057), (2953, 344008), (3396, 173723), (3557, 193090), (5002, 201395), (4365, 214420), (4936, 158408), (2855, 345487), (3008, 291537)]
03/02/2022 01:49:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:49:48 - INFO - __main__ - Starting training!
03/02/2022 01:49:53 - INFO - __main__ - Step 10 Global step 10 Train loss 6.49 on epoch=3
03/02/2022 01:49:56 - INFO - __main__ - Step 20 Global step 20 Train loss 3.38 on epoch=6
03/02/2022 01:49:58 - INFO - __main__ - Step 30 Global step 30 Train loss 1.52 on epoch=9
03/02/2022 01:50:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.98 on epoch=13
03/02/2022 01:50:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.81 on epoch=16
03/02/2022 01:50:03 - INFO - __main__ - Global step 50 Train loss 2.64 Classification-F1 0.15964912280701754 on epoch=16
03/02/2022 01:50:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15964912280701754 on epoch=16, global_step=50
03/02/2022 01:50:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.82 on epoch=19
03/02/2022 01:50:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.65 on epoch=23
03/02/2022 01:50:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.69 on epoch=26
03/02/2022 01:50:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.65 on epoch=29
03/02/2022 01:50:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=33
03/02/2022 01:50:16 - INFO - __main__ - Global step 100 Train loss 0.67 Classification-F1 0.22222222222222224 on epoch=33
03/02/2022 01:50:16 - INFO - __main__ - Saving model with best Classification-F1: 0.15964912280701754 -> 0.22222222222222224 on epoch=33, global_step=100
03/02/2022 01:50:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=36
03/02/2022 01:50:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=39
03/02/2022 01:50:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=43
03/02/2022 01:50:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=46
03/02/2022 01:50:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=49
03/02/2022 01:50:28 - INFO - __main__ - Global step 150 Train loss 0.57 Classification-F1 0.2085278555866791 on epoch=49
03/02/2022 01:50:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=53
03/02/2022 01:50:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=56
03/02/2022 01:50:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=59
03/02/2022 01:50:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.55 on epoch=63
03/02/2022 01:50:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=66
03/02/2022 01:50:40 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.20037986704653368 on epoch=66
03/02/2022 01:50:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=69
03/02/2022 01:50:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=73
03/02/2022 01:50:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=76
03/02/2022 01:50:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
03/02/2022 01:50:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=83
03/02/2022 01:50:52 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.23031684673076283 on epoch=83
03/02/2022 01:50:52 - INFO - __main__ - Saving model with best Classification-F1: 0.22222222222222224 -> 0.23031684673076283 on epoch=83, global_step=250
03/02/2022 01:50:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=86
03/02/2022 01:50:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=89
03/02/2022 01:50:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=93
03/02/2022 01:51:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=96
03/02/2022 01:51:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=99
03/02/2022 01:51:05 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.2362637362637363 on epoch=99
03/02/2022 01:51:05 - INFO - __main__ - Saving model with best Classification-F1: 0.23031684673076283 -> 0.2362637362637363 on epoch=99, global_step=300
03/02/2022 01:51:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=103
03/02/2022 01:51:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=106
03/02/2022 01:51:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=109
03/02/2022 01:51:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=113
03/02/2022 01:51:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=116
03/02/2022 01:51:17 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.2638888888888889 on epoch=116
03/02/2022 01:51:17 - INFO - __main__ - Saving model with best Classification-F1: 0.2362637362637363 -> 0.2638888888888889 on epoch=116, global_step=350
03/02/2022 01:51:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=119
03/02/2022 01:51:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=123
03/02/2022 01:51:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=126
03/02/2022 01:51:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=129
03/02/2022 01:51:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=133
03/02/2022 01:51:30 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.19999999999999998 on epoch=133
03/02/2022 01:51:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=136
03/02/2022 01:51:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=139
03/02/2022 01:51:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=143
03/02/2022 01:51:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=146
03/02/2022 01:51:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=149
03/02/2022 01:51:42 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.1711111111111111 on epoch=149
03/02/2022 01:51:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=153
03/02/2022 01:51:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=156
03/02/2022 01:51:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=159
03/02/2022 01:51:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=163
03/02/2022 01:51:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=166
03/02/2022 01:51:55 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.12091086301612618 on epoch=166
03/02/2022 01:51:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
03/02/2022 01:51:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=173
03/02/2022 01:52:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=176
03/02/2022 01:52:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.31 on epoch=179
03/02/2022 01:52:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=183
03/02/2022 01:52:07 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.16497607655502391 on epoch=183
03/02/2022 01:52:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.28 on epoch=186
03/02/2022 01:52:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=189
03/02/2022 01:52:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.31 on epoch=193
03/02/2022 01:52:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=196
03/02/2022 01:52:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=199
03/02/2022 01:52:20 - INFO - __main__ - Global step 600 Train loss 0.29 Classification-F1 0.19564061669324825 on epoch=199
03/02/2022 01:52:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=203
03/02/2022 01:52:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=206
03/02/2022 01:52:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=209
03/02/2022 01:52:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=213
03/02/2022 01:52:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=216
03/02/2022 01:52:32 - INFO - __main__ - Global step 650 Train loss 0.26 Classification-F1 0.47493734335839594 on epoch=216
03/02/2022 01:52:32 - INFO - __main__ - Saving model with best Classification-F1: 0.2638888888888889 -> 0.47493734335839594 on epoch=216, global_step=650
03/02/2022 01:52:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=219
03/02/2022 01:52:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=223
03/02/2022 01:52:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=226
03/02/2022 01:52:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=229
03/02/2022 01:52:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=233
03/02/2022 01:52:45 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.3235706914344685 on epoch=233
03/02/2022 01:52:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=236
03/02/2022 01:52:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=239
03/02/2022 01:52:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=243
03/02/2022 01:52:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=246
03/02/2022 01:52:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=249
03/02/2022 01:52:57 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.20083694083694084 on epoch=249
03/02/2022 01:53:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=253
03/02/2022 01:53:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=256
03/02/2022 01:53:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=259
03/02/2022 01:53:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=263
03/02/2022 01:53:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=266
03/02/2022 01:53:10 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.12227455084597942 on epoch=266
03/02/2022 01:53:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=269
03/02/2022 01:53:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=273
03/02/2022 01:53:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=276
03/02/2022 01:53:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=279
03/02/2022 01:53:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=283
03/02/2022 01:53:22 - INFO - __main__ - Global step 850 Train loss 0.14 Classification-F1 0.22290909090909095 on epoch=283
03/02/2022 01:53:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=286
03/02/2022 01:53:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=289
03/02/2022 01:53:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=293
03/02/2022 01:53:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=296
03/02/2022 01:53:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=299
03/02/2022 01:53:35 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.22914122914122914 on epoch=299
03/02/2022 01:53:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=303
03/02/2022 01:53:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=306
03/02/2022 01:53:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=309
03/02/2022 01:53:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=313
03/02/2022 01:53:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=316
03/02/2022 01:53:47 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.2329247336464658 on epoch=316
03/02/2022 01:53:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=319
03/02/2022 01:53:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=323
03/02/2022 01:53:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
03/02/2022 01:53:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=329
03/02/2022 01:53:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=333
03/02/2022 01:54:00 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.24741258741258743 on epoch=333
03/02/2022 01:54:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=336
03/02/2022 01:54:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=339
03/02/2022 01:54:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=343
03/02/2022 01:54:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=346
03/02/2022 01:54:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=349
03/02/2022 01:54:12 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.3410950751376283 on epoch=349
03/02/2022 01:54:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=353
03/02/2022 01:54:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=356
03/02/2022 01:54:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=359
03/02/2022 01:54:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
03/02/2022 01:54:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
03/02/2022 01:54:25 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.377112523472483 on epoch=366
03/02/2022 01:54:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
03/02/2022 01:54:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
03/02/2022 01:54:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=376
03/02/2022 01:54:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
03/02/2022 01:54:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=383
03/02/2022 01:54:37 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.17189931350114418 on epoch=383
03/02/2022 01:54:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=386
03/02/2022 01:54:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=389
03/02/2022 01:54:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=393
03/02/2022 01:54:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=396
03/02/2022 01:54:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
03/02/2022 01:54:50 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.41377591706539074 on epoch=399
03/02/2022 01:54:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
03/02/2022 01:54:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
03/02/2022 01:54:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=409
03/02/2022 01:54:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=413
03/02/2022 01:55:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=416
03/02/2022 01:55:02 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.3186560565870911 on epoch=416
03/02/2022 01:55:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
03/02/2022 01:55:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=423
03/02/2022 01:55:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=426
03/02/2022 01:55:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
03/02/2022 01:55:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
03/02/2022 01:55:14 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.4148359966358285 on epoch=433
03/02/2022 01:55:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=436
03/02/2022 01:55:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=439
03/02/2022 01:55:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=443
03/02/2022 01:55:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
03/02/2022 01:55:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
03/02/2022 01:55:27 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.3186335403726708 on epoch=449
03/02/2022 01:55:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
03/02/2022 01:55:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
03/02/2022 01:55:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
03/02/2022 01:55:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
03/02/2022 01:55:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
03/02/2022 01:55:39 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.2937662337662338 on epoch=466
03/02/2022 01:55:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=469
03/02/2022 01:55:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
03/02/2022 01:55:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
03/02/2022 01:55:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
03/02/2022 01:55:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
03/02/2022 01:55:51 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.4013209013209013 on epoch=483
03/02/2022 01:55:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
03/02/2022 01:55:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
03/02/2022 01:55:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
03/02/2022 01:56:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
03/02/2022 01:56:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
03/02/2022 01:56:04 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.4139288668320927 on epoch=499
03/02/2022 01:56:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=503
03/02/2022 01:56:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
03/02/2022 01:56:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
03/02/2022 01:56:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
03/02/2022 01:56:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
03/02/2022 01:56:16 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.5815503875968991 on epoch=516
03/02/2022 01:56:16 - INFO - __main__ - Saving model with best Classification-F1: 0.47493734335839594 -> 0.5815503875968991 on epoch=516, global_step=1550
03/02/2022 01:56:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
03/02/2022 01:56:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=523
03/02/2022 01:56:23 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
03/02/2022 01:56:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
03/02/2022 01:56:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
03/02/2022 01:56:29 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.33882926829268295 on epoch=533
03/02/2022 01:56:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
03/02/2022 01:56:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
03/02/2022 01:56:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
03/02/2022 01:56:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
03/02/2022 01:56:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=549
03/02/2022 01:56:41 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.5351851851851851 on epoch=549
03/02/2022 01:56:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
03/02/2022 01:56:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
03/02/2022 01:56:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
03/02/2022 01:56:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
03/02/2022 01:56:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
03/02/2022 01:56:54 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5638709677419355 on epoch=566
03/02/2022 01:56:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
03/02/2022 01:56:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
03/02/2022 01:57:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=576
03/02/2022 01:57:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=579
03/02/2022 01:57:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
03/02/2022 01:57:06 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.3080519480519481 on epoch=583
03/02/2022 01:57:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 01:57:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/02/2022 01:57:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
03/02/2022 01:57:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/02/2022 01:57:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
03/02/2022 01:57:19 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.5806020799124249 on epoch=599
03/02/2022 01:57:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 01:57:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 01:57:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
03/02/2022 01:57:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/02/2022 01:57:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
03/02/2022 01:57:31 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4268349231584526 on epoch=616
03/02/2022 01:57:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=619
03/02/2022 01:57:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
03/02/2022 01:57:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
03/02/2022 01:57:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 01:57:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 01:57:43 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5683942766295708 on epoch=633
03/02/2022 01:57:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
03/02/2022 01:57:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 01:57:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 01:57:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
03/02/2022 01:57:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
03/02/2022 01:57:55 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.39270868441748114 on epoch=649
03/02/2022 01:57:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
03/02/2022 01:57:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
03/02/2022 01:58:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
03/02/2022 01:58:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
03/02/2022 01:58:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 01:58:07 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.632085020242915 on epoch=666
03/02/2022 01:58:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5815503875968991 -> 0.632085020242915 on epoch=666, global_step=2000
03/02/2022 01:58:09 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
03/02/2022 01:58:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
03/02/2022 01:58:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
03/02/2022 01:58:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=679
03/02/2022 01:58:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 01:58:19 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.493913043478261 on epoch=683
03/02/2022 01:58:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
03/02/2022 01:58:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
03/02/2022 01:58:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 01:58:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=696
03/02/2022 01:58:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 01:58:31 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.41149877149877145 on epoch=699
03/02/2022 01:58:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=703
03/02/2022 01:58:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 01:58:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
03/02/2022 01:58:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 01:58:42 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 01:58:44 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.5417886178861788 on epoch=716
03/02/2022 01:58:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
03/02/2022 01:58:48 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
03/02/2022 01:58:50 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
03/02/2022 01:58:52 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 01:58:55 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 01:58:56 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.5495652173913043 on epoch=733
03/02/2022 01:58:58 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 01:59:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=739
03/02/2022 01:59:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 01:59:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 01:59:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 01:59:08 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6060201377848436 on epoch=749
03/02/2022 01:59:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
03/02/2022 01:59:12 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
03/02/2022 01:59:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 01:59:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
03/02/2022 01:59:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
03/02/2022 01:59:20 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.5884656084656085 on epoch=766
03/02/2022 01:59:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
03/02/2022 01:59:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
03/02/2022 01:59:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.14 on epoch=776
03/02/2022 01:59:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 01:59:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 01:59:32 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.5472043010752689 on epoch=783
03/02/2022 01:59:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 01:59:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 01:59:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
03/02/2022 01:59:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=796
03/02/2022 01:59:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
03/02/2022 01:59:44 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.43149877149877147 on epoch=799
03/02/2022 01:59:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 01:59:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
03/02/2022 01:59:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=809
03/02/2022 01:59:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
03/02/2022 01:59:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/02/2022 01:59:56 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.4165359477124183 on epoch=816
03/02/2022 01:59:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 02:00:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 02:00:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 02:00:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 02:00:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 02:00:09 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.4407731487527753 on epoch=833
03/02/2022 02:00:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 02:00:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 02:00:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 02:00:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
03/02/2022 02:00:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 02:00:21 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.44425019425019424 on epoch=849
03/02/2022 02:00:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
03/02/2022 02:00:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 02:00:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 02:00:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 02:00:32 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 02:00:33 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6250344637441412 on epoch=866
03/02/2022 02:00:35 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 02:00:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 02:00:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=876
03/02/2022 02:00:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
03/02/2022 02:00:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 02:00:45 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.5614673046251993 on epoch=883
03/02/2022 02:00:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 02:00:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
03/02/2022 02:00:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 02:00:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 02:00:56 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 02:00:58 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.5548267408732525 on epoch=899
03/02/2022 02:01:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
03/02/2022 02:01:02 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 02:01:04 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 02:01:06 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 02:01:09 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 02:01:10 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.5469245469245468 on epoch=916
03/02/2022 02:01:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 02:01:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 02:01:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 02:01:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
03/02/2022 02:01:21 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 02:01:22 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.3084033613445378 on epoch=933
03/02/2022 02:01:24 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=936
03/02/2022 02:01:27 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 02:01:29 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 02:01:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 02:01:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 02:01:34 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.3844444444444444 on epoch=949
03/02/2022 02:01:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 02:01:39 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 02:01:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=959
03/02/2022 02:01:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 02:01:46 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 02:01:47 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.34174358974358976 on epoch=966
03/02/2022 02:01:49 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 02:01:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 02:01:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 02:01:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 02:01:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 02:01:59 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.34004765909728063 on epoch=983
03/02/2022 02:02:01 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 02:02:03 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 02:02:06 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 02:02:08 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 02:02:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 02:02:11 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:02:11 - INFO - __main__ - Printing 3 examples
03/02/2022 02:02:11 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 02:02:11 - INFO - __main__ - ['hatespeech']
03/02/2022 02:02:11 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 02:02:11 - INFO - __main__ - ['hatespeech']
03/02/2022 02:02:11 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 02:02:11 - INFO - __main__ - ['hatespeech']
03/02/2022 02:02:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 02:02:11 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:02:11 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.3200808080808081 on epoch=999
03/02/2022 02:02:11 - INFO - __main__ - save last model!
03/02/2022 02:02:11 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:02:11 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:02:11 - INFO - __main__ - Printing 3 examples
03/02/2022 02:02:11 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 02:02:11 - INFO - __main__ - ['hatespeech']
03/02/2022 02:02:11 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 02:02:11 - INFO - __main__ - ['hatespeech']
03/02/2022 02:02:11 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 02:02:11 - INFO - __main__ - ['hatespeech']
03/02/2022 02:02:11 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:02:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 02:02:11 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 02:02:11 - INFO - __main__ - Printing 3 examples
03/02/2022 02:02:11 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 02:02:11 - INFO - __main__ - ['normal']
03/02/2022 02:02:11 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 02:02:11 - INFO - __main__ - ['normal']
03/02/2022 02:02:11 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 02:02:11 - INFO - __main__ - ['normal']
03/02/2022 02:02:11 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:02:11 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:02:11 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:02:12 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:02:14 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 02:02:24 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 02:02:24 - INFO - __main__ - task name: hatexplain
initialize from c4
[(1074, 376286), (672, 948582), (3489, 333521), (6319, 208284), (2690, 378727), (4999, 201462), (4302, 163912), (4946, 154063), (4264, 247591), (3735, 279578), (286, 2755883), (6059, 189253), (3679, 289187), (1958, 502002), (4233, 191191), (2476, 431307), (3339, 264161), (2818, 358712), (3812, 276017), (2550, 332246), (6597, 157538), (1047, 336403), (5997, 168391), (96, 7894015), (5806, 181362), (2027, 478074), (793, 959415), (2480, 384879), (4374, 230866), (3351, 190868), (6096, 168646), (2144, 386724), (3519, 196508), (4992, 190172), (4493, 209311), (106, 4519798), (928, 894961), (1923, 318040), (2361, 413361), (4279, 231910), (900, 380561), (2466, 307035), (6330, 161028), (3939, 254546), (4505, 197810), (5847, 169158), (4974, 176708), (3742, 272355), (3643, 219730), (4895, 169946), (3087, 284156), (2977, 265914), (3683, 206457), (2772, 342861), (3413, 306732), (2721, 367851), (2431, 400094), (1327, 735060), (1602, 475410), (2733, 298680), (1228, 739170), (4132, 174353), (2023, 494514), (2503, 477739), (2299, 422736), (93, 192649), (2957, 336282), (6637, 158975), (1665, 179358), (4163, 157427), (1635, 329909), (1116, 852428), (2082, 471031), (3118, 315958), (2028, 404951), (4197, 260190), (956, 507463), (3505, 311150), (164, 5765797), (5956, 175083), (1305, 698278), (2879, 339182), (503, 1684486), (600, 1458762), (6854, 169166), (3202, 321048), (1048, 702299), (4621, 190154), (1087, 843077), (131, 6653166), (6292, 163131), (3412, 328424), (5201, 199644), (3769, 195111), (5001, 211928), (3879, 275863), (2016, 495776), (581, 1531361), (2912, 367106)]
03/02/2022 02:02:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:02:25 - INFO - __main__ - Starting training!
03/02/2022 02:03:02 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_13_0.3_8_predictions.txt
03/02/2022 02:03:02 - INFO - __main__ - Classification-F1 on test data: 0.1716
03/02/2022 02:03:02 - INFO - __main__ - prefix=hatexplain_16_13, lr=0.3, bsz=8, dev_performance=0.632085020242915, test_performance=0.17164267811870218
03/02/2022 02:03:02 - INFO - __main__ - Running ... prefix=hatexplain_16_13, lr=0.2, bsz=8 ...
03/02/2022 02:03:03 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:03:03 - INFO - __main__ - Printing 3 examples
03/02/2022 02:03:03 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 02:03:03 - INFO - __main__ - ['hatespeech']
03/02/2022 02:03:03 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 02:03:03 - INFO - __main__ - ['hatespeech']
03/02/2022 02:03:03 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 02:03:03 - INFO - __main__ - ['hatespeech']
03/02/2022 02:03:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:03:03 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:03:03 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:03:03 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:03:03 - INFO - __main__ - Printing 3 examples
03/02/2022 02:03:03 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 02:03:03 - INFO - __main__ - ['hatespeech']
03/02/2022 02:03:03 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 02:03:03 - INFO - __main__ - ['hatespeech']
03/02/2022 02:03:03 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 02:03:03 - INFO - __main__ - ['hatespeech']
03/02/2022 02:03:03 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:03:03 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:03:03 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:03:17 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 02:03:17 - INFO - __main__ - task name: hatexplain
initialize from c4
[(1074, 376286), (672, 948582), (3489, 333521), (6319, 208284), (2690, 378727), (4999, 201462), (4302, 163912), (4946, 154063), (4264, 247591), (3735, 279578), (286, 2755883), (6059, 189253), (3679, 289187), (1958, 502002), (4233, 191191), (2476, 431307), (3339, 264161), (2818, 358712), (3812, 276017), (2550, 332246), (6597, 157538), (1047, 336403), (5997, 168391), (96, 7894015), (5806, 181362), (2027, 478074), (793, 959415), (2480, 384879), (4374, 230866), (3351, 190868), (6096, 168646), (2144, 386724), (3519, 196508), (4992, 190172), (4493, 209311), (106, 4519798), (928, 894961), (1923, 318040), (2361, 413361), (4279, 231910), (900, 380561), (2466, 307035), (6330, 161028), (3939, 254546), (4505, 197810), (5847, 169158), (4974, 176708), (3742, 272355), (3643, 219730), (4895, 169946), (3087, 284156), (2977, 265914), (3683, 206457), (2772, 342861), (3413, 306732), (2721, 367851), (2431, 400094), (1327, 735060), (1602, 475410), (2733, 298680), (1228, 739170), (4132, 174353), (2023, 494514), (2503, 477739), (2299, 422736), (93, 192649), (2957, 336282), (6637, 158975), (1665, 179358), (4163, 157427), (1635, 329909), (1116, 852428), (2082, 471031), (3118, 315958), (2028, 404951), (4197, 260190), (956, 507463), (3505, 311150), (164, 5765797), (5956, 175083), (1305, 698278), (2879, 339182), (503, 1684486), (600, 1458762), (6854, 169166), (3202, 321048), (1048, 702299), (4621, 190154), (1087, 843077), (131, 6653166), (6292, 163131), (3412, 328424), (5201, 199644), (3769, 195111), (5001, 211928), (3879, 275863), (2016, 495776), (581, 1531361), (2912, 367106)]
03/02/2022 02:03:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:03:18 - INFO - __main__ - Starting training!
03/02/2022 02:03:24 - INFO - __main__ - Step 10 Global step 10 Train loss 7.08 on epoch=3
03/02/2022 02:03:26 - INFO - __main__ - Step 20 Global step 20 Train loss 4.99 on epoch=6
03/02/2022 02:03:28 - INFO - __main__ - Step 30 Global step 30 Train loss 2.95 on epoch=9
03/02/2022 02:03:30 - INFO - __main__ - Step 40 Global step 40 Train loss 1.74 on epoch=13
03/02/2022 02:03:32 - INFO - __main__ - Step 50 Global step 50 Train loss 1.15 on epoch=16
03/02/2022 02:03:33 - INFO - __main__ - Global step 50 Train loss 3.58 Classification-F1 0.21077127659574468 on epoch=16
03/02/2022 02:03:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.21077127659574468 on epoch=16, global_step=50
03/02/2022 02:03:36 - INFO - __main__ - Step 60 Global step 60 Train loss 0.90 on epoch=19
03/02/2022 02:03:38 - INFO - __main__ - Step 70 Global step 70 Train loss 0.80 on epoch=23
03/02/2022 02:03:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.70 on epoch=26
03/02/2022 02:03:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.69 on epoch=29
03/02/2022 02:03:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.64 on epoch=33
03/02/2022 02:03:45 - INFO - __main__ - Global step 100 Train loss 0.74 Classification-F1 0.13464912280701757 on epoch=33
03/02/2022 02:03:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.64 on epoch=36
03/02/2022 02:03:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=39
03/02/2022 02:03:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=43
03/02/2022 02:03:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.60 on epoch=46
03/02/2022 02:03:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=49
03/02/2022 02:03:57 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.19999999999999998 on epoch=49
03/02/2022 02:03:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=53
03/02/2022 02:04:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=56
03/02/2022 02:04:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=59
03/02/2022 02:04:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=63
03/02/2022 02:04:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=66
03/02/2022 02:04:09 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.21003134796238246 on epoch=66
03/02/2022 02:04:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=69
03/02/2022 02:04:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=73
03/02/2022 02:04:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.57 on epoch=76
03/02/2022 02:04:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=79
03/02/2022 02:04:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=83
03/02/2022 02:04:21 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.2631468677980306 on epoch=83
03/02/2022 02:04:21 - INFO - __main__ - Saving model with best Classification-F1: 0.21077127659574468 -> 0.2631468677980306 on epoch=83, global_step=250
03/02/2022 02:04:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=86
03/02/2022 02:04:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=89
03/02/2022 02:04:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=93
03/02/2022 02:04:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.53 on epoch=96
03/02/2022 02:04:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=99
03/02/2022 02:04:33 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.17204301075268816 on epoch=99
03/02/2022 02:04:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.54 on epoch=103
03/02/2022 02:04:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=106
03/02/2022 02:04:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
03/02/2022 02:04:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=113
03/02/2022 02:04:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=116
03/02/2022 02:04:45 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.2787878787878788 on epoch=116
03/02/2022 02:04:45 - INFO - __main__ - Saving model with best Classification-F1: 0.2631468677980306 -> 0.2787878787878788 on epoch=116, global_step=350
03/02/2022 02:04:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=119
03/02/2022 02:04:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=123
03/02/2022 02:04:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=126
03/02/2022 02:04:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=129
03/02/2022 02:04:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=133
03/02/2022 02:04:57 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.2817218841559612 on epoch=133
03/02/2022 02:04:57 - INFO - __main__ - Saving model with best Classification-F1: 0.2787878787878788 -> 0.2817218841559612 on epoch=133, global_step=400
03/02/2022 02:04:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=136
03/02/2022 02:05:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=139
03/02/2022 02:05:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=143
03/02/2022 02:05:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.53 on epoch=146
03/02/2022 02:05:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=149
03/02/2022 02:05:09 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.3277777777777778 on epoch=149
03/02/2022 02:05:10 - INFO - __main__ - Saving model with best Classification-F1: 0.2817218841559612 -> 0.3277777777777778 on epoch=149, global_step=450
03/02/2022 02:05:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=153
03/02/2022 02:05:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=156
03/02/2022 02:05:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=159
03/02/2022 02:05:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=163
03/02/2022 02:05:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=166
03/02/2022 02:05:22 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.36791497975708504 on epoch=166
03/02/2022 02:05:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3277777777777778 -> 0.36791497975708504 on epoch=166, global_step=500
03/02/2022 02:05:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=169
03/02/2022 02:05:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=173
03/02/2022 02:05:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=176
03/02/2022 02:05:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=179
03/02/2022 02:05:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=183
03/02/2022 02:05:34 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.21528785679729076 on epoch=183
03/02/2022 02:05:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=186
03/02/2022 02:05:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=189
03/02/2022 02:05:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=193
03/02/2022 02:05:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=196
03/02/2022 02:05:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.51 on epoch=199
03/02/2022 02:05:46 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.2460106382978723 on epoch=199
03/02/2022 02:05:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=203
03/02/2022 02:05:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=206
03/02/2022 02:05:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=209
03/02/2022 02:05:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=213
03/02/2022 02:05:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=216
03/02/2022 02:05:58 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.1693121693121693 on epoch=216
03/02/2022 02:06:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=219
03/02/2022 02:06:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=223
03/02/2022 02:06:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=226
03/02/2022 02:06:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=229
03/02/2022 02:06:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=233
03/02/2022 02:06:10 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.2254545454545455 on epoch=233
03/02/2022 02:06:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=236
03/02/2022 02:06:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=239
03/02/2022 02:06:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=243
03/02/2022 02:06:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=246
03/02/2022 02:06:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=249
03/02/2022 02:06:22 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.18013468013468015 on epoch=249
03/02/2022 02:06:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=253
03/02/2022 02:06:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=256
03/02/2022 02:06:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=259
03/02/2022 02:06:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=263
03/02/2022 02:06:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=266
03/02/2022 02:06:34 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.3305832147937411 on epoch=266
03/02/2022 02:06:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=269
03/02/2022 02:06:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=273
03/02/2022 02:06:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=276
03/02/2022 02:06:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=279
03/02/2022 02:06:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=283
03/02/2022 02:06:46 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.3298387436318471 on epoch=283
03/02/2022 02:06:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=286
03/02/2022 02:06:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=289
03/02/2022 02:06:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=293
03/02/2022 02:06:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=296
03/02/2022 02:06:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=299
03/02/2022 02:06:58 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.178169656430526 on epoch=299
03/02/2022 02:07:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=303
03/02/2022 02:07:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=306
03/02/2022 02:07:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=309
03/02/2022 02:07:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=313
03/02/2022 02:07:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=316
03/02/2022 02:07:10 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.35266106442577033 on epoch=316
03/02/2022 02:07:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=319
03/02/2022 02:07:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=323
03/02/2022 02:07:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.32 on epoch=326
03/02/2022 02:07:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=329
03/02/2022 02:07:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=333
03/02/2022 02:07:22 - INFO - __main__ - Global step 1000 Train loss 0.34 Classification-F1 0.1685823754789272 on epoch=333
03/02/2022 02:07:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=336
03/02/2022 02:07:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=339
03/02/2022 02:07:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=343
03/02/2022 02:07:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=346
03/02/2022 02:07:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=349
03/02/2022 02:07:34 - INFO - __main__ - Global step 1050 Train loss 0.33 Classification-F1 0.2575369458128079 on epoch=349
03/02/2022 02:07:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.33 on epoch=353
03/02/2022 02:07:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=356
03/02/2022 02:07:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.27 on epoch=359
03/02/2022 02:07:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=363
03/02/2022 02:07:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=366
03/02/2022 02:07:47 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.2534453781512605 on epoch=366
03/02/2022 02:07:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=369
03/02/2022 02:07:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=373
03/02/2022 02:07:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=376
03/02/2022 02:07:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=379
03/02/2022 02:07:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.25 on epoch=383
03/02/2022 02:07:59 - INFO - __main__ - Global step 1150 Train loss 0.25 Classification-F1 0.12474164133738601 on epoch=383
03/02/2022 02:08:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.29 on epoch=386
03/02/2022 02:08:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=389
03/02/2022 02:08:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.23 on epoch=393
03/02/2022 02:08:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=396
03/02/2022 02:08:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=399
03/02/2022 02:08:11 - INFO - __main__ - Global step 1200 Train loss 0.26 Classification-F1 0.22569444444444445 on epoch=399
03/02/2022 02:08:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.23 on epoch=403
03/02/2022 02:08:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=406
03/02/2022 02:08:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=409
03/02/2022 02:08:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=413
03/02/2022 02:08:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=416
03/02/2022 02:08:23 - INFO - __main__ - Global step 1250 Train loss 0.23 Classification-F1 0.24473642800944143 on epoch=416
03/02/2022 02:08:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.25 on epoch=419
03/02/2022 02:08:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.25 on epoch=423
03/02/2022 02:08:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=426
03/02/2022 02:08:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=429
03/02/2022 02:08:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=433
03/02/2022 02:08:36 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.17953554308780947 on epoch=433
03/02/2022 02:08:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=436
03/02/2022 02:08:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=439
03/02/2022 02:08:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=443
03/02/2022 02:08:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.14 on epoch=446
03/02/2022 02:08:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.17 on epoch=449
03/02/2022 02:08:48 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.291579133510168 on epoch=449
03/02/2022 02:08:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=453
03/02/2022 02:08:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=456
03/02/2022 02:08:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=459
03/02/2022 02:08:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=463
03/02/2022 02:08:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=466
03/02/2022 02:09:00 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.2678508306822198 on epoch=466
03/02/2022 02:09:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=469
03/02/2022 02:09:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=473
03/02/2022 02:09:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=476
03/02/2022 02:09:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=479
03/02/2022 02:09:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=483
03/02/2022 02:09:13 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.39396825396825397 on epoch=483
03/02/2022 02:09:13 - INFO - __main__ - Saving model with best Classification-F1: 0.36791497975708504 -> 0.39396825396825397 on epoch=483, global_step=1450
03/02/2022 02:09:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=486
03/02/2022 02:09:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=489
03/02/2022 02:09:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=493
03/02/2022 02:09:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=496
03/02/2022 02:09:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=499
03/02/2022 02:09:25 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.4078144078144078 on epoch=499
03/02/2022 02:09:25 - INFO - __main__ - Saving model with best Classification-F1: 0.39396825396825397 -> 0.4078144078144078 on epoch=499, global_step=1500
03/02/2022 02:09:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=503
03/02/2022 02:09:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=506
03/02/2022 02:09:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=509
03/02/2022 02:09:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=513
03/02/2022 02:09:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=516
03/02/2022 02:09:38 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.2775883300179975 on epoch=516
03/02/2022 02:09:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=519
03/02/2022 02:09:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=523
03/02/2022 02:09:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=526
03/02/2022 02:09:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=529
03/02/2022 02:09:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=533
03/02/2022 02:09:51 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.26028985507246377 on epoch=533
03/02/2022 02:09:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=536
03/02/2022 02:09:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=539
03/02/2022 02:09:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=543
03/02/2022 02:10:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=546
03/02/2022 02:10:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=549
03/02/2022 02:10:04 - INFO - __main__ - Global step 1650 Train loss 0.08 Classification-F1 0.22460052025269417 on epoch=549
03/02/2022 02:10:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=553
03/02/2022 02:10:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=556
03/02/2022 02:10:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=559
03/02/2022 02:10:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=563
03/02/2022 02:10:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
03/02/2022 02:10:16 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.5664335664335663 on epoch=566
03/02/2022 02:10:16 - INFO - __main__ - Saving model with best Classification-F1: 0.4078144078144078 -> 0.5664335664335663 on epoch=566, global_step=1700
03/02/2022 02:10:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=569
03/02/2022 02:10:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
03/02/2022 02:10:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=576
03/02/2022 02:10:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
03/02/2022 02:10:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=583
03/02/2022 02:10:28 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.39330808080808083 on epoch=583
03/02/2022 02:10:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=586
03/02/2022 02:10:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=589
03/02/2022 02:10:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=593
03/02/2022 02:10:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=596
03/02/2022 02:10:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
03/02/2022 02:10:41 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.38779605263157896 on epoch=599
03/02/2022 02:10:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=603
03/02/2022 02:10:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=606
03/02/2022 02:10:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=609
03/02/2022 02:10:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=613
03/02/2022 02:10:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=616
03/02/2022 02:10:53 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.3958333333333333 on epoch=616
03/02/2022 02:10:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=619
03/02/2022 02:10:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
03/02/2022 02:10:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=626
03/02/2022 02:11:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
03/02/2022 02:11:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
03/02/2022 02:11:05 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.5069264069264069 on epoch=633
03/02/2022 02:11:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=636
03/02/2022 02:11:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=639
03/02/2022 02:11:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=643
03/02/2022 02:11:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=646
03/02/2022 02:11:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=649
03/02/2022 02:11:17 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.5407925407925408 on epoch=649
03/02/2022 02:11:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=653
03/02/2022 02:11:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=656
03/02/2022 02:11:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
03/02/2022 02:11:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=663
03/02/2022 02:11:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=666
03/02/2022 02:11:29 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.41148622175786864 on epoch=666
03/02/2022 02:11:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
03/02/2022 02:11:34 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
03/02/2022 02:11:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=676
03/02/2022 02:11:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=679
03/02/2022 02:11:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=683
03/02/2022 02:11:42 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.29275862068965514 on epoch=683
03/02/2022 02:11:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
03/02/2022 02:11:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=689
03/02/2022 02:11:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
03/02/2022 02:11:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/02/2022 02:11:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
03/02/2022 02:11:54 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.37790697674418605 on epoch=699
03/02/2022 02:11:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=703
03/02/2022 02:11:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=706
03/02/2022 02:12:01 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
03/02/2022 02:12:03 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
03/02/2022 02:12:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=716
03/02/2022 02:12:07 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.590846429556107 on epoch=716
03/02/2022 02:12:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5664335664335663 -> 0.590846429556107 on epoch=716, global_step=2150
03/02/2022 02:12:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=719
03/02/2022 02:12:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=723
03/02/2022 02:12:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
03/02/2022 02:12:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=729
03/02/2022 02:12:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=733
03/02/2022 02:12:19 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.3440846649845809 on epoch=733
03/02/2022 02:12:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
03/02/2022 02:12:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
03/02/2022 02:12:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
03/02/2022 02:12:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
03/02/2022 02:12:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=749
03/02/2022 02:12:32 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.4461869313482217 on epoch=749
03/02/2022 02:12:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=753
03/02/2022 02:12:36 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
03/02/2022 02:12:39 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
03/02/2022 02:12:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
03/02/2022 02:12:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 02:12:44 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.44501228501228507 on epoch=766
03/02/2022 02:12:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
03/02/2022 02:12:49 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
03/02/2022 02:12:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=776
03/02/2022 02:12:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
03/02/2022 02:12:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
03/02/2022 02:12:57 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.35867620751341683 on epoch=783
03/02/2022 02:12:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=786
03/02/2022 02:13:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=789
03/02/2022 02:13:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
03/02/2022 02:13:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
03/02/2022 02:13:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
03/02/2022 02:13:09 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.2876811594202899 on epoch=799
03/02/2022 02:13:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
03/02/2022 02:13:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
03/02/2022 02:13:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=809
03/02/2022 02:13:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
03/02/2022 02:13:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/02/2022 02:13:22 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.3678451178451178 on epoch=816
03/02/2022 02:13:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=819
03/02/2022 02:13:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
03/02/2022 02:13:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
03/02/2022 02:13:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
03/02/2022 02:13:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
03/02/2022 02:13:34 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.5879120879120879 on epoch=833
03/02/2022 02:13:37 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=836
03/02/2022 02:13:39 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
03/02/2022 02:13:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
03/02/2022 02:13:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=846
03/02/2022 02:13:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
03/02/2022 02:13:47 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6082166199813258 on epoch=849
03/02/2022 02:13:47 - INFO - __main__ - Saving model with best Classification-F1: 0.590846429556107 -> 0.6082166199813258 on epoch=849, global_step=2550
03/02/2022 02:13:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
03/02/2022 02:13:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
03/02/2022 02:13:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 02:13:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 02:13:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
03/02/2022 02:13:59 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.5111111111111111 on epoch=866
03/02/2022 02:14:02 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
03/02/2022 02:14:04 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=873
03/02/2022 02:14:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=876
03/02/2022 02:14:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=879
03/02/2022 02:14:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
03/02/2022 02:14:12 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.6085470085470085 on epoch=883
03/02/2022 02:14:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6082166199813258 -> 0.6085470085470085 on epoch=883, global_step=2650
03/02/2022 02:14:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
03/02/2022 02:14:16 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
03/02/2022 02:14:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 02:14:21 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
03/02/2022 02:14:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
03/02/2022 02:14:24 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6325117219426163 on epoch=899
03/02/2022 02:14:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6085470085470085 -> 0.6325117219426163 on epoch=899, global_step=2700
03/02/2022 02:14:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
03/02/2022 02:14:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
03/02/2022 02:14:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
03/02/2022 02:14:33 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
03/02/2022 02:14:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
03/02/2022 02:14:36 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.586810722218628 on epoch=916
03/02/2022 02:14:39 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
03/02/2022 02:14:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
03/02/2022 02:14:43 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
03/02/2022 02:14:45 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
03/02/2022 02:14:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=933
03/02/2022 02:14:49 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.44333333333333336 on epoch=933
03/02/2022 02:14:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
03/02/2022 02:14:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 02:14:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=943
03/02/2022 02:14:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
03/02/2022 02:15:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 02:15:01 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.4584438549955791 on epoch=949
03/02/2022 02:15:04 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
03/02/2022 02:15:06 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=956
03/02/2022 02:15:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 02:15:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 02:15:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
03/02/2022 02:15:14 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.3902097902097902 on epoch=966
03/02/2022 02:15:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
03/02/2022 02:15:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=973
03/02/2022 02:15:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 02:15:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 02:15:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 02:15:26 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.4417771883289125 on epoch=983
03/02/2022 02:15:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 02:15:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 02:15:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=993
03/02/2022 02:15:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 02:15:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
03/02/2022 02:15:38 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:15:38 - INFO - __main__ - Printing 3 examples
03/02/2022 02:15:38 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:15:38 - INFO - __main__ - ['offensive']
03/02/2022 02:15:38 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:15:38 - INFO - __main__ - ['offensive']
03/02/2022 02:15:38 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:15:38 - INFO - __main__ - ['offensive']
03/02/2022 02:15:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 02:15:38 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:15:38 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:15:38 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:15:38 - INFO - __main__ - Printing 3 examples
03/02/2022 02:15:38 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:15:38 - INFO - __main__ - ['offensive']
03/02/2022 02:15:38 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:15:38 - INFO - __main__ - ['offensive']
03/02/2022 02:15:38 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:15:38 - INFO - __main__ - ['offensive']
03/02/2022 02:15:38 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:15:38 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:15:39 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:15:39 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.5361552028218695 on epoch=999
03/02/2022 02:15:39 - INFO - __main__ - save last model!
03/02/2022 02:15:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 02:15:39 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 02:15:39 - INFO - __main__ - Printing 3 examples
03/02/2022 02:15:39 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 02:15:39 - INFO - __main__ - ['normal']
03/02/2022 02:15:39 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 02:15:39 - INFO - __main__ - ['normal']
03/02/2022 02:15:39 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 02:15:39 - INFO - __main__ - ['normal']
03/02/2022 02:15:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:15:39 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:15:41 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 02:15:53 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 02:15:53 - INFO - __main__ - task name: hatexplain
initialize from c4
[(5492, 195474), (1068, 771470), (733, 1214890), (5682, 233612), (5433, 189971), (845, 1086618), (5661, 162439), (590, 1481555), (1953, 589554), (1631, 598174), (6364, 166204), (2896, 309135), (918, 967989), (2332, 372930), (1418, 745074), (6659, 167523), (4626, 225869), (3640, 155110), (6597, 157538), (2460, 406809), (5734, 179839), (3924, 246414), (4737, 208385), (4578, 211218), (2000, 315151), (6461, 159321), (4926, 217922), (2029, 307697), (6465, 164367), (297, 1324121), (6650, 154117), (1042, 857631), (2200, 439737), (2272, 432795), (2930, 350772), (937, 1007700), (1169, 574433), (1748, 386930), (855, 522337), (5413, 169514), (783, 1075853), (1717, 576145), (34, 32364675), (4420, 230091), (6446, 158041), (4775, 199538), (2501, 321680), (379, 2412996), (2253, 398597), (3266, 325487), (3227, 330589), (4913, 206391), (771, 1130105), (2510, 395690), (6723, 158917), (1736, 460906), (4783, 219874), (1221, 767154), (5963, 171669), (2824, 408492), (1449, 598263), (2875, 349182), (1492, 630896), (770, 1128621), (647, 1332269), (1773, 547210), (3278, 305705), (2558, 157757), (4239, 164202), (3065, 325101), (5452, 212734), (1344, 712448), (4852, 199901), (2783, 308812), (3551, 232925), (2940, 341076), (2169, 187157), (2795, 173107), (3463, 189435), (3876, 213357), (866, 1018574), (6803, 157013), (453, 1935608), (5275, 216061), (3008, 291537), (6894, 155616), (970, 369184), (298, 3090275), (324, 909673), (2594, 403711), (4477, 162791), (6539, 159792), (237, 3927936), (3372, 344743), (6209, 162555), (464, 1931312), (3535, 517891), (5859, 199324), (6157, 180769)]
03/02/2022 02:15:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:15:54 - INFO - __main__ - Starting training!
03/02/2022 02:16:44 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_13_0.2_8_predictions.txt
03/02/2022 02:16:44 - INFO - __main__ - Classification-F1 on test data: 0.1413
03/02/2022 02:16:44 - INFO - __main__ - prefix=hatexplain_16_13, lr=0.2, bsz=8, dev_performance=0.6325117219426163, test_performance=0.1412793564434364
03/02/2022 02:16:44 - INFO - __main__ - Running ... prefix=hatexplain_16_21, lr=0.5, bsz=8 ...
03/02/2022 02:16:45 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:16:45 - INFO - __main__ - Printing 3 examples
03/02/2022 02:16:45 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:16:45 - INFO - __main__ - ['offensive']
03/02/2022 02:16:45 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:16:45 - INFO - __main__ - ['offensive']
03/02/2022 02:16:45 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:16:45 - INFO - __main__ - ['offensive']
03/02/2022 02:16:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:16:45 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:16:45 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:16:45 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:16:45 - INFO - __main__ - Printing 3 examples
03/02/2022 02:16:45 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:16:45 - INFO - __main__ - ['offensive']
03/02/2022 02:16:45 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:16:45 - INFO - __main__ - ['offensive']
03/02/2022 02:16:45 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:16:45 - INFO - __main__ - ['offensive']
03/02/2022 02:16:45 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:16:45 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:16:46 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:16:59 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 02:16:59 - INFO - __main__ - task name: hatexplain
initialize from c4
[(5492, 195474), (1068, 771470), (733, 1214890), (5682, 233612), (5433, 189971), (845, 1086618), (5661, 162439), (590, 1481555), (1953, 589554), (1631, 598174), (6364, 166204), (2896, 309135), (918, 967989), (2332, 372930), (1418, 745074), (6659, 167523), (4626, 225869), (3640, 155110), (6597, 157538), (2460, 406809), (5734, 179839), (3924, 246414), (4737, 208385), (4578, 211218), (2000, 315151), (6461, 159321), (4926, 217922), (2029, 307697), (6465, 164367), (297, 1324121), (6650, 154117), (1042, 857631), (2200, 439737), (2272, 432795), (2930, 350772), (937, 1007700), (1169, 574433), (1748, 386930), (855, 522337), (5413, 169514), (783, 1075853), (1717, 576145), (34, 32364675), (4420, 230091), (6446, 158041), (4775, 199538), (2501, 321680), (379, 2412996), (2253, 398597), (3266, 325487), (3227, 330589), (4913, 206391), (771, 1130105), (2510, 395690), (6723, 158917), (1736, 460906), (4783, 219874), (1221, 767154), (5963, 171669), (2824, 408492), (1449, 598263), (2875, 349182), (1492, 630896), (770, 1128621), (647, 1332269), (1773, 547210), (3278, 305705), (2558, 157757), (4239, 164202), (3065, 325101), (5452, 212734), (1344, 712448), (4852, 199901), (2783, 308812), (3551, 232925), (2940, 341076), (2169, 187157), (2795, 173107), (3463, 189435), (3876, 213357), (866, 1018574), (6803, 157013), (453, 1935608), (5275, 216061), (3008, 291537), (6894, 155616), (970, 369184), (298, 3090275), (324, 909673), (2594, 403711), (4477, 162791), (6539, 159792), (237, 3927936), (3372, 344743), (6209, 162555), (464, 1931312), (3535, 517891), (5859, 199324), (6157, 180769)]
03/02/2022 02:17:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:17:00 - INFO - __main__ - Starting training!
03/02/2022 02:17:03 - INFO - __main__ - Step 10 Global step 10 Train loss 5.74 on epoch=3
03/02/2022 02:17:05 - INFO - __main__ - Step 20 Global step 20 Train loss 2.01 on epoch=6
03/02/2022 02:17:07 - INFO - __main__ - Step 30 Global step 30 Train loss 1.01 on epoch=9
03/02/2022 02:17:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.72 on epoch=13
03/02/2022 02:17:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.75 on epoch=16
03/02/2022 02:17:12 - INFO - __main__ - Global step 50 Train loss 2.05 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 02:17:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 02:17:14 - INFO - __main__ - Step 60 Global step 60 Train loss 0.62 on epoch=19
03/02/2022 02:17:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=23
03/02/2022 02:17:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.53 on epoch=26
03/02/2022 02:17:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=29
03/02/2022 02:17:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=33
03/02/2022 02:17:24 - INFO - __main__ - Global step 100 Train loss 0.56 Classification-F1 0.11666666666666667 on epoch=33
03/02/2022 02:17:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=36
03/02/2022 02:17:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=39
03/02/2022 02:17:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
03/02/2022 02:17:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=46
03/02/2022 02:17:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=49
03/02/2022 02:17:36 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.3488034998945815 on epoch=49
03/02/2022 02:17:36 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.3488034998945815 on epoch=49, global_step=150
03/02/2022 02:17:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=53
03/02/2022 02:17:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=56
03/02/2022 02:17:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=59
03/02/2022 02:17:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=63
03/02/2022 02:17:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=66
03/02/2022 02:17:48 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.15873015873015875 on epoch=66
03/02/2022 02:17:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=69
03/02/2022 02:17:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
03/02/2022 02:17:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=76
03/02/2022 02:17:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=79
03/02/2022 02:18:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=83
03/02/2022 02:18:01 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.3766013071895425 on epoch=83
03/02/2022 02:18:01 - INFO - __main__ - Saving model with best Classification-F1: 0.3488034998945815 -> 0.3766013071895425 on epoch=83, global_step=250
03/02/2022 02:18:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=86
03/02/2022 02:18:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=89
03/02/2022 02:18:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=93
03/02/2022 02:18:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=96
03/02/2022 02:18:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=99
03/02/2022 02:18:13 - INFO - __main__ - Global step 300 Train loss 0.33 Classification-F1 0.29306763285024157 on epoch=99
03/02/2022 02:18:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.33 on epoch=103
03/02/2022 02:18:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=106
03/02/2022 02:18:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=109
03/02/2022 02:18:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=113
03/02/2022 02:18:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=116
03/02/2022 02:18:25 - INFO - __main__ - Global step 350 Train loss 0.28 Classification-F1 0.275 on epoch=116
03/02/2022 02:18:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=119
03/02/2022 02:18:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=123
03/02/2022 02:18:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=126
03/02/2022 02:18:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.17 on epoch=129
03/02/2022 02:18:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=133
03/02/2022 02:18:37 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.2293109243697479 on epoch=133
03/02/2022 02:18:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.15 on epoch=136
03/02/2022 02:18:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=139
03/02/2022 02:18:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=143
03/02/2022 02:18:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=146
03/02/2022 02:18:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=149
03/02/2022 02:18:49 - INFO - __main__ - Global step 450 Train loss 0.20 Classification-F1 0.28503163279041593 on epoch=149
03/02/2022 02:18:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.18 on epoch=153
03/02/2022 02:18:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=156
03/02/2022 02:18:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=159
03/02/2022 02:18:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.14 on epoch=163
03/02/2022 02:19:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.11 on epoch=166
03/02/2022 02:19:01 - INFO - __main__ - Global step 500 Train loss 0.14 Classification-F1 0.3598014888337469 on epoch=166
03/02/2022 02:19:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=169
03/02/2022 02:19:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.15 on epoch=173
03/02/2022 02:19:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=176
03/02/2022 02:19:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=179
03/02/2022 02:19:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.12 on epoch=183
03/02/2022 02:19:13 - INFO - __main__ - Global step 550 Train loss 0.13 Classification-F1 0.2508944543828265 on epoch=183
03/02/2022 02:19:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.10 on epoch=186
03/02/2022 02:19:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=189
03/02/2022 02:19:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.09 on epoch=193
03/02/2022 02:19:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=196
03/02/2022 02:19:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=199
03/02/2022 02:19:26 - INFO - __main__ - Global step 600 Train loss 0.10 Classification-F1 0.24999999999999997 on epoch=199
03/02/2022 02:19:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=203
03/02/2022 02:19:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=206
03/02/2022 02:19:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.06 on epoch=209
03/02/2022 02:19:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=213
03/02/2022 02:19:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=216
03/02/2022 02:19:38 - INFO - __main__ - Global step 650 Train loss 0.06 Classification-F1 0.3556832694763729 on epoch=216
03/02/2022 02:19:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=219
03/02/2022 02:19:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=223
03/02/2022 02:19:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.06 on epoch=226
03/02/2022 02:19:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=229
03/02/2022 02:19:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=233
03/02/2022 02:19:50 - INFO - __main__ - Global step 700 Train loss 0.06 Classification-F1 0.3302768549280177 on epoch=233
03/02/2022 02:19:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=236
03/02/2022 02:19:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=239
03/02/2022 02:19:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=243
03/02/2022 02:19:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=246
03/02/2022 02:20:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=249
03/02/2022 02:20:02 - INFO - __main__ - Global step 750 Train loss 0.04 Classification-F1 0.24386568386568386 on epoch=249
03/02/2022 02:20:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=253
03/02/2022 02:20:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=256
03/02/2022 02:20:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=259
03/02/2022 02:20:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=263
03/02/2022 02:20:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=266
03/02/2022 02:20:14 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.2007843137254902 on epoch=266
03/02/2022 02:20:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=269
03/02/2022 02:20:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=273
03/02/2022 02:20:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=276
03/02/2022 02:20:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=279
03/02/2022 02:20:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=283
03/02/2022 02:20:26 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.32021466905187834 on epoch=283
03/02/2022 02:20:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=286
03/02/2022 02:20:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=289
03/02/2022 02:20:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=293
03/02/2022 02:20:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
03/02/2022 02:20:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=299
03/02/2022 02:20:38 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.328921568627451 on epoch=299
03/02/2022 02:20:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=303
03/02/2022 02:20:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=306
03/02/2022 02:20:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=309
03/02/2022 02:20:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=313
03/02/2022 02:20:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=316
03/02/2022 02:20:50 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.3598178137651822 on epoch=316
03/02/2022 02:20:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=319
03/02/2022 02:20:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=323
03/02/2022 02:20:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=326
03/02/2022 02:21:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=329
03/02/2022 02:21:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=333
03/02/2022 02:21:03 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.3247503827823424 on epoch=333
03/02/2022 02:21:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=336
03/02/2022 02:21:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=339
03/02/2022 02:21:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=343
03/02/2022 02:21:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=346
03/02/2022 02:21:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=349
03/02/2022 02:21:15 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.37072739427659535 on epoch=349
03/02/2022 02:21:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
03/02/2022 02:21:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
03/02/2022 02:21:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
03/02/2022 02:21:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=363
03/02/2022 02:21:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
03/02/2022 02:21:28 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.28278809313292075 on epoch=366
03/02/2022 02:21:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
03/02/2022 02:21:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
03/02/2022 02:21:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=376
03/02/2022 02:21:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=379
03/02/2022 02:21:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
03/02/2022 02:21:40 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.4062338200269235 on epoch=383
03/02/2022 02:21:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3766013071895425 -> 0.4062338200269235 on epoch=383, global_step=1150
03/02/2022 02:21:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=386
03/02/2022 02:21:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=389
03/02/2022 02:21:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=393
03/02/2022 02:21:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
03/02/2022 02:21:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
03/02/2022 02:21:52 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.36605182257356167 on epoch=399
03/02/2022 02:21:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
03/02/2022 02:21:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
03/02/2022 02:21:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
03/02/2022 02:22:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
03/02/2022 02:22:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=416
03/02/2022 02:22:05 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.3035939164971423 on epoch=416
03/02/2022 02:22:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=419
03/02/2022 02:22:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
03/02/2022 02:22:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=426
03/02/2022 02:22:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=429
03/02/2022 02:22:16 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=433
03/02/2022 02:22:17 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.33632220455749867 on epoch=433
03/02/2022 02:22:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
03/02/2022 02:22:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
03/02/2022 02:22:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=443
03/02/2022 02:22:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
03/02/2022 02:22:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
03/02/2022 02:22:30 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.28112436344143665 on epoch=449
03/02/2022 02:22:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
03/02/2022 02:22:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
03/02/2022 02:22:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=459
03/02/2022 02:22:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
03/02/2022 02:22:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
03/02/2022 02:22:42 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.3784829721362229 on epoch=466
03/02/2022 02:22:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
03/02/2022 02:22:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
03/02/2022 02:22:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
03/02/2022 02:22:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
03/02/2022 02:22:54 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=483
03/02/2022 02:22:55 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.18531986531986527 on epoch=483
03/02/2022 02:22:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=486
03/02/2022 02:22:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
03/02/2022 02:23:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
03/02/2022 02:23:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
03/02/2022 02:23:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
03/02/2022 02:23:07 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.26638847758783146 on epoch=499
03/02/2022 02:23:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=503
03/02/2022 02:23:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
03/02/2022 02:23:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
03/02/2022 02:23:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
03/02/2022 02:23:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
03/02/2022 02:23:20 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.34655719759168035 on epoch=516
03/02/2022 02:23:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
03/02/2022 02:23:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
03/02/2022 02:23:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
03/02/2022 02:23:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
03/02/2022 02:23:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
03/02/2022 02:23:32 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.3867998433215824 on epoch=533
03/02/2022 02:23:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
03/02/2022 02:23:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
03/02/2022 02:23:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
03/02/2022 02:23:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
03/02/2022 02:23:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
03/02/2022 02:23:44 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.3759401709401709 on epoch=549
03/02/2022 02:23:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
03/02/2022 02:23:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
03/02/2022 02:23:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
03/02/2022 02:23:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
03/02/2022 02:23:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
03/02/2022 02:23:56 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.351051051051051 on epoch=566
03/02/2022 02:23:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
03/02/2022 02:24:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
03/02/2022 02:24:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
03/02/2022 02:24:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
03/02/2022 02:24:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
03/02/2022 02:24:08 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.23990074441687348 on epoch=583
03/02/2022 02:24:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 02:24:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
03/02/2022 02:24:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
03/02/2022 02:24:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
03/02/2022 02:24:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
03/02/2022 02:24:20 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.20522875816993463 on epoch=599
03/02/2022 02:24:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
03/02/2022 02:24:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 02:24:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
03/02/2022 02:24:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/02/2022 02:24:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
03/02/2022 02:24:32 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.2944203944203944 on epoch=616
03/02/2022 02:24:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
03/02/2022 02:24:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
03/02/2022 02:24:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
03/02/2022 02:24:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
03/02/2022 02:24:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 02:24:45 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.2439254158004158 on epoch=633
03/02/2022 02:24:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
03/02/2022 02:24:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 02:24:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 02:24:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 02:24:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
03/02/2022 02:24:57 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.2648809523809524 on epoch=649
03/02/2022 02:24:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 02:25:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
03/02/2022 02:25:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
03/02/2022 02:25:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
03/02/2022 02:25:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
03/02/2022 02:25:09 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.32983193277310924 on epoch=666
03/02/2022 02:25:11 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=669
03/02/2022 02:25:13 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 02:25:15 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 02:25:18 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 02:25:20 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 02:25:21 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.42092092092092087 on epoch=683
03/02/2022 02:25:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4062338200269235 -> 0.42092092092092087 on epoch=683, global_step=2050
03/02/2022 02:25:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
03/02/2022 02:25:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
03/02/2022 02:25:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 02:25:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/02/2022 02:25:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 02:25:34 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.2709167367535744 on epoch=699
03/02/2022 02:25:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=703
03/02/2022 02:25:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 02:25:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/02/2022 02:25:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 02:25:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=716
03/02/2022 02:25:46 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.3603896103896104 on epoch=716
03/02/2022 02:25:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
03/02/2022 02:25:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
03/02/2022 02:25:53 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 02:25:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
03/02/2022 02:25:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
03/02/2022 02:25:59 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.4156346749226006 on epoch=733
03/02/2022 02:26:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
03/02/2022 02:26:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 02:26:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 02:26:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 02:26:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
03/02/2022 02:26:12 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.4499525166191833 on epoch=749
03/02/2022 02:26:12 - INFO - __main__ - Saving model with best Classification-F1: 0.42092092092092087 -> 0.4499525166191833 on epoch=749, global_step=2250
03/02/2022 02:26:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 02:26:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/02/2022 02:26:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 02:26:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 02:26:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 02:26:25 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.2941841045289321 on epoch=766
03/02/2022 02:26:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 02:26:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 02:26:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 02:26:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 02:26:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
03/02/2022 02:26:37 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.28706279809220986 on epoch=783
03/02/2022 02:26:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 02:26:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 02:26:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 02:26:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 02:26:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 02:26:50 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.30885585003232063 on epoch=799
03/02/2022 02:26:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=803
03/02/2022 02:26:55 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 02:26:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 02:27:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 02:27:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=816
03/02/2022 02:27:03 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.3821548821548822 on epoch=816
03/02/2022 02:27:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 02:27:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 02:27:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
03/02/2022 02:27:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 02:27:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 02:27:16 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.3001395534290271 on epoch=833
03/02/2022 02:27:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 02:27:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 02:27:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
03/02/2022 02:27:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 02:27:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 02:27:29 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.4383004926108374 on epoch=849
03/02/2022 02:27:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/02/2022 02:27:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 02:27:36 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
03/02/2022 02:27:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
03/02/2022 02:27:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 02:27:41 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.4698670796231772 on epoch=866
03/02/2022 02:27:42 - INFO - __main__ - Saving model with best Classification-F1: 0.4499525166191833 -> 0.4698670796231772 on epoch=866, global_step=2600
03/02/2022 02:27:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 02:27:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
03/02/2022 02:27:48 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 02:27:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 02:27:53 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 02:27:54 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.4007014007014007 on epoch=883
03/02/2022 02:27:56 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 02:27:59 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 02:28:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 02:28:04 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=896
03/02/2022 02:28:06 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 02:28:07 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.38049938049938054 on epoch=899
03/02/2022 02:28:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 02:28:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 02:28:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 02:28:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 02:28:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=916
03/02/2022 02:28:20 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.3756989247311828 on epoch=916
03/02/2022 02:28:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 02:28:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=923
03/02/2022 02:28:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 02:28:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 02:28:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 02:28:32 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.36427015250544664 on epoch=933
03/02/2022 02:28:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 02:28:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 02:28:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
03/02/2022 02:28:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=946
03/02/2022 02:28:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 02:28:44 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.38206956956956956 on epoch=949
03/02/2022 02:28:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
03/02/2022 02:28:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 02:28:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 02:28:53 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
03/02/2022 02:28:55 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
03/02/2022 02:28:56 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.410768300060496 on epoch=966
03/02/2022 02:28:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
03/02/2022 02:29:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 02:29:03 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
03/02/2022 02:29:05 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 02:29:07 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 02:29:08 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.33903133903133903 on epoch=983
03/02/2022 02:29:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 02:29:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 02:29:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 02:29:17 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
03/02/2022 02:29:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 02:29:21 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.2560096153846154 on epoch=999
03/02/2022 02:29:21 - INFO - __main__ - save last model!
03/02/2022 02:29:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 02:29:21 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 02:29:21 - INFO - __main__ - Printing 3 examples
03/02/2022 02:29:21 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 02:29:21 - INFO - __main__ - ['normal']
03/02/2022 02:29:21 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 02:29:21 - INFO - __main__ - ['normal']
03/02/2022 02:29:21 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 02:29:21 - INFO - __main__ - ['normal']
03/02/2022 02:29:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:29:22 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:29:22 - INFO - __main__ - Printing 3 examples
03/02/2022 02:29:22 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:29:22 - INFO - __main__ - ['offensive']
03/02/2022 02:29:22 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:29:22 - INFO - __main__ - ['offensive']
03/02/2022 02:29:22 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:29:22 - INFO - __main__ - ['offensive']
03/02/2022 02:29:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 02:29:22 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:29:22 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:29:22 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:29:22 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:29:22 - INFO - __main__ - Printing 3 examples
03/02/2022 02:29:22 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:29:22 - INFO - __main__ - ['offensive']
03/02/2022 02:29:22 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:29:22 - INFO - __main__ - ['offensive']
03/02/2022 02:29:22 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:29:22 - INFO - __main__ - ['offensive']
03/02/2022 02:29:22 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:29:22 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:29:22 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:29:24 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 02:29:35 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 02:29:35 - INFO - __main__ - task name: hatexplain
initialize from c4
[(3258, 211425), (3213, 334916), (1909, 502837), (8857, 160161), (4836, 214321), (5433, 189971), (962, 911154), (4497, 199538), (1712, 268688), (355, 230108), (2271, 282308), (2560, 292543), (1090, 829527), (1212, 563029), (3426, 284907), (2600, 228530), (5100, 188863), (362, 325366), (4454, 233931), (5535, 170020), (424, 2133697), (3541, 206040), (1155, 940173), (2043, 299538), (2711, 367153), (3410, 290749), (1277, 761840), (4351, 231959), (4660, 182019), (16, 86594496), (2562, 172369), (5531, 202427), (4427, 227401), (531, 1463009), (1959, 488127), (6058, 180821), (3884, 259889), (4747, 187821), (5105, 211448), (577, 1526764), (2024, 448261), (2891, 347678), (6176, 170373), (526, 676575), (1744, 325562), (4183, 213298), (5308, 192463), (1310, 731782), (233, 2752414), (6865, 154321), (2409, 387991), (1429, 570527), (1618, 302520), (5642, 169703), (102, 5915536), (6195, 169481), (4890, 230195), (1039, 892616), (2451, 408517), (1293, 773200), (4327, 202585), (1248, 751368), (1705, 584257), (5563, 187825), (3381, 321221), (5918, 183921), (2786, 340142), (4285, 226300), (5872, 193577), (5250, 198049), (2692, 378744), (4659, 205143), (4173, 169588), (1469, 619260), (4321, 244144), (1921, 479667), (5352, 156643), (5286, 184140), (1472, 652757), (427, 1146202), (1047, 336403), (4463, 226544), (2403, 276759), (5209, 184306), (2787, 347383), (34, 32364675), (3033, 329937), (3275, 314100), (873, 159860), (6005, 161456), (4593, 190444), (1587, 599851), (3581, 209865), (4857, 173501), (4440, 192534), (2693, 271357), (2668, 283036), (5413, 169514), (6025, 171968)]
03/02/2022 02:29:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:29:36 - INFO - __main__ - Starting training!
03/02/2022 02:30:12 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_21_0.5_8_predictions.txt
03/02/2022 02:30:12 - INFO - __main__ - Classification-F1 on test data: 0.2019
03/02/2022 02:30:12 - INFO - __main__ - prefix=hatexplain_16_21, lr=0.5, bsz=8, dev_performance=0.4698670796231772, test_performance=0.20193232221369203
03/02/2022 02:30:12 - INFO - __main__ - Running ... prefix=hatexplain_16_21, lr=0.4, bsz=8 ...
03/02/2022 02:30:13 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:30:13 - INFO - __main__ - Printing 3 examples
03/02/2022 02:30:13 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:30:13 - INFO - __main__ - ['offensive']
03/02/2022 02:30:13 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:30:13 - INFO - __main__ - ['offensive']
03/02/2022 02:30:13 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:30:13 - INFO - __main__ - ['offensive']
03/02/2022 02:30:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:30:13 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:30:13 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:30:13 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:30:13 - INFO - __main__ - Printing 3 examples
03/02/2022 02:30:13 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:30:13 - INFO - __main__ - ['offensive']
03/02/2022 02:30:13 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:30:13 - INFO - __main__ - ['offensive']
03/02/2022 02:30:13 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:30:13 - INFO - __main__ - ['offensive']
03/02/2022 02:30:13 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:30:13 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:30:14 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:30:28 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 02:30:28 - INFO - __main__ - task name: hatexplain
initialize from c4
[(3258, 211425), (3213, 334916), (1909, 502837), (8857, 160161), (4836, 214321), (5433, 189971), (962, 911154), (4497, 199538), (1712, 268688), (355, 230108), (2271, 282308), (2560, 292543), (1090, 829527), (1212, 563029), (3426, 284907), (2600, 228530), (5100, 188863), (362, 325366), (4454, 233931), (5535, 170020), (424, 2133697), (3541, 206040), (1155, 940173), (2043, 299538), (2711, 367153), (3410, 290749), (1277, 761840), (4351, 231959), (4660, 182019), (16, 86594496), (2562, 172369), (5531, 202427), (4427, 227401), (531, 1463009), (1959, 488127), (6058, 180821), (3884, 259889), (4747, 187821), (5105, 211448), (577, 1526764), (2024, 448261), (2891, 347678), (6176, 170373), (526, 676575), (1744, 325562), (4183, 213298), (5308, 192463), (1310, 731782), (233, 2752414), (6865, 154321), (2409, 387991), (1429, 570527), (1618, 302520), (5642, 169703), (102, 5915536), (6195, 169481), (4890, 230195), (1039, 892616), (2451, 408517), (1293, 773200), (4327, 202585), (1248, 751368), (1705, 584257), (5563, 187825), (3381, 321221), (5918, 183921), (2786, 340142), (4285, 226300), (5872, 193577), (5250, 198049), (2692, 378744), (4659, 205143), (4173, 169588), (1469, 619260), (4321, 244144), (1921, 479667), (5352, 156643), (5286, 184140), (1472, 652757), (427, 1146202), (1047, 336403), (4463, 226544), (2403, 276759), (5209, 184306), (2787, 347383), (34, 32364675), (3033, 329937), (3275, 314100), (873, 159860), (6005, 161456), (4593, 190444), (1587, 599851), (3581, 209865), (4857, 173501), (4440, 192534), (2693, 271357), (2668, 283036), (5413, 169514), (6025, 171968)]
03/02/2022 02:30:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:30:28 - INFO - __main__ - Starting training!
03/02/2022 02:30:31 - INFO - __main__ - Step 10 Global step 10 Train loss 6.38 on epoch=3
03/02/2022 02:30:33 - INFO - __main__ - Step 20 Global step 20 Train loss 2.85 on epoch=6
03/02/2022 02:30:35 - INFO - __main__ - Step 30 Global step 30 Train loss 1.21 on epoch=9
03/02/2022 02:30:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.82 on epoch=13
03/02/2022 02:30:39 - INFO - __main__ - Step 50 Global step 50 Train loss 0.67 on epoch=16
03/02/2022 02:30:41 - INFO - __main__ - Global step 50 Train loss 2.39 Classification-F1 0.3625192012288787 on epoch=16
03/02/2022 02:30:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3625192012288787 on epoch=16, global_step=50
03/02/2022 02:30:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.65 on epoch=19
03/02/2022 02:30:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.63 on epoch=23
03/02/2022 02:30:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.64 on epoch=26
03/02/2022 02:30:49 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=29
03/02/2022 02:30:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=33
03/02/2022 02:30:53 - INFO - __main__ - Global step 100 Train loss 0.60 Classification-F1 0.19841269841269837 on epoch=33
03/02/2022 02:30:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=36
03/02/2022 02:30:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=39
03/02/2022 02:30:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=43
03/02/2022 02:31:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=46
03/02/2022 02:31:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
03/02/2022 02:31:04 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.2111111111111111 on epoch=49
03/02/2022 02:31:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
03/02/2022 02:31:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=56
03/02/2022 02:31:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=59
03/02/2022 02:31:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=63
03/02/2022 02:31:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=66
03/02/2022 02:31:16 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=66
03/02/2022 02:31:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=69
03/02/2022 02:31:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=73
03/02/2022 02:31:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
03/02/2022 02:31:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=79
03/02/2022 02:31:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=83
03/02/2022 02:31:28 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.19907407407407407 on epoch=83
03/02/2022 02:31:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=86
03/02/2022 02:31:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=89
03/02/2022 02:31:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=93
03/02/2022 02:31:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=96
03/02/2022 02:31:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=99
03/02/2022 02:31:39 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.2962962962962963 on epoch=99
03/02/2022 02:31:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=103
03/02/2022 02:31:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=106
03/02/2022 02:31:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=109
03/02/2022 02:31:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=113
03/02/2022 02:31:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.32 on epoch=116
03/02/2022 02:31:51 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.30019875302894167 on epoch=116
03/02/2022 02:31:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=119
03/02/2022 02:31:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=123
03/02/2022 02:31:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=126
03/02/2022 02:32:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=129
03/02/2022 02:32:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=133
03/02/2022 02:32:03 - INFO - __main__ - Global step 400 Train loss 0.33 Classification-F1 0.28613123993558776 on epoch=133
03/02/2022 02:32:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=136
03/02/2022 02:32:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=139
03/02/2022 02:32:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=143
03/02/2022 02:32:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=146
03/02/2022 02:32:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.32 on epoch=149
03/02/2022 02:32:16 - INFO - __main__ - Global step 450 Train loss 0.28 Classification-F1 0.21538461538461537 on epoch=149
03/02/2022 02:32:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=153
03/02/2022 02:32:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=156
03/02/2022 02:32:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=159
03/02/2022 02:32:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.32 on epoch=163
03/02/2022 02:32:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=166
03/02/2022 02:32:28 - INFO - __main__ - Global step 500 Train loss 0.27 Classification-F1 0.2876201366767404 on epoch=166
03/02/2022 02:32:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=169
03/02/2022 02:32:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=173
03/02/2022 02:32:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=176
03/02/2022 02:32:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=179
03/02/2022 02:32:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=183
03/02/2022 02:32:40 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.21060837028824833 on epoch=183
03/02/2022 02:32:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=186
03/02/2022 02:32:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=189
03/02/2022 02:32:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=193
03/02/2022 02:32:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=196
03/02/2022 02:32:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.10 on epoch=199
03/02/2022 02:32:52 - INFO - __main__ - Global step 600 Train loss 0.17 Classification-F1 0.2958985301090564 on epoch=199
03/02/2022 02:32:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=203
03/02/2022 02:32:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=206
03/02/2022 02:32:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=209
03/02/2022 02:33:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=213
03/02/2022 02:33:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=216
03/02/2022 02:33:04 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.45010683760683756 on epoch=216
03/02/2022 02:33:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3625192012288787 -> 0.45010683760683756 on epoch=216, global_step=650
03/02/2022 02:33:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=219
03/02/2022 02:33:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=223
03/02/2022 02:33:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=226
03/02/2022 02:33:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=229
03/02/2022 02:33:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.16 on epoch=233
03/02/2022 02:33:16 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.2919158361018826 on epoch=233
03/02/2022 02:33:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=236
03/02/2022 02:33:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=239
03/02/2022 02:33:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=243
03/02/2022 02:33:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=246
03/02/2022 02:33:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=249
03/02/2022 02:33:29 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.32451990468180547 on epoch=249
03/02/2022 02:33:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=253
03/02/2022 02:33:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=256
03/02/2022 02:33:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=259
03/02/2022 02:33:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=263
03/02/2022 02:33:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=266
03/02/2022 02:33:41 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.25690585158548546 on epoch=266
03/02/2022 02:33:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=269
03/02/2022 02:33:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=273
03/02/2022 02:33:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=276
03/02/2022 02:33:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=279
03/02/2022 02:33:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=283
03/02/2022 02:33:53 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.19647639182522902 on epoch=283
03/02/2022 02:33:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=286
03/02/2022 02:33:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=289
03/02/2022 02:33:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=293
03/02/2022 02:34:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=296
03/02/2022 02:34:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
03/02/2022 02:34:05 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.44423280423280426 on epoch=299
03/02/2022 02:34:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=303
03/02/2022 02:34:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=306
03/02/2022 02:34:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=309
03/02/2022 02:34:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=313
03/02/2022 02:34:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=316
03/02/2022 02:34:17 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.31153846153846154 on epoch=316
03/02/2022 02:34:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=319
03/02/2022 02:34:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=323
03/02/2022 02:34:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
03/02/2022 02:34:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=329
03/02/2022 02:34:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=333
03/02/2022 02:34:29 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.20618401206636502 on epoch=333
03/02/2022 02:34:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=336
03/02/2022 02:34:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=339
03/02/2022 02:34:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=343
03/02/2022 02:34:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=346
03/02/2022 02:34:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=349
03/02/2022 02:34:41 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.23969443849503821 on epoch=349
03/02/2022 02:34:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
03/02/2022 02:34:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=356
03/02/2022 02:34:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=359
03/02/2022 02:34:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=363
03/02/2022 02:34:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=366
03/02/2022 02:34:53 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.16249784371226494 on epoch=366
03/02/2022 02:34:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=369
03/02/2022 02:34:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
03/02/2022 02:35:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=376
03/02/2022 02:35:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
03/02/2022 02:35:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=383
03/02/2022 02:35:06 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.2528769657724329 on epoch=383
03/02/2022 02:35:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
03/02/2022 02:35:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=389
03/02/2022 02:35:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
03/02/2022 02:35:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
03/02/2022 02:35:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
03/02/2022 02:35:19 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.2944203944203944 on epoch=399
03/02/2022 02:35:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=403
03/02/2022 02:35:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
03/02/2022 02:35:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=409
03/02/2022 02:35:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
03/02/2022 02:35:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
03/02/2022 02:35:31 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.4327731092436975 on epoch=416
03/02/2022 02:35:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
03/02/2022 02:35:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=423
03/02/2022 02:35:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
03/02/2022 02:35:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
03/02/2022 02:35:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=433
03/02/2022 02:35:44 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.1761904761904762 on epoch=433
03/02/2022 02:35:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=436
03/02/2022 02:35:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
03/02/2022 02:35:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=443
03/02/2022 02:35:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=446
03/02/2022 02:35:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=449
03/02/2022 02:35:56 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.3129605688429218 on epoch=449
03/02/2022 02:35:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
03/02/2022 02:36:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
03/02/2022 02:36:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
03/02/2022 02:36:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
03/02/2022 02:36:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
03/02/2022 02:36:09 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.22618050422928473 on epoch=466
03/02/2022 02:36:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=469
03/02/2022 02:36:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
03/02/2022 02:36:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
03/02/2022 02:36:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
03/02/2022 02:36:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
03/02/2022 02:36:22 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.4721707246735056 on epoch=483
03/02/2022 02:36:22 - INFO - __main__ - Saving model with best Classification-F1: 0.45010683760683756 -> 0.4721707246735056 on epoch=483, global_step=1450
03/02/2022 02:36:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
03/02/2022 02:36:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
03/02/2022 02:36:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
03/02/2022 02:36:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
03/02/2022 02:36:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
03/02/2022 02:36:35 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.31547619047619047 on epoch=499
03/02/2022 02:36:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
03/02/2022 02:36:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
03/02/2022 02:36:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
03/02/2022 02:36:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
03/02/2022 02:36:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
03/02/2022 02:36:47 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.14778635778635776 on epoch=516
03/02/2022 02:36:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
03/02/2022 02:36:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
03/02/2022 02:36:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
03/02/2022 02:36:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
03/02/2022 02:36:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.11 on epoch=533
03/02/2022 02:37:00 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.30023809523809525 on epoch=533
03/02/2022 02:37:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
03/02/2022 02:37:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
03/02/2022 02:37:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
03/02/2022 02:37:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
03/02/2022 02:37:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
03/02/2022 02:37:12 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.2617816965643053 on epoch=549
03/02/2022 02:37:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
03/02/2022 02:37:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
03/02/2022 02:37:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
03/02/2022 02:37:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
03/02/2022 02:37:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
03/02/2022 02:37:25 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.23455211455211455 on epoch=566
03/02/2022 02:37:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
03/02/2022 02:37:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
03/02/2022 02:37:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
03/02/2022 02:37:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=579
03/02/2022 02:37:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
03/02/2022 02:37:38 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.1903194844371315 on epoch=583
03/02/2022 02:37:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 02:37:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
03/02/2022 02:37:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
03/02/2022 02:37:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/02/2022 02:37:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
03/02/2022 02:37:50 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.29504835996635825 on epoch=599
03/02/2022 02:37:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 02:37:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
03/02/2022 02:37:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
03/02/2022 02:38:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
03/02/2022 02:38:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
03/02/2022 02:38:03 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.28498803827751196 on epoch=616
03/02/2022 02:38:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
03/02/2022 02:38:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
03/02/2022 02:38:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
03/02/2022 02:38:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 02:38:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 02:38:16 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.2520454545454546 on epoch=633
03/02/2022 02:38:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
03/02/2022 02:38:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
03/02/2022 02:38:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 02:38:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 02:38:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
03/02/2022 02:38:29 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.40684624017957355 on epoch=649
03/02/2022 02:38:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 02:38:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
03/02/2022 02:38:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
03/02/2022 02:38:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=663
03/02/2022 02:38:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
03/02/2022 02:38:42 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.2794440853264383 on epoch=666
03/02/2022 02:38:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 02:38:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 02:38:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=676
03/02/2022 02:38:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 02:38:53 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 02:38:54 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.39720430107526883 on epoch=683
03/02/2022 02:38:57 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 02:38:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/02/2022 02:39:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 02:39:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/02/2022 02:39:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 02:39:07 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.39371590002372175 on epoch=699
03/02/2022 02:39:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 02:39:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
03/02/2022 02:39:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
03/02/2022 02:39:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 02:39:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 02:39:20 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.39809081527347784 on epoch=716
03/02/2022 02:39:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/02/2022 02:39:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 02:39:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 02:39:29 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 02:39:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 02:39:33 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.41475943777734053 on epoch=733
03/02/2022 02:39:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 02:39:37 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 02:39:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 02:39:42 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 02:39:44 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=749
03/02/2022 02:39:45 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.3438076085134909 on epoch=749
03/02/2022 02:39:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 02:39:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/02/2022 02:39:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 02:39:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 02:39:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 02:39:58 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.24903798011587153 on epoch=766
03/02/2022 02:40:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
03/02/2022 02:40:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 02:40:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 02:40:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 02:40:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 02:40:11 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.3788288288288288 on epoch=783
03/02/2022 02:40:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 02:40:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 02:40:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 02:40:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 02:40:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
03/02/2022 02:40:24 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.33074534161490676 on epoch=799
03/02/2022 02:40:26 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
03/02/2022 02:40:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 02:40:31 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
03/02/2022 02:40:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 02:40:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/02/2022 02:40:36 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.22708517536103742 on epoch=816
03/02/2022 02:40:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 02:40:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 02:40:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 02:40:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 02:40:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 02:40:49 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.37689550592776405 on epoch=833
03/02/2022 02:40:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 02:40:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 02:40:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 02:40:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=846
03/02/2022 02:41:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 02:41:01 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.40051282051282056 on epoch=849
03/02/2022 02:41:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/02/2022 02:41:06 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 02:41:08 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 02:41:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 02:41:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 02:41:14 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.3640515515515516 on epoch=866
03/02/2022 02:41:16 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 02:41:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 02:41:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 02:41:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=879
03/02/2022 02:41:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 02:41:26 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.41315681444991786 on epoch=883
03/02/2022 02:41:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
03/02/2022 02:41:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 02:41:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 02:41:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 02:41:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
03/02/2022 02:41:38 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.4138071895424837 on epoch=899
03/02/2022 02:41:40 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 02:41:42 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 02:41:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 02:41:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 02:41:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 02:41:50 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.40852974186307517 on epoch=916
03/02/2022 02:41:53 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 02:41:55 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 02:41:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 02:41:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 02:42:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=933
03/02/2022 02:42:03 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.40816714150047484 on epoch=933
03/02/2022 02:42:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 02:42:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 02:42:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
03/02/2022 02:42:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=946
03/02/2022 02:42:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 02:42:15 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.2753393665158371 on epoch=949
03/02/2022 02:42:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 02:42:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 02:42:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 02:42:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 02:42:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 02:42:27 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.3626373626373626 on epoch=966
03/02/2022 02:42:30 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 02:42:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 02:42:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=976
03/02/2022 02:42:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
03/02/2022 02:42:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=983
03/02/2022 02:42:40 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.33413124582174203 on epoch=983
03/02/2022 02:42:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 02:42:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 02:42:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 02:42:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 02:42:51 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 02:42:52 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.38193397016926434 on epoch=999
03/02/2022 02:42:52 - INFO - __main__ - save last model!
03/02/2022 02:42:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 02:42:52 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 02:42:52 - INFO - __main__ - Printing 3 examples
03/02/2022 02:42:52 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 02:42:52 - INFO - __main__ - ['normal']
03/02/2022 02:42:52 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 02:42:52 - INFO - __main__ - ['normal']
03/02/2022 02:42:52 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 02:42:52 - INFO - __main__ - ['normal']
03/02/2022 02:42:52 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:42:53 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:42:53 - INFO - __main__ - Printing 3 examples
03/02/2022 02:42:53 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:42:53 - INFO - __main__ - ['offensive']
03/02/2022 02:42:53 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:42:53 - INFO - __main__ - ['offensive']
03/02/2022 02:42:53 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:42:53 - INFO - __main__ - ['offensive']
03/02/2022 02:42:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 02:42:53 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:42:53 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:42:53 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:42:53 - INFO - __main__ - Printing 3 examples
03/02/2022 02:42:53 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:42:53 - INFO - __main__ - ['offensive']
03/02/2022 02:42:53 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:42:53 - INFO - __main__ - ['offensive']
03/02/2022 02:42:53 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:42:53 - INFO - __main__ - ['offensive']
03/02/2022 02:42:53 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:42:53 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:42:53 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:42:53 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:42:55 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 02:43:06 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 02:43:06 - INFO - __main__ - task name: hatexplain
initialize from c4
[(4269, 248302), (4800, 205648), (3565, 292071), (1941, 484885), (1808, 389120), (6473, 163516), (4674, 244421), (2681, 395660), (4879, 229125), (502, 1761063), (3151, 296309), (4225, 197872), (4109, 245907), (4346, 242679), (1663, 588937), (4271, 188919), (441, 2112542), (1048, 702299), (5459, 186372), (4777, 158874), (3616, 283413), (1107, 853041), (4877, 212046), (553, 856375), (5721, 178984), (6061, 205009), (107, 5404644), (1188, 621816), (3563, 229598), (687, 582989), (863, 1077354), (2740, 199631), (2909, 350152), (2766, 278103), (4119, 235907), (828, 1018934), (2652, 286006), (4174, 175228), (4237, 246742), (3347, 297641), (4212, 191151), (6161, 172774), (420, 2224081), (762, 1164855), (2948, 393735), (3214, 325872), (2028, 404951), (990, 919957), (5028, 214929), (1101, 860462), (1116, 852428), (4378, 210800), (1396, 545283), (3305, 313410), (313, 2745242), (541, 1667482), (3566, 290067), (643, 1424557), (3834, 319344), (3759, 261834), (3698, 250226), (3184, 216978), (1566, 615576), (1845, 378247), (5908, 176655), (4501, 228152), (6326, 165064), (603, 506191), (1842, 531497), (776, 518408), (893, 1024942), (6672, 154162), (3677, 244767), (2326, 385363), (5638, 188290), (6193, 160491), (1361, 625400), (2491, 397483), (4794, 202669), (2669, 364149), (2781, 203418), (2488, 362236), (99, 8503604), (5718, 201254), (3231, 325809), (5154, 171205), (1486, 571578), (829, 1087334), (4929, 211594), (3434, 270859), (4305, 159538), (3164, 313795), (4685, 220739), (2507, 369601), (3757, 205092), (2917, 311362), (2124, 458893), (3479, 243656), (5558, 193177)]
03/02/2022 02:43:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:43:06 - INFO - __main__ - Starting training!
03/02/2022 02:43:50 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_21_0.4_8_predictions.txt
03/02/2022 02:43:50 - INFO - __main__ - Classification-F1 on test data: 0.2335
03/02/2022 02:43:51 - INFO - __main__ - prefix=hatexplain_16_21, lr=0.4, bsz=8, dev_performance=0.4721707246735056, test_performance=0.23353892063632098
03/02/2022 02:43:51 - INFO - __main__ - Running ... prefix=hatexplain_16_21, lr=0.3, bsz=8 ...
03/02/2022 02:43:52 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:43:52 - INFO - __main__ - Printing 3 examples
03/02/2022 02:43:52 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:43:52 - INFO - __main__ - ['offensive']
03/02/2022 02:43:52 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:43:52 - INFO - __main__ - ['offensive']
03/02/2022 02:43:52 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:43:52 - INFO - __main__ - ['offensive']
03/02/2022 02:43:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:43:52 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:43:52 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:43:52 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:43:52 - INFO - __main__ - Printing 3 examples
03/02/2022 02:43:52 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:43:52 - INFO - __main__ - ['offensive']
03/02/2022 02:43:52 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:43:52 - INFO - __main__ - ['offensive']
03/02/2022 02:43:52 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:43:52 - INFO - __main__ - ['offensive']
03/02/2022 02:43:52 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:43:52 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:43:52 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:44:06 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 02:44:06 - INFO - __main__ - task name: hatexplain
initialize from c4
[(4269, 248302), (4800, 205648), (3565, 292071), (1941, 484885), (1808, 389120), (6473, 163516), (4674, 244421), (2681, 395660), (4879, 229125), (502, 1761063), (3151, 296309), (4225, 197872), (4109, 245907), (4346, 242679), (1663, 588937), (4271, 188919), (441, 2112542), (1048, 702299), (5459, 186372), (4777, 158874), (3616, 283413), (1107, 853041), (4877, 212046), (553, 856375), (5721, 178984), (6061, 205009), (107, 5404644), (1188, 621816), (3563, 229598), (687, 582989), (863, 1077354), (2740, 199631), (2909, 350152), (2766, 278103), (4119, 235907), (828, 1018934), (2652, 286006), (4174, 175228), (4237, 246742), (3347, 297641), (4212, 191151), (6161, 172774), (420, 2224081), (762, 1164855), (2948, 393735), (3214, 325872), (2028, 404951), (990, 919957), (5028, 214929), (1101, 860462), (1116, 852428), (4378, 210800), (1396, 545283), (3305, 313410), (313, 2745242), (541, 1667482), (3566, 290067), (643, 1424557), (3834, 319344), (3759, 261834), (3698, 250226), (3184, 216978), (1566, 615576), (1845, 378247), (5908, 176655), (4501, 228152), (6326, 165064), (603, 506191), (1842, 531497), (776, 518408), (893, 1024942), (6672, 154162), (3677, 244767), (2326, 385363), (5638, 188290), (6193, 160491), (1361, 625400), (2491, 397483), (4794, 202669), (2669, 364149), (2781, 203418), (2488, 362236), (99, 8503604), (5718, 201254), (3231, 325809), (5154, 171205), (1486, 571578), (829, 1087334), (4929, 211594), (3434, 270859), (4305, 159538), (3164, 313795), (4685, 220739), (2507, 369601), (3757, 205092), (2917, 311362), (2124, 458893), (3479, 243656), (5558, 193177)]
03/02/2022 02:44:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:44:06 - INFO - __main__ - Starting training!
03/02/2022 02:44:09 - INFO - __main__ - Step 10 Global step 10 Train loss 6.88 on epoch=3
03/02/2022 02:44:11 - INFO - __main__ - Step 20 Global step 20 Train loss 4.05 on epoch=6
03/02/2022 02:44:13 - INFO - __main__ - Step 30 Global step 30 Train loss 2.04 on epoch=9
03/02/2022 02:44:15 - INFO - __main__ - Step 40 Global step 40 Train loss 1.09 on epoch=13
03/02/2022 02:44:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.92 on epoch=16
03/02/2022 02:44:21 - INFO - __main__ - Global step 50 Train loss 3.00 Classification-F1 0.19999999999999998 on epoch=16
03/02/2022 02:44:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.19999999999999998 on epoch=16, global_step=50
03/02/2022 02:44:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.73 on epoch=19
03/02/2022 02:44:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.67 on epoch=23
03/02/2022 02:44:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.59 on epoch=26
03/02/2022 02:44:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.61 on epoch=29
03/02/2022 02:44:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=33
03/02/2022 02:44:32 - INFO - __main__ - Global step 100 Train loss 0.63 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 02:44:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.57 on epoch=36
03/02/2022 02:44:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.59 on epoch=39
03/02/2022 02:44:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=43
03/02/2022 02:44:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=46
03/02/2022 02:44:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=49
03/02/2022 02:44:44 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 02:44:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
03/02/2022 02:44:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=56
03/02/2022 02:44:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=59
03/02/2022 02:44:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=63
03/02/2022 02:44:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=66
03/02/2022 02:44:56 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.34848484848484845 on epoch=66
03/02/2022 02:44:56 - INFO - __main__ - Saving model with best Classification-F1: 0.19999999999999998 -> 0.34848484848484845 on epoch=66, global_step=200
03/02/2022 02:44:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=69
03/02/2022 02:45:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=73
03/02/2022 02:45:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
03/02/2022 02:45:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
03/02/2022 02:45:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
03/02/2022 02:45:08 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.40081979212413993 on epoch=83
03/02/2022 02:45:08 - INFO - __main__ - Saving model with best Classification-F1: 0.34848484848484845 -> 0.40081979212413993 on epoch=83, global_step=250
03/02/2022 02:45:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=86
03/02/2022 02:45:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=89
03/02/2022 02:45:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=93
03/02/2022 02:45:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=96
03/02/2022 02:45:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=99
03/02/2022 02:45:20 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.1803703703703704 on epoch=99
03/02/2022 02:45:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=103
03/02/2022 02:45:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
03/02/2022 02:45:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
03/02/2022 02:45:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=113
03/02/2022 02:45:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=116
03/02/2022 02:45:31 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.15873015873015875 on epoch=116
03/02/2022 02:45:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=119
03/02/2022 02:45:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=123
03/02/2022 02:45:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=126
03/02/2022 02:45:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=129
03/02/2022 02:45:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=133
03/02/2022 02:45:43 - INFO - __main__ - Global step 400 Train loss 0.34 Classification-F1 0.1803703703703704 on epoch=133
03/02/2022 02:45:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=136
03/02/2022 02:45:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=139
03/02/2022 02:45:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=143
03/02/2022 02:45:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.34 on epoch=146
03/02/2022 02:45:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=149
03/02/2022 02:45:55 - INFO - __main__ - Global step 450 Train loss 0.36 Classification-F1 0.2901234567901234 on epoch=149
03/02/2022 02:45:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=153
03/02/2022 02:46:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=156
03/02/2022 02:46:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=159
03/02/2022 02:46:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=163
03/02/2022 02:46:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=166
03/02/2022 02:46:07 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.1693121693121693 on epoch=166
03/02/2022 02:46:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
03/02/2022 02:46:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=173
03/02/2022 02:46:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=176
03/02/2022 02:46:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=179
03/02/2022 02:46:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=183
03/02/2022 02:46:19 - INFO - __main__ - Global step 550 Train loss 0.30 Classification-F1 0.15617615467239526 on epoch=183
03/02/2022 02:46:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.31 on epoch=186
03/02/2022 02:46:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.29 on epoch=189
03/02/2022 02:46:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=193
03/02/2022 02:46:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=196
03/02/2022 02:46:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=199
03/02/2022 02:46:31 - INFO - __main__ - Global step 600 Train loss 0.27 Classification-F1 0.17065217391304346 on epoch=199
03/02/2022 02:46:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=203
03/02/2022 02:46:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.26 on epoch=206
03/02/2022 02:46:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=209
03/02/2022 02:46:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=213
03/02/2022 02:46:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=216
03/02/2022 02:46:43 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.20564516129032256 on epoch=216
03/02/2022 02:46:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=219
03/02/2022 02:46:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=223
03/02/2022 02:46:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=226
03/02/2022 02:46:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=229
03/02/2022 02:46:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=233
03/02/2022 02:46:55 - INFO - __main__ - Global step 700 Train loss 0.24 Classification-F1 0.1568627450980392 on epoch=233
03/02/2022 02:46:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=236
03/02/2022 02:46:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=239
03/02/2022 02:47:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=243
03/02/2022 02:47:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=246
03/02/2022 02:47:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=249
03/02/2022 02:47:07 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.167983367983368 on epoch=249
03/02/2022 02:47:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=253
03/02/2022 02:47:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=256
03/02/2022 02:47:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=259
03/02/2022 02:47:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=263
03/02/2022 02:47:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=266
03/02/2022 02:47:19 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.1407925407925408 on epoch=266
03/02/2022 02:47:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=269
03/02/2022 02:47:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=273
03/02/2022 02:47:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=276
03/02/2022 02:47:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.17 on epoch=279
03/02/2022 02:47:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=283
03/02/2022 02:47:31 - INFO - __main__ - Global step 850 Train loss 0.14 Classification-F1 0.10900412796697627 on epoch=283
03/02/2022 02:47:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=286
03/02/2022 02:47:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=289
03/02/2022 02:47:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=293
03/02/2022 02:47:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=296
03/02/2022 02:47:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=299
03/02/2022 02:47:43 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.12833008447043534 on epoch=299
03/02/2022 02:47:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=303
03/02/2022 02:47:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=306
03/02/2022 02:47:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=309
03/02/2022 02:47:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=313
03/02/2022 02:47:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=316
03/02/2022 02:47:55 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.09504357298474947 on epoch=316
03/02/2022 02:47:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=319
03/02/2022 02:47:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=323
03/02/2022 02:48:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=326
03/02/2022 02:48:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=329
03/02/2022 02:48:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=333
03/02/2022 02:48:07 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.1509209744503862 on epoch=333
03/02/2022 02:48:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=336
03/02/2022 02:48:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=339
03/02/2022 02:48:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=343
03/02/2022 02:48:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=346
03/02/2022 02:48:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=349
03/02/2022 02:48:19 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.2181578947368421 on epoch=349
03/02/2022 02:48:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=353
03/02/2022 02:48:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
03/02/2022 02:48:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
03/02/2022 02:48:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=363
03/02/2022 02:48:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
03/02/2022 02:48:31 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.14705882352941177 on epoch=366
03/02/2022 02:48:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
03/02/2022 02:48:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
03/02/2022 02:48:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=376
03/02/2022 02:48:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=379
03/02/2022 02:48:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
03/02/2022 02:48:43 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.10756302521008404 on epoch=383
03/02/2022 02:48:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=386
03/02/2022 02:48:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=389
03/02/2022 02:48:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
03/02/2022 02:48:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
03/02/2022 02:48:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
03/02/2022 02:48:55 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.14444444444444443 on epoch=399
03/02/2022 02:48:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
03/02/2022 02:48:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
03/02/2022 02:49:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
03/02/2022 02:49:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
03/02/2022 02:49:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=416
03/02/2022 02:49:07 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.08973214285714286 on epoch=416
03/02/2022 02:49:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
03/02/2022 02:49:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=423
03/02/2022 02:49:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
03/02/2022 02:49:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=429
03/02/2022 02:49:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
03/02/2022 02:49:19 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.09194591416813638 on epoch=433
03/02/2022 02:49:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
03/02/2022 02:49:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
03/02/2022 02:49:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
03/02/2022 02:49:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=446
03/02/2022 02:49:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
03/02/2022 02:49:31 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.1714685128478232 on epoch=449
03/02/2022 02:49:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=453
03/02/2022 02:49:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
03/02/2022 02:49:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
03/02/2022 02:49:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=463
03/02/2022 02:49:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=466
03/02/2022 02:49:43 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.13131919434440445 on epoch=466
03/02/2022 02:49:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=469
03/02/2022 02:49:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
03/02/2022 02:49:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
03/02/2022 02:49:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=479
03/02/2022 02:49:54 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
03/02/2022 02:49:55 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.13143500643500644 on epoch=483
03/02/2022 02:49:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
03/02/2022 02:50:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
03/02/2022 02:50:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
03/02/2022 02:50:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
03/02/2022 02:50:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
03/02/2022 02:50:07 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.13834586466165413 on epoch=499
03/02/2022 02:50:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
03/02/2022 02:50:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
03/02/2022 02:50:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
03/02/2022 02:50:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
03/02/2022 02:50:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
03/02/2022 02:50:20 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.10619445479218774 on epoch=516
03/02/2022 02:50:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=519
03/02/2022 02:50:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
03/02/2022 02:50:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
03/02/2022 02:50:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=529
03/02/2022 02:50:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
03/02/2022 02:50:32 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.09728535353535354 on epoch=533
03/02/2022 02:50:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=536
03/02/2022 02:50:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
03/02/2022 02:50:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
03/02/2022 02:50:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
03/02/2022 02:50:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=549
03/02/2022 02:50:44 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.11279076848130556 on epoch=549
03/02/2022 02:50:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
03/02/2022 02:50:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
03/02/2022 02:50:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
03/02/2022 02:50:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
03/02/2022 02:50:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
03/02/2022 02:50:56 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.09383894686907021 on epoch=566
03/02/2022 02:50:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
03/02/2022 02:51:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
03/02/2022 02:51:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=576
03/02/2022 02:51:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=579
03/02/2022 02:51:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
03/02/2022 02:51:08 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.1288340336134454 on epoch=583
03/02/2022 02:51:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 02:51:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=589
03/02/2022 02:51:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
03/02/2022 02:51:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/02/2022 02:51:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
03/02/2022 02:51:20 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.14365808823529413 on epoch=599
03/02/2022 02:51:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
03/02/2022 02:51:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 02:51:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
03/02/2022 02:51:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/02/2022 02:51:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
03/02/2022 02:51:33 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.10524600847181491 on epoch=616
03/02/2022 02:51:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
03/02/2022 02:51:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
03/02/2022 02:51:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
03/02/2022 02:51:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=629
03/02/2022 02:51:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 02:51:45 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.07906432748538013 on epoch=633
03/02/2022 02:51:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
03/02/2022 02:51:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
03/02/2022 02:51:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
03/02/2022 02:51:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 02:51:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
03/02/2022 02:51:58 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.1669915529564652 on epoch=649
03/02/2022 02:52:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 02:52:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
03/02/2022 02:52:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
03/02/2022 02:52:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=663
03/02/2022 02:52:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=666
03/02/2022 02:52:10 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.15018724417220658 on epoch=666
03/02/2022 02:52:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 02:52:14 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 02:52:16 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 02:52:19 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 02:52:21 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 02:52:22 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.08646616541353384 on epoch=683
03/02/2022 02:52:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 02:52:27 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
03/02/2022 02:52:29 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 02:52:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 02:52:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=699
03/02/2022 02:52:35 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.07573313782991203 on epoch=699
03/02/2022 02:52:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=703
03/02/2022 02:52:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
03/02/2022 02:52:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
03/02/2022 02:52:43 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 02:52:46 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
03/02/2022 02:52:47 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.13196785066113562 on epoch=716
03/02/2022 02:52:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
03/02/2022 02:52:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 02:52:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
03/02/2022 02:52:56 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 02:52:58 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 02:52:59 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.08432109685979654 on epoch=733
03/02/2022 02:53:02 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
03/02/2022 02:53:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
03/02/2022 02:53:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 02:53:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
03/02/2022 02:53:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
03/02/2022 02:53:12 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.0808080808080808 on epoch=749
03/02/2022 02:53:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 02:53:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/02/2022 02:53:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 02:53:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 02:53:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 02:53:24 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.08112156043190526 on epoch=766
03/02/2022 02:53:26 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
03/02/2022 02:53:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 02:53:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 02:53:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 02:53:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
03/02/2022 02:53:37 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.0833953570795676 on epoch=783
03/02/2022 02:53:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=786
03/02/2022 02:53:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 02:53:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 02:53:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
03/02/2022 02:53:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
03/02/2022 02:53:49 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.13063325563325565 on epoch=799
03/02/2022 02:53:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 02:53:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 02:53:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=809
03/02/2022 02:53:58 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 02:54:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/02/2022 02:54:01 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.08716906117285625 on epoch=816
03/02/2022 02:54:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 02:54:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=823
03/02/2022 02:54:08 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
03/02/2022 02:54:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 02:54:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
03/02/2022 02:54:14 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.10615384615384615 on epoch=833
03/02/2022 02:54:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=836
03/02/2022 02:54:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 02:54:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
03/02/2022 02:54:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=846
03/02/2022 02:54:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
03/02/2022 02:54:26 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.10153256704980843 on epoch=849
03/02/2022 02:54:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
03/02/2022 02:54:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
03/02/2022 02:54:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
03/02/2022 02:54:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 02:54:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
03/02/2022 02:54:39 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.09976833976833976 on epoch=866
03/02/2022 02:54:41 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 02:54:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
03/02/2022 02:54:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
03/02/2022 02:54:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 02:54:50 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 02:54:51 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.09856630824372761 on epoch=883
03/02/2022 02:54:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 02:54:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 02:54:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
03/02/2022 02:55:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 02:55:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 02:55:03 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.15324675324675321 on epoch=899
03/02/2022 02:55:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
03/02/2022 02:55:08 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 02:55:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 02:55:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 02:55:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
03/02/2022 02:55:16 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.10164609053497942 on epoch=916
03/02/2022 02:55:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 02:55:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 02:55:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
03/02/2022 02:55:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 02:55:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
03/02/2022 02:55:28 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.1274799902983265 on epoch=933
03/02/2022 02:55:31 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 02:55:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 02:55:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
03/02/2022 02:55:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 02:55:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 02:55:41 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.1063707385546466 on epoch=949
03/02/2022 02:55:43 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 02:55:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 02:55:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 02:55:50 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 02:55:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
03/02/2022 02:55:53 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.10141739980449659 on epoch=966
03/02/2022 02:55:55 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
03/02/2022 02:55:57 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
03/02/2022 02:55:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 02:56:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
03/02/2022 02:56:04 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=983
03/02/2022 02:56:05 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.1055299539170507 on epoch=983
03/02/2022 02:56:07 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 02:56:09 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=989
03/02/2022 02:56:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
03/02/2022 02:56:13 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 02:56:16 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 02:56:17 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.13492063492063494 on epoch=999
03/02/2022 02:56:17 - INFO - __main__ - save last model!
03/02/2022 02:56:17 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:56:17 - INFO - __main__ - Printing 3 examples
03/02/2022 02:56:17 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:56:17 - INFO - __main__ - ['offensive']
03/02/2022 02:56:17 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:56:17 - INFO - __main__ - ['offensive']
03/02/2022 02:56:17 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:56:17 - INFO - __main__ - ['offensive']
03/02/2022 02:56:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 02:56:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 02:56:17 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:56:17 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 02:56:17 - INFO - __main__ - Printing 3 examples
03/02/2022 02:56:17 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 02:56:17 - INFO - __main__ - ['normal']
03/02/2022 02:56:17 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 02:56:17 - INFO - __main__ - ['normal']
03/02/2022 02:56:17 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 02:56:17 - INFO - __main__ - ['normal']
03/02/2022 02:56:17 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:56:17 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:56:17 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:56:17 - INFO - __main__ - Printing 3 examples
03/02/2022 02:56:17 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:56:17 - INFO - __main__ - ['offensive']
03/02/2022 02:56:17 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:56:17 - INFO - __main__ - ['offensive']
03/02/2022 02:56:17 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:56:17 - INFO - __main__ - ['offensive']
03/02/2022 02:56:17 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:56:17 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:56:17 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:56:18 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:56:19 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 02:56:31 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 02:56:31 - INFO - __main__ - task name: hatexplain
initialize from c4
[(1130, 821238), (2505, 391017), (2260, 245889), (6412, 163495), (4205, 261446), (6219, 162939), (3217, 316619), (3119, 317140), (261, 3686702), (5120, 236973), (2851, 338896), (71, 9677051), (6326, 165064), (301, 1599306), (3882, 155103), (5676, 188429), (3248, 326955), (2134, 401363), (6997, 154928), (4199, 265295), (2620, 417765), (1389, 480591), (2756, 369138), (1502, 652657), (1040, 867513), (308, 1959584), (3116, 304867), (4158, 211917), (383, 2368094), (7228, 161576), (3889, 257596), (1691, 674454), (493, 921866), (2808, 317238), (2338, 287842), (4285, 226300), (1848, 517255), (2051, 455798), (1202, 672342), (5058, 173227), (3721, 256780), (6643, 176713), (4514, 186655), (2736, 340951), (1719, 551197), (2604, 359999), (2280, 193726), (3579, 277730), (1108, 766167), (3114, 200024), (809, 1073970), (1435, 636297), (2460, 406809), (5542, 236029), (6357, 167884), (3985, 256868), (983, 921089), (4083, 237800), (140, 5830325), (3001, 351252), (5545, 174101), (1427, 715870), (3541, 206040), (3701, 254712), (3026, 353651), (6881, 160252), (4294, 245995), (3221, 252473), (1037, 808814), (2657, 395294), (5030, 198426), (226, 3154509), (6398, 166527), (3438, 286245), (6733, 154076), (2351, 414849), (691, 1251430), (5163, 201510), (3068, 311224), (4456, 230159), (1295, 756208), (1024, 639303), (442, 1688276), (418, 2112116), (2777, 308767), (3180, 189715), (1323, 759069), (1015, 867172), (2948, 393735), (5589, 173942), (1303, 658549), (4033, 240888), (3845, 206498), (3783, 253073), (4871, 173063), (4531, 223921), (6297, 159697), (1656, 582760), (4657, 226628)]
03/02/2022 02:56:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:56:32 - INFO - __main__ - Starting training!
03/02/2022 02:57:02 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_21_0.3_8_predictions.txt
03/02/2022 02:57:02 - INFO - __main__ - Classification-F1 on test data: 0.0418
03/02/2022 02:57:02 - INFO - __main__ - prefix=hatexplain_16_21, lr=0.3, bsz=8, dev_performance=0.40081979212413993, test_performance=0.041782113534851825
03/02/2022 02:57:02 - INFO - __main__ - Running ... prefix=hatexplain_16_21, lr=0.2, bsz=8 ...
03/02/2022 02:57:03 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:57:03 - INFO - __main__ - Printing 3 examples
03/02/2022 02:57:03 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:57:03 - INFO - __main__ - ['offensive']
03/02/2022 02:57:03 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:57:03 - INFO - __main__ - ['offensive']
03/02/2022 02:57:03 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:57:03 - INFO - __main__ - ['offensive']
03/02/2022 02:57:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:57:03 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:57:03 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:57:03 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:57:03 - INFO - __main__ - Printing 3 examples
03/02/2022 02:57:03 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:57:03 - INFO - __main__ - ['offensive']
03/02/2022 02:57:03 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:57:03 - INFO - __main__ - ['offensive']
03/02/2022 02:57:03 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:57:03 - INFO - __main__ - ['offensive']
03/02/2022 02:57:03 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:57:03 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:57:03 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:57:15 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 02:57:15 - INFO - __main__ - task name: hatexplain
initialize from c4
[(1130, 821238), (2505, 391017), (2260, 245889), (6412, 163495), (4205, 261446), (6219, 162939), (3217, 316619), (3119, 317140), (261, 3686702), (5120, 236973), (2851, 338896), (71, 9677051), (6326, 165064), (301, 1599306), (3882, 155103), (5676, 188429), (3248, 326955), (2134, 401363), (6997, 154928), (4199, 265295), (2620, 417765), (1389, 480591), (2756, 369138), (1502, 652657), (1040, 867513), (308, 1959584), (3116, 304867), (4158, 211917), (383, 2368094), (7228, 161576), (3889, 257596), (1691, 674454), (493, 921866), (2808, 317238), (2338, 287842), (4285, 226300), (1848, 517255), (2051, 455798), (1202, 672342), (5058, 173227), (3721, 256780), (6643, 176713), (4514, 186655), (2736, 340951), (1719, 551197), (2604, 359999), (2280, 193726), (3579, 277730), (1108, 766167), (3114, 200024), (809, 1073970), (1435, 636297), (2460, 406809), (5542, 236029), (6357, 167884), (3985, 256868), (983, 921089), (4083, 237800), (140, 5830325), (3001, 351252), (5545, 174101), (1427, 715870), (3541, 206040), (3701, 254712), (3026, 353651), (6881, 160252), (4294, 245995), (3221, 252473), (1037, 808814), (2657, 395294), (5030, 198426), (226, 3154509), (6398, 166527), (3438, 286245), (6733, 154076), (2351, 414849), (691, 1251430), (5163, 201510), (3068, 311224), (4456, 230159), (1295, 756208), (1024, 639303), (442, 1688276), (418, 2112116), (2777, 308767), (3180, 189715), (1323, 759069), (1015, 867172), (2948, 393735), (5589, 173942), (1303, 658549), (4033, 240888), (3845, 206498), (3783, 253073), (4871, 173063), (4531, 223921), (6297, 159697), (1656, 582760), (4657, 226628)]
03/02/2022 02:57:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:57:16 - INFO - __main__ - Starting training!
03/02/2022 02:57:19 - INFO - __main__ - Step 10 Global step 10 Train loss 6.83 on epoch=3
03/02/2022 02:57:22 - INFO - __main__ - Step 20 Global step 20 Train loss 4.49 on epoch=6
03/02/2022 02:57:24 - INFO - __main__ - Step 30 Global step 30 Train loss 2.75 on epoch=9
03/02/2022 02:57:26 - INFO - __main__ - Step 40 Global step 40 Train loss 1.63 on epoch=13
03/02/2022 02:57:28 - INFO - __main__ - Step 50 Global step 50 Train loss 1.26 on epoch=16
03/02/2022 02:57:29 - INFO - __main__ - Global step 50 Train loss 3.39 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 02:57:29 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 02:57:31 - INFO - __main__ - Step 60 Global step 60 Train loss 1.09 on epoch=19
03/02/2022 02:57:34 - INFO - __main__ - Step 70 Global step 70 Train loss 0.86 on epoch=23
03/02/2022 02:57:36 - INFO - __main__ - Step 80 Global step 80 Train loss 0.81 on epoch=26
03/02/2022 02:57:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.68 on epoch=29
03/02/2022 02:57:40 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=33
03/02/2022 02:57:41 - INFO - __main__ - Global step 100 Train loss 0.82 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 02:57:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=36
03/02/2022 02:57:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.70 on epoch=39
03/02/2022 02:57:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.59 on epoch=43
03/02/2022 02:57:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=46
03/02/2022 02:57:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=49
03/02/2022 02:57:53 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.32996632996632996 on epoch=49
03/02/2022 02:57:53 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.32996632996632996 on epoch=49, global_step=150
03/02/2022 02:57:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=53
03/02/2022 02:57:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
03/02/2022 02:58:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.55 on epoch=59
03/02/2022 02:58:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=63
03/02/2022 02:58:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=66
03/02/2022 02:58:05 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=66
03/02/2022 02:58:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=69
03/02/2022 02:58:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=73
03/02/2022 02:58:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=76
03/02/2022 02:58:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
03/02/2022 02:58:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=83
03/02/2022 02:58:18 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.28309360016677093 on epoch=83
03/02/2022 02:58:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=86
03/02/2022 02:58:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=89
03/02/2022 02:58:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=93
03/02/2022 02:58:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=96
03/02/2022 02:58:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=99
03/02/2022 02:58:30 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=99
03/02/2022 02:58:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=103
03/02/2022 02:58:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.54 on epoch=106
03/02/2022 02:58:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=109
03/02/2022 02:58:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=113
03/02/2022 02:58:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=116
03/02/2022 02:58:42 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.29259259259259257 on epoch=116
03/02/2022 02:58:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=119
03/02/2022 02:58:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=123
03/02/2022 02:58:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=126
03/02/2022 02:58:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
03/02/2022 02:58:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=133
03/02/2022 02:58:54 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.15873015873015875 on epoch=133
03/02/2022 02:58:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=136
03/02/2022 02:58:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=139
03/02/2022 02:59:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=143
03/02/2022 02:59:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=146
03/02/2022 02:59:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=149
03/02/2022 02:59:06 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.2772404900064474 on epoch=149
03/02/2022 02:59:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=153
03/02/2022 02:59:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=156
03/02/2022 02:59:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=159
03/02/2022 02:59:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=163
03/02/2022 02:59:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=166
03/02/2022 02:59:18 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.18822393822393824 on epoch=166
03/02/2022 02:59:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=169
03/02/2022 02:59:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=173
03/02/2022 02:59:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=176
03/02/2022 02:59:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=179
03/02/2022 02:59:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=183
03/02/2022 02:59:31 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.21364221364221364 on epoch=183
03/02/2022 02:59:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=186
03/02/2022 02:59:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=189
03/02/2022 02:59:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=193
03/02/2022 02:59:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=196
03/02/2022 02:59:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=199
03/02/2022 02:59:43 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.18300653594771243 on epoch=199
03/02/2022 02:59:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=203
03/02/2022 02:59:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=206
03/02/2022 02:59:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=209
03/02/2022 02:59:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=213
03/02/2022 02:59:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=216
03/02/2022 02:59:55 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.145 on epoch=216
03/02/2022 02:59:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.32 on epoch=219
03/02/2022 03:00:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=223
03/02/2022 03:00:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=226
03/02/2022 03:00:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=229
03/02/2022 03:00:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=233
03/02/2022 03:00:07 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.2043506921555702 on epoch=233
03/02/2022 03:00:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=236
03/02/2022 03:00:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=239
03/02/2022 03:00:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=243
03/02/2022 03:00:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=246
03/02/2022 03:00:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=249
03/02/2022 03:00:19 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.21842105263157896 on epoch=249
03/02/2022 03:00:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=253
03/02/2022 03:00:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.27 on epoch=256
03/02/2022 03:00:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=259
03/02/2022 03:00:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=263
03/02/2022 03:00:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=266
03/02/2022 03:00:32 - INFO - __main__ - Global step 800 Train loss 0.30 Classification-F1 0.2448504983388704 on epoch=266
03/02/2022 03:00:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=269
03/02/2022 03:00:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=273
03/02/2022 03:00:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=276
03/02/2022 03:00:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=279
03/02/2022 03:00:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=283
03/02/2022 03:00:44 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.23274853801169587 on epoch=283
03/02/2022 03:00:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=286
03/02/2022 03:00:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=289
03/02/2022 03:00:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=293
03/02/2022 03:00:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=296
03/02/2022 03:00:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=299
03/02/2022 03:00:57 - INFO - __main__ - Global step 900 Train loss 0.24 Classification-F1 0.216998556998557 on epoch=299
03/02/2022 03:00:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=303
03/02/2022 03:01:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=306
03/02/2022 03:01:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=309
03/02/2022 03:01:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=313
03/02/2022 03:01:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=316
03/02/2022 03:01:09 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.19729729729729728 on epoch=316
03/02/2022 03:01:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=319
03/02/2022 03:01:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=323
03/02/2022 03:01:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=326
03/02/2022 03:01:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=329
03/02/2022 03:01:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.12 on epoch=333
03/02/2022 03:01:22 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.41963054866280675 on epoch=333
03/02/2022 03:01:22 - INFO - __main__ - Saving model with best Classification-F1: 0.32996632996632996 -> 0.41963054866280675 on epoch=333, global_step=1000
03/02/2022 03:01:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=336
03/02/2022 03:01:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=339
03/02/2022 03:01:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=343
03/02/2022 03:01:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=346
03/02/2022 03:01:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=349
03/02/2022 03:01:35 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.34342105263157896 on epoch=349
03/02/2022 03:01:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=353
03/02/2022 03:01:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=356
03/02/2022 03:01:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=359
03/02/2022 03:01:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=363
03/02/2022 03:01:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=366
03/02/2022 03:01:47 - INFO - __main__ - Global step 1100 Train loss 0.14 Classification-F1 0.2356125356125356 on epoch=366
03/02/2022 03:01:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=369
03/02/2022 03:01:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=373
03/02/2022 03:01:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=376
03/02/2022 03:01:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.10 on epoch=379
03/02/2022 03:01:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=383
03/02/2022 03:02:00 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.47840755735492574 on epoch=383
03/02/2022 03:02:00 - INFO - __main__ - Saving model with best Classification-F1: 0.41963054866280675 -> 0.47840755735492574 on epoch=383, global_step=1150
03/02/2022 03:02:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=386
03/02/2022 03:02:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=389
03/02/2022 03:02:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=393
03/02/2022 03:02:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=396
03/02/2022 03:02:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=399
03/02/2022 03:02:12 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.2264356580263143 on epoch=399
03/02/2022 03:02:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=403
03/02/2022 03:02:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=406
03/02/2022 03:02:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=409
03/02/2022 03:02:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=413
03/02/2022 03:02:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=416
03/02/2022 03:02:25 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.25746031746031744 on epoch=416
03/02/2022 03:02:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=419
03/02/2022 03:02:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=423
03/02/2022 03:02:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=426
03/02/2022 03:02:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=429
03/02/2022 03:02:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=433
03/02/2022 03:02:37 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.3905919544068211 on epoch=433
03/02/2022 03:02:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=436
03/02/2022 03:02:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=439
03/02/2022 03:02:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=443
03/02/2022 03:02:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=446
03/02/2022 03:02:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=449
03/02/2022 03:02:50 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.3491532976827095 on epoch=449
03/02/2022 03:02:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=453
03/02/2022 03:02:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=456
03/02/2022 03:02:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=459
03/02/2022 03:02:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=463
03/02/2022 03:03:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=466
03/02/2022 03:03:02 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.44102627973595715 on epoch=466
03/02/2022 03:03:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=469
03/02/2022 03:03:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=473
03/02/2022 03:03:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
03/02/2022 03:03:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
03/02/2022 03:03:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
03/02/2022 03:03:15 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.4285714285714286 on epoch=483
03/02/2022 03:03:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=486
03/02/2022 03:03:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
03/02/2022 03:03:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
03/02/2022 03:03:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=496
03/02/2022 03:03:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=499
03/02/2022 03:03:27 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.41038378503007816 on epoch=499
03/02/2022 03:03:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=503
03/02/2022 03:03:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=506
03/02/2022 03:03:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=509
03/02/2022 03:03:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
03/02/2022 03:03:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
03/02/2022 03:03:40 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.27380662609871537 on epoch=516
03/02/2022 03:03:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
03/02/2022 03:03:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
03/02/2022 03:03:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
03/02/2022 03:03:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=529
03/02/2022 03:03:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
03/02/2022 03:03:52 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.3497219132369299 on epoch=533
03/02/2022 03:03:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=536
03/02/2022 03:03:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=539
03/02/2022 03:03:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
03/02/2022 03:04:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=546
03/02/2022 03:04:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
03/02/2022 03:04:05 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.21466666666666664 on epoch=549
03/02/2022 03:04:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
03/02/2022 03:04:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
03/02/2022 03:04:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
03/02/2022 03:04:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
03/02/2022 03:04:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=566
03/02/2022 03:04:17 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.4047619047619048 on epoch=566
03/02/2022 03:04:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=569
03/02/2022 03:04:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
03/02/2022 03:04:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
03/02/2022 03:04:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=579
03/02/2022 03:04:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
03/02/2022 03:04:30 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.3345983554712208 on epoch=583
03/02/2022 03:04:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
03/02/2022 03:04:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/02/2022 03:04:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=593
03/02/2022 03:04:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=596
03/02/2022 03:04:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=599
03/02/2022 03:04:42 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.4217326521924223 on epoch=599
03/02/2022 03:04:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
03/02/2022 03:04:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
03/02/2022 03:04:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=609
03/02/2022 03:04:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=613
03/02/2022 03:04:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=616
03/02/2022 03:04:55 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.39253420908593323 on epoch=616
03/02/2022 03:04:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
03/02/2022 03:05:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
03/02/2022 03:05:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
03/02/2022 03:05:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=629
03/02/2022 03:05:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
03/02/2022 03:05:08 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.3958333333333333 on epoch=633
03/02/2022 03:05:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
03/02/2022 03:05:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
03/02/2022 03:05:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=643
03/02/2022 03:05:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
03/02/2022 03:05:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
03/02/2022 03:05:20 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.40852974186307517 on epoch=649
03/02/2022 03:05:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
03/02/2022 03:05:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
03/02/2022 03:05:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
03/02/2022 03:05:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
03/02/2022 03:05:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
03/02/2022 03:05:33 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.46173308032890575 on epoch=666
03/02/2022 03:05:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
03/02/2022 03:05:37 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=673
03/02/2022 03:05:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
03/02/2022 03:05:42 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
03/02/2022 03:05:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
03/02/2022 03:05:46 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.4025641025641025 on epoch=683
03/02/2022 03:05:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
03/02/2022 03:05:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
03/02/2022 03:05:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
03/02/2022 03:05:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
03/02/2022 03:05:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 03:05:58 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.3819339701692643 on epoch=699
03/02/2022 03:06:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
03/02/2022 03:06:03 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
03/02/2022 03:06:05 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
03/02/2022 03:06:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
03/02/2022 03:06:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
03/02/2022 03:06:11 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.4295608605953434 on epoch=716
03/02/2022 03:06:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
03/02/2022 03:06:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=723
03/02/2022 03:06:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
03/02/2022 03:06:20 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
03/02/2022 03:06:22 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
03/02/2022 03:06:24 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.30534670008354214 on epoch=733
03/02/2022 03:06:26 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 03:06:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 03:06:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=743
03/02/2022 03:06:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=746
03/02/2022 03:06:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
03/02/2022 03:06:36 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.45858585858585865 on epoch=749
03/02/2022 03:06:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 03:06:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
03/02/2022 03:06:43 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=759
03/02/2022 03:06:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
03/02/2022 03:06:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
03/02/2022 03:06:49 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.37213403880070545 on epoch=766
03/02/2022 03:06:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
03/02/2022 03:06:53 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 03:06:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
03/02/2022 03:06:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=779
03/02/2022 03:07:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
03/02/2022 03:07:01 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.4579520697167756 on epoch=783
03/02/2022 03:07:03 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
03/02/2022 03:07:06 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
03/02/2022 03:07:08 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
03/02/2022 03:07:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
03/02/2022 03:07:12 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
03/02/2022 03:07:14 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.43956043956043955 on epoch=799
03/02/2022 03:07:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 03:07:18 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
03/02/2022 03:07:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 03:07:23 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
03/02/2022 03:07:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 03:07:26 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.3864661654135338 on epoch=816
03/02/2022 03:07:29 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 03:07:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 03:07:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 03:07:35 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 03:07:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
03/02/2022 03:07:39 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.3640267778198813 on epoch=833
03/02/2022 03:07:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 03:07:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
03/02/2022 03:07:46 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=843
03/02/2022 03:07:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
03/02/2022 03:07:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=849
03/02/2022 03:07:52 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.3963980576556641 on epoch=849
03/02/2022 03:07:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
03/02/2022 03:07:56 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
03/02/2022 03:07:59 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=859
03/02/2022 03:08:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 03:08:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 03:08:04 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.3935897435897435 on epoch=866
03/02/2022 03:08:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 03:08:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
03/02/2022 03:08:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=876
03/02/2022 03:08:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 03:08:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 03:08:17 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.41120430775603184 on epoch=883
03/02/2022 03:08:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 03:08:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=889
03/02/2022 03:08:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=893
03/02/2022 03:08:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
03/02/2022 03:08:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 03:08:30 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.3864661654135338 on epoch=899
03/02/2022 03:08:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=903
03/02/2022 03:08:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 03:08:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 03:08:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
03/02/2022 03:08:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 03:08:43 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.3880534670008354 on epoch=916
03/02/2022 03:08:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 03:08:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 03:08:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
03/02/2022 03:08:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
03/02/2022 03:08:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=933
03/02/2022 03:08:55 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.4262322472848788 on epoch=933
03/02/2022 03:08:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 03:09:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
03/02/2022 03:09:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=943
03/02/2022 03:09:04 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=946
03/02/2022 03:09:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 03:09:08 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.41469750293279706 on epoch=949
03/02/2022 03:09:10 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
03/02/2022 03:09:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 03:09:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 03:09:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 03:09:19 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 03:09:21 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.4283121597096189 on epoch=966
03/02/2022 03:09:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 03:09:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 03:09:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
03/02/2022 03:09:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 03:09:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
03/02/2022 03:09:33 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.4518518518518519 on epoch=983
03/02/2022 03:09:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 03:09:38 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 03:09:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=993
03/02/2022 03:09:42 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 03:09:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 03:09:46 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:09:46 - INFO - __main__ - Printing 3 examples
03/02/2022 03:09:46 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/02/2022 03:09:46 - INFO - __main__ - ['hatespeech']
03/02/2022 03:09:46 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/02/2022 03:09:46 - INFO - __main__ - ['hatespeech']
03/02/2022 03:09:46 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/02/2022 03:09:46 - INFO - __main__ - ['hatespeech']
03/02/2022 03:09:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 03:09:46 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:09:46 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:09:46 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:09:46 - INFO - __main__ - Printing 3 examples
03/02/2022 03:09:46 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/02/2022 03:09:46 - INFO - __main__ - ['hatespeech']
03/02/2022 03:09:46 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/02/2022 03:09:46 - INFO - __main__ - ['hatespeech']
03/02/2022 03:09:46 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/02/2022 03:09:46 - INFO - __main__ - ['hatespeech']
03/02/2022 03:09:46 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:09:46 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:09:46 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:09:46 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.41258169934640526 on epoch=999
03/02/2022 03:09:46 - INFO - __main__ - save last model!
03/02/2022 03:09:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 03:09:46 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 03:09:46 - INFO - __main__ - Printing 3 examples
03/02/2022 03:09:46 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 03:09:46 - INFO - __main__ - ['normal']
03/02/2022 03:09:46 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 03:09:46 - INFO - __main__ - ['normal']
03/02/2022 03:09:46 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 03:09:46 - INFO - __main__ - ['normal']
03/02/2022 03:09:46 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:09:47 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:09:49 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 03:10:00 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 03:10:00 - INFO - __main__ - task name: hatexplain
initialize from c4
[(1096, 857963), (4738, 190584), (4049, 230165), (1793, 332226), (208, 2410751), (3699, 253359), (2204, 421709), (5094, 210937), (4813, 210454), (1976, 392476), (4110, 259070), (1053, 857202), (4273, 255574), (6171, 171690), (3084, 261725), (661, 1317687), (4284, 235257), (2728, 337315), (2019, 481757), (1308, 726600), (3541, 206040), (977, 908393), (2476, 431307), (4749, 157283), (241, 3838559), (705, 1548305), (3827, 282190), (2182, 379209), (1380, 703116), (2960, 165540), (2003, 445063), (726, 1159344), (5894, 169006), (1673, 440939), (6562, 159935), (1696, 423464), (2541, 395945), (2652, 286006), (1638, 604699), (6523, 157539), (37, 31055239), (2349, 335938), (1592, 619246), (1317, 681720), (4871, 173063), (2756, 369138), (1027, 519620), (1267, 795407), (298, 3090275), (1371, 673811), (168, 5553682), (3165, 262039), (1757, 390014), (7344, 159238), (2158, 290395), (174, 5233753), (927, 520773), (2859, 349629), (342, 1503107), (1275, 695113), (1919, 222828), (5458, 178957), (1128, 773025), (731, 1250799), (4162, 196307), (4849, 209310), (4187, 259804), (6546, 182629), (6114, 158081), (1172, 802296), (3273, 207524), (5776, 185572), (1572, 414616), (499, 1766405), (6166, 180142), (2715, 311751), (1907, 204437), (283, 2553263), (534, 1311231), (5351, 195051), (3431, 233202), (4676, 219209), (1200, 812133), (5216, 190231), (5182, 210925), (663, 1092211), (874, 1011784), (3106, 291674), (4556, 154640), (1161, 600418), (772, 1209132), (1135, 702740), (2306, 338371), (8857, 160161), (3772, 170716), (3627, 289143), (4234, 246090), (7216, 157191), (3465, 176815)]
03/02/2022 03:10:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:10:01 - INFO - __main__ - Starting training!
03/02/2022 03:10:43 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_21_0.2_8_predictions.txt
03/02/2022 03:10:43 - INFO - __main__ - Classification-F1 on test data: 0.4257
03/02/2022 03:10:43 - INFO - __main__ - prefix=hatexplain_16_21, lr=0.2, bsz=8, dev_performance=0.47840755735492574, test_performance=0.425730584563994
03/02/2022 03:10:43 - INFO - __main__ - Running ... prefix=hatexplain_16_42, lr=0.5, bsz=8 ...
03/02/2022 03:10:44 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:10:44 - INFO - __main__ - Printing 3 examples
03/02/2022 03:10:44 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/02/2022 03:10:44 - INFO - __main__ - ['hatespeech']
03/02/2022 03:10:44 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/02/2022 03:10:44 - INFO - __main__ - ['hatespeech']
03/02/2022 03:10:44 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/02/2022 03:10:44 - INFO - __main__ - ['hatespeech']
03/02/2022 03:10:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:10:44 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:10:44 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:10:44 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:10:44 - INFO - __main__ - Printing 3 examples
03/02/2022 03:10:44 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/02/2022 03:10:44 - INFO - __main__ - ['hatespeech']
03/02/2022 03:10:44 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/02/2022 03:10:44 - INFO - __main__ - ['hatespeech']
03/02/2022 03:10:44 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/02/2022 03:10:44 - INFO - __main__ - ['hatespeech']
03/02/2022 03:10:44 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:10:44 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:10:44 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:10:58 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 03:10:58 - INFO - __main__ - task name: hatexplain
initialize from c4
[(1096, 857963), (4738, 190584), (4049, 230165), (1793, 332226), (208, 2410751), (3699, 253359), (2204, 421709), (5094, 210937), (4813, 210454), (1976, 392476), (4110, 259070), (1053, 857202), (4273, 255574), (6171, 171690), (3084, 261725), (661, 1317687), (4284, 235257), (2728, 337315), (2019, 481757), (1308, 726600), (3541, 206040), (977, 908393), (2476, 431307), (4749, 157283), (241, 3838559), (705, 1548305), (3827, 282190), (2182, 379209), (1380, 703116), (2960, 165540), (2003, 445063), (726, 1159344), (5894, 169006), (1673, 440939), (6562, 159935), (1696, 423464), (2541, 395945), (2652, 286006), (1638, 604699), (6523, 157539), (37, 31055239), (2349, 335938), (1592, 619246), (1317, 681720), (4871, 173063), (2756, 369138), (1027, 519620), (1267, 795407), (298, 3090275), (1371, 673811), (168, 5553682), (3165, 262039), (1757, 390014), (7344, 159238), (2158, 290395), (174, 5233753), (927, 520773), (2859, 349629), (342, 1503107), (1275, 695113), (1919, 222828), (5458, 178957), (1128, 773025), (731, 1250799), (4162, 196307), (4849, 209310), (4187, 259804), (6546, 182629), (6114, 158081), (1172, 802296), (3273, 207524), (5776, 185572), (1572, 414616), (499, 1766405), (6166, 180142), (2715, 311751), (1907, 204437), (283, 2553263), (534, 1311231), (5351, 195051), (3431, 233202), (4676, 219209), (1200, 812133), (5216, 190231), (5182, 210925), (663, 1092211), (874, 1011784), (3106, 291674), (4556, 154640), (1161, 600418), (772, 1209132), (1135, 702740), (2306, 338371), (8857, 160161), (3772, 170716), (3627, 289143), (4234, 246090), (7216, 157191), (3465, 176815)]
03/02/2022 03:10:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:10:59 - INFO - __main__ - Starting training!
03/02/2022 03:11:01 - INFO - __main__ - Step 10 Global step 10 Train loss 6.12 on epoch=3
03/02/2022 03:11:03 - INFO - __main__ - Step 20 Global step 20 Train loss 2.05 on epoch=6
03/02/2022 03:11:06 - INFO - __main__ - Step 30 Global step 30 Train loss 0.94 on epoch=9
03/02/2022 03:11:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.80 on epoch=13
03/02/2022 03:11:10 - INFO - __main__ - Step 50 Global step 50 Train loss 0.76 on epoch=16
03/02/2022 03:11:11 - INFO - __main__ - Global step 50 Train loss 2.13 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 03:11:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 03:11:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=19
03/02/2022 03:11:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.63 on epoch=23
03/02/2022 03:11:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=26
03/02/2022 03:11:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=29
03/02/2022 03:11:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=33
03/02/2022 03:11:23 - INFO - __main__ - Global step 100 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 03:11:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=36
03/02/2022 03:11:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=39
03/02/2022 03:11:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=43
03/02/2022 03:11:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.56 on epoch=46
03/02/2022 03:11:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=49
03/02/2022 03:11:35 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 03:11:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
03/02/2022 03:11:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=56
03/02/2022 03:11:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
03/02/2022 03:11:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=63
03/02/2022 03:11:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=66
03/02/2022 03:11:48 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.18888888888888888 on epoch=66
03/02/2022 03:11:48 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.18888888888888888 on epoch=66, global_step=200
03/02/2022 03:11:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=69
03/02/2022 03:11:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=73
03/02/2022 03:11:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.34 on epoch=76
03/02/2022 03:11:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=79
03/02/2022 03:11:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=83
03/02/2022 03:12:00 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.1617383512544803 on epoch=83
03/02/2022 03:12:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=86
03/02/2022 03:12:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.31 on epoch=89
03/02/2022 03:12:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=93
03/02/2022 03:12:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=96
03/02/2022 03:12:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=99
03/02/2022 03:12:13 - INFO - __main__ - Global step 300 Train loss 0.32 Classification-F1 0.1889328063241107 on epoch=99
03/02/2022 03:12:13 - INFO - __main__ - Saving model with best Classification-F1: 0.18888888888888888 -> 0.1889328063241107 on epoch=99, global_step=300
03/02/2022 03:12:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=103
03/02/2022 03:12:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=106
03/02/2022 03:12:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.20 on epoch=109
03/02/2022 03:12:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=113
03/02/2022 03:12:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=116
03/02/2022 03:12:25 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.10265151515151515 on epoch=116
03/02/2022 03:12:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=119
03/02/2022 03:12:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=123
03/02/2022 03:12:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.19 on epoch=126
03/02/2022 03:12:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.16 on epoch=129
03/02/2022 03:12:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.20 on epoch=133
03/02/2022 03:12:38 - INFO - __main__ - Global step 400 Train loss 0.20 Classification-F1 0.12908289866815212 on epoch=133
03/02/2022 03:12:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=136
03/02/2022 03:12:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.14 on epoch=139
03/02/2022 03:12:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.13 on epoch=143
03/02/2022 03:12:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.12 on epoch=146
03/02/2022 03:12:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.17 on epoch=149
03/02/2022 03:12:50 - INFO - __main__ - Global step 450 Train loss 0.14 Classification-F1 0.19517241379310343 on epoch=149
03/02/2022 03:12:50 - INFO - __main__ - Saving model with best Classification-F1: 0.1889328063241107 -> 0.19517241379310343 on epoch=149, global_step=450
03/02/2022 03:12:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.14 on epoch=153
03/02/2022 03:12:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=156
03/02/2022 03:12:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.05 on epoch=159
03/02/2022 03:12:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=163
03/02/2022 03:13:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=166
03/02/2022 03:13:03 - INFO - __main__ - Global step 500 Train loss 0.09 Classification-F1 0.20124340175953076 on epoch=166
03/02/2022 03:13:03 - INFO - __main__ - Saving model with best Classification-F1: 0.19517241379310343 -> 0.20124340175953076 on epoch=166, global_step=500
03/02/2022 03:13:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.06 on epoch=169
03/02/2022 03:13:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=173
03/02/2022 03:13:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.09 on epoch=176
03/02/2022 03:13:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.08 on epoch=179
03/02/2022 03:13:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=183
03/02/2022 03:13:15 - INFO - __main__ - Global step 550 Train loss 0.06 Classification-F1 0.1897297297297297 on epoch=183
03/02/2022 03:13:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=186
03/02/2022 03:13:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=189
03/02/2022 03:13:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.04 on epoch=193
03/02/2022 03:13:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=196
03/02/2022 03:13:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=199
03/02/2022 03:13:28 - INFO - __main__ - Global step 600 Train loss 0.04 Classification-F1 0.14347961406784937 on epoch=199
03/02/2022 03:13:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.08 on epoch=203
03/02/2022 03:13:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=206
03/02/2022 03:13:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=209
03/02/2022 03:13:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=213
03/02/2022 03:13:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=216
03/02/2022 03:13:40 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.2075917686318131 on epoch=216
03/02/2022 03:13:40 - INFO - __main__ - Saving model with best Classification-F1: 0.20124340175953076 -> 0.2075917686318131 on epoch=216, global_step=650
03/02/2022 03:13:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=219
03/02/2022 03:13:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=223
03/02/2022 03:13:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=226
03/02/2022 03:13:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=229
03/02/2022 03:13:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=233
03/02/2022 03:13:53 - INFO - __main__ - Global step 700 Train loss 0.03 Classification-F1 0.1415204678362573 on epoch=233
03/02/2022 03:13:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=236
03/02/2022 03:13:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=239
03/02/2022 03:13:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=243
03/02/2022 03:14:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=246
03/02/2022 03:14:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=249
03/02/2022 03:14:05 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.25602529358626924 on epoch=249
03/02/2022 03:14:05 - INFO - __main__ - Saving model with best Classification-F1: 0.2075917686318131 -> 0.25602529358626924 on epoch=249, global_step=750
03/02/2022 03:14:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=253
03/02/2022 03:14:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=256
03/02/2022 03:14:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=259
03/02/2022 03:14:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=263
03/02/2022 03:14:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=266
03/02/2022 03:14:18 - INFO - __main__ - Global step 800 Train loss 0.03 Classification-F1 0.2212121212121212 on epoch=266
03/02/2022 03:14:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=269
03/02/2022 03:14:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=273
03/02/2022 03:14:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=276
03/02/2022 03:14:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=279
03/02/2022 03:14:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=283
03/02/2022 03:14:30 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.21894771894771894 on epoch=283
03/02/2022 03:14:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=286
03/02/2022 03:14:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=289
03/02/2022 03:14:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=293
03/02/2022 03:14:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=296
03/02/2022 03:14:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=299
03/02/2022 03:14:43 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.26105072463768114 on epoch=299
03/02/2022 03:14:43 - INFO - __main__ - Saving model with best Classification-F1: 0.25602529358626924 -> 0.26105072463768114 on epoch=299, global_step=900
03/02/2022 03:14:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=303
03/02/2022 03:14:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=306
03/02/2022 03:14:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=309
03/02/2022 03:14:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=313
03/02/2022 03:14:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=316
03/02/2022 03:14:55 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.20249946132299074 on epoch=316
03/02/2022 03:14:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=319
03/02/2022 03:15:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=323
03/02/2022 03:15:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=326
03/02/2022 03:15:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=329
03/02/2022 03:15:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=333
03/02/2022 03:15:08 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.2866847826086956 on epoch=333
03/02/2022 03:15:08 - INFO - __main__ - Saving model with best Classification-F1: 0.26105072463768114 -> 0.2866847826086956 on epoch=333, global_step=1000
03/02/2022 03:15:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
03/02/2022 03:15:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=339
03/02/2022 03:15:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=343
03/02/2022 03:15:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=346
03/02/2022 03:15:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=349
03/02/2022 03:15:20 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.19316239316239314 on epoch=349
03/02/2022 03:15:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=353
03/02/2022 03:15:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=356
03/02/2022 03:15:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=359
03/02/2022 03:15:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=363
03/02/2022 03:15:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
03/02/2022 03:15:33 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.21517241379310342 on epoch=366
03/02/2022 03:15:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=369
03/02/2022 03:15:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=373
03/02/2022 03:15:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=376
03/02/2022 03:15:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=379
03/02/2022 03:15:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
03/02/2022 03:15:46 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.3021155830753354 on epoch=383
03/02/2022 03:15:46 - INFO - __main__ - Saving model with best Classification-F1: 0.2866847826086956 -> 0.3021155830753354 on epoch=383, global_step=1150
03/02/2022 03:15:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
03/02/2022 03:15:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=389
03/02/2022 03:15:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=393
03/02/2022 03:15:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=396
03/02/2022 03:15:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=399
03/02/2022 03:15:58 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.24400996096379057 on epoch=399
03/02/2022 03:16:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
03/02/2022 03:16:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=406
03/02/2022 03:16:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=409
03/02/2022 03:16:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
03/02/2022 03:16:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=416
03/02/2022 03:16:11 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.3551587301587302 on epoch=416
03/02/2022 03:16:11 - INFO - __main__ - Saving model with best Classification-F1: 0.3021155830753354 -> 0.3551587301587302 on epoch=416, global_step=1250
03/02/2022 03:16:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=419
03/02/2022 03:16:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=423
03/02/2022 03:16:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=426
03/02/2022 03:16:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
03/02/2022 03:16:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=433
03/02/2022 03:16:23 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.4028799222597672 on epoch=433
03/02/2022 03:16:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3551587301587302 -> 0.4028799222597672 on epoch=433, global_step=1300
03/02/2022 03:16:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
03/02/2022 03:16:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
03/02/2022 03:16:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=443
03/02/2022 03:16:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
03/02/2022 03:16:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
03/02/2022 03:16:36 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.350140056022409 on epoch=449
03/02/2022 03:16:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
03/02/2022 03:16:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=456
03/02/2022 03:16:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
03/02/2022 03:16:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=463
03/02/2022 03:16:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
03/02/2022 03:16:48 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.2625 on epoch=466
03/02/2022 03:16:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
03/02/2022 03:16:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
03/02/2022 03:16:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
03/02/2022 03:16:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
03/02/2022 03:17:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
03/02/2022 03:17:01 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.21539332201471423 on epoch=483
03/02/2022 03:17:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
03/02/2022 03:17:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
03/02/2022 03:17:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
03/02/2022 03:17:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
03/02/2022 03:17:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
03/02/2022 03:17:13 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.27977936042452173 on epoch=499
03/02/2022 03:17:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
03/02/2022 03:17:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
03/02/2022 03:17:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
03/02/2022 03:17:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
03/02/2022 03:17:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
03/02/2022 03:17:26 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.2721638655462185 on epoch=516
03/02/2022 03:17:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
03/02/2022 03:17:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
03/02/2022 03:17:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
03/02/2022 03:17:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=529
03/02/2022 03:17:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
03/02/2022 03:17:38 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.22460606060606061 on epoch=533
03/02/2022 03:17:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
03/02/2022 03:17:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
03/02/2022 03:17:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
03/02/2022 03:17:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
03/02/2022 03:17:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
03/02/2022 03:17:51 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.3259423503325942 on epoch=549
03/02/2022 03:17:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
03/02/2022 03:17:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
03/02/2022 03:17:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
03/02/2022 03:18:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
03/02/2022 03:18:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
03/02/2022 03:18:03 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.24563218390804598 on epoch=566
03/02/2022 03:18:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
03/02/2022 03:18:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
03/02/2022 03:18:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
03/02/2022 03:18:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
03/02/2022 03:18:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
03/02/2022 03:18:16 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.326241134751773 on epoch=583
03/02/2022 03:18:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 03:18:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/02/2022 03:18:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
03/02/2022 03:18:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/02/2022 03:18:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
03/02/2022 03:18:29 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.26704545454545453 on epoch=599
03/02/2022 03:18:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 03:18:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 03:18:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
03/02/2022 03:18:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/02/2022 03:18:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
03/02/2022 03:18:41 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.3926377685968479 on epoch=616
03/02/2022 03:18:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
03/02/2022 03:18:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
03/02/2022 03:18:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
03/02/2022 03:18:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 03:18:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
03/02/2022 03:18:54 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.33274578926752835 on epoch=633
03/02/2022 03:18:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
03/02/2022 03:18:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 03:19:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
03/02/2022 03:19:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 03:19:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
03/02/2022 03:19:06 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.41914831498164834 on epoch=649
03/02/2022 03:19:06 - INFO - __main__ - Saving model with best Classification-F1: 0.4028799222597672 -> 0.41914831498164834 on epoch=649, global_step=1950
03/02/2022 03:19:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
03/02/2022 03:19:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
03/02/2022 03:19:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
03/02/2022 03:19:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
03/02/2022 03:19:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 03:19:19 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.42817059483726144 on epoch=666
03/02/2022 03:19:19 - INFO - __main__ - Saving model with best Classification-F1: 0.41914831498164834 -> 0.42817059483726144 on epoch=666, global_step=2000
03/02/2022 03:19:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 03:19:23 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 03:19:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 03:19:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 03:19:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 03:19:31 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.3370223647529816 on epoch=683
03/02/2022 03:19:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 03:19:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/02/2022 03:19:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=693
03/02/2022 03:19:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 03:19:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 03:19:43 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.294047619047619 on epoch=699
03/02/2022 03:19:45 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 03:19:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 03:19:50 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/02/2022 03:19:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 03:19:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 03:19:55 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.3685714285714286 on epoch=716
03/02/2022 03:19:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/02/2022 03:20:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 03:20:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 03:20:04 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 03:20:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 03:20:08 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.3499999999999999 on epoch=733
03/02/2022 03:20:10 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 03:20:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 03:20:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 03:20:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 03:20:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 03:20:20 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.37322386425834697 on epoch=749
03/02/2022 03:20:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
03/02/2022 03:20:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/02/2022 03:20:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 03:20:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 03:20:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
03/02/2022 03:20:32 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.2745515354211006 on epoch=766
03/02/2022 03:20:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 03:20:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=773
03/02/2022 03:20:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
03/02/2022 03:20:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 03:20:43 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 03:20:44 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.4217406260749914 on epoch=783
03/02/2022 03:20:46 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 03:20:48 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 03:20:51 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 03:20:53 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 03:20:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 03:20:56 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.28547505126452494 on epoch=799
03/02/2022 03:20:58 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 03:21:00 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
03/02/2022 03:21:03 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 03:21:05 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 03:21:07 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 03:21:08 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.4152191894127378 on epoch=816
03/02/2022 03:21:10 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 03:21:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 03:21:15 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 03:21:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 03:21:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 03:21:20 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.30582137161084527 on epoch=833
03/02/2022 03:21:22 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 03:21:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 03:21:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 03:21:29 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 03:21:31 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 03:21:32 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.30582137161084527 on epoch=849
03/02/2022 03:21:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/02/2022 03:21:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 03:21:39 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 03:21:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 03:21:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 03:21:44 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.2785145888594165 on epoch=866
03/02/2022 03:21:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 03:21:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 03:21:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 03:21:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 03:21:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 03:21:57 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.3646255060728745 on epoch=883
03/02/2022 03:21:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 03:22:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 03:22:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
03/02/2022 03:22:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 03:22:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 03:22:09 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.3024839743589744 on epoch=899
03/02/2022 03:22:11 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 03:22:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=906
03/02/2022 03:22:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 03:22:17 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 03:22:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 03:22:21 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.42048748353096177 on epoch=916
03/02/2022 03:22:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 03:22:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 03:22:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 03:22:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 03:22:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 03:22:33 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.43597883597883597 on epoch=933
03/02/2022 03:22:33 - INFO - __main__ - Saving model with best Classification-F1: 0.42817059483726144 -> 0.43597883597883597 on epoch=933, global_step=2800
03/02/2022 03:22:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 03:22:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 03:22:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 03:22:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 03:22:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 03:22:45 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.43507389162561577 on epoch=949
03/02/2022 03:22:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 03:22:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 03:22:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 03:22:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 03:22:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 03:22:58 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.3324218335983042 on epoch=966
03/02/2022 03:23:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 03:23:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 03:23:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 03:23:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 03:23:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=983
03/02/2022 03:23:10 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.3896400767721205 on epoch=983
03/02/2022 03:23:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 03:23:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 03:23:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 03:23:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 03:23:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 03:23:22 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.2758189385607384 on epoch=999
03/02/2022 03:23:22 - INFO - __main__ - save last model!
03/02/2022 03:23:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 03:23:22 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 03:23:22 - INFO - __main__ - Printing 3 examples
03/02/2022 03:23:22 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 03:23:22 - INFO - __main__ - ['normal']
03/02/2022 03:23:22 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 03:23:22 - INFO - __main__ - ['normal']
03/02/2022 03:23:22 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 03:23:22 - INFO - __main__ - ['normal']
03/02/2022 03:23:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:23:23 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:23:23 - INFO - __main__ - Printing 3 examples
03/02/2022 03:23:23 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/02/2022 03:23:23 - INFO - __main__ - ['hatespeech']
03/02/2022 03:23:23 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/02/2022 03:23:23 - INFO - __main__ - ['hatespeech']
03/02/2022 03:23:23 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/02/2022 03:23:23 - INFO - __main__ - ['hatespeech']
03/02/2022 03:23:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 03:23:23 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:23:23 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:23:23 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:23:23 - INFO - __main__ - Printing 3 examples
03/02/2022 03:23:23 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/02/2022 03:23:23 - INFO - __main__ - ['hatespeech']
03/02/2022 03:23:23 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/02/2022 03:23:23 - INFO - __main__ - ['hatespeech']
03/02/2022 03:23:23 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/02/2022 03:23:23 - INFO - __main__ - ['hatespeech']
03/02/2022 03:23:23 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:23:23 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:23:23 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:23:23 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:23:25 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 03:23:36 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 03:23:36 - INFO - __main__ - task name: hatexplain
initialize from c4
[(1813, 317619), (3845, 206498), (5633, 185013), (3553, 155391), (3808, 217101), (1042, 857631), (1341, 767639), (5810, 169570), (2610, 426074), (3083, 217219), (2494, 207618), (2508, 403980), (3973, 227298), (3711, 277625), (5256, 206528), (4394, 234859), (3268, 226138), (1097, 881855), (3594, 300832), (4680, 218826), (2881, 363181), (3159, 250529), (1646, 587152), (5092, 197494), (751, 1191325), (962, 911154), (973, 965632), (1039, 892616), (4898, 216287), (1049, 871051), (3352, 251355), (1488, 681027), (6088, 168191), (389, 1329969), (1584, 159932), (6963, 166421), (3794, 166375), (2534, 284947), (1127, 724151), (4879, 229125), (2961, 264523), (4845, 221351), (596, 1528627), (3231, 325809), (5798, 156482), (2517, 300897), (3509, 266051), (1273, 833860), (5562, 162019), (3835, 184385), (2334, 430503), (1707, 582614), (3965, 193632), (1886, 409261), (1178, 811299), (3776, 266993), (4904, 168205), (4134, 255300), (1260, 767060), (3074, 334210), (7137, 163092), (2383, 406446), (5146, 217691), (5693, 166587), (4819, 168038), (310, 2990534), (6363, 154270), (3011, 346978), (313, 2745242), (1965, 505323), (2983, 340155), (2826, 304796), (3864, 276837), (3312, 266535), (63, 13320350), (1909, 502837), (1591, 624673), (6603, 165014), (4262, 233607), (761, 1146699), (1003, 631823), (331, 2589436), (1246, 907731), (5763, 176930), (2329, 425780), (4295, 249014), (4336, 223332), (3719, 275802), (1249, 573784), (3693, 253676), (3447, 301910), (3566, 290067), (2788, 164681), (6631, 157532), (909, 970948), (592, 1501058), (1645, 581504), (564, 1570044), (885, 1006302)]
03/02/2022 03:23:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:23:36 - INFO - __main__ - Starting training!
03/02/2022 03:24:17 - INFO - __main__ - Saved prediction in models/T5-large/singletask-hatexplain/hatexplain_16_42_0.5_8_predictions.txt
03/02/2022 03:24:17 - INFO - __main__ - Classification-F1 on test data: 0.1832
03/02/2022 03:24:18 - INFO - __main__ - prefix=hatexplain_16_42, lr=0.5, bsz=8, dev_performance=0.43597883597883597, test_performance=0.18316476051162597
03/02/2022 03:24:18 - INFO - __main__ - Running ... prefix=hatexplain_16_42, lr=0.4, bsz=8 ...
03/02/2022 03:24:19 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:24:19 - INFO - __main__ - Printing 3 examples
03/02/2022 03:24:19 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/02/2022 03:24:19 - INFO - __main__ - ['hatespeech']
03/02/2022 03:24:19 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/02/2022 03:24:19 - INFO - __main__ - ['hatespeech']
03/02/2022 03:24:19 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/02/2022 03:24:19 - INFO - __main__ - ['hatespeech']
03/02/2022 03:24:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:24:19 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:24:19 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:24:19 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:24:19 - INFO - __main__ - Printing 3 examples
03/02/2022 03:24:19 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/02/2022 03:24:19 - INFO - __main__ - ['hatespeech']
03/02/2022 03:24:19 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/02/2022 03:24:19 - INFO - __main__ - ['hatespeech']
03/02/2022 03:24:19 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/02/2022 03:24:19 - INFO - __main__ - ['hatespeech']
03/02/2022 03:24:19 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:24:19 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:24:19 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:24:33 - INFO - __main__ - try to initialize prompt embeddings
03/02/2022 03:24:33 - INFO - __main__ - task name: hatexplain
initialize from c4
[(1813, 317619), (3845, 206498), (5633, 185013), (3553, 155391), (3808, 217101), (1042, 857631), (1341, 767639), (5810, 169570), (2610, 426074), (3083, 217219), (2494, 207618), (2508, 403980), (3973, 227298), (3711, 277625), (5256, 206528), (4394, 234859), (3268, 226138), (1097, 881855), (3594, 300832), (4680, 218826), (2881, 363181), (3159, 250529), (1646, 587152), (5092, 197494), (751, 1191325), (962, 911154), (973, 965632), (1039, 892616), (4898, 216287), (1049, 871051), (3352, 251355), (1488, 681027), (6088, 168191), (389, 1329969), (1584, 159932), (6963, 166421), (3794, 166375), (2534, 284947), (1127, 724151), (4879, 229125), (2961, 264523), (4845, 221351), (596, 1528627), (3231, 325809), (5798, 156482), (2517, 300897), (3509, 266051), (1273, 833860), (5562, 162019), (3835, 184385), (2334, 430503), (1707, 582614), (3965, 193632), (1886, 409261), (1178, 811299), (3776, 266993), (4904, 168205), (4134, 255300), (1260, 767060), (3074, 334210), (7137, 163092), (2383, 406446), (5146, 217691), (5693, 166587), (4819, 168038), (310, 2990534), (6363, 154270), (3011, 346978), (313, 2745242), (1965, 505323), (2983, 340155), (2826, 304796), (3864, 276837), (3312, 266535), (63, 13320350), (1909, 502837), (1591, 624673), (6603, 165014), (4262, 233607), (761, 1146699), (1003, 631823), (331, 2589436), (1246, 907731), (5763, 176930), (2329, 425780), (4295, 249014), (4336, 223332), (3719, 275802), (1249, 573784), (3693, 253676), (3447, 301910), (3566, 290067), (2788, 164681), (6631, 157532), (909, 970948), (592, 1501058), (1645, 581504), (564, 1570044), (885, 1006302)]
03/02/2022 03:24:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:24:33 - INFO - __main__ - Starting training!
03/02/2022 03:24:36 - INFO - __main__ - Step 10 Global step 10 Train loss 6.15 on epoch=3
03/02/2022 03:24:38 - INFO - __main__ - Step 20 Global step 20 Train loss 2.49 on epoch=6
03/02/2022 03:24:40 - INFO - __main__ - Step 30 Global step 30 Train loss 1.16 on epoch=9
03/02/2022 03:24:43 - INFO - __main__ - Step 40 Global step 40 Train loss 0.91 on epoch=13
03/02/2022 03:24:45 - INFO - __main__ - Step 50 Global step 50 Train loss 0.74 on epoch=16
03/02/2022 03:24:46 - INFO - __main__ - Global step 50 Train loss 2.29 Classification-F1 0.1193390452876377 on epoch=16
03/02/2022 03:24:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1193390452876377 on epoch=16, global_step=50
03/02/2022 03:24:48 - INFO - __main__ - Step 60 Global step 60 Train loss 0.62 on epoch=19
03/02/2022 03:24:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.71 on epoch=23
03/02/2022 03:24:53 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=26
03/02/2022 03:24:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.63 on epoch=29
03/02/2022 03:24:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.60 on epoch=33
03/02/2022 03:24:58 - INFO - __main__ - Global step 100 Train loss 0.64 Classification-F1 0.15300546448087435 on epoch=33
03/02/2022 03:24:58 - INFO - __main__ - Saving model with best Classification-F1: 0.1193390452876377 -> 0.15300546448087435 on epoch=33, global_step=100
03/02/2022 03:25:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=36
03/02/2022 03:25:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.59 on epoch=39
03/02/2022 03:25:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=43
03/02/2022 03:25:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=46
03/02/2022 03:25:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=49
03/02/2022 03:25:11 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.17714672075726845 on epoch=49
03/02/2022 03:25:11 - INFO - __main__ - Saving model with best Classification-F1: 0.15300546448087435 -> 0.17714672075726845 on epoch=49, global_step=150
03/02/2022 03:25:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
03/02/2022 03:25:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
03/02/2022 03:25:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
03/02/2022 03:25:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=63
03/02/2022 03:25:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=66
03/02/2022 03:25:23 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.19832189168573602 on epoch=66
03/02/2022 03:25:23 - INFO - __main__ - Saving model with best Classification-F1: 0.17714672075726845 -> 0.19832189168573602 on epoch=66, global_step=200
03/02/2022 03:25:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=69
03/02/2022 03:25:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=73
03/02/2022 03:25:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=76
03/02/2022 03:25:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=79
03/02/2022 03:25:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
03/02/2022 03:25:35 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.15300546448087435 on epoch=83
03/02/2022 03:25:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=86
03/02/2022 03:25:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=89
03/02/2022 03:25:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=93
03/02/2022 03:25:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=96
03/02/2022 03:25:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=99
03/02/2022 03:25:48 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.24074074074074073 on epoch=99
03/02/2022 03:25:48 - INFO - __main__ - Saving model with best Classification-F1: 0.19832189168573602 -> 0.24074074074074073 on epoch=99, global_step=300
03/02/2022 03:25:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=103
03/02/2022 03:25:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=106
03/02/2022 03:25:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
03/02/2022 03:25:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=113
03/02/2022 03:25:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=116
03/02/2022 03:26:00 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.2830124957784532 on epoch=116
03/02/2022 03:26:00 - INFO - __main__ - Saving model with best Classification-F1: 0.24074074074074073 -> 0.2830124957784532 on epoch=116, global_step=350
03/02/2022 03:26:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=119
03/02/2022 03:26:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=123
03/02/2022 03:26:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=126
03/02/2022 03:26:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
03/02/2022 03:26:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=133
03/02/2022 03:26:12 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.2088888888888889 on epoch=133
03/02/2022 03:26:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=136
03/02/2022 03:26:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=139
