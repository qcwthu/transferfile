nohup: ignoring input
t5base para reptile downstream
Task: glue-mrpc, Checkpoint: models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt, Identifier: T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10
06/23/2022 15:28:36 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/23/2022 15:28:36 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc
Output directory () already exists and is not empty.
06/23/2022 15:28:36 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/23/2022 15:28:36 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc
06/23/2022 15:28:38 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/23/2022 15:28:38 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/23/2022 15:28:38 - INFO - __main__ - args.device: cuda:0
06/23/2022 15:28:38 - INFO - __main__ - args.device: cuda:1
06/23/2022 15:28:38 - INFO - __main__ - Using 2 gpus
06/23/2022 15:28:38 - INFO - __main__ - Using 2 gpus
06/23/2022 15:28:38 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/23/2022 15:28:38 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/23/2022 15:28:43 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.5, bsz=8 ...
06/23/2022 15:28:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:28:44 - INFO - __main__ - Printing 3 examples
06/23/2022 15:28:44 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/23/2022 15:28:44 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:28:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/23/2022 15:28:44 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:28:44 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/23/2022 15:28:44 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:28:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 15:28:44 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:28:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 15:28:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:28:44 - INFO - __main__ - Printing 3 examples
06/23/2022 15:28:44 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/23/2022 15:28:44 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:28:44 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/23/2022 15:28:44 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:28:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/23/2022 15:28:44 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:28:44 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:28:44 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:28:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:28:44 - INFO - __main__ - Printing 3 examples
06/23/2022 15:28:44 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/23/2022 15:28:44 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 15:28:44 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:28:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/23/2022 15:28:44 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:28:44 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/23/2022 15:28:44 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:28:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 15:28:44 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:28:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 15:28:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:28:44 - INFO - __main__ - Printing 3 examples
06/23/2022 15:28:44 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/23/2022 15:28:44 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:28:44 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/23/2022 15:28:44 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:28:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/23/2022 15:28:44 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:28:44 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:28:44 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:28:44 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 15:28:50 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 15:28:50 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 15:28:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 15:28:50 - INFO - __main__ - Starting training!
06/23/2022 15:28:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 15:28:55 - INFO - __main__ - Starting training!
06/23/2022 15:28:57 - INFO - __main__ - Step 10 Global step 10 Train loss 6.53 on epoch=4
06/23/2022 15:28:59 - INFO - __main__ - Step 20 Global step 20 Train loss 6.53 on epoch=9
06/23/2022 15:29:00 - INFO - __main__ - Step 30 Global step 30 Train loss 6.58 on epoch=14
06/23/2022 15:29:01 - INFO - __main__ - Step 40 Global step 40 Train loss 6.47 on epoch=19
06/23/2022 15:29:02 - INFO - __main__ - Step 50 Global step 50 Train loss 6.45 on epoch=24
06/23/2022 15:29:05 - INFO - __main__ - Global step 50 Train loss 6.51 ACC 0.0 on epoch=24
06/23/2022 15:29:05 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 15:29:07 - INFO - __main__ - Step 60 Global step 60 Train loss 6.49 on epoch=29
06/23/2022 15:29:08 - INFO - __main__ - Step 70 Global step 70 Train loss 6.43 on epoch=34
06/23/2022 15:29:09 - INFO - __main__ - Step 80 Global step 80 Train loss 6.40 on epoch=39
06/23/2022 15:29:10 - INFO - __main__ - Step 90 Global step 90 Train loss 6.34 on epoch=44
06/23/2022 15:29:12 - INFO - __main__ - Step 100 Global step 100 Train loss 6.27 on epoch=49
06/23/2022 15:29:15 - INFO - __main__ - Global step 100 Train loss 6.39 ACC 0.0 on epoch=49
06/23/2022 15:29:17 - INFO - __main__ - Step 110 Global step 110 Train loss 6.19 on epoch=54
06/23/2022 15:29:18 - INFO - __main__ - Step 120 Global step 120 Train loss 6.05 on epoch=59
06/23/2022 15:29:19 - INFO - __main__ - Step 130 Global step 130 Train loss 6.05 on epoch=64
06/23/2022 15:29:20 - INFO - __main__ - Step 140 Global step 140 Train loss 5.93 on epoch=69
06/23/2022 15:29:22 - INFO - __main__ - Step 150 Global step 150 Train loss 5.93 on epoch=74
06/23/2022 15:29:24 - INFO - __main__ - Global step 150 Train loss 6.03 ACC 0.0 on epoch=74
06/23/2022 15:29:25 - INFO - __main__ - Step 160 Global step 160 Train loss 5.86 on epoch=79
06/23/2022 15:29:27 - INFO - __main__ - Step 170 Global step 170 Train loss 5.85 on epoch=84
06/23/2022 15:29:28 - INFO - __main__ - Step 180 Global step 180 Train loss 5.81 on epoch=89
06/23/2022 15:29:29 - INFO - __main__ - Step 190 Global step 190 Train loss 5.77 on epoch=94
06/23/2022 15:29:30 - INFO - __main__ - Step 200 Global step 200 Train loss 5.70 on epoch=99
06/23/2022 15:29:37 - INFO - __main__ - Global step 200 Train loss 5.80 ACC 0.0 on epoch=99
06/23/2022 15:29:38 - INFO - __main__ - Step 210 Global step 210 Train loss 5.75 on epoch=104
06/23/2022 15:29:39 - INFO - __main__ - Step 220 Global step 220 Train loss 5.73 on epoch=109
06/23/2022 15:29:41 - INFO - __main__ - Step 230 Global step 230 Train loss 5.78 on epoch=114
06/23/2022 15:29:42 - INFO - __main__ - Step 240 Global step 240 Train loss 5.71 on epoch=119
06/23/2022 15:29:43 - INFO - __main__ - Step 250 Global step 250 Train loss 5.61 on epoch=124
06/23/2022 15:29:48 - INFO - __main__ - Global step 250 Train loss 5.72 ACC 0.0 on epoch=124
06/23/2022 15:29:50 - INFO - __main__ - Step 260 Global step 260 Train loss 5.52 on epoch=129
06/23/2022 15:29:51 - INFO - __main__ - Step 270 Global step 270 Train loss 5.49 on epoch=134
06/23/2022 15:29:52 - INFO - __main__ - Step 280 Global step 280 Train loss 5.44 on epoch=139
06/23/2022 15:29:53 - INFO - __main__ - Step 290 Global step 290 Train loss 5.43 on epoch=144
06/23/2022 15:29:55 - INFO - __main__ - Step 300 Global step 300 Train loss 5.39 on epoch=149
06/23/2022 15:30:05 - INFO - __main__ - Global step 300 Train loss 5.45 ACC 0.0 on epoch=149
06/23/2022 15:30:06 - INFO - __main__ - Step 310 Global step 310 Train loss 5.48 on epoch=154
06/23/2022 15:30:08 - INFO - __main__ - Step 320 Global step 320 Train loss 5.49 on epoch=159
06/23/2022 15:30:09 - INFO - __main__ - Step 330 Global step 330 Train loss 5.51 on epoch=164
06/23/2022 15:30:10 - INFO - __main__ - Step 340 Global step 340 Train loss 5.48 on epoch=169
06/23/2022 15:30:11 - INFO - __main__ - Step 350 Global step 350 Train loss 5.37 on epoch=174
06/23/2022 15:30:14 - INFO - __main__ - Global step 350 Train loss 5.46 ACC 0.0 on epoch=174
06/23/2022 15:30:16 - INFO - __main__ - Step 360 Global step 360 Train loss 5.32 on epoch=179
06/23/2022 15:30:17 - INFO - __main__ - Step 370 Global step 370 Train loss 5.22 on epoch=184
06/23/2022 15:30:18 - INFO - __main__ - Step 380 Global step 380 Train loss 5.24 on epoch=189
06/23/2022 15:30:19 - INFO - __main__ - Step 390 Global step 390 Train loss 5.26 on epoch=194
06/23/2022 15:30:21 - INFO - __main__ - Step 400 Global step 400 Train loss 5.16 on epoch=199
06/23/2022 15:30:27 - INFO - __main__ - Global step 400 Train loss 5.24 ACC 0.0 on epoch=199
06/23/2022 15:30:28 - INFO - __main__ - Step 410 Global step 410 Train loss 5.10 on epoch=204
06/23/2022 15:30:29 - INFO - __main__ - Step 420 Global step 420 Train loss 4.96 on epoch=209
06/23/2022 15:30:30 - INFO - __main__ - Step 430 Global step 430 Train loss 4.89 on epoch=214
06/23/2022 15:30:32 - INFO - __main__ - Step 440 Global step 440 Train loss 5.08 on epoch=219
06/23/2022 15:30:33 - INFO - __main__ - Step 450 Global step 450 Train loss 4.93 on epoch=224
06/23/2022 15:30:39 - INFO - __main__ - Global step 450 Train loss 4.99 ACC 0.0 on epoch=224
06/23/2022 15:30:41 - INFO - __main__ - Step 460 Global step 460 Train loss 4.89 on epoch=229
06/23/2022 15:30:42 - INFO - __main__ - Step 470 Global step 470 Train loss 4.81 on epoch=234
06/23/2022 15:30:43 - INFO - __main__ - Step 480 Global step 480 Train loss 4.82 on epoch=239
06/23/2022 15:30:44 - INFO - __main__ - Step 490 Global step 490 Train loss 4.75 on epoch=244
06/23/2022 15:30:46 - INFO - __main__ - Step 500 Global step 500 Train loss 4.66 on epoch=249
06/23/2022 15:30:57 - INFO - __main__ - Global step 500 Train loss 4.79 ACC 0.0 on epoch=249
06/23/2022 15:30:58 - INFO - __main__ - Step 510 Global step 510 Train loss 4.59 on epoch=254
06/23/2022 15:31:00 - INFO - __main__ - Step 520 Global step 520 Train loss 4.57 on epoch=259
06/23/2022 15:31:01 - INFO - __main__ - Step 530 Global step 530 Train loss 4.52 on epoch=264
06/23/2022 15:31:02 - INFO - __main__ - Step 540 Global step 540 Train loss 4.41 on epoch=269
06/23/2022 15:31:04 - INFO - __main__ - Step 550 Global step 550 Train loss 4.27 on epoch=274
06/23/2022 15:31:05 - INFO - __main__ - Global step 550 Train loss 4.47 ACC 0.0 on epoch=274
06/23/2022 15:31:06 - INFO - __main__ - Step 560 Global step 560 Train loss 4.27 on epoch=279
06/23/2022 15:31:08 - INFO - __main__ - Step 570 Global step 570 Train loss 4.19 on epoch=284
06/23/2022 15:31:09 - INFO - __main__ - Step 580 Global step 580 Train loss 4.23 on epoch=289
06/23/2022 15:31:10 - INFO - __main__ - Step 590 Global step 590 Train loss 4.02 on epoch=294
06/23/2022 15:31:12 - INFO - __main__ - Step 600 Global step 600 Train loss 3.96 on epoch=299
06/23/2022 15:31:16 - INFO - __main__ - Global step 600 Train loss 4.13 ACC 0.0 on epoch=299
06/23/2022 15:31:17 - INFO - __main__ - Step 610 Global step 610 Train loss 3.83 on epoch=304
06/23/2022 15:31:18 - INFO - __main__ - Step 620 Global step 620 Train loss 3.77 on epoch=309
06/23/2022 15:31:20 - INFO - __main__ - Step 630 Global step 630 Train loss 3.57 on epoch=314
06/23/2022 15:31:21 - INFO - __main__ - Step 640 Global step 640 Train loss 3.51 on epoch=319
06/23/2022 15:31:22 - INFO - __main__ - Step 650 Global step 650 Train loss 3.43 on epoch=324
06/23/2022 15:31:25 - INFO - __main__ - Global step 650 Train loss 3.62 ACC 0.1875 on epoch=324
06/23/2022 15:31:25 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.1875 on epoch=324, global_step=650
06/23/2022 15:31:27 - INFO - __main__ - Step 660 Global step 660 Train loss 3.43 on epoch=329
06/23/2022 15:31:28 - INFO - __main__ - Step 670 Global step 670 Train loss 3.28 on epoch=334
06/23/2022 15:31:29 - INFO - __main__ - Step 680 Global step 680 Train loss 3.26 on epoch=339
06/23/2022 15:31:31 - INFO - __main__ - Step 690 Global step 690 Train loss 3.09 on epoch=344
06/23/2022 15:31:32 - INFO - __main__ - Step 700 Global step 700 Train loss 2.99 on epoch=349
06/23/2022 15:31:35 - INFO - __main__ - Global step 700 Train loss 3.21 ACC 0.5 on epoch=349
06/23/2022 15:31:35 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.5 on epoch=349, global_step=700
06/23/2022 15:31:36 - INFO - __main__ - Step 710 Global step 710 Train loss 2.89 on epoch=354
06/23/2022 15:31:38 - INFO - __main__ - Step 720 Global step 720 Train loss 2.90 on epoch=359
06/23/2022 15:31:39 - INFO - __main__ - Step 730 Global step 730 Train loss 2.76 on epoch=364
06/23/2022 15:31:40 - INFO - __main__ - Step 740 Global step 740 Train loss 2.71 on epoch=369
06/23/2022 15:31:42 - INFO - __main__ - Step 750 Global step 750 Train loss 2.65 on epoch=374
06/23/2022 15:31:44 - INFO - __main__ - Global step 750 Train loss 2.78 ACC 0.5 on epoch=374
06/23/2022 15:31:46 - INFO - __main__ - Step 760 Global step 760 Train loss 2.47 on epoch=379
06/23/2022 15:31:47 - INFO - __main__ - Step 770 Global step 770 Train loss 2.47 on epoch=384
06/23/2022 15:31:48 - INFO - __main__ - Step 780 Global step 780 Train loss 2.30 on epoch=389
06/23/2022 15:31:50 - INFO - __main__ - Step 790 Global step 790 Train loss 2.22 on epoch=394
06/23/2022 15:31:51 - INFO - __main__ - Step 800 Global step 800 Train loss 2.20 on epoch=399
06/23/2022 15:32:01 - INFO - __main__ - Global step 800 Train loss 2.33 ACC 0.46875 on epoch=399
06/23/2022 15:32:02 - INFO - __main__ - Step 810 Global step 810 Train loss 2.16 on epoch=404
06/23/2022 15:32:04 - INFO - __main__ - Step 820 Global step 820 Train loss 1.96 on epoch=409
06/23/2022 15:32:05 - INFO - __main__ - Step 830 Global step 830 Train loss 1.85 on epoch=414
06/23/2022 15:32:07 - INFO - __main__ - Step 840 Global step 840 Train loss 1.84 on epoch=419
06/23/2022 15:32:08 - INFO - __main__ - Step 850 Global step 850 Train loss 1.82 on epoch=424
06/23/2022 15:32:11 - INFO - __main__ - Global step 850 Train loss 1.93 ACC 0.46875 on epoch=424
06/23/2022 15:32:13 - INFO - __main__ - Step 860 Global step 860 Train loss 1.66 on epoch=429
06/23/2022 15:32:14 - INFO - __main__ - Step 870 Global step 870 Train loss 1.60 on epoch=434
06/23/2022 15:32:16 - INFO - __main__ - Step 880 Global step 880 Train loss 1.60 on epoch=439
06/23/2022 15:32:17 - INFO - __main__ - Step 890 Global step 890 Train loss 1.44 on epoch=444
06/23/2022 15:32:19 - INFO - __main__ - Step 900 Global step 900 Train loss 1.43 on epoch=449
06/23/2022 15:32:22 - INFO - __main__ - Global step 900 Train loss 1.55 ACC 0.46875 on epoch=449
06/23/2022 15:32:23 - INFO - __main__ - Step 910 Global step 910 Train loss 1.33 on epoch=454
06/23/2022 15:32:25 - INFO - __main__ - Step 920 Global step 920 Train loss 1.35 on epoch=459
06/23/2022 15:32:26 - INFO - __main__ - Step 930 Global step 930 Train loss 1.31 on epoch=464
06/23/2022 15:32:28 - INFO - __main__ - Step 940 Global step 940 Train loss 1.19 on epoch=469
06/23/2022 15:32:29 - INFO - __main__ - Step 950 Global step 950 Train loss 1.19 on epoch=474
06/23/2022 15:32:35 - INFO - __main__ - Global step 950 Train loss 1.27 ACC 0.40625 on epoch=474
06/23/2022 15:32:36 - INFO - __main__ - Step 960 Global step 960 Train loss 1.16 on epoch=479
06/23/2022 15:32:38 - INFO - __main__ - Step 970 Global step 970 Train loss 1.11 on epoch=484
06/23/2022 15:32:40 - INFO - __main__ - Step 980 Global step 980 Train loss 1.04 on epoch=489
06/23/2022 15:32:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.96 on epoch=494
06/23/2022 15:32:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.93 on epoch=499
06/23/2022 15:32:45 - INFO - __main__ - Global step 1000 Train loss 1.04 ACC 0.4375 on epoch=499
06/23/2022 15:32:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.90 on epoch=504
06/23/2022 15:32:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.85 on epoch=509
06/23/2022 15:32:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.86 on epoch=514
06/23/2022 15:32:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.86 on epoch=519
06/23/2022 15:32:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.83 on epoch=524
06/23/2022 15:32:54 - INFO - __main__ - Global step 1050 Train loss 0.86 ACC 0.5 on epoch=524
06/23/2022 15:32:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.75 on epoch=529
06/23/2022 15:32:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.79 on epoch=534
06/23/2022 15:32:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.77 on epoch=539
06/23/2022 15:33:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.74 on epoch=544
06/23/2022 15:33:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.74 on epoch=549
06/23/2022 15:33:04 - INFO - __main__ - Global step 1100 Train loss 0.76 ACC 0.5625 on epoch=549
06/23/2022 15:33:04 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=549, global_step=1100
06/23/2022 15:33:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.67 on epoch=554
06/23/2022 15:33:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.65 on epoch=559
06/23/2022 15:33:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.68 on epoch=564
06/23/2022 15:33:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.65 on epoch=569
06/23/2022 15:33:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.66 on epoch=574
06/23/2022 15:33:12 - INFO - __main__ - Global step 1150 Train loss 0.66 ACC 0.5 on epoch=574
06/23/2022 15:33:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.55 on epoch=579
06/23/2022 15:33:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.59 on epoch=584
06/23/2022 15:33:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.61 on epoch=589
06/23/2022 15:33:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.61 on epoch=594
06/23/2022 15:33:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.54 on epoch=599
06/23/2022 15:33:21 - INFO - __main__ - Global step 1200 Train loss 0.58 ACC 0.40625 on epoch=599
06/23/2022 15:33:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.58 on epoch=604
06/23/2022 15:33:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
06/23/2022 15:33:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=614
06/23/2022 15:33:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.60 on epoch=619
06/23/2022 15:33:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.60 on epoch=624
06/23/2022 15:33:30 - INFO - __main__ - Global step 1250 Train loss 0.56 ACC 0.5 on epoch=624
06/23/2022 15:33:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=629
06/23/2022 15:33:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=634
06/23/2022 15:33:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
06/23/2022 15:33:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.49 on epoch=644
06/23/2022 15:33:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=649
06/23/2022 15:33:39 - INFO - __main__ - Global step 1300 Train loss 0.48 ACC 0.375 on epoch=649
06/23/2022 15:33:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=654
06/23/2022 15:33:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.53 on epoch=659
06/23/2022 15:33:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.52 on epoch=664
06/23/2022 15:33:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=669
06/23/2022 15:33:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=674
06/23/2022 15:33:48 - INFO - __main__ - Global step 1350 Train loss 0.49 ACC 0.5 on epoch=674
06/23/2022 15:33:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=679
06/23/2022 15:33:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=684
06/23/2022 15:33:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=689
06/23/2022 15:33:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=694
06/23/2022 15:33:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=699
06/23/2022 15:33:57 - INFO - __main__ - Global step 1400 Train loss 0.46 ACC 0.375 on epoch=699
06/23/2022 15:33:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=704
06/23/2022 15:34:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=709
06/23/2022 15:34:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=714
06/23/2022 15:34:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=719
06/23/2022 15:34:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=724
06/23/2022 15:34:06 - INFO - __main__ - Global step 1450 Train loss 0.41 ACC 0.46875 on epoch=724
06/23/2022 15:34:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=729
06/23/2022 15:34:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.47 on epoch=734
06/23/2022 15:34:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=739
06/23/2022 15:34:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=744
06/23/2022 15:34:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=749
06/23/2022 15:34:13 - INFO - __main__ - Global step 1500 Train loss 0.41 ACC 0.53125 on epoch=749
06/23/2022 15:34:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=754
06/23/2022 15:34:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=759
06/23/2022 15:34:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=764
06/23/2022 15:34:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.39 on epoch=769
06/23/2022 15:34:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=774
06/23/2022 15:34:21 - INFO - __main__ - Global step 1550 Train loss 0.40 ACC 0.25 on epoch=774
06/23/2022 15:34:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=779
06/23/2022 15:34:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=784
06/23/2022 15:34:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=789
06/23/2022 15:34:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=794
06/23/2022 15:34:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=799
06/23/2022 15:34:28 - INFO - __main__ - Global step 1600 Train loss 0.41 ACC 0.46875 on epoch=799
06/23/2022 15:34:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=804
06/23/2022 15:34:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=809
06/23/2022 15:34:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.48 on epoch=814
06/23/2022 15:34:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=819
06/23/2022 15:34:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=824
06/23/2022 15:34:36 - INFO - __main__ - Global step 1650 Train loss 0.44 ACC 0.5 on epoch=824
06/23/2022 15:34:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=829
06/23/2022 15:34:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=834
06/23/2022 15:34:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=839
06/23/2022 15:34:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=844
06/23/2022 15:34:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.35 on epoch=849
06/23/2022 15:34:44 - INFO - __main__ - Global step 1700 Train loss 0.40 ACC 0.5 on epoch=849
06/23/2022 15:34:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
06/23/2022 15:34:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=859
06/23/2022 15:34:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=864
06/23/2022 15:34:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=869
06/23/2022 15:34:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=874
06/23/2022 15:34:52 - INFO - __main__ - Global step 1750 Train loss 0.38 ACC 0.5 on epoch=874
06/23/2022 15:34:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=879
06/23/2022 15:34:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
06/23/2022 15:34:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=889
06/23/2022 15:34:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=894
06/23/2022 15:34:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
06/23/2022 15:34:59 - INFO - __main__ - Global step 1800 Train loss 0.38 ACC 0.5 on epoch=899
06/23/2022 15:35:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=904
06/23/2022 15:35:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=909
06/23/2022 15:35:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
06/23/2022 15:35:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.33 on epoch=919
06/23/2022 15:35:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
06/23/2022 15:35:07 - INFO - __main__ - Global step 1850 Train loss 0.35 ACC 0.4375 on epoch=924
06/23/2022 15:35:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
06/23/2022 15:35:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=934
06/23/2022 15:35:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=939
06/23/2022 15:35:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
06/23/2022 15:35:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
06/23/2022 15:35:14 - INFO - __main__ - Global step 1900 Train loss 0.36 ACC 0.59375 on epoch=949
06/23/2022 15:35:14 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=949, global_step=1900
06/23/2022 15:35:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=954
06/23/2022 15:35:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
06/23/2022 15:35:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=964
06/23/2022 15:35:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=969
06/23/2022 15:35:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=974
06/23/2022 15:35:22 - INFO - __main__ - Global step 1950 Train loss 0.34 ACC 0.4375 on epoch=974
06/23/2022 15:35:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=979
06/23/2022 15:35:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=984
06/23/2022 15:35:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.35 on epoch=989
06/23/2022 15:35:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=994
06/23/2022 15:35:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=999
06/23/2022 15:35:30 - INFO - __main__ - Global step 2000 Train loss 0.35 ACC 0.4375 on epoch=999
06/23/2022 15:35:30 - INFO - __main__ - save last model!
06/23/2022 15:35:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 15:35:30 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 15:35:30 - INFO - __main__ - Printing 3 examples
06/23/2022 15:35:30 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 15:35:30 - INFO - __main__ - ['equivalent']
06/23/2022 15:35:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 15:35:30 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:30 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 15:35:30 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:30 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:35:30 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:35:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:35:30 - INFO - __main__ - Printing 3 examples
06/23/2022 15:35:30 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/23/2022 15:35:30 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/23/2022 15:35:30 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:30 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/23/2022 15:35:30 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 15:35:30 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:35:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 15:35:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:35:30 - INFO - __main__ - Printing 3 examples
06/23/2022 15:35:30 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/23/2022 15:35:30 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:30 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/23/2022 15:35:30 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/23/2022 15:35:30 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:30 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:35:30 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:35:31 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 15:35:31 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 15:35:37 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 15:35:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 15:35:38 - INFO - __main__ - Starting training!
06/23/2022 15:35:39 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_100_0.5_8_predictions.txt
06/23/2022 15:35:39 - INFO - __main__ - ACC on test data: 0.3382
06/23/2022 15:35:40 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.5, bsz=8, dev_performance=0.59375, test_performance=0.3382352941176471
06/23/2022 15:35:40 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.4, bsz=8 ...
06/23/2022 15:35:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:35:40 - INFO - __main__ - Printing 3 examples
06/23/2022 15:35:40 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/23/2022 15:35:40 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:40 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/23/2022 15:35:40 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:40 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/23/2022 15:35:40 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 15:35:41 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:35:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 15:35:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:35:41 - INFO - __main__ - Printing 3 examples
06/23/2022 15:35:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/23/2022 15:35:41 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:41 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/23/2022 15:35:41 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/23/2022 15:35:41 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:35:41 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:35:41 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:35:41 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 15:35:48 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 15:35:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 15:35:48 - INFO - __main__ - Starting training!
06/23/2022 15:35:50 - INFO - __main__ - Step 10 Global step 10 Train loss 6.50 on epoch=4
06/23/2022 15:35:51 - INFO - __main__ - Step 20 Global step 20 Train loss 6.55 on epoch=9
06/23/2022 15:35:52 - INFO - __main__ - Step 30 Global step 30 Train loss 6.53 on epoch=14
06/23/2022 15:35:54 - INFO - __main__ - Step 40 Global step 40 Train loss 6.52 on epoch=19
06/23/2022 15:35:55 - INFO - __main__ - Step 50 Global step 50 Train loss 6.47 on epoch=24
06/23/2022 15:36:00 - INFO - __main__ - Global step 50 Train loss 6.51 ACC 0.0 on epoch=24
06/23/2022 15:36:00 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 15:36:01 - INFO - __main__ - Step 60 Global step 60 Train loss 6.50 on epoch=29
06/23/2022 15:36:03 - INFO - __main__ - Step 70 Global step 70 Train loss 6.41 on epoch=34
06/23/2022 15:36:04 - INFO - __main__ - Step 80 Global step 80 Train loss 6.38 on epoch=39
06/23/2022 15:36:05 - INFO - __main__ - Step 90 Global step 90 Train loss 6.37 on epoch=44
06/23/2022 15:36:07 - INFO - __main__ - Step 100 Global step 100 Train loss 6.32 on epoch=49
06/23/2022 15:36:10 - INFO - __main__ - Global step 100 Train loss 6.40 ACC 0.0 on epoch=49
06/23/2022 15:36:12 - INFO - __main__ - Step 110 Global step 110 Train loss 6.19 on epoch=54
06/23/2022 15:36:13 - INFO - __main__ - Step 120 Global step 120 Train loss 6.16 on epoch=59
06/23/2022 15:36:15 - INFO - __main__ - Step 130 Global step 130 Train loss 6.18 on epoch=64
06/23/2022 15:36:16 - INFO - __main__ - Step 140 Global step 140 Train loss 6.09 on epoch=69
06/23/2022 15:36:18 - INFO - __main__ - Step 150 Global step 150 Train loss 6.06 on epoch=74
06/23/2022 15:36:25 - INFO - __main__ - Global step 150 Train loss 6.14 ACC 0.0 on epoch=74
06/23/2022 15:36:26 - INFO - __main__ - Step 160 Global step 160 Train loss 6.02 on epoch=79
06/23/2022 15:36:27 - INFO - __main__ - Step 170 Global step 170 Train loss 6.09 on epoch=84
06/23/2022 15:36:29 - INFO - __main__ - Step 180 Global step 180 Train loss 5.93 on epoch=89
06/23/2022 15:36:30 - INFO - __main__ - Step 190 Global step 190 Train loss 5.92 on epoch=94
06/23/2022 15:36:32 - INFO - __main__ - Step 200 Global step 200 Train loss 5.84 on epoch=99
06/23/2022 15:36:40 - INFO - __main__ - Global step 200 Train loss 5.96 ACC 0.0 on epoch=99
06/23/2022 15:36:41 - INFO - __main__ - Step 210 Global step 210 Train loss 5.75 on epoch=104
06/23/2022 15:36:43 - INFO - __main__ - Step 220 Global step 220 Train loss 5.74 on epoch=109
06/23/2022 15:36:44 - INFO - __main__ - Step 230 Global step 230 Train loss 5.83 on epoch=114
06/23/2022 15:36:46 - INFO - __main__ - Step 240 Global step 240 Train loss 5.75 on epoch=119
06/23/2022 15:36:47 - INFO - __main__ - Step 250 Global step 250 Train loss 5.71 on epoch=124
06/23/2022 15:36:55 - INFO - __main__ - Global step 250 Train loss 5.76 ACC 0.0 on epoch=124
06/23/2022 15:36:56 - INFO - __main__ - Step 260 Global step 260 Train loss 5.63 on epoch=129
06/23/2022 15:36:57 - INFO - __main__ - Step 270 Global step 270 Train loss 5.55 on epoch=134
06/23/2022 15:36:59 - INFO - __main__ - Step 280 Global step 280 Train loss 5.64 on epoch=139
06/23/2022 15:37:00 - INFO - __main__ - Step 290 Global step 290 Train loss 5.48 on epoch=144
06/23/2022 15:37:02 - INFO - __main__ - Step 300 Global step 300 Train loss 5.44 on epoch=149
06/23/2022 15:37:03 - INFO - __main__ - Global step 300 Train loss 5.55 ACC 0.0 on epoch=149
06/23/2022 15:37:05 - INFO - __main__ - Step 310 Global step 310 Train loss 5.44 on epoch=154
06/23/2022 15:37:06 - INFO - __main__ - Step 320 Global step 320 Train loss 5.38 on epoch=159
06/23/2022 15:37:07 - INFO - __main__ - Step 330 Global step 330 Train loss 5.34 on epoch=164
06/23/2022 15:37:09 - INFO - __main__ - Step 340 Global step 340 Train loss 5.27 on epoch=169
06/23/2022 15:37:11 - INFO - __main__ - Step 350 Global step 350 Train loss 5.18 on epoch=174
06/23/2022 15:37:13 - INFO - __main__ - Global step 350 Train loss 5.32 ACC 0.0 on epoch=174
06/23/2022 15:37:14 - INFO - __main__ - Step 360 Global step 360 Train loss 5.11 on epoch=179
06/23/2022 15:37:16 - INFO - __main__ - Step 370 Global step 370 Train loss 5.07 on epoch=184
06/23/2022 15:37:17 - INFO - __main__ - Step 380 Global step 380 Train loss 5.06 on epoch=189
06/23/2022 15:37:18 - INFO - __main__ - Step 390 Global step 390 Train loss 5.09 on epoch=194
06/23/2022 15:37:20 - INFO - __main__ - Step 400 Global step 400 Train loss 5.02 on epoch=199
06/23/2022 15:37:21 - INFO - __main__ - Global step 400 Train loss 5.07 ACC 0.0 on epoch=199
06/23/2022 15:37:23 - INFO - __main__ - Step 410 Global step 410 Train loss 4.87 on epoch=204
06/23/2022 15:37:24 - INFO - __main__ - Step 420 Global step 420 Train loss 4.97 on epoch=209
06/23/2022 15:37:25 - INFO - __main__ - Step 430 Global step 430 Train loss 4.81 on epoch=214
06/23/2022 15:37:27 - INFO - __main__ - Step 440 Global step 440 Train loss 4.75 on epoch=219
06/23/2022 15:37:28 - INFO - __main__ - Step 450 Global step 450 Train loss 4.75 on epoch=224
06/23/2022 15:37:30 - INFO - __main__ - Global step 450 Train loss 4.83 ACC 0.0 on epoch=224
06/23/2022 15:37:32 - INFO - __main__ - Step 460 Global step 460 Train loss 4.81 on epoch=229
06/23/2022 15:37:33 - INFO - __main__ - Step 470 Global step 470 Train loss 4.64 on epoch=234
06/23/2022 15:37:35 - INFO - __main__ - Step 480 Global step 480 Train loss 4.57 on epoch=239
06/23/2022 15:37:37 - INFO - __main__ - Step 490 Global step 490 Train loss 4.56 on epoch=244
06/23/2022 15:37:39 - INFO - __main__ - Step 500 Global step 500 Train loss 4.48 on epoch=249
06/23/2022 15:37:40 - INFO - __main__ - Global step 500 Train loss 4.61 ACC 0.0 on epoch=249
06/23/2022 15:37:41 - INFO - __main__ - Step 510 Global step 510 Train loss 4.55 on epoch=254
06/23/2022 15:37:43 - INFO - __main__ - Step 520 Global step 520 Train loss 4.45 on epoch=259
06/23/2022 15:37:44 - INFO - __main__ - Step 530 Global step 530 Train loss 4.32 on epoch=264
06/23/2022 15:37:46 - INFO - __main__ - Step 540 Global step 540 Train loss 4.30 on epoch=269
06/23/2022 15:37:47 - INFO - __main__ - Step 550 Global step 550 Train loss 4.28 on epoch=274
06/23/2022 15:37:48 - INFO - __main__ - Global step 550 Train loss 4.38 ACC 0.0 on epoch=274
06/23/2022 15:37:50 - INFO - __main__ - Step 560 Global step 560 Train loss 4.25 on epoch=279
06/23/2022 15:37:51 - INFO - __main__ - Step 570 Global step 570 Train loss 4.09 on epoch=284
06/23/2022 15:37:52 - INFO - __main__ - Step 580 Global step 580 Train loss 4.14 on epoch=289
06/23/2022 15:37:54 - INFO - __main__ - Step 590 Global step 590 Train loss 3.95 on epoch=294
06/23/2022 15:37:55 - INFO - __main__ - Step 600 Global step 600 Train loss 3.86 on epoch=299
06/23/2022 15:37:57 - INFO - __main__ - Global step 600 Train loss 4.06 ACC 0.0 on epoch=299
06/23/2022 15:37:58 - INFO - __main__ - Step 610 Global step 610 Train loss 3.86 on epoch=304
06/23/2022 15:38:00 - INFO - __main__ - Step 620 Global step 620 Train loss 3.77 on epoch=309
06/23/2022 15:38:01 - INFO - __main__ - Step 630 Global step 630 Train loss 3.69 on epoch=314
06/23/2022 15:38:02 - INFO - __main__ - Step 640 Global step 640 Train loss 3.61 on epoch=319
06/23/2022 15:38:04 - INFO - __main__ - Step 650 Global step 650 Train loss 3.49 on epoch=324
06/23/2022 15:38:06 - INFO - __main__ - Global step 650 Train loss 3.68 ACC 0.0 on epoch=324
06/23/2022 15:38:08 - INFO - __main__ - Step 660 Global step 660 Train loss 3.48 on epoch=329
06/23/2022 15:38:09 - INFO - __main__ - Step 670 Global step 670 Train loss 3.48 on epoch=334
06/23/2022 15:38:10 - INFO - __main__ - Step 680 Global step 680 Train loss 3.33 on epoch=339
06/23/2022 15:38:12 - INFO - __main__ - Step 690 Global step 690 Train loss 3.23 on epoch=344
06/23/2022 15:38:13 - INFO - __main__ - Step 700 Global step 700 Train loss 3.19 on epoch=349
06/23/2022 15:38:16 - INFO - __main__ - Global step 700 Train loss 3.34 ACC 0.0 on epoch=349
06/23/2022 15:38:17 - INFO - __main__ - Step 710 Global step 710 Train loss 3.15 on epoch=354
06/23/2022 15:38:19 - INFO - __main__ - Step 720 Global step 720 Train loss 2.98 on epoch=359
06/23/2022 15:38:20 - INFO - __main__ - Step 730 Global step 730 Train loss 3.02 on epoch=364
06/23/2022 15:38:22 - INFO - __main__ - Step 740 Global step 740 Train loss 2.96 on epoch=369
06/23/2022 15:38:23 - INFO - __main__ - Step 750 Global step 750 Train loss 2.93 on epoch=374
06/23/2022 15:38:26 - INFO - __main__ - Global step 750 Train loss 3.01 ACC 0.03125 on epoch=374
06/23/2022 15:38:26 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=374, global_step=750
06/23/2022 15:38:28 - INFO - __main__ - Step 760 Global step 760 Train loss 2.77 on epoch=379
06/23/2022 15:38:29 - INFO - __main__ - Step 770 Global step 770 Train loss 2.76 on epoch=384
06/23/2022 15:38:31 - INFO - __main__ - Step 780 Global step 780 Train loss 2.64 on epoch=389
06/23/2022 15:38:32 - INFO - __main__ - Step 790 Global step 790 Train loss 2.67 on epoch=394
06/23/2022 15:38:34 - INFO - __main__ - Step 800 Global step 800 Train loss 2.55 on epoch=399
06/23/2022 15:38:41 - INFO - __main__ - Global step 800 Train loss 2.68 ACC 0.25 on epoch=399
06/23/2022 15:38:41 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.25 on epoch=399, global_step=800
06/23/2022 15:38:42 - INFO - __main__ - Step 810 Global step 810 Train loss 2.46 on epoch=404
06/23/2022 15:38:44 - INFO - __main__ - Step 820 Global step 820 Train loss 2.47 on epoch=409
06/23/2022 15:38:45 - INFO - __main__ - Step 830 Global step 830 Train loss 2.31 on epoch=414
06/23/2022 15:38:47 - INFO - __main__ - Step 840 Global step 840 Train loss 2.40 on epoch=419
06/23/2022 15:38:48 - INFO - __main__ - Step 850 Global step 850 Train loss 2.28 on epoch=424
06/23/2022 15:38:50 - INFO - __main__ - Global step 850 Train loss 2.38 ACC 0.40625 on epoch=424
06/23/2022 15:38:50 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.40625 on epoch=424, global_step=850
06/23/2022 15:38:52 - INFO - __main__ - Step 860 Global step 860 Train loss 2.16 on epoch=429
06/23/2022 15:38:53 - INFO - __main__ - Step 870 Global step 870 Train loss 2.18 on epoch=434
06/23/2022 15:38:55 - INFO - __main__ - Step 880 Global step 880 Train loss 2.12 on epoch=439
06/23/2022 15:38:56 - INFO - __main__ - Step 890 Global step 890 Train loss 2.06 on epoch=444
06/23/2022 15:38:57 - INFO - __main__ - Step 900 Global step 900 Train loss 1.99 on epoch=449
06/23/2022 15:38:59 - INFO - __main__ - Global step 900 Train loss 2.10 ACC 0.5625 on epoch=449
06/23/2022 15:38:59 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.5625 on epoch=449, global_step=900
06/23/2022 15:39:00 - INFO - __main__ - Step 910 Global step 910 Train loss 1.84 on epoch=454
06/23/2022 15:39:01 - INFO - __main__ - Step 920 Global step 920 Train loss 1.91 on epoch=459
06/23/2022 15:39:03 - INFO - __main__ - Step 930 Global step 930 Train loss 1.90 on epoch=464
06/23/2022 15:39:04 - INFO - __main__ - Step 940 Global step 940 Train loss 1.80 on epoch=469
06/23/2022 15:39:05 - INFO - __main__ - Step 950 Global step 950 Train loss 1.72 on epoch=474
06/23/2022 15:39:06 - INFO - __main__ - Global step 950 Train loss 1.84 ACC 0.46875 on epoch=474
06/23/2022 15:39:08 - INFO - __main__ - Step 960 Global step 960 Train loss 1.64 on epoch=479
06/23/2022 15:39:09 - INFO - __main__ - Step 970 Global step 970 Train loss 1.61 on epoch=484
06/23/2022 15:39:11 - INFO - __main__ - Step 980 Global step 980 Train loss 1.49 on epoch=489
06/23/2022 15:39:12 - INFO - __main__ - Step 990 Global step 990 Train loss 1.56 on epoch=494
06/23/2022 15:39:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.44 on epoch=499
06/23/2022 15:39:15 - INFO - __main__ - Global step 1000 Train loss 1.55 ACC 0.53125 on epoch=499
06/23/2022 15:39:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.32 on epoch=504
06/23/2022 15:39:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.25 on epoch=509
06/23/2022 15:39:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.24 on epoch=514
06/23/2022 15:39:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.26 on epoch=519
06/23/2022 15:39:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.15 on epoch=524
06/23/2022 15:39:23 - INFO - __main__ - Global step 1050 Train loss 1.24 ACC 0.5 on epoch=524
06/23/2022 15:39:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.15 on epoch=529
06/23/2022 15:39:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.15 on epoch=534
06/23/2022 15:39:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.03 on epoch=539
06/23/2022 15:39:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.01 on epoch=544
06/23/2022 15:39:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.04 on epoch=549
06/23/2022 15:39:31 - INFO - __main__ - Global step 1100 Train loss 1.07 ACC 0.5 on epoch=549
06/23/2022 15:39:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.95 on epoch=554
06/23/2022 15:39:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.86 on epoch=559
06/23/2022 15:39:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.86 on epoch=564
06/23/2022 15:39:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.94 on epoch=569
06/23/2022 15:39:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.91 on epoch=574
06/23/2022 15:39:39 - INFO - __main__ - Global step 1150 Train loss 0.90 ACC 0.5 on epoch=574
06/23/2022 15:39:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.83 on epoch=579
06/23/2022 15:39:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.85 on epoch=584
06/23/2022 15:39:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.82 on epoch=589
06/23/2022 15:39:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=594
06/23/2022 15:39:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.80 on epoch=599
06/23/2022 15:39:47 - INFO - __main__ - Global step 1200 Train loss 0.83 ACC 0.5 on epoch=599
06/23/2022 15:39:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.82 on epoch=604
06/23/2022 15:39:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.76 on epoch=609
06/23/2022 15:39:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.79 on epoch=614
06/23/2022 15:39:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.79 on epoch=619
06/23/2022 15:39:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.71 on epoch=624
06/23/2022 15:39:55 - INFO - __main__ - Global step 1250 Train loss 0.77 ACC 0.5 on epoch=624
06/23/2022 15:39:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.78 on epoch=629
06/23/2022 15:39:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=634
06/23/2022 15:39:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.75 on epoch=639
06/23/2022 15:40:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.69 on epoch=644
06/23/2022 15:40:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.72 on epoch=649
06/23/2022 15:40:03 - INFO - __main__ - Global step 1300 Train loss 0.74 ACC 0.5 on epoch=649
06/23/2022 15:40:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.74 on epoch=654
06/23/2022 15:40:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.62 on epoch=659
06/23/2022 15:40:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.69 on epoch=664
06/23/2022 15:40:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.71 on epoch=669
06/23/2022 15:40:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.68 on epoch=674
06/23/2022 15:40:11 - INFO - __main__ - Global step 1350 Train loss 0.69 ACC 0.5 on epoch=674
06/23/2022 15:40:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.65 on epoch=679
06/23/2022 15:40:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.68 on epoch=684
06/23/2022 15:40:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.59 on epoch=689
06/23/2022 15:40:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.56 on epoch=694
06/23/2022 15:40:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.65 on epoch=699
06/23/2022 15:40:19 - INFO - __main__ - Global step 1400 Train loss 0.63 ACC 0.5 on epoch=699
06/23/2022 15:40:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.53 on epoch=704
06/23/2022 15:40:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.58 on epoch=709
06/23/2022 15:40:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.59 on epoch=714
06/23/2022 15:40:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.59 on epoch=719
06/23/2022 15:40:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.57 on epoch=724
06/23/2022 15:40:27 - INFO - __main__ - Global step 1450 Train loss 0.57 ACC 0.5 on epoch=724
06/23/2022 15:40:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
06/23/2022 15:40:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.53 on epoch=734
06/23/2022 15:40:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.55 on epoch=739
06/23/2022 15:40:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.54 on epoch=744
06/23/2022 15:40:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.53 on epoch=749
06/23/2022 15:40:35 - INFO - __main__ - Global step 1500 Train loss 0.54 ACC 0.5 on epoch=749
06/23/2022 15:40:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.52 on epoch=754
06/23/2022 15:40:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.53 on epoch=759
06/23/2022 15:40:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.51 on epoch=764
06/23/2022 15:40:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=769
06/23/2022 15:40:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.54 on epoch=774
06/23/2022 15:40:43 - INFO - __main__ - Global step 1550 Train loss 0.51 ACC 0.53125 on epoch=774
06/23/2022 15:40:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.49 on epoch=779
06/23/2022 15:40:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=784
06/23/2022 15:40:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.49 on epoch=789
06/23/2022 15:40:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.51 on epoch=794
06/23/2022 15:40:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=799
06/23/2022 15:40:51 - INFO - __main__ - Global step 1600 Train loss 0.49 ACC 0.5 on epoch=799
06/23/2022 15:40:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.47 on epoch=804
06/23/2022 15:40:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.56 on epoch=809
06/23/2022 15:40:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=814
06/23/2022 15:40:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=819
06/23/2022 15:40:58 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.51 on epoch=824
06/23/2022 15:40:59 - INFO - __main__ - Global step 1650 Train loss 0.49 ACC 0.53125 on epoch=824
06/23/2022 15:41:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=829
06/23/2022 15:41:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=834
06/23/2022 15:41:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=839
06/23/2022 15:41:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=844
06/23/2022 15:41:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=849
06/23/2022 15:41:07 - INFO - __main__ - Global step 1700 Train loss 0.48 ACC 0.5 on epoch=849
06/23/2022 15:41:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=854
06/23/2022 15:41:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.49 on epoch=859
06/23/2022 15:41:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.49 on epoch=864
06/23/2022 15:41:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=869
06/23/2022 15:41:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=874
06/23/2022 15:41:14 - INFO - __main__ - Global step 1750 Train loss 0.48 ACC 0.5 on epoch=874
06/23/2022 15:41:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=879
06/23/2022 15:41:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.46 on epoch=884
06/23/2022 15:41:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=889
06/23/2022 15:41:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=894
06/23/2022 15:41:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=899
06/23/2022 15:41:22 - INFO - __main__ - Global step 1800 Train loss 0.46 ACC 0.5 on epoch=899
06/23/2022 15:41:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.48 on epoch=904
06/23/2022 15:41:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.46 on epoch=909
06/23/2022 15:41:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
06/23/2022 15:41:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=919
06/23/2022 15:41:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=924
06/23/2022 15:41:30 - INFO - __main__ - Global step 1850 Train loss 0.43 ACC 0.5 on epoch=924
06/23/2022 15:41:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
06/23/2022 15:41:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=934
06/23/2022 15:41:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.49 on epoch=939
06/23/2022 15:41:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.47 on epoch=944
06/23/2022 15:41:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=949
06/23/2022 15:41:38 - INFO - __main__ - Global step 1900 Train loss 0.44 ACC 0.5 on epoch=949
06/23/2022 15:41:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=954
06/23/2022 15:41:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/23/2022 15:41:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=964
06/23/2022 15:41:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
06/23/2022 15:41:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.47 on epoch=974
06/23/2022 15:41:45 - INFO - __main__ - Global step 1950 Train loss 0.42 ACC 0.5 on epoch=974
06/23/2022 15:41:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=979
06/23/2022 15:41:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=984
06/23/2022 15:41:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.45 on epoch=989
06/23/2022 15:41:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=994
06/23/2022 15:41:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=999
06/23/2022 15:41:53 - INFO - __main__ - Global step 2000 Train loss 0.41 ACC 0.5 on epoch=999
06/23/2022 15:41:53 - INFO - __main__ - save last model!
06/23/2022 15:41:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 15:41:54 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 15:41:54 - INFO - __main__ - Printing 3 examples
06/23/2022 15:41:54 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 15:41:54 - INFO - __main__ - ['equivalent']
06/23/2022 15:41:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 15:41:54 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:41:54 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 15:41:54 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:41:54 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:41:54 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:41:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:41:54 - INFO - __main__ - Printing 3 examples
06/23/2022 15:41:54 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/23/2022 15:41:54 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:41:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/23/2022 15:41:54 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:41:54 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/23/2022 15:41:54 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:41:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 15:41:54 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:41:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 15:41:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:41:54 - INFO - __main__ - Printing 3 examples
06/23/2022 15:41:54 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/23/2022 15:41:54 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:41:54 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/23/2022 15:41:54 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:41:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/23/2022 15:41:54 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:41:54 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:41:54 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:41:54 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 15:41:54 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 15:42:00 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 15:42:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 15:42:01 - INFO - __main__ - Starting training!
06/23/2022 15:42:03 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_100_0.4_8_predictions.txt
06/23/2022 15:42:03 - INFO - __main__ - ACC on test data: 0.6814
06/23/2022 15:42:03 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.4, bsz=8, dev_performance=0.5625, test_performance=0.6813725490196079
06/23/2022 15:42:03 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.3, bsz=8 ...
06/23/2022 15:42:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:42:04 - INFO - __main__ - Printing 3 examples
06/23/2022 15:42:04 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/23/2022 15:42:04 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:42:04 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/23/2022 15:42:04 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:42:04 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/23/2022 15:42:04 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:42:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 15:42:04 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:42:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 15:42:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:42:04 - INFO - __main__ - Printing 3 examples
06/23/2022 15:42:04 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/23/2022 15:42:04 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:42:04 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/23/2022 15:42:04 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:42:04 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/23/2022 15:42:04 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:42:04 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:42:04 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:42:04 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 15:42:10 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 15:42:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 15:42:10 - INFO - __main__ - Starting training!
06/23/2022 15:42:12 - INFO - __main__ - Step 10 Global step 10 Train loss 6.58 on epoch=4
06/23/2022 15:42:13 - INFO - __main__ - Step 20 Global step 20 Train loss 6.60 on epoch=9
06/23/2022 15:42:14 - INFO - __main__ - Step 30 Global step 30 Train loss 6.55 on epoch=14
06/23/2022 15:42:16 - INFO - __main__ - Step 40 Global step 40 Train loss 6.55 on epoch=19
06/23/2022 15:42:17 - INFO - __main__ - Step 50 Global step 50 Train loss 6.43 on epoch=24
06/23/2022 15:42:19 - INFO - __main__ - Global step 50 Train loss 6.54 ACC 0.0 on epoch=24
06/23/2022 15:42:19 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 15:42:20 - INFO - __main__ - Step 60 Global step 60 Train loss 6.56 on epoch=29
06/23/2022 15:42:22 - INFO - __main__ - Step 70 Global step 70 Train loss 6.51 on epoch=34
06/23/2022 15:42:23 - INFO - __main__ - Step 80 Global step 80 Train loss 6.48 on epoch=39
06/23/2022 15:42:25 - INFO - __main__ - Step 90 Global step 90 Train loss 6.41 on epoch=44
06/23/2022 15:42:26 - INFO - __main__ - Step 100 Global step 100 Train loss 6.43 on epoch=49
06/23/2022 15:42:31 - INFO - __main__ - Global step 100 Train loss 6.48 ACC 0.0 on epoch=49
06/23/2022 15:42:32 - INFO - __main__ - Step 110 Global step 110 Train loss 6.42 on epoch=54
06/23/2022 15:42:34 - INFO - __main__ - Step 120 Global step 120 Train loss 6.41 on epoch=59
06/23/2022 15:42:35 - INFO - __main__ - Step 130 Global step 130 Train loss 6.36 on epoch=64
06/23/2022 15:42:36 - INFO - __main__ - Step 140 Global step 140 Train loss 6.28 on epoch=69
06/23/2022 15:42:38 - INFO - __main__ - Step 150 Global step 150 Train loss 6.26 on epoch=74
06/23/2022 15:42:38 - INFO - __main__ - Global step 150 Train loss 6.35 ACC 0.0 on epoch=74
06/23/2022 15:42:40 - INFO - __main__ - Step 160 Global step 160 Train loss 6.26 on epoch=79
06/23/2022 15:42:41 - INFO - __main__ - Step 170 Global step 170 Train loss 6.22 on epoch=84
06/23/2022 15:42:43 - INFO - __main__ - Step 180 Global step 180 Train loss 6.19 on epoch=89
06/23/2022 15:42:44 - INFO - __main__ - Step 190 Global step 190 Train loss 6.19 on epoch=94
06/23/2022 15:42:46 - INFO - __main__ - Step 200 Global step 200 Train loss 6.24 on epoch=99
06/23/2022 15:42:48 - INFO - __main__ - Global step 200 Train loss 6.22 ACC 0.0 on epoch=99
06/23/2022 15:42:49 - INFO - __main__ - Step 210 Global step 210 Train loss 6.14 on epoch=104
06/23/2022 15:42:51 - INFO - __main__ - Step 220 Global step 220 Train loss 6.07 on epoch=109
06/23/2022 15:42:52 - INFO - __main__ - Step 230 Global step 230 Train loss 6.07 on epoch=114
06/23/2022 15:42:54 - INFO - __main__ - Step 240 Global step 240 Train loss 5.98 on epoch=119
06/23/2022 15:42:55 - INFO - __main__ - Step 250 Global step 250 Train loss 5.93 on epoch=124
06/23/2022 15:42:58 - INFO - __main__ - Global step 250 Train loss 6.04 ACC 0.0 on epoch=124
06/23/2022 15:43:00 - INFO - __main__ - Step 260 Global step 260 Train loss 5.93 on epoch=129
06/23/2022 15:43:01 - INFO - __main__ - Step 270 Global step 270 Train loss 5.91 on epoch=134
06/23/2022 15:43:03 - INFO - __main__ - Step 280 Global step 280 Train loss 5.85 on epoch=139
06/23/2022 15:43:04 - INFO - __main__ - Step 290 Global step 290 Train loss 5.87 on epoch=144
06/23/2022 15:43:05 - INFO - __main__ - Step 300 Global step 300 Train loss 5.72 on epoch=149
06/23/2022 15:43:12 - INFO - __main__ - Global step 300 Train loss 5.86 ACC 0.0 on epoch=149
06/23/2022 15:43:14 - INFO - __main__ - Step 310 Global step 310 Train loss 5.76 on epoch=154
06/23/2022 15:43:15 - INFO - __main__ - Step 320 Global step 320 Train loss 5.75 on epoch=159
06/23/2022 15:43:16 - INFO - __main__ - Step 330 Global step 330 Train loss 5.63 on epoch=164
06/23/2022 15:43:18 - INFO - __main__ - Step 340 Global step 340 Train loss 5.65 on epoch=169
06/23/2022 15:43:19 - INFO - __main__ - Step 350 Global step 350 Train loss 5.44 on epoch=174
06/23/2022 15:43:21 - INFO - __main__ - Global step 350 Train loss 5.65 ACC 0.0 on epoch=174
06/23/2022 15:43:22 - INFO - __main__ - Step 360 Global step 360 Train loss 5.55 on epoch=179
06/23/2022 15:43:23 - INFO - __main__ - Step 370 Global step 370 Train loss 5.40 on epoch=184
06/23/2022 15:43:25 - INFO - __main__ - Step 380 Global step 380 Train loss 5.42 on epoch=189
06/23/2022 15:43:26 - INFO - __main__ - Step 390 Global step 390 Train loss 5.30 on epoch=194
06/23/2022 15:43:28 - INFO - __main__ - Step 400 Global step 400 Train loss 5.28 on epoch=199
06/23/2022 15:43:30 - INFO - __main__ - Global step 400 Train loss 5.39 ACC 0.0 on epoch=199
06/23/2022 15:43:31 - INFO - __main__ - Step 410 Global step 410 Train loss 5.31 on epoch=204
06/23/2022 15:43:33 - INFO - __main__ - Step 420 Global step 420 Train loss 5.18 on epoch=209
06/23/2022 15:43:34 - INFO - __main__ - Step 430 Global step 430 Train loss 5.27 on epoch=214
06/23/2022 15:43:35 - INFO - __main__ - Step 440 Global step 440 Train loss 5.11 on epoch=219
06/23/2022 15:43:37 - INFO - __main__ - Step 450 Global step 450 Train loss 5.10 on epoch=224
06/23/2022 15:43:43 - INFO - __main__ - Global step 450 Train loss 5.19 ACC 0.0 on epoch=224
06/23/2022 15:43:44 - INFO - __main__ - Step 460 Global step 460 Train loss 5.10 on epoch=229
06/23/2022 15:43:46 - INFO - __main__ - Step 470 Global step 470 Train loss 5.01 on epoch=234
06/23/2022 15:43:47 - INFO - __main__ - Step 480 Global step 480 Train loss 5.08 on epoch=239
06/23/2022 15:43:48 - INFO - __main__ - Step 490 Global step 490 Train loss 4.88 on epoch=244
06/23/2022 15:43:50 - INFO - __main__ - Step 500 Global step 500 Train loss 4.85 on epoch=249
06/23/2022 15:43:53 - INFO - __main__ - Global step 500 Train loss 4.99 ACC 0.0 on epoch=249
06/23/2022 15:43:54 - INFO - __main__ - Step 510 Global step 510 Train loss 4.97 on epoch=254
06/23/2022 15:43:56 - INFO - __main__ - Step 520 Global step 520 Train loss 4.85 on epoch=259
06/23/2022 15:43:57 - INFO - __main__ - Step 530 Global step 530 Train loss 4.74 on epoch=264
06/23/2022 15:43:59 - INFO - __main__ - Step 540 Global step 540 Train loss 4.76 on epoch=269
06/23/2022 15:44:00 - INFO - __main__ - Step 550 Global step 550 Train loss 4.77 on epoch=274
06/23/2022 15:44:07 - INFO - __main__ - Global step 550 Train loss 4.82 ACC 0.0 on epoch=274
06/23/2022 15:44:08 - INFO - __main__ - Step 560 Global step 560 Train loss 4.69 on epoch=279
06/23/2022 15:44:10 - INFO - __main__ - Step 570 Global step 570 Train loss 4.64 on epoch=284
06/23/2022 15:44:11 - INFO - __main__ - Step 580 Global step 580 Train loss 4.59 on epoch=289
06/23/2022 15:44:12 - INFO - __main__ - Step 590 Global step 590 Train loss 4.61 on epoch=294
06/23/2022 15:44:14 - INFO - __main__ - Step 600 Global step 600 Train loss 4.56 on epoch=299
06/23/2022 15:44:18 - INFO - __main__ - Global step 600 Train loss 4.62 ACC 0.0 on epoch=299
06/23/2022 15:44:19 - INFO - __main__ - Step 610 Global step 610 Train loss 4.59 on epoch=304
06/23/2022 15:44:21 - INFO - __main__ - Step 620 Global step 620 Train loss 4.40 on epoch=309
06/23/2022 15:44:22 - INFO - __main__ - Step 630 Global step 630 Train loss 4.55 on epoch=314
06/23/2022 15:44:24 - INFO - __main__ - Step 640 Global step 640 Train loss 4.44 on epoch=319
06/23/2022 15:44:25 - INFO - __main__ - Step 650 Global step 650 Train loss 4.34 on epoch=324
06/23/2022 15:44:34 - INFO - __main__ - Global step 650 Train loss 4.46 ACC 0.0 on epoch=324
06/23/2022 15:44:36 - INFO - __main__ - Step 660 Global step 660 Train loss 4.30 on epoch=329
06/23/2022 15:44:37 - INFO - __main__ - Step 670 Global step 670 Train loss 4.32 on epoch=334
06/23/2022 15:44:39 - INFO - __main__ - Step 680 Global step 680 Train loss 4.14 on epoch=339
06/23/2022 15:44:40 - INFO - __main__ - Step 690 Global step 690 Train loss 4.20 on epoch=344
06/23/2022 15:44:41 - INFO - __main__ - Step 700 Global step 700 Train loss 4.12 on epoch=349
06/23/2022 15:44:50 - INFO - __main__ - Global step 700 Train loss 4.22 ACC 0.0 on epoch=349
06/23/2022 15:44:51 - INFO - __main__ - Step 710 Global step 710 Train loss 4.10 on epoch=354
06/23/2022 15:44:52 - INFO - __main__ - Step 720 Global step 720 Train loss 4.09 on epoch=359
06/23/2022 15:44:54 - INFO - __main__ - Step 730 Global step 730 Train loss 4.03 on epoch=364
06/23/2022 15:44:55 - INFO - __main__ - Step 740 Global step 740 Train loss 3.86 on epoch=369
06/23/2022 15:44:57 - INFO - __main__ - Step 750 Global step 750 Train loss 3.89 on epoch=374
06/23/2022 15:45:05 - INFO - __main__ - Global step 750 Train loss 3.99 ACC 0.0 on epoch=374
06/23/2022 15:45:06 - INFO - __main__ - Step 760 Global step 760 Train loss 3.89 on epoch=379
06/23/2022 15:45:08 - INFO - __main__ - Step 770 Global step 770 Train loss 3.69 on epoch=384
06/23/2022 15:45:09 - INFO - __main__ - Step 780 Global step 780 Train loss 3.65 on epoch=389
06/23/2022 15:45:11 - INFO - __main__ - Step 790 Global step 790 Train loss 3.65 on epoch=394
06/23/2022 15:45:13 - INFO - __main__ - Step 800 Global step 800 Train loss 3.47 on epoch=399
06/23/2022 15:45:22 - INFO - __main__ - Global step 800 Train loss 3.67 ACC 0.0 on epoch=399
06/23/2022 15:45:24 - INFO - __main__ - Step 810 Global step 810 Train loss 3.48 on epoch=404
06/23/2022 15:45:25 - INFO - __main__ - Step 820 Global step 820 Train loss 3.37 on epoch=409
06/23/2022 15:45:26 - INFO - __main__ - Step 830 Global step 830 Train loss 3.25 on epoch=414
06/23/2022 15:45:28 - INFO - __main__ - Step 840 Global step 840 Train loss 3.09 on epoch=419
06/23/2022 15:45:29 - INFO - __main__ - Step 850 Global step 850 Train loss 3.18 on epoch=424
06/23/2022 15:45:41 - INFO - __main__ - Global step 850 Train loss 3.27 ACC 0.0 on epoch=424
06/23/2022 15:45:42 - INFO - __main__ - Step 860 Global step 860 Train loss 3.15 on epoch=429
06/23/2022 15:45:44 - INFO - __main__ - Step 870 Global step 870 Train loss 2.92 on epoch=434
06/23/2022 15:45:45 - INFO - __main__ - Step 880 Global step 880 Train loss 3.06 on epoch=439
06/23/2022 15:45:46 - INFO - __main__ - Step 890 Global step 890 Train loss 2.97 on epoch=444
06/23/2022 15:45:48 - INFO - __main__ - Step 900 Global step 900 Train loss 2.94 on epoch=449
06/23/2022 15:46:00 - INFO - __main__ - Global step 900 Train loss 3.01 ACC 0.0 on epoch=449
06/23/2022 15:46:01 - INFO - __main__ - Step 910 Global step 910 Train loss 2.84 on epoch=454
06/23/2022 15:46:03 - INFO - __main__ - Step 920 Global step 920 Train loss 2.78 on epoch=459
06/23/2022 15:46:04 - INFO - __main__ - Step 930 Global step 930 Train loss 2.65 on epoch=464
06/23/2022 15:46:05 - INFO - __main__ - Step 940 Global step 940 Train loss 2.70 on epoch=469
06/23/2022 15:46:07 - INFO - __main__ - Step 950 Global step 950 Train loss 2.53 on epoch=474
06/23/2022 15:46:18 - INFO - __main__ - Global step 950 Train loss 2.70 ACC 0.0625 on epoch=474
06/23/2022 15:46:18 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=474, global_step=950
06/23/2022 15:46:19 - INFO - __main__ - Step 960 Global step 960 Train loss 2.48 on epoch=479
06/23/2022 15:46:21 - INFO - __main__ - Step 970 Global step 970 Train loss 2.56 on epoch=484
06/23/2022 15:46:22 - INFO - __main__ - Step 980 Global step 980 Train loss 2.38 on epoch=489
06/23/2022 15:46:24 - INFO - __main__ - Step 990 Global step 990 Train loss 2.35 on epoch=494
06/23/2022 15:46:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.42 on epoch=499
06/23/2022 15:46:32 - INFO - __main__ - Global step 1000 Train loss 2.44 ACC 0.3125 on epoch=499
06/23/2022 15:46:32 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.3125 on epoch=499, global_step=1000
06/23/2022 15:46:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.47 on epoch=504
06/23/2022 15:46:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.34 on epoch=509
06/23/2022 15:46:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.16 on epoch=514
06/23/2022 15:46:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.14 on epoch=519
06/23/2022 15:46:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.18 on epoch=524
06/23/2022 15:46:40 - INFO - __main__ - Global step 1050 Train loss 2.26 ACC 0.40625 on epoch=524
06/23/2022 15:46:40 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.40625 on epoch=524, global_step=1050
06/23/2022 15:46:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.07 on epoch=529
06/23/2022 15:46:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.01 on epoch=534
06/23/2022 15:46:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.92 on epoch=539
06/23/2022 15:46:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.07 on epoch=544
06/23/2022 15:46:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.06 on epoch=549
06/23/2022 15:46:48 - INFO - __main__ - Global step 1100 Train loss 2.03 ACC 0.5 on epoch=549
06/23/2022 15:46:48 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.5 on epoch=549, global_step=1100
06/23/2022 15:46:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.02 on epoch=554
06/23/2022 15:46:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.94 on epoch=559
06/23/2022 15:46:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.83 on epoch=564
06/23/2022 15:46:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.93 on epoch=569
06/23/2022 15:46:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.89 on epoch=574
06/23/2022 15:46:56 - INFO - __main__ - Global step 1150 Train loss 1.92 ACC 0.5 on epoch=574
06/23/2022 15:46:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.98 on epoch=579
06/23/2022 15:46:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.83 on epoch=584
06/23/2022 15:47:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.78 on epoch=589
06/23/2022 15:47:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.67 on epoch=594
06/23/2022 15:47:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.88 on epoch=599
06/23/2022 15:47:04 - INFO - __main__ - Global step 1200 Train loss 1.83 ACC 0.5 on epoch=599
06/23/2022 15:47:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.77 on epoch=604
06/23/2022 15:47:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.76 on epoch=609
06/23/2022 15:47:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.70 on epoch=614
06/23/2022 15:47:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.79 on epoch=619
06/23/2022 15:47:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.48 on epoch=624
06/23/2022 15:47:11 - INFO - __main__ - Global step 1250 Train loss 1.70 ACC 0.5 on epoch=624
06/23/2022 15:47:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.68 on epoch=629
06/23/2022 15:47:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.47 on epoch=634
06/23/2022 15:47:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.62 on epoch=639
06/23/2022 15:47:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.51 on epoch=644
06/23/2022 15:47:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.47 on epoch=649
06/23/2022 15:47:19 - INFO - __main__ - Global step 1300 Train loss 1.55 ACC 0.5 on epoch=649
06/23/2022 15:47:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.43 on epoch=654
06/23/2022 15:47:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.43 on epoch=659
06/23/2022 15:47:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.48 on epoch=664
06/23/2022 15:47:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.42 on epoch=669
06/23/2022 15:47:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.44 on epoch=674
06/23/2022 15:47:27 - INFO - __main__ - Global step 1350 Train loss 1.44 ACC 0.5 on epoch=674
06/23/2022 15:47:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.42 on epoch=679
06/23/2022 15:47:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.43 on epoch=684
06/23/2022 15:47:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.47 on epoch=689
06/23/2022 15:47:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.24 on epoch=694
06/23/2022 15:47:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.32 on epoch=699
06/23/2022 15:47:36 - INFO - __main__ - Global step 1400 Train loss 1.38 ACC 0.5 on epoch=699
06/23/2022 15:47:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.36 on epoch=704
06/23/2022 15:47:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.24 on epoch=709
06/23/2022 15:47:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.29 on epoch=714
06/23/2022 15:47:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.31 on epoch=719
06/23/2022 15:47:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.20 on epoch=724
06/23/2022 15:47:44 - INFO - __main__ - Global step 1450 Train loss 1.28 ACC 0.5 on epoch=724
06/23/2022 15:47:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.16 on epoch=729
06/23/2022 15:47:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.18 on epoch=734
06/23/2022 15:47:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.14 on epoch=739
06/23/2022 15:47:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.17 on epoch=744
06/23/2022 15:47:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.14 on epoch=749
06/23/2022 15:47:52 - INFO - __main__ - Global step 1500 Train loss 1.16 ACC 0.5 on epoch=749
06/23/2022 15:47:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.13 on epoch=754
06/23/2022 15:47:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.17 on epoch=759
06/23/2022 15:47:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.11 on epoch=764
06/23/2022 15:47:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.13 on epoch=769
06/23/2022 15:47:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.05 on epoch=774
06/23/2022 15:48:00 - INFO - __main__ - Global step 1550 Train loss 1.12 ACC 0.5 on epoch=774
06/23/2022 15:48:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.01 on epoch=779
06/23/2022 15:48:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.95 on epoch=784
06/23/2022 15:48:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.04 on epoch=789
06/23/2022 15:48:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.95 on epoch=794
06/23/2022 15:48:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.95 on epoch=799
06/23/2022 15:48:09 - INFO - __main__ - Global step 1600 Train loss 0.98 ACC 0.5 on epoch=799
06/23/2022 15:48:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.98 on epoch=804
06/23/2022 15:48:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.92 on epoch=809
06/23/2022 15:48:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.88 on epoch=814
06/23/2022 15:48:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.98 on epoch=819
06/23/2022 15:48:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.86 on epoch=824
06/23/2022 15:48:17 - INFO - __main__ - Global step 1650 Train loss 0.92 ACC 0.5 on epoch=824
06/23/2022 15:48:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.95 on epoch=829
06/23/2022 15:48:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.90 on epoch=834
06/23/2022 15:48:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.88 on epoch=839
06/23/2022 15:48:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.83 on epoch=844
06/23/2022 15:48:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.83 on epoch=849
06/23/2022 15:48:25 - INFO - __main__ - Global step 1700 Train loss 0.88 ACC 0.5 on epoch=849
06/23/2022 15:48:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.85 on epoch=854
06/23/2022 15:48:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.85 on epoch=859
06/23/2022 15:48:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.82 on epoch=864
06/23/2022 15:48:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.86 on epoch=869
06/23/2022 15:48:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.82 on epoch=874
06/23/2022 15:48:33 - INFO - __main__ - Global step 1750 Train loss 0.84 ACC 0.5 on epoch=874
06/23/2022 15:48:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.79 on epoch=879
06/23/2022 15:48:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.76 on epoch=884
06/23/2022 15:48:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.76 on epoch=889
06/23/2022 15:48:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.83 on epoch=894
06/23/2022 15:48:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.75 on epoch=899
06/23/2022 15:48:41 - INFO - __main__ - Global step 1800 Train loss 0.78 ACC 0.5 on epoch=899
06/23/2022 15:48:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.77 on epoch=904
06/23/2022 15:48:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.75 on epoch=909
06/23/2022 15:48:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.72 on epoch=914
06/23/2022 15:48:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.77 on epoch=919
06/23/2022 15:48:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.80 on epoch=924
06/23/2022 15:48:48 - INFO - __main__ - Global step 1850 Train loss 0.76 ACC 0.5 on epoch=924
06/23/2022 15:48:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.73 on epoch=929
06/23/2022 15:48:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.75 on epoch=934
06/23/2022 15:48:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.77 on epoch=939
06/23/2022 15:48:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.73 on epoch=944
06/23/2022 15:48:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.80 on epoch=949
06/23/2022 15:48:56 - INFO - __main__ - Global step 1900 Train loss 0.75 ACC 0.5 on epoch=949
06/23/2022 15:48:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.73 on epoch=954
06/23/2022 15:48:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.57 on epoch=959
06/23/2022 15:49:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.65 on epoch=964
06/23/2022 15:49:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.71 on epoch=969
06/23/2022 15:49:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.61 on epoch=974
06/23/2022 15:49:04 - INFO - __main__ - Global step 1950 Train loss 0.65 ACC 0.5 on epoch=974
06/23/2022 15:49:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.56 on epoch=979
06/23/2022 15:49:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.62 on epoch=984
06/23/2022 15:49:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.68 on epoch=989
06/23/2022 15:49:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.58 on epoch=994
06/23/2022 15:49:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.62 on epoch=999
06/23/2022 15:49:11 - INFO - __main__ - Global step 2000 Train loss 0.61 ACC 0.5 on epoch=999
06/23/2022 15:49:11 - INFO - __main__ - save last model!
06/23/2022 15:49:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 15:49:11 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 15:49:11 - INFO - __main__ - Printing 3 examples
06/23/2022 15:49:11 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 15:49:11 - INFO - __main__ - ['equivalent']
06/23/2022 15:49:11 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 15:49:11 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:11 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 15:49:11 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:11 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:49:12 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:49:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:49:12 - INFO - __main__ - Printing 3 examples
06/23/2022 15:49:12 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/23/2022 15:49:12 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/23/2022 15:49:12 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:12 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/23/2022 15:49:12 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 15:49:12 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:49:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 15:49:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:49:12 - INFO - __main__ - Printing 3 examples
06/23/2022 15:49:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/23/2022 15:49:12 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:12 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/23/2022 15:49:12 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/23/2022 15:49:12 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:12 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:49:12 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:49:12 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 15:49:12 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 15:49:19 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 15:49:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 15:49:19 - INFO - __main__ - Starting training!
06/23/2022 15:49:21 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_100_0.3_8_predictions.txt
06/23/2022 15:49:21 - INFO - __main__ - ACC on test data: 0.6838
06/23/2022 15:49:21 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.3, bsz=8, dev_performance=0.5, test_performance=0.6838235294117647
06/23/2022 15:49:21 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.2, bsz=8 ...
06/23/2022 15:49:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:49:22 - INFO - __main__ - Printing 3 examples
06/23/2022 15:49:22 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/23/2022 15:49:22 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/23/2022 15:49:22 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:22 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/23/2022 15:49:22 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 15:49:22 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:49:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 15:49:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:49:22 - INFO - __main__ - Printing 3 examples
06/23/2022 15:49:22 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/23/2022 15:49:22 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:22 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/23/2022 15:49:22 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/23/2022 15:49:22 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:49:22 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:49:22 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:49:22 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 15:49:29 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 15:49:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 15:49:29 - INFO - __main__ - Starting training!
06/23/2022 15:49:31 - INFO - __main__ - Step 10 Global step 10 Train loss 6.58 on epoch=4
06/23/2022 15:49:32 - INFO - __main__ - Step 20 Global step 20 Train loss 6.51 on epoch=9
06/23/2022 15:49:34 - INFO - __main__ - Step 30 Global step 30 Train loss 6.44 on epoch=14
06/23/2022 15:49:35 - INFO - __main__ - Step 40 Global step 40 Train loss 6.53 on epoch=19
06/23/2022 15:49:37 - INFO - __main__ - Step 50 Global step 50 Train loss 6.63 on epoch=24
06/23/2022 15:49:38 - INFO - __main__ - Global step 50 Train loss 6.54 ACC 0.0 on epoch=24
06/23/2022 15:49:38 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 15:49:39 - INFO - __main__ - Step 60 Global step 60 Train loss 6.55 on epoch=29
06/23/2022 15:49:41 - INFO - __main__ - Step 70 Global step 70 Train loss 6.51 on epoch=34
06/23/2022 15:49:42 - INFO - __main__ - Step 80 Global step 80 Train loss 6.57 on epoch=39
06/23/2022 15:49:44 - INFO - __main__ - Step 90 Global step 90 Train loss 6.51 on epoch=44
06/23/2022 15:49:45 - INFO - __main__ - Step 100 Global step 100 Train loss 6.51 on epoch=49
06/23/2022 15:49:46 - INFO - __main__ - Global step 100 Train loss 6.53 ACC 0.0 on epoch=49
06/23/2022 15:49:48 - INFO - __main__ - Step 110 Global step 110 Train loss 6.48 on epoch=54
06/23/2022 15:49:49 - INFO - __main__ - Step 120 Global step 120 Train loss 6.46 on epoch=59
06/23/2022 15:49:51 - INFO - __main__ - Step 130 Global step 130 Train loss 6.49 on epoch=64
06/23/2022 15:49:52 - INFO - __main__ - Step 140 Global step 140 Train loss 6.46 on epoch=69
06/23/2022 15:49:53 - INFO - __main__ - Step 150 Global step 150 Train loss 6.46 on epoch=74
06/23/2022 15:49:55 - INFO - __main__ - Global step 150 Train loss 6.47 ACC 0.0 on epoch=74
06/23/2022 15:49:56 - INFO - __main__ - Step 160 Global step 160 Train loss 6.44 on epoch=79
06/23/2022 15:49:57 - INFO - __main__ - Step 170 Global step 170 Train loss 6.40 on epoch=84
06/23/2022 15:49:59 - INFO - __main__ - Step 180 Global step 180 Train loss 6.44 on epoch=89
06/23/2022 15:50:00 - INFO - __main__ - Step 190 Global step 190 Train loss 6.35 on epoch=94
06/23/2022 15:50:02 - INFO - __main__ - Step 200 Global step 200 Train loss 6.35 on epoch=99
06/23/2022 15:50:02 - INFO - __main__ - Global step 200 Train loss 6.39 ACC 0.0 on epoch=99
06/23/2022 15:50:04 - INFO - __main__ - Step 210 Global step 210 Train loss 6.37 on epoch=104
06/23/2022 15:50:05 - INFO - __main__ - Step 220 Global step 220 Train loss 6.38 on epoch=109
06/23/2022 15:50:07 - INFO - __main__ - Step 230 Global step 230 Train loss 6.37 on epoch=114
06/23/2022 15:50:08 - INFO - __main__ - Step 240 Global step 240 Train loss 6.32 on epoch=119
06/23/2022 15:50:10 - INFO - __main__ - Step 250 Global step 250 Train loss 6.29 on epoch=124
06/23/2022 15:50:20 - INFO - __main__ - Global step 250 Train loss 6.35 ACC 0.0 on epoch=124
06/23/2022 15:50:22 - INFO - __main__ - Step 260 Global step 260 Train loss 6.23 on epoch=129
06/23/2022 15:50:23 - INFO - __main__ - Step 270 Global step 270 Train loss 6.26 on epoch=134
06/23/2022 15:50:24 - INFO - __main__ - Step 280 Global step 280 Train loss 6.14 on epoch=139
06/23/2022 15:50:26 - INFO - __main__ - Step 290 Global step 290 Train loss 6.19 on epoch=144
06/23/2022 15:50:27 - INFO - __main__ - Step 300 Global step 300 Train loss 6.09 on epoch=149
06/23/2022 15:50:30 - INFO - __main__ - Global step 300 Train loss 6.18 ACC 0.0 on epoch=149
06/23/2022 15:50:31 - INFO - __main__ - Step 310 Global step 310 Train loss 6.16 on epoch=154
06/23/2022 15:50:32 - INFO - __main__ - Step 320 Global step 320 Train loss 6.10 on epoch=159
06/23/2022 15:50:34 - INFO - __main__ - Step 330 Global step 330 Train loss 6.04 on epoch=164
06/23/2022 15:50:35 - INFO - __main__ - Step 340 Global step 340 Train loss 5.99 on epoch=169
06/23/2022 15:50:37 - INFO - __main__ - Step 350 Global step 350 Train loss 5.96 on epoch=174
06/23/2022 15:50:40 - INFO - __main__ - Global step 350 Train loss 6.05 ACC 0.0 on epoch=174
06/23/2022 15:50:41 - INFO - __main__ - Step 360 Global step 360 Train loss 5.96 on epoch=179
06/23/2022 15:50:43 - INFO - __main__ - Step 370 Global step 370 Train loss 5.99 on epoch=184
06/23/2022 15:50:44 - INFO - __main__ - Step 380 Global step 380 Train loss 5.95 on epoch=189
06/23/2022 15:50:45 - INFO - __main__ - Step 390 Global step 390 Train loss 5.85 on epoch=194
06/23/2022 15:50:47 - INFO - __main__ - Step 400 Global step 400 Train loss 5.78 on epoch=199
06/23/2022 15:50:54 - INFO - __main__ - Global step 400 Train loss 5.91 ACC 0.0 on epoch=199
06/23/2022 15:50:55 - INFO - __main__ - Step 410 Global step 410 Train loss 5.77 on epoch=204
06/23/2022 15:50:57 - INFO - __main__ - Step 420 Global step 420 Train loss 5.86 on epoch=209
06/23/2022 15:50:58 - INFO - __main__ - Step 430 Global step 430 Train loss 5.78 on epoch=214
06/23/2022 15:51:00 - INFO - __main__ - Step 440 Global step 440 Train loss 5.87 on epoch=219
06/23/2022 15:51:01 - INFO - __main__ - Step 450 Global step 450 Train loss 5.79 on epoch=224
06/23/2022 15:51:03 - INFO - __main__ - Global step 450 Train loss 5.81 ACC 0.0 on epoch=224
06/23/2022 15:51:05 - INFO - __main__ - Step 460 Global step 460 Train loss 5.69 on epoch=229
06/23/2022 15:51:06 - INFO - __main__ - Step 470 Global step 470 Train loss 5.64 on epoch=234
06/23/2022 15:51:08 - INFO - __main__ - Step 480 Global step 480 Train loss 5.69 on epoch=239
06/23/2022 15:51:09 - INFO - __main__ - Step 490 Global step 490 Train loss 5.61 on epoch=244
06/23/2022 15:51:10 - INFO - __main__ - Step 500 Global step 500 Train loss 5.66 on epoch=249
06/23/2022 15:51:18 - INFO - __main__ - Global step 500 Train loss 5.66 ACC 0.0 on epoch=249
06/23/2022 15:51:20 - INFO - __main__ - Step 510 Global step 510 Train loss 5.59 on epoch=254
06/23/2022 15:51:21 - INFO - __main__ - Step 520 Global step 520 Train loss 5.60 on epoch=259
06/23/2022 15:51:22 - INFO - __main__ - Step 530 Global step 530 Train loss 5.63 on epoch=264
06/23/2022 15:51:24 - INFO - __main__ - Step 540 Global step 540 Train loss 5.72 on epoch=269
06/23/2022 15:51:25 - INFO - __main__ - Step 550 Global step 550 Train loss 5.79 on epoch=274
06/23/2022 15:51:37 - INFO - __main__ - Global step 550 Train loss 5.67 ACC 0.0 on epoch=274
06/23/2022 15:51:38 - INFO - __main__ - Step 560 Global step 560 Train loss 5.70 on epoch=279
06/23/2022 15:51:39 - INFO - __main__ - Step 570 Global step 570 Train loss 5.68 on epoch=284
06/23/2022 15:51:41 - INFO - __main__ - Step 580 Global step 580 Train loss 5.70 on epoch=289
06/23/2022 15:51:42 - INFO - __main__ - Step 590 Global step 590 Train loss 5.64 on epoch=294
06/23/2022 15:51:44 - INFO - __main__ - Step 600 Global step 600 Train loss 5.52 on epoch=299
06/23/2022 15:51:47 - INFO - __main__ - Global step 600 Train loss 5.65 ACC 0.0 on epoch=299
06/23/2022 15:51:49 - INFO - __main__ - Step 610 Global step 610 Train loss 5.69 on epoch=304
06/23/2022 15:51:50 - INFO - __main__ - Step 620 Global step 620 Train loss 5.72 on epoch=309
06/23/2022 15:51:51 - INFO - __main__ - Step 630 Global step 630 Train loss 5.66 on epoch=314
06/23/2022 15:51:53 - INFO - __main__ - Step 640 Global step 640 Train loss 5.64 on epoch=319
06/23/2022 15:51:54 - INFO - __main__ - Step 650 Global step 650 Train loss 5.48 on epoch=324
06/23/2022 15:51:56 - INFO - __main__ - Global step 650 Train loss 5.64 ACC 0.0 on epoch=324
06/23/2022 15:51:57 - INFO - __main__ - Step 660 Global step 660 Train loss 5.52 on epoch=329
06/23/2022 15:51:59 - INFO - __main__ - Step 670 Global step 670 Train loss 5.61 on epoch=334
06/23/2022 15:52:00 - INFO - __main__ - Step 680 Global step 680 Train loss 5.59 on epoch=339
06/23/2022 15:52:01 - INFO - __main__ - Step 690 Global step 690 Train loss 5.54 on epoch=344
06/23/2022 15:52:03 - INFO - __main__ - Step 700 Global step 700 Train loss 5.49 on epoch=349
06/23/2022 15:52:06 - INFO - __main__ - Global step 700 Train loss 5.55 ACC 0.0 on epoch=349
06/23/2022 15:52:07 - INFO - __main__ - Step 710 Global step 710 Train loss 5.43 on epoch=354
06/23/2022 15:52:08 - INFO - __main__ - Step 720 Global step 720 Train loss 5.49 on epoch=359
06/23/2022 15:52:10 - INFO - __main__ - Step 730 Global step 730 Train loss 5.49 on epoch=364
06/23/2022 15:52:11 - INFO - __main__ - Step 740 Global step 740 Train loss 5.43 on epoch=369
06/23/2022 15:52:12 - INFO - __main__ - Step 750 Global step 750 Train loss 5.40 on epoch=374
06/23/2022 15:52:19 - INFO - __main__ - Global step 750 Train loss 5.45 ACC 0.0 on epoch=374
06/23/2022 15:52:20 - INFO - __main__ - Step 760 Global step 760 Train loss 5.37 on epoch=379
06/23/2022 15:52:21 - INFO - __main__ - Step 770 Global step 770 Train loss 5.38 on epoch=384
06/23/2022 15:52:23 - INFO - __main__ - Step 780 Global step 780 Train loss 5.40 on epoch=389
06/23/2022 15:52:24 - INFO - __main__ - Step 790 Global step 790 Train loss 5.36 on epoch=394
06/23/2022 15:52:25 - INFO - __main__ - Step 800 Global step 800 Train loss 5.34 on epoch=399
06/23/2022 15:52:31 - INFO - __main__ - Global step 800 Train loss 5.37 ACC 0.0 on epoch=399
06/23/2022 15:52:32 - INFO - __main__ - Step 810 Global step 810 Train loss 5.29 on epoch=404
06/23/2022 15:52:34 - INFO - __main__ - Step 820 Global step 820 Train loss 5.30 on epoch=409
06/23/2022 15:52:36 - INFO - __main__ - Step 830 Global step 830 Train loss 5.28 on epoch=414
06/23/2022 15:52:37 - INFO - __main__ - Step 840 Global step 840 Train loss 5.33 on epoch=419
06/23/2022 15:52:39 - INFO - __main__ - Step 850 Global step 850 Train loss 5.33 on epoch=424
06/23/2022 15:52:41 - INFO - __main__ - Global step 850 Train loss 5.31 ACC 0.0 on epoch=424
06/23/2022 15:52:42 - INFO - __main__ - Step 860 Global step 860 Train loss 5.31 on epoch=429
06/23/2022 15:52:43 - INFO - __main__ - Step 870 Global step 870 Train loss 5.41 on epoch=434
06/23/2022 15:52:45 - INFO - __main__ - Step 880 Global step 880 Train loss 5.26 on epoch=439
06/23/2022 15:52:46 - INFO - __main__ - Step 890 Global step 890 Train loss 5.29 on epoch=444
06/23/2022 15:52:48 - INFO - __main__ - Step 900 Global step 900 Train loss 5.20 on epoch=449
06/23/2022 15:52:54 - INFO - __main__ - Global step 900 Train loss 5.29 ACC 0.0 on epoch=449
06/23/2022 15:52:55 - INFO - __main__ - Step 910 Global step 910 Train loss 5.31 on epoch=454
06/23/2022 15:52:57 - INFO - __main__ - Step 920 Global step 920 Train loss 5.25 on epoch=459
06/23/2022 15:52:58 - INFO - __main__ - Step 930 Global step 930 Train loss 5.12 on epoch=464
06/23/2022 15:53:00 - INFO - __main__ - Step 940 Global step 940 Train loss 5.26 on epoch=469
06/23/2022 15:53:01 - INFO - __main__ - Step 950 Global step 950 Train loss 5.15 on epoch=474
06/23/2022 15:53:05 - INFO - __main__ - Global step 950 Train loss 5.22 ACC 0.0 on epoch=474
06/23/2022 15:53:07 - INFO - __main__ - Step 960 Global step 960 Train loss 5.25 on epoch=479
06/23/2022 15:53:08 - INFO - __main__ - Step 970 Global step 970 Train loss 5.15 on epoch=484
06/23/2022 15:53:10 - INFO - __main__ - Step 980 Global step 980 Train loss 5.20 on epoch=489
06/23/2022 15:53:11 - INFO - __main__ - Step 990 Global step 990 Train loss 5.04 on epoch=494
06/23/2022 15:53:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 5.17 on epoch=499
06/23/2022 15:53:18 - INFO - __main__ - Global step 1000 Train loss 5.16 ACC 0.0 on epoch=499
06/23/2022 15:53:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 5.12 on epoch=504
06/23/2022 15:53:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 5.10 on epoch=509
06/23/2022 15:53:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 5.03 on epoch=514
06/23/2022 15:53:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 5.06 on epoch=519
06/23/2022 15:53:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 5.05 on epoch=524
06/23/2022 15:53:33 - INFO - __main__ - Global step 1050 Train loss 5.07 ACC 0.0 on epoch=524
06/23/2022 15:53:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 4.94 on epoch=529
06/23/2022 15:53:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 4.92 on epoch=534
06/23/2022 15:53:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 4.82 on epoch=539
06/23/2022 15:53:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 5.06 on epoch=544
06/23/2022 15:53:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 4.96 on epoch=549
06/23/2022 15:53:47 - INFO - __main__ - Global step 1100 Train loss 4.94 ACC 0.0 on epoch=549
06/23/2022 15:53:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 5.01 on epoch=554
06/23/2022 15:53:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 4.94 on epoch=559
06/23/2022 15:53:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 4.84 on epoch=564
06/23/2022 15:53:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 4.85 on epoch=569
06/23/2022 15:53:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 4.81 on epoch=574
06/23/2022 15:54:00 - INFO - __main__ - Global step 1150 Train loss 4.89 ACC 0.0 on epoch=574
06/23/2022 15:54:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 4.88 on epoch=579
06/23/2022 15:54:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 4.80 on epoch=584
06/23/2022 15:54:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 4.81 on epoch=589
06/23/2022 15:54:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 4.80 on epoch=594
06/23/2022 15:54:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 4.69 on epoch=599
06/23/2022 15:54:14 - INFO - __main__ - Global step 1200 Train loss 4.80 ACC 0.0 on epoch=599
06/23/2022 15:54:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 4.61 on epoch=604
06/23/2022 15:54:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 4.63 on epoch=609
06/23/2022 15:54:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 4.61 on epoch=614
06/23/2022 15:54:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 4.67 on epoch=619
06/23/2022 15:54:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 4.56 on epoch=624
06/23/2022 15:54:22 - INFO - __main__ - Global step 1250 Train loss 4.62 ACC 0.0 on epoch=624
06/23/2022 15:54:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 4.55 on epoch=629
06/23/2022 15:54:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 4.38 on epoch=634
06/23/2022 15:54:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 4.34 on epoch=639
06/23/2022 15:54:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 4.35 on epoch=644
06/23/2022 15:54:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 4.22 on epoch=649
06/23/2022 15:54:31 - INFO - __main__ - Global step 1300 Train loss 4.37 ACC 0.0 on epoch=649
06/23/2022 15:54:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 4.40 on epoch=654
06/23/2022 15:54:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 4.22 on epoch=659
06/23/2022 15:54:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 4.20 on epoch=664
06/23/2022 15:54:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 4.17 on epoch=669
06/23/2022 15:54:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 4.09 on epoch=674
06/23/2022 15:54:48 - INFO - __main__ - Global step 1350 Train loss 4.22 ACC 0.0 on epoch=674
06/23/2022 15:54:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 4.10 on epoch=679
06/23/2022 15:54:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 4.13 on epoch=684
06/23/2022 15:54:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 4.02 on epoch=689
06/23/2022 15:54:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 4.06 on epoch=694
06/23/2022 15:54:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 3.98 on epoch=699
06/23/2022 15:54:57 - INFO - __main__ - Global step 1400 Train loss 4.06 ACC 0.0 on epoch=699
06/23/2022 15:54:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 3.88 on epoch=704
06/23/2022 15:54:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 3.89 on epoch=709
06/23/2022 15:55:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 3.95 on epoch=714
06/23/2022 15:55:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 3.84 on epoch=719
06/23/2022 15:55:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 3.83 on epoch=724
06/23/2022 15:55:06 - INFO - __main__ - Global step 1450 Train loss 3.88 ACC 0.0 on epoch=724
06/23/2022 15:55:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 3.83 on epoch=729
06/23/2022 15:55:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 3.70 on epoch=734
06/23/2022 15:55:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 3.78 on epoch=739
06/23/2022 15:55:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 3.74 on epoch=744
06/23/2022 15:55:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 3.67 on epoch=749
06/23/2022 15:55:15 - INFO - __main__ - Global step 1500 Train loss 3.74 ACC 0.0 on epoch=749
06/23/2022 15:55:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 3.63 on epoch=754
06/23/2022 15:55:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 3.75 on epoch=759
06/23/2022 15:55:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 3.54 on epoch=764
06/23/2022 15:55:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 3.47 on epoch=769
06/23/2022 15:55:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 3.60 on epoch=774
06/23/2022 15:55:31 - INFO - __main__ - Global step 1550 Train loss 3.60 ACC 0.03125 on epoch=774
06/23/2022 15:55:31 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=774, global_step=1550
06/23/2022 15:55:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 3.60 on epoch=779
06/23/2022 15:55:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 3.53 on epoch=784
06/23/2022 15:55:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 3.57 on epoch=789
06/23/2022 15:55:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 3.55 on epoch=794
06/23/2022 15:55:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 3.48 on epoch=799
06/23/2022 15:55:49 - INFO - __main__ - Global step 1600 Train loss 3.55 ACC 0.03125 on epoch=799
06/23/2022 15:55:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 3.44 on epoch=804
06/23/2022 15:55:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 3.44 on epoch=809
06/23/2022 15:55:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 3.46 on epoch=814
06/23/2022 15:55:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 3.39 on epoch=819
06/23/2022 15:55:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 3.31 on epoch=824
06/23/2022 15:56:03 - INFO - __main__ - Global step 1650 Train loss 3.41 ACC 0.0 on epoch=824
06/23/2022 15:56:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 3.27 on epoch=829
06/23/2022 15:56:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 3.24 on epoch=834
06/23/2022 15:56:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 3.20 on epoch=839
06/23/2022 15:56:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 3.18 on epoch=844
06/23/2022 15:56:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 3.24 on epoch=849
06/23/2022 15:56:22 - INFO - __main__ - Global step 1700 Train loss 3.23 ACC 0.28125 on epoch=849
06/23/2022 15:56:22 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.28125 on epoch=849, global_step=1700
06/23/2022 15:56:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 3.16 on epoch=854
06/23/2022 15:56:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 3.12 on epoch=859
06/23/2022 15:56:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 3.12 on epoch=864
06/23/2022 15:56:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 3.05 on epoch=869
06/23/2022 15:56:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 3.01 on epoch=874
06/23/2022 15:56:33 - INFO - __main__ - Global step 1750 Train loss 3.09 ACC 0.4375 on epoch=874
06/23/2022 15:56:33 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.4375 on epoch=874, global_step=1750
06/23/2022 15:56:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 2.92 on epoch=879
06/23/2022 15:56:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 2.88 on epoch=884
06/23/2022 15:56:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 2.85 on epoch=889
06/23/2022 15:56:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 2.85 on epoch=894
06/23/2022 15:56:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 2.74 on epoch=899
06/23/2022 15:56:43 - INFO - __main__ - Global step 1800 Train loss 2.85 ACC 0.5 on epoch=899
06/23/2022 15:56:43 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=899, global_step=1800
06/23/2022 15:56:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 2.67 on epoch=904
06/23/2022 15:56:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 2.77 on epoch=909
06/23/2022 15:56:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 2.70 on epoch=914
06/23/2022 15:56:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 2.63 on epoch=919
06/23/2022 15:56:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 2.57 on epoch=924
06/23/2022 15:57:03 - INFO - __main__ - Global step 1850 Train loss 2.67 ACC 0.4375 on epoch=924
06/23/2022 15:57:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 2.54 on epoch=929
06/23/2022 15:57:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 2.48 on epoch=934
06/23/2022 15:57:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 2.48 on epoch=939
06/23/2022 15:57:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 2.47 on epoch=944
06/23/2022 15:57:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 2.26 on epoch=949
06/23/2022 15:57:12 - INFO - __main__ - Global step 1900 Train loss 2.45 ACC 0.5 on epoch=949
06/23/2022 15:57:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 2.44 on epoch=954
06/23/2022 15:57:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 2.34 on epoch=959
06/23/2022 15:57:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 2.24 on epoch=964
06/23/2022 15:57:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 2.29 on epoch=969
06/23/2022 15:57:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 2.13 on epoch=974
06/23/2022 15:57:21 - INFO - __main__ - Global step 1950 Train loss 2.29 ACC 0.5 on epoch=974
06/23/2022 15:57:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 2.15 on epoch=979
06/23/2022 15:57:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 2.18 on epoch=984
06/23/2022 15:57:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 2.09 on epoch=989
06/23/2022 15:57:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 2.11 on epoch=994
06/23/2022 15:57:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 2.06 on epoch=999
06/23/2022 15:57:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:57:30 - INFO - __main__ - Printing 3 examples
06/23/2022 15:57:30 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/23/2022 15:57:30 - INFO - __main__ - ['equivalent']
06/23/2022 15:57:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/23/2022 15:57:30 - INFO - __main__ - ['equivalent']
06/23/2022 15:57:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/23/2022 15:57:30 - INFO - __main__ - ['equivalent']
06/23/2022 15:57:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 15:57:30 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:57:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 15:57:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:57:30 - INFO - __main__ - Printing 3 examples
06/23/2022 15:57:30 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/23/2022 15:57:30 - INFO - __main__ - ['equivalent']
06/23/2022 15:57:30 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/23/2022 15:57:30 - INFO - __main__ - ['equivalent']
06/23/2022 15:57:30 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/23/2022 15:57:30 - INFO - __main__ - ['equivalent']
06/23/2022 15:57:30 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:57:30 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:57:30 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 15:57:31 - INFO - __main__ - Global step 2000 Train loss 2.12 ACC 0.5 on epoch=999
06/23/2022 15:57:31 - INFO - __main__ - save last model!
06/23/2022 15:57:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 15:57:31 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 15:57:31 - INFO - __main__ - Printing 3 examples
06/23/2022 15:57:31 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 15:57:31 - INFO - __main__ - ['equivalent']
06/23/2022 15:57:31 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 15:57:31 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:57:31 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 15:57:31 - INFO - __main__ - ['not_equivalent']
06/23/2022 15:57:31 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:57:31 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:57:32 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 15:57:37 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 15:57:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 15:57:37 - INFO - __main__ - Starting training!
06/23/2022 15:58:04 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_100_0.2_8_predictions.txt
06/23/2022 15:58:04 - INFO - __main__ - ACC on test data: 0.6667
06/23/2022 15:58:04 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.6666666666666666
06/23/2022 15:58:04 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.5, bsz=8 ...
06/23/2022 15:58:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:58:05 - INFO - __main__ - Printing 3 examples
06/23/2022 15:58:05 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/23/2022 15:58:05 - INFO - __main__ - ['equivalent']
06/23/2022 15:58:05 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/23/2022 15:58:05 - INFO - __main__ - ['equivalent']
06/23/2022 15:58:05 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/23/2022 15:58:05 - INFO - __main__ - ['equivalent']
06/23/2022 15:58:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 15:58:05 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:58:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 15:58:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 15:58:05 - INFO - __main__ - Printing 3 examples
06/23/2022 15:58:05 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/23/2022 15:58:05 - INFO - __main__ - ['equivalent']
06/23/2022 15:58:05 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/23/2022 15:58:05 - INFO - __main__ - ['equivalent']
06/23/2022 15:58:05 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/23/2022 15:58:05 - INFO - __main__ - ['equivalent']
06/23/2022 15:58:05 - INFO - __main__ - Tokenizing Input ...
06/23/2022 15:58:05 - INFO - __main__ - Tokenizing Output ...
06/23/2022 15:58:05 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 15:58:12 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 15:58:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 15:58:12 - INFO - __main__ - Starting training!
06/23/2022 15:58:14 - INFO - __main__ - Step 10 Global step 10 Train loss 6.92 on epoch=4
06/23/2022 15:58:15 - INFO - __main__ - Step 20 Global step 20 Train loss 6.90 on epoch=9
06/23/2022 15:58:16 - INFO - __main__ - Step 30 Global step 30 Train loss 6.86 on epoch=14
06/23/2022 15:58:18 - INFO - __main__ - Step 40 Global step 40 Train loss 6.83 on epoch=19
06/23/2022 15:58:19 - INFO - __main__ - Step 50 Global step 50 Train loss 6.72 on epoch=24
06/23/2022 15:58:21 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/23/2022 15:58:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 15:58:23 - INFO - __main__ - Step 60 Global step 60 Train loss 6.85 on epoch=29
06/23/2022 15:58:24 - INFO - __main__ - Step 70 Global step 70 Train loss 6.74 on epoch=34
06/23/2022 15:58:26 - INFO - __main__ - Step 80 Global step 80 Train loss 6.75 on epoch=39
06/23/2022 15:58:27 - INFO - __main__ - Step 90 Global step 90 Train loss 6.66 on epoch=44
06/23/2022 15:58:28 - INFO - __main__ - Step 100 Global step 100 Train loss 6.73 on epoch=49
06/23/2022 15:58:32 - INFO - __main__ - Global step 100 Train loss 6.75 ACC 0.0 on epoch=49
06/23/2022 15:58:33 - INFO - __main__ - Step 110 Global step 110 Train loss 6.64 on epoch=54
06/23/2022 15:58:34 - INFO - __main__ - Step 120 Global step 120 Train loss 6.63 on epoch=59
06/23/2022 15:58:36 - INFO - __main__ - Step 130 Global step 130 Train loss 6.62 on epoch=64
06/23/2022 15:58:37 - INFO - __main__ - Step 140 Global step 140 Train loss 6.53 on epoch=69
06/23/2022 15:58:38 - INFO - __main__ - Step 150 Global step 150 Train loss 6.67 on epoch=74
06/23/2022 15:58:40 - INFO - __main__ - Global step 150 Train loss 6.62 ACC 0.0 on epoch=74
06/23/2022 15:58:41 - INFO - __main__ - Step 160 Global step 160 Train loss 6.60 on epoch=79
06/23/2022 15:58:43 - INFO - __main__ - Step 170 Global step 170 Train loss 6.50 on epoch=84
06/23/2022 15:58:44 - INFO - __main__ - Step 180 Global step 180 Train loss 6.51 on epoch=89
06/23/2022 15:58:45 - INFO - __main__ - Step 190 Global step 190 Train loss 6.48 on epoch=94
06/23/2022 15:58:47 - INFO - __main__ - Step 200 Global step 200 Train loss 6.40 on epoch=99
06/23/2022 15:58:57 - INFO - __main__ - Global step 200 Train loss 6.50 ACC 0.0 on epoch=99
06/23/2022 15:58:58 - INFO - __main__ - Step 210 Global step 210 Train loss 6.30 on epoch=104
06/23/2022 15:59:00 - INFO - __main__ - Step 220 Global step 220 Train loss 6.32 on epoch=109
06/23/2022 15:59:01 - INFO - __main__ - Step 230 Global step 230 Train loss 6.22 on epoch=114
06/23/2022 15:59:02 - INFO - __main__ - Step 240 Global step 240 Train loss 6.15 on epoch=119
06/23/2022 15:59:04 - INFO - __main__ - Step 250 Global step 250 Train loss 6.07 on epoch=124
06/23/2022 15:59:10 - INFO - __main__ - Global step 250 Train loss 6.21 ACC 0.0 on epoch=124
06/23/2022 15:59:12 - INFO - __main__ - Step 260 Global step 260 Train loss 5.94 on epoch=129
06/23/2022 15:59:13 - INFO - __main__ - Step 270 Global step 270 Train loss 5.86 on epoch=134
06/23/2022 15:59:14 - INFO - __main__ - Step 280 Global step 280 Train loss 5.80 on epoch=139
06/23/2022 15:59:16 - INFO - __main__ - Step 290 Global step 290 Train loss 5.60 on epoch=144
06/23/2022 15:59:17 - INFO - __main__ - Step 300 Global step 300 Train loss 5.36 on epoch=149
06/23/2022 15:59:23 - INFO - __main__ - Global step 300 Train loss 5.71 ACC 0.0 on epoch=149
06/23/2022 15:59:24 - INFO - __main__ - Step 310 Global step 310 Train loss 5.34 on epoch=154
06/23/2022 15:59:26 - INFO - __main__ - Step 320 Global step 320 Train loss 5.25 on epoch=159
06/23/2022 15:59:27 - INFO - __main__ - Step 330 Global step 330 Train loss 5.16 on epoch=164
06/23/2022 15:59:28 - INFO - __main__ - Step 340 Global step 340 Train loss 5.10 on epoch=169
06/23/2022 15:59:30 - INFO - __main__ - Step 350 Global step 350 Train loss 4.91 on epoch=174
06/23/2022 15:59:31 - INFO - __main__ - Global step 350 Train loss 5.15 ACC 0.0 on epoch=174
06/23/2022 15:59:33 - INFO - __main__ - Step 360 Global step 360 Train loss 4.84 on epoch=179
06/23/2022 15:59:34 - INFO - __main__ - Step 370 Global step 370 Train loss 4.69 on epoch=184
06/23/2022 15:59:35 - INFO - __main__ - Step 380 Global step 380 Train loss 4.67 on epoch=189
06/23/2022 15:59:37 - INFO - __main__ - Step 390 Global step 390 Train loss 4.62 on epoch=194
06/23/2022 15:59:38 - INFO - __main__ - Step 400 Global step 400 Train loss 4.53 on epoch=199
06/23/2022 15:59:39 - INFO - __main__ - Global step 400 Train loss 4.67 ACC 0.0 on epoch=199
06/23/2022 15:59:41 - INFO - __main__ - Step 410 Global step 410 Train loss 4.48 on epoch=204
06/23/2022 15:59:42 - INFO - __main__ - Step 420 Global step 420 Train loss 4.41 on epoch=209
06/23/2022 15:59:43 - INFO - __main__ - Step 430 Global step 430 Train loss 4.31 on epoch=214
06/23/2022 15:59:45 - INFO - __main__ - Step 440 Global step 440 Train loss 4.31 on epoch=219
06/23/2022 15:59:46 - INFO - __main__ - Step 450 Global step 450 Train loss 4.13 on epoch=224
06/23/2022 15:59:48 - INFO - __main__ - Global step 450 Train loss 4.33 ACC 0.0 on epoch=224
06/23/2022 15:59:49 - INFO - __main__ - Step 460 Global step 460 Train loss 4.23 on epoch=229
06/23/2022 15:59:50 - INFO - __main__ - Step 470 Global step 470 Train loss 4.15 on epoch=234
06/23/2022 15:59:52 - INFO - __main__ - Step 480 Global step 480 Train loss 3.93 on epoch=239
06/23/2022 15:59:53 - INFO - __main__ - Step 490 Global step 490 Train loss 3.83 on epoch=244
06/23/2022 15:59:55 - INFO - __main__ - Step 500 Global step 500 Train loss 3.91 on epoch=249
06/23/2022 15:59:56 - INFO - __main__ - Global step 500 Train loss 4.01 ACC 0.0 on epoch=249
06/23/2022 15:59:57 - INFO - __main__ - Step 510 Global step 510 Train loss 3.77 on epoch=254
06/23/2022 15:59:58 - INFO - __main__ - Step 520 Global step 520 Train loss 3.62 on epoch=259
06/23/2022 16:00:00 - INFO - __main__ - Step 530 Global step 530 Train loss 3.67 on epoch=264
06/23/2022 16:00:01 - INFO - __main__ - Step 540 Global step 540 Train loss 3.43 on epoch=269
06/23/2022 16:00:03 - INFO - __main__ - Step 550 Global step 550 Train loss 3.33 on epoch=274
06/23/2022 16:00:04 - INFO - __main__ - Global step 550 Train loss 3.56 ACC 0.0 on epoch=274
06/23/2022 16:00:06 - INFO - __main__ - Step 560 Global step 560 Train loss 3.38 on epoch=279
06/23/2022 16:00:07 - INFO - __main__ - Step 570 Global step 570 Train loss 3.47 on epoch=284
06/23/2022 16:00:09 - INFO - __main__ - Step 580 Global step 580 Train loss 3.27 on epoch=289
06/23/2022 16:00:10 - INFO - __main__ - Step 590 Global step 590 Train loss 3.20 on epoch=294
06/23/2022 16:00:11 - INFO - __main__ - Step 600 Global step 600 Train loss 3.15 on epoch=299
06/23/2022 16:00:13 - INFO - __main__ - Global step 600 Train loss 3.29 ACC 0.0 on epoch=299
06/23/2022 16:00:15 - INFO - __main__ - Step 610 Global step 610 Train loss 3.08 on epoch=304
06/23/2022 16:00:16 - INFO - __main__ - Step 620 Global step 620 Train loss 3.01 on epoch=309
06/23/2022 16:00:17 - INFO - __main__ - Step 630 Global step 630 Train loss 3.09 on epoch=314
06/23/2022 16:00:19 - INFO - __main__ - Step 640 Global step 640 Train loss 3.04 on epoch=319
06/23/2022 16:00:20 - INFO - __main__ - Step 650 Global step 650 Train loss 3.01 on epoch=324
06/23/2022 16:00:22 - INFO - __main__ - Global step 650 Train loss 3.05 ACC 0.0 on epoch=324
06/23/2022 16:00:23 - INFO - __main__ - Step 660 Global step 660 Train loss 2.91 on epoch=329
06/23/2022 16:00:25 - INFO - __main__ - Step 670 Global step 670 Train loss 2.77 on epoch=334
06/23/2022 16:00:26 - INFO - __main__ - Step 680 Global step 680 Train loss 2.74 on epoch=339
06/23/2022 16:00:28 - INFO - __main__ - Step 690 Global step 690 Train loss 2.78 on epoch=344
06/23/2022 16:00:29 - INFO - __main__ - Step 700 Global step 700 Train loss 2.71 on epoch=349
06/23/2022 16:00:32 - INFO - __main__ - Global step 700 Train loss 2.78 ACC 0.0 on epoch=349
06/23/2022 16:00:34 - INFO - __main__ - Step 710 Global step 710 Train loss 2.59 on epoch=354
06/23/2022 16:00:35 - INFO - __main__ - Step 720 Global step 720 Train loss 2.54 on epoch=359
06/23/2022 16:00:36 - INFO - __main__ - Step 730 Global step 730 Train loss 2.53 on epoch=364
06/23/2022 16:00:38 - INFO - __main__ - Step 740 Global step 740 Train loss 2.53 on epoch=369
06/23/2022 16:00:39 - INFO - __main__ - Step 750 Global step 750 Train loss 2.46 on epoch=374
06/23/2022 16:00:41 - INFO - __main__ - Global step 750 Train loss 2.53 ACC 0.0 on epoch=374
06/23/2022 16:00:43 - INFO - __main__ - Step 760 Global step 760 Train loss 2.39 on epoch=379
06/23/2022 16:00:44 - INFO - __main__ - Step 770 Global step 770 Train loss 2.27 on epoch=384
06/23/2022 16:00:45 - INFO - __main__ - Step 780 Global step 780 Train loss 2.23 on epoch=389
06/23/2022 16:00:47 - INFO - __main__ - Step 790 Global step 790 Train loss 2.30 on epoch=394
06/23/2022 16:00:48 - INFO - __main__ - Step 800 Global step 800 Train loss 2.15 on epoch=399
06/23/2022 16:00:49 - INFO - __main__ - Global step 800 Train loss 2.27 ACC 0.0 on epoch=399
06/23/2022 16:00:51 - INFO - __main__ - Step 810 Global step 810 Train loss 2.17 on epoch=404
06/23/2022 16:00:52 - INFO - __main__ - Step 820 Global step 820 Train loss 2.14 on epoch=409
06/23/2022 16:00:54 - INFO - __main__ - Step 830 Global step 830 Train loss 2.01 on epoch=414
06/23/2022 16:00:55 - INFO - __main__ - Step 840 Global step 840 Train loss 1.91 on epoch=419
06/23/2022 16:00:56 - INFO - __main__ - Step 850 Global step 850 Train loss 1.87 on epoch=424
06/23/2022 16:00:58 - INFO - __main__ - Global step 850 Train loss 2.02 ACC 0.28125 on epoch=424
06/23/2022 16:00:58 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.28125 on epoch=424, global_step=850
06/23/2022 16:01:00 - INFO - __main__ - Step 860 Global step 860 Train loss 1.85 on epoch=429
06/23/2022 16:01:01 - INFO - __main__ - Step 870 Global step 870 Train loss 1.96 on epoch=434
06/23/2022 16:01:02 - INFO - __main__ - Step 880 Global step 880 Train loss 1.79 on epoch=439
06/23/2022 16:01:04 - INFO - __main__ - Step 890 Global step 890 Train loss 1.84 on epoch=444
06/23/2022 16:01:05 - INFO - __main__ - Step 900 Global step 900 Train loss 1.78 on epoch=449
06/23/2022 16:01:08 - INFO - __main__ - Global step 900 Train loss 1.84 ACC 0.46875 on epoch=449
06/23/2022 16:01:08 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.46875 on epoch=449, global_step=900
06/23/2022 16:01:09 - INFO - __main__ - Step 910 Global step 910 Train loss 1.72 on epoch=454
06/23/2022 16:01:10 - INFO - __main__ - Step 920 Global step 920 Train loss 1.71 on epoch=459
06/23/2022 16:01:12 - INFO - __main__ - Step 930 Global step 930 Train loss 1.62 on epoch=464
06/23/2022 16:01:13 - INFO - __main__ - Step 940 Global step 940 Train loss 1.72 on epoch=469
06/23/2022 16:01:15 - INFO - __main__ - Step 950 Global step 950 Train loss 1.57 on epoch=474
06/23/2022 16:01:17 - INFO - __main__ - Global step 950 Train loss 1.67 ACC 0.5 on epoch=474
06/23/2022 16:01:17 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=474, global_step=950
06/23/2022 16:01:18 - INFO - __main__ - Step 960 Global step 960 Train loss 1.53 on epoch=479
06/23/2022 16:01:19 - INFO - __main__ - Step 970 Global step 970 Train loss 1.46 on epoch=484
06/23/2022 16:01:21 - INFO - __main__ - Step 980 Global step 980 Train loss 1.46 on epoch=489
06/23/2022 16:01:22 - INFO - __main__ - Step 990 Global step 990 Train loss 1.50 on epoch=494
06/23/2022 16:01:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.46 on epoch=499
06/23/2022 16:01:26 - INFO - __main__ - Global step 1000 Train loss 1.48 ACC 0.65625 on epoch=499
06/23/2022 16:01:26 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.65625 on epoch=499, global_step=1000
06/23/2022 16:01:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.37 on epoch=504
06/23/2022 16:01:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.40 on epoch=509
06/23/2022 16:01:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.38 on epoch=514
06/23/2022 16:01:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.46 on epoch=519
06/23/2022 16:01:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.28 on epoch=524
06/23/2022 16:01:38 - INFO - __main__ - Global step 1050 Train loss 1.38 ACC 0.4375 on epoch=524
06/23/2022 16:01:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.27 on epoch=529
06/23/2022 16:01:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.26 on epoch=534
06/23/2022 16:01:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.20 on epoch=539
06/23/2022 16:01:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.16 on epoch=544
06/23/2022 16:01:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.12 on epoch=549
06/23/2022 16:01:47 - INFO - __main__ - Global step 1100 Train loss 1.20 ACC 0.5 on epoch=549
06/23/2022 16:01:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.03 on epoch=554
06/23/2022 16:01:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.04 on epoch=559
06/23/2022 16:01:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.00 on epoch=564
06/23/2022 16:01:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.98 on epoch=569
06/23/2022 16:01:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.04 on epoch=574
06/23/2022 16:01:56 - INFO - __main__ - Global step 1150 Train loss 1.02 ACC 0.5 on epoch=574
06/23/2022 16:01:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.96 on epoch=579
06/23/2022 16:01:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.89 on epoch=584
06/23/2022 16:02:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.89 on epoch=589
06/23/2022 16:02:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.89 on epoch=594
06/23/2022 16:02:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.86 on epoch=599
06/23/2022 16:02:05 - INFO - __main__ - Global step 1200 Train loss 0.90 ACC 0.5 on epoch=599
06/23/2022 16:02:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.73 on epoch=604
06/23/2022 16:02:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.71 on epoch=609
06/23/2022 16:02:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.78 on epoch=614
06/23/2022 16:02:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=619
06/23/2022 16:02:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.76 on epoch=624
06/23/2022 16:02:14 - INFO - __main__ - Global step 1250 Train loss 0.76 ACC 0.5 on epoch=624
06/23/2022 16:02:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.77 on epoch=629
06/23/2022 16:02:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.70 on epoch=634
06/23/2022 16:02:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.69 on epoch=639
06/23/2022 16:02:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.68 on epoch=644
06/23/2022 16:02:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.65 on epoch=649
06/23/2022 16:02:22 - INFO - __main__ - Global step 1300 Train loss 0.70 ACC 0.5 on epoch=649
06/23/2022 16:02:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.52 on epoch=654
06/23/2022 16:02:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.59 on epoch=659
06/23/2022 16:02:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.64 on epoch=664
06/23/2022 16:02:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.57 on epoch=669
06/23/2022 16:02:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.60 on epoch=674
06/23/2022 16:02:31 - INFO - __main__ - Global step 1350 Train loss 0.58 ACC 0.5 on epoch=674
06/23/2022 16:02:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.69 on epoch=679
06/23/2022 16:02:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.54 on epoch=684
06/23/2022 16:02:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=689
06/23/2022 16:02:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.54 on epoch=694
06/23/2022 16:02:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.54 on epoch=699
06/23/2022 16:02:39 - INFO - __main__ - Global step 1400 Train loss 0.56 ACC 0.5 on epoch=699
06/23/2022 16:02:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.52 on epoch=704
06/23/2022 16:02:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=709
06/23/2022 16:02:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.53 on epoch=714
06/23/2022 16:02:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.57 on epoch=719
06/23/2022 16:02:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.49 on epoch=724
06/23/2022 16:02:46 - INFO - __main__ - Global step 1450 Train loss 0.51 ACC 0.5 on epoch=724
06/23/2022 16:02:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
06/23/2022 16:02:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.52 on epoch=734
06/23/2022 16:02:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
06/23/2022 16:02:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.50 on epoch=744
06/23/2022 16:02:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=749
06/23/2022 16:02:54 - INFO - __main__ - Global step 1500 Train loss 0.48 ACC 0.5 on epoch=749
06/23/2022 16:02:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
06/23/2022 16:02:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.46 on epoch=759
06/23/2022 16:02:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
06/23/2022 16:03:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=769
06/23/2022 16:03:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=774
06/23/2022 16:03:02 - INFO - __main__ - Global step 1550 Train loss 0.43 ACC 0.5 on epoch=774
06/23/2022 16:03:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.47 on epoch=779
06/23/2022 16:03:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=784
06/23/2022 16:03:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.47 on epoch=789
06/23/2022 16:03:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=794
06/23/2022 16:03:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=799
06/23/2022 16:03:09 - INFO - __main__ - Global step 1600 Train loss 0.44 ACC 0.5 on epoch=799
06/23/2022 16:03:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=804
06/23/2022 16:03:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.46 on epoch=809
06/23/2022 16:03:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=814
06/23/2022 16:03:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=819
06/23/2022 16:03:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.51 on epoch=824
06/23/2022 16:03:17 - INFO - __main__ - Global step 1650 Train loss 0.45 ACC 0.53125 on epoch=824
06/23/2022 16:03:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=829
06/23/2022 16:03:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=834
06/23/2022 16:03:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=839
06/23/2022 16:03:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.43 on epoch=844
06/23/2022 16:03:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=849
06/23/2022 16:03:25 - INFO - __main__ - Global step 1700 Train loss 0.43 ACC 0.46875 on epoch=849
06/23/2022 16:03:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
06/23/2022 16:03:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=859
06/23/2022 16:03:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=864
06/23/2022 16:03:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
06/23/2022 16:03:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
06/23/2022 16:03:34 - INFO - __main__ - Global step 1750 Train loss 0.40 ACC 0.5 on epoch=874
06/23/2022 16:03:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=879
06/23/2022 16:03:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=884
06/23/2022 16:03:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=889
06/23/2022 16:03:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=894
06/23/2022 16:03:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
06/23/2022 16:03:41 - INFO - __main__ - Global step 1800 Train loss 0.39 ACC 0.5 on epoch=899
06/23/2022 16:03:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=904
06/23/2022 16:03:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=909
06/23/2022 16:03:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=914
06/23/2022 16:03:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=919
06/23/2022 16:03:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=924
06/23/2022 16:03:49 - INFO - __main__ - Global step 1850 Train loss 0.40 ACC 0.5 on epoch=924
06/23/2022 16:03:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=929
06/23/2022 16:03:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
06/23/2022 16:03:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=939
06/23/2022 16:03:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=944
06/23/2022 16:03:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=949
06/23/2022 16:03:57 - INFO - __main__ - Global step 1900 Train loss 0.40 ACC 0.5 on epoch=949
06/23/2022 16:03:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=954
06/23/2022 16:03:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=959
06/23/2022 16:04:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=964
06/23/2022 16:04:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=969
06/23/2022 16:04:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=974
06/23/2022 16:04:04 - INFO - __main__ - Global step 1950 Train loss 0.35 ACC 0.5 on epoch=974
06/23/2022 16:04:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=979
06/23/2022 16:04:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.31 on epoch=984
06/23/2022 16:04:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=989
06/23/2022 16:04:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=994
06/23/2022 16:04:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=999
06/23/2022 16:04:12 - INFO - __main__ - Global step 2000 Train loss 0.35 ACC 0.5 on epoch=999
06/23/2022 16:04:12 - INFO - __main__ - save last model!
06/23/2022 16:04:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 16:04:12 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 16:04:12 - INFO - __main__ - Printing 3 examples
06/23/2022 16:04:12 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 16:04:12 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 16:04:12 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:04:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 16:04:12 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:04:12 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:04:12 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:04:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:04:12 - INFO - __main__ - Printing 3 examples
06/23/2022 16:04:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/23/2022 16:04:12 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/23/2022 16:04:12 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/23/2022 16:04:12 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 16:04:12 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:04:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:04:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:04:12 - INFO - __main__ - Printing 3 examples
06/23/2022 16:04:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/23/2022 16:04:12 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/23/2022 16:04:12 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:12 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/23/2022 16:04:12 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:12 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:04:12 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:04:12 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:04:12 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 16:04:18 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:04:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:04:19 - INFO - __main__ - Starting training!
06/23/2022 16:04:20 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_13_0.5_8_predictions.txt
06/23/2022 16:04:20 - INFO - __main__ - ACC on test data: 0.6863
06/23/2022 16:04:20 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.5, bsz=8, dev_performance=0.65625, test_performance=0.6862745098039216
06/23/2022 16:04:20 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.4, bsz=8 ...
06/23/2022 16:04:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:04:21 - INFO - __main__ - Printing 3 examples
06/23/2022 16:04:21 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/23/2022 16:04:21 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/23/2022 16:04:21 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/23/2022 16:04:21 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 16:04:21 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:04:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:04:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:04:21 - INFO - __main__ - Printing 3 examples
06/23/2022 16:04:21 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/23/2022 16:04:21 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:21 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/23/2022 16:04:21 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:21 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/23/2022 16:04:21 - INFO - __main__ - ['equivalent']
06/23/2022 16:04:21 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:04:21 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:04:21 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:04:28 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:04:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:04:28 - INFO - __main__ - Starting training!
06/23/2022 16:04:30 - INFO - __main__ - Step 10 Global step 10 Train loss 6.89 on epoch=4
06/23/2022 16:04:31 - INFO - __main__ - Step 20 Global step 20 Train loss 6.90 on epoch=9
06/23/2022 16:04:32 - INFO - __main__ - Step 30 Global step 30 Train loss 6.76 on epoch=14
06/23/2022 16:04:34 - INFO - __main__ - Step 40 Global step 40 Train loss 6.79 on epoch=19
06/23/2022 16:04:35 - INFO - __main__ - Step 50 Global step 50 Train loss 6.89 on epoch=24
06/23/2022 16:04:39 - INFO - __main__ - Global step 50 Train loss 6.84 ACC 0.0 on epoch=24
06/23/2022 16:04:39 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 16:04:40 - INFO - __main__ - Step 60 Global step 60 Train loss 6.83 on epoch=29
06/23/2022 16:04:42 - INFO - __main__ - Step 70 Global step 70 Train loss 6.82 on epoch=34
06/23/2022 16:04:43 - INFO - __main__ - Step 80 Global step 80 Train loss 6.85 on epoch=39
06/23/2022 16:04:45 - INFO - __main__ - Step 90 Global step 90 Train loss 6.81 on epoch=44
06/23/2022 16:04:46 - INFO - __main__ - Step 100 Global step 100 Train loss 6.68 on epoch=49
06/23/2022 16:04:48 - INFO - __main__ - Global step 100 Train loss 6.80 ACC 0.0 on epoch=49
06/23/2022 16:04:50 - INFO - __main__ - Step 110 Global step 110 Train loss 6.84 on epoch=54
06/23/2022 16:04:51 - INFO - __main__ - Step 120 Global step 120 Train loss 6.68 on epoch=59
06/23/2022 16:04:53 - INFO - __main__ - Step 130 Global step 130 Train loss 6.72 on epoch=64
06/23/2022 16:04:54 - INFO - __main__ - Step 140 Global step 140 Train loss 6.66 on epoch=69
06/23/2022 16:04:56 - INFO - __main__ - Step 150 Global step 150 Train loss 6.53 on epoch=74
06/23/2022 16:04:59 - INFO - __main__ - Global step 150 Train loss 6.68 ACC 0.0 on epoch=74
06/23/2022 16:05:00 - INFO - __main__ - Step 160 Global step 160 Train loss 6.60 on epoch=79
06/23/2022 16:05:02 - INFO - __main__ - Step 170 Global step 170 Train loss 6.43 on epoch=84
06/23/2022 16:05:03 - INFO - __main__ - Step 180 Global step 180 Train loss 6.47 on epoch=89
06/23/2022 16:05:04 - INFO - __main__ - Step 190 Global step 190 Train loss 6.30 on epoch=94
06/23/2022 16:05:06 - INFO - __main__ - Step 200 Global step 200 Train loss 6.20 on epoch=99
06/23/2022 16:05:13 - INFO - __main__ - Global step 200 Train loss 6.40 ACC 0.0 on epoch=99
06/23/2022 16:05:14 - INFO - __main__ - Step 210 Global step 210 Train loss 6.19 on epoch=104
06/23/2022 16:05:16 - INFO - __main__ - Step 220 Global step 220 Train loss 5.96 on epoch=109
06/23/2022 16:05:17 - INFO - __main__ - Step 230 Global step 230 Train loss 5.93 on epoch=114
06/23/2022 16:05:19 - INFO - __main__ - Step 240 Global step 240 Train loss 5.88 on epoch=119
06/23/2022 16:05:20 - INFO - __main__ - Step 250 Global step 250 Train loss 5.74 on epoch=124
06/23/2022 16:05:26 - INFO - __main__ - Global step 250 Train loss 5.94 ACC 0.0 on epoch=124
06/23/2022 16:05:27 - INFO - __main__ - Step 260 Global step 260 Train loss 5.84 on epoch=129
06/23/2022 16:05:28 - INFO - __main__ - Step 270 Global step 270 Train loss 5.68 on epoch=134
06/23/2022 16:05:30 - INFO - __main__ - Step 280 Global step 280 Train loss 5.64 on epoch=139
06/23/2022 16:05:31 - INFO - __main__ - Step 290 Global step 290 Train loss 5.65 on epoch=144
06/23/2022 16:05:32 - INFO - __main__ - Step 300 Global step 300 Train loss 5.53 on epoch=149
06/23/2022 16:05:43 - INFO - __main__ - Global step 300 Train loss 5.67 ACC 0.0 on epoch=149
06/23/2022 16:05:44 - INFO - __main__ - Step 310 Global step 310 Train loss 5.52 on epoch=154
06/23/2022 16:05:46 - INFO - __main__ - Step 320 Global step 320 Train loss 5.46 on epoch=159
06/23/2022 16:05:47 - INFO - __main__ - Step 330 Global step 330 Train loss 5.26 on epoch=164
06/23/2022 16:05:48 - INFO - __main__ - Step 340 Global step 340 Train loss 5.21 on epoch=169
06/23/2022 16:05:49 - INFO - __main__ - Step 350 Global step 350 Train loss 5.09 on epoch=174
06/23/2022 16:05:51 - INFO - __main__ - Global step 350 Train loss 5.31 ACC 0.0 on epoch=174
06/23/2022 16:05:52 - INFO - __main__ - Step 360 Global step 360 Train loss 5.15 on epoch=179
06/23/2022 16:05:54 - INFO - __main__ - Step 370 Global step 370 Train loss 5.13 on epoch=184
06/23/2022 16:05:55 - INFO - __main__ - Step 380 Global step 380 Train loss 5.00 on epoch=189
06/23/2022 16:05:56 - INFO - __main__ - Step 390 Global step 390 Train loss 4.94 on epoch=194
06/23/2022 16:05:58 - INFO - __main__ - Step 400 Global step 400 Train loss 4.84 on epoch=199
06/23/2022 16:06:00 - INFO - __main__ - Global step 400 Train loss 5.01 ACC 0.0 on epoch=199
06/23/2022 16:06:01 - INFO - __main__ - Step 410 Global step 410 Train loss 4.79 on epoch=204
06/23/2022 16:06:02 - INFO - __main__ - Step 420 Global step 420 Train loss 4.64 on epoch=209
06/23/2022 16:06:04 - INFO - __main__ - Step 430 Global step 430 Train loss 4.54 on epoch=214
06/23/2022 16:06:05 - INFO - __main__ - Step 440 Global step 440 Train loss 4.39 on epoch=219
06/23/2022 16:06:06 - INFO - __main__ - Step 450 Global step 450 Train loss 4.23 on epoch=224
06/23/2022 16:06:07 - INFO - __main__ - Global step 450 Train loss 4.52 ACC 0.0 on epoch=224
06/23/2022 16:06:09 - INFO - __main__ - Step 460 Global step 460 Train loss 4.17 on epoch=229
06/23/2022 16:06:10 - INFO - __main__ - Step 470 Global step 470 Train loss 4.14 on epoch=234
06/23/2022 16:06:11 - INFO - __main__ - Step 480 Global step 480 Train loss 4.05 on epoch=239
06/23/2022 16:06:13 - INFO - __main__ - Step 490 Global step 490 Train loss 3.94 on epoch=244
06/23/2022 16:06:14 - INFO - __main__ - Step 500 Global step 500 Train loss 3.80 on epoch=249
06/23/2022 16:06:17 - INFO - __main__ - Global step 500 Train loss 4.02 ACC 0.0 on epoch=249
06/23/2022 16:06:18 - INFO - __main__ - Step 510 Global step 510 Train loss 3.69 on epoch=254
06/23/2022 16:06:20 - INFO - __main__ - Step 520 Global step 520 Train loss 3.54 on epoch=259
06/23/2022 16:06:21 - INFO - __main__ - Step 530 Global step 530 Train loss 3.54 on epoch=264
06/23/2022 16:06:22 - INFO - __main__ - Step 540 Global step 540 Train loss 3.32 on epoch=269
06/23/2022 16:06:24 - INFO - __main__ - Step 550 Global step 550 Train loss 3.35 on epoch=274
06/23/2022 16:06:31 - INFO - __main__ - Global step 550 Train loss 3.49 ACC 0.0 on epoch=274
06/23/2022 16:06:33 - INFO - __main__ - Step 560 Global step 560 Train loss 3.26 on epoch=279
06/23/2022 16:06:34 - INFO - __main__ - Step 570 Global step 570 Train loss 3.06 on epoch=284
06/23/2022 16:06:36 - INFO - __main__ - Step 580 Global step 580 Train loss 3.02 on epoch=289
06/23/2022 16:06:37 - INFO - __main__ - Step 590 Global step 590 Train loss 2.79 on epoch=294
06/23/2022 16:06:38 - INFO - __main__ - Step 600 Global step 600 Train loss 2.84 on epoch=299
06/23/2022 16:06:41 - INFO - __main__ - Global step 600 Train loss 2.99 ACC 0.09375 on epoch=299
06/23/2022 16:06:41 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=299, global_step=600
06/23/2022 16:06:42 - INFO - __main__ - Step 610 Global step 610 Train loss 2.67 on epoch=304
06/23/2022 16:06:43 - INFO - __main__ - Step 620 Global step 620 Train loss 2.61 on epoch=309
06/23/2022 16:06:45 - INFO - __main__ - Step 630 Global step 630 Train loss 2.50 on epoch=314
06/23/2022 16:06:46 - INFO - __main__ - Step 640 Global step 640 Train loss 2.35 on epoch=319
06/23/2022 16:06:47 - INFO - __main__ - Step 650 Global step 650 Train loss 2.41 on epoch=324
06/23/2022 16:06:50 - INFO - __main__ - Global step 650 Train loss 2.51 ACC 0.5 on epoch=324
06/23/2022 16:06:50 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.5 on epoch=324, global_step=650
06/23/2022 16:06:51 - INFO - __main__ - Step 660 Global step 660 Train loss 2.28 on epoch=329
06/23/2022 16:06:53 - INFO - __main__ - Step 670 Global step 670 Train loss 2.17 on epoch=334
06/23/2022 16:06:54 - INFO - __main__ - Step 680 Global step 680 Train loss 2.20 on epoch=339
06/23/2022 16:06:55 - INFO - __main__ - Step 690 Global step 690 Train loss 2.10 on epoch=344
06/23/2022 16:06:57 - INFO - __main__ - Step 700 Global step 700 Train loss 1.85 on epoch=349
06/23/2022 16:07:04 - INFO - __main__ - Global step 700 Train loss 2.12 ACC 0.5 on epoch=349
06/23/2022 16:07:06 - INFO - __main__ - Step 710 Global step 710 Train loss 1.93 on epoch=354
06/23/2022 16:07:07 - INFO - __main__ - Step 720 Global step 720 Train loss 1.82 on epoch=359
06/23/2022 16:07:08 - INFO - __main__ - Step 730 Global step 730 Train loss 1.70 on epoch=364
06/23/2022 16:07:10 - INFO - __main__ - Step 740 Global step 740 Train loss 1.78 on epoch=369
06/23/2022 16:07:11 - INFO - __main__ - Step 750 Global step 750 Train loss 1.43 on epoch=374
06/23/2022 16:07:17 - INFO - __main__ - Global step 750 Train loss 1.73 ACC 0.5 on epoch=374
06/23/2022 16:07:19 - INFO - __main__ - Step 760 Global step 760 Train loss 1.50 on epoch=379
06/23/2022 16:07:20 - INFO - __main__ - Step 770 Global step 770 Train loss 1.42 on epoch=384
06/23/2022 16:07:21 - INFO - __main__ - Step 780 Global step 780 Train loss 1.40 on epoch=389
06/23/2022 16:07:23 - INFO - __main__ - Step 790 Global step 790 Train loss 1.32 on epoch=394
06/23/2022 16:07:24 - INFO - __main__ - Step 800 Global step 800 Train loss 1.27 on epoch=399
06/23/2022 16:07:26 - INFO - __main__ - Global step 800 Train loss 1.38 ACC 0.53125 on epoch=399
06/23/2022 16:07:26 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=399, global_step=800
06/23/2022 16:07:27 - INFO - __main__ - Step 810 Global step 810 Train loss 1.23 on epoch=404
06/23/2022 16:07:29 - INFO - __main__ - Step 820 Global step 820 Train loss 1.23 on epoch=409
06/23/2022 16:07:30 - INFO - __main__ - Step 830 Global step 830 Train loss 1.11 on epoch=414
06/23/2022 16:07:32 - INFO - __main__ - Step 840 Global step 840 Train loss 1.13 on epoch=419
06/23/2022 16:07:33 - INFO - __main__ - Step 850 Global step 850 Train loss 1.14 on epoch=424
06/23/2022 16:07:35 - INFO - __main__ - Global step 850 Train loss 1.17 ACC 0.5 on epoch=424
06/23/2022 16:07:36 - INFO - __main__ - Step 860 Global step 860 Train loss 1.10 on epoch=429
06/23/2022 16:07:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.99 on epoch=434
06/23/2022 16:07:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.97 on epoch=439
06/23/2022 16:07:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.94 on epoch=444
06/23/2022 16:07:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.90 on epoch=449
06/23/2022 16:07:48 - INFO - __main__ - Global step 900 Train loss 0.98 ACC 0.5625 on epoch=449
06/23/2022 16:07:48 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=449, global_step=900
06/23/2022 16:07:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.86 on epoch=454
06/23/2022 16:07:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.77 on epoch=459
06/23/2022 16:07:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.78 on epoch=464
06/23/2022 16:07:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.81 on epoch=469
06/23/2022 16:07:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=474
06/23/2022 16:07:56 - INFO - __main__ - Global step 950 Train loss 0.81 ACC 0.5 on epoch=474
06/23/2022 16:07:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.72 on epoch=479
06/23/2022 16:07:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.70 on epoch=484
06/23/2022 16:08:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.76 on epoch=489
06/23/2022 16:08:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.66 on epoch=494
06/23/2022 16:08:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.64 on epoch=499
06/23/2022 16:08:04 - INFO - __main__ - Global step 1000 Train loss 0.70 ACC 0.5 on epoch=499
06/23/2022 16:08:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.70 on epoch=504
06/23/2022 16:08:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.62 on epoch=509
06/23/2022 16:08:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.54 on epoch=514
06/23/2022 16:08:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.66 on epoch=519
06/23/2022 16:08:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.54 on epoch=524
06/23/2022 16:08:12 - INFO - __main__ - Global step 1050 Train loss 0.61 ACC 0.5 on epoch=524
06/23/2022 16:08:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.58 on epoch=529
06/23/2022 16:08:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=534
06/23/2022 16:08:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.59 on epoch=539
06/23/2022 16:08:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.54 on epoch=544
06/23/2022 16:08:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.54 on epoch=549
06/23/2022 16:08:19 - INFO - __main__ - Global step 1100 Train loss 0.56 ACC 0.65625 on epoch=549
06/23/2022 16:08:19 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.65625 on epoch=549, global_step=1100
06/23/2022 16:08:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.62 on epoch=554
06/23/2022 16:08:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.60 on epoch=559
06/23/2022 16:08:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.56 on epoch=564
06/23/2022 16:08:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.56 on epoch=569
06/23/2022 16:08:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.56 on epoch=574
06/23/2022 16:08:27 - INFO - __main__ - Global step 1150 Train loss 0.58 ACC 0.53125 on epoch=574
06/23/2022 16:08:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=579
06/23/2022 16:08:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.51 on epoch=584
06/23/2022 16:08:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.51 on epoch=589
06/23/2022 16:08:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.52 on epoch=594
06/23/2022 16:08:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.56 on epoch=599
06/23/2022 16:08:35 - INFO - __main__ - Global step 1200 Train loss 0.51 ACC 0.53125 on epoch=599
06/23/2022 16:08:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=604
06/23/2022 16:08:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.53 on epoch=609
06/23/2022 16:08:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=614
06/23/2022 16:08:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.56 on epoch=619
06/23/2022 16:08:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.52 on epoch=624
06/23/2022 16:08:42 - INFO - __main__ - Global step 1250 Train loss 0.52 ACC 0.5 on epoch=624
06/23/2022 16:08:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=629
06/23/2022 16:08:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.48 on epoch=634
06/23/2022 16:08:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.55 on epoch=639
06/23/2022 16:08:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.52 on epoch=644
06/23/2022 16:08:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=649
06/23/2022 16:08:50 - INFO - __main__ - Global step 1300 Train loss 0.49 ACC 0.5 on epoch=649
06/23/2022 16:08:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=654
06/23/2022 16:08:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=659
06/23/2022 16:08:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=664
06/23/2022 16:08:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=669
06/23/2022 16:08:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=674
06/23/2022 16:08:57 - INFO - __main__ - Global step 1350 Train loss 0.47 ACC 0.5 on epoch=674
06/23/2022 16:08:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=679
06/23/2022 16:09:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=684
06/23/2022 16:09:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=689
06/23/2022 16:09:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=694
06/23/2022 16:09:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.45 on epoch=699
06/23/2022 16:09:05 - INFO - __main__ - Global step 1400 Train loss 0.41 ACC 0.53125 on epoch=699
06/23/2022 16:09:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=704
06/23/2022 16:09:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=709
06/23/2022 16:09:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
06/23/2022 16:09:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=719
06/23/2022 16:09:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=724
06/23/2022 16:09:13 - INFO - __main__ - Global step 1450 Train loss 0.39 ACC 0.5 on epoch=724
06/23/2022 16:09:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=729
06/23/2022 16:09:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.48 on epoch=734
06/23/2022 16:09:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=739
06/23/2022 16:09:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.49 on epoch=744
06/23/2022 16:09:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=749
06/23/2022 16:09:21 - INFO - __main__ - Global step 1500 Train loss 0.45 ACC 0.5 on epoch=749
06/23/2022 16:09:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
06/23/2022 16:09:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.52 on epoch=759
06/23/2022 16:09:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=764
06/23/2022 16:09:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=769
06/23/2022 16:09:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=774
06/23/2022 16:09:28 - INFO - __main__ - Global step 1550 Train loss 0.44 ACC 0.53125 on epoch=774
06/23/2022 16:09:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=779
06/23/2022 16:09:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
06/23/2022 16:09:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=789
06/23/2022 16:09:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=794
06/23/2022 16:09:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
06/23/2022 16:09:36 - INFO - __main__ - Global step 1600 Train loss 0.39 ACC 0.34375 on epoch=799
06/23/2022 16:09:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.44 on epoch=804
06/23/2022 16:09:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=809
06/23/2022 16:09:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.36 on epoch=814
06/23/2022 16:09:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=819
06/23/2022 16:09:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=824
06/23/2022 16:09:44 - INFO - __main__ - Global step 1650 Train loss 0.39 ACC 0.53125 on epoch=824
06/23/2022 16:09:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=829
06/23/2022 16:09:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=834
06/23/2022 16:09:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=839
06/23/2022 16:09:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=844
06/23/2022 16:09:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=849
06/23/2022 16:09:51 - INFO - __main__ - Global step 1700 Train loss 0.41 ACC 0.5 on epoch=849
06/23/2022 16:09:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=854
06/23/2022 16:09:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=859
06/23/2022 16:09:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=864
06/23/2022 16:09:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=869
06/23/2022 16:09:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=874
06/23/2022 16:09:59 - INFO - __main__ - Global step 1750 Train loss 0.42 ACC 0.5 on epoch=874
06/23/2022 16:10:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=879
06/23/2022 16:10:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=884
06/23/2022 16:10:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
06/23/2022 16:10:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
06/23/2022 16:10:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=899
06/23/2022 16:10:07 - INFO - __main__ - Global step 1800 Train loss 0.37 ACC 0.5 on epoch=899
06/23/2022 16:10:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=904
06/23/2022 16:10:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=909
06/23/2022 16:10:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=914
06/23/2022 16:10:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=919
06/23/2022 16:10:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
06/23/2022 16:10:14 - INFO - __main__ - Global step 1850 Train loss 0.38 ACC 0.4375 on epoch=924
06/23/2022 16:10:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=929
06/23/2022 16:10:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=934
06/23/2022 16:10:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=939
06/23/2022 16:10:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=944
06/23/2022 16:10:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
06/23/2022 16:10:22 - INFO - __main__ - Global step 1900 Train loss 0.37 ACC 0.4375 on epoch=949
06/23/2022 16:10:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
06/23/2022 16:10:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=959
06/23/2022 16:10:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=964
06/23/2022 16:10:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=969
06/23/2022 16:10:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
06/23/2022 16:10:30 - INFO - __main__ - Global step 1950 Train loss 0.37 ACC 0.4375 on epoch=974
06/23/2022 16:10:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=979
06/23/2022 16:10:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=984
06/23/2022 16:10:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=989
06/23/2022 16:10:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
06/23/2022 16:10:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
06/23/2022 16:10:37 - INFO - __main__ - Global step 2000 Train loss 0.35 ACC 0.59375 on epoch=999
06/23/2022 16:10:37 - INFO - __main__ - save last model!
06/23/2022 16:10:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 16:10:38 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 16:10:38 - INFO - __main__ - Printing 3 examples
06/23/2022 16:10:38 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 16:10:38 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:38 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 16:10:38 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:10:38 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 16:10:38 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:10:38 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:10:38 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:10:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:10:38 - INFO - __main__ - Printing 3 examples
06/23/2022 16:10:38 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/23/2022 16:10:38 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:38 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/23/2022 16:10:38 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:38 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/23/2022 16:10:38 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 16:10:38 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:10:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:10:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:10:38 - INFO - __main__ - Printing 3 examples
06/23/2022 16:10:38 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/23/2022 16:10:38 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:38 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/23/2022 16:10:38 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:38 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/23/2022 16:10:38 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:38 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:10:38 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:10:38 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:10:39 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 16:10:44 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:10:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:10:44 - INFO - __main__ - Starting training!
06/23/2022 16:10:48 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_13_0.4_8_predictions.txt
06/23/2022 16:10:48 - INFO - __main__ - ACC on test data: 0.5735
06/23/2022 16:10:48 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.4, bsz=8, dev_performance=0.65625, test_performance=0.5735294117647058
06/23/2022 16:10:48 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.3, bsz=8 ...
06/23/2022 16:10:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:10:49 - INFO - __main__ - Printing 3 examples
06/23/2022 16:10:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/23/2022 16:10:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/23/2022 16:10:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/23/2022 16:10:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 16:10:49 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:10:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:10:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:10:49 - INFO - __main__ - Printing 3 examples
06/23/2022 16:10:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/23/2022 16:10:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/23/2022 16:10:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:49 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/23/2022 16:10:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:10:49 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:10:49 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:10:49 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:10:55 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:10:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:10:56 - INFO - __main__ - Starting training!
06/23/2022 16:10:57 - INFO - __main__ - Step 10 Global step 10 Train loss 6.85 on epoch=4
06/23/2022 16:10:59 - INFO - __main__ - Step 20 Global step 20 Train loss 6.91 on epoch=9
06/23/2022 16:11:00 - INFO - __main__ - Step 30 Global step 30 Train loss 6.90 on epoch=14
06/23/2022 16:11:01 - INFO - __main__ - Step 40 Global step 40 Train loss 6.87 on epoch=19
06/23/2022 16:11:03 - INFO - __main__ - Step 50 Global step 50 Train loss 6.82 on epoch=24
06/23/2022 16:11:06 - INFO - __main__ - Global step 50 Train loss 6.87 ACC 0.0 on epoch=24
06/23/2022 16:11:06 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 16:11:08 - INFO - __main__ - Step 60 Global step 60 Train loss 6.79 on epoch=29
06/23/2022 16:11:09 - INFO - __main__ - Step 70 Global step 70 Train loss 6.87 on epoch=34
06/23/2022 16:11:11 - INFO - __main__ - Step 80 Global step 80 Train loss 6.72 on epoch=39
06/23/2022 16:11:12 - INFO - __main__ - Step 90 Global step 90 Train loss 6.79 on epoch=44
06/23/2022 16:11:13 - INFO - __main__ - Step 100 Global step 100 Train loss 6.74 on epoch=49
06/23/2022 16:11:14 - INFO - __main__ - Global step 100 Train loss 6.78 ACC 0.0 on epoch=49
06/23/2022 16:11:16 - INFO - __main__ - Step 110 Global step 110 Train loss 6.70 on epoch=54
06/23/2022 16:11:17 - INFO - __main__ - Step 120 Global step 120 Train loss 6.70 on epoch=59
06/23/2022 16:11:19 - INFO - __main__ - Step 130 Global step 130 Train loss 6.68 on epoch=64
06/23/2022 16:11:20 - INFO - __main__ - Step 140 Global step 140 Train loss 6.66 on epoch=69
06/23/2022 16:11:21 - INFO - __main__ - Step 150 Global step 150 Train loss 6.55 on epoch=74
06/23/2022 16:11:33 - INFO - __main__ - Global step 150 Train loss 6.66 ACC 0.0 on epoch=74
06/23/2022 16:11:35 - INFO - __main__ - Step 160 Global step 160 Train loss 6.59 on epoch=79
06/23/2022 16:11:36 - INFO - __main__ - Step 170 Global step 170 Train loss 6.54 on epoch=84
06/23/2022 16:11:38 - INFO - __main__ - Step 180 Global step 180 Train loss 6.48 on epoch=89
06/23/2022 16:11:40 - INFO - __main__ - Step 190 Global step 190 Train loss 6.50 on epoch=94
06/23/2022 16:11:41 - INFO - __main__ - Step 200 Global step 200 Train loss 6.35 on epoch=99
06/23/2022 16:11:50 - INFO - __main__ - Global step 200 Train loss 6.49 ACC 0.0 on epoch=99
06/23/2022 16:11:51 - INFO - __main__ - Step 210 Global step 210 Train loss 6.44 on epoch=104
06/23/2022 16:11:52 - INFO - __main__ - Step 220 Global step 220 Train loss 6.34 on epoch=109
06/23/2022 16:11:54 - INFO - __main__ - Step 230 Global step 230 Train loss 6.27 on epoch=114
06/23/2022 16:11:55 - INFO - __main__ - Step 240 Global step 240 Train loss 6.21 on epoch=119
06/23/2022 16:11:56 - INFO - __main__ - Step 250 Global step 250 Train loss 6.20 on epoch=124
06/23/2022 16:12:08 - INFO - __main__ - Global step 250 Train loss 6.29 ACC 0.0 on epoch=124
06/23/2022 16:12:10 - INFO - __main__ - Step 260 Global step 260 Train loss 6.20 on epoch=129
06/23/2022 16:12:11 - INFO - __main__ - Step 270 Global step 270 Train loss 6.17 on epoch=134
06/23/2022 16:12:12 - INFO - __main__ - Step 280 Global step 280 Train loss 6.28 on epoch=139
06/23/2022 16:12:14 - INFO - __main__ - Step 290 Global step 290 Train loss 6.13 on epoch=144
06/23/2022 16:12:15 - INFO - __main__ - Step 300 Global step 300 Train loss 6.17 on epoch=149
06/23/2022 16:12:22 - INFO - __main__ - Global step 300 Train loss 6.19 ACC 0.0 on epoch=149
06/23/2022 16:12:23 - INFO - __main__ - Step 310 Global step 310 Train loss 6.13 on epoch=154
06/23/2022 16:12:25 - INFO - __main__ - Step 320 Global step 320 Train loss 6.08 on epoch=159
06/23/2022 16:12:26 - INFO - __main__ - Step 330 Global step 330 Train loss 6.01 on epoch=164
06/23/2022 16:12:27 - INFO - __main__ - Step 340 Global step 340 Train loss 6.06 on epoch=169
06/23/2022 16:12:29 - INFO - __main__ - Step 350 Global step 350 Train loss 5.99 on epoch=174
06/23/2022 16:12:35 - INFO - __main__ - Global step 350 Train loss 6.05 ACC 0.0 on epoch=174
06/23/2022 16:12:37 - INFO - __main__ - Step 360 Global step 360 Train loss 5.99 on epoch=179
06/23/2022 16:12:38 - INFO - __main__ - Step 370 Global step 370 Train loss 5.99 on epoch=184
06/23/2022 16:12:40 - INFO - __main__ - Step 380 Global step 380 Train loss 5.97 on epoch=189
06/23/2022 16:12:41 - INFO - __main__ - Step 390 Global step 390 Train loss 5.81 on epoch=194
06/23/2022 16:12:43 - INFO - __main__ - Step 400 Global step 400 Train loss 5.89 on epoch=199
06/23/2022 16:12:53 - INFO - __main__ - Global step 400 Train loss 5.93 ACC 0.0 on epoch=199
06/23/2022 16:12:54 - INFO - __main__ - Step 410 Global step 410 Train loss 5.77 on epoch=204
06/23/2022 16:12:55 - INFO - __main__ - Step 420 Global step 420 Train loss 5.74 on epoch=209
06/23/2022 16:12:57 - INFO - __main__ - Step 430 Global step 430 Train loss 5.78 on epoch=214
06/23/2022 16:12:58 - INFO - __main__ - Step 440 Global step 440 Train loss 5.59 on epoch=219
06/23/2022 16:13:00 - INFO - __main__ - Step 450 Global step 450 Train loss 5.58 on epoch=224
06/23/2022 16:13:02 - INFO - __main__ - Global step 450 Train loss 5.69 ACC 0.0 on epoch=224
06/23/2022 16:13:03 - INFO - __main__ - Step 460 Global step 460 Train loss 5.59 on epoch=229
06/23/2022 16:13:04 - INFO - __main__ - Step 470 Global step 470 Train loss 5.53 on epoch=234
06/23/2022 16:13:06 - INFO - __main__ - Step 480 Global step 480 Train loss 5.63 on epoch=239
06/23/2022 16:13:07 - INFO - __main__ - Step 490 Global step 490 Train loss 5.62 on epoch=244
06/23/2022 16:13:09 - INFO - __main__ - Step 500 Global step 500 Train loss 5.62 on epoch=249
06/23/2022 16:13:16 - INFO - __main__ - Global step 500 Train loss 5.60 ACC 0.0 on epoch=249
06/23/2022 16:13:17 - INFO - __main__ - Step 510 Global step 510 Train loss 5.60 on epoch=254
06/23/2022 16:13:19 - INFO - __main__ - Step 520 Global step 520 Train loss 5.41 on epoch=259
06/23/2022 16:13:20 - INFO - __main__ - Step 530 Global step 530 Train loss 5.31 on epoch=264
06/23/2022 16:13:22 - INFO - __main__ - Step 540 Global step 540 Train loss 5.38 on epoch=269
06/23/2022 16:13:24 - INFO - __main__ - Step 550 Global step 550 Train loss 5.35 on epoch=274
06/23/2022 16:13:26 - INFO - __main__ - Global step 550 Train loss 5.41 ACC 0.0 on epoch=274
06/23/2022 16:13:28 - INFO - __main__ - Step 560 Global step 560 Train loss 5.31 on epoch=279
06/23/2022 16:13:29 - INFO - __main__ - Step 570 Global step 570 Train loss 5.41 on epoch=284
06/23/2022 16:13:31 - INFO - __main__ - Step 580 Global step 580 Train loss 5.23 on epoch=289
06/23/2022 16:13:32 - INFO - __main__ - Step 590 Global step 590 Train loss 5.20 on epoch=294
06/23/2022 16:13:33 - INFO - __main__ - Step 600 Global step 600 Train loss 5.11 on epoch=299
06/23/2022 16:13:37 - INFO - __main__ - Global step 600 Train loss 5.25 ACC 0.0 on epoch=299
06/23/2022 16:13:38 - INFO - __main__ - Step 610 Global step 610 Train loss 5.22 on epoch=304
06/23/2022 16:13:40 - INFO - __main__ - Step 620 Global step 620 Train loss 5.14 on epoch=309
06/23/2022 16:13:41 - INFO - __main__ - Step 630 Global step 630 Train loss 5.02 on epoch=314
06/23/2022 16:13:43 - INFO - __main__ - Step 640 Global step 640 Train loss 5.07 on epoch=319
06/23/2022 16:13:44 - INFO - __main__ - Step 650 Global step 650 Train loss 5.01 on epoch=324
06/23/2022 16:13:45 - INFO - __main__ - Global step 650 Train loss 5.09 ACC 0.0 on epoch=324
06/23/2022 16:13:47 - INFO - __main__ - Step 660 Global step 660 Train loss 5.04 on epoch=329
06/23/2022 16:13:48 - INFO - __main__ - Step 670 Global step 670 Train loss 5.04 on epoch=334
06/23/2022 16:13:50 - INFO - __main__ - Step 680 Global step 680 Train loss 5.15 on epoch=339
06/23/2022 16:13:51 - INFO - __main__ - Step 690 Global step 690 Train loss 5.06 on epoch=344
06/23/2022 16:13:53 - INFO - __main__ - Step 700 Global step 700 Train loss 4.95 on epoch=349
06/23/2022 16:13:59 - INFO - __main__ - Global step 700 Train loss 5.05 ACC 0.0 on epoch=349
06/23/2022 16:14:01 - INFO - __main__ - Step 710 Global step 710 Train loss 4.80 on epoch=354
06/23/2022 16:14:02 - INFO - __main__ - Step 720 Global step 720 Train loss 4.78 on epoch=359
06/23/2022 16:14:03 - INFO - __main__ - Step 730 Global step 730 Train loss 4.86 on epoch=364
06/23/2022 16:14:05 - INFO - __main__ - Step 740 Global step 740 Train loss 4.79 on epoch=369
06/23/2022 16:14:06 - INFO - __main__ - Step 750 Global step 750 Train loss 4.72 on epoch=374
06/23/2022 16:14:08 - INFO - __main__ - Global step 750 Train loss 4.79 ACC 0.0 on epoch=374
06/23/2022 16:14:09 - INFO - __main__ - Step 760 Global step 760 Train loss 4.61 on epoch=379
06/23/2022 16:14:11 - INFO - __main__ - Step 770 Global step 770 Train loss 4.76 on epoch=384
06/23/2022 16:14:12 - INFO - __main__ - Step 780 Global step 780 Train loss 4.61 on epoch=389
06/23/2022 16:14:13 - INFO - __main__ - Step 790 Global step 790 Train loss 4.58 on epoch=394
06/23/2022 16:14:15 - INFO - __main__ - Step 800 Global step 800 Train loss 4.46 on epoch=399
06/23/2022 16:14:16 - INFO - __main__ - Global step 800 Train loss 4.60 ACC 0.0 on epoch=399
06/23/2022 16:14:17 - INFO - __main__ - Step 810 Global step 810 Train loss 4.51 on epoch=404
06/23/2022 16:14:19 - INFO - __main__ - Step 820 Global step 820 Train loss 4.35 on epoch=409
06/23/2022 16:14:20 - INFO - __main__ - Step 830 Global step 830 Train loss 4.49 on epoch=414
06/23/2022 16:14:21 - INFO - __main__ - Step 840 Global step 840 Train loss 4.42 on epoch=419
06/23/2022 16:14:23 - INFO - __main__ - Step 850 Global step 850 Train loss 4.34 on epoch=424
06/23/2022 16:14:24 - INFO - __main__ - Global step 850 Train loss 4.42 ACC 0.0 on epoch=424
06/23/2022 16:14:25 - INFO - __main__ - Step 860 Global step 860 Train loss 4.22 on epoch=429
06/23/2022 16:14:27 - INFO - __main__ - Step 870 Global step 870 Train loss 4.26 on epoch=434
06/23/2022 16:14:28 - INFO - __main__ - Step 880 Global step 880 Train loss 4.12 on epoch=439
06/23/2022 16:14:30 - INFO - __main__ - Step 890 Global step 890 Train loss 4.08 on epoch=444
06/23/2022 16:14:31 - INFO - __main__ - Step 900 Global step 900 Train loss 4.08 on epoch=449
06/23/2022 16:14:32 - INFO - __main__ - Global step 900 Train loss 4.15 ACC 0.0 on epoch=449
06/23/2022 16:14:33 - INFO - __main__ - Step 910 Global step 910 Train loss 4.22 on epoch=454
06/23/2022 16:14:35 - INFO - __main__ - Step 920 Global step 920 Train loss 4.05 on epoch=459
06/23/2022 16:14:36 - INFO - __main__ - Step 930 Global step 930 Train loss 3.96 on epoch=464
06/23/2022 16:14:37 - INFO - __main__ - Step 940 Global step 940 Train loss 3.94 on epoch=469
06/23/2022 16:14:39 - INFO - __main__ - Step 950 Global step 950 Train loss 3.89 on epoch=474
06/23/2022 16:14:40 - INFO - __main__ - Global step 950 Train loss 4.01 ACC 0.0 on epoch=474
06/23/2022 16:14:42 - INFO - __main__ - Step 960 Global step 960 Train loss 3.86 on epoch=479
06/23/2022 16:14:43 - INFO - __main__ - Step 970 Global step 970 Train loss 3.69 on epoch=484
06/23/2022 16:14:44 - INFO - __main__ - Step 980 Global step 980 Train loss 3.74 on epoch=489
06/23/2022 16:14:46 - INFO - __main__ - Step 990 Global step 990 Train loss 3.84 on epoch=494
06/23/2022 16:14:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.74 on epoch=499
06/23/2022 16:14:48 - INFO - __main__ - Global step 1000 Train loss 3.77 ACC 0.0 on epoch=499
06/23/2022 16:14:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.71 on epoch=504
06/23/2022 16:14:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.71 on epoch=509
06/23/2022 16:14:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.65 on epoch=514
06/23/2022 16:14:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.48 on epoch=519
06/23/2022 16:14:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.58 on epoch=524
06/23/2022 16:14:57 - INFO - __main__ - Global step 1050 Train loss 3.63 ACC 0.0 on epoch=524
06/23/2022 16:14:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.49 on epoch=529
06/23/2022 16:15:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.42 on epoch=534
06/23/2022 16:15:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.30 on epoch=539
06/23/2022 16:15:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.40 on epoch=544
06/23/2022 16:15:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.35 on epoch=549
06/23/2022 16:15:07 - INFO - __main__ - Global step 1100 Train loss 3.39 ACC 0.0 on epoch=549
06/23/2022 16:15:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.27 on epoch=554
06/23/2022 16:15:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.25 on epoch=559
06/23/2022 16:15:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.17 on epoch=564
06/23/2022 16:15:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.17 on epoch=569
06/23/2022 16:15:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.25 on epoch=574
06/23/2022 16:15:18 - INFO - __main__ - Global step 1150 Train loss 3.22 ACC 0.0 on epoch=574
06/23/2022 16:15:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.91 on epoch=579
06/23/2022 16:15:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.97 on epoch=584
06/23/2022 16:15:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.05 on epoch=589
06/23/2022 16:15:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.05 on epoch=594
06/23/2022 16:15:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.96 on epoch=599
06/23/2022 16:15:29 - INFO - __main__ - Global step 1200 Train loss 2.99 ACC 0.0 on epoch=599
06/23/2022 16:15:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.86 on epoch=604
06/23/2022 16:15:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.75 on epoch=609
06/23/2022 16:15:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.75 on epoch=614
06/23/2022 16:15:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.64 on epoch=619
06/23/2022 16:15:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.63 on epoch=624
06/23/2022 16:15:38 - INFO - __main__ - Global step 1250 Train loss 2.73 ACC 0.0 on epoch=624
06/23/2022 16:15:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.64 on epoch=629
06/23/2022 16:15:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.53 on epoch=634
06/23/2022 16:15:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.44 on epoch=639
06/23/2022 16:15:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.53 on epoch=644
06/23/2022 16:15:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.42 on epoch=649
06/23/2022 16:15:48 - INFO - __main__ - Global step 1300 Train loss 2.51 ACC 0.0 on epoch=649
06/23/2022 16:15:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.45 on epoch=654
06/23/2022 16:15:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.45 on epoch=659
06/23/2022 16:15:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.36 on epoch=664
06/23/2022 16:15:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.44 on epoch=669
06/23/2022 16:15:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.37 on epoch=674
06/23/2022 16:15:57 - INFO - __main__ - Global step 1350 Train loss 2.41 ACC 0.0 on epoch=674
06/23/2022 16:15:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.43 on epoch=679
06/23/2022 16:16:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.35 on epoch=684
06/23/2022 16:16:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.29 on epoch=689
06/23/2022 16:16:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.19 on epoch=694
06/23/2022 16:16:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.19 on epoch=699
06/23/2022 16:16:06 - INFO - __main__ - Global step 1400 Train loss 2.29 ACC 0.0 on epoch=699
06/23/2022 16:16:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.19 on epoch=704
06/23/2022 16:16:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.11 on epoch=709
06/23/2022 16:16:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.11 on epoch=714
06/23/2022 16:16:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.10 on epoch=719
06/23/2022 16:16:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.14 on epoch=724
06/23/2022 16:16:16 - INFO - __main__ - Global step 1450 Train loss 2.13 ACC 0.0 on epoch=724
06/23/2022 16:16:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.11 on epoch=729
06/23/2022 16:16:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.05 on epoch=734
06/23/2022 16:16:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.00 on epoch=739
06/23/2022 16:16:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.04 on epoch=744
06/23/2022 16:16:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.95 on epoch=749
06/23/2022 16:16:24 - INFO - __main__ - Global step 1500 Train loss 2.03 ACC 0.21875 on epoch=749
06/23/2022 16:16:24 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.21875 on epoch=749, global_step=1500
06/23/2022 16:16:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.98 on epoch=754
06/23/2022 16:16:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.90 on epoch=759
06/23/2022 16:16:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.90 on epoch=764
06/23/2022 16:16:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.83 on epoch=769
06/23/2022 16:16:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.86 on epoch=774
06/23/2022 16:16:32 - INFO - __main__ - Global step 1550 Train loss 1.89 ACC 0.5 on epoch=774
06/23/2022 16:16:32 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.5 on epoch=774, global_step=1550
06/23/2022 16:16:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.95 on epoch=779
06/23/2022 16:16:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.88 on epoch=784
06/23/2022 16:16:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.79 on epoch=789
06/23/2022 16:16:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.82 on epoch=794
06/23/2022 16:16:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.84 on epoch=799
06/23/2022 16:16:40 - INFO - __main__ - Global step 1600 Train loss 1.86 ACC 0.5 on epoch=799
06/23/2022 16:16:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.73 on epoch=804
06/23/2022 16:16:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.78 on epoch=809
06/23/2022 16:16:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.60 on epoch=814
06/23/2022 16:16:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.66 on epoch=819
06/23/2022 16:16:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.68 on epoch=824
06/23/2022 16:16:49 - INFO - __main__ - Global step 1650 Train loss 1.69 ACC 0.5 on epoch=824
06/23/2022 16:16:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.61 on epoch=829
06/23/2022 16:16:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.60 on epoch=834
06/23/2022 16:16:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.52 on epoch=839
06/23/2022 16:16:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.70 on epoch=844
06/23/2022 16:16:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.51 on epoch=849
06/23/2022 16:16:58 - INFO - __main__ - Global step 1700 Train loss 1.59 ACC 0.5 on epoch=849
06/23/2022 16:16:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.68 on epoch=854
06/23/2022 16:17:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.55 on epoch=859
06/23/2022 16:17:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.53 on epoch=864
06/23/2022 16:17:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.42 on epoch=869
06/23/2022 16:17:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.43 on epoch=874
06/23/2022 16:17:07 - INFO - __main__ - Global step 1750 Train loss 1.52 ACC 0.5 on epoch=874
06/23/2022 16:17:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.38 on epoch=879
06/23/2022 16:17:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.50 on epoch=884
06/23/2022 16:17:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.40 on epoch=889
06/23/2022 16:17:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.29 on epoch=894
06/23/2022 16:17:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.34 on epoch=899
06/23/2022 16:17:16 - INFO - __main__ - Global step 1800 Train loss 1.38 ACC 0.5 on epoch=899
06/23/2022 16:17:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.32 on epoch=904
06/23/2022 16:17:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.22 on epoch=909
06/23/2022 16:17:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.24 on epoch=914
06/23/2022 16:17:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.21 on epoch=919
06/23/2022 16:17:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.25 on epoch=924
06/23/2022 16:17:25 - INFO - __main__ - Global step 1850 Train loss 1.25 ACC 0.5 on epoch=924
06/23/2022 16:17:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.20 on epoch=929
06/23/2022 16:17:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.23 on epoch=934
06/23/2022 16:17:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.20 on epoch=939
06/23/2022 16:17:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.13 on epoch=944
06/23/2022 16:17:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.10 on epoch=949
06/23/2022 16:17:34 - INFO - __main__ - Global step 1900 Train loss 1.17 ACC 0.5 on epoch=949
06/23/2022 16:17:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.06 on epoch=954
06/23/2022 16:17:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.10 on epoch=959
06/23/2022 16:17:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.07 on epoch=964
06/23/2022 16:17:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.03 on epoch=969
06/23/2022 16:17:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.94 on epoch=974
06/23/2022 16:17:49 - INFO - __main__ - Global step 1950 Train loss 1.04 ACC 0.46875 on epoch=974
06/23/2022 16:17:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.08 on epoch=979
06/23/2022 16:17:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.03 on epoch=984
06/23/2022 16:17:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.02 on epoch=989
06/23/2022 16:17:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.00 on epoch=994
06/23/2022 16:17:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.90 on epoch=999
06/23/2022 16:17:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:17:57 - INFO - __main__ - Printing 3 examples
06/23/2022 16:17:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/23/2022 16:17:57 - INFO - __main__ - ['equivalent']
06/23/2022 16:17:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/23/2022 16:17:57 - INFO - __main__ - ['equivalent']
06/23/2022 16:17:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/23/2022 16:17:57 - INFO - __main__ - ['equivalent']
06/23/2022 16:17:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 16:17:57 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:17:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:17:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:17:57 - INFO - __main__ - Printing 3 examples
06/23/2022 16:17:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/23/2022 16:17:57 - INFO - __main__ - ['equivalent']
06/23/2022 16:17:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/23/2022 16:17:57 - INFO - __main__ - ['equivalent']
06/23/2022 16:17:57 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/23/2022 16:17:57 - INFO - __main__ - ['equivalent']
06/23/2022 16:17:57 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:17:57 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:17:57 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:17:59 - INFO - __main__ - Global step 2000 Train loss 1.01 ACC 0.5625 on epoch=999
06/23/2022 16:17:59 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=999, global_step=2000
06/23/2022 16:17:59 - INFO - __main__ - save last model!
06/23/2022 16:17:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 16:17:59 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 16:17:59 - INFO - __main__ - Printing 3 examples
06/23/2022 16:17:59 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 16:17:59 - INFO - __main__ - ['equivalent']
06/23/2022 16:17:59 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 16:17:59 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:17:59 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 16:17:59 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:17:59 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:17:59 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:17:59 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 16:18:04 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:18:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:18:04 - INFO - __main__ - Starting training!
06/23/2022 16:18:50 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_13_0.3_8_predictions.txt
06/23/2022 16:18:50 - INFO - __main__ - ACC on test data: 0.3554
06/23/2022 16:18:50 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.3, bsz=8, dev_performance=0.5625, test_performance=0.3553921568627451
06/23/2022 16:18:50 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.2, bsz=8 ...
06/23/2022 16:18:51 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:18:51 - INFO - __main__ - Printing 3 examples
06/23/2022 16:18:51 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/23/2022 16:18:51 - INFO - __main__ - ['equivalent']
06/23/2022 16:18:51 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/23/2022 16:18:51 - INFO - __main__ - ['equivalent']
06/23/2022 16:18:51 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/23/2022 16:18:51 - INFO - __main__ - ['equivalent']
06/23/2022 16:18:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 16:18:51 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:18:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:18:51 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:18:51 - INFO - __main__ - Printing 3 examples
06/23/2022 16:18:51 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/23/2022 16:18:51 - INFO - __main__ - ['equivalent']
06/23/2022 16:18:51 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/23/2022 16:18:51 - INFO - __main__ - ['equivalent']
06/23/2022 16:18:51 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/23/2022 16:18:51 - INFO - __main__ - ['equivalent']
06/23/2022 16:18:51 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:18:51 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:18:51 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:18:58 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:18:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:18:58 - INFO - __main__ - Starting training!
06/23/2022 16:19:00 - INFO - __main__ - Step 10 Global step 10 Train loss 6.82 on epoch=4
06/23/2022 16:19:01 - INFO - __main__ - Step 20 Global step 20 Train loss 6.84 on epoch=9
06/23/2022 16:19:03 - INFO - __main__ - Step 30 Global step 30 Train loss 6.89 on epoch=14
06/23/2022 16:19:04 - INFO - __main__ - Step 40 Global step 40 Train loss 6.83 on epoch=19
06/23/2022 16:19:06 - INFO - __main__ - Step 50 Global step 50 Train loss 6.87 on epoch=24
06/23/2022 16:19:07 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/23/2022 16:19:07 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 16:19:08 - INFO - __main__ - Step 60 Global step 60 Train loss 6.85 on epoch=29
06/23/2022 16:19:10 - INFO - __main__ - Step 70 Global step 70 Train loss 6.71 on epoch=34
06/23/2022 16:19:11 - INFO - __main__ - Step 80 Global step 80 Train loss 6.84 on epoch=39
06/23/2022 16:19:12 - INFO - __main__ - Step 90 Global step 90 Train loss 6.80 on epoch=44
06/23/2022 16:19:14 - INFO - __main__ - Step 100 Global step 100 Train loss 6.79 on epoch=49
06/23/2022 16:19:16 - INFO - __main__ - Global step 100 Train loss 6.80 ACC 0.0 on epoch=49
06/23/2022 16:19:17 - INFO - __main__ - Step 110 Global step 110 Train loss 6.78 on epoch=54
06/23/2022 16:19:19 - INFO - __main__ - Step 120 Global step 120 Train loss 6.81 on epoch=59
06/23/2022 16:19:20 - INFO - __main__ - Step 130 Global step 130 Train loss 6.80 on epoch=64
06/23/2022 16:19:22 - INFO - __main__ - Step 140 Global step 140 Train loss 6.79 on epoch=69
06/23/2022 16:19:23 - INFO - __main__ - Step 150 Global step 150 Train loss 6.77 on epoch=74
06/23/2022 16:19:24 - INFO - __main__ - Global step 150 Train loss 6.79 ACC 0.0 on epoch=74
06/23/2022 16:19:26 - INFO - __main__ - Step 160 Global step 160 Train loss 6.73 on epoch=79
06/23/2022 16:19:27 - INFO - __main__ - Step 170 Global step 170 Train loss 6.76 on epoch=84
06/23/2022 16:19:29 - INFO - __main__ - Step 180 Global step 180 Train loss 6.73 on epoch=89
06/23/2022 16:19:30 - INFO - __main__ - Step 190 Global step 190 Train loss 6.76 on epoch=94
06/23/2022 16:19:32 - INFO - __main__ - Step 200 Global step 200 Train loss 6.65 on epoch=99
06/23/2022 16:19:36 - INFO - __main__ - Global step 200 Train loss 6.73 ACC 0.0 on epoch=99
06/23/2022 16:19:37 - INFO - __main__ - Step 210 Global step 210 Train loss 6.74 on epoch=104
06/23/2022 16:19:39 - INFO - __main__ - Step 220 Global step 220 Train loss 6.66 on epoch=109
06/23/2022 16:19:40 - INFO - __main__ - Step 230 Global step 230 Train loss 6.66 on epoch=114
06/23/2022 16:19:42 - INFO - __main__ - Step 240 Global step 240 Train loss 6.55 on epoch=119
06/23/2022 16:19:43 - INFO - __main__ - Step 250 Global step 250 Train loss 6.64 on epoch=124
06/23/2022 16:19:50 - INFO - __main__ - Global step 250 Train loss 6.65 ACC 0.0 on epoch=124
06/23/2022 16:19:51 - INFO - __main__ - Step 260 Global step 260 Train loss 6.51 on epoch=129
06/23/2022 16:19:53 - INFO - __main__ - Step 270 Global step 270 Train loss 6.54 on epoch=134
06/23/2022 16:19:54 - INFO - __main__ - Step 280 Global step 280 Train loss 6.58 on epoch=139
06/23/2022 16:19:56 - INFO - __main__ - Step 290 Global step 290 Train loss 6.53 on epoch=144
06/23/2022 16:19:57 - INFO - __main__ - Step 300 Global step 300 Train loss 6.49 on epoch=149
06/23/2022 16:20:04 - INFO - __main__ - Global step 300 Train loss 6.53 ACC 0.0 on epoch=149
06/23/2022 16:20:06 - INFO - __main__ - Step 310 Global step 310 Train loss 6.49 on epoch=154
06/23/2022 16:20:07 - INFO - __main__ - Step 320 Global step 320 Train loss 6.47 on epoch=159
06/23/2022 16:20:09 - INFO - __main__ - Step 330 Global step 330 Train loss 6.51 on epoch=164
06/23/2022 16:20:10 - INFO - __main__ - Step 340 Global step 340 Train loss 6.32 on epoch=169
06/23/2022 16:20:12 - INFO - __main__ - Step 350 Global step 350 Train loss 6.35 on epoch=174
06/23/2022 16:20:16 - INFO - __main__ - Global step 350 Train loss 6.43 ACC 0.0 on epoch=174
06/23/2022 16:20:17 - INFO - __main__ - Step 360 Global step 360 Train loss 6.37 on epoch=179
06/23/2022 16:20:19 - INFO - __main__ - Step 370 Global step 370 Train loss 6.30 on epoch=184
06/23/2022 16:20:20 - INFO - __main__ - Step 380 Global step 380 Train loss 6.30 on epoch=189
06/23/2022 16:20:22 - INFO - __main__ - Step 390 Global step 390 Train loss 6.33 on epoch=194
06/23/2022 16:20:23 - INFO - __main__ - Step 400 Global step 400 Train loss 6.19 on epoch=199
06/23/2022 16:20:30 - INFO - __main__ - Global step 400 Train loss 6.30 ACC 0.0 on epoch=199
06/23/2022 16:20:32 - INFO - __main__ - Step 410 Global step 410 Train loss 6.13 on epoch=204
06/23/2022 16:20:34 - INFO - __main__ - Step 420 Global step 420 Train loss 6.19 on epoch=209
06/23/2022 16:20:35 - INFO - __main__ - Step 430 Global step 430 Train loss 6.11 on epoch=214
06/23/2022 16:20:37 - INFO - __main__ - Step 440 Global step 440 Train loss 6.21 on epoch=219
06/23/2022 16:20:39 - INFO - __main__ - Step 450 Global step 450 Train loss 6.21 on epoch=224
06/23/2022 16:20:50 - INFO - __main__ - Global step 450 Train loss 6.17 ACC 0.0 on epoch=224
06/23/2022 16:20:51 - INFO - __main__ - Step 460 Global step 460 Train loss 6.21 on epoch=229
06/23/2022 16:20:53 - INFO - __main__ - Step 470 Global step 470 Train loss 6.14 on epoch=234
06/23/2022 16:20:54 - INFO - __main__ - Step 480 Global step 480 Train loss 6.07 on epoch=239
06/23/2022 16:20:56 - INFO - __main__ - Step 490 Global step 490 Train loss 6.18 on epoch=244
06/23/2022 16:20:57 - INFO - __main__ - Step 500 Global step 500 Train loss 5.99 on epoch=249
06/23/2022 16:21:08 - INFO - __main__ - Global step 500 Train loss 6.12 ACC 0.0 on epoch=249
06/23/2022 16:21:10 - INFO - __main__ - Step 510 Global step 510 Train loss 5.92 on epoch=254
06/23/2022 16:21:11 - INFO - __main__ - Step 520 Global step 520 Train loss 6.03 on epoch=259
06/23/2022 16:21:12 - INFO - __main__ - Step 530 Global step 530 Train loss 5.94 on epoch=264
06/23/2022 16:21:14 - INFO - __main__ - Step 540 Global step 540 Train loss 5.90 on epoch=269
06/23/2022 16:21:15 - INFO - __main__ - Step 550 Global step 550 Train loss 5.91 on epoch=274
06/23/2022 16:21:26 - INFO - __main__ - Global step 550 Train loss 5.94 ACC 0.0 on epoch=274
06/23/2022 16:21:28 - INFO - __main__ - Step 560 Global step 560 Train loss 5.92 on epoch=279
06/23/2022 16:21:29 - INFO - __main__ - Step 570 Global step 570 Train loss 5.84 on epoch=284
06/23/2022 16:21:30 - INFO - __main__ - Step 580 Global step 580 Train loss 5.88 on epoch=289
06/23/2022 16:21:32 - INFO - __main__ - Step 590 Global step 590 Train loss 5.84 on epoch=294
06/23/2022 16:21:33 - INFO - __main__ - Step 600 Global step 600 Train loss 5.79 on epoch=299
06/23/2022 16:21:41 - INFO - __main__ - Global step 600 Train loss 5.86 ACC 0.0 on epoch=299
06/23/2022 16:21:42 - INFO - __main__ - Step 610 Global step 610 Train loss 5.77 on epoch=304
06/23/2022 16:21:44 - INFO - __main__ - Step 620 Global step 620 Train loss 5.83 on epoch=309
06/23/2022 16:21:45 - INFO - __main__ - Step 630 Global step 630 Train loss 5.85 on epoch=314
06/23/2022 16:21:46 - INFO - __main__ - Step 640 Global step 640 Train loss 5.79 on epoch=319
06/23/2022 16:21:48 - INFO - __main__ - Step 650 Global step 650 Train loss 5.75 on epoch=324
06/23/2022 16:21:58 - INFO - __main__ - Global step 650 Train loss 5.80 ACC 0.0 on epoch=324
06/23/2022 16:22:00 - INFO - __main__ - Step 660 Global step 660 Train loss 5.79 on epoch=329
06/23/2022 16:22:01 - INFO - __main__ - Step 670 Global step 670 Train loss 5.69 on epoch=334
06/23/2022 16:22:03 - INFO - __main__ - Step 680 Global step 680 Train loss 5.71 on epoch=339
06/23/2022 16:22:04 - INFO - __main__ - Step 690 Global step 690 Train loss 5.66 on epoch=344
06/23/2022 16:22:05 - INFO - __main__ - Step 700 Global step 700 Train loss 5.65 on epoch=349
06/23/2022 16:22:13 - INFO - __main__ - Global step 700 Train loss 5.70 ACC 0.0 on epoch=349
06/23/2022 16:22:15 - INFO - __main__ - Step 710 Global step 710 Train loss 5.60 on epoch=354
06/23/2022 16:22:16 - INFO - __main__ - Step 720 Global step 720 Train loss 5.68 on epoch=359
06/23/2022 16:22:18 - INFO - __main__ - Step 730 Global step 730 Train loss 5.65 on epoch=364
06/23/2022 16:22:19 - INFO - __main__ - Step 740 Global step 740 Train loss 5.65 on epoch=369
06/23/2022 16:22:21 - INFO - __main__ - Step 750 Global step 750 Train loss 5.63 on epoch=374
06/23/2022 16:22:31 - INFO - __main__ - Global step 750 Train loss 5.64 ACC 0.0 on epoch=374
06/23/2022 16:22:33 - INFO - __main__ - Step 760 Global step 760 Train loss 5.50 on epoch=379
06/23/2022 16:22:34 - INFO - __main__ - Step 770 Global step 770 Train loss 5.52 on epoch=384
06/23/2022 16:22:36 - INFO - __main__ - Step 780 Global step 780 Train loss 5.52 on epoch=389
06/23/2022 16:22:37 - INFO - __main__ - Step 790 Global step 790 Train loss 5.41 on epoch=394
06/23/2022 16:22:39 - INFO - __main__ - Step 800 Global step 800 Train loss 5.47 on epoch=399
06/23/2022 16:22:45 - INFO - __main__ - Global step 800 Train loss 5.48 ACC 0.0 on epoch=399
06/23/2022 16:22:47 - INFO - __main__ - Step 810 Global step 810 Train loss 5.37 on epoch=404
06/23/2022 16:22:48 - INFO - __main__ - Step 820 Global step 820 Train loss 5.41 on epoch=409
06/23/2022 16:22:49 - INFO - __main__ - Step 830 Global step 830 Train loss 5.40 on epoch=414
06/23/2022 16:22:51 - INFO - __main__ - Step 840 Global step 840 Train loss 5.38 on epoch=419
06/23/2022 16:22:52 - INFO - __main__ - Step 850 Global step 850 Train loss 5.21 on epoch=424
06/23/2022 16:22:55 - INFO - __main__ - Global step 850 Train loss 5.35 ACC 0.0 on epoch=424
06/23/2022 16:22:56 - INFO - __main__ - Step 860 Global step 860 Train loss 5.21 on epoch=429
06/23/2022 16:22:57 - INFO - __main__ - Step 870 Global step 870 Train loss 5.34 on epoch=434
06/23/2022 16:22:59 - INFO - __main__ - Step 880 Global step 880 Train loss 5.14 on epoch=439
06/23/2022 16:23:00 - INFO - __main__ - Step 890 Global step 890 Train loss 5.12 on epoch=444
06/23/2022 16:23:02 - INFO - __main__ - Step 900 Global step 900 Train loss 5.20 on epoch=449
06/23/2022 16:23:06 - INFO - __main__ - Global step 900 Train loss 5.20 ACC 0.0 on epoch=449
06/23/2022 16:23:08 - INFO - __main__ - Step 910 Global step 910 Train loss 5.14 on epoch=454
06/23/2022 16:23:09 - INFO - __main__ - Step 920 Global step 920 Train loss 5.14 on epoch=459
06/23/2022 16:23:11 - INFO - __main__ - Step 930 Global step 930 Train loss 5.02 on epoch=464
06/23/2022 16:23:12 - INFO - __main__ - Step 940 Global step 940 Train loss 5.01 on epoch=469
06/23/2022 16:23:13 - INFO - __main__ - Step 950 Global step 950 Train loss 4.96 on epoch=474
06/23/2022 16:23:15 - INFO - __main__ - Global step 950 Train loss 5.05 ACC 0.0 on epoch=474
06/23/2022 16:23:16 - INFO - __main__ - Step 960 Global step 960 Train loss 4.93 on epoch=479
06/23/2022 16:23:17 - INFO - __main__ - Step 970 Global step 970 Train loss 4.87 on epoch=484
06/23/2022 16:23:19 - INFO - __main__ - Step 980 Global step 980 Train loss 4.82 on epoch=489
06/23/2022 16:23:20 - INFO - __main__ - Step 990 Global step 990 Train loss 4.69 on epoch=494
06/23/2022 16:23:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 4.68 on epoch=499
06/23/2022 16:23:24 - INFO - __main__ - Global step 1000 Train loss 4.80 ACC 0.0 on epoch=499
06/23/2022 16:23:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 4.60 on epoch=504
06/23/2022 16:23:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 4.56 on epoch=509
06/23/2022 16:23:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 4.54 on epoch=514
06/23/2022 16:23:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 4.59 on epoch=519
06/23/2022 16:23:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 4.51 on epoch=524
06/23/2022 16:23:34 - INFO - __main__ - Global step 1050 Train loss 4.56 ACC 0.0 on epoch=524
06/23/2022 16:23:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 4.44 on epoch=529
06/23/2022 16:23:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 4.24 on epoch=534
06/23/2022 16:23:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 4.19 on epoch=539
06/23/2022 16:23:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 4.20 on epoch=544
06/23/2022 16:23:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 4.23 on epoch=549
06/23/2022 16:23:45 - INFO - __main__ - Global step 1100 Train loss 4.26 ACC 0.0 on epoch=549
06/23/2022 16:23:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 4.18 on epoch=554
06/23/2022 16:23:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 4.02 on epoch=559
06/23/2022 16:23:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.98 on epoch=564
06/23/2022 16:23:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.81 on epoch=569
06/23/2022 16:23:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.89 on epoch=574
06/23/2022 16:23:55 - INFO - __main__ - Global step 1150 Train loss 3.98 ACC 0.0 on epoch=574
06/23/2022 16:23:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.71 on epoch=579
06/23/2022 16:23:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.84 on epoch=584
06/23/2022 16:23:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.77 on epoch=589
06/23/2022 16:24:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.54 on epoch=594
06/23/2022 16:24:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.53 on epoch=599
06/23/2022 16:24:06 - INFO - __main__ - Global step 1200 Train loss 3.68 ACC 0.0 on epoch=599
06/23/2022 16:24:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.61 on epoch=604
06/23/2022 16:24:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.60 on epoch=609
06/23/2022 16:24:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.58 on epoch=614
06/23/2022 16:24:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.60 on epoch=619
06/23/2022 16:24:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.42 on epoch=624
06/23/2022 16:24:21 - INFO - __main__ - Global step 1250 Train loss 3.56 ACC 0.0 on epoch=624
06/23/2022 16:24:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.46 on epoch=629
06/23/2022 16:24:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.41 on epoch=634
06/23/2022 16:24:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.28 on epoch=639
06/23/2022 16:24:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.35 on epoch=644
06/23/2022 16:24:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.27 on epoch=649
06/23/2022 16:24:31 - INFO - __main__ - Global step 1300 Train loss 3.35 ACC 0.0 on epoch=649
06/23/2022 16:24:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.23 on epoch=654
06/23/2022 16:24:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.23 on epoch=659
06/23/2022 16:24:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 3.15 on epoch=664
06/23/2022 16:24:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 3.09 on epoch=669
06/23/2022 16:24:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 3.04 on epoch=674
06/23/2022 16:24:45 - INFO - __main__ - Global step 1350 Train loss 3.15 ACC 0.09375 on epoch=674
06/23/2022 16:24:45 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=674, global_step=1350
06/23/2022 16:24:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 3.03 on epoch=679
06/23/2022 16:24:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 3.13 on epoch=684
06/23/2022 16:24:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.86 on epoch=689
06/23/2022 16:24:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.81 on epoch=694
06/23/2022 16:24:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.87 on epoch=699
06/23/2022 16:24:57 - INFO - __main__ - Global step 1400 Train loss 2.94 ACC 0.21875 on epoch=699
06/23/2022 16:24:57 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.21875 on epoch=699, global_step=1400
06/23/2022 16:24:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.72 on epoch=704
06/23/2022 16:25:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.72 on epoch=709
06/23/2022 16:25:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.75 on epoch=714
06/23/2022 16:25:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.58 on epoch=719
06/23/2022 16:25:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.71 on epoch=724
06/23/2022 16:25:12 - INFO - __main__ - Global step 1450 Train loss 2.70 ACC 0.375 on epoch=724
06/23/2022 16:25:12 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.375 on epoch=724, global_step=1450
06/23/2022 16:25:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.59 on epoch=729
06/23/2022 16:25:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.58 on epoch=734
06/23/2022 16:25:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.45 on epoch=739
06/23/2022 16:25:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.46 on epoch=744
06/23/2022 16:25:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.35 on epoch=749
06/23/2022 16:25:21 - INFO - __main__ - Global step 1500 Train loss 2.49 ACC 0.46875 on epoch=749
06/23/2022 16:25:21 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.46875 on epoch=749, global_step=1500
06/23/2022 16:25:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.40 on epoch=754
06/23/2022 16:25:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.34 on epoch=759
06/23/2022 16:25:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 2.21 on epoch=764
06/23/2022 16:25:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 2.25 on epoch=769
06/23/2022 16:25:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 2.11 on epoch=774
06/23/2022 16:25:31 - INFO - __main__ - Global step 1550 Train loss 2.26 ACC 0.5 on epoch=774
06/23/2022 16:25:31 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=774, global_step=1550
06/23/2022 16:25:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 2.07 on epoch=779
06/23/2022 16:25:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 2.02 on epoch=784
06/23/2022 16:25:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 2.13 on epoch=789
06/23/2022 16:25:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 2.03 on epoch=794
06/23/2022 16:25:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 2.15 on epoch=799
06/23/2022 16:25:42 - INFO - __main__ - Global step 1600 Train loss 2.08 ACC 0.5 on epoch=799
06/23/2022 16:25:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.90 on epoch=804
06/23/2022 16:25:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.05 on epoch=809
06/23/2022 16:25:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.88 on epoch=814
06/23/2022 16:25:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.91 on epoch=819
06/23/2022 16:25:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.99 on epoch=824
06/23/2022 16:25:51 - INFO - __main__ - Global step 1650 Train loss 1.95 ACC 0.46875 on epoch=824
06/23/2022 16:25:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.80 on epoch=829
06/23/2022 16:25:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.81 on epoch=834
06/23/2022 16:25:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.82 on epoch=839
06/23/2022 16:25:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.70 on epoch=844
06/23/2022 16:25:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.77 on epoch=849
06/23/2022 16:26:05 - INFO - __main__ - Global step 1700 Train loss 1.78 ACC 0.5 on epoch=849
06/23/2022 16:26:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.67 on epoch=854
06/23/2022 16:26:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.76 on epoch=859
06/23/2022 16:26:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.70 on epoch=864
06/23/2022 16:26:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.61 on epoch=869
06/23/2022 16:26:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.69 on epoch=874
06/23/2022 16:26:14 - INFO - __main__ - Global step 1750 Train loss 1.69 ACC 0.46875 on epoch=874
06/23/2022 16:26:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.68 on epoch=879
06/23/2022 16:26:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.67 on epoch=884
06/23/2022 16:26:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.51 on epoch=889
06/23/2022 16:26:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.61 on epoch=894
06/23/2022 16:26:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.55 on epoch=899
06/23/2022 16:26:23 - INFO - __main__ - Global step 1800 Train loss 1.60 ACC 0.53125 on epoch=899
06/23/2022 16:26:23 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=899, global_step=1800
06/23/2022 16:26:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.51 on epoch=904
06/23/2022 16:26:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.53 on epoch=909
06/23/2022 16:26:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.44 on epoch=914
06/23/2022 16:26:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.36 on epoch=919
06/23/2022 16:26:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.29 on epoch=924
06/23/2022 16:26:36 - INFO - __main__ - Global step 1850 Train loss 1.43 ACC 0.5 on epoch=924
06/23/2022 16:26:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.25 on epoch=929
06/23/2022 16:26:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.30 on epoch=934
06/23/2022 16:26:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.35 on epoch=939
06/23/2022 16:26:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.34 on epoch=944
06/23/2022 16:26:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.27 on epoch=949
06/23/2022 16:26:46 - INFO - __main__ - Global step 1900 Train loss 1.30 ACC 0.5 on epoch=949
06/23/2022 16:26:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.25 on epoch=954
06/23/2022 16:26:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.27 on epoch=959
06/23/2022 16:26:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.28 on epoch=964
06/23/2022 16:26:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.24 on epoch=969
06/23/2022 16:26:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.19 on epoch=974
06/23/2022 16:26:55 - INFO - __main__ - Global step 1950 Train loss 1.24 ACC 0.5 on epoch=974
06/23/2022 16:26:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.14 on epoch=979
06/23/2022 16:26:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.19 on epoch=984
06/23/2022 16:26:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.03 on epoch=989
06/23/2022 16:27:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.04 on epoch=994
06/23/2022 16:27:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.10 on epoch=999
06/23/2022 16:27:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:27:03 - INFO - __main__ - Printing 3 examples
06/23/2022 16:27:03 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/23/2022 16:27:03 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:03 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/23/2022 16:27:03 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/23/2022 16:27:03 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 16:27:03 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:27:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:27:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:27:03 - INFO - __main__ - Printing 3 examples
06/23/2022 16:27:03 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/23/2022 16:27:03 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:03 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/23/2022 16:27:03 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/23/2022 16:27:03 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:03 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:27:03 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:27:03 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:27:04 - INFO - __main__ - Global step 2000 Train loss 1.10 ACC 0.5 on epoch=999
06/23/2022 16:27:04 - INFO - __main__ - save last model!
06/23/2022 16:27:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 16:27:04 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 16:27:04 - INFO - __main__ - Printing 3 examples
06/23/2022 16:27:04 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 16:27:04 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:04 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 16:27:04 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:27:04 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 16:27:04 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:27:04 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:27:05 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:27:05 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 16:27:10 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:27:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:27:10 - INFO - __main__ - Starting training!
06/23/2022 16:27:29 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_13_0.2_8_predictions.txt
06/23/2022 16:27:29 - INFO - __main__ - ACC on test data: 0.6838
06/23/2022 16:27:29 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.2, bsz=8, dev_performance=0.53125, test_performance=0.6838235294117647
06/23/2022 16:27:29 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.5, bsz=8 ...
06/23/2022 16:27:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:27:30 - INFO - __main__ - Printing 3 examples
06/23/2022 16:27:30 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/23/2022 16:27:30 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:30 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/23/2022 16:27:30 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/23/2022 16:27:30 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 16:27:30 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:27:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:27:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:27:30 - INFO - __main__ - Printing 3 examples
06/23/2022 16:27:30 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/23/2022 16:27:30 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:30 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/23/2022 16:27:30 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/23/2022 16:27:30 - INFO - __main__ - ['equivalent']
06/23/2022 16:27:30 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:27:30 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:27:30 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:27:37 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:27:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:27:37 - INFO - __main__ - Starting training!
06/23/2022 16:27:39 - INFO - __main__ - Step 10 Global step 10 Train loss 6.84 on epoch=4
06/23/2022 16:27:40 - INFO - __main__ - Step 20 Global step 20 Train loss 6.84 on epoch=9
06/23/2022 16:27:42 - INFO - __main__ - Step 30 Global step 30 Train loss 6.79 on epoch=14
06/23/2022 16:27:43 - INFO - __main__ - Step 40 Global step 40 Train loss 6.81 on epoch=19
06/23/2022 16:27:44 - INFO - __main__ - Step 50 Global step 50 Train loss 6.73 on epoch=24
06/23/2022 16:27:46 - INFO - __main__ - Global step 50 Train loss 6.80 ACC 0.0 on epoch=24
06/23/2022 16:27:46 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 16:27:47 - INFO - __main__ - Step 60 Global step 60 Train loss 6.68 on epoch=29
06/23/2022 16:27:49 - INFO - __main__ - Step 70 Global step 70 Train loss 6.74 on epoch=34
06/23/2022 16:27:50 - INFO - __main__ - Step 80 Global step 80 Train loss 6.75 on epoch=39
06/23/2022 16:27:52 - INFO - __main__ - Step 90 Global step 90 Train loss 6.72 on epoch=44
06/23/2022 16:27:53 - INFO - __main__ - Step 100 Global step 100 Train loss 6.58 on epoch=49
06/23/2022 16:27:54 - INFO - __main__ - Global step 100 Train loss 6.69 ACC 0.0 on epoch=49
06/23/2022 16:27:56 - INFO - __main__ - Step 110 Global step 110 Train loss 6.55 on epoch=54
06/23/2022 16:27:57 - INFO - __main__ - Step 120 Global step 120 Train loss 6.44 on epoch=59
06/23/2022 16:27:59 - INFO - __main__ - Step 130 Global step 130 Train loss 6.42 on epoch=64
06/23/2022 16:28:00 - INFO - __main__ - Step 140 Global step 140 Train loss 6.36 on epoch=69
06/23/2022 16:28:02 - INFO - __main__ - Step 150 Global step 150 Train loss 6.32 on epoch=74
06/23/2022 16:28:07 - INFO - __main__ - Global step 150 Train loss 6.42 ACC 0.0 on epoch=74
06/23/2022 16:28:09 - INFO - __main__ - Step 160 Global step 160 Train loss 6.18 on epoch=79
06/23/2022 16:28:10 - INFO - __main__ - Step 170 Global step 170 Train loss 6.12 on epoch=84
06/23/2022 16:28:12 - INFO - __main__ - Step 180 Global step 180 Train loss 6.23 on epoch=89
06/23/2022 16:28:13 - INFO - __main__ - Step 190 Global step 190 Train loss 6.00 on epoch=94
06/23/2022 16:28:14 - INFO - __main__ - Step 200 Global step 200 Train loss 5.96 on epoch=99
06/23/2022 16:28:21 - INFO - __main__ - Global step 200 Train loss 6.10 ACC 0.0 on epoch=99
06/23/2022 16:28:23 - INFO - __main__ - Step 210 Global step 210 Train loss 5.98 on epoch=104
06/23/2022 16:28:24 - INFO - __main__ - Step 220 Global step 220 Train loss 5.95 on epoch=109
06/23/2022 16:28:26 - INFO - __main__ - Step 230 Global step 230 Train loss 5.80 on epoch=114
06/23/2022 16:28:28 - INFO - __main__ - Step 240 Global step 240 Train loss 5.61 on epoch=119
06/23/2022 16:28:29 - INFO - __main__ - Step 250 Global step 250 Train loss 5.59 on epoch=124
06/23/2022 16:28:40 - INFO - __main__ - Global step 250 Train loss 5.79 ACC 0.0 on epoch=124
06/23/2022 16:28:42 - INFO - __main__ - Step 260 Global step 260 Train loss 5.60 on epoch=129
06/23/2022 16:28:43 - INFO - __main__ - Step 270 Global step 270 Train loss 5.48 on epoch=134
06/23/2022 16:28:45 - INFO - __main__ - Step 280 Global step 280 Train loss 5.23 on epoch=139
06/23/2022 16:28:46 - INFO - __main__ - Step 290 Global step 290 Train loss 5.19 on epoch=144
06/23/2022 16:28:48 - INFO - __main__ - Step 300 Global step 300 Train loss 5.18 on epoch=149
06/23/2022 16:28:49 - INFO - __main__ - Global step 300 Train loss 5.34 ACC 0.0 on epoch=149
06/23/2022 16:28:50 - INFO - __main__ - Step 310 Global step 310 Train loss 5.04 on epoch=154
06/23/2022 16:28:52 - INFO - __main__ - Step 320 Global step 320 Train loss 4.81 on epoch=159
06/23/2022 16:28:53 - INFO - __main__ - Step 330 Global step 330 Train loss 4.92 on epoch=164
06/23/2022 16:28:55 - INFO - __main__ - Step 340 Global step 340 Train loss 4.78 on epoch=169
06/23/2022 16:28:56 - INFO - __main__ - Step 350 Global step 350 Train loss 4.77 on epoch=174
06/23/2022 16:28:57 - INFO - __main__ - Global step 350 Train loss 4.86 ACC 0.0 on epoch=174
06/23/2022 16:28:59 - INFO - __main__ - Step 360 Global step 360 Train loss 4.67 on epoch=179
06/23/2022 16:29:00 - INFO - __main__ - Step 370 Global step 370 Train loss 4.55 on epoch=184
06/23/2022 16:29:02 - INFO - __main__ - Step 380 Global step 380 Train loss 4.45 on epoch=189
06/23/2022 16:29:03 - INFO - __main__ - Step 390 Global step 390 Train loss 4.40 on epoch=194
06/23/2022 16:29:05 - INFO - __main__ - Step 400 Global step 400 Train loss 4.43 on epoch=199
06/23/2022 16:29:06 - INFO - __main__ - Global step 400 Train loss 4.50 ACC 0.0 on epoch=199
06/23/2022 16:29:07 - INFO - __main__ - Step 410 Global step 410 Train loss 4.27 on epoch=204
06/23/2022 16:29:09 - INFO - __main__ - Step 420 Global step 420 Train loss 4.16 on epoch=209
06/23/2022 16:29:10 - INFO - __main__ - Step 430 Global step 430 Train loss 4.01 on epoch=214
06/23/2022 16:29:12 - INFO - __main__ - Step 440 Global step 440 Train loss 4.03 on epoch=219
06/23/2022 16:29:13 - INFO - __main__ - Step 450 Global step 450 Train loss 3.96 on epoch=224
06/23/2022 16:29:15 - INFO - __main__ - Global step 450 Train loss 4.09 ACC 0.0 on epoch=224
06/23/2022 16:29:16 - INFO - __main__ - Step 460 Global step 460 Train loss 3.81 on epoch=229
06/23/2022 16:29:17 - INFO - __main__ - Step 470 Global step 470 Train loss 3.78 on epoch=234
06/23/2022 16:29:19 - INFO - __main__ - Step 480 Global step 480 Train loss 3.67 on epoch=239
06/23/2022 16:29:20 - INFO - __main__ - Step 490 Global step 490 Train loss 3.66 on epoch=244
06/23/2022 16:29:22 - INFO - __main__ - Step 500 Global step 500 Train loss 3.59 on epoch=249
06/23/2022 16:29:23 - INFO - __main__ - Global step 500 Train loss 3.70 ACC 0.0 on epoch=249
06/23/2022 16:29:24 - INFO - __main__ - Step 510 Global step 510 Train loss 3.57 on epoch=254
06/23/2022 16:29:26 - INFO - __main__ - Step 520 Global step 520 Train loss 3.60 on epoch=259
06/23/2022 16:29:27 - INFO - __main__ - Step 530 Global step 530 Train loss 3.57 on epoch=264
06/23/2022 16:29:29 - INFO - __main__ - Step 540 Global step 540 Train loss 3.36 on epoch=269
06/23/2022 16:29:30 - INFO - __main__ - Step 550 Global step 550 Train loss 3.37 on epoch=274
06/23/2022 16:29:32 - INFO - __main__ - Global step 550 Train loss 3.49 ACC 0.0 on epoch=274
06/23/2022 16:29:33 - INFO - __main__ - Step 560 Global step 560 Train loss 3.30 on epoch=279
06/23/2022 16:29:35 - INFO - __main__ - Step 570 Global step 570 Train loss 3.29 on epoch=284
06/23/2022 16:29:36 - INFO - __main__ - Step 580 Global step 580 Train loss 3.25 on epoch=289
06/23/2022 16:29:38 - INFO - __main__ - Step 590 Global step 590 Train loss 3.10 on epoch=294
06/23/2022 16:29:39 - INFO - __main__ - Step 600 Global step 600 Train loss 3.24 on epoch=299
06/23/2022 16:29:42 - INFO - __main__ - Global step 600 Train loss 3.24 ACC 0.0 on epoch=299
06/23/2022 16:29:44 - INFO - __main__ - Step 610 Global step 610 Train loss 3.19 on epoch=304
06/23/2022 16:29:45 - INFO - __main__ - Step 620 Global step 620 Train loss 3.16 on epoch=309
06/23/2022 16:29:47 - INFO - __main__ - Step 630 Global step 630 Train loss 3.15 on epoch=314
06/23/2022 16:29:48 - INFO - __main__ - Step 640 Global step 640 Train loss 2.99 on epoch=319
06/23/2022 16:29:50 - INFO - __main__ - Step 650 Global step 650 Train loss 3.08 on epoch=324
06/23/2022 16:29:57 - INFO - __main__ - Global step 650 Train loss 3.12 ACC 0.0 on epoch=324
06/23/2022 16:29:59 - INFO - __main__ - Step 660 Global step 660 Train loss 3.03 on epoch=329
06/23/2022 16:30:00 - INFO - __main__ - Step 670 Global step 670 Train loss 2.87 on epoch=334
06/23/2022 16:30:01 - INFO - __main__ - Step 680 Global step 680 Train loss 2.81 on epoch=339
06/23/2022 16:30:03 - INFO - __main__ - Step 690 Global step 690 Train loss 2.87 on epoch=344
06/23/2022 16:30:04 - INFO - __main__ - Step 700 Global step 700 Train loss 2.74 on epoch=349
06/23/2022 16:30:07 - INFO - __main__ - Global step 700 Train loss 2.86 ACC 0.0 on epoch=349
06/23/2022 16:30:08 - INFO - __main__ - Step 710 Global step 710 Train loss 2.78 on epoch=354
06/23/2022 16:30:10 - INFO - __main__ - Step 720 Global step 720 Train loss 2.58 on epoch=359
06/23/2022 16:30:11 - INFO - __main__ - Step 730 Global step 730 Train loss 2.59 on epoch=364
06/23/2022 16:30:13 - INFO - __main__ - Step 740 Global step 740 Train loss 2.57 on epoch=369
06/23/2022 16:30:14 - INFO - __main__ - Step 750 Global step 750 Train loss 2.47 on epoch=374
06/23/2022 16:30:22 - INFO - __main__ - Global step 750 Train loss 2.60 ACC 0.0 on epoch=374
06/23/2022 16:30:24 - INFO - __main__ - Step 760 Global step 760 Train loss 2.49 on epoch=379
06/23/2022 16:30:25 - INFO - __main__ - Step 770 Global step 770 Train loss 2.39 on epoch=384
06/23/2022 16:30:26 - INFO - __main__ - Step 780 Global step 780 Train loss 2.48 on epoch=389
06/23/2022 16:30:28 - INFO - __main__ - Step 790 Global step 790 Train loss 2.35 on epoch=394
06/23/2022 16:30:29 - INFO - __main__ - Step 800 Global step 800 Train loss 2.40 on epoch=399
06/23/2022 16:30:33 - INFO - __main__ - Global step 800 Train loss 2.42 ACC 0.0 on epoch=399
06/23/2022 16:30:35 - INFO - __main__ - Step 810 Global step 810 Train loss 2.22 on epoch=404
06/23/2022 16:30:36 - INFO - __main__ - Step 820 Global step 820 Train loss 2.37 on epoch=409
06/23/2022 16:30:38 - INFO - __main__ - Step 830 Global step 830 Train loss 2.37 on epoch=414
06/23/2022 16:30:39 - INFO - __main__ - Step 840 Global step 840 Train loss 2.16 on epoch=419
06/23/2022 16:30:40 - INFO - __main__ - Step 850 Global step 850 Train loss 2.12 on epoch=424
06/23/2022 16:30:44 - INFO - __main__ - Global step 850 Train loss 2.25 ACC 0.09375 on epoch=424
06/23/2022 16:30:44 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=424, global_step=850
06/23/2022 16:30:45 - INFO - __main__ - Step 860 Global step 860 Train loss 2.09 on epoch=429
06/23/2022 16:30:47 - INFO - __main__ - Step 870 Global step 870 Train loss 2.14 on epoch=434
06/23/2022 16:30:48 - INFO - __main__ - Step 880 Global step 880 Train loss 1.98 on epoch=439
06/23/2022 16:30:50 - INFO - __main__ - Step 890 Global step 890 Train loss 2.03 on epoch=444
06/23/2022 16:30:51 - INFO - __main__ - Step 900 Global step 900 Train loss 1.94 on epoch=449
06/23/2022 16:30:52 - INFO - __main__ - Global step 900 Train loss 2.04 ACC 0.40625 on epoch=449
06/23/2022 16:30:52 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.40625 on epoch=449, global_step=900
06/23/2022 16:30:53 - INFO - __main__ - Step 910 Global step 910 Train loss 1.95 on epoch=454
06/23/2022 16:30:55 - INFO - __main__ - Step 920 Global step 920 Train loss 1.84 on epoch=459
06/23/2022 16:30:56 - INFO - __main__ - Step 930 Global step 930 Train loss 1.84 on epoch=464
06/23/2022 16:30:58 - INFO - __main__ - Step 940 Global step 940 Train loss 1.76 on epoch=469
06/23/2022 16:30:59 - INFO - __main__ - Step 950 Global step 950 Train loss 1.76 on epoch=474
06/23/2022 16:31:00 - INFO - __main__ - Global step 950 Train loss 1.83 ACC 0.25 on epoch=474
06/23/2022 16:31:01 - INFO - __main__ - Step 960 Global step 960 Train loss 1.77 on epoch=479
06/23/2022 16:31:03 - INFO - __main__ - Step 970 Global step 970 Train loss 1.81 on epoch=484
06/23/2022 16:31:04 - INFO - __main__ - Step 980 Global step 980 Train loss 1.78 on epoch=489
06/23/2022 16:31:06 - INFO - __main__ - Step 990 Global step 990 Train loss 1.66 on epoch=494
06/23/2022 16:31:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.65 on epoch=499
06/23/2022 16:31:08 - INFO - __main__ - Global step 1000 Train loss 1.73 ACC 0.53125 on epoch=499
06/23/2022 16:31:08 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.53125 on epoch=499, global_step=1000
06/23/2022 16:31:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.63 on epoch=504
06/23/2022 16:31:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.52 on epoch=509
06/23/2022 16:31:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.59 on epoch=514
06/23/2022 16:31:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.47 on epoch=519
06/23/2022 16:31:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.42 on epoch=524
06/23/2022 16:31:16 - INFO - __main__ - Global step 1050 Train loss 1.53 ACC 0.5 on epoch=524
06/23/2022 16:31:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.34 on epoch=529
06/23/2022 16:31:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.25 on epoch=534
06/23/2022 16:31:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.23 on epoch=539
06/23/2022 16:31:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.22 on epoch=544
06/23/2022 16:31:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.19 on epoch=549
06/23/2022 16:31:25 - INFO - __main__ - Global step 1100 Train loss 1.24 ACC 0.5 on epoch=549
06/23/2022 16:31:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.02 on epoch=554
06/23/2022 16:31:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.11 on epoch=559
06/23/2022 16:31:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.19 on epoch=564
06/23/2022 16:31:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.03 on epoch=569
06/23/2022 16:31:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.00 on epoch=574
06/23/2022 16:31:37 - INFO - __main__ - Global step 1150 Train loss 1.07 ACC 0.5 on epoch=574
06/23/2022 16:31:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.00 on epoch=579
06/23/2022 16:31:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.01 on epoch=584
06/23/2022 16:31:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.89 on epoch=589
06/23/2022 16:31:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.84 on epoch=594
06/23/2022 16:31:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.84 on epoch=599
06/23/2022 16:31:45 - INFO - __main__ - Global step 1200 Train loss 0.92 ACC 0.5 on epoch=599
06/23/2022 16:31:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.83 on epoch=604
06/23/2022 16:31:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.99 on epoch=609
06/23/2022 16:31:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.82 on epoch=614
06/23/2022 16:31:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.80 on epoch=619
06/23/2022 16:31:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.76 on epoch=624
06/23/2022 16:31:57 - INFO - __main__ - Global step 1250 Train loss 0.84 ACC 0.5 on epoch=624
06/23/2022 16:31:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.79 on epoch=629
06/23/2022 16:31:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.77 on epoch=634
06/23/2022 16:32:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.61 on epoch=639
06/23/2022 16:32:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.72 on epoch=644
06/23/2022 16:32:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.74 on epoch=649
06/23/2022 16:32:04 - INFO - __main__ - Global step 1300 Train loss 0.73 ACC 0.5 on epoch=649
06/23/2022 16:32:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=654
06/23/2022 16:32:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.76 on epoch=659
06/23/2022 16:32:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.65 on epoch=664
06/23/2022 16:32:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.71 on epoch=669
06/23/2022 16:32:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.68 on epoch=674
06/23/2022 16:32:12 - INFO - __main__ - Global step 1350 Train loss 0.72 ACC 0.5 on epoch=674
06/23/2022 16:32:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.65 on epoch=679
06/23/2022 16:32:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.64 on epoch=684
06/23/2022 16:32:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.64 on epoch=689
06/23/2022 16:32:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.61 on epoch=694
06/23/2022 16:32:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.63 on epoch=699
06/23/2022 16:32:20 - INFO - __main__ - Global step 1400 Train loss 0.63 ACC 0.5 on epoch=699
06/23/2022 16:32:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.60 on epoch=704
06/23/2022 16:32:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.59 on epoch=709
06/23/2022 16:32:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.56 on epoch=714
06/23/2022 16:32:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.59 on epoch=719
06/23/2022 16:32:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.64 on epoch=724
06/23/2022 16:32:28 - INFO - __main__ - Global step 1450 Train loss 0.60 ACC 0.5 on epoch=724
06/23/2022 16:32:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.57 on epoch=729
06/23/2022 16:32:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.53 on epoch=734
06/23/2022 16:32:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.56 on epoch=739
06/23/2022 16:32:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.64 on epoch=744
06/23/2022 16:32:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.56 on epoch=749
06/23/2022 16:32:37 - INFO - __main__ - Global step 1500 Train loss 0.57 ACC 0.5 on epoch=749
06/23/2022 16:32:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.56 on epoch=754
06/23/2022 16:32:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.50 on epoch=759
06/23/2022 16:32:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.57 on epoch=764
06/23/2022 16:32:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.57 on epoch=769
06/23/2022 16:32:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=774
06/23/2022 16:32:45 - INFO - __main__ - Global step 1550 Train loss 0.53 ACC 0.5 on epoch=774
06/23/2022 16:32:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.59 on epoch=779
06/23/2022 16:32:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=784
06/23/2022 16:32:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=789
06/23/2022 16:32:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.48 on epoch=794
06/23/2022 16:32:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=799
06/23/2022 16:32:53 - INFO - __main__ - Global step 1600 Train loss 0.50 ACC 0.5 on epoch=799
06/23/2022 16:32:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.57 on epoch=804
06/23/2022 16:32:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=809
06/23/2022 16:32:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.57 on epoch=814
06/23/2022 16:32:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.48 on epoch=819
06/23/2022 16:33:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=824
06/23/2022 16:33:00 - INFO - __main__ - Global step 1650 Train loss 0.50 ACC 0.5 on epoch=824
06/23/2022 16:33:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=829
06/23/2022 16:33:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.50 on epoch=834
06/23/2022 16:33:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.49 on epoch=839
06/23/2022 16:33:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=844
06/23/2022 16:33:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.48 on epoch=849
06/23/2022 16:33:08 - INFO - __main__ - Global step 1700 Train loss 0.48 ACC 0.5 on epoch=849
06/23/2022 16:33:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.51 on epoch=854
06/23/2022 16:33:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.53 on epoch=859
06/23/2022 16:33:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=864
06/23/2022 16:33:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=869
06/23/2022 16:33:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=874
06/23/2022 16:33:16 - INFO - __main__ - Global step 1750 Train loss 0.46 ACC 0.53125 on epoch=874
06/23/2022 16:33:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.50 on epoch=879
06/23/2022 16:33:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=884
06/23/2022 16:33:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
06/23/2022 16:33:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
06/23/2022 16:33:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
06/23/2022 16:33:24 - INFO - __main__ - Global step 1800 Train loss 0.44 ACC 0.5 on epoch=899
06/23/2022 16:33:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.50 on epoch=904
06/23/2022 16:33:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=909
06/23/2022 16:33:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=914
06/23/2022 16:33:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.49 on epoch=919
06/23/2022 16:33:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=924
06/23/2022 16:33:31 - INFO - __main__ - Global step 1850 Train loss 0.45 ACC 0.5 on epoch=924
06/23/2022 16:33:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
06/23/2022 16:33:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=934
06/23/2022 16:33:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=939
06/23/2022 16:33:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=944
06/23/2022 16:33:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=949
06/23/2022 16:33:39 - INFO - __main__ - Global step 1900 Train loss 0.42 ACC 0.5 on epoch=949
06/23/2022 16:33:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.44 on epoch=954
06/23/2022 16:33:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/23/2022 16:33:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=964
06/23/2022 16:33:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=969
06/23/2022 16:33:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=974
06/23/2022 16:33:46 - INFO - __main__ - Global step 1950 Train loss 0.42 ACC 0.5 on epoch=974
06/23/2022 16:33:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
06/23/2022 16:33:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=984
06/23/2022 16:33:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=989
06/23/2022 16:33:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=994
06/23/2022 16:33:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=999
06/23/2022 16:33:54 - INFO - __main__ - Global step 2000 Train loss 0.42 ACC 0.5 on epoch=999
06/23/2022 16:33:54 - INFO - __main__ - save last model!
06/23/2022 16:33:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 16:33:54 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 16:33:54 - INFO - __main__ - Printing 3 examples
06/23/2022 16:33:54 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 16:33:54 - INFO - __main__ - ['equivalent']
06/23/2022 16:33:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 16:33:54 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:33:54 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 16:33:54 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:33:54 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:33:54 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:33:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:33:55 - INFO - __main__ - Printing 3 examples
06/23/2022 16:33:55 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/23/2022 16:33:55 - INFO - __main__ - ['equivalent']
06/23/2022 16:33:55 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/23/2022 16:33:55 - INFO - __main__ - ['equivalent']
06/23/2022 16:33:55 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/23/2022 16:33:55 - INFO - __main__ - ['equivalent']
06/23/2022 16:33:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 16:33:55 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:33:55 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 16:33:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:33:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:33:55 - INFO - __main__ - Printing 3 examples
06/23/2022 16:33:55 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/23/2022 16:33:55 - INFO - __main__ - ['equivalent']
06/23/2022 16:33:55 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/23/2022 16:33:55 - INFO - __main__ - ['equivalent']
06/23/2022 16:33:55 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/23/2022 16:33:55 - INFO - __main__ - ['equivalent']
06/23/2022 16:33:55 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:33:55 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:33:55 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:34:01 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:34:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:34:01 - INFO - __main__ - Starting training!
06/23/2022 16:34:03 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_21_0.5_8_predictions.txt
06/23/2022 16:34:03 - INFO - __main__ - ACC on test data: 0.6716
06/23/2022 16:34:03 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.5, bsz=8, dev_performance=0.53125, test_performance=0.6715686274509803
06/23/2022 16:34:03 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.4, bsz=8 ...
06/23/2022 16:34:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:34:04 - INFO - __main__ - Printing 3 examples
06/23/2022 16:34:04 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/23/2022 16:34:04 - INFO - __main__ - ['equivalent']
06/23/2022 16:34:04 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/23/2022 16:34:04 - INFO - __main__ - ['equivalent']
06/23/2022 16:34:04 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/23/2022 16:34:04 - INFO - __main__ - ['equivalent']
06/23/2022 16:34:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 16:34:04 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:34:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:34:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:34:04 - INFO - __main__ - Printing 3 examples
06/23/2022 16:34:04 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/23/2022 16:34:04 - INFO - __main__ - ['equivalent']
06/23/2022 16:34:04 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/23/2022 16:34:04 - INFO - __main__ - ['equivalent']
06/23/2022 16:34:04 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/23/2022 16:34:04 - INFO - __main__ - ['equivalent']
06/23/2022 16:34:04 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:34:04 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:34:04 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:34:11 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:34:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:34:11 - INFO - __main__ - Starting training!
06/23/2022 16:34:13 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=4
06/23/2022 16:34:14 - INFO - __main__ - Step 20 Global step 20 Train loss 6.88 on epoch=9
06/23/2022 16:34:16 - INFO - __main__ - Step 30 Global step 30 Train loss 6.85 on epoch=14
06/23/2022 16:34:17 - INFO - __main__ - Step 40 Global step 40 Train loss 6.77 on epoch=19
06/23/2022 16:34:19 - INFO - __main__ - Step 50 Global step 50 Train loss 6.86 on epoch=24
06/23/2022 16:34:19 - INFO - __main__ - Global step 50 Train loss 6.84 ACC 0.0 on epoch=24
06/23/2022 16:34:19 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 16:34:21 - INFO - __main__ - Step 60 Global step 60 Train loss 6.75 on epoch=29
06/23/2022 16:34:22 - INFO - __main__ - Step 70 Global step 70 Train loss 6.73 on epoch=34
06/23/2022 16:34:24 - INFO - __main__ - Step 80 Global step 80 Train loss 6.69 on epoch=39
06/23/2022 16:34:25 - INFO - __main__ - Step 90 Global step 90 Train loss 6.61 on epoch=44
06/23/2022 16:34:27 - INFO - __main__ - Step 100 Global step 100 Train loss 6.54 on epoch=49
06/23/2022 16:34:29 - INFO - __main__ - Global step 100 Train loss 6.66 ACC 0.0 on epoch=49
06/23/2022 16:34:31 - INFO - __main__ - Step 110 Global step 110 Train loss 6.62 on epoch=54
06/23/2022 16:34:32 - INFO - __main__ - Step 120 Global step 120 Train loss 6.59 on epoch=59
06/23/2022 16:34:33 - INFO - __main__ - Step 130 Global step 130 Train loss 6.44 on epoch=64
06/23/2022 16:34:35 - INFO - __main__ - Step 140 Global step 140 Train loss 6.46 on epoch=69
06/23/2022 16:34:36 - INFO - __main__ - Step 150 Global step 150 Train loss 6.39 on epoch=74
06/23/2022 16:34:39 - INFO - __main__ - Global step 150 Train loss 6.50 ACC 0.0 on epoch=74
06/23/2022 16:34:40 - INFO - __main__ - Step 160 Global step 160 Train loss 6.31 on epoch=79
06/23/2022 16:34:42 - INFO - __main__ - Step 170 Global step 170 Train loss 6.33 on epoch=84
06/23/2022 16:34:43 - INFO - __main__ - Step 180 Global step 180 Train loss 6.25 on epoch=89
06/23/2022 16:34:45 - INFO - __main__ - Step 190 Global step 190 Train loss 6.24 on epoch=94
06/23/2022 16:34:46 - INFO - __main__ - Step 200 Global step 200 Train loss 6.18 on epoch=99
06/23/2022 16:34:50 - INFO - __main__ - Global step 200 Train loss 6.26 ACC 0.0 on epoch=99
06/23/2022 16:34:52 - INFO - __main__ - Step 210 Global step 210 Train loss 6.10 on epoch=104
06/23/2022 16:34:53 - INFO - __main__ - Step 220 Global step 220 Train loss 5.99 on epoch=109
06/23/2022 16:34:54 - INFO - __main__ - Step 230 Global step 230 Train loss 5.96 on epoch=114
06/23/2022 16:34:56 - INFO - __main__ - Step 240 Global step 240 Train loss 5.83 on epoch=119
06/23/2022 16:34:57 - INFO - __main__ - Step 250 Global step 250 Train loss 5.92 on epoch=124
06/23/2022 16:35:05 - INFO - __main__ - Global step 250 Train loss 5.96 ACC 0.0 on epoch=124
06/23/2022 16:35:06 - INFO - __main__ - Step 260 Global step 260 Train loss 5.69 on epoch=129
06/23/2022 16:35:07 - INFO - __main__ - Step 270 Global step 270 Train loss 5.90 on epoch=134
06/23/2022 16:35:09 - INFO - __main__ - Step 280 Global step 280 Train loss 5.74 on epoch=139
06/23/2022 16:35:10 - INFO - __main__ - Step 290 Global step 290 Train loss 5.87 on epoch=144
06/23/2022 16:35:12 - INFO - __main__ - Step 300 Global step 300 Train loss 5.72 on epoch=149
06/23/2022 16:35:14 - INFO - __main__ - Global step 300 Train loss 5.78 ACC 0.0 on epoch=149
06/23/2022 16:35:16 - INFO - __main__ - Step 310 Global step 310 Train loss 5.58 on epoch=154
06/23/2022 16:35:17 - INFO - __main__ - Step 320 Global step 320 Train loss 5.54 on epoch=159
06/23/2022 16:35:19 - INFO - __main__ - Step 330 Global step 330 Train loss 5.52 on epoch=164
06/23/2022 16:35:20 - INFO - __main__ - Step 340 Global step 340 Train loss 5.35 on epoch=169
06/23/2022 16:35:22 - INFO - __main__ - Step 350 Global step 350 Train loss 5.36 on epoch=174
06/23/2022 16:35:25 - INFO - __main__ - Global step 350 Train loss 5.47 ACC 0.0 on epoch=174
06/23/2022 16:35:26 - INFO - __main__ - Step 360 Global step 360 Train loss 5.17 on epoch=179
06/23/2022 16:35:27 - INFO - __main__ - Step 370 Global step 370 Train loss 5.18 on epoch=184
06/23/2022 16:35:29 - INFO - __main__ - Step 380 Global step 380 Train loss 4.99 on epoch=189
06/23/2022 16:35:30 - INFO - __main__ - Step 390 Global step 390 Train loss 4.91 on epoch=194
06/23/2022 16:35:32 - INFO - __main__ - Step 400 Global step 400 Train loss 4.82 on epoch=199
06/23/2022 16:35:35 - INFO - __main__ - Global step 400 Train loss 5.01 ACC 0.0 on epoch=199
06/23/2022 16:35:37 - INFO - __main__ - Step 410 Global step 410 Train loss 4.81 on epoch=204
06/23/2022 16:35:38 - INFO - __main__ - Step 420 Global step 420 Train loss 4.71 on epoch=209
06/23/2022 16:35:40 - INFO - __main__ - Step 430 Global step 430 Train loss 4.68 on epoch=214
06/23/2022 16:35:41 - INFO - __main__ - Step 440 Global step 440 Train loss 4.47 on epoch=219
06/23/2022 16:35:43 - INFO - __main__ - Step 450 Global step 450 Train loss 4.42 on epoch=224
06/23/2022 16:35:44 - INFO - __main__ - Global step 450 Train loss 4.62 ACC 0.0 on epoch=224
06/23/2022 16:35:46 - INFO - __main__ - Step 460 Global step 460 Train loss 4.27 on epoch=229
06/23/2022 16:35:47 - INFO - __main__ - Step 470 Global step 470 Train loss 4.25 on epoch=234
06/23/2022 16:35:49 - INFO - __main__ - Step 480 Global step 480 Train loss 4.13 on epoch=239
06/23/2022 16:35:50 - INFO - __main__ - Step 490 Global step 490 Train loss 4.10 on epoch=244
06/23/2022 16:35:52 - INFO - __main__ - Step 500 Global step 500 Train loss 4.04 on epoch=249
06/23/2022 16:35:59 - INFO - __main__ - Global step 500 Train loss 4.16 ACC 0.0 on epoch=249
06/23/2022 16:36:00 - INFO - __main__ - Step 510 Global step 510 Train loss 3.93 on epoch=254
06/23/2022 16:36:01 - INFO - __main__ - Step 520 Global step 520 Train loss 3.81 on epoch=259
06/23/2022 16:36:03 - INFO - __main__ - Step 530 Global step 530 Train loss 3.73 on epoch=264
06/23/2022 16:36:04 - INFO - __main__ - Step 540 Global step 540 Train loss 3.64 on epoch=269
06/23/2022 16:36:05 - INFO - __main__ - Step 550 Global step 550 Train loss 3.60 on epoch=274
06/23/2022 16:36:13 - INFO - __main__ - Global step 550 Train loss 3.74 ACC 0.0 on epoch=274
06/23/2022 16:36:14 - INFO - __main__ - Step 560 Global step 560 Train loss 3.50 on epoch=279
06/23/2022 16:36:16 - INFO - __main__ - Step 570 Global step 570 Train loss 3.50 on epoch=284
06/23/2022 16:36:17 - INFO - __main__ - Step 580 Global step 580 Train loss 3.44 on epoch=289
06/23/2022 16:36:19 - INFO - __main__ - Step 590 Global step 590 Train loss 3.51 on epoch=294
06/23/2022 16:36:20 - INFO - __main__ - Step 600 Global step 600 Train loss 3.28 on epoch=299
06/23/2022 16:36:25 - INFO - __main__ - Global step 600 Train loss 3.44 ACC 0.0 on epoch=299
06/23/2022 16:36:26 - INFO - __main__ - Step 610 Global step 610 Train loss 3.27 on epoch=304
06/23/2022 16:36:27 - INFO - __main__ - Step 620 Global step 620 Train loss 3.19 on epoch=309
06/23/2022 16:36:29 - INFO - __main__ - Step 630 Global step 630 Train loss 3.24 on epoch=314
06/23/2022 16:36:30 - INFO - __main__ - Step 640 Global step 640 Train loss 3.06 on epoch=319
06/23/2022 16:36:32 - INFO - __main__ - Step 650 Global step 650 Train loss 2.95 on epoch=324
06/23/2022 16:36:41 - INFO - __main__ - Global step 650 Train loss 3.14 ACC 0.0 on epoch=324
06/23/2022 16:36:42 - INFO - __main__ - Step 660 Global step 660 Train loss 3.00 on epoch=329
06/23/2022 16:36:44 - INFO - __main__ - Step 670 Global step 670 Train loss 2.84 on epoch=334
06/23/2022 16:36:45 - INFO - __main__ - Step 680 Global step 680 Train loss 2.78 on epoch=339
06/23/2022 16:36:46 - INFO - __main__ - Step 690 Global step 690 Train loss 2.77 on epoch=344
06/23/2022 16:36:48 - INFO - __main__ - Step 700 Global step 700 Train loss 2.69 on epoch=349
06/23/2022 16:36:59 - INFO - __main__ - Global step 700 Train loss 2.81 ACC 0.0 on epoch=349
06/23/2022 16:37:01 - INFO - __main__ - Step 710 Global step 710 Train loss 2.69 on epoch=354
06/23/2022 16:37:02 - INFO - __main__ - Step 720 Global step 720 Train loss 2.54 on epoch=359
06/23/2022 16:37:04 - INFO - __main__ - Step 730 Global step 730 Train loss 2.54 on epoch=364
06/23/2022 16:37:05 - INFO - __main__ - Step 740 Global step 740 Train loss 2.41 on epoch=369
06/23/2022 16:37:06 - INFO - __main__ - Step 750 Global step 750 Train loss 2.39 on epoch=374
06/23/2022 16:37:10 - INFO - __main__ - Global step 750 Train loss 2.51 ACC 0.03125 on epoch=374
06/23/2022 16:37:10 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=374, global_step=750
06/23/2022 16:37:11 - INFO - __main__ - Step 760 Global step 760 Train loss 2.39 on epoch=379
06/23/2022 16:37:13 - INFO - __main__ - Step 770 Global step 770 Train loss 2.37 on epoch=384
06/23/2022 16:37:14 - INFO - __main__ - Step 780 Global step 780 Train loss 2.28 on epoch=389
06/23/2022 16:37:15 - INFO - __main__ - Step 790 Global step 790 Train loss 2.24 on epoch=394
06/23/2022 16:37:17 - INFO - __main__ - Step 800 Global step 800 Train loss 2.03 on epoch=399
06/23/2022 16:37:19 - INFO - __main__ - Global step 800 Train loss 2.26 ACC 0.4375 on epoch=399
06/23/2022 16:37:19 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.4375 on epoch=399, global_step=800
06/23/2022 16:37:20 - INFO - __main__ - Step 810 Global step 810 Train loss 2.09 on epoch=404
06/23/2022 16:37:22 - INFO - __main__ - Step 820 Global step 820 Train loss 1.95 on epoch=409
06/23/2022 16:37:23 - INFO - __main__ - Step 830 Global step 830 Train loss 1.97 on epoch=414
06/23/2022 16:37:24 - INFO - __main__ - Step 840 Global step 840 Train loss 1.88 on epoch=419
06/23/2022 16:37:26 - INFO - __main__ - Step 850 Global step 850 Train loss 1.87 on epoch=424
06/23/2022 16:37:27 - INFO - __main__ - Global step 850 Train loss 1.95 ACC 0.5 on epoch=424
06/23/2022 16:37:27 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=424, global_step=850
06/23/2022 16:37:28 - INFO - __main__ - Step 860 Global step 860 Train loss 1.80 on epoch=429
06/23/2022 16:37:29 - INFO - __main__ - Step 870 Global step 870 Train loss 1.84 on epoch=434
06/23/2022 16:37:31 - INFO - __main__ - Step 880 Global step 880 Train loss 1.68 on epoch=439
06/23/2022 16:37:32 - INFO - __main__ - Step 890 Global step 890 Train loss 1.75 on epoch=444
06/23/2022 16:37:34 - INFO - __main__ - Step 900 Global step 900 Train loss 1.62 on epoch=449
06/23/2022 16:37:35 - INFO - __main__ - Global step 900 Train loss 1.74 ACC 0.53125 on epoch=449
06/23/2022 16:37:35 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=449, global_step=900
06/23/2022 16:37:36 - INFO - __main__ - Step 910 Global step 910 Train loss 1.64 on epoch=454
06/23/2022 16:37:38 - INFO - __main__ - Step 920 Global step 920 Train loss 1.58 on epoch=459
06/23/2022 16:37:39 - INFO - __main__ - Step 930 Global step 930 Train loss 1.56 on epoch=464
06/23/2022 16:37:41 - INFO - __main__ - Step 940 Global step 940 Train loss 1.49 on epoch=469
06/23/2022 16:37:42 - INFO - __main__ - Step 950 Global step 950 Train loss 1.57 on epoch=474
06/23/2022 16:37:43 - INFO - __main__ - Global step 950 Train loss 1.57 ACC 0.40625 on epoch=474
06/23/2022 16:37:45 - INFO - __main__ - Step 960 Global step 960 Train loss 1.60 on epoch=479
06/23/2022 16:37:46 - INFO - __main__ - Step 970 Global step 970 Train loss 1.45 on epoch=484
06/23/2022 16:37:48 - INFO - __main__ - Step 980 Global step 980 Train loss 1.46 on epoch=489
06/23/2022 16:37:49 - INFO - __main__ - Step 990 Global step 990 Train loss 1.41 on epoch=494
06/23/2022 16:37:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.28 on epoch=499
06/23/2022 16:37:52 - INFO - __main__ - Global step 1000 Train loss 1.44 ACC 0.40625 on epoch=499
06/23/2022 16:37:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.43 on epoch=504
06/23/2022 16:37:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.31 on epoch=509
06/23/2022 16:37:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.38 on epoch=514
06/23/2022 16:37:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.22 on epoch=519
06/23/2022 16:37:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.26 on epoch=524
06/23/2022 16:38:00 - INFO - __main__ - Global step 1050 Train loss 1.32 ACC 0.53125 on epoch=524
06/23/2022 16:38:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.18 on epoch=529
06/23/2022 16:38:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.28 on epoch=534
06/23/2022 16:38:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.16 on epoch=539
06/23/2022 16:38:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.11 on epoch=544
06/23/2022 16:38:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.04 on epoch=549
06/23/2022 16:38:09 - INFO - __main__ - Global step 1100 Train loss 1.16 ACC 0.5 on epoch=549
06/23/2022 16:38:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.16 on epoch=554
06/23/2022 16:38:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.03 on epoch=559
06/23/2022 16:38:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.07 on epoch=564
06/23/2022 16:38:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.99 on epoch=569
06/23/2022 16:38:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.07 on epoch=574
06/23/2022 16:38:17 - INFO - __main__ - Global step 1150 Train loss 1.06 ACC 0.5 on epoch=574
06/23/2022 16:38:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.02 on epoch=579
06/23/2022 16:38:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.00 on epoch=584
06/23/2022 16:38:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.03 on epoch=589
06/23/2022 16:38:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.99 on epoch=594
06/23/2022 16:38:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.90 on epoch=599
06/23/2022 16:38:25 - INFO - __main__ - Global step 1200 Train loss 0.99 ACC 0.5 on epoch=599
06/23/2022 16:38:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.86 on epoch=604
06/23/2022 16:38:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.94 on epoch=609
06/23/2022 16:38:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.86 on epoch=614
06/23/2022 16:38:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.88 on epoch=619
06/23/2022 16:38:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.84 on epoch=624
06/23/2022 16:38:33 - INFO - __main__ - Global step 1250 Train loss 0.88 ACC 0.5 on epoch=624
06/23/2022 16:38:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.78 on epoch=629
06/23/2022 16:38:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.83 on epoch=634
06/23/2022 16:38:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.81 on epoch=639
06/23/2022 16:38:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.72 on epoch=644
06/23/2022 16:38:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.76 on epoch=649
06/23/2022 16:38:41 - INFO - __main__ - Global step 1300 Train loss 0.78 ACC 0.5 on epoch=649
06/23/2022 16:38:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.76 on epoch=654
06/23/2022 16:38:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.77 on epoch=659
06/23/2022 16:38:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.74 on epoch=664
06/23/2022 16:38:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.71 on epoch=669
06/23/2022 16:38:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.66 on epoch=674
06/23/2022 16:38:51 - INFO - __main__ - Global step 1350 Train loss 0.73 ACC 0.5 on epoch=674
06/23/2022 16:38:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.67 on epoch=679
06/23/2022 16:38:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.71 on epoch=684
06/23/2022 16:38:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.62 on epoch=689
06/23/2022 16:38:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.73 on epoch=694
06/23/2022 16:38:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.65 on epoch=699
06/23/2022 16:38:59 - INFO - __main__ - Global step 1400 Train loss 0.68 ACC 0.5 on epoch=699
06/23/2022 16:39:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.67 on epoch=704
06/23/2022 16:39:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.66 on epoch=709
06/23/2022 16:39:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.60 on epoch=714
06/23/2022 16:39:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.57 on epoch=719
06/23/2022 16:39:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.67 on epoch=724
06/23/2022 16:39:07 - INFO - __main__ - Global step 1450 Train loss 0.64 ACC 0.5 on epoch=724
06/23/2022 16:39:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
06/23/2022 16:39:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.57 on epoch=734
06/23/2022 16:39:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.62 on epoch=739
06/23/2022 16:39:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.59 on epoch=744
06/23/2022 16:39:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.60 on epoch=749
06/23/2022 16:39:17 - INFO - __main__ - Global step 1500 Train loss 0.58 ACC 0.5 on epoch=749
06/23/2022 16:39:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.55 on epoch=754
06/23/2022 16:39:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.58 on epoch=759
06/23/2022 16:39:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.50 on epoch=764
06/23/2022 16:39:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.56 on epoch=769
06/23/2022 16:39:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.56 on epoch=774
06/23/2022 16:39:27 - INFO - __main__ - Global step 1550 Train loss 0.55 ACC 0.5 on epoch=774
06/23/2022 16:39:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.56 on epoch=779
06/23/2022 16:39:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.54 on epoch=784
06/23/2022 16:39:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.53 on epoch=789
06/23/2022 16:39:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.55 on epoch=794
06/23/2022 16:39:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.51 on epoch=799
06/23/2022 16:39:38 - INFO - __main__ - Global step 1600 Train loss 0.54 ACC 0.5 on epoch=799
06/23/2022 16:39:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=804
06/23/2022 16:39:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.56 on epoch=809
06/23/2022 16:39:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
06/23/2022 16:39:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=819
06/23/2022 16:39:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.55 on epoch=824
06/23/2022 16:39:49 - INFO - __main__ - Global step 1650 Train loss 0.51 ACC 0.5 on epoch=824
06/23/2022 16:39:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=829
06/23/2022 16:39:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=834
06/23/2022 16:39:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.50 on epoch=839
06/23/2022 16:39:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=844
06/23/2022 16:39:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
06/23/2022 16:39:59 - INFO - __main__ - Global step 1700 Train loss 0.46 ACC 0.5 on epoch=849
06/23/2022 16:40:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.49 on epoch=854
06/23/2022 16:40:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=859
06/23/2022 16:40:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.51 on epoch=864
06/23/2022 16:40:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=869
06/23/2022 16:40:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.51 on epoch=874
06/23/2022 16:40:08 - INFO - __main__ - Global step 1750 Train loss 0.50 ACC 0.5 on epoch=874
06/23/2022 16:40:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=879
06/23/2022 16:40:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=884
06/23/2022 16:40:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=889
06/23/2022 16:40:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.47 on epoch=894
06/23/2022 16:40:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.45 on epoch=899
06/23/2022 16:40:18 - INFO - __main__ - Global step 1800 Train loss 0.45 ACC 0.5 on epoch=899
06/23/2022 16:40:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.50 on epoch=904
06/23/2022 16:40:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=909
06/23/2022 16:40:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=914
06/23/2022 16:40:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=919
06/23/2022 16:40:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.45 on epoch=924
06/23/2022 16:40:25 - INFO - __main__ - Global step 1850 Train loss 0.45 ACC 0.5 on epoch=924
06/23/2022 16:40:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
06/23/2022 16:40:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.46 on epoch=934
06/23/2022 16:40:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=939
06/23/2022 16:40:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.50 on epoch=944
06/23/2022 16:40:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=949
06/23/2022 16:40:33 - INFO - __main__ - Global step 1900 Train loss 0.46 ACC 0.5 on epoch=949
06/23/2022 16:40:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=954
06/23/2022 16:40:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=959
06/23/2022 16:40:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
06/23/2022 16:40:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.48 on epoch=969
06/23/2022 16:40:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=974
06/23/2022 16:40:41 - INFO - __main__ - Global step 1950 Train loss 0.43 ACC 0.5 on epoch=974
06/23/2022 16:40:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.47 on epoch=979
06/23/2022 16:40:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=984
06/23/2022 16:40:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=989
06/23/2022 16:40:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=994
06/23/2022 16:40:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.43 on epoch=999
06/23/2022 16:40:49 - INFO - __main__ - Global step 2000 Train loss 0.44 ACC 0.5 on epoch=999
06/23/2022 16:40:49 - INFO - __main__ - save last model!
06/23/2022 16:40:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 16:40:49 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 16:40:49 - INFO - __main__ - Printing 3 examples
06/23/2022 16:40:49 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 16:40:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:40:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 16:40:49 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:40:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 16:40:49 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:40:49 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:40:49 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:40:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:40:49 - INFO - __main__ - Printing 3 examples
06/23/2022 16:40:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/23/2022 16:40:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:40:49 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/23/2022 16:40:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:40:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/23/2022 16:40:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:40:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 16:40:49 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:40:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:40:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:40:49 - INFO - __main__ - Printing 3 examples
06/23/2022 16:40:49 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/23/2022 16:40:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:40:49 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/23/2022 16:40:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:40:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/23/2022 16:40:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:40:49 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:40:49 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:40:49 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:40:50 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 16:40:56 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:40:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:40:56 - INFO - __main__ - Starting training!
06/23/2022 16:40:58 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_21_0.4_8_predictions.txt
06/23/2022 16:40:58 - INFO - __main__ - ACC on test data: 0.6814
06/23/2022 16:40:58 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.4, bsz=8, dev_performance=0.53125, test_performance=0.6813725490196079
06/23/2022 16:40:58 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.3, bsz=8 ...
06/23/2022 16:40:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:40:59 - INFO - __main__ - Printing 3 examples
06/23/2022 16:40:59 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/23/2022 16:40:59 - INFO - __main__ - ['equivalent']
06/23/2022 16:40:59 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/23/2022 16:40:59 - INFO - __main__ - ['equivalent']
06/23/2022 16:40:59 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/23/2022 16:40:59 - INFO - __main__ - ['equivalent']
06/23/2022 16:40:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 16:40:59 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:41:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:41:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:41:00 - INFO - __main__ - Printing 3 examples
06/23/2022 16:41:00 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/23/2022 16:41:00 - INFO - __main__ - ['equivalent']
06/23/2022 16:41:00 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/23/2022 16:41:00 - INFO - __main__ - ['equivalent']
06/23/2022 16:41:00 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/23/2022 16:41:00 - INFO - __main__ - ['equivalent']
06/23/2022 16:41:00 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:41:00 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:41:00 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:41:05 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:41:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:41:06 - INFO - __main__ - Starting training!
06/23/2022 16:41:08 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=4
06/23/2022 16:41:09 - INFO - __main__ - Step 20 Global step 20 Train loss 6.95 on epoch=9
06/23/2022 16:41:11 - INFO - __main__ - Step 30 Global step 30 Train loss 6.87 on epoch=14
06/23/2022 16:41:12 - INFO - __main__ - Step 40 Global step 40 Train loss 6.77 on epoch=19
06/23/2022 16:41:13 - INFO - __main__ - Step 50 Global step 50 Train loss 6.81 on epoch=24
06/23/2022 16:41:21 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/23/2022 16:41:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 16:41:23 - INFO - __main__ - Step 60 Global step 60 Train loss 6.74 on epoch=29
06/23/2022 16:41:24 - INFO - __main__ - Step 70 Global step 70 Train loss 6.73 on epoch=34
06/23/2022 16:41:25 - INFO - __main__ - Step 80 Global step 80 Train loss 6.68 on epoch=39
06/23/2022 16:41:27 - INFO - __main__ - Step 90 Global step 90 Train loss 6.69 on epoch=44
06/23/2022 16:41:28 - INFO - __main__ - Step 100 Global step 100 Train loss 6.68 on epoch=49
06/23/2022 16:41:32 - INFO - __main__ - Global step 100 Train loss 6.70 ACC 0.0 on epoch=49
06/23/2022 16:41:33 - INFO - __main__ - Step 110 Global step 110 Train loss 6.61 on epoch=54
06/23/2022 16:41:34 - INFO - __main__ - Step 120 Global step 120 Train loss 6.71 on epoch=59
06/23/2022 16:41:36 - INFO - __main__ - Step 130 Global step 130 Train loss 6.57 on epoch=64
06/23/2022 16:41:37 - INFO - __main__ - Step 140 Global step 140 Train loss 6.65 on epoch=69
06/23/2022 16:41:38 - INFO - __main__ - Step 150 Global step 150 Train loss 6.66 on epoch=74
06/23/2022 16:41:46 - INFO - __main__ - Global step 150 Train loss 6.64 ACC 0.0 on epoch=74
06/23/2022 16:41:47 - INFO - __main__ - Step 160 Global step 160 Train loss 6.61 on epoch=79
06/23/2022 16:41:49 - INFO - __main__ - Step 170 Global step 170 Train loss 6.54 on epoch=84
06/23/2022 16:41:50 - INFO - __main__ - Step 180 Global step 180 Train loss 6.44 on epoch=89
06/23/2022 16:41:51 - INFO - __main__ - Step 190 Global step 190 Train loss 6.48 on epoch=94
06/23/2022 16:41:53 - INFO - __main__ - Step 200 Global step 200 Train loss 6.38 on epoch=99
06/23/2022 16:42:00 - INFO - __main__ - Global step 200 Train loss 6.49 ACC 0.0 on epoch=99
06/23/2022 16:42:01 - INFO - __main__ - Step 210 Global step 210 Train loss 6.42 on epoch=104
06/23/2022 16:42:03 - INFO - __main__ - Step 220 Global step 220 Train loss 6.38 on epoch=109
06/23/2022 16:42:04 - INFO - __main__ - Step 230 Global step 230 Train loss 6.35 on epoch=114
06/23/2022 16:42:05 - INFO - __main__ - Step 240 Global step 240 Train loss 6.40 on epoch=119
06/23/2022 16:42:07 - INFO - __main__ - Step 250 Global step 250 Train loss 6.27 on epoch=124
06/23/2022 16:42:17 - INFO - __main__ - Global step 250 Train loss 6.36 ACC 0.0 on epoch=124
06/23/2022 16:42:19 - INFO - __main__ - Step 260 Global step 260 Train loss 6.15 on epoch=129
06/23/2022 16:42:20 - INFO - __main__ - Step 270 Global step 270 Train loss 6.13 on epoch=134
06/23/2022 16:42:22 - INFO - __main__ - Step 280 Global step 280 Train loss 6.05 on epoch=139
06/23/2022 16:42:23 - INFO - __main__ - Step 290 Global step 290 Train loss 5.95 on epoch=144
06/23/2022 16:42:24 - INFO - __main__ - Step 300 Global step 300 Train loss 5.94 on epoch=149
06/23/2022 16:42:28 - INFO - __main__ - Global step 300 Train loss 6.04 ACC 0.0 on epoch=149
06/23/2022 16:42:30 - INFO - __main__ - Step 310 Global step 310 Train loss 5.73 on epoch=154
06/23/2022 16:42:31 - INFO - __main__ - Step 320 Global step 320 Train loss 5.71 on epoch=159
06/23/2022 16:42:32 - INFO - __main__ - Step 330 Global step 330 Train loss 5.63 on epoch=164
06/23/2022 16:42:34 - INFO - __main__ - Step 340 Global step 340 Train loss 5.60 on epoch=169
06/23/2022 16:42:35 - INFO - __main__ - Step 350 Global step 350 Train loss 5.55 on epoch=174
06/23/2022 16:42:37 - INFO - __main__ - Global step 350 Train loss 5.64 ACC 0.0 on epoch=174
06/23/2022 16:42:38 - INFO - __main__ - Step 360 Global step 360 Train loss 5.32 on epoch=179
06/23/2022 16:42:39 - INFO - __main__ - Step 370 Global step 370 Train loss 5.32 on epoch=184
06/23/2022 16:42:41 - INFO - __main__ - Step 380 Global step 380 Train loss 5.22 on epoch=189
06/23/2022 16:42:42 - INFO - __main__ - Step 390 Global step 390 Train loss 5.21 on epoch=194
06/23/2022 16:42:43 - INFO - __main__ - Step 400 Global step 400 Train loss 4.98 on epoch=199
06/23/2022 16:42:46 - INFO - __main__ - Global step 400 Train loss 5.21 ACC 0.0 on epoch=199
06/23/2022 16:42:47 - INFO - __main__ - Step 410 Global step 410 Train loss 5.01 on epoch=204
06/23/2022 16:42:49 - INFO - __main__ - Step 420 Global step 420 Train loss 4.94 on epoch=209
06/23/2022 16:42:50 - INFO - __main__ - Step 430 Global step 430 Train loss 4.94 on epoch=214
06/23/2022 16:42:51 - INFO - __main__ - Step 440 Global step 440 Train loss 4.88 on epoch=219
06/23/2022 16:42:53 - INFO - __main__ - Step 450 Global step 450 Train loss 4.67 on epoch=224
06/23/2022 16:42:54 - INFO - __main__ - Global step 450 Train loss 4.89 ACC 0.0 on epoch=224
06/23/2022 16:42:55 - INFO - __main__ - Step 460 Global step 460 Train loss 4.68 on epoch=229
06/23/2022 16:42:57 - INFO - __main__ - Step 470 Global step 470 Train loss 4.51 on epoch=234
06/23/2022 16:42:58 - INFO - __main__ - Step 480 Global step 480 Train loss 4.54 on epoch=239
06/23/2022 16:42:59 - INFO - __main__ - Step 490 Global step 490 Train loss 4.51 on epoch=244
06/23/2022 16:43:01 - INFO - __main__ - Step 500 Global step 500 Train loss 4.44 on epoch=249
06/23/2022 16:43:02 - INFO - __main__ - Global step 500 Train loss 4.54 ACC 0.0 on epoch=249
06/23/2022 16:43:03 - INFO - __main__ - Step 510 Global step 510 Train loss 4.58 on epoch=254
06/23/2022 16:43:05 - INFO - __main__ - Step 520 Global step 520 Train loss 4.48 on epoch=259
06/23/2022 16:43:06 - INFO - __main__ - Step 530 Global step 530 Train loss 4.37 on epoch=264
06/23/2022 16:43:07 - INFO - __main__ - Step 540 Global step 540 Train loss 4.37 on epoch=269
06/23/2022 16:43:09 - INFO - __main__ - Step 550 Global step 550 Train loss 4.25 on epoch=274
06/23/2022 16:43:16 - INFO - __main__ - Global step 550 Train loss 4.41 ACC 0.0 on epoch=274
06/23/2022 16:43:17 - INFO - __main__ - Step 560 Global step 560 Train loss 4.31 on epoch=279
06/23/2022 16:43:19 - INFO - __main__ - Step 570 Global step 570 Train loss 4.16 on epoch=284
06/23/2022 16:43:20 - INFO - __main__ - Step 580 Global step 580 Train loss 4.27 on epoch=289
06/23/2022 16:43:21 - INFO - __main__ - Step 590 Global step 590 Train loss 4.08 on epoch=294
06/23/2022 16:43:23 - INFO - __main__ - Step 600 Global step 600 Train loss 4.10 on epoch=299
06/23/2022 16:43:27 - INFO - __main__ - Global step 600 Train loss 4.18 ACC 0.0 on epoch=299
06/23/2022 16:43:28 - INFO - __main__ - Step 610 Global step 610 Train loss 4.18 on epoch=304
06/23/2022 16:43:29 - INFO - __main__ - Step 620 Global step 620 Train loss 3.96 on epoch=309
06/23/2022 16:43:31 - INFO - __main__ - Step 630 Global step 630 Train loss 3.93 on epoch=314
06/23/2022 16:43:32 - INFO - __main__ - Step 640 Global step 640 Train loss 3.90 on epoch=319
06/23/2022 16:43:33 - INFO - __main__ - Step 650 Global step 650 Train loss 3.88 on epoch=324
06/23/2022 16:43:41 - INFO - __main__ - Global step 650 Train loss 3.97 ACC 0.0 on epoch=324
06/23/2022 16:43:42 - INFO - __main__ - Step 660 Global step 660 Train loss 3.76 on epoch=329
06/23/2022 16:43:43 - INFO - __main__ - Step 670 Global step 670 Train loss 3.76 on epoch=334
06/23/2022 16:43:45 - INFO - __main__ - Step 680 Global step 680 Train loss 3.81 on epoch=339
06/23/2022 16:43:46 - INFO - __main__ - Step 690 Global step 690 Train loss 3.59 on epoch=344
06/23/2022 16:43:47 - INFO - __main__ - Step 700 Global step 700 Train loss 3.65 on epoch=349
06/23/2022 16:43:59 - INFO - __main__ - Global step 700 Train loss 3.71 ACC 0.0 on epoch=349
06/23/2022 16:44:01 - INFO - __main__ - Step 710 Global step 710 Train loss 3.58 on epoch=354
06/23/2022 16:44:02 - INFO - __main__ - Step 720 Global step 720 Train loss 3.52 on epoch=359
06/23/2022 16:44:03 - INFO - __main__ - Step 730 Global step 730 Train loss 3.41 on epoch=364
06/23/2022 16:44:05 - INFO - __main__ - Step 740 Global step 740 Train loss 3.42 on epoch=369
06/23/2022 16:44:06 - INFO - __main__ - Step 750 Global step 750 Train loss 3.38 on epoch=374
06/23/2022 16:44:17 - INFO - __main__ - Global step 750 Train loss 3.46 ACC 0.0 on epoch=374
06/23/2022 16:44:19 - INFO - __main__ - Step 760 Global step 760 Train loss 3.17 on epoch=379
06/23/2022 16:44:20 - INFO - __main__ - Step 770 Global step 770 Train loss 3.24 on epoch=384
06/23/2022 16:44:21 - INFO - __main__ - Step 780 Global step 780 Train loss 3.22 on epoch=389
06/23/2022 16:44:23 - INFO - __main__ - Step 790 Global step 790 Train loss 3.35 on epoch=394
06/23/2022 16:44:24 - INFO - __main__ - Step 800 Global step 800 Train loss 3.12 on epoch=399
06/23/2022 16:44:34 - INFO - __main__ - Global step 800 Train loss 3.22 ACC 0.0 on epoch=399
06/23/2022 16:44:36 - INFO - __main__ - Step 810 Global step 810 Train loss 3.05 on epoch=404
06/23/2022 16:44:37 - INFO - __main__ - Step 820 Global step 820 Train loss 2.99 on epoch=409
06/23/2022 16:44:38 - INFO - __main__ - Step 830 Global step 830 Train loss 3.18 on epoch=414
06/23/2022 16:44:40 - INFO - __main__ - Step 840 Global step 840 Train loss 3.03 on epoch=419
06/23/2022 16:44:41 - INFO - __main__ - Step 850 Global step 850 Train loss 2.96 on epoch=424
06/23/2022 16:44:51 - INFO - __main__ - Global step 850 Train loss 3.04 ACC 0.0 on epoch=424
06/23/2022 16:44:53 - INFO - __main__ - Step 860 Global step 860 Train loss 2.95 on epoch=429
06/23/2022 16:44:54 - INFO - __main__ - Step 870 Global step 870 Train loss 2.88 on epoch=434
06/23/2022 16:44:55 - INFO - __main__ - Step 880 Global step 880 Train loss 2.82 on epoch=439
06/23/2022 16:44:57 - INFO - __main__ - Step 890 Global step 890 Train loss 2.66 on epoch=444
06/23/2022 16:44:58 - INFO - __main__ - Step 900 Global step 900 Train loss 2.70 on epoch=449
06/23/2022 16:45:09 - INFO - __main__ - Global step 900 Train loss 2.80 ACC 0.0 on epoch=449
06/23/2022 16:45:10 - INFO - __main__ - Step 910 Global step 910 Train loss 2.67 on epoch=454
06/23/2022 16:45:11 - INFO - __main__ - Step 920 Global step 920 Train loss 2.75 on epoch=459
06/23/2022 16:45:12 - INFO - __main__ - Step 930 Global step 930 Train loss 2.67 on epoch=464
06/23/2022 16:45:14 - INFO - __main__ - Step 940 Global step 940 Train loss 2.55 on epoch=469
06/23/2022 16:45:15 - INFO - __main__ - Step 950 Global step 950 Train loss 2.44 on epoch=474
06/23/2022 16:45:22 - INFO - __main__ - Global step 950 Train loss 2.62 ACC 0.0 on epoch=474
06/23/2022 16:45:24 - INFO - __main__ - Step 960 Global step 960 Train loss 2.56 on epoch=479
06/23/2022 16:45:25 - INFO - __main__ - Step 970 Global step 970 Train loss 2.44 on epoch=484
06/23/2022 16:45:26 - INFO - __main__ - Step 980 Global step 980 Train loss 2.36 on epoch=489
06/23/2022 16:45:28 - INFO - __main__ - Step 990 Global step 990 Train loss 2.32 on epoch=494
06/23/2022 16:45:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.25 on epoch=499
06/23/2022 16:45:31 - INFO - __main__ - Global step 1000 Train loss 2.39 ACC 0.15625 on epoch=499
06/23/2022 16:45:31 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.15625 on epoch=499, global_step=1000
06/23/2022 16:45:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.22 on epoch=504
06/23/2022 16:45:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.24 on epoch=509
06/23/2022 16:45:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.14 on epoch=514
06/23/2022 16:45:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.17 on epoch=519
06/23/2022 16:45:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.10 on epoch=524
06/23/2022 16:45:38 - INFO - __main__ - Global step 1050 Train loss 2.17 ACC 0.53125 on epoch=524
06/23/2022 16:45:39 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.53125 on epoch=524, global_step=1050
06/23/2022 16:45:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.21 on epoch=529
06/23/2022 16:45:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.09 on epoch=534
06/23/2022 16:45:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.12 on epoch=539
06/23/2022 16:45:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.13 on epoch=544
06/23/2022 16:45:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.04 on epoch=549
06/23/2022 16:45:46 - INFO - __main__ - Global step 1100 Train loss 2.11 ACC 0.5 on epoch=549
06/23/2022 16:45:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.93 on epoch=554
06/23/2022 16:45:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.95 on epoch=559
06/23/2022 16:45:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.02 on epoch=564
06/23/2022 16:45:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.86 on epoch=569
06/23/2022 16:45:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.93 on epoch=574
06/23/2022 16:45:57 - INFO - __main__ - Global step 1150 Train loss 1.94 ACC 0.5625 on epoch=574
06/23/2022 16:45:57 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=574, global_step=1150
06/23/2022 16:45:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.86 on epoch=579
06/23/2022 16:45:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.77 on epoch=584
06/23/2022 16:46:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.75 on epoch=589
06/23/2022 16:46:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.68 on epoch=594
06/23/2022 16:46:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.59 on epoch=599
06/23/2022 16:46:09 - INFO - __main__ - Global step 1200 Train loss 1.73 ACC 0.46875 on epoch=599
06/23/2022 16:46:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.61 on epoch=604
06/23/2022 16:46:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.56 on epoch=609
06/23/2022 16:46:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.52 on epoch=614
06/23/2022 16:46:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.46 on epoch=619
06/23/2022 16:46:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.54 on epoch=624
06/23/2022 16:46:17 - INFO - __main__ - Global step 1250 Train loss 1.54 ACC 0.53125 on epoch=624
06/23/2022 16:46:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.40 on epoch=629
06/23/2022 16:46:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.32 on epoch=634
06/23/2022 16:46:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.28 on epoch=639
06/23/2022 16:46:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.42 on epoch=644
06/23/2022 16:46:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.29 on epoch=649
06/23/2022 16:46:27 - INFO - __main__ - Global step 1300 Train loss 1.34 ACC 0.46875 on epoch=649
06/23/2022 16:46:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.23 on epoch=654
06/23/2022 16:46:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.25 on epoch=659
06/23/2022 16:46:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.24 on epoch=664
06/23/2022 16:46:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.28 on epoch=669
06/23/2022 16:46:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.28 on epoch=674
06/23/2022 16:46:35 - INFO - __main__ - Global step 1350 Train loss 1.26 ACC 0.4375 on epoch=674
06/23/2022 16:46:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.25 on epoch=679
06/23/2022 16:46:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.12 on epoch=684
06/23/2022 16:46:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.13 on epoch=689
06/23/2022 16:46:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.14 on epoch=694
06/23/2022 16:46:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.05 on epoch=699
06/23/2022 16:46:48 - INFO - __main__ - Global step 1400 Train loss 1.14 ACC 0.5625 on epoch=699
06/23/2022 16:46:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.03 on epoch=704
06/23/2022 16:46:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.17 on epoch=709
06/23/2022 16:46:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.04 on epoch=714
06/23/2022 16:46:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.00 on epoch=719
06/23/2022 16:46:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.00 on epoch=724
06/23/2022 16:46:57 - INFO - __main__ - Global step 1450 Train loss 1.05 ACC 0.5 on epoch=724
06/23/2022 16:46:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.01 on epoch=729
06/23/2022 16:47:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.03 on epoch=734
06/23/2022 16:47:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.93 on epoch=739
06/23/2022 16:47:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.87 on epoch=744
06/23/2022 16:47:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.91 on epoch=749
06/23/2022 16:47:06 - INFO - __main__ - Global step 1500 Train loss 0.95 ACC 0.53125 on epoch=749
06/23/2022 16:47:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.92 on epoch=754
06/23/2022 16:47:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.89 on epoch=759
06/23/2022 16:47:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.84 on epoch=764
06/23/2022 16:47:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.85 on epoch=769
06/23/2022 16:47:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.84 on epoch=774
06/23/2022 16:47:14 - INFO - __main__ - Global step 1550 Train loss 0.87 ACC 0.5 on epoch=774
06/23/2022 16:47:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.85 on epoch=779
06/23/2022 16:47:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.79 on epoch=784
06/23/2022 16:47:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.71 on epoch=789
06/23/2022 16:47:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.67 on epoch=794
06/23/2022 16:47:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.75 on epoch=799
06/23/2022 16:47:22 - INFO - __main__ - Global step 1600 Train loss 0.76 ACC 0.5 on epoch=799
06/23/2022 16:47:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.71 on epoch=804
06/23/2022 16:47:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.75 on epoch=809
06/23/2022 16:47:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.70 on epoch=814
06/23/2022 16:47:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.66 on epoch=819
06/23/2022 16:47:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.58 on epoch=824
06/23/2022 16:47:30 - INFO - __main__ - Global step 1650 Train loss 0.68 ACC 0.5 on epoch=824
06/23/2022 16:47:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.57 on epoch=829
06/23/2022 16:47:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.58 on epoch=834
06/23/2022 16:47:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.58 on epoch=839
06/23/2022 16:47:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.56 on epoch=844
06/23/2022 16:47:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.57 on epoch=849
06/23/2022 16:47:38 - INFO - __main__ - Global step 1700 Train loss 0.57 ACC 0.5 on epoch=849
06/23/2022 16:47:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.62 on epoch=854
06/23/2022 16:47:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.60 on epoch=859
06/23/2022 16:47:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.50 on epoch=864
06/23/2022 16:47:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.61 on epoch=869
06/23/2022 16:47:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.50 on epoch=874
06/23/2022 16:47:48 - INFO - __main__ - Global step 1750 Train loss 0.57 ACC 0.5 on epoch=874
06/23/2022 16:47:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.59 on epoch=879
06/23/2022 16:47:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.60 on epoch=884
06/23/2022 16:47:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.64 on epoch=889
06/23/2022 16:47:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.51 on epoch=894
06/23/2022 16:47:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.56 on epoch=899
06/23/2022 16:47:55 - INFO - __main__ - Global step 1800 Train loss 0.58 ACC 0.5 on epoch=899
06/23/2022 16:47:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.59 on epoch=904
06/23/2022 16:47:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.51 on epoch=909
06/23/2022 16:47:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.50 on epoch=914
06/23/2022 16:48:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.49 on epoch=919
06/23/2022 16:48:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.47 on epoch=924
06/23/2022 16:48:02 - INFO - __main__ - Global step 1850 Train loss 0.51 ACC 0.5 on epoch=924
06/23/2022 16:48:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.59 on epoch=929
06/23/2022 16:48:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.54 on epoch=934
06/23/2022 16:48:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=939
06/23/2022 16:48:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.55 on epoch=944
06/23/2022 16:48:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.49 on epoch=949
06/23/2022 16:48:09 - INFO - __main__ - Global step 1900 Train loss 0.51 ACC 0.53125 on epoch=949
06/23/2022 16:48:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.50 on epoch=954
06/23/2022 16:48:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/23/2022 16:48:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.51 on epoch=964
06/23/2022 16:48:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.53 on epoch=969
06/23/2022 16:48:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.46 on epoch=974
06/23/2022 16:48:17 - INFO - __main__ - Global step 1950 Train loss 0.49 ACC 0.59375 on epoch=974
06/23/2022 16:48:17 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=974, global_step=1950
06/23/2022 16:48:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=979
06/23/2022 16:48:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=984
06/23/2022 16:48:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.45 on epoch=989
06/23/2022 16:48:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=994
06/23/2022 16:48:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=999
06/23/2022 16:48:26 - INFO - __main__ - Global step 2000 Train loss 0.44 ACC 0.46875 on epoch=999
06/23/2022 16:48:26 - INFO - __main__ - save last model!
06/23/2022 16:48:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 16:48:26 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 16:48:26 - INFO - __main__ - Printing 3 examples
06/23/2022 16:48:26 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 16:48:26 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:26 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 16:48:26 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:48:26 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 16:48:26 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:48:26 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:48:27 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:48:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:48:27 - INFO - __main__ - Printing 3 examples
06/23/2022 16:48:27 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/23/2022 16:48:27 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:27 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/23/2022 16:48:27 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/23/2022 16:48:27 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 16:48:27 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:48:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:48:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:48:27 - INFO - __main__ - Printing 3 examples
06/23/2022 16:48:27 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/23/2022 16:48:27 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:27 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/23/2022 16:48:27 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/23/2022 16:48:27 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:27 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:48:27 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:48:27 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:48:27 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 16:48:33 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:48:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:48:34 - INFO - __main__ - Starting training!
06/23/2022 16:48:36 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_21_0.3_8_predictions.txt
06/23/2022 16:48:36 - INFO - __main__ - ACC on test data: 0.6569
06/23/2022 16:48:36 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.3, bsz=8, dev_performance=0.59375, test_performance=0.6568627450980392
06/23/2022 16:48:36 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.2, bsz=8 ...
06/23/2022 16:48:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:48:37 - INFO - __main__ - Printing 3 examples
06/23/2022 16:48:37 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/23/2022 16:48:37 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:37 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/23/2022 16:48:37 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:37 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/23/2022 16:48:37 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 16:48:37 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:48:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:48:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:48:37 - INFO - __main__ - Printing 3 examples
06/23/2022 16:48:37 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/23/2022 16:48:37 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:37 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/23/2022 16:48:37 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:37 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/23/2022 16:48:37 - INFO - __main__ - ['equivalent']
06/23/2022 16:48:37 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:48:37 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:48:37 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:48:44 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:48:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:48:44 - INFO - __main__ - Starting training!
06/23/2022 16:48:46 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=4
06/23/2022 16:48:47 - INFO - __main__ - Step 20 Global step 20 Train loss 6.78 on epoch=9
06/23/2022 16:48:48 - INFO - __main__ - Step 30 Global step 30 Train loss 6.83 on epoch=14
06/23/2022 16:48:50 - INFO - __main__ - Step 40 Global step 40 Train loss 6.87 on epoch=19
06/23/2022 16:48:51 - INFO - __main__ - Step 50 Global step 50 Train loss 6.82 on epoch=24
06/23/2022 16:48:55 - INFO - __main__ - Global step 50 Train loss 6.83 ACC 0.0 on epoch=24
06/23/2022 16:48:55 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 16:48:56 - INFO - __main__ - Step 60 Global step 60 Train loss 6.76 on epoch=29
06/23/2022 16:48:58 - INFO - __main__ - Step 70 Global step 70 Train loss 6.83 on epoch=34
06/23/2022 16:48:59 - INFO - __main__ - Step 80 Global step 80 Train loss 6.91 on epoch=39
06/23/2022 16:49:01 - INFO - __main__ - Step 90 Global step 90 Train loss 6.86 on epoch=44
06/23/2022 16:49:02 - INFO - __main__ - Step 100 Global step 100 Train loss 6.88 on epoch=49
06/23/2022 16:49:05 - INFO - __main__ - Global step 100 Train loss 6.85 ACC 0.0 on epoch=49
06/23/2022 16:49:06 - INFO - __main__ - Step 110 Global step 110 Train loss 6.83 on epoch=54
06/23/2022 16:49:08 - INFO - __main__ - Step 120 Global step 120 Train loss 6.81 on epoch=59
06/23/2022 16:49:09 - INFO - __main__ - Step 130 Global step 130 Train loss 6.80 on epoch=64
06/23/2022 16:49:10 - INFO - __main__ - Step 140 Global step 140 Train loss 6.78 on epoch=69
06/23/2022 16:49:12 - INFO - __main__ - Step 150 Global step 150 Train loss 6.75 on epoch=74
06/23/2022 16:49:13 - INFO - __main__ - Global step 150 Train loss 6.79 ACC 0.0 on epoch=74
06/23/2022 16:49:15 - INFO - __main__ - Step 160 Global step 160 Train loss 6.71 on epoch=79
06/23/2022 16:49:16 - INFO - __main__ - Step 170 Global step 170 Train loss 6.69 on epoch=84
06/23/2022 16:49:17 - INFO - __main__ - Step 180 Global step 180 Train loss 6.67 on epoch=89
06/23/2022 16:49:19 - INFO - __main__ - Step 190 Global step 190 Train loss 6.77 on epoch=94
06/23/2022 16:49:20 - INFO - __main__ - Step 200 Global step 200 Train loss 6.66 on epoch=99
06/23/2022 16:49:23 - INFO - __main__ - Global step 200 Train loss 6.70 ACC 0.0 on epoch=99
06/23/2022 16:49:25 - INFO - __main__ - Step 210 Global step 210 Train loss 6.67 on epoch=104
06/23/2022 16:49:27 - INFO - __main__ - Step 220 Global step 220 Train loss 6.64 on epoch=109
06/23/2022 16:49:28 - INFO - __main__ - Step 230 Global step 230 Train loss 6.68 on epoch=114
06/23/2022 16:49:30 - INFO - __main__ - Step 240 Global step 240 Train loss 6.57 on epoch=119
06/23/2022 16:49:31 - INFO - __main__ - Step 250 Global step 250 Train loss 6.51 on epoch=124
06/23/2022 16:49:40 - INFO - __main__ - Global step 250 Train loss 6.61 ACC 0.0 on epoch=124
06/23/2022 16:49:41 - INFO - __main__ - Step 260 Global step 260 Train loss 6.55 on epoch=129
06/23/2022 16:49:43 - INFO - __main__ - Step 270 Global step 270 Train loss 6.54 on epoch=134
06/23/2022 16:49:44 - INFO - __main__ - Step 280 Global step 280 Train loss 6.42 on epoch=139
06/23/2022 16:49:46 - INFO - __main__ - Step 290 Global step 290 Train loss 6.47 on epoch=144
06/23/2022 16:49:47 - INFO - __main__ - Step 300 Global step 300 Train loss 6.38 on epoch=149
06/23/2022 16:49:56 - INFO - __main__ - Global step 300 Train loss 6.47 ACC 0.0 on epoch=149
06/23/2022 16:49:57 - INFO - __main__ - Step 310 Global step 310 Train loss 6.37 on epoch=154
06/23/2022 16:49:58 - INFO - __main__ - Step 320 Global step 320 Train loss 6.40 on epoch=159
06/23/2022 16:50:00 - INFO - __main__ - Step 330 Global step 330 Train loss 6.38 on epoch=164
06/23/2022 16:50:01 - INFO - __main__ - Step 340 Global step 340 Train loss 6.27 on epoch=169
06/23/2022 16:50:03 - INFO - __main__ - Step 350 Global step 350 Train loss 6.22 on epoch=174
06/23/2022 16:50:08 - INFO - __main__ - Global step 350 Train loss 6.33 ACC 0.0 on epoch=174
06/23/2022 16:50:09 - INFO - __main__ - Step 360 Global step 360 Train loss 6.18 on epoch=179
06/23/2022 16:50:11 - INFO - __main__ - Step 370 Global step 370 Train loss 6.24 on epoch=184
06/23/2022 16:50:12 - INFO - __main__ - Step 380 Global step 380 Train loss 6.11 on epoch=189
06/23/2022 16:50:14 - INFO - __main__ - Step 390 Global step 390 Train loss 6.10 on epoch=194
06/23/2022 16:50:15 - INFO - __main__ - Step 400 Global step 400 Train loss 6.16 on epoch=199
06/23/2022 16:50:18 - INFO - __main__ - Global step 400 Train loss 6.16 ACC 0.0 on epoch=199
06/23/2022 16:50:19 - INFO - __main__ - Step 410 Global step 410 Train loss 6.07 on epoch=204
06/23/2022 16:50:21 - INFO - __main__ - Step 420 Global step 420 Train loss 6.07 on epoch=209
06/23/2022 16:50:22 - INFO - __main__ - Step 430 Global step 430 Train loss 5.95 on epoch=214
06/23/2022 16:50:23 - INFO - __main__ - Step 440 Global step 440 Train loss 5.89 on epoch=219
06/23/2022 16:50:25 - INFO - __main__ - Step 450 Global step 450 Train loss 5.96 on epoch=224
06/23/2022 16:50:27 - INFO - __main__ - Global step 450 Train loss 5.99 ACC 0.0 on epoch=224
06/23/2022 16:50:29 - INFO - __main__ - Step 460 Global step 460 Train loss 5.75 on epoch=229
06/23/2022 16:50:30 - INFO - __main__ - Step 470 Global step 470 Train loss 5.71 on epoch=234
06/23/2022 16:50:32 - INFO - __main__ - Step 480 Global step 480 Train loss 5.71 on epoch=239
06/23/2022 16:50:33 - INFO - __main__ - Step 490 Global step 490 Train loss 5.78 on epoch=244
06/23/2022 16:50:35 - INFO - __main__ - Step 500 Global step 500 Train loss 5.56 on epoch=249
06/23/2022 16:50:37 - INFO - __main__ - Global step 500 Train loss 5.70 ACC 0.0 on epoch=249
06/23/2022 16:50:38 - INFO - __main__ - Step 510 Global step 510 Train loss 5.48 on epoch=254
06/23/2022 16:50:40 - INFO - __main__ - Step 520 Global step 520 Train loss 5.48 on epoch=259
06/23/2022 16:50:41 - INFO - __main__ - Step 530 Global step 530 Train loss 5.35 on epoch=264
06/23/2022 16:50:42 - INFO - __main__ - Step 540 Global step 540 Train loss 5.21 on epoch=269
06/23/2022 16:50:44 - INFO - __main__ - Step 550 Global step 550 Train loss 5.33 on epoch=274
06/23/2022 16:50:46 - INFO - __main__ - Global step 550 Train loss 5.37 ACC 0.0 on epoch=274
06/23/2022 16:50:47 - INFO - __main__ - Step 560 Global step 560 Train loss 5.33 on epoch=279
06/23/2022 16:50:49 - INFO - __main__ - Step 570 Global step 570 Train loss 4.98 on epoch=284
06/23/2022 16:50:50 - INFO - __main__ - Step 580 Global step 580 Train loss 5.21 on epoch=289
06/23/2022 16:50:52 - INFO - __main__ - Step 590 Global step 590 Train loss 5.41 on epoch=294
06/23/2022 16:50:53 - INFO - __main__ - Step 600 Global step 600 Train loss 5.43 on epoch=299
06/23/2022 16:50:59 - INFO - __main__ - Global step 600 Train loss 5.27 ACC 0.0 on epoch=299
06/23/2022 16:51:00 - INFO - __main__ - Step 610 Global step 610 Train loss 5.31 on epoch=304
06/23/2022 16:51:02 - INFO - __main__ - Step 620 Global step 620 Train loss 5.32 on epoch=309
06/23/2022 16:51:03 - INFO - __main__ - Step 630 Global step 630 Train loss 5.13 on epoch=314
06/23/2022 16:51:04 - INFO - __main__ - Step 640 Global step 640 Train loss 4.95 on epoch=319
06/23/2022 16:51:06 - INFO - __main__ - Step 650 Global step 650 Train loss 4.93 on epoch=324
06/23/2022 16:51:12 - INFO - __main__ - Global step 650 Train loss 5.13 ACC 0.0 on epoch=324
06/23/2022 16:51:14 - INFO - __main__ - Step 660 Global step 660 Train loss 4.94 on epoch=329
06/23/2022 16:51:15 - INFO - __main__ - Step 670 Global step 670 Train loss 4.91 on epoch=334
06/23/2022 16:51:16 - INFO - __main__ - Step 680 Global step 680 Train loss 4.84 on epoch=339
06/23/2022 16:51:18 - INFO - __main__ - Step 690 Global step 690 Train loss 4.91 on epoch=344
06/23/2022 16:51:19 - INFO - __main__ - Step 700 Global step 700 Train loss 4.83 on epoch=349
06/23/2022 16:51:22 - INFO - __main__ - Global step 700 Train loss 4.89 ACC 0.0 on epoch=349
06/23/2022 16:51:24 - INFO - __main__ - Step 710 Global step 710 Train loss 4.77 on epoch=354
06/23/2022 16:51:25 - INFO - __main__ - Step 720 Global step 720 Train loss 4.85 on epoch=359
06/23/2022 16:51:27 - INFO - __main__ - Step 730 Global step 730 Train loss 4.70 on epoch=364
06/23/2022 16:51:28 - INFO - __main__ - Step 740 Global step 740 Train loss 4.63 on epoch=369
06/23/2022 16:51:29 - INFO - __main__ - Step 750 Global step 750 Train loss 4.66 on epoch=374
06/23/2022 16:51:31 - INFO - __main__ - Global step 750 Train loss 4.72 ACC 0.0 on epoch=374
06/23/2022 16:51:33 - INFO - __main__ - Step 760 Global step 760 Train loss 4.63 on epoch=379
06/23/2022 16:51:34 - INFO - __main__ - Step 770 Global step 770 Train loss 4.67 on epoch=384
06/23/2022 16:51:35 - INFO - __main__ - Step 780 Global step 780 Train loss 4.79 on epoch=389
06/23/2022 16:51:37 - INFO - __main__ - Step 790 Global step 790 Train loss 4.63 on epoch=394
06/23/2022 16:51:38 - INFO - __main__ - Step 800 Global step 800 Train loss 4.68 on epoch=399
06/23/2022 16:51:47 - INFO - __main__ - Global step 800 Train loss 4.68 ACC 0.0 on epoch=399
06/23/2022 16:51:48 - INFO - __main__ - Step 810 Global step 810 Train loss 4.46 on epoch=404
06/23/2022 16:51:49 - INFO - __main__ - Step 820 Global step 820 Train loss 4.53 on epoch=409
06/23/2022 16:51:51 - INFO - __main__ - Step 830 Global step 830 Train loss 4.43 on epoch=414
06/23/2022 16:51:52 - INFO - __main__ - Step 840 Global step 840 Train loss 4.56 on epoch=419
06/23/2022 16:51:54 - INFO - __main__ - Step 850 Global step 850 Train loss 4.51 on epoch=424
06/23/2022 16:51:55 - INFO - __main__ - Global step 850 Train loss 4.50 ACC 0.0 on epoch=424
06/23/2022 16:51:57 - INFO - __main__ - Step 860 Global step 860 Train loss 4.43 on epoch=429
06/23/2022 16:51:59 - INFO - __main__ - Step 870 Global step 870 Train loss 4.43 on epoch=434
06/23/2022 16:52:00 - INFO - __main__ - Step 880 Global step 880 Train loss 4.53 on epoch=439
06/23/2022 16:52:02 - INFO - __main__ - Step 890 Global step 890 Train loss 4.47 on epoch=444
06/23/2022 16:52:03 - INFO - __main__ - Step 900 Global step 900 Train loss 4.33 on epoch=449
06/23/2022 16:52:04 - INFO - __main__ - Global step 900 Train loss 4.44 ACC 0.0 on epoch=449
06/23/2022 16:52:06 - INFO - __main__ - Step 910 Global step 910 Train loss 4.44 on epoch=454
06/23/2022 16:52:07 - INFO - __main__ - Step 920 Global step 920 Train loss 4.35 on epoch=459
06/23/2022 16:52:09 - INFO - __main__ - Step 930 Global step 930 Train loss 4.32 on epoch=464
06/23/2022 16:52:10 - INFO - __main__ - Step 940 Global step 940 Train loss 4.41 on epoch=469
06/23/2022 16:52:12 - INFO - __main__ - Step 950 Global step 950 Train loss 4.34 on epoch=474
06/23/2022 16:52:13 - INFO - __main__ - Global step 950 Train loss 4.37 ACC 0.0 on epoch=474
06/23/2022 16:52:14 - INFO - __main__ - Step 960 Global step 960 Train loss 4.48 on epoch=479
06/23/2022 16:52:16 - INFO - __main__ - Step 970 Global step 970 Train loss 4.20 on epoch=484
06/23/2022 16:52:17 - INFO - __main__ - Step 980 Global step 980 Train loss 4.35 on epoch=489
06/23/2022 16:52:19 - INFO - __main__ - Step 990 Global step 990 Train loss 4.26 on epoch=494
06/23/2022 16:52:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 4.27 on epoch=499
06/23/2022 16:52:22 - INFO - __main__ - Global step 1000 Train loss 4.31 ACC 0.0 on epoch=499
06/23/2022 16:52:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 4.31 on epoch=504
06/23/2022 16:52:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 4.15 on epoch=509
06/23/2022 16:52:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 4.13 on epoch=514
06/23/2022 16:52:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 4.18 on epoch=519
06/23/2022 16:52:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 4.18 on epoch=524
06/23/2022 16:52:30 - INFO - __main__ - Global step 1050 Train loss 4.19 ACC 0.0 on epoch=524
06/23/2022 16:52:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 4.12 on epoch=529
06/23/2022 16:52:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 4.04 on epoch=534
06/23/2022 16:52:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 4.09 on epoch=539
06/23/2022 16:52:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 4.06 on epoch=544
06/23/2022 16:52:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 4.00 on epoch=549
06/23/2022 16:52:40 - INFO - __main__ - Global step 1100 Train loss 4.06 ACC 0.0 on epoch=549
06/23/2022 16:52:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 4.03 on epoch=554
06/23/2022 16:52:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.95 on epoch=559
06/23/2022 16:52:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.94 on epoch=564
06/23/2022 16:52:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.92 on epoch=569
06/23/2022 16:52:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 4.08 on epoch=574
06/23/2022 16:52:49 - INFO - __main__ - Global step 1150 Train loss 3.98 ACC 0.0 on epoch=574
06/23/2022 16:52:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 4.05 on epoch=579
06/23/2022 16:52:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.96 on epoch=584
06/23/2022 16:52:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.98 on epoch=589
06/23/2022 16:52:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.88 on epoch=594
06/23/2022 16:52:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.93 on epoch=599
06/23/2022 16:52:58 - INFO - __main__ - Global step 1200 Train loss 3.96 ACC 0.0 on epoch=599
06/23/2022 16:52:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.90 on epoch=604
06/23/2022 16:53:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.65 on epoch=609
06/23/2022 16:53:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.88 on epoch=614
06/23/2022 16:53:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.81 on epoch=619
06/23/2022 16:53:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.80 on epoch=624
06/23/2022 16:53:06 - INFO - __main__ - Global step 1250 Train loss 3.81 ACC 0.0 on epoch=624
06/23/2022 16:53:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.80 on epoch=629
06/23/2022 16:53:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.88 on epoch=634
06/23/2022 16:53:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.75 on epoch=639
06/23/2022 16:53:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.66 on epoch=644
06/23/2022 16:53:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.66 on epoch=649
06/23/2022 16:53:21 - INFO - __main__ - Global step 1300 Train loss 3.75 ACC 0.0 on epoch=649
06/23/2022 16:53:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.48 on epoch=654
06/23/2022 16:53:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.65 on epoch=659
06/23/2022 16:53:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 3.63 on epoch=664
06/23/2022 16:53:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 3.65 on epoch=669
06/23/2022 16:53:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 3.52 on epoch=674
06/23/2022 16:53:34 - INFO - __main__ - Global step 1350 Train loss 3.59 ACC 0.0 on epoch=674
06/23/2022 16:53:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 3.60 on epoch=679
06/23/2022 16:53:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 3.46 on epoch=684
06/23/2022 16:53:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 3.61 on epoch=689
06/23/2022 16:53:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 3.48 on epoch=694
06/23/2022 16:53:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 3.40 on epoch=699
06/23/2022 16:53:45 - INFO - __main__ - Global step 1400 Train loss 3.51 ACC 0.0 on epoch=699
06/23/2022 16:53:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 3.37 on epoch=704
06/23/2022 16:53:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 3.36 on epoch=709
06/23/2022 16:53:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 3.32 on epoch=714
06/23/2022 16:53:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 3.31 on epoch=719
06/23/2022 16:53:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 3.23 on epoch=724
06/23/2022 16:53:55 - INFO - __main__ - Global step 1450 Train loss 3.31 ACC 0.0 on epoch=724
06/23/2022 16:53:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 3.37 on epoch=729
06/23/2022 16:53:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 3.40 on epoch=734
06/23/2022 16:53:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 3.30 on epoch=739
06/23/2022 16:54:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 3.15 on epoch=744
06/23/2022 16:54:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 3.29 on epoch=749
06/23/2022 16:54:05 - INFO - __main__ - Global step 1500 Train loss 3.30 ACC 0.0 on epoch=749
06/23/2022 16:54:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 3.32 on epoch=754
06/23/2022 16:54:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 3.17 on epoch=759
06/23/2022 16:54:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 3.25 on epoch=764
06/23/2022 16:54:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 3.11 on epoch=769
06/23/2022 16:54:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 3.20 on epoch=774
06/23/2022 16:54:21 - INFO - __main__ - Global step 1550 Train loss 3.21 ACC 0.0 on epoch=774
06/23/2022 16:54:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 3.19 on epoch=779
06/23/2022 16:54:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 3.07 on epoch=784
06/23/2022 16:54:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 3.19 on epoch=789
06/23/2022 16:54:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 3.05 on epoch=794
06/23/2022 16:54:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 3.03 on epoch=799
06/23/2022 16:54:30 - INFO - __main__ - Global step 1600 Train loss 3.11 ACC 0.0 on epoch=799
06/23/2022 16:54:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 2.89 on epoch=804
06/23/2022 16:54:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.95 on epoch=809
06/23/2022 16:54:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 2.90 on epoch=814
06/23/2022 16:54:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 2.95 on epoch=819
06/23/2022 16:54:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 2.93 on epoch=824
06/23/2022 16:54:40 - INFO - __main__ - Global step 1650 Train loss 2.92 ACC 0.0 on epoch=824
06/23/2022 16:54:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 2.78 on epoch=829
06/23/2022 16:54:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 2.89 on epoch=834
06/23/2022 16:54:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 2.82 on epoch=839
06/23/2022 16:54:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 2.90 on epoch=844
06/23/2022 16:54:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 2.72 on epoch=849
06/23/2022 16:54:51 - INFO - __main__ - Global step 1700 Train loss 2.82 ACC 0.0 on epoch=849
06/23/2022 16:54:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 2.78 on epoch=854
06/23/2022 16:54:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 2.74 on epoch=859
06/23/2022 16:54:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 2.71 on epoch=864
06/23/2022 16:54:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 2.71 on epoch=869
06/23/2022 16:54:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 2.69 on epoch=874
06/23/2022 16:55:03 - INFO - __main__ - Global step 1750 Train loss 2.73 ACC 0.0 on epoch=874
06/23/2022 16:55:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 2.71 on epoch=879
06/23/2022 16:55:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 2.66 on epoch=884
06/23/2022 16:55:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 2.61 on epoch=889
06/23/2022 16:55:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 2.56 on epoch=894
06/23/2022 16:55:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 2.54 on epoch=899
06/23/2022 16:55:11 - INFO - __main__ - Global step 1800 Train loss 2.62 ACC 0.0 on epoch=899
06/23/2022 16:55:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 2.45 on epoch=904
06/23/2022 16:55:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 2.52 on epoch=909
06/23/2022 16:55:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 2.43 on epoch=914
06/23/2022 16:55:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 2.38 on epoch=919
06/23/2022 16:55:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 2.41 on epoch=924
06/23/2022 16:55:20 - INFO - __main__ - Global step 1850 Train loss 2.44 ACC 0.0 on epoch=924
06/23/2022 16:55:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 2.33 on epoch=929
06/23/2022 16:55:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 2.32 on epoch=934
06/23/2022 16:55:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 2.28 on epoch=939
06/23/2022 16:55:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 2.32 on epoch=944
06/23/2022 16:55:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 2.27 on epoch=949
06/23/2022 16:55:30 - INFO - __main__ - Global step 1900 Train loss 2.30 ACC 0.0 on epoch=949
06/23/2022 16:55:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 2.25 on epoch=954
06/23/2022 16:55:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 2.16 on epoch=959
06/23/2022 16:55:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 2.17 on epoch=964
06/23/2022 16:55:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 2.17 on epoch=969
06/23/2022 16:55:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 2.25 on epoch=974
06/23/2022 16:55:40 - INFO - __main__ - Global step 1950 Train loss 2.20 ACC 0.0 on epoch=974
06/23/2022 16:55:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 2.18 on epoch=979
06/23/2022 16:55:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 2.11 on epoch=984
06/23/2022 16:55:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.93 on epoch=989
06/23/2022 16:55:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 2.06 on epoch=994
06/23/2022 16:55:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 2.03 on epoch=999
06/23/2022 16:55:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:55:48 - INFO - __main__ - Printing 3 examples
06/23/2022 16:55:48 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/23/2022 16:55:48 - INFO - __main__ - ['equivalent']
06/23/2022 16:55:48 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/23/2022 16:55:48 - INFO - __main__ - ['equivalent']
06/23/2022 16:55:48 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/23/2022 16:55:48 - INFO - __main__ - ['equivalent']
06/23/2022 16:55:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 16:55:48 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:55:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:55:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:55:48 - INFO - __main__ - Printing 3 examples
06/23/2022 16:55:48 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/23/2022 16:55:48 - INFO - __main__ - ['equivalent']
06/23/2022 16:55:48 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/23/2022 16:55:48 - INFO - __main__ - ['equivalent']
06/23/2022 16:55:48 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/23/2022 16:55:48 - INFO - __main__ - ['equivalent']
06/23/2022 16:55:48 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:55:48 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:55:48 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:55:49 - INFO - __main__ - Global step 2000 Train loss 2.06 ACC 0.09375 on epoch=999
06/23/2022 16:55:49 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=999, global_step=2000
06/23/2022 16:55:49 - INFO - __main__ - save last model!
06/23/2022 16:55:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 16:55:49 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 16:55:49 - INFO - __main__ - Printing 3 examples
06/23/2022 16:55:49 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 16:55:49 - INFO - __main__ - ['equivalent']
06/23/2022 16:55:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 16:55:49 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:55:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 16:55:49 - INFO - __main__ - ['not_equivalent']
06/23/2022 16:55:49 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:55:49 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:55:49 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 16:55:55 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:55:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:55:55 - INFO - __main__ - Starting training!
06/23/2022 16:56:16 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_21_0.2_8_predictions.txt
06/23/2022 16:56:16 - INFO - __main__ - ACC on test data: 0.1103
06/23/2022 16:56:16 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.2, bsz=8, dev_performance=0.09375, test_performance=0.11029411764705882
06/23/2022 16:56:16 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.5, bsz=8 ...
06/23/2022 16:56:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:56:17 - INFO - __main__ - Printing 3 examples
06/23/2022 16:56:17 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/23/2022 16:56:17 - INFO - __main__ - ['equivalent']
06/23/2022 16:56:17 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/23/2022 16:56:17 - INFO - __main__ - ['equivalent']
06/23/2022 16:56:17 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/23/2022 16:56:17 - INFO - __main__ - ['equivalent']
06/23/2022 16:56:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 16:56:17 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:56:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 16:56:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 16:56:17 - INFO - __main__ - Printing 3 examples
06/23/2022 16:56:17 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/23/2022 16:56:17 - INFO - __main__ - ['equivalent']
06/23/2022 16:56:17 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/23/2022 16:56:17 - INFO - __main__ - ['equivalent']
06/23/2022 16:56:17 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/23/2022 16:56:17 - INFO - __main__ - ['equivalent']
06/23/2022 16:56:17 - INFO - __main__ - Tokenizing Input ...
06/23/2022 16:56:17 - INFO - __main__ - Tokenizing Output ...
06/23/2022 16:56:17 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 16:56:23 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 16:56:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 16:56:23 - INFO - __main__ - Starting training!
06/23/2022 16:56:25 - INFO - __main__ - Step 10 Global step 10 Train loss 6.84 on epoch=4
06/23/2022 16:56:26 - INFO - __main__ - Step 20 Global step 20 Train loss 6.84 on epoch=9
06/23/2022 16:56:27 - INFO - __main__ - Step 30 Global step 30 Train loss 6.82 on epoch=14
06/23/2022 16:56:29 - INFO - __main__ - Step 40 Global step 40 Train loss 6.83 on epoch=19
06/23/2022 16:56:30 - INFO - __main__ - Step 50 Global step 50 Train loss 6.83 on epoch=24
06/23/2022 16:56:32 - INFO - __main__ - Global step 50 Train loss 6.83 ACC 0.0 on epoch=24
06/23/2022 16:56:32 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 16:56:34 - INFO - __main__ - Step 60 Global step 60 Train loss 6.75 on epoch=29
06/23/2022 16:56:35 - INFO - __main__ - Step 70 Global step 70 Train loss 6.79 on epoch=34
06/23/2022 16:56:36 - INFO - __main__ - Step 80 Global step 80 Train loss 6.72 on epoch=39
06/23/2022 16:56:38 - INFO - __main__ - Step 90 Global step 90 Train loss 6.73 on epoch=44
06/23/2022 16:56:39 - INFO - __main__ - Step 100 Global step 100 Train loss 6.58 on epoch=49
06/23/2022 16:56:49 - INFO - __main__ - Global step 100 Train loss 6.72 ACC 0.0 on epoch=49
06/23/2022 16:56:51 - INFO - __main__ - Step 110 Global step 110 Train loss 6.50 on epoch=54
06/23/2022 16:56:52 - INFO - __main__ - Step 120 Global step 120 Train loss 6.46 on epoch=59
06/23/2022 16:56:53 - INFO - __main__ - Step 130 Global step 130 Train loss 6.24 on epoch=64
06/23/2022 16:56:55 - INFO - __main__ - Step 140 Global step 140 Train loss 6.19 on epoch=69
06/23/2022 16:56:56 - INFO - __main__ - Step 150 Global step 150 Train loss 6.05 on epoch=74
06/23/2022 16:56:58 - INFO - __main__ - Global step 150 Train loss 6.29 ACC 0.0 on epoch=74
06/23/2022 16:57:00 - INFO - __main__ - Step 160 Global step 160 Train loss 6.13 on epoch=79
06/23/2022 16:57:01 - INFO - __main__ - Step 170 Global step 170 Train loss 6.00 on epoch=84
06/23/2022 16:57:02 - INFO - __main__ - Step 180 Global step 180 Train loss 5.83 on epoch=89
06/23/2022 16:57:04 - INFO - __main__ - Step 190 Global step 190 Train loss 5.75 on epoch=94
06/23/2022 16:57:05 - INFO - __main__ - Step 200 Global step 200 Train loss 5.60 on epoch=99
06/23/2022 16:57:11 - INFO - __main__ - Global step 200 Train loss 5.86 ACC 0.0 on epoch=99
06/23/2022 16:57:12 - INFO - __main__ - Step 210 Global step 210 Train loss 5.62 on epoch=104
06/23/2022 16:57:13 - INFO - __main__ - Step 220 Global step 220 Train loss 5.37 on epoch=109
06/23/2022 16:57:15 - INFO - __main__ - Step 230 Global step 230 Train loss 5.23 on epoch=114
06/23/2022 16:57:16 - INFO - __main__ - Step 240 Global step 240 Train loss 5.12 on epoch=119
06/23/2022 16:57:17 - INFO - __main__ - Step 250 Global step 250 Train loss 5.09 on epoch=124
06/23/2022 16:57:18 - INFO - __main__ - Global step 250 Train loss 5.29 ACC 0.0 on epoch=124
06/23/2022 16:57:20 - INFO - __main__ - Step 260 Global step 260 Train loss 4.89 on epoch=129
06/23/2022 16:57:21 - INFO - __main__ - Step 270 Global step 270 Train loss 4.76 on epoch=134
06/23/2022 16:57:22 - INFO - __main__ - Step 280 Global step 280 Train loss 4.73 on epoch=139
06/23/2022 16:57:23 - INFO - __main__ - Step 290 Global step 290 Train loss 4.48 on epoch=144
06/23/2022 16:57:25 - INFO - __main__ - Step 300 Global step 300 Train loss 4.32 on epoch=149
06/23/2022 16:57:26 - INFO - __main__ - Global step 300 Train loss 4.64 ACC 0.0 on epoch=149
06/23/2022 16:57:27 - INFO - __main__ - Step 310 Global step 310 Train loss 4.22 on epoch=154
06/23/2022 16:57:29 - INFO - __main__ - Step 320 Global step 320 Train loss 4.14 on epoch=159
06/23/2022 16:57:30 - INFO - __main__ - Step 330 Global step 330 Train loss 4.05 on epoch=164
06/23/2022 16:57:31 - INFO - __main__ - Step 340 Global step 340 Train loss 4.06 on epoch=169
06/23/2022 16:57:32 - INFO - __main__ - Step 350 Global step 350 Train loss 3.92 on epoch=174
06/23/2022 16:57:34 - INFO - __main__ - Global step 350 Train loss 4.08 ACC 0.0 on epoch=174
06/23/2022 16:57:36 - INFO - __main__ - Step 360 Global step 360 Train loss 3.86 on epoch=179
06/23/2022 16:57:37 - INFO - __main__ - Step 370 Global step 370 Train loss 3.59 on epoch=184
06/23/2022 16:57:38 - INFO - __main__ - Step 380 Global step 380 Train loss 3.56 on epoch=189
06/23/2022 16:57:40 - INFO - __main__ - Step 390 Global step 390 Train loss 3.63 on epoch=194
06/23/2022 16:57:41 - INFO - __main__ - Step 400 Global step 400 Train loss 3.39 on epoch=199
06/23/2022 16:57:43 - INFO - __main__ - Global step 400 Train loss 3.61 ACC 0.0 on epoch=199
06/23/2022 16:57:44 - INFO - __main__ - Step 410 Global step 410 Train loss 3.22 on epoch=204
06/23/2022 16:57:46 - INFO - __main__ - Step 420 Global step 420 Train loss 3.20 on epoch=209
06/23/2022 16:57:47 - INFO - __main__ - Step 430 Global step 430 Train loss 3.01 on epoch=214
06/23/2022 16:57:48 - INFO - __main__ - Step 440 Global step 440 Train loss 3.22 on epoch=219
06/23/2022 16:57:50 - INFO - __main__ - Step 450 Global step 450 Train loss 3.00 on epoch=224
06/23/2022 16:58:00 - INFO - __main__ - Global step 450 Train loss 3.13 ACC 0.0 on epoch=224
06/23/2022 16:58:01 - INFO - __main__ - Step 460 Global step 460 Train loss 2.93 on epoch=229
06/23/2022 16:58:03 - INFO - __main__ - Step 470 Global step 470 Train loss 2.76 on epoch=234
06/23/2022 16:58:04 - INFO - __main__ - Step 480 Global step 480 Train loss 2.81 on epoch=239
06/23/2022 16:58:05 - INFO - __main__ - Step 490 Global step 490 Train loss 2.59 on epoch=244
06/23/2022 16:58:07 - INFO - __main__ - Step 500 Global step 500 Train loss 2.54 on epoch=249
06/23/2022 16:58:17 - INFO - __main__ - Global step 500 Train loss 2.73 ACC 0.0 on epoch=249
06/23/2022 16:58:18 - INFO - __main__ - Step 510 Global step 510 Train loss 2.41 on epoch=254
06/23/2022 16:58:19 - INFO - __main__ - Step 520 Global step 520 Train loss 2.52 on epoch=259
06/23/2022 16:58:21 - INFO - __main__ - Step 530 Global step 530 Train loss 2.39 on epoch=264
06/23/2022 16:58:22 - INFO - __main__ - Step 540 Global step 540 Train loss 2.44 on epoch=269
06/23/2022 16:58:23 - INFO - __main__ - Step 550 Global step 550 Train loss 2.39 on epoch=274
06/23/2022 16:58:27 - INFO - __main__ - Global step 550 Train loss 2.43 ACC 0.0 on epoch=274
06/23/2022 16:58:28 - INFO - __main__ - Step 560 Global step 560 Train loss 2.16 on epoch=279
06/23/2022 16:58:29 - INFO - __main__ - Step 570 Global step 570 Train loss 2.18 on epoch=284
06/23/2022 16:58:31 - INFO - __main__ - Step 580 Global step 580 Train loss 2.02 on epoch=289
06/23/2022 16:58:32 - INFO - __main__ - Step 590 Global step 590 Train loss 1.90 on epoch=294
06/23/2022 16:58:33 - INFO - __main__ - Step 600 Global step 600 Train loss 1.87 on epoch=299
06/23/2022 16:58:35 - INFO - __main__ - Global step 600 Train loss 2.02 ACC 0.4375 on epoch=299
06/23/2022 16:58:35 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.4375 on epoch=299, global_step=600
06/23/2022 16:58:36 - INFO - __main__ - Step 610 Global step 610 Train loss 1.73 on epoch=304
06/23/2022 16:58:37 - INFO - __main__ - Step 620 Global step 620 Train loss 1.89 on epoch=309
06/23/2022 16:58:39 - INFO - __main__ - Step 630 Global step 630 Train loss 1.68 on epoch=314
06/23/2022 16:58:40 - INFO - __main__ - Step 640 Global step 640 Train loss 1.71 on epoch=319
06/23/2022 16:58:42 - INFO - __main__ - Step 650 Global step 650 Train loss 1.63 on epoch=324
06/23/2022 16:58:43 - INFO - __main__ - Global step 650 Train loss 1.73 ACC 0.5 on epoch=324
06/23/2022 16:58:43 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=324, global_step=650
06/23/2022 16:58:44 - INFO - __main__ - Step 660 Global step 660 Train loss 1.64 on epoch=329
06/23/2022 16:58:46 - INFO - __main__ - Step 670 Global step 670 Train loss 1.51 on epoch=334
06/23/2022 16:58:47 - INFO - __main__ - Step 680 Global step 680 Train loss 1.45 on epoch=339
06/23/2022 16:58:48 - INFO - __main__ - Step 690 Global step 690 Train loss 1.28 on epoch=344
06/23/2022 16:58:50 - INFO - __main__ - Step 700 Global step 700 Train loss 1.27 on epoch=349
06/23/2022 16:58:52 - INFO - __main__ - Global step 700 Train loss 1.43 ACC 0.46875 on epoch=349
06/23/2022 16:58:53 - INFO - __main__ - Step 710 Global step 710 Train loss 1.18 on epoch=354
06/23/2022 16:58:55 - INFO - __main__ - Step 720 Global step 720 Train loss 1.11 on epoch=359
06/23/2022 16:58:56 - INFO - __main__ - Step 730 Global step 730 Train loss 1.23 on epoch=364
06/23/2022 16:58:58 - INFO - __main__ - Step 740 Global step 740 Train loss 1.18 on epoch=369
06/23/2022 16:58:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.99 on epoch=374
06/23/2022 16:59:03 - INFO - __main__ - Global step 750 Train loss 1.14 ACC 0.46875 on epoch=374
06/23/2022 16:59:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.89 on epoch=379
06/23/2022 16:59:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.80 on epoch=384
06/23/2022 16:59:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=389
06/23/2022 16:59:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.80 on epoch=394
06/23/2022 16:59:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.78 on epoch=399
06/23/2022 16:59:12 - INFO - __main__ - Global step 800 Train loss 0.82 ACC 0.53125 on epoch=399
06/23/2022 16:59:12 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=399, global_step=800
06/23/2022 16:59:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.72 on epoch=404
06/23/2022 16:59:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.66 on epoch=409
06/23/2022 16:59:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.65 on epoch=414
06/23/2022 16:59:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.64 on epoch=419
06/23/2022 16:59:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=424
06/23/2022 16:59:19 - INFO - __main__ - Global step 850 Train loss 0.65 ACC 0.53125 on epoch=424
06/23/2022 16:59:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.54 on epoch=429
06/23/2022 16:59:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.67 on epoch=434
06/23/2022 16:59:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.68 on epoch=439
06/23/2022 16:59:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.54 on epoch=444
06/23/2022 16:59:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.64 on epoch=449
06/23/2022 16:59:27 - INFO - __main__ - Global step 900 Train loss 0.61 ACC 0.40625 on epoch=449
06/23/2022 16:59:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.55 on epoch=454
06/23/2022 16:59:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.59 on epoch=459
06/23/2022 16:59:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.53 on epoch=464
06/23/2022 16:59:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.59 on epoch=469
06/23/2022 16:59:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.51 on epoch=474
06/23/2022 16:59:35 - INFO - __main__ - Global step 950 Train loss 0.55 ACC 0.5 on epoch=474
06/23/2022 16:59:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.56 on epoch=479
06/23/2022 16:59:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=484
06/23/2022 16:59:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=489
06/23/2022 16:59:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=494
06/23/2022 16:59:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=499
06/23/2022 16:59:42 - INFO - __main__ - Global step 1000 Train loss 0.46 ACC 0.5 on epoch=499
06/23/2022 16:59:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=504
06/23/2022 16:59:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=509
06/23/2022 16:59:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.51 on epoch=514
06/23/2022 16:59:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.52 on epoch=519
06/23/2022 16:59:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=524
06/23/2022 16:59:50 - INFO - __main__ - Global step 1050 Train loss 0.48 ACC 0.59375 on epoch=524
06/23/2022 16:59:50 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=524, global_step=1050
06/23/2022 16:59:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=529
06/23/2022 16:59:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=534
06/23/2022 16:59:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.50 on epoch=539
06/23/2022 16:59:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
06/23/2022 16:59:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=549
06/23/2022 16:59:59 - INFO - __main__ - Global step 1100 Train loss 0.43 ACC 0.53125 on epoch=549
06/23/2022 17:00:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=554
06/23/2022 17:00:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=559
06/23/2022 17:00:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=564
06/23/2022 17:00:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=569
06/23/2022 17:00:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.49 on epoch=574
06/23/2022 17:00:06 - INFO - __main__ - Global step 1150 Train loss 0.45 ACC 0.5 on epoch=574
06/23/2022 17:00:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=579
06/23/2022 17:00:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=584
06/23/2022 17:00:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=589
06/23/2022 17:00:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=594
06/23/2022 17:00:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=599
06/23/2022 17:00:14 - INFO - __main__ - Global step 1200 Train loss 0.39 ACC 0.46875 on epoch=599
06/23/2022 17:00:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=604
06/23/2022 17:00:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=609
06/23/2022 17:00:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=614
06/23/2022 17:00:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=619
06/23/2022 17:00:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=624
06/23/2022 17:00:22 - INFO - __main__ - Global step 1250 Train loss 0.39 ACC 0.5 on epoch=624
06/23/2022 17:00:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=629
06/23/2022 17:00:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=634
06/23/2022 17:00:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=639
06/23/2022 17:00:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=644
06/23/2022 17:00:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=649
06/23/2022 17:00:30 - INFO - __main__ - Global step 1300 Train loss 0.39 ACC 0.5 on epoch=649
06/23/2022 17:00:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=654
06/23/2022 17:00:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=659
06/23/2022 17:00:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=664
06/23/2022 17:00:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=669
06/23/2022 17:00:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=674
06/23/2022 17:00:37 - INFO - __main__ - Global step 1350 Train loss 0.39 ACC 0.625 on epoch=674
06/23/2022 17:00:37 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=674, global_step=1350
06/23/2022 17:00:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=679
06/23/2022 17:00:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=684
06/23/2022 17:00:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=689
06/23/2022 17:00:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=694
06/23/2022 17:00:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=699
06/23/2022 17:00:45 - INFO - __main__ - Global step 1400 Train loss 0.37 ACC 0.65625 on epoch=699
06/23/2022 17:00:45 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=699, global_step=1400
06/23/2022 17:00:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=704
06/23/2022 17:00:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=709
06/23/2022 17:00:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=714
06/23/2022 17:00:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
06/23/2022 17:00:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=724
06/23/2022 17:00:52 - INFO - __main__ - Global step 1450 Train loss 0.36 ACC 0.5 on epoch=724
06/23/2022 17:00:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=729
06/23/2022 17:00:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=734
06/23/2022 17:00:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=739
06/23/2022 17:00:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=744
06/23/2022 17:00:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.37 on epoch=749
06/23/2022 17:01:00 - INFO - __main__ - Global step 1500 Train loss 0.35 ACC 0.46875 on epoch=749
06/23/2022 17:01:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=754
06/23/2022 17:01:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.35 on epoch=759
06/23/2022 17:01:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.34 on epoch=764
06/23/2022 17:01:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=769
06/23/2022 17:01:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=774
06/23/2022 17:01:08 - INFO - __main__ - Global step 1550 Train loss 0.35 ACC 0.5625 on epoch=774
06/23/2022 17:01:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=779
06/23/2022 17:01:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=784
06/23/2022 17:01:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=789
06/23/2022 17:01:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=794
06/23/2022 17:01:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=799
06/23/2022 17:01:15 - INFO - __main__ - Global step 1600 Train loss 0.35 ACC 0.5 on epoch=799
06/23/2022 17:01:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=804
06/23/2022 17:01:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=809
06/23/2022 17:01:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=814
06/23/2022 17:01:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=819
06/23/2022 17:01:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=824
06/23/2022 17:01:23 - INFO - __main__ - Global step 1650 Train loss 0.37 ACC 0.5 on epoch=824
06/23/2022 17:01:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=829
06/23/2022 17:01:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=834
06/23/2022 17:01:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.34 on epoch=839
06/23/2022 17:01:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=844
06/23/2022 17:01:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.32 on epoch=849
06/23/2022 17:01:31 - INFO - __main__ - Global step 1700 Train loss 0.34 ACC 0.5 on epoch=849
06/23/2022 17:01:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=854
06/23/2022 17:01:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=859
06/23/2022 17:01:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.32 on epoch=864
06/23/2022 17:01:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=869
06/23/2022 17:01:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.40 on epoch=874
06/23/2022 17:01:38 - INFO - __main__ - Global step 1750 Train loss 0.35 ACC 0.5 on epoch=874
06/23/2022 17:01:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=879
06/23/2022 17:01:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=884
06/23/2022 17:01:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=889
06/23/2022 17:01:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.35 on epoch=894
06/23/2022 17:01:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=899
06/23/2022 17:01:46 - INFO - __main__ - Global step 1800 Train loss 0.35 ACC 0.5 on epoch=899
06/23/2022 17:01:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=904
06/23/2022 17:01:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=909
06/23/2022 17:01:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=914
06/23/2022 17:01:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.33 on epoch=919
06/23/2022 17:01:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.32 on epoch=924
06/23/2022 17:01:54 - INFO - __main__ - Global step 1850 Train loss 0.33 ACC 0.53125 on epoch=924
06/23/2022 17:01:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=929
06/23/2022 17:01:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=934
06/23/2022 17:01:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=939
06/23/2022 17:01:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=944
06/23/2022 17:02:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.30 on epoch=949
06/23/2022 17:02:01 - INFO - __main__ - Global step 1900 Train loss 0.33 ACC 0.46875 on epoch=949
06/23/2022 17:02:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.36 on epoch=954
06/23/2022 17:02:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
06/23/2022 17:02:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=964
06/23/2022 17:02:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=969
06/23/2022 17:02:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=974
06/23/2022 17:02:09 - INFO - __main__ - Global step 1950 Train loss 0.33 ACC 0.5 on epoch=974
06/23/2022 17:02:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.31 on epoch=979
06/23/2022 17:02:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=984
06/23/2022 17:02:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
06/23/2022 17:02:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.29 on epoch=994
06/23/2022 17:02:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
06/23/2022 17:02:17 - INFO - __main__ - Global step 2000 Train loss 0.32 ACC 0.5 on epoch=999
06/23/2022 17:02:17 - INFO - __main__ - save last model!
06/23/2022 17:02:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 17:02:17 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 17:02:17 - INFO - __main__ - Printing 3 examples
06/23/2022 17:02:17 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 17:02:17 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:17 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 17:02:17 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:02:17 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 17:02:17 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:02:17 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:02:17 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:02:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:02:17 - INFO - __main__ - Printing 3 examples
06/23/2022 17:02:17 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/23/2022 17:02:17 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:17 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/23/2022 17:02:17 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:17 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/23/2022 17:02:17 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 17:02:17 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:02:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:02:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:02:17 - INFO - __main__ - Printing 3 examples
06/23/2022 17:02:17 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/23/2022 17:02:17 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:17 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/23/2022 17:02:17 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:17 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/23/2022 17:02:17 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:17 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:02:17 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:02:17 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:02:18 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 17:02:23 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:02:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:02:23 - INFO - __main__ - Starting training!
06/23/2022 17:02:26 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_42_0.5_8_predictions.txt
06/23/2022 17:02:26 - INFO - __main__ - ACC on test data: 0.3260
06/23/2022 17:02:26 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.5, bsz=8, dev_performance=0.65625, test_performance=0.32598039215686275
06/23/2022 17:02:26 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.4, bsz=8 ...
06/23/2022 17:02:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:02:27 - INFO - __main__ - Printing 3 examples
06/23/2022 17:02:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/23/2022 17:02:27 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/23/2022 17:02:27 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:27 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/23/2022 17:02:27 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 17:02:27 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:02:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:02:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:02:27 - INFO - __main__ - Printing 3 examples
06/23/2022 17:02:27 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/23/2022 17:02:27 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:27 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/23/2022 17:02:27 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:27 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/23/2022 17:02:27 - INFO - __main__ - ['equivalent']
06/23/2022 17:02:27 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:02:27 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:02:27 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:02:34 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:02:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:02:34 - INFO - __main__ - Starting training!
06/23/2022 17:02:36 - INFO - __main__ - Step 10 Global step 10 Train loss 6.93 on epoch=4
06/23/2022 17:02:37 - INFO - __main__ - Step 20 Global step 20 Train loss 6.88 on epoch=9
06/23/2022 17:02:38 - INFO - __main__ - Step 30 Global step 30 Train loss 6.83 on epoch=14
06/23/2022 17:02:40 - INFO - __main__ - Step 40 Global step 40 Train loss 6.90 on epoch=19
06/23/2022 17:02:41 - INFO - __main__ - Step 50 Global step 50 Train loss 6.78 on epoch=24
06/23/2022 17:02:42 - INFO - __main__ - Global step 50 Train loss 6.86 ACC 0.0 on epoch=24
06/23/2022 17:02:42 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 17:02:43 - INFO - __main__ - Step 60 Global step 60 Train loss 6.76 on epoch=29
06/23/2022 17:02:45 - INFO - __main__ - Step 70 Global step 70 Train loss 6.72 on epoch=34
06/23/2022 17:02:46 - INFO - __main__ - Step 80 Global step 80 Train loss 6.79 on epoch=39
06/23/2022 17:02:47 - INFO - __main__ - Step 90 Global step 90 Train loss 6.73 on epoch=44
06/23/2022 17:02:49 - INFO - __main__ - Step 100 Global step 100 Train loss 6.73 on epoch=49
06/23/2022 17:02:51 - INFO - __main__ - Global step 100 Train loss 6.75 ACC 0.0 on epoch=49
06/23/2022 17:02:52 - INFO - __main__ - Step 110 Global step 110 Train loss 6.67 on epoch=54
06/23/2022 17:02:53 - INFO - __main__ - Step 120 Global step 120 Train loss 6.64 on epoch=59
06/23/2022 17:02:55 - INFO - __main__ - Step 130 Global step 130 Train loss 6.56 on epoch=64
06/23/2022 17:02:56 - INFO - __main__ - Step 140 Global step 140 Train loss 6.51 on epoch=69
06/23/2022 17:02:58 - INFO - __main__ - Step 150 Global step 150 Train loss 6.41 on epoch=74
06/23/2022 17:03:00 - INFO - __main__ - Global step 150 Train loss 6.56 ACC 0.0 on epoch=74
06/23/2022 17:03:01 - INFO - __main__ - Step 160 Global step 160 Train loss 6.33 on epoch=79
06/23/2022 17:03:02 - INFO - __main__ - Step 170 Global step 170 Train loss 6.25 on epoch=84
06/23/2022 17:03:04 - INFO - __main__ - Step 180 Global step 180 Train loss 6.27 on epoch=89
06/23/2022 17:03:05 - INFO - __main__ - Step 190 Global step 190 Train loss 6.20 on epoch=94
06/23/2022 17:03:06 - INFO - __main__ - Step 200 Global step 200 Train loss 6.18 on epoch=99
06/23/2022 17:03:11 - INFO - __main__ - Global step 200 Train loss 6.25 ACC 0.0 on epoch=99
06/23/2022 17:03:13 - INFO - __main__ - Step 210 Global step 210 Train loss 6.09 on epoch=104
06/23/2022 17:03:14 - INFO - __main__ - Step 220 Global step 220 Train loss 6.10 on epoch=109
06/23/2022 17:03:15 - INFO - __main__ - Step 230 Global step 230 Train loss 6.01 on epoch=114
06/23/2022 17:03:17 - INFO - __main__ - Step 240 Global step 240 Train loss 5.96 on epoch=119
06/23/2022 17:03:18 - INFO - __main__ - Step 250 Global step 250 Train loss 5.97 on epoch=124
06/23/2022 17:03:29 - INFO - __main__ - Global step 250 Train loss 6.03 ACC 0.0 on epoch=124
06/23/2022 17:03:31 - INFO - __main__ - Step 260 Global step 260 Train loss 5.91 on epoch=129
06/23/2022 17:03:32 - INFO - __main__ - Step 270 Global step 270 Train loss 5.91 on epoch=134
06/23/2022 17:03:34 - INFO - __main__ - Step 280 Global step 280 Train loss 5.84 on epoch=139
06/23/2022 17:03:35 - INFO - __main__ - Step 290 Global step 290 Train loss 5.82 on epoch=144
06/23/2022 17:03:36 - INFO - __main__ - Step 300 Global step 300 Train loss 5.74 on epoch=149
06/23/2022 17:03:48 - INFO - __main__ - Global step 300 Train loss 5.84 ACC 0.0 on epoch=149
06/23/2022 17:03:50 - INFO - __main__ - Step 310 Global step 310 Train loss 5.80 on epoch=154
06/23/2022 17:03:51 - INFO - __main__ - Step 320 Global step 320 Train loss 5.71 on epoch=159
06/23/2022 17:03:52 - INFO - __main__ - Step 330 Global step 330 Train loss 5.62 on epoch=164
06/23/2022 17:03:54 - INFO - __main__ - Step 340 Global step 340 Train loss 5.64 on epoch=169
06/23/2022 17:03:55 - INFO - __main__ - Step 350 Global step 350 Train loss 5.47 on epoch=174
06/23/2022 17:04:03 - INFO - __main__ - Global step 350 Train loss 5.65 ACC 0.0 on epoch=174
06/23/2022 17:04:04 - INFO - __main__ - Step 360 Global step 360 Train loss 5.54 on epoch=179
06/23/2022 17:04:06 - INFO - __main__ - Step 370 Global step 370 Train loss 5.40 on epoch=184
06/23/2022 17:04:07 - INFO - __main__ - Step 380 Global step 380 Train loss 5.31 on epoch=189
06/23/2022 17:04:08 - INFO - __main__ - Step 390 Global step 390 Train loss 5.40 on epoch=194
06/23/2022 17:04:10 - INFO - __main__ - Step 400 Global step 400 Train loss 5.46 on epoch=199
06/23/2022 17:04:22 - INFO - __main__ - Global step 400 Train loss 5.42 ACC 0.0 on epoch=199
06/23/2022 17:04:23 - INFO - __main__ - Step 410 Global step 410 Train loss 5.63 on epoch=204
06/23/2022 17:04:25 - INFO - __main__ - Step 420 Global step 420 Train loss 5.58 on epoch=209
06/23/2022 17:04:26 - INFO - __main__ - Step 430 Global step 430 Train loss 5.43 on epoch=214
06/23/2022 17:04:28 - INFO - __main__ - Step 440 Global step 440 Train loss 5.38 on epoch=219
06/23/2022 17:04:29 - INFO - __main__ - Step 450 Global step 450 Train loss 5.27 on epoch=224
06/23/2022 17:04:33 - INFO - __main__ - Global step 450 Train loss 5.46 ACC 0.0 on epoch=224
06/23/2022 17:04:34 - INFO - __main__ - Step 460 Global step 460 Train loss 5.16 on epoch=229
06/23/2022 17:04:36 - INFO - __main__ - Step 470 Global step 470 Train loss 5.02 on epoch=234
06/23/2022 17:04:37 - INFO - __main__ - Step 480 Global step 480 Train loss 4.95 on epoch=239
06/23/2022 17:04:38 - INFO - __main__ - Step 490 Global step 490 Train loss 4.95 on epoch=244
06/23/2022 17:04:40 - INFO - __main__ - Step 500 Global step 500 Train loss 4.84 on epoch=249
06/23/2022 17:04:42 - INFO - __main__ - Global step 500 Train loss 4.98 ACC 0.0 on epoch=249
06/23/2022 17:04:43 - INFO - __main__ - Step 510 Global step 510 Train loss 4.78 on epoch=254
06/23/2022 17:04:44 - INFO - __main__ - Step 520 Global step 520 Train loss 4.70 on epoch=259
06/23/2022 17:04:46 - INFO - __main__ - Step 530 Global step 530 Train loss 4.61 on epoch=264
06/23/2022 17:04:47 - INFO - __main__ - Step 540 Global step 540 Train loss 4.46 on epoch=269
06/23/2022 17:04:49 - INFO - __main__ - Step 550 Global step 550 Train loss 4.51 on epoch=274
06/23/2022 17:04:51 - INFO - __main__ - Global step 550 Train loss 4.61 ACC 0.0 on epoch=274
06/23/2022 17:04:52 - INFO - __main__ - Step 560 Global step 560 Train loss 4.59 on epoch=279
06/23/2022 17:04:54 - INFO - __main__ - Step 570 Global step 570 Train loss 4.45 on epoch=284
06/23/2022 17:04:55 - INFO - __main__ - Step 580 Global step 580 Train loss 4.46 on epoch=289
06/23/2022 17:04:56 - INFO - __main__ - Step 590 Global step 590 Train loss 4.40 on epoch=294
06/23/2022 17:04:58 - INFO - __main__ - Step 600 Global step 600 Train loss 4.40 on epoch=299
06/23/2022 17:05:00 - INFO - __main__ - Global step 600 Train loss 4.46 ACC 0.0 on epoch=299
06/23/2022 17:05:01 - INFO - __main__ - Step 610 Global step 610 Train loss 4.39 on epoch=304
06/23/2022 17:05:03 - INFO - __main__ - Step 620 Global step 620 Train loss 4.28 on epoch=309
06/23/2022 17:05:04 - INFO - __main__ - Step 630 Global step 630 Train loss 4.36 on epoch=314
06/23/2022 17:05:06 - INFO - __main__ - Step 640 Global step 640 Train loss 4.15 on epoch=319
06/23/2022 17:05:07 - INFO - __main__ - Step 650 Global step 650 Train loss 4.16 on epoch=324
06/23/2022 17:05:09 - INFO - __main__ - Global step 650 Train loss 4.27 ACC 0.0 on epoch=324
06/23/2022 17:05:10 - INFO - __main__ - Step 660 Global step 660 Train loss 4.36 on epoch=329
06/23/2022 17:05:12 - INFO - __main__ - Step 670 Global step 670 Train loss 3.94 on epoch=334
06/23/2022 17:05:13 - INFO - __main__ - Step 680 Global step 680 Train loss 3.99 on epoch=339
06/23/2022 17:05:14 - INFO - __main__ - Step 690 Global step 690 Train loss 4.06 on epoch=344
06/23/2022 17:05:16 - INFO - __main__ - Step 700 Global step 700 Train loss 4.00 on epoch=349
06/23/2022 17:05:19 - INFO - __main__ - Global step 700 Train loss 4.07 ACC 0.0 on epoch=349
06/23/2022 17:05:20 - INFO - __main__ - Step 710 Global step 710 Train loss 4.16 on epoch=354
06/23/2022 17:05:22 - INFO - __main__ - Step 720 Global step 720 Train loss 3.88 on epoch=359
06/23/2022 17:05:23 - INFO - __main__ - Step 730 Global step 730 Train loss 3.84 on epoch=364
06/23/2022 17:05:24 - INFO - __main__ - Step 740 Global step 740 Train loss 3.81 on epoch=369
06/23/2022 17:05:26 - INFO - __main__ - Step 750 Global step 750 Train loss 3.71 on epoch=374
06/23/2022 17:05:28 - INFO - __main__ - Global step 750 Train loss 3.88 ACC 0.0 on epoch=374
06/23/2022 17:05:29 - INFO - __main__ - Step 760 Global step 760 Train loss 3.68 on epoch=379
06/23/2022 17:05:31 - INFO - __main__ - Step 770 Global step 770 Train loss 3.58 on epoch=384
06/23/2022 17:05:32 - INFO - __main__ - Step 780 Global step 780 Train loss 3.63 on epoch=389
06/23/2022 17:05:34 - INFO - __main__ - Step 790 Global step 790 Train loss 3.49 on epoch=394
06/23/2022 17:05:35 - INFO - __main__ - Step 800 Global step 800 Train loss 3.40 on epoch=399
06/23/2022 17:05:38 - INFO - __main__ - Global step 800 Train loss 3.55 ACC 0.0 on epoch=399
06/23/2022 17:05:39 - INFO - __main__ - Step 810 Global step 810 Train loss 3.37 on epoch=404
06/23/2022 17:05:40 - INFO - __main__ - Step 820 Global step 820 Train loss 3.36 on epoch=409
06/23/2022 17:05:42 - INFO - __main__ - Step 830 Global step 830 Train loss 3.28 on epoch=414
06/23/2022 17:05:43 - INFO - __main__ - Step 840 Global step 840 Train loss 3.21 on epoch=419
06/23/2022 17:05:45 - INFO - __main__ - Step 850 Global step 850 Train loss 3.14 on epoch=424
06/23/2022 17:05:47 - INFO - __main__ - Global step 850 Train loss 3.27 ACC 0.0 on epoch=424
06/23/2022 17:05:48 - INFO - __main__ - Step 860 Global step 860 Train loss 3.04 on epoch=429
06/23/2022 17:05:50 - INFO - __main__ - Step 870 Global step 870 Train loss 3.14 on epoch=434
06/23/2022 17:05:51 - INFO - __main__ - Step 880 Global step 880 Train loss 2.93 on epoch=439
06/23/2022 17:05:53 - INFO - __main__ - Step 890 Global step 890 Train loss 2.96 on epoch=444
06/23/2022 17:05:54 - INFO - __main__ - Step 900 Global step 900 Train loss 2.89 on epoch=449
06/23/2022 17:05:57 - INFO - __main__ - Global step 900 Train loss 2.99 ACC 0.0 on epoch=449
06/23/2022 17:05:58 - INFO - __main__ - Step 910 Global step 910 Train loss 2.87 on epoch=454
06/23/2022 17:05:59 - INFO - __main__ - Step 920 Global step 920 Train loss 2.83 on epoch=459
06/23/2022 17:06:01 - INFO - __main__ - Step 930 Global step 930 Train loss 2.83 on epoch=464
06/23/2022 17:06:02 - INFO - __main__ - Step 940 Global step 940 Train loss 2.78 on epoch=469
06/23/2022 17:06:04 - INFO - __main__ - Step 950 Global step 950 Train loss 2.61 on epoch=474
06/23/2022 17:06:16 - INFO - __main__ - Global step 950 Train loss 2.78 ACC 0.0 on epoch=474
06/23/2022 17:06:18 - INFO - __main__ - Step 960 Global step 960 Train loss 2.59 on epoch=479
06/23/2022 17:06:19 - INFO - __main__ - Step 970 Global step 970 Train loss 2.66 on epoch=484
06/23/2022 17:06:20 - INFO - __main__ - Step 980 Global step 980 Train loss 2.61 on epoch=489
06/23/2022 17:06:22 - INFO - __main__ - Step 990 Global step 990 Train loss 2.61 on epoch=494
06/23/2022 17:06:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.55 on epoch=499
06/23/2022 17:06:36 - INFO - __main__ - Global step 1000 Train loss 2.60 ACC 0.0 on epoch=499
06/23/2022 17:06:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.47 on epoch=504
06/23/2022 17:06:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.58 on epoch=509
06/23/2022 17:06:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.53 on epoch=514
06/23/2022 17:06:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.37 on epoch=519
06/23/2022 17:06:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.29 on epoch=524
06/23/2022 17:06:55 - INFO - __main__ - Global step 1050 Train loss 2.45 ACC 0.0625 on epoch=524
06/23/2022 17:06:55 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=524, global_step=1050
06/23/2022 17:06:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.22 on epoch=529
06/23/2022 17:06:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.35 on epoch=534
06/23/2022 17:07:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.37 on epoch=539
06/23/2022 17:07:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.34 on epoch=544
06/23/2022 17:07:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.23 on epoch=549
06/23/2022 17:07:09 - INFO - __main__ - Global step 1100 Train loss 2.30 ACC 0.28125 on epoch=549
06/23/2022 17:07:09 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.28125 on epoch=549, global_step=1100
06/23/2022 17:07:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.16 on epoch=554
06/23/2022 17:07:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 2.16 on epoch=559
06/23/2022 17:07:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.18 on epoch=564
06/23/2022 17:07:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 2.10 on epoch=569
06/23/2022 17:07:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.98 on epoch=574
06/23/2022 17:07:24 - INFO - __main__ - Global step 1150 Train loss 2.12 ACC 0.3125 on epoch=574
06/23/2022 17:07:24 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.3125 on epoch=574, global_step=1150
06/23/2022 17:07:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.98 on epoch=579
06/23/2022 17:07:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.90 on epoch=584
06/23/2022 17:07:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.89 on epoch=589
06/23/2022 17:07:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.90 on epoch=594
06/23/2022 17:07:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.84 on epoch=599
06/23/2022 17:07:37 - INFO - __main__ - Global step 1200 Train loss 1.90 ACC 0.4375 on epoch=599
06/23/2022 17:07:37 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.4375 on epoch=599, global_step=1200
06/23/2022 17:07:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.86 on epoch=604
06/23/2022 17:07:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.77 on epoch=609
06/23/2022 17:07:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.80 on epoch=614
06/23/2022 17:07:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.70 on epoch=619
06/23/2022 17:07:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.73 on epoch=624
06/23/2022 17:07:56 - INFO - __main__ - Global step 1250 Train loss 1.77 ACC 0.46875 on epoch=624
06/23/2022 17:07:56 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.46875 on epoch=624, global_step=1250
06/23/2022 17:07:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.67 on epoch=629
06/23/2022 17:07:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.62 on epoch=634
06/23/2022 17:08:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.62 on epoch=639
06/23/2022 17:08:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.60 on epoch=644
06/23/2022 17:08:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.58 on epoch=649
06/23/2022 17:08:04 - INFO - __main__ - Global step 1300 Train loss 1.62 ACC 0.5 on epoch=649
06/23/2022 17:08:04 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=649, global_step=1300
06/23/2022 17:08:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.47 on epoch=654
06/23/2022 17:08:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.50 on epoch=659
06/23/2022 17:08:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.53 on epoch=664
06/23/2022 17:08:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.52 on epoch=669
06/23/2022 17:08:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.45 on epoch=674
06/23/2022 17:08:11 - INFO - __main__ - Global step 1350 Train loss 1.50 ACC 0.5 on epoch=674
06/23/2022 17:08:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.40 on epoch=679
06/23/2022 17:08:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.46 on epoch=684
06/23/2022 17:08:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.33 on epoch=689
06/23/2022 17:08:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.28 on epoch=694
06/23/2022 17:08:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.35 on epoch=699
06/23/2022 17:08:18 - INFO - __main__ - Global step 1400 Train loss 1.37 ACC 0.5 on epoch=699
06/23/2022 17:08:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.32 on epoch=704
06/23/2022 17:08:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.33 on epoch=709
06/23/2022 17:08:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.30 on epoch=714
06/23/2022 17:08:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.30 on epoch=719
06/23/2022 17:08:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.23 on epoch=724
06/23/2022 17:08:26 - INFO - __main__ - Global step 1450 Train loss 1.30 ACC 0.5 on epoch=724
06/23/2022 17:08:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.23 on epoch=729
06/23/2022 17:08:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.27 on epoch=734
06/23/2022 17:08:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.15 on epoch=739
06/23/2022 17:08:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.19 on epoch=744
06/23/2022 17:08:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.08 on epoch=749
06/23/2022 17:08:33 - INFO - __main__ - Global step 1500 Train loss 1.19 ACC 0.5 on epoch=749
06/23/2022 17:08:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.15 on epoch=754
06/23/2022 17:08:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.14 on epoch=759
06/23/2022 17:08:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.14 on epoch=764
06/23/2022 17:08:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.18 on epoch=769
06/23/2022 17:08:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.09 on epoch=774
06/23/2022 17:08:41 - INFO - __main__ - Global step 1550 Train loss 1.14 ACC 0.5 on epoch=774
06/23/2022 17:08:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.04 on epoch=779
06/23/2022 17:08:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.99 on epoch=784
06/23/2022 17:08:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.12 on epoch=789
06/23/2022 17:08:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.08 on epoch=794
06/23/2022 17:08:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.00 on epoch=799
06/23/2022 17:08:49 - INFO - __main__ - Global step 1600 Train loss 1.05 ACC 0.5 on epoch=799
06/23/2022 17:08:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.94 on epoch=804
06/23/2022 17:08:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.87 on epoch=809
06/23/2022 17:08:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.01 on epoch=814
06/23/2022 17:08:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.97 on epoch=819
06/23/2022 17:08:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.96 on epoch=824
06/23/2022 17:08:56 - INFO - __main__ - Global step 1650 Train loss 0.95 ACC 0.5 on epoch=824
06/23/2022 17:08:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.86 on epoch=829
06/23/2022 17:08:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.94 on epoch=834
06/23/2022 17:09:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.90 on epoch=839
06/23/2022 17:09:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.85 on epoch=844
06/23/2022 17:09:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.88 on epoch=849
06/23/2022 17:09:05 - INFO - __main__ - Global step 1700 Train loss 0.89 ACC 0.5 on epoch=849
06/23/2022 17:09:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.82 on epoch=854
06/23/2022 17:09:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.83 on epoch=859
06/23/2022 17:09:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.83 on epoch=864
06/23/2022 17:09:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.84 on epoch=869
06/23/2022 17:09:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.83 on epoch=874
06/23/2022 17:09:14 - INFO - __main__ - Global step 1750 Train loss 0.83 ACC 0.5 on epoch=874
06/23/2022 17:09:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.83 on epoch=879
06/23/2022 17:09:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.79 on epoch=884
06/23/2022 17:09:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.82 on epoch=889
06/23/2022 17:09:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.82 on epoch=894
06/23/2022 17:09:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.75 on epoch=899
06/23/2022 17:09:26 - INFO - __main__ - Global step 1800 Train loss 0.80 ACC 0.59375 on epoch=899
06/23/2022 17:09:26 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=899, global_step=1800
06/23/2022 17:09:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.72 on epoch=904
06/23/2022 17:09:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.82 on epoch=909
06/23/2022 17:09:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.65 on epoch=914
06/23/2022 17:09:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.69 on epoch=919
06/23/2022 17:09:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.77 on epoch=924
06/23/2022 17:09:38 - INFO - __main__ - Global step 1850 Train loss 0.73 ACC 0.625 on epoch=924
06/23/2022 17:09:38 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=924, global_step=1850
06/23/2022 17:09:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.70 on epoch=929
06/23/2022 17:09:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.68 on epoch=934
06/23/2022 17:09:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.66 on epoch=939
06/23/2022 17:09:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.78 on epoch=944
06/23/2022 17:09:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.69 on epoch=949
06/23/2022 17:09:51 - INFO - __main__ - Global step 1900 Train loss 0.70 ACC 0.5 on epoch=949
06/23/2022 17:09:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.72 on epoch=954
06/23/2022 17:09:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.72 on epoch=959
06/23/2022 17:09:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.72 on epoch=964
06/23/2022 17:09:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.69 on epoch=969
06/23/2022 17:09:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.71 on epoch=974
06/23/2022 17:09:59 - INFO - __main__ - Global step 1950 Train loss 0.71 ACC 0.75 on epoch=974
06/23/2022 17:09:59 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.75 on epoch=974, global_step=1950
06/23/2022 17:10:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.69 on epoch=979
06/23/2022 17:10:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.67 on epoch=984
06/23/2022 17:10:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.61 on epoch=989
06/23/2022 17:10:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.65 on epoch=994
06/23/2022 17:10:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.59 on epoch=999
06/23/2022 17:10:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:10:07 - INFO - __main__ - Printing 3 examples
06/23/2022 17:10:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/23/2022 17:10:07 - INFO - __main__ - ['equivalent']
06/23/2022 17:10:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/23/2022 17:10:07 - INFO - __main__ - ['equivalent']
06/23/2022 17:10:07 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/23/2022 17:10:07 - INFO - __main__ - ['equivalent']
06/23/2022 17:10:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 17:10:07 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:10:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:10:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:10:07 - INFO - __main__ - Printing 3 examples
06/23/2022 17:10:07 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/23/2022 17:10:07 - INFO - __main__ - ['equivalent']
06/23/2022 17:10:07 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/23/2022 17:10:07 - INFO - __main__ - ['equivalent']
06/23/2022 17:10:07 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/23/2022 17:10:07 - INFO - __main__ - ['equivalent']
06/23/2022 17:10:07 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:10:07 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:10:07 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:10:08 - INFO - __main__ - Global step 2000 Train loss 0.64 ACC 0.625 on epoch=999
06/23/2022 17:10:08 - INFO - __main__ - save last model!
06/23/2022 17:10:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 17:10:08 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 17:10:08 - INFO - __main__ - Printing 3 examples
06/23/2022 17:10:08 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 17:10:08 - INFO - __main__ - ['equivalent']
06/23/2022 17:10:08 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 17:10:08 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:10:08 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 17:10:08 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:10:08 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:10:08 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:10:09 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 17:10:13 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:10:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:10:13 - INFO - __main__ - Starting training!
06/23/2022 17:11:12 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_42_0.4_8_predictions.txt
06/23/2022 17:11:12 - INFO - __main__ - ACC on test data: 0.5147
06/23/2022 17:11:12 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.4, bsz=8, dev_performance=0.75, test_performance=0.5147058823529411
06/23/2022 17:11:12 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.3, bsz=8 ...
06/23/2022 17:11:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:11:13 - INFO - __main__ - Printing 3 examples
06/23/2022 17:11:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/23/2022 17:11:13 - INFO - __main__ - ['equivalent']
06/23/2022 17:11:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/23/2022 17:11:13 - INFO - __main__ - ['equivalent']
06/23/2022 17:11:13 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/23/2022 17:11:13 - INFO - __main__ - ['equivalent']
06/23/2022 17:11:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 17:11:13 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:11:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:11:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:11:13 - INFO - __main__ - Printing 3 examples
06/23/2022 17:11:13 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/23/2022 17:11:13 - INFO - __main__ - ['equivalent']
06/23/2022 17:11:13 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/23/2022 17:11:13 - INFO - __main__ - ['equivalent']
06/23/2022 17:11:13 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/23/2022 17:11:13 - INFO - __main__ - ['equivalent']
06/23/2022 17:11:13 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:11:13 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:11:13 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:11:20 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:11:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:11:20 - INFO - __main__ - Starting training!
06/23/2022 17:11:22 - INFO - __main__ - Step 10 Global step 10 Train loss 6.86 on epoch=4
06/23/2022 17:11:23 - INFO - __main__ - Step 20 Global step 20 Train loss 6.85 on epoch=9
06/23/2022 17:11:24 - INFO - __main__ - Step 30 Global step 30 Train loss 6.87 on epoch=14
06/23/2022 17:11:26 - INFO - __main__ - Step 40 Global step 40 Train loss 6.83 on epoch=19
06/23/2022 17:11:27 - INFO - __main__ - Step 50 Global step 50 Train loss 6.68 on epoch=24
06/23/2022 17:11:31 - INFO - __main__ - Global step 50 Train loss 6.82 ACC 0.0 on epoch=24
06/23/2022 17:11:31 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 17:11:33 - INFO - __main__ - Step 60 Global step 60 Train loss 6.82 on epoch=29
06/23/2022 17:11:34 - INFO - __main__ - Step 70 Global step 70 Train loss 6.82 on epoch=34
06/23/2022 17:11:36 - INFO - __main__ - Step 80 Global step 80 Train loss 6.79 on epoch=39
06/23/2022 17:11:37 - INFO - __main__ - Step 90 Global step 90 Train loss 6.74 on epoch=44
06/23/2022 17:11:39 - INFO - __main__ - Step 100 Global step 100 Train loss 6.79 on epoch=49
06/23/2022 17:11:40 - INFO - __main__ - Global step 100 Train loss 6.79 ACC 0.0 on epoch=49
06/23/2022 17:11:42 - INFO - __main__ - Step 110 Global step 110 Train loss 6.75 on epoch=54
06/23/2022 17:11:43 - INFO - __main__ - Step 120 Global step 120 Train loss 6.71 on epoch=59
06/23/2022 17:11:45 - INFO - __main__ - Step 130 Global step 130 Train loss 6.72 on epoch=64
06/23/2022 17:11:46 - INFO - __main__ - Step 140 Global step 140 Train loss 6.59 on epoch=69
06/23/2022 17:11:47 - INFO - __main__ - Step 150 Global step 150 Train loss 6.74 on epoch=74
06/23/2022 17:11:52 - INFO - __main__ - Global step 150 Train loss 6.70 ACC 0.0 on epoch=74
06/23/2022 17:11:53 - INFO - __main__ - Step 160 Global step 160 Train loss 6.70 on epoch=79
06/23/2022 17:11:54 - INFO - __main__ - Step 170 Global step 170 Train loss 6.70 on epoch=84
06/23/2022 17:11:56 - INFO - __main__ - Step 180 Global step 180 Train loss 6.65 on epoch=89
06/23/2022 17:11:57 - INFO - __main__ - Step 190 Global step 190 Train loss 6.66 on epoch=94
06/23/2022 17:11:59 - INFO - __main__ - Step 200 Global step 200 Train loss 6.61 on epoch=99
06/23/2022 17:12:02 - INFO - __main__ - Global step 200 Train loss 6.66 ACC 0.0 on epoch=99
06/23/2022 17:12:03 - INFO - __main__ - Step 210 Global step 210 Train loss 6.64 on epoch=104
06/23/2022 17:12:05 - INFO - __main__ - Step 220 Global step 220 Train loss 6.58 on epoch=109
06/23/2022 17:12:06 - INFO - __main__ - Step 230 Global step 230 Train loss 6.57 on epoch=114
06/23/2022 17:12:07 - INFO - __main__ - Step 240 Global step 240 Train loss 6.58 on epoch=119
06/23/2022 17:12:09 - INFO - __main__ - Step 250 Global step 250 Train loss 6.45 on epoch=124
06/23/2022 17:12:15 - INFO - __main__ - Global step 250 Train loss 6.56 ACC 0.0 on epoch=124
06/23/2022 17:12:17 - INFO - __main__ - Step 260 Global step 260 Train loss 6.45 on epoch=129
06/23/2022 17:12:18 - INFO - __main__ - Step 270 Global step 270 Train loss 6.43 on epoch=134
06/23/2022 17:12:19 - INFO - __main__ - Step 280 Global step 280 Train loss 6.36 on epoch=139
06/23/2022 17:12:21 - INFO - __main__ - Step 290 Global step 290 Train loss 6.35 on epoch=144
06/23/2022 17:12:22 - INFO - __main__ - Step 300 Global step 300 Train loss 6.37 on epoch=149
06/23/2022 17:12:34 - INFO - __main__ - Global step 300 Train loss 6.39 ACC 0.0 on epoch=149
06/23/2022 17:12:36 - INFO - __main__ - Step 310 Global step 310 Train loss 6.26 on epoch=154
06/23/2022 17:12:37 - INFO - __main__ - Step 320 Global step 320 Train loss 6.20 on epoch=159
06/23/2022 17:12:39 - INFO - __main__ - Step 330 Global step 330 Train loss 6.25 on epoch=164
06/23/2022 17:12:40 - INFO - __main__ - Step 340 Global step 340 Train loss 6.05 on epoch=169
06/23/2022 17:12:41 - INFO - __main__ - Step 350 Global step 350 Train loss 5.98 on epoch=174
06/23/2022 17:12:46 - INFO - __main__ - Global step 350 Train loss 6.15 ACC 0.0 on epoch=174
06/23/2022 17:12:47 - INFO - __main__ - Step 360 Global step 360 Train loss 6.02 on epoch=179
06/23/2022 17:12:48 - INFO - __main__ - Step 370 Global step 370 Train loss 5.97 on epoch=184
06/23/2022 17:12:50 - INFO - __main__ - Step 380 Global step 380 Train loss 5.88 on epoch=189
06/23/2022 17:12:51 - INFO - __main__ - Step 390 Global step 390 Train loss 5.75 on epoch=194
06/23/2022 17:12:53 - INFO - __main__ - Step 400 Global step 400 Train loss 5.79 on epoch=199
06/23/2022 17:13:01 - INFO - __main__ - Global step 400 Train loss 5.88 ACC 0.0 on epoch=199
06/23/2022 17:13:02 - INFO - __main__ - Step 410 Global step 410 Train loss 5.61 on epoch=204
06/23/2022 17:13:04 - INFO - __main__ - Step 420 Global step 420 Train loss 5.44 on epoch=209
06/23/2022 17:13:05 - INFO - __main__ - Step 430 Global step 430 Train loss 5.57 on epoch=214
06/23/2022 17:13:06 - INFO - __main__ - Step 440 Global step 440 Train loss 5.34 on epoch=219
06/23/2022 17:13:08 - INFO - __main__ - Step 450 Global step 450 Train loss 5.30 on epoch=224
06/23/2022 17:13:09 - INFO - __main__ - Global step 450 Train loss 5.45 ACC 0.0 on epoch=224
06/23/2022 17:13:11 - INFO - __main__ - Step 460 Global step 460 Train loss 5.28 on epoch=229
06/23/2022 17:13:12 - INFO - __main__ - Step 470 Global step 470 Train loss 5.20 on epoch=234
06/23/2022 17:13:14 - INFO - __main__ - Step 480 Global step 480 Train loss 5.24 on epoch=239
06/23/2022 17:13:15 - INFO - __main__ - Step 490 Global step 490 Train loss 5.04 on epoch=244
06/23/2022 17:13:16 - INFO - __main__ - Step 500 Global step 500 Train loss 4.99 on epoch=249
06/23/2022 17:13:18 - INFO - __main__ - Global step 500 Train loss 5.15 ACC 0.0 on epoch=249
06/23/2022 17:13:20 - INFO - __main__ - Step 510 Global step 510 Train loss 4.94 on epoch=254
06/23/2022 17:13:21 - INFO - __main__ - Step 520 Global step 520 Train loss 5.02 on epoch=259
06/23/2022 17:13:23 - INFO - __main__ - Step 530 Global step 530 Train loss 4.79 on epoch=264
06/23/2022 17:13:24 - INFO - __main__ - Step 540 Global step 540 Train loss 4.72 on epoch=269
06/23/2022 17:13:25 - INFO - __main__ - Step 550 Global step 550 Train loss 4.72 on epoch=274
06/23/2022 17:13:27 - INFO - __main__ - Global step 550 Train loss 4.84 ACC 0.0 on epoch=274
06/23/2022 17:13:29 - INFO - __main__ - Step 560 Global step 560 Train loss 4.47 on epoch=279
06/23/2022 17:13:30 - INFO - __main__ - Step 570 Global step 570 Train loss 4.54 on epoch=284
06/23/2022 17:13:32 - INFO - __main__ - Step 580 Global step 580 Train loss 4.49 on epoch=289
06/23/2022 17:13:33 - INFO - __main__ - Step 590 Global step 590 Train loss 4.42 on epoch=294
06/23/2022 17:13:34 - INFO - __main__ - Step 600 Global step 600 Train loss 4.40 on epoch=299
06/23/2022 17:13:36 - INFO - __main__ - Global step 600 Train loss 4.46 ACC 0.0 on epoch=299
06/23/2022 17:13:38 - INFO - __main__ - Step 610 Global step 610 Train loss 4.31 on epoch=304
06/23/2022 17:13:39 - INFO - __main__ - Step 620 Global step 620 Train loss 4.33 on epoch=309
06/23/2022 17:13:40 - INFO - __main__ - Step 630 Global step 630 Train loss 4.27 on epoch=314
06/23/2022 17:13:42 - INFO - __main__ - Step 640 Global step 640 Train loss 4.17 on epoch=319
06/23/2022 17:13:43 - INFO - __main__ - Step 650 Global step 650 Train loss 4.08 on epoch=324
06/23/2022 17:13:45 - INFO - __main__ - Global step 650 Train loss 4.23 ACC 0.0 on epoch=324
06/23/2022 17:13:47 - INFO - __main__ - Step 660 Global step 660 Train loss 4.03 on epoch=329
06/23/2022 17:13:48 - INFO - __main__ - Step 670 Global step 670 Train loss 4.14 on epoch=334
06/23/2022 17:13:49 - INFO - __main__ - Step 680 Global step 680 Train loss 3.99 on epoch=339
06/23/2022 17:13:51 - INFO - __main__ - Step 690 Global step 690 Train loss 3.90 on epoch=344
06/23/2022 17:13:52 - INFO - __main__ - Step 700 Global step 700 Train loss 3.70 on epoch=349
06/23/2022 17:13:55 - INFO - __main__ - Global step 700 Train loss 3.95 ACC 0.0 on epoch=349
06/23/2022 17:13:56 - INFO - __main__ - Step 710 Global step 710 Train loss 3.79 on epoch=354
06/23/2022 17:13:58 - INFO - __main__ - Step 720 Global step 720 Train loss 3.70 on epoch=359
06/23/2022 17:13:59 - INFO - __main__ - Step 730 Global step 730 Train loss 3.59 on epoch=364
06/23/2022 17:14:00 - INFO - __main__ - Step 740 Global step 740 Train loss 3.55 on epoch=369
06/23/2022 17:14:02 - INFO - __main__ - Step 750 Global step 750 Train loss 3.38 on epoch=374
06/23/2022 17:14:04 - INFO - __main__ - Global step 750 Train loss 3.60 ACC 0.0 on epoch=374
06/23/2022 17:14:05 - INFO - __main__ - Step 760 Global step 760 Train loss 3.41 on epoch=379
06/23/2022 17:14:07 - INFO - __main__ - Step 770 Global step 770 Train loss 3.24 on epoch=384
06/23/2022 17:14:08 - INFO - __main__ - Step 780 Global step 780 Train loss 3.33 on epoch=389
06/23/2022 17:14:10 - INFO - __main__ - Step 790 Global step 790 Train loss 3.25 on epoch=394
06/23/2022 17:14:11 - INFO - __main__ - Step 800 Global step 800 Train loss 3.13 on epoch=399
06/23/2022 17:14:13 - INFO - __main__ - Global step 800 Train loss 3.27 ACC 0.0 on epoch=399
06/23/2022 17:14:14 - INFO - __main__ - Step 810 Global step 810 Train loss 3.17 on epoch=404
06/23/2022 17:14:16 - INFO - __main__ - Step 820 Global step 820 Train loss 3.15 on epoch=409
06/23/2022 17:14:17 - INFO - __main__ - Step 830 Global step 830 Train loss 3.04 on epoch=414
06/23/2022 17:14:19 - INFO - __main__ - Step 840 Global step 840 Train loss 3.01 on epoch=419
06/23/2022 17:14:20 - INFO - __main__ - Step 850 Global step 850 Train loss 3.00 on epoch=424
06/23/2022 17:14:30 - INFO - __main__ - Global step 850 Train loss 3.07 ACC 0.0 on epoch=424
06/23/2022 17:14:31 - INFO - __main__ - Step 860 Global step 860 Train loss 3.03 on epoch=429
06/23/2022 17:14:33 - INFO - __main__ - Step 870 Global step 870 Train loss 2.92 on epoch=434
06/23/2022 17:14:34 - INFO - __main__ - Step 880 Global step 880 Train loss 2.90 on epoch=439
06/23/2022 17:14:36 - INFO - __main__ - Step 890 Global step 890 Train loss 2.78 on epoch=444
06/23/2022 17:14:37 - INFO - __main__ - Step 900 Global step 900 Train loss 2.76 on epoch=449
06/23/2022 17:14:41 - INFO - __main__ - Global step 900 Train loss 2.88 ACC 0.0 on epoch=449
06/23/2022 17:14:43 - INFO - __main__ - Step 910 Global step 910 Train loss 2.73 on epoch=454
06/23/2022 17:14:44 - INFO - __main__ - Step 920 Global step 920 Train loss 2.78 on epoch=459
06/23/2022 17:14:46 - INFO - __main__ - Step 930 Global step 930 Train loss 2.65 on epoch=464
06/23/2022 17:14:47 - INFO - __main__ - Step 940 Global step 940 Train loss 2.58 on epoch=469
06/23/2022 17:14:49 - INFO - __main__ - Step 950 Global step 950 Train loss 2.63 on epoch=474
06/23/2022 17:14:53 - INFO - __main__ - Global step 950 Train loss 2.67 ACC 0.0 on epoch=474
06/23/2022 17:14:55 - INFO - __main__ - Step 960 Global step 960 Train loss 2.60 on epoch=479
06/23/2022 17:14:56 - INFO - __main__ - Step 970 Global step 970 Train loss 2.52 on epoch=484
06/23/2022 17:14:58 - INFO - __main__ - Step 980 Global step 980 Train loss 2.31 on epoch=489
06/23/2022 17:14:59 - INFO - __main__ - Step 990 Global step 990 Train loss 2.36 on epoch=494
06/23/2022 17:15:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.31 on epoch=499
06/23/2022 17:15:08 - INFO - __main__ - Global step 1000 Train loss 2.42 ACC 0.0 on epoch=499
06/23/2022 17:15:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.35 on epoch=504
06/23/2022 17:15:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.31 on epoch=509
06/23/2022 17:15:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.34 on epoch=514
06/23/2022 17:15:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.28 on epoch=519
06/23/2022 17:15:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.22 on epoch=524
06/23/2022 17:15:27 - INFO - __main__ - Global step 1050 Train loss 2.30 ACC 0.0 on epoch=524
06/23/2022 17:15:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.19 on epoch=529
06/23/2022 17:15:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.20 on epoch=534
06/23/2022 17:15:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.15 on epoch=539
06/23/2022 17:15:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.06 on epoch=544
06/23/2022 17:15:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.00 on epoch=549
06/23/2022 17:15:44 - INFO - __main__ - Global step 1100 Train loss 2.12 ACC 0.25 on epoch=549
06/23/2022 17:15:44 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.25 on epoch=549, global_step=1100
06/23/2022 17:15:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.01 on epoch=554
06/23/2022 17:15:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 2.07 on epoch=559
06/23/2022 17:15:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.99 on epoch=564
06/23/2022 17:15:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.94 on epoch=569
06/23/2022 17:15:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 2.02 on epoch=574
06/23/2022 17:15:52 - INFO - __main__ - Global step 1150 Train loss 2.01 ACC 0.5625 on epoch=574
06/23/2022 17:15:52 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.5625 on epoch=574, global_step=1150
06/23/2022 17:15:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.88 on epoch=579
06/23/2022 17:15:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.85 on epoch=584
06/23/2022 17:15:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.73 on epoch=589
06/23/2022 17:15:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.71 on epoch=594
06/23/2022 17:16:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.73 on epoch=599
06/23/2022 17:16:07 - INFO - __main__ - Global step 1200 Train loss 1.78 ACC 0.5 on epoch=599
06/23/2022 17:16:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.73 on epoch=604
06/23/2022 17:16:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.66 on epoch=609
06/23/2022 17:16:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.65 on epoch=614
06/23/2022 17:16:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.72 on epoch=619
06/23/2022 17:16:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.70 on epoch=624
06/23/2022 17:16:22 - INFO - __main__ - Global step 1250 Train loss 1.69 ACC 0.5 on epoch=624
06/23/2022 17:16:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.60 on epoch=629
06/23/2022 17:16:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.57 on epoch=634
06/23/2022 17:16:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.45 on epoch=639
06/23/2022 17:16:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.52 on epoch=644
06/23/2022 17:16:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.37 on epoch=649
06/23/2022 17:16:33 - INFO - __main__ - Global step 1300 Train loss 1.50 ACC 0.5625 on epoch=649
06/23/2022 17:16:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.38 on epoch=654
06/23/2022 17:16:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.37 on epoch=659
06/23/2022 17:16:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.27 on epoch=664
06/23/2022 17:16:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.23 on epoch=669
06/23/2022 17:16:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.23 on epoch=674
06/23/2022 17:16:43 - INFO - __main__ - Global step 1350 Train loss 1.29 ACC 0.5 on epoch=674
06/23/2022 17:16:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.27 on epoch=679
06/23/2022 17:16:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.23 on epoch=684
06/23/2022 17:16:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.16 on epoch=689
06/23/2022 17:16:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.16 on epoch=694
06/23/2022 17:16:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.06 on epoch=699
06/23/2022 17:16:59 - INFO - __main__ - Global step 1400 Train loss 1.18 ACC 0.5 on epoch=699
06/23/2022 17:17:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.15 on epoch=704
06/23/2022 17:17:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.03 on epoch=709
06/23/2022 17:17:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.05 on epoch=714
06/23/2022 17:17:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.11 on epoch=719
06/23/2022 17:17:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.02 on epoch=724
06/23/2022 17:17:12 - INFO - __main__ - Global step 1450 Train loss 1.07 ACC 0.5 on epoch=724
06/23/2022 17:17:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.86 on epoch=729
06/23/2022 17:17:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.96 on epoch=734
06/23/2022 17:17:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.86 on epoch=739
06/23/2022 17:17:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.84 on epoch=744
06/23/2022 17:17:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.92 on epoch=749
06/23/2022 17:17:22 - INFO - __main__ - Global step 1500 Train loss 0.89 ACC 0.5 on epoch=749
06/23/2022 17:17:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.75 on epoch=754
06/23/2022 17:17:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.82 on epoch=759
06/23/2022 17:17:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.75 on epoch=764
06/23/2022 17:17:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.77 on epoch=769
06/23/2022 17:17:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.74 on epoch=774
06/23/2022 17:17:31 - INFO - __main__ - Global step 1550 Train loss 0.76 ACC 0.5 on epoch=774
06/23/2022 17:17:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.70 on epoch=779
06/23/2022 17:17:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.74 on epoch=784
06/23/2022 17:17:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.71 on epoch=789
06/23/2022 17:17:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.67 on epoch=794
06/23/2022 17:17:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.71 on epoch=799
06/23/2022 17:17:40 - INFO - __main__ - Global step 1600 Train loss 0.71 ACC 0.53125 on epoch=799
06/23/2022 17:17:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.66 on epoch=804
06/23/2022 17:17:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.70 on epoch=809
06/23/2022 17:17:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.66 on epoch=814
06/23/2022 17:17:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.66 on epoch=819
06/23/2022 17:17:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.61 on epoch=824
06/23/2022 17:17:48 - INFO - __main__ - Global step 1650 Train loss 0.66 ACC 0.46875 on epoch=824
06/23/2022 17:17:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.69 on epoch=829
06/23/2022 17:17:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.70 on epoch=834
06/23/2022 17:17:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.62 on epoch=839
06/23/2022 17:17:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.70 on epoch=844
06/23/2022 17:17:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.66 on epoch=849
06/23/2022 17:17:56 - INFO - __main__ - Global step 1700 Train loss 0.67 ACC 0.5625 on epoch=849
06/23/2022 17:17:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.63 on epoch=854
06/23/2022 17:17:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.51 on epoch=859
06/23/2022 17:18:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.56 on epoch=864
06/23/2022 17:18:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.65 on epoch=869
06/23/2022 17:18:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.60 on epoch=874
06/23/2022 17:18:04 - INFO - __main__ - Global step 1750 Train loss 0.59 ACC 0.5 on epoch=874
06/23/2022 17:18:05 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.75 on epoch=879
06/23/2022 17:18:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.55 on epoch=884
06/23/2022 17:18:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.63 on epoch=889
06/23/2022 17:18:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.57 on epoch=894
06/23/2022 17:18:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.53 on epoch=899
06/23/2022 17:18:11 - INFO - __main__ - Global step 1800 Train loss 0.61 ACC 0.59375 on epoch=899
06/23/2022 17:18:11 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=899, global_step=1800
06/23/2022 17:18:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.63 on epoch=904
06/23/2022 17:18:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.60 on epoch=909
06/23/2022 17:18:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=914
06/23/2022 17:18:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.58 on epoch=919
06/23/2022 17:18:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.53 on epoch=924
06/23/2022 17:18:19 - INFO - __main__ - Global step 1850 Train loss 0.58 ACC 0.5 on epoch=924
06/23/2022 17:18:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.60 on epoch=929
06/23/2022 17:18:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.57 on epoch=934
06/23/2022 17:18:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.53 on epoch=939
06/23/2022 17:18:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.54 on epoch=944
06/23/2022 17:18:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.61 on epoch=949
06/23/2022 17:18:26 - INFO - __main__ - Global step 1900 Train loss 0.57 ACC 0.5 on epoch=949
06/23/2022 17:18:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.53 on epoch=954
06/23/2022 17:18:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.53 on epoch=959
06/23/2022 17:18:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.46 on epoch=964
06/23/2022 17:18:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.47 on epoch=969
06/23/2022 17:18:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.52 on epoch=974
06/23/2022 17:18:34 - INFO - __main__ - Global step 1950 Train loss 0.50 ACC 0.5 on epoch=974
06/23/2022 17:18:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.54 on epoch=979
06/23/2022 17:18:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.52 on epoch=984
06/23/2022 17:18:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.54 on epoch=989
06/23/2022 17:18:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.49 on epoch=994
06/23/2022 17:18:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.56 on epoch=999
06/23/2022 17:18:41 - INFO - __main__ - Global step 2000 Train loss 0.53 ACC 0.5 on epoch=999
06/23/2022 17:18:41 - INFO - __main__ - save last model!
06/23/2022 17:18:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 17:18:41 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 17:18:41 - INFO - __main__ - Printing 3 examples
06/23/2022 17:18:41 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 17:18:41 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 17:18:41 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:18:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 17:18:41 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:18:41 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:18:41 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:18:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:18:42 - INFO - __main__ - Printing 3 examples
06/23/2022 17:18:42 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/23/2022 17:18:42 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:42 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/23/2022 17:18:42 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:42 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/23/2022 17:18:42 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 17:18:42 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:18:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:18:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:18:42 - INFO - __main__ - Printing 3 examples
06/23/2022 17:18:42 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/23/2022 17:18:42 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:42 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/23/2022 17:18:42 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:42 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/23/2022 17:18:42 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:42 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:18:42 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:18:42 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:18:42 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 17:18:48 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:18:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:18:48 - INFO - __main__ - Starting training!
06/23/2022 17:18:51 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_42_0.3_8_predictions.txt
06/23/2022 17:18:51 - INFO - __main__ - ACC on test data: 0.6642
06/23/2022 17:18:51 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.3, bsz=8, dev_performance=0.59375, test_performance=0.6642156862745098
06/23/2022 17:18:51 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.2, bsz=8 ...
06/23/2022 17:18:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:18:52 - INFO - __main__ - Printing 3 examples
06/23/2022 17:18:52 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/23/2022 17:18:52 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:52 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/23/2022 17:18:52 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:52 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/23/2022 17:18:52 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 17:18:52 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:18:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:18:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:18:52 - INFO - __main__ - Printing 3 examples
06/23/2022 17:18:52 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/23/2022 17:18:52 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:52 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/23/2022 17:18:52 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:52 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/23/2022 17:18:52 - INFO - __main__ - ['equivalent']
06/23/2022 17:18:52 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:18:52 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:18:52 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:18:58 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:18:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:18:59 - INFO - __main__ - Starting training!
06/23/2022 17:19:00 - INFO - __main__ - Step 10 Global step 10 Train loss 6.91 on epoch=4
06/23/2022 17:19:02 - INFO - __main__ - Step 20 Global step 20 Train loss 6.83 on epoch=9
06/23/2022 17:19:03 - INFO - __main__ - Step 30 Global step 30 Train loss 6.90 on epoch=14
06/23/2022 17:19:05 - INFO - __main__ - Step 40 Global step 40 Train loss 6.90 on epoch=19
06/23/2022 17:19:06 - INFO - __main__ - Step 50 Global step 50 Train loss 6.70 on epoch=24
06/23/2022 17:19:07 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/23/2022 17:19:07 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 17:19:09 - INFO - __main__ - Step 60 Global step 60 Train loss 6.88 on epoch=29
06/23/2022 17:19:11 - INFO - __main__ - Step 70 Global step 70 Train loss 6.80 on epoch=34
06/23/2022 17:19:13 - INFO - __main__ - Step 80 Global step 80 Train loss 6.79 on epoch=39
06/23/2022 17:19:14 - INFO - __main__ - Step 90 Global step 90 Train loss 6.78 on epoch=44
06/23/2022 17:19:16 - INFO - __main__ - Step 100 Global step 100 Train loss 6.79 on epoch=49
06/23/2022 17:19:20 - INFO - __main__ - Global step 100 Train loss 6.81 ACC 0.0 on epoch=49
06/23/2022 17:19:21 - INFO - __main__ - Step 110 Global step 110 Train loss 6.78 on epoch=54
06/23/2022 17:19:22 - INFO - __main__ - Step 120 Global step 120 Train loss 6.77 on epoch=59
06/23/2022 17:19:24 - INFO - __main__ - Step 130 Global step 130 Train loss 6.79 on epoch=64
06/23/2022 17:19:25 - INFO - __main__ - Step 140 Global step 140 Train loss 6.79 on epoch=69
06/23/2022 17:19:27 - INFO - __main__ - Step 150 Global step 150 Train loss 6.82 on epoch=74
06/23/2022 17:19:31 - INFO - __main__ - Global step 150 Train loss 6.79 ACC 0.0 on epoch=74
06/23/2022 17:19:33 - INFO - __main__ - Step 160 Global step 160 Train loss 6.77 on epoch=79
06/23/2022 17:19:34 - INFO - __main__ - Step 170 Global step 170 Train loss 6.68 on epoch=84
06/23/2022 17:19:35 - INFO - __main__ - Step 180 Global step 180 Train loss 6.73 on epoch=89
06/23/2022 17:19:37 - INFO - __main__ - Step 190 Global step 190 Train loss 6.72 on epoch=94
06/23/2022 17:19:38 - INFO - __main__ - Step 200 Global step 200 Train loss 6.69 on epoch=99
06/23/2022 17:19:40 - INFO - __main__ - Global step 200 Train loss 6.72 ACC 0.0 on epoch=99
06/23/2022 17:19:42 - INFO - __main__ - Step 210 Global step 210 Train loss 6.74 on epoch=104
06/23/2022 17:19:43 - INFO - __main__ - Step 220 Global step 220 Train loss 6.63 on epoch=109
06/23/2022 17:19:44 - INFO - __main__ - Step 230 Global step 230 Train loss 6.70 on epoch=114
06/23/2022 17:19:46 - INFO - __main__ - Step 240 Global step 240 Train loss 6.66 on epoch=119
06/23/2022 17:19:47 - INFO - __main__ - Step 250 Global step 250 Train loss 6.66 on epoch=124
06/23/2022 17:19:59 - INFO - __main__ - Global step 250 Train loss 6.67 ACC 0.0 on epoch=124
06/23/2022 17:20:00 - INFO - __main__ - Step 260 Global step 260 Train loss 6.65 on epoch=129
06/23/2022 17:20:02 - INFO - __main__ - Step 270 Global step 270 Train loss 6.60 on epoch=134
06/23/2022 17:20:03 - INFO - __main__ - Step 280 Global step 280 Train loss 6.62 on epoch=139
06/23/2022 17:20:05 - INFO - __main__ - Step 290 Global step 290 Train loss 6.57 on epoch=144
06/23/2022 17:20:06 - INFO - __main__ - Step 300 Global step 300 Train loss 6.51 on epoch=149
06/23/2022 17:20:14 - INFO - __main__ - Global step 300 Train loss 6.59 ACC 0.0 on epoch=149
06/23/2022 17:20:15 - INFO - __main__ - Step 310 Global step 310 Train loss 6.44 on epoch=154
06/23/2022 17:20:16 - INFO - __main__ - Step 320 Global step 320 Train loss 6.46 on epoch=159
06/23/2022 17:20:18 - INFO - __main__ - Step 330 Global step 330 Train loss 6.47 on epoch=164
06/23/2022 17:20:20 - INFO - __main__ - Step 340 Global step 340 Train loss 6.36 on epoch=169
06/23/2022 17:20:21 - INFO - __main__ - Step 350 Global step 350 Train loss 6.43 on epoch=174
06/23/2022 17:20:24 - INFO - __main__ - Global step 350 Train loss 6.43 ACC 0.0 on epoch=174
06/23/2022 17:20:26 - INFO - __main__ - Step 360 Global step 360 Train loss 6.42 on epoch=179
06/23/2022 17:20:27 - INFO - __main__ - Step 370 Global step 370 Train loss 6.35 on epoch=184
06/23/2022 17:20:29 - INFO - __main__ - Step 380 Global step 380 Train loss 6.28 on epoch=189
06/23/2022 17:20:30 - INFO - __main__ - Step 390 Global step 390 Train loss 6.33 on epoch=194
06/23/2022 17:20:32 - INFO - __main__ - Step 400 Global step 400 Train loss 6.32 on epoch=199
06/23/2022 17:20:43 - INFO - __main__ - Global step 400 Train loss 6.34 ACC 0.0 on epoch=199
06/23/2022 17:20:45 - INFO - __main__ - Step 410 Global step 410 Train loss 6.20 on epoch=204
06/23/2022 17:20:46 - INFO - __main__ - Step 420 Global step 420 Train loss 6.22 on epoch=209
06/23/2022 17:20:47 - INFO - __main__ - Step 430 Global step 430 Train loss 6.25 on epoch=214
06/23/2022 17:20:49 - INFO - __main__ - Step 440 Global step 440 Train loss 6.21 on epoch=219
06/23/2022 17:20:50 - INFO - __main__ - Step 450 Global step 450 Train loss 6.20 on epoch=224
06/23/2022 17:21:02 - INFO - __main__ - Global step 450 Train loss 6.21 ACC 0.0 on epoch=224
06/23/2022 17:21:03 - INFO - __main__ - Step 460 Global step 460 Train loss 6.18 on epoch=229
06/23/2022 17:21:05 - INFO - __main__ - Step 470 Global step 470 Train loss 6.11 on epoch=234
06/23/2022 17:21:06 - INFO - __main__ - Step 480 Global step 480 Train loss 6.06 on epoch=239
06/23/2022 17:21:08 - INFO - __main__ - Step 490 Global step 490 Train loss 6.05 on epoch=244
06/23/2022 17:21:09 - INFO - __main__ - Step 500 Global step 500 Train loss 6.12 on epoch=249
06/23/2022 17:21:21 - INFO - __main__ - Global step 500 Train loss 6.10 ACC 0.0 on epoch=249
06/23/2022 17:21:23 - INFO - __main__ - Step 510 Global step 510 Train loss 6.10 on epoch=254
06/23/2022 17:21:24 - INFO - __main__ - Step 520 Global step 520 Train loss 5.97 on epoch=259
06/23/2022 17:21:26 - INFO - __main__ - Step 530 Global step 530 Train loss 6.03 on epoch=264
06/23/2022 17:21:27 - INFO - __main__ - Step 540 Global step 540 Train loss 5.96 on epoch=269
06/23/2022 17:21:28 - INFO - __main__ - Step 550 Global step 550 Train loss 5.95 on epoch=274
06/23/2022 17:21:40 - INFO - __main__ - Global step 550 Train loss 6.00 ACC 0.0 on epoch=274
06/23/2022 17:21:42 - INFO - __main__ - Step 560 Global step 560 Train loss 5.98 on epoch=279
06/23/2022 17:21:43 - INFO - __main__ - Step 570 Global step 570 Train loss 6.02 on epoch=284
06/23/2022 17:21:45 - INFO - __main__ - Step 580 Global step 580 Train loss 5.88 on epoch=289
06/23/2022 17:21:46 - INFO - __main__ - Step 590 Global step 590 Train loss 5.90 on epoch=294
06/23/2022 17:21:48 - INFO - __main__ - Step 600 Global step 600 Train loss 5.81 on epoch=299
06/23/2022 17:21:54 - INFO - __main__ - Global step 600 Train loss 5.92 ACC 0.0 on epoch=299
06/23/2022 17:21:56 - INFO - __main__ - Step 610 Global step 610 Train loss 5.82 on epoch=304
06/23/2022 17:21:57 - INFO - __main__ - Step 620 Global step 620 Train loss 5.81 on epoch=309
06/23/2022 17:21:58 - INFO - __main__ - Step 630 Global step 630 Train loss 5.68 on epoch=314
06/23/2022 17:22:00 - INFO - __main__ - Step 640 Global step 640 Train loss 5.71 on epoch=319
06/23/2022 17:22:02 - INFO - __main__ - Step 650 Global step 650 Train loss 5.65 on epoch=324
06/23/2022 17:22:04 - INFO - __main__ - Global step 650 Train loss 5.74 ACC 0.0 on epoch=324
06/23/2022 17:22:05 - INFO - __main__ - Step 660 Global step 660 Train loss 5.73 on epoch=329
06/23/2022 17:22:07 - INFO - __main__ - Step 670 Global step 670 Train loss 5.63 on epoch=334
06/23/2022 17:22:08 - INFO - __main__ - Step 680 Global step 680 Train loss 5.46 on epoch=339
06/23/2022 17:22:10 - INFO - __main__ - Step 690 Global step 690 Train loss 5.55 on epoch=344
06/23/2022 17:22:11 - INFO - __main__ - Step 700 Global step 700 Train loss 5.40 on epoch=349
06/23/2022 17:22:19 - INFO - __main__ - Global step 700 Train loss 5.56 ACC 0.0 on epoch=349
06/23/2022 17:22:20 - INFO - __main__ - Step 710 Global step 710 Train loss 5.42 on epoch=354
06/23/2022 17:22:22 - INFO - __main__ - Step 720 Global step 720 Train loss 5.40 on epoch=359
06/23/2022 17:22:23 - INFO - __main__ - Step 730 Global step 730 Train loss 5.24 on epoch=364
06/23/2022 17:22:25 - INFO - __main__ - Step 740 Global step 740 Train loss 5.26 on epoch=369
06/23/2022 17:22:26 - INFO - __main__ - Step 750 Global step 750 Train loss 5.22 on epoch=374
06/23/2022 17:22:35 - INFO - __main__ - Global step 750 Train loss 5.31 ACC 0.0 on epoch=374
06/23/2022 17:22:37 - INFO - __main__ - Step 760 Global step 760 Train loss 5.22 on epoch=379
06/23/2022 17:22:38 - INFO - __main__ - Step 770 Global step 770 Train loss 5.16 on epoch=384
06/23/2022 17:22:39 - INFO - __main__ - Step 780 Global step 780 Train loss 5.31 on epoch=389
06/23/2022 17:22:41 - INFO - __main__ - Step 790 Global step 790 Train loss 5.05 on epoch=394
06/23/2022 17:22:42 - INFO - __main__ - Step 800 Global step 800 Train loss 5.03 on epoch=399
06/23/2022 17:22:54 - INFO - __main__ - Global step 800 Train loss 5.15 ACC 0.0 on epoch=399
06/23/2022 17:22:55 - INFO - __main__ - Step 810 Global step 810 Train loss 4.94 on epoch=404
06/23/2022 17:22:57 - INFO - __main__ - Step 820 Global step 820 Train loss 4.88 on epoch=409
06/23/2022 17:22:58 - INFO - __main__ - Step 830 Global step 830 Train loss 4.96 on epoch=414
06/23/2022 17:23:00 - INFO - __main__ - Step 840 Global step 840 Train loss 4.97 on epoch=419
06/23/2022 17:23:02 - INFO - __main__ - Step 850 Global step 850 Train loss 4.76 on epoch=424
06/23/2022 17:23:09 - INFO - __main__ - Global step 850 Train loss 4.90 ACC 0.0 on epoch=424
06/23/2022 17:23:11 - INFO - __main__ - Step 860 Global step 860 Train loss 4.75 on epoch=429
06/23/2022 17:23:12 - INFO - __main__ - Step 870 Global step 870 Train loss 4.69 on epoch=434
06/23/2022 17:23:14 - INFO - __main__ - Step 880 Global step 880 Train loss 4.70 on epoch=439
06/23/2022 17:23:15 - INFO - __main__ - Step 890 Global step 890 Train loss 4.61 on epoch=444
06/23/2022 17:23:16 - INFO - __main__ - Step 900 Global step 900 Train loss 4.57 on epoch=449
06/23/2022 17:23:19 - INFO - __main__ - Global step 900 Train loss 4.67 ACC 0.0 on epoch=449
06/23/2022 17:23:21 - INFO - __main__ - Step 910 Global step 910 Train loss 4.49 on epoch=454
06/23/2022 17:23:22 - INFO - __main__ - Step 920 Global step 920 Train loss 4.54 on epoch=459
06/23/2022 17:23:23 - INFO - __main__ - Step 930 Global step 930 Train loss 4.38 on epoch=464
06/23/2022 17:23:25 - INFO - __main__ - Step 940 Global step 940 Train loss 4.34 on epoch=469
06/23/2022 17:23:26 - INFO - __main__ - Step 950 Global step 950 Train loss 4.18 on epoch=474
06/23/2022 17:23:34 - INFO - __main__ - Global step 950 Train loss 4.39 ACC 0.0 on epoch=474
06/23/2022 17:23:35 - INFO - __main__ - Step 960 Global step 960 Train loss 4.23 on epoch=479
06/23/2022 17:23:37 - INFO - __main__ - Step 970 Global step 970 Train loss 4.17 on epoch=484
06/23/2022 17:23:38 - INFO - __main__ - Step 980 Global step 980 Train loss 4.12 on epoch=489
06/23/2022 17:23:39 - INFO - __main__ - Step 990 Global step 990 Train loss 4.01 on epoch=494
06/23/2022 17:23:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.95 on epoch=499
06/23/2022 17:23:43 - INFO - __main__ - Global step 1000 Train loss 4.09 ACC 0.0 on epoch=499
06/23/2022 17:23:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.90 on epoch=504
06/23/2022 17:23:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.80 on epoch=509
06/23/2022 17:23:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.80 on epoch=514
06/23/2022 17:23:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.93 on epoch=519
06/23/2022 17:23:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.83 on epoch=524
06/23/2022 17:23:54 - INFO - __main__ - Global step 1050 Train loss 3.85 ACC 0.0 on epoch=524
06/23/2022 17:23:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.64 on epoch=529
06/23/2022 17:23:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.62 on epoch=534
06/23/2022 17:23:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.55 on epoch=539
06/23/2022 17:23:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.43 on epoch=544
06/23/2022 17:24:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.45 on epoch=549
06/23/2022 17:24:03 - INFO - __main__ - Global step 1100 Train loss 3.54 ACC 0.0 on epoch=549
06/23/2022 17:24:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.46 on epoch=554
06/23/2022 17:24:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.26 on epoch=559
06/23/2022 17:24:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.21 on epoch=564
06/23/2022 17:24:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.18 on epoch=569
06/23/2022 17:24:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.09 on epoch=574
06/23/2022 17:24:13 - INFO - __main__ - Global step 1150 Train loss 3.24 ACC 0.0 on epoch=574
06/23/2022 17:24:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.02 on epoch=579
06/23/2022 17:24:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.13 on epoch=584
06/23/2022 17:24:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.03 on epoch=589
06/23/2022 17:24:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.99 on epoch=594
06/23/2022 17:24:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.01 on epoch=599
06/23/2022 17:24:24 - INFO - __main__ - Global step 1200 Train loss 3.03 ACC 0.0 on epoch=599
06/23/2022 17:24:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.07 on epoch=604
06/23/2022 17:24:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.82 on epoch=609
06/23/2022 17:24:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.77 on epoch=614
06/23/2022 17:24:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.79 on epoch=619
06/23/2022 17:24:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.70 on epoch=624
06/23/2022 17:24:34 - INFO - __main__ - Global step 1250 Train loss 2.83 ACC 0.0625 on epoch=624
06/23/2022 17:24:34 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=624, global_step=1250
06/23/2022 17:24:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.72 on epoch=629
06/23/2022 17:24:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.64 on epoch=634
06/23/2022 17:24:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.58 on epoch=639
06/23/2022 17:24:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.55 on epoch=644
06/23/2022 17:24:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.38 on epoch=649
06/23/2022 17:24:45 - INFO - __main__ - Global step 1300 Train loss 2.57 ACC 0.3125 on epoch=649
06/23/2022 17:24:45 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.3125 on epoch=649, global_step=1300
06/23/2022 17:24:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.50 on epoch=654
06/23/2022 17:24:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.35 on epoch=659
06/23/2022 17:24:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.37 on epoch=664
06/23/2022 17:24:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.35 on epoch=669
06/23/2022 17:24:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.22 on epoch=674
06/23/2022 17:24:53 - INFO - __main__ - Global step 1350 Train loss 2.36 ACC 0.5 on epoch=674
06/23/2022 17:24:53 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.5 on epoch=674, global_step=1350
06/23/2022 17:24:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.33 on epoch=679
06/23/2022 17:24:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.18 on epoch=684
06/23/2022 17:24:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.15 on epoch=689
06/23/2022 17:24:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.06 on epoch=694
06/23/2022 17:25:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.07 on epoch=699
06/23/2022 17:25:02 - INFO - __main__ - Global step 1400 Train loss 2.16 ACC 0.5 on epoch=699
06/23/2022 17:25:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.96 on epoch=704
06/23/2022 17:25:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.08 on epoch=709
06/23/2022 17:25:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.96 on epoch=714
06/23/2022 17:25:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.87 on epoch=719
06/23/2022 17:25:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.81 on epoch=724
06/23/2022 17:25:14 - INFO - __main__ - Global step 1450 Train loss 1.94 ACC 0.46875 on epoch=724
06/23/2022 17:25:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.79 on epoch=729
06/23/2022 17:25:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.66 on epoch=734
06/23/2022 17:25:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.82 on epoch=739
06/23/2022 17:25:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.77 on epoch=744
06/23/2022 17:25:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.70 on epoch=749
06/23/2022 17:25:23 - INFO - __main__ - Global step 1500 Train loss 1.75 ACC 0.5 on epoch=749
06/23/2022 17:25:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.68 on epoch=754
06/23/2022 17:25:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.61 on epoch=759
06/23/2022 17:25:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.56 on epoch=764
06/23/2022 17:25:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.51 on epoch=769
06/23/2022 17:25:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.55 on epoch=774
06/23/2022 17:25:33 - INFO - __main__ - Global step 1550 Train loss 1.58 ACC 0.375 on epoch=774
06/23/2022 17:25:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.45 on epoch=779
06/23/2022 17:25:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.55 on epoch=784
06/23/2022 17:25:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.49 on epoch=789
06/23/2022 17:25:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.52 on epoch=794
06/23/2022 17:25:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.48 on epoch=799
06/23/2022 17:25:42 - INFO - __main__ - Global step 1600 Train loss 1.50 ACC 0.5 on epoch=799
06/23/2022 17:25:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.39 on epoch=804
06/23/2022 17:25:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.47 on epoch=809
06/23/2022 17:25:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.36 on epoch=814
06/23/2022 17:25:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.19 on epoch=819
06/23/2022 17:25:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.19 on epoch=824
06/23/2022 17:25:51 - INFO - __main__ - Global step 1650 Train loss 1.32 ACC 0.5 on epoch=824
06/23/2022 17:25:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.31 on epoch=829
06/23/2022 17:25:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.33 on epoch=834
06/23/2022 17:25:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.19 on epoch=839
06/23/2022 17:25:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.20 on epoch=844
06/23/2022 17:25:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.11 on epoch=849
06/23/2022 17:26:01 - INFO - __main__ - Global step 1700 Train loss 1.23 ACC 0.5 on epoch=849
06/23/2022 17:26:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.14 on epoch=854
06/23/2022 17:26:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.16 on epoch=859
06/23/2022 17:26:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.04 on epoch=864
06/23/2022 17:26:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.18 on epoch=869
06/23/2022 17:26:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.09 on epoch=874
06/23/2022 17:26:09 - INFO - __main__ - Global step 1750 Train loss 1.12 ACC 0.5 on epoch=874
06/23/2022 17:26:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.08 on epoch=879
06/23/2022 17:26:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.13 on epoch=884
06/23/2022 17:26:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.02 on epoch=889
06/23/2022 17:26:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.02 on epoch=894
06/23/2022 17:26:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.98 on epoch=899
06/23/2022 17:26:19 - INFO - __main__ - Global step 1800 Train loss 1.05 ACC 0.5 on epoch=899
06/23/2022 17:26:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.95 on epoch=904
06/23/2022 17:26:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.95 on epoch=909
06/23/2022 17:26:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.98 on epoch=914
06/23/2022 17:26:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.98 on epoch=919
06/23/2022 17:26:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.97 on epoch=924
06/23/2022 17:26:28 - INFO - __main__ - Global step 1850 Train loss 0.97 ACC 0.5 on epoch=924
06/23/2022 17:26:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.87 on epoch=929
06/23/2022 17:26:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.96 on epoch=934
06/23/2022 17:26:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.88 on epoch=939
06/23/2022 17:26:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.82 on epoch=944
06/23/2022 17:26:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.85 on epoch=949
06/23/2022 17:26:37 - INFO - __main__ - Global step 1900 Train loss 0.87 ACC 0.5 on epoch=949
06/23/2022 17:26:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.88 on epoch=954
06/23/2022 17:26:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.93 on epoch=959
06/23/2022 17:26:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.85 on epoch=964
06/23/2022 17:26:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.96 on epoch=969
06/23/2022 17:26:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.85 on epoch=974
06/23/2022 17:26:46 - INFO - __main__ - Global step 1950 Train loss 0.89 ACC 0.5 on epoch=974
06/23/2022 17:26:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.84 on epoch=979
06/23/2022 17:26:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.84 on epoch=984
06/23/2022 17:26:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.83 on epoch=989
06/23/2022 17:26:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.87 on epoch=994
06/23/2022 17:26:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.78 on epoch=999
06/23/2022 17:26:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:26:54 - INFO - __main__ - Printing 3 examples
06/23/2022 17:26:54 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/23/2022 17:26:54 - INFO - __main__ - ['equivalent']
06/23/2022 17:26:54 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/23/2022 17:26:54 - INFO - __main__ - ['equivalent']
06/23/2022 17:26:54 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/23/2022 17:26:54 - INFO - __main__ - ['equivalent']
06/23/2022 17:26:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 17:26:54 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:26:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:26:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:26:54 - INFO - __main__ - Printing 3 examples
06/23/2022 17:26:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/23/2022 17:26:54 - INFO - __main__ - ['equivalent']
06/23/2022 17:26:54 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/23/2022 17:26:54 - INFO - __main__ - ['equivalent']
06/23/2022 17:26:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/23/2022 17:26:54 - INFO - __main__ - ['equivalent']
06/23/2022 17:26:54 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:26:54 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:26:55 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:26:55 - INFO - __main__ - Global step 2000 Train loss 0.83 ACC 0.5 on epoch=999
06/23/2022 17:26:55 - INFO - __main__ - save last model!
06/23/2022 17:26:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 17:26:55 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 17:26:55 - INFO - __main__ - Printing 3 examples
06/23/2022 17:26:55 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 17:26:55 - INFO - __main__ - ['equivalent']
06/23/2022 17:26:55 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 17:26:55 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:26:55 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 17:26:55 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:26:55 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:26:55 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:26:56 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 17:27:01 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:27:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:27:01 - INFO - __main__ - Starting training!
06/23/2022 17:27:22 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_42_0.2_8_predictions.txt
06/23/2022 17:27:22 - INFO - __main__ - ACC on test data: 0.6838
06/23/2022 17:27:22 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.6838235294117647
06/23/2022 17:27:22 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.5, bsz=8 ...
06/23/2022 17:27:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:27:23 - INFO - __main__ - Printing 3 examples
06/23/2022 17:27:23 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/23/2022 17:27:23 - INFO - __main__ - ['equivalent']
06/23/2022 17:27:23 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/23/2022 17:27:23 - INFO - __main__ - ['equivalent']
06/23/2022 17:27:23 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/23/2022 17:27:23 - INFO - __main__ - ['equivalent']
06/23/2022 17:27:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 17:27:23 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:27:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:27:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:27:23 - INFO - __main__ - Printing 3 examples
06/23/2022 17:27:23 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/23/2022 17:27:23 - INFO - __main__ - ['equivalent']
06/23/2022 17:27:23 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/23/2022 17:27:23 - INFO - __main__ - ['equivalent']
06/23/2022 17:27:23 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/23/2022 17:27:23 - INFO - __main__ - ['equivalent']
06/23/2022 17:27:23 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:27:23 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:27:23 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:27:30 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:27:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:27:30 - INFO - __main__ - Starting training!
06/23/2022 17:27:32 - INFO - __main__ - Step 10 Global step 10 Train loss 6.95 on epoch=4
06/23/2022 17:27:34 - INFO - __main__ - Step 20 Global step 20 Train loss 6.91 on epoch=9
06/23/2022 17:27:35 - INFO - __main__ - Step 30 Global step 30 Train loss 6.83 on epoch=14
06/23/2022 17:27:36 - INFO - __main__ - Step 40 Global step 40 Train loss 6.75 on epoch=19
06/23/2022 17:27:38 - INFO - __main__ - Step 50 Global step 50 Train loss 6.78 on epoch=24
06/23/2022 17:27:40 - INFO - __main__ - Global step 50 Train loss 6.84 ACC 0.0 on epoch=24
06/23/2022 17:27:40 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 17:27:42 - INFO - __main__ - Step 60 Global step 60 Train loss 6.73 on epoch=29
06/23/2022 17:27:43 - INFO - __main__ - Step 70 Global step 70 Train loss 6.68 on epoch=34
06/23/2022 17:27:44 - INFO - __main__ - Step 80 Global step 80 Train loss 6.61 on epoch=39
06/23/2022 17:27:46 - INFO - __main__ - Step 90 Global step 90 Train loss 6.61 on epoch=44
06/23/2022 17:27:47 - INFO - __main__ - Step 100 Global step 100 Train loss 6.57 on epoch=49
06/23/2022 17:27:55 - INFO - __main__ - Global step 100 Train loss 6.64 ACC 0.0 on epoch=49
06/23/2022 17:27:56 - INFO - __main__ - Step 110 Global step 110 Train loss 6.41 on epoch=54
06/23/2022 17:27:57 - INFO - __main__ - Step 120 Global step 120 Train loss 6.45 on epoch=59
06/23/2022 17:27:59 - INFO - __main__ - Step 130 Global step 130 Train loss 6.33 on epoch=64
06/23/2022 17:28:00 - INFO - __main__ - Step 140 Global step 140 Train loss 6.41 on epoch=69
06/23/2022 17:28:02 - INFO - __main__ - Step 150 Global step 150 Train loss 6.25 on epoch=74
06/23/2022 17:28:06 - INFO - __main__ - Global step 150 Train loss 6.37 ACC 0.0 on epoch=74
06/23/2022 17:28:08 - INFO - __main__ - Step 160 Global step 160 Train loss 6.17 on epoch=79
06/23/2022 17:28:09 - INFO - __main__ - Step 170 Global step 170 Train loss 6.08 on epoch=84
06/23/2022 17:28:11 - INFO - __main__ - Step 180 Global step 180 Train loss 6.08 on epoch=89
06/23/2022 17:28:12 - INFO - __main__ - Step 190 Global step 190 Train loss 5.97 on epoch=94
06/23/2022 17:28:13 - INFO - __main__ - Step 200 Global step 200 Train loss 6.00 on epoch=99
06/23/2022 17:28:22 - INFO - __main__ - Global step 200 Train loss 6.06 ACC 0.0 on epoch=99
06/23/2022 17:28:23 - INFO - __main__ - Step 210 Global step 210 Train loss 5.87 on epoch=104
06/23/2022 17:28:25 - INFO - __main__ - Step 220 Global step 220 Train loss 5.85 on epoch=109
06/23/2022 17:28:26 - INFO - __main__ - Step 230 Global step 230 Train loss 5.75 on epoch=114
06/23/2022 17:28:27 - INFO - __main__ - Step 240 Global step 240 Train loss 5.89 on epoch=119
06/23/2022 17:28:29 - INFO - __main__ - Step 250 Global step 250 Train loss 5.76 on epoch=124
06/23/2022 17:28:32 - INFO - __main__ - Global step 250 Train loss 5.82 ACC 0.0 on epoch=124
06/23/2022 17:28:33 - INFO - __main__ - Step 260 Global step 260 Train loss 5.69 on epoch=129
06/23/2022 17:28:35 - INFO - __main__ - Step 270 Global step 270 Train loss 5.73 on epoch=134
06/23/2022 17:28:36 - INFO - __main__ - Step 280 Global step 280 Train loss 5.77 on epoch=139
06/23/2022 17:28:37 - INFO - __main__ - Step 290 Global step 290 Train loss 5.63 on epoch=144
06/23/2022 17:28:39 - INFO - __main__ - Step 300 Global step 300 Train loss 5.53 on epoch=149
06/23/2022 17:28:50 - INFO - __main__ - Global step 300 Train loss 5.67 ACC 0.0 on epoch=149
06/23/2022 17:28:52 - INFO - __main__ - Step 310 Global step 310 Train loss 5.52 on epoch=154
06/23/2022 17:28:53 - INFO - __main__ - Step 320 Global step 320 Train loss 5.36 on epoch=159
06/23/2022 17:28:55 - INFO - __main__ - Step 330 Global step 330 Train loss 5.33 on epoch=164
06/23/2022 17:28:56 - INFO - __main__ - Step 340 Global step 340 Train loss 5.36 on epoch=169
06/23/2022 17:28:57 - INFO - __main__ - Step 350 Global step 350 Train loss 5.20 on epoch=174
06/23/2022 17:29:05 - INFO - __main__ - Global step 350 Train loss 5.35 ACC 0.0 on epoch=174
06/23/2022 17:29:06 - INFO - __main__ - Step 360 Global step 360 Train loss 5.13 on epoch=179
06/23/2022 17:29:07 - INFO - __main__ - Step 370 Global step 370 Train loss 4.93 on epoch=184
06/23/2022 17:29:09 - INFO - __main__ - Step 380 Global step 380 Train loss 4.96 on epoch=189
06/23/2022 17:29:10 - INFO - __main__ - Step 390 Global step 390 Train loss 5.02 on epoch=194
06/23/2022 17:29:12 - INFO - __main__ - Step 400 Global step 400 Train loss 4.77 on epoch=199
06/23/2022 17:29:22 - INFO - __main__ - Global step 400 Train loss 4.96 ACC 0.0 on epoch=199
06/23/2022 17:29:23 - INFO - __main__ - Step 410 Global step 410 Train loss 4.68 on epoch=204
06/23/2022 17:29:25 - INFO - __main__ - Step 420 Global step 420 Train loss 4.71 on epoch=209
06/23/2022 17:29:26 - INFO - __main__ - Step 430 Global step 430 Train loss 4.63 on epoch=214
06/23/2022 17:29:27 - INFO - __main__ - Step 440 Global step 440 Train loss 4.54 on epoch=219
06/23/2022 17:29:29 - INFO - __main__ - Step 450 Global step 450 Train loss 4.50 on epoch=224
06/23/2022 17:29:39 - INFO - __main__ - Global step 450 Train loss 4.61 ACC 0.0 on epoch=224
06/23/2022 17:29:41 - INFO - __main__ - Step 460 Global step 460 Train loss 4.42 on epoch=229
06/23/2022 17:29:42 - INFO - __main__ - Step 470 Global step 470 Train loss 4.34 on epoch=234
06/23/2022 17:29:43 - INFO - __main__ - Step 480 Global step 480 Train loss 4.31 on epoch=239
06/23/2022 17:29:45 - INFO - __main__ - Step 490 Global step 490 Train loss 4.20 on epoch=244
06/23/2022 17:29:46 - INFO - __main__ - Step 500 Global step 500 Train loss 4.15 on epoch=249
06/23/2022 17:29:58 - INFO - __main__ - Global step 500 Train loss 4.28 ACC 0.0 on epoch=249
06/23/2022 17:29:59 - INFO - __main__ - Step 510 Global step 510 Train loss 4.15 on epoch=254
06/23/2022 17:30:01 - INFO - __main__ - Step 520 Global step 520 Train loss 4.07 on epoch=259
06/23/2022 17:30:02 - INFO - __main__ - Step 530 Global step 530 Train loss 3.95 on epoch=264
06/23/2022 17:30:03 - INFO - __main__ - Step 540 Global step 540 Train loss 3.85 on epoch=269
06/23/2022 17:30:05 - INFO - __main__ - Step 550 Global step 550 Train loss 3.90 on epoch=274
06/23/2022 17:30:16 - INFO - __main__ - Global step 550 Train loss 3.99 ACC 0.0 on epoch=274
06/23/2022 17:30:18 - INFO - __main__ - Step 560 Global step 560 Train loss 3.72 on epoch=279
06/23/2022 17:30:19 - INFO - __main__ - Step 570 Global step 570 Train loss 3.60 on epoch=284
06/23/2022 17:30:21 - INFO - __main__ - Step 580 Global step 580 Train loss 3.58 on epoch=289
06/23/2022 17:30:22 - INFO - __main__ - Step 590 Global step 590 Train loss 3.51 on epoch=294
06/23/2022 17:30:24 - INFO - __main__ - Step 600 Global step 600 Train loss 3.50 on epoch=299
06/23/2022 17:30:36 - INFO - __main__ - Global step 600 Train loss 3.58 ACC 0.0 on epoch=299
06/23/2022 17:30:37 - INFO - __main__ - Step 610 Global step 610 Train loss 3.52 on epoch=304
06/23/2022 17:30:39 - INFO - __main__ - Step 620 Global step 620 Train loss 3.53 on epoch=309
06/23/2022 17:30:40 - INFO - __main__ - Step 630 Global step 630 Train loss 3.47 on epoch=314
06/23/2022 17:30:41 - INFO - __main__ - Step 640 Global step 640 Train loss 3.34 on epoch=319
06/23/2022 17:30:43 - INFO - __main__ - Step 650 Global step 650 Train loss 3.40 on epoch=324
06/23/2022 17:30:50 - INFO - __main__ - Global step 650 Train loss 3.45 ACC 0.0 on epoch=324
06/23/2022 17:30:51 - INFO - __main__ - Step 660 Global step 660 Train loss 3.13 on epoch=329
06/23/2022 17:30:53 - INFO - __main__ - Step 670 Global step 670 Train loss 3.15 on epoch=334
06/23/2022 17:30:54 - INFO - __main__ - Step 680 Global step 680 Train loss 3.12 on epoch=339
06/23/2022 17:30:56 - INFO - __main__ - Step 690 Global step 690 Train loss 2.95 on epoch=344
06/23/2022 17:30:57 - INFO - __main__ - Step 700 Global step 700 Train loss 2.90 on epoch=349
06/23/2022 17:31:08 - INFO - __main__ - Global step 700 Train loss 3.05 ACC 0.0 on epoch=349
06/23/2022 17:31:09 - INFO - __main__ - Step 710 Global step 710 Train loss 2.89 on epoch=354
06/23/2022 17:31:11 - INFO - __main__ - Step 720 Global step 720 Train loss 2.85 on epoch=359
06/23/2022 17:31:12 - INFO - __main__ - Step 730 Global step 730 Train loss 2.70 on epoch=364
06/23/2022 17:31:13 - INFO - __main__ - Step 740 Global step 740 Train loss 2.66 on epoch=369
06/23/2022 17:31:15 - INFO - __main__ - Step 750 Global step 750 Train loss 2.50 on epoch=374
06/23/2022 17:31:25 - INFO - __main__ - Global step 750 Train loss 2.72 ACC 0.125 on epoch=374
06/23/2022 17:31:25 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.125 on epoch=374, global_step=750
06/23/2022 17:31:27 - INFO - __main__ - Step 760 Global step 760 Train loss 2.60 on epoch=379
06/23/2022 17:31:28 - INFO - __main__ - Step 770 Global step 770 Train loss 2.59 on epoch=384
06/23/2022 17:31:30 - INFO - __main__ - Step 780 Global step 780 Train loss 2.42 on epoch=389
06/23/2022 17:31:31 - INFO - __main__ - Step 790 Global step 790 Train loss 2.29 on epoch=394
06/23/2022 17:31:33 - INFO - __main__ - Step 800 Global step 800 Train loss 2.27 on epoch=399
06/23/2022 17:31:43 - INFO - __main__ - Global step 800 Train loss 2.43 ACC 0.21875 on epoch=399
06/23/2022 17:31:43 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.21875 on epoch=399, global_step=800
06/23/2022 17:31:45 - INFO - __main__ - Step 810 Global step 810 Train loss 2.24 on epoch=404
06/23/2022 17:31:46 - INFO - __main__ - Step 820 Global step 820 Train loss 2.22 on epoch=409
06/23/2022 17:31:47 - INFO - __main__ - Step 830 Global step 830 Train loss 2.11 on epoch=414
06/23/2022 17:31:49 - INFO - __main__ - Step 840 Global step 840 Train loss 2.08 on epoch=419
06/23/2022 17:31:50 - INFO - __main__ - Step 850 Global step 850 Train loss 2.11 on epoch=424
06/23/2022 17:31:52 - INFO - __main__ - Global step 850 Train loss 2.15 ACC 0.5625 on epoch=424
06/23/2022 17:31:52 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.5625 on epoch=424, global_step=850
06/23/2022 17:31:53 - INFO - __main__ - Step 860 Global step 860 Train loss 1.97 on epoch=429
06/23/2022 17:31:54 - INFO - __main__ - Step 870 Global step 870 Train loss 1.89 on epoch=434
06/23/2022 17:31:56 - INFO - __main__ - Step 880 Global step 880 Train loss 1.80 on epoch=439
06/23/2022 17:31:57 - INFO - __main__ - Step 890 Global step 890 Train loss 1.77 on epoch=444
06/23/2022 17:31:58 - INFO - __main__ - Step 900 Global step 900 Train loss 1.78 on epoch=449
06/23/2022 17:32:00 - INFO - __main__ - Global step 900 Train loss 1.84 ACC 0.5 on epoch=449
06/23/2022 17:32:01 - INFO - __main__ - Step 910 Global step 910 Train loss 1.59 on epoch=454
06/23/2022 17:32:02 - INFO - __main__ - Step 920 Global step 920 Train loss 1.72 on epoch=459
06/23/2022 17:32:04 - INFO - __main__ - Step 930 Global step 930 Train loss 1.68 on epoch=464
06/23/2022 17:32:05 - INFO - __main__ - Step 940 Global step 940 Train loss 1.64 on epoch=469
06/23/2022 17:32:06 - INFO - __main__ - Step 950 Global step 950 Train loss 1.58 on epoch=474
06/23/2022 17:32:08 - INFO - __main__ - Global step 950 Train loss 1.64 ACC 0.5 on epoch=474
06/23/2022 17:32:09 - INFO - __main__ - Step 960 Global step 960 Train loss 1.44 on epoch=479
06/23/2022 17:32:11 - INFO - __main__ - Step 970 Global step 970 Train loss 1.46 on epoch=484
06/23/2022 17:32:13 - INFO - __main__ - Step 980 Global step 980 Train loss 1.39 on epoch=489
06/23/2022 17:32:15 - INFO - __main__ - Step 990 Global step 990 Train loss 1.26 on epoch=494
06/23/2022 17:32:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.32 on epoch=499
06/23/2022 17:32:18 - INFO - __main__ - Global step 1000 Train loss 1.37 ACC 0.5 on epoch=499
06/23/2022 17:32:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.39 on epoch=504
06/23/2022 17:32:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.32 on epoch=509
06/23/2022 17:32:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.26 on epoch=514
06/23/2022 17:32:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.22 on epoch=519
06/23/2022 17:32:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.11 on epoch=524
06/23/2022 17:32:27 - INFO - __main__ - Global step 1050 Train loss 1.26 ACC 0.5 on epoch=524
06/23/2022 17:32:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.23 on epoch=529
06/23/2022 17:32:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.11 on epoch=534
06/23/2022 17:32:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.07 on epoch=539
06/23/2022 17:32:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.12 on epoch=544
06/23/2022 17:32:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.05 on epoch=549
06/23/2022 17:32:36 - INFO - __main__ - Global step 1100 Train loss 1.12 ACC 0.46875 on epoch=549
06/23/2022 17:32:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.97 on epoch=554
06/23/2022 17:32:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.05 on epoch=559
06/23/2022 17:32:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.96 on epoch=564
06/23/2022 17:32:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.91 on epoch=569
06/23/2022 17:32:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.92 on epoch=574
06/23/2022 17:32:46 - INFO - __main__ - Global step 1150 Train loss 0.96 ACC 0.46875 on epoch=574
06/23/2022 17:32:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.82 on epoch=579
06/23/2022 17:32:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.85 on epoch=584
06/23/2022 17:32:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.81 on epoch=589
06/23/2022 17:32:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.75 on epoch=594
06/23/2022 17:32:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.77 on epoch=599
06/23/2022 17:32:56 - INFO - __main__ - Global step 1200 Train loss 0.80 ACC 0.5 on epoch=599
06/23/2022 17:32:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.78 on epoch=604
06/23/2022 17:32:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=609
06/23/2022 17:33:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.84 on epoch=614
06/23/2022 17:33:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.80 on epoch=619
06/23/2022 17:33:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.73 on epoch=624
06/23/2022 17:33:03 - INFO - __main__ - Global step 1250 Train loss 0.79 ACC 0.5 on epoch=624
06/23/2022 17:33:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.76 on epoch=629
06/23/2022 17:33:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.77 on epoch=634
06/23/2022 17:33:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.72 on epoch=639
06/23/2022 17:33:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.70 on epoch=644
06/23/2022 17:33:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.56 on epoch=649
06/23/2022 17:33:11 - INFO - __main__ - Global step 1300 Train loss 0.70 ACC 0.5 on epoch=649
06/23/2022 17:33:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.64 on epoch=654
06/23/2022 17:33:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.66 on epoch=659
06/23/2022 17:33:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.56 on epoch=664
06/23/2022 17:33:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.62 on epoch=669
06/23/2022 17:33:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.65 on epoch=674
06/23/2022 17:33:19 - INFO - __main__ - Global step 1350 Train loss 0.63 ACC 0.5 on epoch=674
06/23/2022 17:33:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.61 on epoch=679
06/23/2022 17:33:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.58 on epoch=684
06/23/2022 17:33:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.52 on epoch=689
06/23/2022 17:33:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.59 on epoch=694
06/23/2022 17:33:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.61 on epoch=699
06/23/2022 17:33:27 - INFO - __main__ - Global step 1400 Train loss 0.58 ACC 0.5 on epoch=699
06/23/2022 17:33:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.57 on epoch=704
06/23/2022 17:33:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.55 on epoch=709
06/23/2022 17:33:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.50 on epoch=714
06/23/2022 17:33:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.55 on epoch=719
06/23/2022 17:33:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.60 on epoch=724
06/23/2022 17:33:34 - INFO - __main__ - Global step 1450 Train loss 0.55 ACC 0.46875 on epoch=724
06/23/2022 17:33:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.55 on epoch=729
06/23/2022 17:33:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.51 on epoch=734
06/23/2022 17:33:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=739
06/23/2022 17:33:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.52 on epoch=744
06/23/2022 17:33:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.53 on epoch=749
06/23/2022 17:33:42 - INFO - __main__ - Global step 1500 Train loss 0.53 ACC 0.5 on epoch=749
06/23/2022 17:33:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.48 on epoch=754
06/23/2022 17:33:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=759
06/23/2022 17:33:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
06/23/2022 17:33:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.48 on epoch=769
06/23/2022 17:33:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.51 on epoch=774
06/23/2022 17:33:50 - INFO - __main__ - Global step 1550 Train loss 0.47 ACC 0.46875 on epoch=774
06/23/2022 17:33:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.50 on epoch=779
06/23/2022 17:33:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=784
06/23/2022 17:33:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=789
06/23/2022 17:33:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.49 on epoch=794
06/23/2022 17:33:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=799
06/23/2022 17:33:58 - INFO - __main__ - Global step 1600 Train loss 0.49 ACC 0.46875 on epoch=799
06/23/2022 17:33:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.47 on epoch=804
06/23/2022 17:34:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.54 on epoch=809
06/23/2022 17:34:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
06/23/2022 17:34:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.48 on epoch=819
06/23/2022 17:34:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=824
06/23/2022 17:34:05 - INFO - __main__ - Global step 1650 Train loss 0.49 ACC 0.40625 on epoch=824
06/23/2022 17:34:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=829
06/23/2022 17:34:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=834
06/23/2022 17:34:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
06/23/2022 17:34:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=844
06/23/2022 17:34:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
06/23/2022 17:34:13 - INFO - __main__ - Global step 1700 Train loss 0.40 ACC 0.5 on epoch=849
06/23/2022 17:34:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=854
06/23/2022 17:34:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=859
06/23/2022 17:34:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.48 on epoch=864
06/23/2022 17:34:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=869
06/23/2022 17:34:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
06/23/2022 17:34:20 - INFO - __main__ - Global step 1750 Train loss 0.43 ACC 0.5 on epoch=874
06/23/2022 17:34:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=879
06/23/2022 17:34:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=884
06/23/2022 17:34:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=889
06/23/2022 17:34:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=894
06/23/2022 17:34:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=899
06/23/2022 17:34:28 - INFO - __main__ - Global step 1800 Train loss 0.41 ACC 0.53125 on epoch=899
06/23/2022 17:34:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=904
06/23/2022 17:34:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=909
06/23/2022 17:34:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=914
06/23/2022 17:34:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=919
06/23/2022 17:34:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=924
06/23/2022 17:34:36 - INFO - __main__ - Global step 1850 Train loss 0.39 ACC 0.5 on epoch=924
06/23/2022 17:34:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=929
06/23/2022 17:34:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=934
06/23/2022 17:34:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=939
06/23/2022 17:34:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=944
06/23/2022 17:34:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=949
06/23/2022 17:34:43 - INFO - __main__ - Global step 1900 Train loss 0.39 ACC 0.5 on epoch=949
06/23/2022 17:34:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=954
06/23/2022 17:34:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
06/23/2022 17:34:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
06/23/2022 17:34:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=969
06/23/2022 17:34:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=974
06/23/2022 17:34:51 - INFO - __main__ - Global step 1950 Train loss 0.38 ACC 0.5 on epoch=974
06/23/2022 17:34:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=979
06/23/2022 17:34:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=984
06/23/2022 17:34:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=989
06/23/2022 17:34:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=994
06/23/2022 17:34:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.43 on epoch=999
06/23/2022 17:34:59 - INFO - __main__ - Global step 2000 Train loss 0.40 ACC 0.5 on epoch=999
06/23/2022 17:34:59 - INFO - __main__ - save last model!
06/23/2022 17:34:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 17:34:59 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 17:34:59 - INFO - __main__ - Printing 3 examples
06/23/2022 17:34:59 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 17:34:59 - INFO - __main__ - ['equivalent']
06/23/2022 17:34:59 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 17:34:59 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:34:59 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 17:34:59 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:34:59 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:34:59 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:35:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:35:00 - INFO - __main__ - Printing 3 examples
06/23/2022 17:35:00 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/23/2022 17:35:00 - INFO - __main__ - ['equivalent']
06/23/2022 17:35:00 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/23/2022 17:35:00 - INFO - __main__ - ['equivalent']
06/23/2022 17:35:00 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/23/2022 17:35:00 - INFO - __main__ - ['equivalent']
06/23/2022 17:35:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 17:35:00 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:35:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:35:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:35:00 - INFO - __main__ - Printing 3 examples
06/23/2022 17:35:00 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/23/2022 17:35:00 - INFO - __main__ - ['equivalent']
06/23/2022 17:35:00 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/23/2022 17:35:00 - INFO - __main__ - ['equivalent']
06/23/2022 17:35:00 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 17:35:00 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/23/2022 17:35:00 - INFO - __main__ - ['equivalent']
06/23/2022 17:35:00 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:35:00 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:35:00 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:35:06 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:35:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:35:06 - INFO - __main__ - Starting training!
06/23/2022 17:35:07 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_87_0.5_8_predictions.txt
06/23/2022 17:35:07 - INFO - __main__ - ACC on test data: 0.6789
06/23/2022 17:35:08 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.5, bsz=8, dev_performance=0.5625, test_performance=0.678921568627451
06/23/2022 17:35:08 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.4, bsz=8 ...
06/23/2022 17:35:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:35:08 - INFO - __main__ - Printing 3 examples
06/23/2022 17:35:08 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/23/2022 17:35:08 - INFO - __main__ - ['equivalent']
06/23/2022 17:35:08 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/23/2022 17:35:08 - INFO - __main__ - ['equivalent']
06/23/2022 17:35:08 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/23/2022 17:35:08 - INFO - __main__ - ['equivalent']
06/23/2022 17:35:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 17:35:09 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:35:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:35:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:35:09 - INFO - __main__ - Printing 3 examples
06/23/2022 17:35:09 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/23/2022 17:35:09 - INFO - __main__ - ['equivalent']
06/23/2022 17:35:09 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/23/2022 17:35:09 - INFO - __main__ - ['equivalent']
06/23/2022 17:35:09 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/23/2022 17:35:09 - INFO - __main__ - ['equivalent']
06/23/2022 17:35:09 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:35:09 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:35:09 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:35:15 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:35:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:35:15 - INFO - __main__ - Starting training!
06/23/2022 17:35:17 - INFO - __main__ - Step 10 Global step 10 Train loss 6.80 on epoch=4
06/23/2022 17:35:18 - INFO - __main__ - Step 20 Global step 20 Train loss 6.81 on epoch=9
06/23/2022 17:35:20 - INFO - __main__ - Step 30 Global step 30 Train loss 6.77 on epoch=14
06/23/2022 17:35:21 - INFO - __main__ - Step 40 Global step 40 Train loss 6.75 on epoch=19
06/23/2022 17:35:23 - INFO - __main__ - Step 50 Global step 50 Train loss 6.78 on epoch=24
06/23/2022 17:35:27 - INFO - __main__ - Global step 50 Train loss 6.78 ACC 0.0 on epoch=24
06/23/2022 17:35:27 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 17:35:29 - INFO - __main__ - Step 60 Global step 60 Train loss 6.76 on epoch=29
06/23/2022 17:35:30 - INFO - __main__ - Step 70 Global step 70 Train loss 6.76 on epoch=34
06/23/2022 17:35:31 - INFO - __main__ - Step 80 Global step 80 Train loss 6.74 on epoch=39
06/23/2022 17:35:33 - INFO - __main__ - Step 90 Global step 90 Train loss 6.73 on epoch=44
06/23/2022 17:35:34 - INFO - __main__ - Step 100 Global step 100 Train loss 6.79 on epoch=49
06/23/2022 17:35:39 - INFO - __main__ - Global step 100 Train loss 6.76 ACC 0.0 on epoch=49
06/23/2022 17:35:40 - INFO - __main__ - Step 110 Global step 110 Train loss 6.65 on epoch=54
06/23/2022 17:35:42 - INFO - __main__ - Step 120 Global step 120 Train loss 6.69 on epoch=59
06/23/2022 17:35:43 - INFO - __main__ - Step 130 Global step 130 Train loss 6.63 on epoch=64
06/23/2022 17:35:44 - INFO - __main__ - Step 140 Global step 140 Train loss 6.66 on epoch=69
06/23/2022 17:35:46 - INFO - __main__ - Step 150 Global step 150 Train loss 6.62 on epoch=74
06/23/2022 17:35:50 - INFO - __main__ - Global step 150 Train loss 6.65 ACC 0.0 on epoch=74
06/23/2022 17:35:51 - INFO - __main__ - Step 160 Global step 160 Train loss 6.59 on epoch=79
06/23/2022 17:35:53 - INFO - __main__ - Step 170 Global step 170 Train loss 6.52 on epoch=84
06/23/2022 17:35:54 - INFO - __main__ - Step 180 Global step 180 Train loss 6.51 on epoch=89
06/23/2022 17:35:56 - INFO - __main__ - Step 190 Global step 190 Train loss 6.42 on epoch=94
06/23/2022 17:35:57 - INFO - __main__ - Step 200 Global step 200 Train loss 6.45 on epoch=99
06/23/2022 17:36:04 - INFO - __main__ - Global step 200 Train loss 6.50 ACC 0.0 on epoch=99
06/23/2022 17:36:06 - INFO - __main__ - Step 210 Global step 210 Train loss 6.38 on epoch=104
06/23/2022 17:36:07 - INFO - __main__ - Step 220 Global step 220 Train loss 6.25 on epoch=109
06/23/2022 17:36:08 - INFO - __main__ - Step 230 Global step 230 Train loss 6.22 on epoch=114
06/23/2022 17:36:10 - INFO - __main__ - Step 240 Global step 240 Train loss 6.07 on epoch=119
06/23/2022 17:36:11 - INFO - __main__ - Step 250 Global step 250 Train loss 5.97 on epoch=124
06/23/2022 17:36:14 - INFO - __main__ - Global step 250 Train loss 6.18 ACC 0.0 on epoch=124
06/23/2022 17:36:15 - INFO - __main__ - Step 260 Global step 260 Train loss 6.00 on epoch=129
06/23/2022 17:36:17 - INFO - __main__ - Step 270 Global step 270 Train loss 5.84 on epoch=134
06/23/2022 17:36:18 - INFO - __main__ - Step 280 Global step 280 Train loss 5.77 on epoch=139
06/23/2022 17:36:20 - INFO - __main__ - Step 290 Global step 290 Train loss 5.63 on epoch=144
06/23/2022 17:36:21 - INFO - __main__ - Step 300 Global step 300 Train loss 5.63 on epoch=149
06/23/2022 17:36:27 - INFO - __main__ - Global step 300 Train loss 5.77 ACC 0.0 on epoch=149
06/23/2022 17:36:29 - INFO - __main__ - Step 310 Global step 310 Train loss 5.52 on epoch=154
06/23/2022 17:36:30 - INFO - __main__ - Step 320 Global step 320 Train loss 5.37 on epoch=159
06/23/2022 17:36:32 - INFO - __main__ - Step 330 Global step 330 Train loss 5.20 on epoch=164
06/23/2022 17:36:33 - INFO - __main__ - Step 340 Global step 340 Train loss 5.19 on epoch=169
06/23/2022 17:36:34 - INFO - __main__ - Step 350 Global step 350 Train loss 5.33 on epoch=174
06/23/2022 17:36:37 - INFO - __main__ - Global step 350 Train loss 5.32 ACC 0.0 on epoch=174
06/23/2022 17:36:38 - INFO - __main__ - Step 360 Global step 360 Train loss 5.08 on epoch=179
06/23/2022 17:36:39 - INFO - __main__ - Step 370 Global step 370 Train loss 5.02 on epoch=184
06/23/2022 17:36:41 - INFO - __main__ - Step 380 Global step 380 Train loss 4.92 on epoch=189
06/23/2022 17:36:42 - INFO - __main__ - Step 390 Global step 390 Train loss 4.92 on epoch=194
06/23/2022 17:36:43 - INFO - __main__ - Step 400 Global step 400 Train loss 4.86 on epoch=199
06/23/2022 17:36:44 - INFO - __main__ - Global step 400 Train loss 4.96 ACC 0.0 on epoch=199
06/23/2022 17:36:46 - INFO - __main__ - Step 410 Global step 410 Train loss 4.84 on epoch=204
06/23/2022 17:36:47 - INFO - __main__ - Step 420 Global step 420 Train loss 4.74 on epoch=209
06/23/2022 17:36:48 - INFO - __main__ - Step 430 Global step 430 Train loss 4.75 on epoch=214
06/23/2022 17:36:50 - INFO - __main__ - Step 440 Global step 440 Train loss 4.64 on epoch=219
06/23/2022 17:36:51 - INFO - __main__ - Step 450 Global step 450 Train loss 4.66 on epoch=224
06/23/2022 17:36:53 - INFO - __main__ - Global step 450 Train loss 4.72 ACC 0.0 on epoch=224
06/23/2022 17:36:54 - INFO - __main__ - Step 460 Global step 460 Train loss 4.79 on epoch=229
06/23/2022 17:36:56 - INFO - __main__ - Step 470 Global step 470 Train loss 4.66 on epoch=234
06/23/2022 17:36:57 - INFO - __main__ - Step 480 Global step 480 Train loss 4.62 on epoch=239
06/23/2022 17:36:58 - INFO - __main__ - Step 490 Global step 490 Train loss 4.56 on epoch=244
06/23/2022 17:37:00 - INFO - __main__ - Step 500 Global step 500 Train loss 4.51 on epoch=249
06/23/2022 17:37:03 - INFO - __main__ - Global step 500 Train loss 4.63 ACC 0.0 on epoch=249
06/23/2022 17:37:05 - INFO - __main__ - Step 510 Global step 510 Train loss 4.49 on epoch=254
06/23/2022 17:37:06 - INFO - __main__ - Step 520 Global step 520 Train loss 4.46 on epoch=259
06/23/2022 17:37:08 - INFO - __main__ - Step 530 Global step 530 Train loss 4.26 on epoch=264
06/23/2022 17:37:09 - INFO - __main__ - Step 540 Global step 540 Train loss 4.30 on epoch=269
06/23/2022 17:37:11 - INFO - __main__ - Step 550 Global step 550 Train loss 4.40 on epoch=274
06/23/2022 17:37:13 - INFO - __main__ - Global step 550 Train loss 4.38 ACC 0.0 on epoch=274
06/23/2022 17:37:15 - INFO - __main__ - Step 560 Global step 560 Train loss 4.36 on epoch=279
06/23/2022 17:37:16 - INFO - __main__ - Step 570 Global step 570 Train loss 4.11 on epoch=284
06/23/2022 17:37:18 - INFO - __main__ - Step 580 Global step 580 Train loss 4.23 on epoch=289
06/23/2022 17:37:19 - INFO - __main__ - Step 590 Global step 590 Train loss 4.17 on epoch=294
06/23/2022 17:37:21 - INFO - __main__ - Step 600 Global step 600 Train loss 4.02 on epoch=299
06/23/2022 17:37:28 - INFO - __main__ - Global step 600 Train loss 4.18 ACC 0.0 on epoch=299
06/23/2022 17:37:30 - INFO - __main__ - Step 610 Global step 610 Train loss 4.01 on epoch=304
06/23/2022 17:37:31 - INFO - __main__ - Step 620 Global step 620 Train loss 4.11 on epoch=309
06/23/2022 17:37:33 - INFO - __main__ - Step 630 Global step 630 Train loss 4.09 on epoch=314
06/23/2022 17:37:34 - INFO - __main__ - Step 640 Global step 640 Train loss 3.92 on epoch=319
06/23/2022 17:37:35 - INFO - __main__ - Step 650 Global step 650 Train loss 3.96 on epoch=324
06/23/2022 17:37:39 - INFO - __main__ - Global step 650 Train loss 4.02 ACC 0.0 on epoch=324
06/23/2022 17:37:41 - INFO - __main__ - Step 660 Global step 660 Train loss 3.94 on epoch=329
06/23/2022 17:37:42 - INFO - __main__ - Step 670 Global step 670 Train loss 3.83 on epoch=334
06/23/2022 17:37:44 - INFO - __main__ - Step 680 Global step 680 Train loss 3.81 on epoch=339
06/23/2022 17:37:45 - INFO - __main__ - Step 690 Global step 690 Train loss 3.88 on epoch=344
06/23/2022 17:37:46 - INFO - __main__ - Step 700 Global step 700 Train loss 3.67 on epoch=349
06/23/2022 17:37:50 - INFO - __main__ - Global step 700 Train loss 3.83 ACC 0.0 on epoch=349
06/23/2022 17:37:51 - INFO - __main__ - Step 710 Global step 710 Train loss 3.62 on epoch=354
06/23/2022 17:37:53 - INFO - __main__ - Step 720 Global step 720 Train loss 3.51 on epoch=359
06/23/2022 17:37:54 - INFO - __main__ - Step 730 Global step 730 Train loss 3.56 on epoch=364
06/23/2022 17:37:55 - INFO - __main__ - Step 740 Global step 740 Train loss 3.37 on epoch=369
06/23/2022 17:37:57 - INFO - __main__ - Step 750 Global step 750 Train loss 3.33 on epoch=374
06/23/2022 17:38:09 - INFO - __main__ - Global step 750 Train loss 3.48 ACC 0.0625 on epoch=374
06/23/2022 17:38:09 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=374, global_step=750
06/23/2022 17:38:10 - INFO - __main__ - Step 760 Global step 760 Train loss 3.30 on epoch=379
06/23/2022 17:38:12 - INFO - __main__ - Step 770 Global step 770 Train loss 3.20 on epoch=384
06/23/2022 17:38:13 - INFO - __main__ - Step 780 Global step 780 Train loss 3.12 on epoch=389
06/23/2022 17:38:15 - INFO - __main__ - Step 790 Global step 790 Train loss 3.13 on epoch=394
06/23/2022 17:38:17 - INFO - __main__ - Step 800 Global step 800 Train loss 3.10 on epoch=399
06/23/2022 17:38:24 - INFO - __main__ - Global step 800 Train loss 3.17 ACC 0.1875 on epoch=399
06/23/2022 17:38:24 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.1875 on epoch=399, global_step=800
06/23/2022 17:38:25 - INFO - __main__ - Step 810 Global step 810 Train loss 3.03 on epoch=404
06/23/2022 17:38:26 - INFO - __main__ - Step 820 Global step 820 Train loss 2.92 on epoch=409
06/23/2022 17:38:28 - INFO - __main__ - Step 830 Global step 830 Train loss 2.81 on epoch=414
06/23/2022 17:38:29 - INFO - __main__ - Step 840 Global step 840 Train loss 2.83 on epoch=419
06/23/2022 17:38:31 - INFO - __main__ - Step 850 Global step 850 Train loss 2.73 on epoch=424
06/23/2022 17:38:41 - INFO - __main__ - Global step 850 Train loss 2.86 ACC 0.40625 on epoch=424
06/23/2022 17:38:41 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.40625 on epoch=424, global_step=850
06/23/2022 17:38:42 - INFO - __main__ - Step 860 Global step 860 Train loss 2.79 on epoch=429
06/23/2022 17:38:43 - INFO - __main__ - Step 870 Global step 870 Train loss 2.63 on epoch=434
06/23/2022 17:38:45 - INFO - __main__ - Step 880 Global step 880 Train loss 2.65 on epoch=439
06/23/2022 17:38:46 - INFO - __main__ - Step 890 Global step 890 Train loss 2.60 on epoch=444
06/23/2022 17:38:48 - INFO - __main__ - Step 900 Global step 900 Train loss 2.68 on epoch=449
06/23/2022 17:39:00 - INFO - __main__ - Global step 900 Train loss 2.67 ACC 0.34375 on epoch=449
06/23/2022 17:39:02 - INFO - __main__ - Step 910 Global step 910 Train loss 2.69 on epoch=454
06/23/2022 17:39:03 - INFO - __main__ - Step 920 Global step 920 Train loss 2.57 on epoch=459
06/23/2022 17:39:05 - INFO - __main__ - Step 930 Global step 930 Train loss 2.60 on epoch=464
06/23/2022 17:39:06 - INFO - __main__ - Step 940 Global step 940 Train loss 2.57 on epoch=469
06/23/2022 17:39:07 - INFO - __main__ - Step 950 Global step 950 Train loss 2.47 on epoch=474
06/23/2022 17:39:13 - INFO - __main__ - Global step 950 Train loss 2.58 ACC 0.46875 on epoch=474
06/23/2022 17:39:13 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=474, global_step=950
06/23/2022 17:39:14 - INFO - __main__ - Step 960 Global step 960 Train loss 2.46 on epoch=479
06/23/2022 17:39:16 - INFO - __main__ - Step 970 Global step 970 Train loss 2.43 on epoch=484
06/23/2022 17:39:17 - INFO - __main__ - Step 980 Global step 980 Train loss 2.47 on epoch=489
06/23/2022 17:39:19 - INFO - __main__ - Step 990 Global step 990 Train loss 2.49 on epoch=494
06/23/2022 17:39:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.38 on epoch=499
06/23/2022 17:39:21 - INFO - __main__ - Global step 1000 Train loss 2.44 ACC 0.5 on epoch=499
06/23/2022 17:39:21 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=499, global_step=1000
06/23/2022 17:39:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.26 on epoch=504
06/23/2022 17:39:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.36 on epoch=509
06/23/2022 17:39:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.16 on epoch=514
06/23/2022 17:39:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.28 on epoch=519
06/23/2022 17:39:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.26 on epoch=524
06/23/2022 17:39:29 - INFO - __main__ - Global step 1050 Train loss 2.27 ACC 0.53125 on epoch=524
06/23/2022 17:39:29 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=524, global_step=1050
06/23/2022 17:39:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.17 on epoch=529
06/23/2022 17:39:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.13 on epoch=534
06/23/2022 17:39:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.17 on epoch=539
06/23/2022 17:39:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.10 on epoch=544
06/23/2022 17:39:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.99 on epoch=549
06/23/2022 17:39:37 - INFO - __main__ - Global step 1100 Train loss 2.11 ACC 0.5 on epoch=549
06/23/2022 17:39:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.93 on epoch=554
06/23/2022 17:39:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.80 on epoch=559
06/23/2022 17:39:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.87 on epoch=564
06/23/2022 17:39:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.95 on epoch=569
06/23/2022 17:39:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.95 on epoch=574
06/23/2022 17:39:45 - INFO - __main__ - Global step 1150 Train loss 1.90 ACC 0.46875 on epoch=574
06/23/2022 17:39:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.76 on epoch=579
06/23/2022 17:39:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.72 on epoch=584
06/23/2022 17:39:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.79 on epoch=589
06/23/2022 17:39:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.76 on epoch=594
06/23/2022 17:39:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.69 on epoch=599
06/23/2022 17:39:54 - INFO - __main__ - Global step 1200 Train loss 1.75 ACC 0.5 on epoch=599
06/23/2022 17:39:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.68 on epoch=604
06/23/2022 17:39:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.56 on epoch=609
06/23/2022 17:39:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.63 on epoch=614
06/23/2022 17:40:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.63 on epoch=619
06/23/2022 17:40:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.55 on epoch=624
06/23/2022 17:40:02 - INFO - __main__ - Global step 1250 Train loss 1.61 ACC 0.5 on epoch=624
06/23/2022 17:40:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.58 on epoch=629
06/23/2022 17:40:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.64 on epoch=634
06/23/2022 17:40:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.61 on epoch=639
06/23/2022 17:40:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.59 on epoch=644
06/23/2022 17:40:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.42 on epoch=649
06/23/2022 17:40:10 - INFO - __main__ - Global step 1300 Train loss 1.57 ACC 0.5 on epoch=649
06/23/2022 17:40:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.41 on epoch=654
06/23/2022 17:40:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.46 on epoch=659
06/23/2022 17:40:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.53 on epoch=664
06/23/2022 17:40:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.38 on epoch=669
06/23/2022 17:40:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.45 on epoch=674
06/23/2022 17:40:18 - INFO - __main__ - Global step 1350 Train loss 1.44 ACC 0.5 on epoch=674
06/23/2022 17:40:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.34 on epoch=679
06/23/2022 17:40:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.25 on epoch=684
06/23/2022 17:40:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.48 on epoch=689
06/23/2022 17:40:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.31 on epoch=694
06/23/2022 17:40:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.21 on epoch=699
06/23/2022 17:40:29 - INFO - __main__ - Global step 1400 Train loss 1.32 ACC 0.5 on epoch=699
06/23/2022 17:40:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.26 on epoch=704
06/23/2022 17:40:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.28 on epoch=709
06/23/2022 17:40:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.25 on epoch=714
06/23/2022 17:40:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.35 on epoch=719
06/23/2022 17:40:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.25 on epoch=724
06/23/2022 17:40:42 - INFO - __main__ - Global step 1450 Train loss 1.28 ACC 0.5 on epoch=724
06/23/2022 17:40:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.32 on epoch=729
06/23/2022 17:40:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.30 on epoch=734
06/23/2022 17:40:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.08 on epoch=739
06/23/2022 17:40:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.14 on epoch=744
06/23/2022 17:40:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.01 on epoch=749
06/23/2022 17:40:50 - INFO - __main__ - Global step 1500 Train loss 1.17 ACC 0.5 on epoch=749
06/23/2022 17:40:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.16 on epoch=754
06/23/2022 17:40:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.10 on epoch=759
06/23/2022 17:40:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.03 on epoch=764
06/23/2022 17:40:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.98 on epoch=769
06/23/2022 17:40:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.92 on epoch=774
06/23/2022 17:40:58 - INFO - __main__ - Global step 1550 Train loss 1.04 ACC 0.5 on epoch=774
06/23/2022 17:41:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.94 on epoch=779
06/23/2022 17:41:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.08 on epoch=784
06/23/2022 17:41:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.90 on epoch=789
06/23/2022 17:41:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.87 on epoch=794
06/23/2022 17:41:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.98 on epoch=799
06/23/2022 17:41:06 - INFO - __main__ - Global step 1600 Train loss 0.96 ACC 0.5 on epoch=799
06/23/2022 17:41:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.98 on epoch=804
06/23/2022 17:41:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.84 on epoch=809
06/23/2022 17:41:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.83 on epoch=814
06/23/2022 17:41:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.85 on epoch=819
06/23/2022 17:41:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.77 on epoch=824
06/23/2022 17:41:14 - INFO - __main__ - Global step 1650 Train loss 0.85 ACC 0.5 on epoch=824
06/23/2022 17:41:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.93 on epoch=829
06/23/2022 17:41:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.83 on epoch=834
06/23/2022 17:41:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.83 on epoch=839
06/23/2022 17:41:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.77 on epoch=844
06/23/2022 17:41:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.80 on epoch=849
06/23/2022 17:41:26 - INFO - __main__ - Global step 1700 Train loss 0.83 ACC 0.5 on epoch=849
06/23/2022 17:41:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.84 on epoch=854
06/23/2022 17:41:29 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.83 on epoch=859
06/23/2022 17:41:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.75 on epoch=864
06/23/2022 17:41:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.72 on epoch=869
06/23/2022 17:41:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.79 on epoch=874
06/23/2022 17:41:34 - INFO - __main__ - Global step 1750 Train loss 0.79 ACC 0.40625 on epoch=874
06/23/2022 17:41:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.73 on epoch=879
06/23/2022 17:41:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.76 on epoch=884
06/23/2022 17:41:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.74 on epoch=889
06/23/2022 17:41:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.75 on epoch=894
06/23/2022 17:41:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.72 on epoch=899
06/23/2022 17:41:42 - INFO - __main__ - Global step 1800 Train loss 0.74 ACC 0.5 on epoch=899
06/23/2022 17:41:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.72 on epoch=904
06/23/2022 17:41:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.69 on epoch=909
06/23/2022 17:41:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.66 on epoch=914
06/23/2022 17:41:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.75 on epoch=919
06/23/2022 17:41:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.70 on epoch=924
06/23/2022 17:41:50 - INFO - __main__ - Global step 1850 Train loss 0.71 ACC 0.5 on epoch=924
06/23/2022 17:41:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.68 on epoch=929
06/23/2022 17:41:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.76 on epoch=934
06/23/2022 17:41:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.64 on epoch=939
06/23/2022 17:41:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.58 on epoch=944
06/23/2022 17:41:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.61 on epoch=949
06/23/2022 17:41:58 - INFO - __main__ - Global step 1900 Train loss 0.66 ACC 0.5 on epoch=949
06/23/2022 17:41:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.65 on epoch=954
06/23/2022 17:42:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.68 on epoch=959
06/23/2022 17:42:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.65 on epoch=964
06/23/2022 17:42:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.64 on epoch=969
06/23/2022 17:42:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.64 on epoch=974
06/23/2022 17:42:06 - INFO - __main__ - Global step 1950 Train loss 0.65 ACC 0.5 on epoch=974
06/23/2022 17:42:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.60 on epoch=979
06/23/2022 17:42:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.64 on epoch=984
06/23/2022 17:42:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.65 on epoch=989
06/23/2022 17:42:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.57 on epoch=994
06/23/2022 17:42:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.58 on epoch=999
06/23/2022 17:42:14 - INFO - __main__ - Global step 2000 Train loss 0.61 ACC 0.5 on epoch=999
06/23/2022 17:42:14 - INFO - __main__ - save last model!
06/23/2022 17:42:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 17:42:14 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 17:42:14 - INFO - __main__ - Printing 3 examples
06/23/2022 17:42:14 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 17:42:14 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:14 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 17:42:14 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:42:14 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 17:42:14 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:42:14 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:42:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:42:14 - INFO - __main__ - Printing 3 examples
06/23/2022 17:42:14 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/23/2022 17:42:14 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:14 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/23/2022 17:42:14 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:42:14 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:14 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/23/2022 17:42:14 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 17:42:14 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:42:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:42:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:42:14 - INFO - __main__ - Printing 3 examples
06/23/2022 17:42:14 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/23/2022 17:42:14 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:14 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/23/2022 17:42:14 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:14 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/23/2022 17:42:14 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:14 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:42:14 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:42:14 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:42:15 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 17:42:20 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:42:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:42:21 - INFO - __main__ - Starting training!
06/23/2022 17:42:23 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_87_0.4_8_predictions.txt
06/23/2022 17:42:23 - INFO - __main__ - ACC on test data: 0.6838
06/23/2022 17:42:23 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.4, bsz=8, dev_performance=0.53125, test_performance=0.6838235294117647
06/23/2022 17:42:23 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.3, bsz=8 ...
06/23/2022 17:42:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:42:24 - INFO - __main__ - Printing 3 examples
06/23/2022 17:42:24 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/23/2022 17:42:24 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:24 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/23/2022 17:42:24 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:24 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/23/2022 17:42:24 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 17:42:24 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:42:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:42:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:42:24 - INFO - __main__ - Printing 3 examples
06/23/2022 17:42:24 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/23/2022 17:42:24 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:24 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/23/2022 17:42:24 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:24 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/23/2022 17:42:24 - INFO - __main__ - ['equivalent']
06/23/2022 17:42:24 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:42:24 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:42:24 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:42:31 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:42:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:42:31 - INFO - __main__ - Starting training!
06/23/2022 17:42:33 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=4
06/23/2022 17:42:34 - INFO - __main__ - Step 20 Global step 20 Train loss 6.84 on epoch=9
06/23/2022 17:42:35 - INFO - __main__ - Step 30 Global step 30 Train loss 6.86 on epoch=14
06/23/2022 17:42:37 - INFO - __main__ - Step 40 Global step 40 Train loss 6.83 on epoch=19
06/23/2022 17:42:38 - INFO - __main__ - Step 50 Global step 50 Train loss 6.84 on epoch=24
06/23/2022 17:42:40 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/23/2022 17:42:40 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 17:42:41 - INFO - __main__ - Step 60 Global step 60 Train loss 6.87 on epoch=29
06/23/2022 17:42:43 - INFO - __main__ - Step 70 Global step 70 Train loss 6.79 on epoch=34
06/23/2022 17:42:44 - INFO - __main__ - Step 80 Global step 80 Train loss 6.78 on epoch=39
06/23/2022 17:42:45 - INFO - __main__ - Step 90 Global step 90 Train loss 6.72 on epoch=44
06/23/2022 17:42:47 - INFO - __main__ - Step 100 Global step 100 Train loss 6.69 on epoch=49
06/23/2022 17:42:49 - INFO - __main__ - Global step 100 Train loss 6.77 ACC 0.0 on epoch=49
06/23/2022 17:42:51 - INFO - __main__ - Step 110 Global step 110 Train loss 6.74 on epoch=54
06/23/2022 17:42:52 - INFO - __main__ - Step 120 Global step 120 Train loss 6.70 on epoch=59
06/23/2022 17:42:54 - INFO - __main__ - Step 130 Global step 130 Train loss 6.72 on epoch=64
06/23/2022 17:42:55 - INFO - __main__ - Step 140 Global step 140 Train loss 6.75 on epoch=69
06/23/2022 17:42:56 - INFO - __main__ - Step 150 Global step 150 Train loss 6.78 on epoch=74
06/23/2022 17:42:59 - INFO - __main__ - Global step 150 Train loss 6.74 ACC 0.0 on epoch=74
06/23/2022 17:43:00 - INFO - __main__ - Step 160 Global step 160 Train loss 6.62 on epoch=79
06/23/2022 17:43:01 - INFO - __main__ - Step 170 Global step 170 Train loss 6.73 on epoch=84
06/23/2022 17:43:03 - INFO - __main__ - Step 180 Global step 180 Train loss 6.62 on epoch=89
06/23/2022 17:43:04 - INFO - __main__ - Step 190 Global step 190 Train loss 6.59 on epoch=94
06/23/2022 17:43:06 - INFO - __main__ - Step 200 Global step 200 Train loss 6.57 on epoch=99
06/23/2022 17:43:17 - INFO - __main__ - Global step 200 Train loss 6.63 ACC 0.0 on epoch=99
06/23/2022 17:43:18 - INFO - __main__ - Step 210 Global step 210 Train loss 6.54 on epoch=104
06/23/2022 17:43:20 - INFO - __main__ - Step 220 Global step 220 Train loss 6.57 on epoch=109
06/23/2022 17:43:21 - INFO - __main__ - Step 230 Global step 230 Train loss 6.48 on epoch=114
06/23/2022 17:43:22 - INFO - __main__ - Step 240 Global step 240 Train loss 6.51 on epoch=119
06/23/2022 17:43:24 - INFO - __main__ - Step 250 Global step 250 Train loss 6.36 on epoch=124
06/23/2022 17:43:35 - INFO - __main__ - Global step 250 Train loss 6.49 ACC 0.0 on epoch=124
06/23/2022 17:43:36 - INFO - __main__ - Step 260 Global step 260 Train loss 6.32 on epoch=129
06/23/2022 17:43:37 - INFO - __main__ - Step 270 Global step 270 Train loss 6.31 on epoch=134
06/23/2022 17:43:39 - INFO - __main__ - Step 280 Global step 280 Train loss 6.31 on epoch=139
06/23/2022 17:43:40 - INFO - __main__ - Step 290 Global step 290 Train loss 6.28 on epoch=144
06/23/2022 17:43:41 - INFO - __main__ - Step 300 Global step 300 Train loss 6.14 on epoch=149
06/23/2022 17:43:46 - INFO - __main__ - Global step 300 Train loss 6.27 ACC 0.0 on epoch=149
06/23/2022 17:43:47 - INFO - __main__ - Step 310 Global step 310 Train loss 6.32 on epoch=154
06/23/2022 17:43:49 - INFO - __main__ - Step 320 Global step 320 Train loss 6.18 on epoch=159
06/23/2022 17:43:50 - INFO - __main__ - Step 330 Global step 330 Train loss 6.05 on epoch=164
06/23/2022 17:43:51 - INFO - __main__ - Step 340 Global step 340 Train loss 6.05 on epoch=169
06/23/2022 17:43:53 - INFO - __main__ - Step 350 Global step 350 Train loss 6.09 on epoch=174
06/23/2022 17:44:02 - INFO - __main__ - Global step 350 Train loss 6.14 ACC 0.0 on epoch=174
06/23/2022 17:44:03 - INFO - __main__ - Step 360 Global step 360 Train loss 6.07 on epoch=179
06/23/2022 17:44:04 - INFO - __main__ - Step 370 Global step 370 Train loss 6.03 on epoch=184
06/23/2022 17:44:06 - INFO - __main__ - Step 380 Global step 380 Train loss 5.98 on epoch=189
06/23/2022 17:44:07 - INFO - __main__ - Step 390 Global step 390 Train loss 5.87 on epoch=194
06/23/2022 17:44:09 - INFO - __main__ - Step 400 Global step 400 Train loss 5.92 on epoch=199
06/23/2022 17:44:17 - INFO - __main__ - Global step 400 Train loss 5.98 ACC 0.0 on epoch=199
06/23/2022 17:44:18 - INFO - __main__ - Step 410 Global step 410 Train loss 5.84 on epoch=204
06/23/2022 17:44:20 - INFO - __main__ - Step 420 Global step 420 Train loss 5.76 on epoch=209
06/23/2022 17:44:21 - INFO - __main__ - Step 430 Global step 430 Train loss 5.90 on epoch=214
06/23/2022 17:44:22 - INFO - __main__ - Step 440 Global step 440 Train loss 5.71 on epoch=219
06/23/2022 17:44:24 - INFO - __main__ - Step 450 Global step 450 Train loss 5.71 on epoch=224
06/23/2022 17:44:32 - INFO - __main__ - Global step 450 Train loss 5.78 ACC 0.0 on epoch=224
06/23/2022 17:44:33 - INFO - __main__ - Step 460 Global step 460 Train loss 5.57 on epoch=229
06/23/2022 17:44:34 - INFO - __main__ - Step 470 Global step 470 Train loss 5.48 on epoch=234
06/23/2022 17:44:36 - INFO - __main__ - Step 480 Global step 480 Train loss 5.50 on epoch=239
06/23/2022 17:44:37 - INFO - __main__ - Step 490 Global step 490 Train loss 5.49 on epoch=244
06/23/2022 17:44:39 - INFO - __main__ - Step 500 Global step 500 Train loss 5.37 on epoch=249
06/23/2022 17:44:40 - INFO - __main__ - Global step 500 Train loss 5.48 ACC 0.0 on epoch=249
06/23/2022 17:44:41 - INFO - __main__ - Step 510 Global step 510 Train loss 5.29 on epoch=254
06/23/2022 17:44:43 - INFO - __main__ - Step 520 Global step 520 Train loss 5.34 on epoch=259
06/23/2022 17:44:44 - INFO - __main__ - Step 530 Global step 530 Train loss 5.17 on epoch=264
06/23/2022 17:44:45 - INFO - __main__ - Step 540 Global step 540 Train loss 5.14 on epoch=269
06/23/2022 17:44:47 - INFO - __main__ - Step 550 Global step 550 Train loss 5.21 on epoch=274
06/23/2022 17:44:49 - INFO - __main__ - Global step 550 Train loss 5.23 ACC 0.0 on epoch=274
06/23/2022 17:44:50 - INFO - __main__ - Step 560 Global step 560 Train loss 5.14 on epoch=279
06/23/2022 17:44:52 - INFO - __main__ - Step 570 Global step 570 Train loss 5.02 on epoch=284
06/23/2022 17:44:53 - INFO - __main__ - Step 580 Global step 580 Train loss 4.96 on epoch=289
06/23/2022 17:44:54 - INFO - __main__ - Step 590 Global step 590 Train loss 4.98 on epoch=294
06/23/2022 17:44:56 - INFO - __main__ - Step 600 Global step 600 Train loss 5.07 on epoch=299
06/23/2022 17:44:58 - INFO - __main__ - Global step 600 Train loss 5.03 ACC 0.0 on epoch=299
06/23/2022 17:45:00 - INFO - __main__ - Step 610 Global step 610 Train loss 4.87 on epoch=304
06/23/2022 17:45:01 - INFO - __main__ - Step 620 Global step 620 Train loss 4.96 on epoch=309
06/23/2022 17:45:02 - INFO - __main__ - Step 630 Global step 630 Train loss 4.83 on epoch=314
06/23/2022 17:45:04 - INFO - __main__ - Step 640 Global step 640 Train loss 4.88 on epoch=319
06/23/2022 17:45:05 - INFO - __main__ - Step 650 Global step 650 Train loss 4.77 on epoch=324
06/23/2022 17:45:08 - INFO - __main__ - Global step 650 Train loss 4.86 ACC 0.0 on epoch=324
06/23/2022 17:45:09 - INFO - __main__ - Step 660 Global step 660 Train loss 4.93 on epoch=329
06/23/2022 17:45:11 - INFO - __main__ - Step 670 Global step 670 Train loss 4.88 on epoch=334
06/23/2022 17:45:12 - INFO - __main__ - Step 680 Global step 680 Train loss 4.70 on epoch=339
06/23/2022 17:45:14 - INFO - __main__ - Step 690 Global step 690 Train loss 4.76 on epoch=344
06/23/2022 17:45:15 - INFO - __main__ - Step 700 Global step 700 Train loss 4.73 on epoch=349
06/23/2022 17:45:17 - INFO - __main__ - Global step 700 Train loss 4.80 ACC 0.0 on epoch=349
06/23/2022 17:45:19 - INFO - __main__ - Step 710 Global step 710 Train loss 4.70 on epoch=354
06/23/2022 17:45:20 - INFO - __main__ - Step 720 Global step 720 Train loss 4.49 on epoch=359
06/23/2022 17:45:21 - INFO - __main__ - Step 730 Global step 730 Train loss 4.55 on epoch=364
06/23/2022 17:45:23 - INFO - __main__ - Step 740 Global step 740 Train loss 4.47 on epoch=369
06/23/2022 17:45:24 - INFO - __main__ - Step 750 Global step 750 Train loss 4.44 on epoch=374
06/23/2022 17:45:27 - INFO - __main__ - Global step 750 Train loss 4.53 ACC 0.0 on epoch=374
06/23/2022 17:45:28 - INFO - __main__ - Step 760 Global step 760 Train loss 4.43 on epoch=379
06/23/2022 17:45:29 - INFO - __main__ - Step 770 Global step 770 Train loss 4.48 on epoch=384
06/23/2022 17:45:31 - INFO - __main__ - Step 780 Global step 780 Train loss 4.36 on epoch=389
06/23/2022 17:45:32 - INFO - __main__ - Step 790 Global step 790 Train loss 4.30 on epoch=394
06/23/2022 17:45:33 - INFO - __main__ - Step 800 Global step 800 Train loss 4.33 on epoch=399
06/23/2022 17:45:38 - INFO - __main__ - Global step 800 Train loss 4.38 ACC 0.0 on epoch=399
06/23/2022 17:45:39 - INFO - __main__ - Step 810 Global step 810 Train loss 4.38 on epoch=404
06/23/2022 17:45:40 - INFO - __main__ - Step 820 Global step 820 Train loss 4.26 on epoch=409
06/23/2022 17:45:42 - INFO - __main__ - Step 830 Global step 830 Train loss 4.23 on epoch=414
06/23/2022 17:45:43 - INFO - __main__ - Step 840 Global step 840 Train loss 4.19 on epoch=419
06/23/2022 17:45:45 - INFO - __main__ - Step 850 Global step 850 Train loss 4.15 on epoch=424
06/23/2022 17:45:49 - INFO - __main__ - Global step 850 Train loss 4.24 ACC 0.0 on epoch=424
06/23/2022 17:45:50 - INFO - __main__ - Step 860 Global step 860 Train loss 4.00 on epoch=429
06/23/2022 17:45:52 - INFO - __main__ - Step 870 Global step 870 Train loss 3.99 on epoch=434
06/23/2022 17:45:53 - INFO - __main__ - Step 880 Global step 880 Train loss 4.06 on epoch=439
06/23/2022 17:45:54 - INFO - __main__ - Step 890 Global step 890 Train loss 3.95 on epoch=444
06/23/2022 17:45:56 - INFO - __main__ - Step 900 Global step 900 Train loss 3.89 on epoch=449
06/23/2022 17:46:08 - INFO - __main__ - Global step 900 Train loss 3.98 ACC 0.0 on epoch=449
06/23/2022 17:46:09 - INFO - __main__ - Step 910 Global step 910 Train loss 3.86 on epoch=454
06/23/2022 17:46:11 - INFO - __main__ - Step 920 Global step 920 Train loss 3.78 on epoch=459
06/23/2022 17:46:12 - INFO - __main__ - Step 930 Global step 930 Train loss 3.78 on epoch=464
06/23/2022 17:46:13 - INFO - __main__ - Step 940 Global step 940 Train loss 3.81 on epoch=469
06/23/2022 17:46:15 - INFO - __main__ - Step 950 Global step 950 Train loss 3.81 on epoch=474
06/23/2022 17:46:17 - INFO - __main__ - Global step 950 Train loss 3.81 ACC 0.0 on epoch=474
06/23/2022 17:46:19 - INFO - __main__ - Step 960 Global step 960 Train loss 3.59 on epoch=479
06/23/2022 17:46:20 - INFO - __main__ - Step 970 Global step 970 Train loss 3.70 on epoch=484
06/23/2022 17:46:21 - INFO - __main__ - Step 980 Global step 980 Train loss 3.60 on epoch=489
06/23/2022 17:46:23 - INFO - __main__ - Step 990 Global step 990 Train loss 3.58 on epoch=494
06/23/2022 17:46:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.51 on epoch=499
06/23/2022 17:46:27 - INFO - __main__ - Global step 1000 Train loss 3.60 ACC 0.03125 on epoch=499
06/23/2022 17:46:27 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=499, global_step=1000
06/23/2022 17:46:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.40 on epoch=504
06/23/2022 17:46:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.43 on epoch=509
06/23/2022 17:46:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.39 on epoch=514
06/23/2022 17:46:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.27 on epoch=519
06/23/2022 17:46:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.28 on epoch=524
06/23/2022 17:46:42 - INFO - __main__ - Global step 1050 Train loss 3.35 ACC 0.25 on epoch=524
06/23/2022 17:46:42 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.25 on epoch=524, global_step=1050
06/23/2022 17:46:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.28 on epoch=529
06/23/2022 17:46:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.18 on epoch=534
06/23/2022 17:46:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.04 on epoch=539
06/23/2022 17:46:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.16 on epoch=544
06/23/2022 17:46:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.05 on epoch=549
06/23/2022 17:46:56 - INFO - __main__ - Global step 1100 Train loss 3.14 ACC 0.375 on epoch=549
06/23/2022 17:46:56 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.375 on epoch=549, global_step=1100
06/23/2022 17:46:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.09 on epoch=554
06/23/2022 17:46:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.07 on epoch=559
06/23/2022 17:47:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.91 on epoch=564
06/23/2022 17:47:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 2.92 on epoch=569
06/23/2022 17:47:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 2.82 on epoch=574
06/23/2022 17:47:15 - INFO - __main__ - Global step 1150 Train loss 2.96 ACC 0.40625 on epoch=574
06/23/2022 17:47:15 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.40625 on epoch=574, global_step=1150
06/23/2022 17:47:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.74 on epoch=579
06/23/2022 17:47:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.81 on epoch=584
06/23/2022 17:47:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 2.68 on epoch=589
06/23/2022 17:47:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.72 on epoch=594
06/23/2022 17:47:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.62 on epoch=599
06/23/2022 17:47:24 - INFO - __main__ - Global step 1200 Train loss 2.72 ACC 0.46875 on epoch=599
06/23/2022 17:47:24 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=599, global_step=1200
06/23/2022 17:47:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.57 on epoch=604
06/23/2022 17:47:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.53 on epoch=609
06/23/2022 17:47:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.58 on epoch=614
06/23/2022 17:47:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.45 on epoch=619
06/23/2022 17:47:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.43 on epoch=624
06/23/2022 17:47:44 - INFO - __main__ - Global step 1250 Train loss 2.51 ACC 0.40625 on epoch=624
06/23/2022 17:47:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.50 on epoch=629
06/23/2022 17:47:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.40 on epoch=634
06/23/2022 17:47:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.31 on epoch=639
06/23/2022 17:47:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.09 on epoch=644
06/23/2022 17:47:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.18 on epoch=649
06/23/2022 17:47:52 - INFO - __main__ - Global step 1300 Train loss 2.30 ACC 0.4375 on epoch=649
06/23/2022 17:47:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.08 on epoch=654
06/23/2022 17:47:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.07 on epoch=659
06/23/2022 17:47:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.03 on epoch=664
06/23/2022 17:47:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.08 on epoch=669
06/23/2022 17:47:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.01 on epoch=674
06/23/2022 17:48:02 - INFO - __main__ - Global step 1350 Train loss 2.05 ACC 0.4375 on epoch=674
06/23/2022 17:48:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.88 on epoch=679
06/23/2022 17:48:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.90 on epoch=684
06/23/2022 17:48:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.86 on epoch=689
06/23/2022 17:48:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.86 on epoch=694
06/23/2022 17:48:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.64 on epoch=699
06/23/2022 17:48:10 - INFO - __main__ - Global step 1400 Train loss 1.83 ACC 0.53125 on epoch=699
06/23/2022 17:48:10 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.53125 on epoch=699, global_step=1400
06/23/2022 17:48:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.72 on epoch=704
06/23/2022 17:48:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.74 on epoch=709
06/23/2022 17:48:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.76 on epoch=714
06/23/2022 17:48:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.86 on epoch=719
06/23/2022 17:48:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.74 on epoch=724
06/23/2022 17:48:18 - INFO - __main__ - Global step 1450 Train loss 1.76 ACC 0.53125 on epoch=724
06/23/2022 17:48:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.67 on epoch=729
06/23/2022 17:48:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.54 on epoch=734
06/23/2022 17:48:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.58 on epoch=739
06/23/2022 17:48:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.61 on epoch=744
06/23/2022 17:48:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.66 on epoch=749
06/23/2022 17:48:27 - INFO - __main__ - Global step 1500 Train loss 1.61 ACC 0.5 on epoch=749
06/23/2022 17:48:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.51 on epoch=754
06/23/2022 17:48:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.47 on epoch=759
06/23/2022 17:48:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.46 on epoch=764
06/23/2022 17:48:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.37 on epoch=769
06/23/2022 17:48:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.39 on epoch=774
06/23/2022 17:48:35 - INFO - __main__ - Global step 1550 Train loss 1.44 ACC 0.5 on epoch=774
06/23/2022 17:48:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.41 on epoch=779
06/23/2022 17:48:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.37 on epoch=784
06/23/2022 17:48:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.44 on epoch=789
06/23/2022 17:48:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.41 on epoch=794
06/23/2022 17:48:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.36 on epoch=799
06/23/2022 17:48:44 - INFO - __main__ - Global step 1600 Train loss 1.40 ACC 0.5 on epoch=799
06/23/2022 17:48:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.30 on epoch=804
06/23/2022 17:48:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.31 on epoch=809
06/23/2022 17:48:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.28 on epoch=814
06/23/2022 17:48:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.32 on epoch=819
06/23/2022 17:48:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.30 on epoch=824
06/23/2022 17:48:53 - INFO - __main__ - Global step 1650 Train loss 1.30 ACC 0.5 on epoch=824
06/23/2022 17:48:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.23 on epoch=829
06/23/2022 17:48:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.23 on epoch=834
06/23/2022 17:48:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.34 on epoch=839
06/23/2022 17:48:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.10 on epoch=844
06/23/2022 17:49:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.16 on epoch=849
06/23/2022 17:49:02 - INFO - __main__ - Global step 1700 Train loss 1.21 ACC 0.5 on epoch=849
06/23/2022 17:49:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.14 on epoch=854
06/23/2022 17:49:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.21 on epoch=859
06/23/2022 17:49:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.06 on epoch=864
06/23/2022 17:49:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.97 on epoch=869
06/23/2022 17:49:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.04 on epoch=874
06/23/2022 17:49:12 - INFO - __main__ - Global step 1750 Train loss 1.08 ACC 0.5 on epoch=874
06/23/2022 17:49:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.04 on epoch=879
06/23/2022 17:49:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.02 on epoch=884
06/23/2022 17:49:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.04 on epoch=889
06/23/2022 17:49:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.03 on epoch=894
06/23/2022 17:49:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.96 on epoch=899
06/23/2022 17:49:21 - INFO - __main__ - Global step 1800 Train loss 1.02 ACC 0.5 on epoch=899
06/23/2022 17:49:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.96 on epoch=904
06/23/2022 17:49:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.95 on epoch=909
06/23/2022 17:49:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.83 on epoch=914
06/23/2022 17:49:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.91 on epoch=919
06/23/2022 17:49:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.90 on epoch=924
06/23/2022 17:49:37 - INFO - __main__ - Global step 1850 Train loss 0.91 ACC 0.5 on epoch=924
06/23/2022 17:49:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.89 on epoch=929
06/23/2022 17:49:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.88 on epoch=934
06/23/2022 17:49:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.96 on epoch=939
06/23/2022 17:49:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.90 on epoch=944
06/23/2022 17:49:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.90 on epoch=949
06/23/2022 17:49:47 - INFO - __main__ - Global step 1900 Train loss 0.90 ACC 0.5 on epoch=949
06/23/2022 17:49:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.95 on epoch=954
06/23/2022 17:49:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.89 on epoch=959
06/23/2022 17:49:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.79 on epoch=964
06/23/2022 17:49:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.88 on epoch=969
06/23/2022 17:49:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.74 on epoch=974
06/23/2022 17:49:57 - INFO - __main__ - Global step 1950 Train loss 0.85 ACC 0.5 on epoch=974
06/23/2022 17:49:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.87 on epoch=979
06/23/2022 17:50:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.83 on epoch=984
06/23/2022 17:50:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.80 on epoch=989
06/23/2022 17:50:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.78 on epoch=994
06/23/2022 17:50:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.81 on epoch=999
06/23/2022 17:50:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:50:05 - INFO - __main__ - Printing 3 examples
06/23/2022 17:50:05 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/23/2022 17:50:05 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:05 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/23/2022 17:50:05 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:05 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/23/2022 17:50:05 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 17:50:05 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:50:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:50:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:50:05 - INFO - __main__ - Printing 3 examples
06/23/2022 17:50:05 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/23/2022 17:50:05 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:05 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/23/2022 17:50:05 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:05 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/23/2022 17:50:05 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:05 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:50:05 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:50:05 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:50:06 - INFO - __main__ - Global step 2000 Train loss 0.82 ACC 0.5 on epoch=999
06/23/2022 17:50:06 - INFO - __main__ - save last model!
06/23/2022 17:50:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 17:50:06 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 17:50:06 - INFO - __main__ - Printing 3 examples
06/23/2022 17:50:06 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 17:50:06 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 17:50:06 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:50:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 17:50:06 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:50:06 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:50:06 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:50:07 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 17:50:12 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:50:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:50:12 - INFO - __main__ - Starting training!
06/23/2022 17:50:30 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_87_0.3_8_predictions.txt
06/23/2022 17:50:30 - INFO - __main__ - ACC on test data: 0.6814
06/23/2022 17:50:30 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.3, bsz=8, dev_performance=0.53125, test_performance=0.6813725490196079
06/23/2022 17:50:30 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.2, bsz=8 ...
06/23/2022 17:50:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:50:31 - INFO - __main__ - Printing 3 examples
06/23/2022 17:50:31 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/23/2022 17:50:31 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:31 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/23/2022 17:50:31 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:31 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/23/2022 17:50:31 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 17:50:31 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:50:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:50:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:50:31 - INFO - __main__ - Printing 3 examples
06/23/2022 17:50:31 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/23/2022 17:50:31 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:31 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/23/2022 17:50:31 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:31 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/23/2022 17:50:32 - INFO - __main__ - ['equivalent']
06/23/2022 17:50:32 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:50:32 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:50:32 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:50:38 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:50:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:50:38 - INFO - __main__ - Starting training!
06/23/2022 17:50:40 - INFO - __main__ - Step 10 Global step 10 Train loss 6.92 on epoch=4
06/23/2022 17:50:42 - INFO - __main__ - Step 20 Global step 20 Train loss 6.87 on epoch=9
06/23/2022 17:50:43 - INFO - __main__ - Step 30 Global step 30 Train loss 6.78 on epoch=14
06/23/2022 17:50:45 - INFO - __main__ - Step 40 Global step 40 Train loss 6.80 on epoch=19
06/23/2022 17:50:46 - INFO - __main__ - Step 50 Global step 50 Train loss 6.80 on epoch=24
06/23/2022 17:50:48 - INFO - __main__ - Global step 50 Train loss 6.83 ACC 0.0 on epoch=24
06/23/2022 17:50:48 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 17:50:49 - INFO - __main__ - Step 60 Global step 60 Train loss 6.77 on epoch=29
06/23/2022 17:50:51 - INFO - __main__ - Step 70 Global step 70 Train loss 6.76 on epoch=34
06/23/2022 17:50:52 - INFO - __main__ - Step 80 Global step 80 Train loss 6.78 on epoch=39
06/23/2022 17:50:53 - INFO - __main__ - Step 90 Global step 90 Train loss 6.74 on epoch=44
06/23/2022 17:50:55 - INFO - __main__ - Step 100 Global step 100 Train loss 6.76 on epoch=49
06/23/2022 17:50:58 - INFO - __main__ - Global step 100 Train loss 6.76 ACC 0.0 on epoch=49
06/23/2022 17:50:59 - INFO - __main__ - Step 110 Global step 110 Train loss 6.75 on epoch=54
06/23/2022 17:51:01 - INFO - __main__ - Step 120 Global step 120 Train loss 6.76 on epoch=59
06/23/2022 17:51:02 - INFO - __main__ - Step 130 Global step 130 Train loss 6.66 on epoch=64
06/23/2022 17:51:03 - INFO - __main__ - Step 140 Global step 140 Train loss 6.65 on epoch=69
06/23/2022 17:51:05 - INFO - __main__ - Step 150 Global step 150 Train loss 6.69 on epoch=74
06/23/2022 17:51:06 - INFO - __main__ - Global step 150 Train loss 6.70 ACC 0.0 on epoch=74
06/23/2022 17:51:07 - INFO - __main__ - Step 160 Global step 160 Train loss 6.69 on epoch=79
06/23/2022 17:51:09 - INFO - __main__ - Step 170 Global step 170 Train loss 6.65 on epoch=84
06/23/2022 17:51:10 - INFO - __main__ - Step 180 Global step 180 Train loss 6.57 on epoch=89
06/23/2022 17:51:12 - INFO - __main__ - Step 190 Global step 190 Train loss 6.62 on epoch=94
06/23/2022 17:51:13 - INFO - __main__ - Step 200 Global step 200 Train loss 6.67 on epoch=99
06/23/2022 17:51:17 - INFO - __main__ - Global step 200 Train loss 6.64 ACC 0.0 on epoch=99
06/23/2022 17:51:18 - INFO - __main__ - Step 210 Global step 210 Train loss 6.65 on epoch=104
06/23/2022 17:51:20 - INFO - __main__ - Step 220 Global step 220 Train loss 6.56 on epoch=109
06/23/2022 17:51:21 - INFO - __main__ - Step 230 Global step 230 Train loss 6.56 on epoch=114
06/23/2022 17:51:23 - INFO - __main__ - Step 240 Global step 240 Train loss 6.53 on epoch=119
06/23/2022 17:51:24 - INFO - __main__ - Step 250 Global step 250 Train loss 6.63 on epoch=124
06/23/2022 17:51:27 - INFO - __main__ - Global step 250 Train loss 6.59 ACC 0.0 on epoch=124
06/23/2022 17:51:28 - INFO - __main__ - Step 260 Global step 260 Train loss 6.46 on epoch=129
06/23/2022 17:51:30 - INFO - __main__ - Step 270 Global step 270 Train loss 6.47 on epoch=134
06/23/2022 17:51:31 - INFO - __main__ - Step 280 Global step 280 Train loss 6.51 on epoch=139
06/23/2022 17:51:33 - INFO - __main__ - Step 290 Global step 290 Train loss 6.45 on epoch=144
06/23/2022 17:51:34 - INFO - __main__ - Step 300 Global step 300 Train loss 6.40 on epoch=149
06/23/2022 17:51:39 - INFO - __main__ - Global step 300 Train loss 6.46 ACC 0.0 on epoch=149
06/23/2022 17:51:41 - INFO - __main__ - Step 310 Global step 310 Train loss 6.41 on epoch=154
06/23/2022 17:51:42 - INFO - __main__ - Step 320 Global step 320 Train loss 6.30 on epoch=159
06/23/2022 17:51:43 - INFO - __main__ - Step 330 Global step 330 Train loss 6.25 on epoch=164
06/23/2022 17:51:45 - INFO - __main__ - Step 340 Global step 340 Train loss 6.24 on epoch=169
06/23/2022 17:51:46 - INFO - __main__ - Step 350 Global step 350 Train loss 6.22 on epoch=174
06/23/2022 17:51:51 - INFO - __main__ - Global step 350 Train loss 6.29 ACC 0.0 on epoch=174
06/23/2022 17:51:52 - INFO - __main__ - Step 360 Global step 360 Train loss 6.11 on epoch=179
06/23/2022 17:51:54 - INFO - __main__ - Step 370 Global step 370 Train loss 6.22 on epoch=184
06/23/2022 17:51:55 - INFO - __main__ - Step 380 Global step 380 Train loss 6.19 on epoch=189
06/23/2022 17:51:57 - INFO - __main__ - Step 390 Global step 390 Train loss 6.16 on epoch=194
06/23/2022 17:51:58 - INFO - __main__ - Step 400 Global step 400 Train loss 6.09 on epoch=199
06/23/2022 17:52:07 - INFO - __main__ - Global step 400 Train loss 6.15 ACC 0.0 on epoch=199
06/23/2022 17:52:09 - INFO - __main__ - Step 410 Global step 410 Train loss 6.05 on epoch=204
06/23/2022 17:52:10 - INFO - __main__ - Step 420 Global step 420 Train loss 6.03 on epoch=209
06/23/2022 17:52:12 - INFO - __main__ - Step 430 Global step 430 Train loss 6.00 on epoch=214
06/23/2022 17:52:13 - INFO - __main__ - Step 440 Global step 440 Train loss 5.87 on epoch=219
06/23/2022 17:52:14 - INFO - __main__ - Step 450 Global step 450 Train loss 5.92 on epoch=224
06/23/2022 17:52:19 - INFO - __main__ - Global step 450 Train loss 5.98 ACC 0.0 on epoch=224
06/23/2022 17:52:20 - INFO - __main__ - Step 460 Global step 460 Train loss 5.87 on epoch=229
06/23/2022 17:52:21 - INFO - __main__ - Step 470 Global step 470 Train loss 5.74 on epoch=234
06/23/2022 17:52:23 - INFO - __main__ - Step 480 Global step 480 Train loss 5.65 on epoch=239
06/23/2022 17:52:24 - INFO - __main__ - Step 490 Global step 490 Train loss 5.74 on epoch=244
06/23/2022 17:52:26 - INFO - __main__ - Step 500 Global step 500 Train loss 5.51 on epoch=249
06/23/2022 17:52:28 - INFO - __main__ - Global step 500 Train loss 5.70 ACC 0.0 on epoch=249
06/23/2022 17:52:30 - INFO - __main__ - Step 510 Global step 510 Train loss 5.64 on epoch=254
06/23/2022 17:52:31 - INFO - __main__ - Step 520 Global step 520 Train loss 5.38 on epoch=259
06/23/2022 17:52:33 - INFO - __main__ - Step 530 Global step 530 Train loss 5.40 on epoch=264
06/23/2022 17:52:34 - INFO - __main__ - Step 540 Global step 540 Train loss 5.26 on epoch=269
06/23/2022 17:52:35 - INFO - __main__ - Step 550 Global step 550 Train loss 5.33 on epoch=274
06/23/2022 17:52:37 - INFO - __main__ - Global step 550 Train loss 5.40 ACC 0.0 on epoch=274
06/23/2022 17:52:38 - INFO - __main__ - Step 560 Global step 560 Train loss 5.18 on epoch=279
06/23/2022 17:52:40 - INFO - __main__ - Step 570 Global step 570 Train loss 5.26 on epoch=284
06/23/2022 17:52:41 - INFO - __main__ - Step 580 Global step 580 Train loss 5.15 on epoch=289
06/23/2022 17:52:42 - INFO - __main__ - Step 590 Global step 590 Train loss 5.09 on epoch=294
06/23/2022 17:52:44 - INFO - __main__ - Step 600 Global step 600 Train loss 5.00 on epoch=299
06/23/2022 17:52:45 - INFO - __main__ - Global step 600 Train loss 5.14 ACC 0.0 on epoch=299
06/23/2022 17:52:47 - INFO - __main__ - Step 610 Global step 610 Train loss 4.88 on epoch=304
06/23/2022 17:52:48 - INFO - __main__ - Step 620 Global step 620 Train loss 4.94 on epoch=309
06/23/2022 17:52:50 - INFO - __main__ - Step 630 Global step 630 Train loss 4.87 on epoch=314
06/23/2022 17:52:51 - INFO - __main__ - Step 640 Global step 640 Train loss 4.80 on epoch=319
06/23/2022 17:52:52 - INFO - __main__ - Step 650 Global step 650 Train loss 4.80 on epoch=324
06/23/2022 17:52:56 - INFO - __main__ - Global step 650 Train loss 4.86 ACC 0.0 on epoch=324
06/23/2022 17:52:57 - INFO - __main__ - Step 660 Global step 660 Train loss 4.74 on epoch=329
06/23/2022 17:52:58 - INFO - __main__ - Step 670 Global step 670 Train loss 4.68 on epoch=334
06/23/2022 17:53:00 - INFO - __main__ - Step 680 Global step 680 Train loss 4.67 on epoch=339
06/23/2022 17:53:01 - INFO - __main__ - Step 690 Global step 690 Train loss 4.57 on epoch=344
06/23/2022 17:53:03 - INFO - __main__ - Step 700 Global step 700 Train loss 4.58 on epoch=349
06/23/2022 17:53:05 - INFO - __main__ - Global step 700 Train loss 4.65 ACC 0.0 on epoch=349
06/23/2022 17:53:06 - INFO - __main__ - Step 710 Global step 710 Train loss 4.64 on epoch=354
06/23/2022 17:53:08 - INFO - __main__ - Step 720 Global step 720 Train loss 4.50 on epoch=359
06/23/2022 17:53:09 - INFO - __main__ - Step 730 Global step 730 Train loss 4.47 on epoch=364
06/23/2022 17:53:10 - INFO - __main__ - Step 740 Global step 740 Train loss 4.38 on epoch=369
06/23/2022 17:53:12 - INFO - __main__ - Step 750 Global step 750 Train loss 4.29 on epoch=374
06/23/2022 17:53:13 - INFO - __main__ - Global step 750 Train loss 4.46 ACC 0.0 on epoch=374
06/23/2022 17:53:14 - INFO - __main__ - Step 760 Global step 760 Train loss 4.21 on epoch=379
06/23/2022 17:53:16 - INFO - __main__ - Step 770 Global step 770 Train loss 4.38 on epoch=384
06/23/2022 17:53:17 - INFO - __main__ - Step 780 Global step 780 Train loss 4.23 on epoch=389
06/23/2022 17:53:19 - INFO - __main__ - Step 790 Global step 790 Train loss 4.45 on epoch=394
06/23/2022 17:53:20 - INFO - __main__ - Step 800 Global step 800 Train loss 4.27 on epoch=399
06/23/2022 17:53:23 - INFO - __main__ - Global step 800 Train loss 4.31 ACC 0.0 on epoch=399
06/23/2022 17:53:25 - INFO - __main__ - Step 810 Global step 810 Train loss 4.21 on epoch=404
06/23/2022 17:53:26 - INFO - __main__ - Step 820 Global step 820 Train loss 4.16 on epoch=409
06/23/2022 17:53:27 - INFO - __main__ - Step 830 Global step 830 Train loss 4.20 on epoch=414
06/23/2022 17:53:29 - INFO - __main__ - Step 840 Global step 840 Train loss 4.21 on epoch=419
06/23/2022 17:53:30 - INFO - __main__ - Step 850 Global step 850 Train loss 4.04 on epoch=424
06/23/2022 17:53:32 - INFO - __main__ - Global step 850 Train loss 4.16 ACC 0.0 on epoch=424
06/23/2022 17:53:33 - INFO - __main__ - Step 860 Global step 860 Train loss 4.02 on epoch=429
06/23/2022 17:53:34 - INFO - __main__ - Step 870 Global step 870 Train loss 4.05 on epoch=434
06/23/2022 17:53:36 - INFO - __main__ - Step 880 Global step 880 Train loss 3.91 on epoch=439
06/23/2022 17:53:37 - INFO - __main__ - Step 890 Global step 890 Train loss 4.01 on epoch=444
06/23/2022 17:53:39 - INFO - __main__ - Step 900 Global step 900 Train loss 3.99 on epoch=449
06/23/2022 17:53:40 - INFO - __main__ - Global step 900 Train loss 3.99 ACC 0.0 on epoch=449
06/23/2022 17:53:41 - INFO - __main__ - Step 910 Global step 910 Train loss 3.88 on epoch=454
06/23/2022 17:53:43 - INFO - __main__ - Step 920 Global step 920 Train loss 3.90 on epoch=459
06/23/2022 17:53:44 - INFO - __main__ - Step 930 Global step 930 Train loss 4.01 on epoch=464
06/23/2022 17:53:46 - INFO - __main__ - Step 940 Global step 940 Train loss 3.75 on epoch=469
06/23/2022 17:53:47 - INFO - __main__ - Step 950 Global step 950 Train loss 3.82 on epoch=474
06/23/2022 17:53:48 - INFO - __main__ - Global step 950 Train loss 3.87 ACC 0.0 on epoch=474
06/23/2022 17:53:50 - INFO - __main__ - Step 960 Global step 960 Train loss 3.63 on epoch=479
06/23/2022 17:53:51 - INFO - __main__ - Step 970 Global step 970 Train loss 3.82 on epoch=484
06/23/2022 17:53:53 - INFO - __main__ - Step 980 Global step 980 Train loss 3.69 on epoch=489
06/23/2022 17:53:54 - INFO - __main__ - Step 990 Global step 990 Train loss 3.73 on epoch=494
06/23/2022 17:53:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.62 on epoch=499
06/23/2022 17:53:57 - INFO - __main__ - Global step 1000 Train loss 3.70 ACC 0.0 on epoch=499
06/23/2022 17:53:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.62 on epoch=504
06/23/2022 17:53:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.54 on epoch=509
06/23/2022 17:54:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.46 on epoch=514
06/23/2022 17:54:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.44 on epoch=519
06/23/2022 17:54:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.36 on epoch=524
06/23/2022 17:54:08 - INFO - __main__ - Global step 1050 Train loss 3.48 ACC 0.0 on epoch=524
06/23/2022 17:54:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.35 on epoch=529
06/23/2022 17:54:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.45 on epoch=534
06/23/2022 17:54:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.35 on epoch=539
06/23/2022 17:54:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.29 on epoch=544
06/23/2022 17:54:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.37 on epoch=549
06/23/2022 17:54:17 - INFO - __main__ - Global step 1100 Train loss 3.36 ACC 0.0 on epoch=549
06/23/2022 17:54:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.23 on epoch=554
06/23/2022 17:54:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.16 on epoch=559
06/23/2022 17:54:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.01 on epoch=564
06/23/2022 17:54:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.16 on epoch=569
06/23/2022 17:54:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.12 on epoch=574
06/23/2022 17:54:27 - INFO - __main__ - Global step 1150 Train loss 3.14 ACC 0.0 on epoch=574
06/23/2022 17:54:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.97 on epoch=579
06/23/2022 17:54:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.94 on epoch=584
06/23/2022 17:54:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 2.99 on epoch=589
06/23/2022 17:54:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.93 on epoch=594
06/23/2022 17:54:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.93 on epoch=599
06/23/2022 17:54:35 - INFO - __main__ - Global step 1200 Train loss 2.95 ACC 0.0 on epoch=599
06/23/2022 17:54:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.86 on epoch=604
06/23/2022 17:54:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.78 on epoch=609
06/23/2022 17:54:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.82 on epoch=614
06/23/2022 17:54:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.68 on epoch=619
06/23/2022 17:54:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.66 on epoch=624
06/23/2022 17:54:43 - INFO - __main__ - Global step 1250 Train loss 2.76 ACC 0.0 on epoch=624
06/23/2022 17:54:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.64 on epoch=629
06/23/2022 17:54:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.60 on epoch=634
06/23/2022 17:54:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.68 on epoch=639
06/23/2022 17:54:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.50 on epoch=644
06/23/2022 17:54:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.52 on epoch=649
06/23/2022 17:54:50 - INFO - __main__ - Global step 1300 Train loss 2.59 ACC 0.1875 on epoch=649
06/23/2022 17:54:50 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.1875 on epoch=649, global_step=1300
06/23/2022 17:54:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.43 on epoch=654
06/23/2022 17:54:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.41 on epoch=659
06/23/2022 17:54:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.37 on epoch=664
06/23/2022 17:54:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.34 on epoch=669
06/23/2022 17:54:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.37 on epoch=674
06/23/2022 17:54:59 - INFO - __main__ - Global step 1350 Train loss 2.39 ACC 0.25 on epoch=674
06/23/2022 17:54:59 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.25 on epoch=674, global_step=1350
06/23/2022 17:55:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.29 on epoch=679
06/23/2022 17:55:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.27 on epoch=684
06/23/2022 17:55:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.20 on epoch=689
06/23/2022 17:55:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.19 on epoch=694
06/23/2022 17:55:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.12 on epoch=699
06/23/2022 17:55:07 - INFO - __main__ - Global step 1400 Train loss 2.21 ACC 0.46875 on epoch=699
06/23/2022 17:55:07 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.46875 on epoch=699, global_step=1400
06/23/2022 17:55:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.01 on epoch=704
06/23/2022 17:55:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.14 on epoch=709
06/23/2022 17:55:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.10 on epoch=714
06/23/2022 17:55:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.00 on epoch=719
06/23/2022 17:55:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.00 on epoch=724
06/23/2022 17:55:16 - INFO - __main__ - Global step 1450 Train loss 2.05 ACC 0.5 on epoch=724
06/23/2022 17:55:17 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=724, global_step=1450
06/23/2022 17:55:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.98 on epoch=729
06/23/2022 17:55:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.89 on epoch=734
06/23/2022 17:55:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.78 on epoch=739
06/23/2022 17:55:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.72 on epoch=744
06/23/2022 17:55:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.70 on epoch=749
06/23/2022 17:55:29 - INFO - __main__ - Global step 1500 Train loss 1.82 ACC 0.5 on epoch=749
06/23/2022 17:55:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.76 on epoch=754
06/23/2022 17:55:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.72 on epoch=759
06/23/2022 17:55:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.66 on epoch=764
06/23/2022 17:55:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.61 on epoch=769
06/23/2022 17:55:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.66 on epoch=774
06/23/2022 17:55:39 - INFO - __main__ - Global step 1550 Train loss 1.68 ACC 0.5 on epoch=774
06/23/2022 17:55:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.64 on epoch=779
06/23/2022 17:55:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.58 on epoch=784
06/23/2022 17:55:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.46 on epoch=789
06/23/2022 17:55:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.54 on epoch=794
06/23/2022 17:55:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.49 on epoch=799
06/23/2022 17:55:47 - INFO - __main__ - Global step 1600 Train loss 1.54 ACC 0.375 on epoch=799
06/23/2022 17:55:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.48 on epoch=804
06/23/2022 17:55:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.54 on epoch=809
06/23/2022 17:55:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.53 on epoch=814
06/23/2022 17:55:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.46 on epoch=819
06/23/2022 17:55:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.44 on epoch=824
06/23/2022 17:55:56 - INFO - __main__ - Global step 1650 Train loss 1.49 ACC 0.46875 on epoch=824
06/23/2022 17:55:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.34 on epoch=829
06/23/2022 17:55:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.37 on epoch=834
06/23/2022 17:56:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.37 on epoch=839
06/23/2022 17:56:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.36 on epoch=844
06/23/2022 17:56:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.40 on epoch=849
06/23/2022 17:56:05 - INFO - __main__ - Global step 1700 Train loss 1.37 ACC 0.46875 on epoch=849
06/23/2022 17:56:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.25 on epoch=854
06/23/2022 17:56:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.34 on epoch=859
06/23/2022 17:56:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.23 on epoch=864
06/23/2022 17:56:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.26 on epoch=869
06/23/2022 17:56:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.20 on epoch=874
06/23/2022 17:56:15 - INFO - __main__ - Global step 1750 Train loss 1.26 ACC 0.5 on epoch=874
06/23/2022 17:56:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.11 on epoch=879
06/23/2022 17:56:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.09 on epoch=884
06/23/2022 17:56:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.14 on epoch=889
06/23/2022 17:56:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.18 on epoch=894
06/23/2022 17:56:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.04 on epoch=899
06/23/2022 17:56:30 - INFO - __main__ - Global step 1800 Train loss 1.11 ACC 0.53125 on epoch=899
06/23/2022 17:56:30 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=899, global_step=1800
06/23/2022 17:56:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.10 on epoch=904
06/23/2022 17:56:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.21 on epoch=909
06/23/2022 17:56:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.04 on epoch=914
06/23/2022 17:56:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.08 on epoch=919
06/23/2022 17:56:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.98 on epoch=924
06/23/2022 17:56:44 - INFO - __main__ - Global step 1850 Train loss 1.08 ACC 0.46875 on epoch=924
06/23/2022 17:56:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.12 on epoch=929
06/23/2022 17:56:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.99 on epoch=934
06/23/2022 17:56:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.05 on epoch=939
06/23/2022 17:56:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.95 on epoch=944
06/23/2022 17:56:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.94 on epoch=949
06/23/2022 17:56:55 - INFO - __main__ - Global step 1900 Train loss 1.01 ACC 0.5 on epoch=949
06/23/2022 17:56:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.96 on epoch=954
06/23/2022 17:56:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.94 on epoch=959
06/23/2022 17:56:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.91 on epoch=964
06/23/2022 17:57:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.85 on epoch=969
06/23/2022 17:57:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.87 on epoch=974
06/23/2022 17:57:05 - INFO - __main__ - Global step 1950 Train loss 0.90 ACC 0.5 on epoch=974
06/23/2022 17:57:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.88 on epoch=979
06/23/2022 17:57:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.78 on epoch=984
06/23/2022 17:57:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.80 on epoch=989
06/23/2022 17:57:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.83 on epoch=994
06/23/2022 17:57:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.82 on epoch=999
06/23/2022 17:57:14 - INFO - __main__ - Global step 2000 Train loss 0.82 ACC 0.5 on epoch=999
06/23/2022 17:57:14 - INFO - __main__ - save last model!
06/23/2022 17:57:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 17:57:14 - INFO - __main__ - Start tokenizing ... 408 instances
06/23/2022 17:57:14 - INFO - __main__ - Printing 3 examples
06/23/2022 17:57:14 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/23/2022 17:57:14 - INFO - __main__ - ['equivalent']
06/23/2022 17:57:14 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/23/2022 17:57:14 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:57:14 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/23/2022 17:57:14 - INFO - __main__ - ['not_equivalent']
06/23/2022 17:57:14 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:57:14 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:57:15 - INFO - __main__ - Loaded 408 examples from test data
06/23/2022 17:57:58 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_87_0.2_8_predictions.txt
06/23/2022 17:57:58 - INFO - __main__ - ACC on test data: 0.6765
06/23/2022 17:57:58 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.2, bsz=8, dev_performance=0.53125, test_performance=0.6764705882352942
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (90818): No such process
Task: glue-qqp, Checkpoint: models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt, Identifier: T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10
Output directory () already exists and is not empty.
06/23/2022 17:58:03 - INFO - __main__ - Namespace(task_dir='data/glue-qqp/', task_name='glue-qqp', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/23/2022 17:58:03 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp
06/23/2022 17:58:03 - INFO - __main__ - Namespace(task_dir='data/glue-qqp/', task_name='glue-qqp', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/23/2022 17:58:03 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp
06/23/2022 17:58:05 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/23/2022 17:58:05 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/23/2022 17:58:05 - INFO - __main__ - args.device: cuda:1
06/23/2022 17:58:05 - INFO - __main__ - args.device: cuda:0
06/23/2022 17:58:05 - INFO - __main__ - Using 2 gpus
06/23/2022 17:58:05 - INFO - __main__ - Using 2 gpus
06/23/2022 17:58:05 - INFO - __main__ - Fine-tuning the following samples: ['glue-qqp_16_100', 'glue-qqp_16_13', 'glue-qqp_16_21', 'glue-qqp_16_42', 'glue-qqp_16_87']
06/23/2022 17:58:05 - INFO - __main__ - Fine-tuning the following samples: ['glue-qqp_16_100', 'glue-qqp_16_13', 'glue-qqp_16_21', 'glue-qqp_16_42', 'glue-qqp_16_87']
06/23/2022 17:58:10 - INFO - __main__ - Running ... prefix=glue-qqp_16_100, lr=0.5, bsz=8 ...
06/23/2022 17:58:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:58:11 - INFO - __main__ - Printing 3 examples
06/23/2022 17:58:11 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/23/2022 17:58:11 - INFO - __main__ - ['not_duplicate']
06/23/2022 17:58:11 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/23/2022 17:58:11 - INFO - __main__ - ['not_duplicate']
06/23/2022 17:58:11 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/23/2022 17:58:11 - INFO - __main__ - ['not_duplicate']
06/23/2022 17:58:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 17:58:11 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:58:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:58:11 - INFO - __main__ - Printing 3 examples
06/23/2022 17:58:11 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/23/2022 17:58:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:58:11 - INFO - __main__ - ['not_duplicate']
06/23/2022 17:58:11 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/23/2022 17:58:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:58:11 - INFO - __main__ - ['not_duplicate']
06/23/2022 17:58:11 - INFO - __main__ - Printing 3 examples
06/23/2022 17:58:11 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/23/2022 17:58:11 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/23/2022 17:58:11 - INFO - __main__ - ['not_duplicate']
06/23/2022 17:58:11 - INFO - __main__ - ['not_duplicate']
06/23/2022 17:58:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 17:58:11 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/23/2022 17:58:11 - INFO - __main__ - ['not_duplicate']
06/23/2022 17:58:11 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/23/2022 17:58:11 - INFO - __main__ - ['not_duplicate']
06/23/2022 17:58:11 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:58:11 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:58:11 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:58:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 17:58:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 17:58:11 - INFO - __main__ - Printing 3 examples
06/23/2022 17:58:11 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/23/2022 17:58:11 - INFO - __main__ - ['not_duplicate']
06/23/2022 17:58:11 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/23/2022 17:58:11 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:58:11 - INFO - __main__ - ['not_duplicate']
06/23/2022 17:58:11 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/23/2022 17:58:11 - INFO - __main__ - ['not_duplicate']
06/23/2022 17:58:11 - INFO - __main__ - Tokenizing Input ...
06/23/2022 17:58:11 - INFO - __main__ - Tokenizing Output ...
06/23/2022 17:58:11 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 17:58:18 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:58:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:58:18 - INFO - __main__ - Starting training!
06/23/2022 17:58:18 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 17:58:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 17:58:24 - INFO - __main__ - Starting training!
06/23/2022 17:58:26 - INFO - __main__ - Step 10 Global step 10 Train loss 6.81 on epoch=4
06/23/2022 17:58:28 - INFO - __main__ - Step 20 Global step 20 Train loss 6.82 on epoch=9
06/23/2022 17:58:29 - INFO - __main__ - Step 30 Global step 30 Train loss 6.79 on epoch=14
06/23/2022 17:58:30 - INFO - __main__ - Step 40 Global step 40 Train loss 6.75 on epoch=19
06/23/2022 17:58:32 - INFO - __main__ - Step 50 Global step 50 Train loss 6.70 on epoch=24
06/23/2022 17:58:35 - INFO - __main__ - Global step 50 Train loss 6.77 ACC 0.0 on epoch=24
06/23/2022 17:58:35 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 17:58:36 - INFO - __main__ - Step 60 Global step 60 Train loss 6.65 on epoch=29
06/23/2022 17:58:38 - INFO - __main__ - Step 70 Global step 70 Train loss 6.67 on epoch=34
06/23/2022 17:58:39 - INFO - __main__ - Step 80 Global step 80 Train loss 6.69 on epoch=39
06/23/2022 17:58:40 - INFO - __main__ - Step 90 Global step 90 Train loss 6.57 on epoch=44
06/23/2022 17:58:42 - INFO - __main__ - Step 100 Global step 100 Train loss 6.53 on epoch=49
06/23/2022 17:58:43 - INFO - __main__ - Global step 100 Train loss 6.62 ACC 0.0 on epoch=49
06/23/2022 17:58:45 - INFO - __main__ - Step 110 Global step 110 Train loss 6.51 on epoch=54
06/23/2022 17:58:46 - INFO - __main__ - Step 120 Global step 120 Train loss 6.39 on epoch=59
06/23/2022 17:58:48 - INFO - __main__ - Step 130 Global step 130 Train loss 6.34 on epoch=64
06/23/2022 17:58:49 - INFO - __main__ - Step 140 Global step 140 Train loss 6.24 on epoch=69
06/23/2022 17:58:50 - INFO - __main__ - Step 150 Global step 150 Train loss 6.14 on epoch=74
06/23/2022 17:58:54 - INFO - __main__ - Global step 150 Train loss 6.33 ACC 0.0 on epoch=74
06/23/2022 17:58:55 - INFO - __main__ - Step 160 Global step 160 Train loss 5.84 on epoch=79
06/23/2022 17:58:57 - INFO - __main__ - Step 170 Global step 170 Train loss 5.71 on epoch=84
06/23/2022 17:58:58 - INFO - __main__ - Step 180 Global step 180 Train loss 5.51 on epoch=89
06/23/2022 17:58:59 - INFO - __main__ - Step 190 Global step 190 Train loss 5.28 on epoch=94
06/23/2022 17:59:01 - INFO - __main__ - Step 200 Global step 200 Train loss 5.24 on epoch=99
06/23/2022 17:59:11 - INFO - __main__ - Global step 200 Train loss 5.52 ACC 0.0 on epoch=99
06/23/2022 17:59:13 - INFO - __main__ - Step 210 Global step 210 Train loss 5.12 on epoch=104
06/23/2022 17:59:14 - INFO - __main__ - Step 220 Global step 220 Train loss 5.06 on epoch=109
06/23/2022 17:59:15 - INFO - __main__ - Step 230 Global step 230 Train loss 4.95 on epoch=114
06/23/2022 17:59:17 - INFO - __main__ - Step 240 Global step 240 Train loss 4.73 on epoch=119
06/23/2022 17:59:18 - INFO - __main__ - Step 250 Global step 250 Train loss 4.62 on epoch=124
06/23/2022 17:59:30 - INFO - __main__ - Global step 250 Train loss 4.90 ACC 0.0 on epoch=124
06/23/2022 17:59:32 - INFO - __main__ - Step 260 Global step 260 Train loss 4.53 on epoch=129
06/23/2022 17:59:33 - INFO - __main__ - Step 270 Global step 270 Train loss 4.44 on epoch=134
06/23/2022 17:59:35 - INFO - __main__ - Step 280 Global step 280 Train loss 4.24 on epoch=139
06/23/2022 17:59:36 - INFO - __main__ - Step 290 Global step 290 Train loss 4.19 on epoch=144
06/23/2022 17:59:37 - INFO - __main__ - Step 300 Global step 300 Train loss 4.11 on epoch=149
06/23/2022 17:59:39 - INFO - __main__ - Global step 300 Train loss 4.30 ACC 0.0 on epoch=149
06/23/2022 17:59:41 - INFO - __main__ - Step 310 Global step 310 Train loss 3.92 on epoch=154
06/23/2022 17:59:42 - INFO - __main__ - Step 320 Global step 320 Train loss 3.90 on epoch=159
06/23/2022 17:59:44 - INFO - __main__ - Step 330 Global step 330 Train loss 3.70 on epoch=164
06/23/2022 17:59:45 - INFO - __main__ - Step 340 Global step 340 Train loss 3.56 on epoch=169
06/23/2022 17:59:46 - INFO - __main__ - Step 350 Global step 350 Train loss 3.54 on epoch=174
06/23/2022 17:59:48 - INFO - __main__ - Global step 350 Train loss 3.73 ACC 0.25 on epoch=174
06/23/2022 17:59:48 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.25 on epoch=174, global_step=350
06/23/2022 17:59:49 - INFO - __main__ - Step 360 Global step 360 Train loss 3.50 on epoch=179
06/23/2022 17:59:51 - INFO - __main__ - Step 370 Global step 370 Train loss 3.40 on epoch=184
06/23/2022 17:59:52 - INFO - __main__ - Step 380 Global step 380 Train loss 3.32 on epoch=189
06/23/2022 17:59:53 - INFO - __main__ - Step 390 Global step 390 Train loss 3.26 on epoch=194
06/23/2022 17:59:55 - INFO - __main__ - Step 400 Global step 400 Train loss 3.17 on epoch=199
06/23/2022 17:59:57 - INFO - __main__ - Global step 400 Train loss 3.33 ACC 0.125 on epoch=199
06/23/2022 17:59:58 - INFO - __main__ - Step 410 Global step 410 Train loss 3.08 on epoch=204
06/23/2022 18:00:00 - INFO - __main__ - Step 420 Global step 420 Train loss 3.02 on epoch=209
06/23/2022 18:00:01 - INFO - __main__ - Step 430 Global step 430 Train loss 2.97 on epoch=214
06/23/2022 18:00:02 - INFO - __main__ - Step 440 Global step 440 Train loss 2.88 on epoch=219
06/23/2022 18:00:04 - INFO - __main__ - Step 450 Global step 450 Train loss 2.62 on epoch=224
06/23/2022 18:00:06 - INFO - __main__ - Global step 450 Train loss 2.91 ACC 0.40625 on epoch=224
06/23/2022 18:00:06 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.40625 on epoch=224, global_step=450
06/23/2022 18:00:08 - INFO - __main__ - Step 460 Global step 460 Train loss 2.44 on epoch=229
06/23/2022 18:00:09 - INFO - __main__ - Step 470 Global step 470 Train loss 2.42 on epoch=234
06/23/2022 18:00:11 - INFO - __main__ - Step 480 Global step 480 Train loss 2.32 on epoch=239
06/23/2022 18:00:12 - INFO - __main__ - Step 490 Global step 490 Train loss 2.30 on epoch=244
06/23/2022 18:00:13 - INFO - __main__ - Step 500 Global step 500 Train loss 2.23 on epoch=249
06/23/2022 18:00:16 - INFO - __main__ - Global step 500 Train loss 2.34 ACC 0.46875 on epoch=249
06/23/2022 18:00:16 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=249, global_step=500
06/23/2022 18:00:17 - INFO - __main__ - Step 510 Global step 510 Train loss 2.14 on epoch=254
06/23/2022 18:00:19 - INFO - __main__ - Step 520 Global step 520 Train loss 2.10 on epoch=259
06/23/2022 18:00:20 - INFO - __main__ - Step 530 Global step 530 Train loss 2.03 on epoch=264
06/23/2022 18:00:22 - INFO - __main__ - Step 540 Global step 540 Train loss 2.00 on epoch=269
06/23/2022 18:00:23 - INFO - __main__ - Step 550 Global step 550 Train loss 1.92 on epoch=274
06/23/2022 18:00:26 - INFO - __main__ - Global step 550 Train loss 2.04 ACC 0.40625 on epoch=274
06/23/2022 18:00:27 - INFO - __main__ - Step 560 Global step 560 Train loss 1.89 on epoch=279
06/23/2022 18:00:29 - INFO - __main__ - Step 570 Global step 570 Train loss 1.84 on epoch=284
06/23/2022 18:00:30 - INFO - __main__ - Step 580 Global step 580 Train loss 1.74 on epoch=289
06/23/2022 18:00:31 - INFO - __main__ - Step 590 Global step 590 Train loss 1.73 on epoch=294
06/23/2022 18:00:33 - INFO - __main__ - Step 600 Global step 600 Train loss 1.75 on epoch=299
06/23/2022 18:00:36 - INFO - __main__ - Global step 600 Train loss 1.79 ACC 0.3125 on epoch=299
06/23/2022 18:00:38 - INFO - __main__ - Step 610 Global step 610 Train loss 1.70 on epoch=304
06/23/2022 18:00:39 - INFO - __main__ - Step 620 Global step 620 Train loss 1.53 on epoch=309
06/23/2022 18:00:41 - INFO - __main__ - Step 630 Global step 630 Train loss 1.59 on epoch=314
06/23/2022 18:00:42 - INFO - __main__ - Step 640 Global step 640 Train loss 1.64 on epoch=319
06/23/2022 18:00:43 - INFO - __main__ - Step 650 Global step 650 Train loss 1.61 on epoch=324
06/23/2022 18:00:46 - INFO - __main__ - Global step 650 Train loss 1.61 ACC 0.5 on epoch=324
06/23/2022 18:00:46 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=324, global_step=650
06/23/2022 18:00:47 - INFO - __main__ - Step 660 Global step 660 Train loss 1.59 on epoch=329
06/23/2022 18:00:48 - INFO - __main__ - Step 670 Global step 670 Train loss 1.52 on epoch=334
06/23/2022 18:00:50 - INFO - __main__ - Step 680 Global step 680 Train loss 1.52 on epoch=339
06/23/2022 18:00:51 - INFO - __main__ - Step 690 Global step 690 Train loss 1.56 on epoch=344
06/23/2022 18:00:52 - INFO - __main__ - Step 700 Global step 700 Train loss 1.47 on epoch=349
06/23/2022 18:00:55 - INFO - __main__ - Global step 700 Train loss 1.53 ACC 0.5 on epoch=349
06/23/2022 18:00:57 - INFO - __main__ - Step 710 Global step 710 Train loss 1.34 on epoch=354
06/23/2022 18:00:58 - INFO - __main__ - Step 720 Global step 720 Train loss 1.32 on epoch=359
06/23/2022 18:01:00 - INFO - __main__ - Step 730 Global step 730 Train loss 1.23 on epoch=364
06/23/2022 18:01:01 - INFO - __main__ - Step 740 Global step 740 Train loss 1.43 on epoch=369
06/23/2022 18:01:02 - INFO - __main__ - Step 750 Global step 750 Train loss 1.24 on epoch=374
06/23/2022 18:01:05 - INFO - __main__ - Global step 750 Train loss 1.31 ACC 0.5 on epoch=374
06/23/2022 18:01:07 - INFO - __main__ - Step 760 Global step 760 Train loss 1.24 on epoch=379
06/23/2022 18:01:08 - INFO - __main__ - Step 770 Global step 770 Train loss 1.22 on epoch=384
06/23/2022 18:01:09 - INFO - __main__ - Step 780 Global step 780 Train loss 1.19 on epoch=389
06/23/2022 18:01:11 - INFO - __main__ - Step 790 Global step 790 Train loss 1.12 on epoch=394
06/23/2022 18:01:12 - INFO - __main__ - Step 800 Global step 800 Train loss 1.11 on epoch=399
06/23/2022 18:01:15 - INFO - __main__ - Global step 800 Train loss 1.17 ACC 0.5 on epoch=399
06/23/2022 18:01:16 - INFO - __main__ - Step 810 Global step 810 Train loss 1.08 on epoch=404
06/23/2022 18:01:18 - INFO - __main__ - Step 820 Global step 820 Train loss 1.06 on epoch=409
06/23/2022 18:01:19 - INFO - __main__ - Step 830 Global step 830 Train loss 1.04 on epoch=414
06/23/2022 18:01:20 - INFO - __main__ - Step 840 Global step 840 Train loss 1.07 on epoch=419
06/23/2022 18:01:22 - INFO - __main__ - Step 850 Global step 850 Train loss 1.01 on epoch=424
06/23/2022 18:01:24 - INFO - __main__ - Global step 850 Train loss 1.05 ACC 0.5 on epoch=424
06/23/2022 18:01:25 - INFO - __main__ - Step 860 Global step 860 Train loss 1.03 on epoch=429
06/23/2022 18:01:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.97 on epoch=434
06/23/2022 18:01:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.97 on epoch=439
06/23/2022 18:01:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.90 on epoch=444
06/23/2022 18:01:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.94 on epoch=449
06/23/2022 18:01:33 - INFO - __main__ - Global step 900 Train loss 0.96 ACC 0.5 on epoch=449
06/23/2022 18:01:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.93 on epoch=454
06/23/2022 18:01:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.89 on epoch=459
06/23/2022 18:01:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.87 on epoch=464
06/23/2022 18:01:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.82 on epoch=469
06/23/2022 18:01:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.83 on epoch=474
06/23/2022 18:01:42 - INFO - __main__ - Global step 950 Train loss 0.87 ACC 0.5 on epoch=474
06/23/2022 18:01:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.81 on epoch=479
06/23/2022 18:01:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.84 on epoch=484
06/23/2022 18:01:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.71 on epoch=489
06/23/2022 18:01:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.78 on epoch=494
06/23/2022 18:01:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.74 on epoch=499
06/23/2022 18:01:51 - INFO - __main__ - Global step 1000 Train loss 0.78 ACC 0.5 on epoch=499
06/23/2022 18:01:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.79 on epoch=504
06/23/2022 18:01:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.80 on epoch=509
06/23/2022 18:01:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.66 on epoch=514
06/23/2022 18:01:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.71 on epoch=519
06/23/2022 18:01:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.75 on epoch=524
06/23/2022 18:01:59 - INFO - __main__ - Global step 1050 Train loss 0.74 ACC 0.5 on epoch=524
06/23/2022 18:02:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.67 on epoch=529
06/23/2022 18:02:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.68 on epoch=534
06/23/2022 18:02:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.68 on epoch=539
06/23/2022 18:02:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.74 on epoch=544
06/23/2022 18:02:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.65 on epoch=549
06/23/2022 18:02:08 - INFO - __main__ - Global step 1100 Train loss 0.68 ACC 0.5 on epoch=549
06/23/2022 18:02:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.63 on epoch=554
06/23/2022 18:02:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.63 on epoch=559
06/23/2022 18:02:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.61 on epoch=564
06/23/2022 18:02:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.66 on epoch=569
06/23/2022 18:02:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.56 on epoch=574
06/23/2022 18:02:18 - INFO - __main__ - Global step 1150 Train loss 0.62 ACC 0.5 on epoch=574
06/23/2022 18:02:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.56 on epoch=579
06/23/2022 18:02:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.82 on epoch=584
06/23/2022 18:02:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.60 on epoch=589
06/23/2022 18:02:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.68 on epoch=594
06/23/2022 18:02:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.57 on epoch=599
06/23/2022 18:02:27 - INFO - __main__ - Global step 1200 Train loss 0.85 ACC 0.5 on epoch=599
06/23/2022 18:02:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.64 on epoch=604
06/23/2022 18:02:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
06/23/2022 18:02:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.53 on epoch=614
06/23/2022 18:02:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.54 on epoch=619
06/23/2022 18:02:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=624
06/23/2022 18:02:37 - INFO - __main__ - Global step 1250 Train loss 0.54 ACC 0.5 on epoch=624
06/23/2022 18:02:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=629
06/23/2022 18:02:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.56 on epoch=634
06/23/2022 18:02:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.53 on epoch=639
06/23/2022 18:02:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.57 on epoch=644
06/23/2022 18:02:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.51 on epoch=649
06/23/2022 18:02:46 - INFO - __main__ - Global step 1300 Train loss 0.53 ACC 0.5 on epoch=649
06/23/2022 18:02:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=654
06/23/2022 18:02:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.48 on epoch=659
06/23/2022 18:02:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=664
06/23/2022 18:02:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.48 on epoch=669
06/23/2022 18:02:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.54 on epoch=674
06/23/2022 18:02:54 - INFO - __main__ - Global step 1350 Train loss 0.49 ACC 0.5 on epoch=674
06/23/2022 18:02:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=679
06/23/2022 18:02:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=684
06/23/2022 18:02:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
06/23/2022 18:03:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.52 on epoch=694
06/23/2022 18:03:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.53 on epoch=699
06/23/2022 18:03:04 - INFO - __main__ - Global step 1400 Train loss 0.51 ACC 0.5 on epoch=699
06/23/2022 18:03:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=704
06/23/2022 18:03:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=709
06/23/2022 18:03:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.47 on epoch=714
06/23/2022 18:03:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=719
06/23/2022 18:03:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=724
06/23/2022 18:03:13 - INFO - __main__ - Global step 1450 Train loss 0.47 ACC 0.5 on epoch=724
06/23/2022 18:03:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=729
06/23/2022 18:03:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.49 on epoch=734
06/23/2022 18:03:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.58 on epoch=739
06/23/2022 18:03:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.44 on epoch=744
06/23/2022 18:03:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=749
06/23/2022 18:03:24 - INFO - __main__ - Global step 1500 Train loss 0.48 ACC 0.5 on epoch=749
06/23/2022 18:03:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.45 on epoch=754
06/23/2022 18:03:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=759
06/23/2022 18:03:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
06/23/2022 18:03:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=769
06/23/2022 18:03:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=774
06/23/2022 18:03:34 - INFO - __main__ - Global step 1550 Train loss 0.45 ACC 0.5 on epoch=774
06/23/2022 18:03:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.49 on epoch=779
06/23/2022 18:03:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=784
06/23/2022 18:03:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=789
06/23/2022 18:03:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.48 on epoch=794
06/23/2022 18:03:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=799
06/23/2022 18:03:47 - INFO - __main__ - Global step 1600 Train loss 0.45 ACC 0.5 on epoch=799
06/23/2022 18:03:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=804
06/23/2022 18:03:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=809
06/23/2022 18:03:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=814
06/23/2022 18:03:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=819
06/23/2022 18:03:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=824
06/23/2022 18:03:58 - INFO - __main__ - Global step 1650 Train loss 0.43 ACC 0.5 on epoch=824
06/23/2022 18:04:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=829
06/23/2022 18:04:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.49 on epoch=834
06/23/2022 18:04:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.97 on epoch=839
06/23/2022 18:04:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.33 on epoch=844
06/23/2022 18:04:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.57 on epoch=849
06/23/2022 18:04:08 - INFO - __main__ - Global step 1700 Train loss 0.75 ACC 0.5 on epoch=849
06/23/2022 18:04:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=854
06/23/2022 18:04:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.46 on epoch=859
06/23/2022 18:04:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=864
06/23/2022 18:04:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=869
06/23/2022 18:04:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=874
06/23/2022 18:04:16 - INFO - __main__ - Global step 1750 Train loss 0.43 ACC 0.5 on epoch=874
06/23/2022 18:04:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=879
06/23/2022 18:04:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=884
06/23/2022 18:04:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=889
06/23/2022 18:04:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
06/23/2022 18:04:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=899
06/23/2022 18:04:29 - INFO - __main__ - Global step 1800 Train loss 0.39 ACC 0.5 on epoch=899
06/23/2022 18:04:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=904
06/23/2022 18:04:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=909
06/23/2022 18:04:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=914
06/23/2022 18:04:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=919
06/23/2022 18:04:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=924
06/23/2022 18:04:38 - INFO - __main__ - Global step 1850 Train loss 0.39 ACC 0.5 on epoch=924
06/23/2022 18:04:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=929
06/23/2022 18:04:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=934
06/23/2022 18:04:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=939
06/23/2022 18:04:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.30 on epoch=944
06/23/2022 18:04:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
06/23/2022 18:04:48 - INFO - __main__ - Global step 1900 Train loss 0.34 ACC 0.5 on epoch=949
06/23/2022 18:04:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
06/23/2022 18:04:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=959
06/23/2022 18:04:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.34 on epoch=964
06/23/2022 18:04:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=969
06/23/2022 18:04:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
06/23/2022 18:05:01 - INFO - __main__ - Global step 1950 Train loss 0.38 ACC 0.5 on epoch=974
06/23/2022 18:05:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=979
06/23/2022 18:05:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=984
06/23/2022 18:05:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=989
06/23/2022 18:05:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=994
06/23/2022 18:05:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=999
06/23/2022 18:05:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 18:05:09 - INFO - __main__ - Printing 3 examples
06/23/2022 18:05:09 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/23/2022 18:05:09 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:05:09 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/23/2022 18:05:09 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:05:09 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/23/2022 18:05:09 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:05:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 18:05:09 - INFO - __main__ - Tokenizing Output ...
06/23/2022 18:05:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 18:05:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 18:05:09 - INFO - __main__ - Printing 3 examples
06/23/2022 18:05:09 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/23/2022 18:05:09 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:05:09 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/23/2022 18:05:09 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:05:09 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/23/2022 18:05:09 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:05:09 - INFO - __main__ - Tokenizing Input ...
06/23/2022 18:05:09 - INFO - __main__ - Tokenizing Output ...
06/23/2022 18:05:09 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 18:05:09 - INFO - __main__ - Global step 2000 Train loss 0.36 ACC 0.5 on epoch=999
06/23/2022 18:05:09 - INFO - __main__ - save last model!
06/23/2022 18:05:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 18:05:09 - INFO - __main__ - Start tokenizing ... 40430 instances
06/23/2022 18:05:09 - INFO - __main__ - Printing 3 examples
06/23/2022 18:05:09 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/23/2022 18:05:09 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:05:09 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/23/2022 18:05:09 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:05:09 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/23/2022 18:05:09 - INFO - __main__ - ['duplicate']
06/23/2022 18:05:09 - INFO - __main__ - Tokenizing Input ...
06/23/2022 18:05:15 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 18:05:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 18:05:15 - INFO - __main__ - Starting training!
06/23/2022 18:05:32 - INFO - __main__ - Tokenizing Output ...
06/23/2022 18:06:25 - INFO - __main__ - Loaded 40430 examples from test data
06/23/2022 18:36:59 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_100_0.5_8_predictions.txt
06/23/2022 18:36:59 - INFO - __main__ - ACC on test data: 0.3684
06/23/2022 18:36:59 - INFO - __main__ - prefix=glue-qqp_16_100, lr=0.5, bsz=8, dev_performance=0.5, test_performance=0.3683650754390304
06/23/2022 18:36:59 - INFO - __main__ - Running ... prefix=glue-qqp_16_100, lr=0.4, bsz=8 ...
06/23/2022 18:37:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 18:37:00 - INFO - __main__ - Printing 3 examples
06/23/2022 18:37:00 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/23/2022 18:37:00 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:37:00 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/23/2022 18:37:00 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:37:00 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/23/2022 18:37:00 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:37:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 18:37:00 - INFO - __main__ - Tokenizing Output ...
06/23/2022 18:37:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 18:37:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 18:37:00 - INFO - __main__ - Printing 3 examples
06/23/2022 18:37:00 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/23/2022 18:37:00 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:37:00 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/23/2022 18:37:00 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:37:00 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/23/2022 18:37:00 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:37:00 - INFO - __main__ - Tokenizing Input ...
06/23/2022 18:37:00 - INFO - __main__ - Tokenizing Output ...
06/23/2022 18:37:00 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 18:37:06 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 18:37:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 18:37:07 - INFO - __main__ - Starting training!
06/23/2022 18:37:08 - INFO - __main__ - Step 10 Global step 10 Train loss 6.86 on epoch=4
06/23/2022 18:37:10 - INFO - __main__ - Step 20 Global step 20 Train loss 6.76 on epoch=9
06/23/2022 18:37:11 - INFO - __main__ - Step 30 Global step 30 Train loss 6.81 on epoch=14
06/23/2022 18:37:12 - INFO - __main__ - Step 40 Global step 40 Train loss 6.88 on epoch=19
06/23/2022 18:37:14 - INFO - __main__ - Step 50 Global step 50 Train loss 6.70 on epoch=24
06/23/2022 18:37:16 - INFO - __main__ - Global step 50 Train loss 6.80 ACC 0.0 on epoch=24
06/23/2022 18:37:16 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 18:37:17 - INFO - __main__ - Step 60 Global step 60 Train loss 6.67 on epoch=29
06/23/2022 18:37:19 - INFO - __main__ - Step 70 Global step 70 Train loss 6.71 on epoch=34
06/23/2022 18:37:20 - INFO - __main__ - Step 80 Global step 80 Train loss 6.67 on epoch=39
06/23/2022 18:37:22 - INFO - __main__ - Step 90 Global step 90 Train loss 6.59 on epoch=44
06/23/2022 18:37:23 - INFO - __main__ - Step 100 Global step 100 Train loss 6.59 on epoch=49
06/23/2022 18:37:31 - INFO - __main__ - Global step 100 Train loss 6.65 ACC 0.0 on epoch=49
06/23/2022 18:37:33 - INFO - __main__ - Step 110 Global step 110 Train loss 6.58 on epoch=54
06/23/2022 18:37:34 - INFO - __main__ - Step 120 Global step 120 Train loss 6.46 on epoch=59
06/23/2022 18:37:36 - INFO - __main__ - Step 130 Global step 130 Train loss 6.49 on epoch=64
06/23/2022 18:37:37 - INFO - __main__ - Step 140 Global step 140 Train loss 6.45 on epoch=69
06/23/2022 18:37:38 - INFO - __main__ - Step 150 Global step 150 Train loss 6.34 on epoch=74
06/23/2022 18:37:39 - INFO - __main__ - Global step 150 Train loss 6.46 ACC 0.0 on epoch=74
06/23/2022 18:37:41 - INFO - __main__ - Step 160 Global step 160 Train loss 6.22 on epoch=79
06/23/2022 18:37:42 - INFO - __main__ - Step 170 Global step 170 Train loss 6.03 on epoch=84
06/23/2022 18:37:43 - INFO - __main__ - Step 180 Global step 180 Train loss 5.99 on epoch=89
06/23/2022 18:37:45 - INFO - __main__ - Step 190 Global step 190 Train loss 5.78 on epoch=94
06/23/2022 18:37:46 - INFO - __main__ - Step 200 Global step 200 Train loss 5.76 on epoch=99
06/23/2022 18:37:50 - INFO - __main__ - Global step 200 Train loss 5.96 ACC 0.0 on epoch=99
06/23/2022 18:37:51 - INFO - __main__ - Step 210 Global step 210 Train loss 5.66 on epoch=104
06/23/2022 18:37:52 - INFO - __main__ - Step 220 Global step 220 Train loss 5.57 on epoch=109
06/23/2022 18:37:54 - INFO - __main__ - Step 230 Global step 230 Train loss 5.45 on epoch=114
06/23/2022 18:37:55 - INFO - __main__ - Step 240 Global step 240 Train loss 5.39 on epoch=119
06/23/2022 18:37:57 - INFO - __main__ - Step 250 Global step 250 Train loss 5.31 on epoch=124
06/23/2022 18:38:06 - INFO - __main__ - Global step 250 Train loss 5.48 ACC 0.0 on epoch=124
06/23/2022 18:38:08 - INFO - __main__ - Step 260 Global step 260 Train loss 5.22 on epoch=129
06/23/2022 18:38:09 - INFO - __main__ - Step 270 Global step 270 Train loss 4.99 on epoch=134
06/23/2022 18:38:11 - INFO - __main__ - Step 280 Global step 280 Train loss 4.98 on epoch=139
06/23/2022 18:38:12 - INFO - __main__ - Step 290 Global step 290 Train loss 4.82 on epoch=144
06/23/2022 18:38:13 - INFO - __main__ - Step 300 Global step 300 Train loss 4.72 on epoch=149
06/23/2022 18:38:24 - INFO - __main__ - Global step 300 Train loss 4.95 ACC 0.0 on epoch=149
06/23/2022 18:38:25 - INFO - __main__ - Step 310 Global step 310 Train loss 4.62 on epoch=154
06/23/2022 18:38:26 - INFO - __main__ - Step 320 Global step 320 Train loss 4.55 on epoch=159
06/23/2022 18:38:28 - INFO - __main__ - Step 330 Global step 330 Train loss 4.61 on epoch=164
06/23/2022 18:38:29 - INFO - __main__ - Step 340 Global step 340 Train loss 4.50 on epoch=169
06/23/2022 18:38:30 - INFO - __main__ - Step 350 Global step 350 Train loss 4.35 on epoch=174
06/23/2022 18:38:32 - INFO - __main__ - Global step 350 Train loss 4.53 ACC 0.0 on epoch=174
06/23/2022 18:38:33 - INFO - __main__ - Step 360 Global step 360 Train loss 4.24 on epoch=179
06/23/2022 18:38:35 - INFO - __main__ - Step 370 Global step 370 Train loss 4.03 on epoch=184
06/23/2022 18:38:36 - INFO - __main__ - Step 380 Global step 380 Train loss 4.15 on epoch=189
06/23/2022 18:38:37 - INFO - __main__ - Step 390 Global step 390 Train loss 3.85 on epoch=194
06/23/2022 18:38:39 - INFO - __main__ - Step 400 Global step 400 Train loss 3.72 on epoch=199
06/23/2022 18:38:41 - INFO - __main__ - Global step 400 Train loss 4.00 ACC 0.0 on epoch=199
06/23/2022 18:38:42 - INFO - __main__ - Step 410 Global step 410 Train loss 3.68 on epoch=204
06/23/2022 18:38:43 - INFO - __main__ - Step 420 Global step 420 Train loss 3.60 on epoch=209
06/23/2022 18:38:44 - INFO - __main__ - Step 430 Global step 430 Train loss 3.50 on epoch=214
06/23/2022 18:38:46 - INFO - __main__ - Step 440 Global step 440 Train loss 3.43 on epoch=219
06/23/2022 18:38:47 - INFO - __main__ - Step 450 Global step 450 Train loss 3.34 on epoch=224
06/23/2022 18:38:50 - INFO - __main__ - Global step 450 Train loss 3.51 ACC 0.03125 on epoch=224
06/23/2022 18:38:50 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=224, global_step=450
06/23/2022 18:38:51 - INFO - __main__ - Step 460 Global step 460 Train loss 3.13 on epoch=229
06/23/2022 18:38:52 - INFO - __main__ - Step 470 Global step 470 Train loss 3.10 on epoch=234
06/23/2022 18:38:54 - INFO - __main__ - Step 480 Global step 480 Train loss 3.03 on epoch=239
06/23/2022 18:38:55 - INFO - __main__ - Step 490 Global step 490 Train loss 2.97 on epoch=244
06/23/2022 18:38:56 - INFO - __main__ - Step 500 Global step 500 Train loss 2.60 on epoch=249
06/23/2022 18:39:06 - INFO - __main__ - Global step 500 Train loss 2.96 ACC 0.3125 on epoch=249
06/23/2022 18:39:06 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.3125 on epoch=249, global_step=500
06/23/2022 18:39:08 - INFO - __main__ - Step 510 Global step 510 Train loss 2.54 on epoch=254
06/23/2022 18:39:09 - INFO - __main__ - Step 520 Global step 520 Train loss 2.53 on epoch=259
06/23/2022 18:39:10 - INFO - __main__ - Step 530 Global step 530 Train loss 2.30 on epoch=264
06/23/2022 18:39:12 - INFO - __main__ - Step 540 Global step 540 Train loss 2.25 on epoch=269
06/23/2022 18:39:13 - INFO - __main__ - Step 550 Global step 550 Train loss 2.07 on epoch=274
06/23/2022 18:39:15 - INFO - __main__ - Global step 550 Train loss 2.34 ACC 0.5 on epoch=274
06/23/2022 18:39:15 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.5 on epoch=274, global_step=550
06/23/2022 18:39:17 - INFO - __main__ - Step 560 Global step 560 Train loss 2.20 on epoch=279
06/23/2022 18:39:18 - INFO - __main__ - Step 570 Global step 570 Train loss 2.00 on epoch=284
06/23/2022 18:39:19 - INFO - __main__ - Step 580 Global step 580 Train loss 1.95 on epoch=289
06/23/2022 18:39:21 - INFO - __main__ - Step 590 Global step 590 Train loss 1.90 on epoch=294
06/23/2022 18:39:22 - INFO - __main__ - Step 600 Global step 600 Train loss 1.81 on epoch=299
06/23/2022 18:39:24 - INFO - __main__ - Global step 600 Train loss 1.97 ACC 0.5 on epoch=299
06/23/2022 18:39:25 - INFO - __main__ - Step 610 Global step 610 Train loss 1.86 on epoch=304
06/23/2022 18:39:27 - INFO - __main__ - Step 620 Global step 620 Train loss 1.77 on epoch=309
06/23/2022 18:39:28 - INFO - __main__ - Step 630 Global step 630 Train loss 1.70 on epoch=314
06/23/2022 18:39:29 - INFO - __main__ - Step 640 Global step 640 Train loss 1.68 on epoch=319
06/23/2022 18:39:31 - INFO - __main__ - Step 650 Global step 650 Train loss 1.59 on epoch=324
06/23/2022 18:39:33 - INFO - __main__ - Global step 650 Train loss 1.72 ACC 0.5 on epoch=324
06/23/2022 18:39:35 - INFO - __main__ - Step 660 Global step 660 Train loss 1.50 on epoch=329
06/23/2022 18:39:36 - INFO - __main__ - Step 670 Global step 670 Train loss 1.35 on epoch=334
06/23/2022 18:39:37 - INFO - __main__ - Step 680 Global step 680 Train loss 1.30 on epoch=339
06/23/2022 18:39:39 - INFO - __main__ - Step 690 Global step 690 Train loss 1.30 on epoch=344
06/23/2022 18:39:40 - INFO - __main__ - Step 700 Global step 700 Train loss 1.27 on epoch=349
06/23/2022 18:39:43 - INFO - __main__ - Global step 700 Train loss 1.35 ACC 0.5 on epoch=349
06/23/2022 18:39:44 - INFO - __main__ - Step 710 Global step 710 Train loss 1.23 on epoch=354
06/23/2022 18:39:46 - INFO - __main__ - Step 720 Global step 720 Train loss 1.13 on epoch=359
06/23/2022 18:39:47 - INFO - __main__ - Step 730 Global step 730 Train loss 1.24 on epoch=364
06/23/2022 18:39:49 - INFO - __main__ - Step 740 Global step 740 Train loss 1.03 on epoch=369
06/23/2022 18:39:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.97 on epoch=374
06/23/2022 18:39:52 - INFO - __main__ - Global step 750 Train loss 1.12 ACC 0.5 on epoch=374
06/23/2022 18:39:53 - INFO - __main__ - Step 760 Global step 760 Train loss 1.00 on epoch=379
06/23/2022 18:39:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.98 on epoch=384
06/23/2022 18:39:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.94 on epoch=389
06/23/2022 18:39:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.89 on epoch=394
06/23/2022 18:39:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.80 on epoch=399
06/23/2022 18:40:03 - INFO - __main__ - Global step 800 Train loss 0.92 ACC 0.5 on epoch=399
06/23/2022 18:40:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.85 on epoch=404
06/23/2022 18:40:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.83 on epoch=409
06/23/2022 18:40:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=414
06/23/2022 18:40:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.75 on epoch=419
06/23/2022 18:40:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.85 on epoch=424
06/23/2022 18:40:12 - INFO - __main__ - Global step 850 Train loss 0.82 ACC 0.5 on epoch=424
06/23/2022 18:40:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.62 on epoch=429
06/23/2022 18:40:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.71 on epoch=434
06/23/2022 18:40:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.63 on epoch=439
06/23/2022 18:40:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.71 on epoch=444
06/23/2022 18:40:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.62 on epoch=449
06/23/2022 18:40:23 - INFO - __main__ - Global step 900 Train loss 0.66 ACC 0.5 on epoch=449
06/23/2022 18:40:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.61 on epoch=454
06/23/2022 18:40:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.65 on epoch=459
06/23/2022 18:40:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.56 on epoch=464
06/23/2022 18:40:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.56 on epoch=469
06/23/2022 18:40:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.55 on epoch=474
06/23/2022 18:40:32 - INFO - __main__ - Global step 950 Train loss 0.58 ACC 0.5 on epoch=474
06/23/2022 18:40:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.54 on epoch=479
06/23/2022 18:40:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.60 on epoch=484
06/23/2022 18:40:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.53 on epoch=489
06/23/2022 18:40:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.55 on epoch=494
06/23/2022 18:40:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.52 on epoch=499
06/23/2022 18:40:43 - INFO - __main__ - Global step 1000 Train loss 0.55 ACC 0.5 on epoch=499
06/23/2022 18:40:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=504
06/23/2022 18:40:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=509
06/23/2022 18:40:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.59 on epoch=514
06/23/2022 18:40:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=519
06/23/2022 18:40:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.50 on epoch=524
06/23/2022 18:40:53 - INFO - __main__ - Global step 1050 Train loss 0.50 ACC 0.5 on epoch=524
06/23/2022 18:40:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=529
06/23/2022 18:40:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=534
06/23/2022 18:40:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.45 on epoch=539
06/23/2022 18:40:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=544
06/23/2022 18:41:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=549
06/23/2022 18:41:03 - INFO - __main__ - Global step 1100 Train loss 0.45 ACC 0.5 on epoch=549
06/23/2022 18:41:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=554
06/23/2022 18:41:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=559
06/23/2022 18:41:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=564
06/23/2022 18:41:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=569
06/23/2022 18:41:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.50 on epoch=574
06/23/2022 18:41:14 - INFO - __main__ - Global step 1150 Train loss 0.46 ACC 0.5 on epoch=574
06/23/2022 18:41:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.47 on epoch=579
06/23/2022 18:41:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=584
06/23/2022 18:41:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=589
06/23/2022 18:41:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=594
06/23/2022 18:41:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=599
06/23/2022 18:41:22 - INFO - __main__ - Global step 1200 Train loss 0.43 ACC 0.53125 on epoch=599
06/23/2022 18:41:22 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=599, global_step=1200
06/23/2022 18:41:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=604
06/23/2022 18:41:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=609
06/23/2022 18:41:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.49 on epoch=614
06/23/2022 18:41:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=619
06/23/2022 18:41:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=624
06/23/2022 18:41:32 - INFO - __main__ - Global step 1250 Train loss 0.43 ACC 0.5 on epoch=624
06/23/2022 18:41:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.43 on epoch=629
06/23/2022 18:41:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=634
06/23/2022 18:41:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=639
06/23/2022 18:41:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.46 on epoch=644
06/23/2022 18:41:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.43 on epoch=649
06/23/2022 18:41:41 - INFO - __main__ - Global step 1300 Train loss 0.43 ACC 0.5 on epoch=649
06/23/2022 18:41:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=654
06/23/2022 18:41:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=659
06/23/2022 18:41:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=664
06/23/2022 18:41:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=669
06/23/2022 18:41:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=674
06/23/2022 18:41:50 - INFO - __main__ - Global step 1350 Train loss 0.39 ACC 0.5 on epoch=674
06/23/2022 18:41:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=679
06/23/2022 18:41:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=684
06/23/2022 18:41:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=689
06/23/2022 18:41:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=694
06/23/2022 18:41:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=699
06/23/2022 18:41:59 - INFO - __main__ - Global step 1400 Train loss 0.39 ACC 0.5 on epoch=699
06/23/2022 18:42:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=704
06/23/2022 18:42:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=709
06/23/2022 18:42:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=714
06/23/2022 18:42:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=719
06/23/2022 18:42:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=724
06/23/2022 18:42:09 - INFO - __main__ - Global step 1450 Train loss 0.37 ACC 0.5 on epoch=724
06/23/2022 18:42:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.39 on epoch=729
06/23/2022 18:42:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=734
06/23/2022 18:42:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=739
06/23/2022 18:42:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.47 on epoch=744
06/23/2022 18:42:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=749
06/23/2022 18:42:18 - INFO - __main__ - Global step 1500 Train loss 0.39 ACC 0.5 on epoch=749
06/23/2022 18:42:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=754
06/23/2022 18:42:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=759
06/23/2022 18:42:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=764
06/23/2022 18:42:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=769
06/23/2022 18:42:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=774
06/23/2022 18:42:26 - INFO - __main__ - Global step 1550 Train loss 0.36 ACC 0.5 on epoch=774
06/23/2022 18:42:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=779
06/23/2022 18:42:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.34 on epoch=784
06/23/2022 18:42:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=789
06/23/2022 18:42:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=794
06/23/2022 18:42:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=799
06/23/2022 18:42:34 - INFO - __main__ - Global step 1600 Train loss 0.35 ACC 0.5 on epoch=799
06/23/2022 18:42:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=804
06/23/2022 18:42:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=809
06/23/2022 18:42:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.36 on epoch=814
06/23/2022 18:42:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=819
06/23/2022 18:42:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=824
06/23/2022 18:42:42 - INFO - __main__ - Global step 1650 Train loss 0.33 ACC 0.5 on epoch=824
06/23/2022 18:42:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.35 on epoch=829
06/23/2022 18:42:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.34 on epoch=834
06/23/2022 18:42:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=839
06/23/2022 18:42:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.31 on epoch=844
06/23/2022 18:42:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=849
06/23/2022 18:42:49 - INFO - __main__ - Global step 1700 Train loss 0.33 ACC 0.5 on epoch=849
06/23/2022 18:42:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.32 on epoch=854
06/23/2022 18:42:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=859
06/23/2022 18:42:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.31 on epoch=864
06/23/2022 18:42:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.33 on epoch=869
06/23/2022 18:42:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
06/23/2022 18:42:57 - INFO - __main__ - Global step 1750 Train loss 0.33 ACC 0.5 on epoch=874
06/23/2022 18:42:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=879
06/23/2022 18:43:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.32 on epoch=884
06/23/2022 18:43:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
06/23/2022 18:43:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=894
06/23/2022 18:43:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=899
06/23/2022 18:43:04 - INFO - __main__ - Global step 1800 Train loss 0.35 ACC 0.5 on epoch=899
06/23/2022 18:43:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.31 on epoch=904
06/23/2022 18:43:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=909
06/23/2022 18:43:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=914
06/23/2022 18:43:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=919
06/23/2022 18:43:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=924
06/23/2022 18:43:12 - INFO - __main__ - Global step 1850 Train loss 0.34 ACC 0.5 on epoch=924
06/23/2022 18:43:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.28 on epoch=929
06/23/2022 18:43:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=934
06/23/2022 18:43:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=939
06/23/2022 18:43:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=944
06/23/2022 18:43:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
06/23/2022 18:43:19 - INFO - __main__ - Global step 1900 Train loss 0.33 ACC 0.5 on epoch=949
06/23/2022 18:43:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.30 on epoch=954
06/23/2022 18:43:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=959
06/23/2022 18:43:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=964
06/23/2022 18:43:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.33 on epoch=969
06/23/2022 18:43:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
06/23/2022 18:43:27 - INFO - __main__ - Global step 1950 Train loss 0.31 ACC 0.5 on epoch=974
06/23/2022 18:43:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=979
06/23/2022 18:43:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=984
06/23/2022 18:43:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=989
06/23/2022 18:43:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=994
06/23/2022 18:43:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=999
06/23/2022 18:43:35 - INFO - __main__ - Global step 2000 Train loss 0.31 ACC 0.5 on epoch=999
06/23/2022 18:43:35 - INFO - __main__ - save last model!
06/23/2022 18:43:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 18:43:35 - INFO - __main__ - Start tokenizing ... 40430 instances
06/23/2022 18:43:35 - INFO - __main__ - Printing 3 examples
06/23/2022 18:43:35 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/23/2022 18:43:35 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:43:35 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/23/2022 18:43:35 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:43:35 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/23/2022 18:43:35 - INFO - __main__ - ['duplicate']
06/23/2022 18:43:35 - INFO - __main__ - Tokenizing Input ...
06/23/2022 18:43:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 18:43:35 - INFO - __main__ - Printing 3 examples
06/23/2022 18:43:35 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/23/2022 18:43:35 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:43:35 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/23/2022 18:43:35 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:43:35 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/23/2022 18:43:35 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:43:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 18:43:35 - INFO - __main__ - Tokenizing Output ...
06/23/2022 18:43:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 18:43:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 18:43:35 - INFO - __main__ - Printing 3 examples
06/23/2022 18:43:35 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/23/2022 18:43:35 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:43:35 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/23/2022 18:43:35 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:43:35 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/23/2022 18:43:35 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:43:35 - INFO - __main__ - Tokenizing Input ...
06/23/2022 18:43:35 - INFO - __main__ - Tokenizing Output ...
06/23/2022 18:43:35 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 18:43:42 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 18:43:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 18:43:42 - INFO - __main__ - Starting training!
06/23/2022 18:43:53 - INFO - __main__ - Tokenizing Output ...
06/23/2022 18:44:34 - INFO - __main__ - Loaded 40430 examples from test data
06/23/2022 18:56:31 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_100_0.4_8_predictions.txt
06/23/2022 18:56:31 - INFO - __main__ - ACC on test data: 0.3681
06/23/2022 18:56:31 - INFO - __main__ - prefix=glue-qqp_16_100, lr=0.4, bsz=8, dev_performance=0.53125, test_performance=0.3681424684640119
06/23/2022 18:56:31 - INFO - __main__ - Running ... prefix=glue-qqp_16_100, lr=0.3, bsz=8 ...
06/23/2022 18:56:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 18:56:32 - INFO - __main__ - Printing 3 examples
06/23/2022 18:56:32 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/23/2022 18:56:32 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:56:32 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/23/2022 18:56:32 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:56:32 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/23/2022 18:56:32 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:56:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 18:56:32 - INFO - __main__ - Tokenizing Output ...
06/23/2022 18:56:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 18:56:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 18:56:32 - INFO - __main__ - Printing 3 examples
06/23/2022 18:56:32 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/23/2022 18:56:32 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:56:32 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/23/2022 18:56:32 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:56:32 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/23/2022 18:56:32 - INFO - __main__ - ['not_duplicate']
06/23/2022 18:56:32 - INFO - __main__ - Tokenizing Input ...
06/23/2022 18:56:32 - INFO - __main__ - Tokenizing Output ...
06/23/2022 18:56:32 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 18:56:38 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 18:56:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 18:56:39 - INFO - __main__ - Starting training!
06/23/2022 18:56:41 - INFO - __main__ - Step 10 Global step 10 Train loss 6.90 on epoch=4
06/23/2022 18:56:42 - INFO - __main__ - Step 20 Global step 20 Train loss 6.87 on epoch=9
06/23/2022 18:56:43 - INFO - __main__ - Step 30 Global step 30 Train loss 6.84 on epoch=14
06/23/2022 18:56:45 - INFO - __main__ - Step 40 Global step 40 Train loss 6.80 on epoch=19
06/23/2022 18:56:46 - INFO - __main__ - Step 50 Global step 50 Train loss 6.77 on epoch=24
06/23/2022 18:56:48 - INFO - __main__ - Global step 50 Train loss 6.84 ACC 0.0 on epoch=24
06/23/2022 18:56:49 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 18:56:50 - INFO - __main__ - Step 60 Global step 60 Train loss 6.73 on epoch=29
06/23/2022 18:56:51 - INFO - __main__ - Step 70 Global step 70 Train loss 6.79 on epoch=34
06/23/2022 18:56:53 - INFO - __main__ - Step 80 Global step 80 Train loss 6.65 on epoch=39
06/23/2022 18:56:54 - INFO - __main__ - Step 90 Global step 90 Train loss 6.69 on epoch=44
06/23/2022 18:56:55 - INFO - __main__ - Step 100 Global step 100 Train loss 6.71 on epoch=49
06/23/2022 18:56:57 - INFO - __main__ - Global step 100 Train loss 6.71 ACC 0.0 on epoch=49
06/23/2022 18:56:59 - INFO - __main__ - Step 110 Global step 110 Train loss 6.65 on epoch=54
06/23/2022 18:57:00 - INFO - __main__ - Step 120 Global step 120 Train loss 6.75 on epoch=59
06/23/2022 18:57:02 - INFO - __main__ - Step 130 Global step 130 Train loss 6.66 on epoch=64
06/23/2022 18:57:03 - INFO - __main__ - Step 140 Global step 140 Train loss 6.62 on epoch=69
06/23/2022 18:57:04 - INFO - __main__ - Step 150 Global step 150 Train loss 6.66 on epoch=74
06/23/2022 18:57:06 - INFO - __main__ - Global step 150 Train loss 6.67 ACC 0.0 on epoch=74
06/23/2022 18:57:07 - INFO - __main__ - Step 160 Global step 160 Train loss 6.66 on epoch=79
06/23/2022 18:57:09 - INFO - __main__ - Step 170 Global step 170 Train loss 6.54 on epoch=84
06/23/2022 18:57:10 - INFO - __main__ - Step 180 Global step 180 Train loss 6.56 on epoch=89
06/23/2022 18:57:12 - INFO - __main__ - Step 190 Global step 190 Train loss 6.49 on epoch=94
06/23/2022 18:57:13 - INFO - __main__ - Step 200 Global step 200 Train loss 6.47 on epoch=99
06/23/2022 18:57:15 - INFO - __main__ - Global step 200 Train loss 6.55 ACC 0.0 on epoch=99
06/23/2022 18:57:16 - INFO - __main__ - Step 210 Global step 210 Train loss 6.45 on epoch=104
06/23/2022 18:57:18 - INFO - __main__ - Step 220 Global step 220 Train loss 6.43 on epoch=109
06/23/2022 18:57:19 - INFO - __main__ - Step 230 Global step 230 Train loss 6.44 on epoch=114
06/23/2022 18:57:20 - INFO - __main__ - Step 240 Global step 240 Train loss 6.37 on epoch=119
06/23/2022 18:57:22 - INFO - __main__ - Step 250 Global step 250 Train loss 6.25 on epoch=124
06/23/2022 18:57:24 - INFO - __main__ - Global step 250 Train loss 6.39 ACC 0.0 on epoch=124
06/23/2022 18:57:25 - INFO - __main__ - Step 260 Global step 260 Train loss 6.14 on epoch=129
06/23/2022 18:57:27 - INFO - __main__ - Step 270 Global step 270 Train loss 6.11 on epoch=134
06/23/2022 18:57:28 - INFO - __main__ - Step 280 Global step 280 Train loss 6.04 on epoch=139
06/23/2022 18:57:29 - INFO - __main__ - Step 290 Global step 290 Train loss 5.92 on epoch=144
06/23/2022 18:57:31 - INFO - __main__ - Step 300 Global step 300 Train loss 5.91 on epoch=149
06/23/2022 18:57:34 - INFO - __main__ - Global step 300 Train loss 6.03 ACC 0.0 on epoch=149
06/23/2022 18:57:35 - INFO - __main__ - Step 310 Global step 310 Train loss 5.85 on epoch=154
06/23/2022 18:57:37 - INFO - __main__ - Step 320 Global step 320 Train loss 5.72 on epoch=159
06/23/2022 18:57:38 - INFO - __main__ - Step 330 Global step 330 Train loss 5.65 on epoch=164
06/23/2022 18:57:40 - INFO - __main__ - Step 340 Global step 340 Train loss 5.69 on epoch=169
06/23/2022 18:57:41 - INFO - __main__ - Step 350 Global step 350 Train loss 5.66 on epoch=174
06/23/2022 18:57:44 - INFO - __main__ - Global step 350 Train loss 5.71 ACC 0.0 on epoch=174
06/23/2022 18:57:46 - INFO - __main__ - Step 360 Global step 360 Train loss 5.56 on epoch=179
06/23/2022 18:57:47 - INFO - __main__ - Step 370 Global step 370 Train loss 5.50 on epoch=184
06/23/2022 18:57:49 - INFO - __main__ - Step 380 Global step 380 Train loss 5.47 on epoch=189
06/23/2022 18:57:50 - INFO - __main__ - Step 390 Global step 390 Train loss 5.43 on epoch=194
06/23/2022 18:57:52 - INFO - __main__ - Step 400 Global step 400 Train loss 5.44 on epoch=199
06/23/2022 18:57:54 - INFO - __main__ - Global step 400 Train loss 5.48 ACC 0.0 on epoch=199
06/23/2022 18:57:55 - INFO - __main__ - Step 410 Global step 410 Train loss 5.38 on epoch=204
06/23/2022 18:57:56 - INFO - __main__ - Step 420 Global step 420 Train loss 5.44 on epoch=209
06/23/2022 18:57:58 - INFO - __main__ - Step 430 Global step 430 Train loss 5.36 on epoch=214
06/23/2022 18:57:59 - INFO - __main__ - Step 440 Global step 440 Train loss 5.31 on epoch=219
06/23/2022 18:58:01 - INFO - __main__ - Step 450 Global step 450 Train loss 5.26 on epoch=224
06/23/2022 18:58:03 - INFO - __main__ - Global step 450 Train loss 5.35 ACC 0.0 on epoch=224
06/23/2022 18:58:05 - INFO - __main__ - Step 460 Global step 460 Train loss 5.22 on epoch=229
06/23/2022 18:58:06 - INFO - __main__ - Step 470 Global step 470 Train loss 5.22 on epoch=234
06/23/2022 18:58:08 - INFO - __main__ - Step 480 Global step 480 Train loss 5.18 on epoch=239
06/23/2022 18:58:09 - INFO - __main__ - Step 490 Global step 490 Train loss 5.19 on epoch=244
06/23/2022 18:58:11 - INFO - __main__ - Step 500 Global step 500 Train loss 5.14 on epoch=249
06/23/2022 18:58:15 - INFO - __main__ - Global step 500 Train loss 5.19 ACC 0.0 on epoch=249
06/23/2022 18:58:16 - INFO - __main__ - Step 510 Global step 510 Train loss 5.14 on epoch=254
06/23/2022 18:58:17 - INFO - __main__ - Step 520 Global step 520 Train loss 5.03 on epoch=259
06/23/2022 18:58:19 - INFO - __main__ - Step 530 Global step 530 Train loss 5.03 on epoch=264
06/23/2022 18:58:20 - INFO - __main__ - Step 540 Global step 540 Train loss 5.00 on epoch=269
06/23/2022 18:58:22 - INFO - __main__ - Step 550 Global step 550 Train loss 4.91 on epoch=274
06/23/2022 18:58:25 - INFO - __main__ - Global step 550 Train loss 5.02 ACC 0.0 on epoch=274
06/23/2022 18:58:27 - INFO - __main__ - Step 560 Global step 560 Train loss 4.91 on epoch=279
06/23/2022 18:58:28 - INFO - __main__ - Step 570 Global step 570 Train loss 4.84 on epoch=284
06/23/2022 18:58:29 - INFO - __main__ - Step 580 Global step 580 Train loss 4.93 on epoch=289
06/23/2022 18:58:31 - INFO - __main__ - Step 590 Global step 590 Train loss 4.85 on epoch=294
06/23/2022 18:58:32 - INFO - __main__ - Step 600 Global step 600 Train loss 4.89 on epoch=299
06/23/2022 18:58:40 - INFO - __main__ - Global step 600 Train loss 4.88 ACC 0.0 on epoch=299
06/23/2022 18:58:41 - INFO - __main__ - Step 610 Global step 610 Train loss 4.85 on epoch=304
06/23/2022 18:58:42 - INFO - __main__ - Step 620 Global step 620 Train loss 4.68 on epoch=309
06/23/2022 18:58:44 - INFO - __main__ - Step 630 Global step 630 Train loss 4.72 on epoch=314
06/23/2022 18:58:45 - INFO - __main__ - Step 640 Global step 640 Train loss 4.76 on epoch=319
06/23/2022 18:58:46 - INFO - __main__ - Step 650 Global step 650 Train loss 4.70 on epoch=324
06/23/2022 18:58:57 - INFO - __main__ - Global step 650 Train loss 4.74 ACC 0.0 on epoch=324
06/23/2022 18:58:58 - INFO - __main__ - Step 660 Global step 660 Train loss 4.54 on epoch=329
06/23/2022 18:59:00 - INFO - __main__ - Step 670 Global step 670 Train loss 4.71 on epoch=334
06/23/2022 18:59:01 - INFO - __main__ - Step 680 Global step 680 Train loss 4.53 on epoch=339
06/23/2022 18:59:02 - INFO - __main__ - Step 690 Global step 690 Train loss 4.46 on epoch=344
06/23/2022 18:59:04 - INFO - __main__ - Step 700 Global step 700 Train loss 4.39 on epoch=349
06/23/2022 18:59:16 - INFO - __main__ - Global step 700 Train loss 4.53 ACC 0.0 on epoch=349
06/23/2022 18:59:17 - INFO - __main__ - Step 710 Global step 710 Train loss 4.51 on epoch=354
06/23/2022 18:59:19 - INFO - __main__ - Step 720 Global step 720 Train loss 4.35 on epoch=359
06/23/2022 18:59:20 - INFO - __main__ - Step 730 Global step 730 Train loss 4.28 on epoch=364
06/23/2022 18:59:21 - INFO - __main__ - Step 740 Global step 740 Train loss 4.24 on epoch=369
06/23/2022 18:59:23 - INFO - __main__ - Step 750 Global step 750 Train loss 4.17 on epoch=374
06/23/2022 18:59:32 - INFO - __main__ - Global step 750 Train loss 4.31 ACC 0.03125 on epoch=374
06/23/2022 18:59:32 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=374, global_step=750
06/23/2022 18:59:34 - INFO - __main__ - Step 760 Global step 760 Train loss 4.12 on epoch=379
06/23/2022 18:59:35 - INFO - __main__ - Step 770 Global step 770 Train loss 4.13 on epoch=384
06/23/2022 18:59:36 - INFO - __main__ - Step 780 Global step 780 Train loss 3.99 on epoch=389
06/23/2022 18:59:38 - INFO - __main__ - Step 790 Global step 790 Train loss 3.93 on epoch=394
06/23/2022 18:59:39 - INFO - __main__ - Step 800 Global step 800 Train loss 3.93 on epoch=399
06/23/2022 18:59:42 - INFO - __main__ - Global step 800 Train loss 4.02 ACC 0.09375 on epoch=399
06/23/2022 18:59:42 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.09375 on epoch=399, global_step=800
06/23/2022 18:59:44 - INFO - __main__ - Step 810 Global step 810 Train loss 3.91 on epoch=404
06/23/2022 18:59:45 - INFO - __main__ - Step 820 Global step 820 Train loss 3.90 on epoch=409
06/23/2022 18:59:46 - INFO - __main__ - Step 830 Global step 830 Train loss 3.85 on epoch=414
06/23/2022 18:59:48 - INFO - __main__ - Step 840 Global step 840 Train loss 3.85 on epoch=419
06/23/2022 18:59:49 - INFO - __main__ - Step 850 Global step 850 Train loss 3.74 on epoch=424
06/23/2022 18:59:51 - INFO - __main__ - Global step 850 Train loss 3.85 ACC 0.375 on epoch=424
06/23/2022 18:59:51 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.375 on epoch=424, global_step=850
06/23/2022 18:59:52 - INFO - __main__ - Step 860 Global step 860 Train loss 3.80 on epoch=429
06/23/2022 18:59:54 - INFO - __main__ - Step 870 Global step 870 Train loss 3.60 on epoch=434
06/23/2022 18:59:55 - INFO - __main__ - Step 880 Global step 880 Train loss 3.50 on epoch=439
06/23/2022 18:59:57 - INFO - __main__ - Step 890 Global step 890 Train loss 3.66 on epoch=444
06/23/2022 18:59:58 - INFO - __main__ - Step 900 Global step 900 Train loss 3.37 on epoch=449
06/23/2022 19:00:00 - INFO - __main__ - Global step 900 Train loss 3.59 ACC 0.46875 on epoch=449
06/23/2022 19:00:00 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.46875 on epoch=449, global_step=900
06/23/2022 19:00:02 - INFO - __main__ - Step 910 Global step 910 Train loss 3.38 on epoch=454
06/23/2022 19:00:03 - INFO - __main__ - Step 920 Global step 920 Train loss 3.26 on epoch=459
06/23/2022 19:00:05 - INFO - __main__ - Step 930 Global step 930 Train loss 3.15 on epoch=464
06/23/2022 19:00:06 - INFO - __main__ - Step 940 Global step 940 Train loss 3.23 on epoch=469
06/23/2022 19:00:07 - INFO - __main__ - Step 950 Global step 950 Train loss 3.17 on epoch=474
06/23/2022 19:00:10 - INFO - __main__ - Global step 950 Train loss 3.24 ACC 0.46875 on epoch=474
06/23/2022 19:00:12 - INFO - __main__ - Step 960 Global step 960 Train loss 3.15 on epoch=479
06/23/2022 19:00:13 - INFO - __main__ - Step 970 Global step 970 Train loss 3.03 on epoch=484
06/23/2022 19:00:15 - INFO - __main__ - Step 980 Global step 980 Train loss 2.95 on epoch=489
06/23/2022 19:00:16 - INFO - __main__ - Step 990 Global step 990 Train loss 2.86 on epoch=494
06/23/2022 19:00:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.93 on epoch=499
06/23/2022 19:00:20 - INFO - __main__ - Global step 1000 Train loss 2.98 ACC 0.46875 on epoch=499
06/23/2022 19:00:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.77 on epoch=504
06/23/2022 19:00:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.61 on epoch=509
06/23/2022 19:00:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.59 on epoch=514
06/23/2022 19:00:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.53 on epoch=519
06/23/2022 19:00:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.44 on epoch=524
06/23/2022 19:00:30 - INFO - __main__ - Global step 1050 Train loss 2.59 ACC 0.5 on epoch=524
06/23/2022 19:00:30 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=524, global_step=1050
06/23/2022 19:00:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.40 on epoch=529
06/23/2022 19:00:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.30 on epoch=534
06/23/2022 19:00:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.16 on epoch=539
06/23/2022 19:00:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.07 on epoch=544
06/23/2022 19:00:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.05 on epoch=549
06/23/2022 19:00:41 - INFO - __main__ - Global step 1100 Train loss 2.20 ACC 0.5 on epoch=549
06/23/2022 19:00:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.03 on epoch=554
06/23/2022 19:00:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.89 on epoch=559
06/23/2022 19:00:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.04 on epoch=564
06/23/2022 19:00:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.93 on epoch=569
06/23/2022 19:00:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.87 on epoch=574
06/23/2022 19:00:50 - INFO - __main__ - Global step 1150 Train loss 1.95 ACC 0.5 on epoch=574
06/23/2022 19:00:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.74 on epoch=579
06/23/2022 19:00:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.56 on epoch=584
06/23/2022 19:00:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.60 on epoch=589
06/23/2022 19:00:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.63 on epoch=594
06/23/2022 19:00:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.54 on epoch=599
06/23/2022 19:01:00 - INFO - __main__ - Global step 1200 Train loss 1.61 ACC 0.5 on epoch=599
06/23/2022 19:01:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.48 on epoch=604
06/23/2022 19:01:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.49 on epoch=609
06/23/2022 19:01:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.38 on epoch=614
06/23/2022 19:01:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.41 on epoch=619
06/23/2022 19:01:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.35 on epoch=624
06/23/2022 19:01:09 - INFO - __main__ - Global step 1250 Train loss 1.42 ACC 0.5 on epoch=624
06/23/2022 19:01:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.35 on epoch=629
06/23/2022 19:01:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.21 on epoch=634
06/23/2022 19:01:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.21 on epoch=639
06/23/2022 19:01:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.20 on epoch=644
06/23/2022 19:01:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.15 on epoch=649
06/23/2022 19:01:20 - INFO - __main__ - Global step 1300 Train loss 1.23 ACC 0.5 on epoch=649
06/23/2022 19:01:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.12 on epoch=654
06/23/2022 19:01:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.12 on epoch=659
06/23/2022 19:01:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.12 on epoch=664
06/23/2022 19:01:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.10 on epoch=669
06/23/2022 19:01:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.05 on epoch=674
06/23/2022 19:01:29 - INFO - __main__ - Global step 1350 Train loss 1.10 ACC 0.5 on epoch=674
06/23/2022 19:01:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.06 on epoch=679
06/23/2022 19:01:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.00 on epoch=684
06/23/2022 19:01:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.01 on epoch=689
06/23/2022 19:01:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.90 on epoch=694
06/23/2022 19:01:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.91 on epoch=699
06/23/2022 19:01:44 - INFO - __main__ - Global step 1400 Train loss 0.98 ACC 0.5 on epoch=699
06/23/2022 19:01:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.93 on epoch=704
06/23/2022 19:01:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.84 on epoch=709
06/23/2022 19:01:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.87 on epoch=714
06/23/2022 19:01:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.82 on epoch=719
06/23/2022 19:01:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.78 on epoch=724
06/23/2022 19:01:56 - INFO - __main__ - Global step 1450 Train loss 0.85 ACC 0.5 on epoch=724
06/23/2022 19:01:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.80 on epoch=729
06/23/2022 19:01:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.76 on epoch=734
06/23/2022 19:02:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.77 on epoch=739
06/23/2022 19:02:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.71 on epoch=744
06/23/2022 19:02:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.73 on epoch=749
06/23/2022 19:02:08 - INFO - __main__ - Global step 1500 Train loss 0.75 ACC 0.5 on epoch=749
06/23/2022 19:02:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.68 on epoch=754
06/23/2022 19:02:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.68 on epoch=759
06/23/2022 19:02:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.69 on epoch=764
06/23/2022 19:02:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.68 on epoch=769
06/23/2022 19:02:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.60 on epoch=774
06/23/2022 19:02:17 - INFO - __main__ - Global step 1550 Train loss 0.67 ACC 0.5 on epoch=774
06/23/2022 19:02:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.62 on epoch=779
06/23/2022 19:02:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.65 on epoch=784
06/23/2022 19:02:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.65 on epoch=789
06/23/2022 19:02:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.58 on epoch=794
06/23/2022 19:02:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.65 on epoch=799
06/23/2022 19:02:27 - INFO - __main__ - Global step 1600 Train loss 0.63 ACC 0.5 on epoch=799
06/23/2022 19:02:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.57 on epoch=804
06/23/2022 19:02:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.67 on epoch=809
06/23/2022 19:02:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.62 on epoch=814
06/23/2022 19:02:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.57 on epoch=819
06/23/2022 19:02:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.60 on epoch=824
06/23/2022 19:02:40 - INFO - __main__ - Global step 1650 Train loss 0.61 ACC 0.5 on epoch=824
06/23/2022 19:02:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.57 on epoch=829
06/23/2022 19:02:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.53 on epoch=834
06/23/2022 19:02:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.54 on epoch=839
06/23/2022 19:02:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.54 on epoch=844
06/23/2022 19:02:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.48 on epoch=849
06/23/2022 19:02:49 - INFO - __main__ - Global step 1700 Train loss 0.54 ACC 0.5 on epoch=849
06/23/2022 19:02:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.50 on epoch=854
06/23/2022 19:02:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.51 on epoch=859
06/23/2022 19:02:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.51 on epoch=864
06/23/2022 19:02:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.52 on epoch=869
06/23/2022 19:02:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=874
06/23/2022 19:02:59 - INFO - __main__ - Global step 1750 Train loss 0.50 ACC 0.5 on epoch=874
06/23/2022 19:03:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=879
06/23/2022 19:03:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.49 on epoch=884
06/23/2022 19:03:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.53 on epoch=889
06/23/2022 19:03:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.50 on epoch=894
06/23/2022 19:03:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.49 on epoch=899
06/23/2022 19:03:09 - INFO - __main__ - Global step 1800 Train loss 0.49 ACC 0.5 on epoch=899
06/23/2022 19:03:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=904
06/23/2022 19:03:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.50 on epoch=909
06/23/2022 19:03:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=914
06/23/2022 19:03:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.45 on epoch=919
06/23/2022 19:03:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=924
06/23/2022 19:03:18 - INFO - __main__ - Global step 1850 Train loss 0.46 ACC 0.5 on epoch=924
06/23/2022 19:03:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
06/23/2022 19:03:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.47 on epoch=934
06/23/2022 19:03:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.44 on epoch=939
06/23/2022 19:03:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=944
06/23/2022 19:03:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.49 on epoch=949
06/23/2022 19:03:28 - INFO - __main__ - Global step 1900 Train loss 0.44 ACC 0.5 on epoch=949
06/23/2022 19:03:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.42 on epoch=954
06/23/2022 19:03:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/23/2022 19:03:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.46 on epoch=964
06/23/2022 19:03:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=969
06/23/2022 19:03:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=974
06/23/2022 19:03:37 - INFO - __main__ - Global step 1950 Train loss 0.43 ACC 0.5 on epoch=974
06/23/2022 19:03:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
06/23/2022 19:03:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
06/23/2022 19:03:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.48 on epoch=989
06/23/2022 19:03:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=994
06/23/2022 19:03:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=999
06/23/2022 19:03:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 19:03:45 - INFO - __main__ - Printing 3 examples
06/23/2022 19:03:45 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/23/2022 19:03:45 - INFO - __main__ - ['not_duplicate']
06/23/2022 19:03:45 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/23/2022 19:03:45 - INFO - __main__ - ['not_duplicate']
06/23/2022 19:03:45 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/23/2022 19:03:45 - INFO - __main__ - ['not_duplicate']
06/23/2022 19:03:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 19:03:45 - INFO - __main__ - Tokenizing Output ...
06/23/2022 19:03:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 19:03:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 19:03:45 - INFO - __main__ - Printing 3 examples
06/23/2022 19:03:45 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/23/2022 19:03:45 - INFO - __main__ - ['not_duplicate']
06/23/2022 19:03:45 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/23/2022 19:03:45 - INFO - __main__ - ['not_duplicate']
06/23/2022 19:03:45 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/23/2022 19:03:45 - INFO - __main__ - ['not_duplicate']
06/23/2022 19:03:45 - INFO - __main__ - Tokenizing Input ...
06/23/2022 19:03:45 - INFO - __main__ - Tokenizing Output ...
06/23/2022 19:03:45 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 19:03:50 - INFO - __main__ - Global step 2000 Train loss 0.41 ACC 0.5 on epoch=999
06/23/2022 19:03:50 - INFO - __main__ - save last model!
06/23/2022 19:03:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 19:03:50 - INFO - __main__ - Start tokenizing ... 40430 instances
06/23/2022 19:03:50 - INFO - __main__ - Printing 3 examples
06/23/2022 19:03:50 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/23/2022 19:03:50 - INFO - __main__ - ['not_duplicate']
06/23/2022 19:03:50 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/23/2022 19:03:50 - INFO - __main__ - ['not_duplicate']
06/23/2022 19:03:50 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/23/2022 19:03:50 - INFO - __main__ - ['duplicate']
06/23/2022 19:03:50 - INFO - __main__ - Tokenizing Input ...
06/23/2022 19:03:51 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 19:03:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 19:03:52 - INFO - __main__ - Starting training!
06/23/2022 19:04:08 - INFO - __main__ - Tokenizing Output ...
06/23/2022 19:04:49 - INFO - __main__ - Loaded 40430 examples from test data
06/23/2022 20:01:48 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_100_0.3_8_predictions.txt
06/23/2022 20:01:48 - INFO - __main__ - ACC on test data: 0.3682
06/23/2022 20:01:49 - INFO - __main__ - prefix=glue-qqp_16_100, lr=0.3, bsz=8, dev_performance=0.5, test_performance=0.36816720257234725
06/23/2022 20:01:49 - INFO - __main__ - Running ... prefix=glue-qqp_16_100, lr=0.2, bsz=8 ...
06/23/2022 20:01:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 20:01:50 - INFO - __main__ - Printing 3 examples
06/23/2022 20:01:50 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/23/2022 20:01:50 - INFO - __main__ - ['not_duplicate']
06/23/2022 20:01:50 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/23/2022 20:01:50 - INFO - __main__ - ['not_duplicate']
06/23/2022 20:01:50 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/23/2022 20:01:50 - INFO - __main__ - ['not_duplicate']
06/23/2022 20:01:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 20:01:50 - INFO - __main__ - Tokenizing Output ...
06/23/2022 20:01:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 20:01:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 20:01:50 - INFO - __main__ - Printing 3 examples
06/23/2022 20:01:50 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/23/2022 20:01:50 - INFO - __main__ - ['not_duplicate']
06/23/2022 20:01:50 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/23/2022 20:01:50 - INFO - __main__ - ['not_duplicate']
06/23/2022 20:01:50 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/23/2022 20:01:50 - INFO - __main__ - ['not_duplicate']
06/23/2022 20:01:50 - INFO - __main__ - Tokenizing Input ...
06/23/2022 20:01:50 - INFO - __main__ - Tokenizing Output ...
06/23/2022 20:01:50 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 20:01:57 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 20:01:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 20:01:57 - INFO - __main__ - Starting training!
06/23/2022 20:01:59 - INFO - __main__ - Step 10 Global step 10 Train loss 6.88 on epoch=4
06/23/2022 20:02:00 - INFO - __main__ - Step 20 Global step 20 Train loss 6.77 on epoch=9
06/23/2022 20:02:01 - INFO - __main__ - Step 30 Global step 30 Train loss 6.84 on epoch=14
06/23/2022 20:02:03 - INFO - __main__ - Step 40 Global step 40 Train loss 6.81 on epoch=19
06/23/2022 20:02:04 - INFO - __main__ - Step 50 Global step 50 Train loss 6.83 on epoch=24
06/23/2022 20:02:06 - INFO - __main__ - Global step 50 Train loss 6.83 ACC 0.0 on epoch=24
06/23/2022 20:02:06 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 20:02:07 - INFO - __main__ - Step 60 Global step 60 Train loss 6.79 on epoch=29
06/23/2022 20:02:09 - INFO - __main__ - Step 70 Global step 70 Train loss 6.67 on epoch=34
06/23/2022 20:02:10 - INFO - __main__ - Step 80 Global step 80 Train loss 6.76 on epoch=39
06/23/2022 20:02:12 - INFO - __main__ - Step 90 Global step 90 Train loss 6.77 on epoch=44
06/23/2022 20:02:13 - INFO - __main__ - Step 100 Global step 100 Train loss 6.73 on epoch=49
06/23/2022 20:02:15 - INFO - __main__ - Global step 100 Train loss 6.75 ACC 0.0 on epoch=49
06/23/2022 20:02:16 - INFO - __main__ - Step 110 Global step 110 Train loss 6.74 on epoch=54
06/23/2022 20:02:18 - INFO - __main__ - Step 120 Global step 120 Train loss 6.70 on epoch=59
06/23/2022 20:02:19 - INFO - __main__ - Step 130 Global step 130 Train loss 6.70 on epoch=64
06/23/2022 20:02:21 - INFO - __main__ - Step 140 Global step 140 Train loss 6.70 on epoch=69
06/23/2022 20:02:22 - INFO - __main__ - Step 150 Global step 150 Train loss 6.59 on epoch=74
06/23/2022 20:02:25 - INFO - __main__ - Global step 150 Train loss 6.69 ACC 0.0 on epoch=74
06/23/2022 20:02:27 - INFO - __main__ - Step 160 Global step 160 Train loss 6.67 on epoch=79
06/23/2022 20:02:28 - INFO - __main__ - Step 170 Global step 170 Train loss 6.63 on epoch=84
06/23/2022 20:02:30 - INFO - __main__ - Step 180 Global step 180 Train loss 6.62 on epoch=89
06/23/2022 20:02:31 - INFO - __main__ - Step 190 Global step 190 Train loss 6.69 on epoch=94
06/23/2022 20:02:33 - INFO - __main__ - Step 200 Global step 200 Train loss 6.62 on epoch=99
06/23/2022 20:02:34 - INFO - __main__ - Global step 200 Train loss 6.65 ACC 0.0 on epoch=99
06/23/2022 20:02:36 - INFO - __main__ - Step 210 Global step 210 Train loss 6.63 on epoch=104
06/23/2022 20:02:37 - INFO - __main__ - Step 220 Global step 220 Train loss 6.53 on epoch=109
06/23/2022 20:02:39 - INFO - __main__ - Step 230 Global step 230 Train loss 6.58 on epoch=114
06/23/2022 20:02:40 - INFO - __main__ - Step 240 Global step 240 Train loss 6.48 on epoch=119
06/23/2022 20:02:42 - INFO - __main__ - Step 250 Global step 250 Train loss 6.50 on epoch=124
06/23/2022 20:02:45 - INFO - __main__ - Global step 250 Train loss 6.54 ACC 0.0 on epoch=124
06/23/2022 20:02:46 - INFO - __main__ - Step 260 Global step 260 Train loss 6.50 on epoch=129
06/23/2022 20:02:48 - INFO - __main__ - Step 270 Global step 270 Train loss 6.45 on epoch=134
06/23/2022 20:02:49 - INFO - __main__ - Step 280 Global step 280 Train loss 6.42 on epoch=139
06/23/2022 20:02:51 - INFO - __main__ - Step 290 Global step 290 Train loss 6.45 on epoch=144
06/23/2022 20:02:52 - INFO - __main__ - Step 300 Global step 300 Train loss 6.43 on epoch=149
06/23/2022 20:02:59 - INFO - __main__ - Global step 300 Train loss 6.45 ACC 0.0 on epoch=149
06/23/2022 20:03:00 - INFO - __main__ - Step 310 Global step 310 Train loss 6.38 on epoch=154
06/23/2022 20:03:02 - INFO - __main__ - Step 320 Global step 320 Train loss 6.39 on epoch=159
06/23/2022 20:03:03 - INFO - __main__ - Step 330 Global step 330 Train loss 6.41 on epoch=164
06/23/2022 20:03:05 - INFO - __main__ - Step 340 Global step 340 Train loss 6.32 on epoch=169
06/23/2022 20:03:06 - INFO - __main__ - Step 350 Global step 350 Train loss 6.25 on epoch=174
06/23/2022 20:03:09 - INFO - __main__ - Global step 350 Train loss 6.35 ACC 0.0 on epoch=174
06/23/2022 20:03:10 - INFO - __main__ - Step 360 Global step 360 Train loss 6.18 on epoch=179
06/23/2022 20:03:12 - INFO - __main__ - Step 370 Global step 370 Train loss 6.10 on epoch=184
06/23/2022 20:03:13 - INFO - __main__ - Step 380 Global step 380 Train loss 6.10 on epoch=189
06/23/2022 20:03:14 - INFO - __main__ - Step 390 Global step 390 Train loss 6.08 on epoch=194
06/23/2022 20:03:16 - INFO - __main__ - Step 400 Global step 400 Train loss 5.91 on epoch=199
06/23/2022 20:03:17 - INFO - __main__ - Global step 400 Train loss 6.07 ACC 0.0 on epoch=199
06/23/2022 20:03:19 - INFO - __main__ - Step 410 Global step 410 Train loss 5.94 on epoch=204
06/23/2022 20:03:20 - INFO - __main__ - Step 420 Global step 420 Train loss 5.87 on epoch=209
06/23/2022 20:03:22 - INFO - __main__ - Step 430 Global step 430 Train loss 5.77 on epoch=214
06/23/2022 20:03:23 - INFO - __main__ - Step 440 Global step 440 Train loss 5.72 on epoch=219
06/23/2022 20:03:24 - INFO - __main__ - Step 450 Global step 450 Train loss 5.70 on epoch=224
06/23/2022 20:03:27 - INFO - __main__ - Global step 450 Train loss 5.80 ACC 0.0 on epoch=224
06/23/2022 20:03:28 - INFO - __main__ - Step 460 Global step 460 Train loss 5.66 on epoch=229
06/23/2022 20:03:30 - INFO - __main__ - Step 470 Global step 470 Train loss 5.68 on epoch=234
06/23/2022 20:03:31 - INFO - __main__ - Step 480 Global step 480 Train loss 5.66 on epoch=239
06/23/2022 20:03:32 - INFO - __main__ - Step 490 Global step 490 Train loss 5.61 on epoch=244
06/23/2022 20:03:34 - INFO - __main__ - Step 500 Global step 500 Train loss 5.58 on epoch=249
06/23/2022 20:03:38 - INFO - __main__ - Global step 500 Train loss 5.64 ACC 0.0 on epoch=249
06/23/2022 20:03:40 - INFO - __main__ - Step 510 Global step 510 Train loss 5.42 on epoch=254
06/23/2022 20:03:41 - INFO - __main__ - Step 520 Global step 520 Train loss 5.39 on epoch=259
06/23/2022 20:03:42 - INFO - __main__ - Step 530 Global step 530 Train loss 5.42 on epoch=264
06/23/2022 20:03:44 - INFO - __main__ - Step 540 Global step 540 Train loss 5.31 on epoch=269
06/23/2022 20:03:45 - INFO - __main__ - Step 550 Global step 550 Train loss 5.28 on epoch=274
06/23/2022 20:03:54 - INFO - __main__ - Global step 550 Train loss 5.36 ACC 0.0 on epoch=274
06/23/2022 20:03:55 - INFO - __main__ - Step 560 Global step 560 Train loss 5.24 on epoch=279
06/23/2022 20:03:57 - INFO - __main__ - Step 570 Global step 570 Train loss 5.15 on epoch=284
06/23/2022 20:03:58 - INFO - __main__ - Step 580 Global step 580 Train loss 5.10 on epoch=289
06/23/2022 20:03:59 - INFO - __main__ - Step 590 Global step 590 Train loss 5.16 on epoch=294
06/23/2022 20:04:01 - INFO - __main__ - Step 600 Global step 600 Train loss 4.97 on epoch=299
06/23/2022 20:04:13 - INFO - __main__ - Global step 600 Train loss 5.12 ACC 0.0 on epoch=299
06/23/2022 20:04:14 - INFO - __main__ - Step 610 Global step 610 Train loss 4.93 on epoch=304
06/23/2022 20:04:15 - INFO - __main__ - Step 620 Global step 620 Train loss 4.80 on epoch=309
06/23/2022 20:04:17 - INFO - __main__ - Step 630 Global step 630 Train loss 4.84 on epoch=314
06/23/2022 20:04:18 - INFO - __main__ - Step 640 Global step 640 Train loss 4.78 on epoch=319
06/23/2022 20:04:20 - INFO - __main__ - Step 650 Global step 650 Train loss 4.79 on epoch=324
06/23/2022 20:04:32 - INFO - __main__ - Global step 650 Train loss 4.83 ACC 0.0 on epoch=324
06/23/2022 20:04:33 - INFO - __main__ - Step 660 Global step 660 Train loss 4.57 on epoch=329
06/23/2022 20:04:35 - INFO - __main__ - Step 670 Global step 670 Train loss 4.63 on epoch=334
06/23/2022 20:04:36 - INFO - __main__ - Step 680 Global step 680 Train loss 4.45 on epoch=339
06/23/2022 20:04:37 - INFO - __main__ - Step 690 Global step 690 Train loss 4.49 on epoch=344
06/23/2022 20:04:39 - INFO - __main__ - Step 700 Global step 700 Train loss 4.48 on epoch=349
06/23/2022 20:04:46 - INFO - __main__ - Global step 700 Train loss 4.52 ACC 0.0 on epoch=349
06/23/2022 20:04:47 - INFO - __main__ - Step 710 Global step 710 Train loss 4.36 on epoch=354
06/23/2022 20:04:48 - INFO - __main__ - Step 720 Global step 720 Train loss 4.34 on epoch=359
06/23/2022 20:04:50 - INFO - __main__ - Step 730 Global step 730 Train loss 4.35 on epoch=364
06/23/2022 20:04:51 - INFO - __main__ - Step 740 Global step 740 Train loss 4.21 on epoch=369
06/23/2022 20:04:53 - INFO - __main__ - Step 750 Global step 750 Train loss 4.33 on epoch=374
06/23/2022 20:04:55 - INFO - __main__ - Global step 750 Train loss 4.32 ACC 0.0 on epoch=374
06/23/2022 20:04:56 - INFO - __main__ - Step 760 Global step 760 Train loss 4.17 on epoch=379
06/23/2022 20:04:58 - INFO - __main__ - Step 770 Global step 770 Train loss 4.27 on epoch=384
06/23/2022 20:04:59 - INFO - __main__ - Step 780 Global step 780 Train loss 4.03 on epoch=389
06/23/2022 20:05:01 - INFO - __main__ - Step 790 Global step 790 Train loss 4.10 on epoch=394
06/23/2022 20:05:02 - INFO - __main__ - Step 800 Global step 800 Train loss 4.00 on epoch=399
06/23/2022 20:05:05 - INFO - __main__ - Global step 800 Train loss 4.11 ACC 0.0 on epoch=399
06/23/2022 20:05:06 - INFO - __main__ - Step 810 Global step 810 Train loss 4.10 on epoch=404
06/23/2022 20:05:07 - INFO - __main__ - Step 820 Global step 820 Train loss 3.95 on epoch=409
06/23/2022 20:05:09 - INFO - __main__ - Step 830 Global step 830 Train loss 4.03 on epoch=414
06/23/2022 20:05:10 - INFO - __main__ - Step 840 Global step 840 Train loss 3.85 on epoch=419
06/23/2022 20:05:12 - INFO - __main__ - Step 850 Global step 850 Train loss 3.88 on epoch=424
06/23/2022 20:05:15 - INFO - __main__ - Global step 850 Train loss 3.96 ACC 0.0 on epoch=424
06/23/2022 20:05:16 - INFO - __main__ - Step 860 Global step 860 Train loss 3.80 on epoch=429
06/23/2022 20:05:18 - INFO - __main__ - Step 870 Global step 870 Train loss 3.69 on epoch=434
06/23/2022 20:05:19 - INFO - __main__ - Step 880 Global step 880 Train loss 3.71 on epoch=439
06/23/2022 20:05:20 - INFO - __main__ - Step 890 Global step 890 Train loss 3.77 on epoch=444
06/23/2022 20:05:22 - INFO - __main__ - Step 900 Global step 900 Train loss 3.68 on epoch=449
06/23/2022 20:05:24 - INFO - __main__ - Global step 900 Train loss 3.73 ACC 0.0 on epoch=449
06/23/2022 20:05:25 - INFO - __main__ - Step 910 Global step 910 Train loss 3.56 on epoch=454
06/23/2022 20:05:27 - INFO - __main__ - Step 920 Global step 920 Train loss 3.50 on epoch=459
06/23/2022 20:05:28 - INFO - __main__ - Step 930 Global step 930 Train loss 3.49 on epoch=464
06/23/2022 20:05:29 - INFO - __main__ - Step 940 Global step 940 Train loss 3.38 on epoch=469
06/23/2022 20:05:31 - INFO - __main__ - Step 950 Global step 950 Train loss 3.36 on epoch=474
06/23/2022 20:05:37 - INFO - __main__ - Global step 950 Train loss 3.46 ACC 0.3125 on epoch=474
06/23/2022 20:05:37 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.3125 on epoch=474, global_step=950
06/23/2022 20:05:38 - INFO - __main__ - Step 960 Global step 960 Train loss 3.41 on epoch=479
06/23/2022 20:05:39 - INFO - __main__ - Step 970 Global step 970 Train loss 3.22 on epoch=484
06/23/2022 20:05:41 - INFO - __main__ - Step 980 Global step 980 Train loss 3.23 on epoch=489
06/23/2022 20:05:42 - INFO - __main__ - Step 990 Global step 990 Train loss 3.24 on epoch=494
06/23/2022 20:05:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.21 on epoch=499
06/23/2022 20:05:49 - INFO - __main__ - Global step 1000 Train loss 3.26 ACC 0.21875 on epoch=499
06/23/2022 20:05:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.09 on epoch=504
06/23/2022 20:05:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.03 on epoch=509
06/23/2022 20:05:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.04 on epoch=514
06/23/2022 20:05:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.03 on epoch=519
06/23/2022 20:05:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.94 on epoch=524
06/23/2022 20:05:59 - INFO - __main__ - Global step 1050 Train loss 3.03 ACC 0.3125 on epoch=524
06/23/2022 20:06:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.82 on epoch=529
06/23/2022 20:06:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.90 on epoch=534
06/23/2022 20:06:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.86 on epoch=539
06/23/2022 20:06:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.82 on epoch=544
06/23/2022 20:06:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.83 on epoch=549
06/23/2022 20:06:10 - INFO - __main__ - Global step 1100 Train loss 2.84 ACC 0.125 on epoch=549
06/23/2022 20:06:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.80 on epoch=554
06/23/2022 20:06:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 2.55 on epoch=559
06/23/2022 20:06:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.63 on epoch=564
06/23/2022 20:06:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 2.51 on epoch=569
06/23/2022 20:06:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 2.56 on epoch=574
06/23/2022 20:06:20 - INFO - __main__ - Global step 1150 Train loss 2.61 ACC 0.28125 on epoch=574
06/23/2022 20:06:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.52 on epoch=579
06/23/2022 20:06:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.43 on epoch=584
06/23/2022 20:06:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 2.35 on epoch=589
06/23/2022 20:06:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.35 on epoch=594
06/23/2022 20:06:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.42 on epoch=599
06/23/2022 20:06:30 - INFO - __main__ - Global step 1200 Train loss 2.41 ACC 0.03125 on epoch=599
06/23/2022 20:06:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.31 on epoch=604
06/23/2022 20:06:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.23 on epoch=609
06/23/2022 20:06:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.24 on epoch=614
06/23/2022 20:06:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.20 on epoch=619
06/23/2022 20:06:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.25 on epoch=624
06/23/2022 20:06:41 - INFO - __main__ - Global step 1250 Train loss 2.25 ACC 0.1875 on epoch=624
06/23/2022 20:06:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.11 on epoch=629
06/23/2022 20:06:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.13 on epoch=634
06/23/2022 20:06:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.14 on epoch=639
06/23/2022 20:06:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.08 on epoch=644
06/23/2022 20:06:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.16 on epoch=649
06/23/2022 20:06:52 - INFO - __main__ - Global step 1300 Train loss 2.12 ACC 0.125 on epoch=649
06/23/2022 20:06:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.03 on epoch=654
06/23/2022 20:06:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.97 on epoch=659
06/23/2022 20:06:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.98 on epoch=664
06/23/2022 20:06:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.98 on epoch=669
06/23/2022 20:06:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.96 on epoch=674
06/23/2022 20:07:03 - INFO - __main__ - Global step 1350 Train loss 1.98 ACC 0.46875 on epoch=674
06/23/2022 20:07:03 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.46875 on epoch=674, global_step=1350
06/23/2022 20:07:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.87 on epoch=679
06/23/2022 20:07:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.81 on epoch=684
06/23/2022 20:07:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.88 on epoch=689
06/23/2022 20:07:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.87 on epoch=694
06/23/2022 20:07:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.75 on epoch=699
06/23/2022 20:07:13 - INFO - __main__ - Global step 1400 Train loss 1.83 ACC 0.46875 on epoch=699
06/23/2022 20:07:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.69 on epoch=704
06/23/2022 20:07:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.69 on epoch=709
06/23/2022 20:07:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.77 on epoch=714
06/23/2022 20:07:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.79 on epoch=719
06/23/2022 20:07:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.63 on epoch=724
06/23/2022 20:07:23 - INFO - __main__ - Global step 1450 Train loss 1.71 ACC 0.5 on epoch=724
06/23/2022 20:07:24 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=724, global_step=1450
06/23/2022 20:07:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.68 on epoch=729
06/23/2022 20:07:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.61 on epoch=734
06/23/2022 20:07:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.60 on epoch=739
06/23/2022 20:07:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.60 on epoch=744
06/23/2022 20:07:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.46 on epoch=749
06/23/2022 20:07:34 - INFO - __main__ - Global step 1500 Train loss 1.59 ACC 0.5 on epoch=749
06/23/2022 20:07:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.41 on epoch=754
06/23/2022 20:07:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.48 on epoch=759
06/23/2022 20:07:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.33 on epoch=764
06/23/2022 20:07:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.45 on epoch=769
06/23/2022 20:07:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.44 on epoch=774
06/23/2022 20:07:44 - INFO - __main__ - Global step 1550 Train loss 1.42 ACC 0.5 on epoch=774
06/23/2022 20:07:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.36 on epoch=779
06/23/2022 20:07:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.28 on epoch=784
06/23/2022 20:07:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.34 on epoch=789
06/23/2022 20:07:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.26 on epoch=794
06/23/2022 20:07:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.20 on epoch=799
06/23/2022 20:07:54 - INFO - __main__ - Global step 1600 Train loss 1.29 ACC 0.5 on epoch=799
06/23/2022 20:07:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.25 on epoch=804
06/23/2022 20:07:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.22 on epoch=809
06/23/2022 20:07:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.21 on epoch=814
06/23/2022 20:07:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.08 on epoch=819
06/23/2022 20:08:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.19 on epoch=824
06/23/2022 20:08:03 - INFO - __main__ - Global step 1650 Train loss 1.19 ACC 0.5 on epoch=824
06/23/2022 20:08:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.16 on epoch=829
06/23/2022 20:08:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.12 on epoch=834
06/23/2022 20:08:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.12 on epoch=839
06/23/2022 20:08:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.14 on epoch=844
06/23/2022 20:08:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.12 on epoch=849
06/23/2022 20:08:13 - INFO - __main__ - Global step 1700 Train loss 1.13 ACC 0.5 on epoch=849
06/23/2022 20:08:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.02 on epoch=854
06/23/2022 20:08:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.02 on epoch=859
06/23/2022 20:08:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.98 on epoch=864
06/23/2022 20:08:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.98 on epoch=869
06/23/2022 20:08:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.97 on epoch=874
06/23/2022 20:08:21 - INFO - __main__ - Global step 1750 Train loss 0.99 ACC 0.5 on epoch=874
06/23/2022 20:08:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.98 on epoch=879
06/23/2022 20:08:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.92 on epoch=884
06/23/2022 20:08:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.97 on epoch=889
06/23/2022 20:08:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.90 on epoch=894
06/23/2022 20:08:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.93 on epoch=899
06/23/2022 20:08:30 - INFO - __main__ - Global step 1800 Train loss 0.94 ACC 0.5 on epoch=899
06/23/2022 20:08:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.89 on epoch=904
06/23/2022 20:08:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.86 on epoch=909
06/23/2022 20:08:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.88 on epoch=914
06/23/2022 20:08:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.85 on epoch=919
06/23/2022 20:08:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.84 on epoch=924
06/23/2022 20:08:39 - INFO - __main__ - Global step 1850 Train loss 0.87 ACC 0.5 on epoch=924
06/23/2022 20:08:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.84 on epoch=929
06/23/2022 20:08:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.78 on epoch=934
06/23/2022 20:08:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.75 on epoch=939
06/23/2022 20:08:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.76 on epoch=944
06/23/2022 20:08:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.83 on epoch=949
06/23/2022 20:08:49 - INFO - __main__ - Global step 1900 Train loss 0.79 ACC 0.5 on epoch=949
06/23/2022 20:08:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.79 on epoch=954
06/23/2022 20:08:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.77 on epoch=959
06/23/2022 20:08:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.75 on epoch=964
06/23/2022 20:08:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.70 on epoch=969
06/23/2022 20:08:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.75 on epoch=974
06/23/2022 20:08:59 - INFO - __main__ - Global step 1950 Train loss 0.75 ACC 0.5 on epoch=974
06/23/2022 20:09:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.76 on epoch=979
06/23/2022 20:09:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.81 on epoch=984
06/23/2022 20:09:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.71 on epoch=989
06/23/2022 20:09:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.90 on epoch=994
06/23/2022 20:09:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.69 on epoch=999
06/23/2022 20:09:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 20:09:07 - INFO - __main__ - Printing 3 examples
06/23/2022 20:09:07 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/23/2022 20:09:07 - INFO - __main__ - ['duplicate']
06/23/2022 20:09:07 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/23/2022 20:09:07 - INFO - __main__ - ['duplicate']
06/23/2022 20:09:07 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/23/2022 20:09:07 - INFO - __main__ - ['duplicate']
06/23/2022 20:09:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 20:09:07 - INFO - __main__ - Tokenizing Output ...
06/23/2022 20:09:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 20:09:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 20:09:08 - INFO - __main__ - Printing 3 examples
06/23/2022 20:09:08 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/23/2022 20:09:08 - INFO - __main__ - ['duplicate']
06/23/2022 20:09:08 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/23/2022 20:09:08 - INFO - __main__ - ['duplicate']
06/23/2022 20:09:08 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/23/2022 20:09:08 - INFO - __main__ - ['duplicate']
06/23/2022 20:09:08 - INFO - __main__ - Tokenizing Input ...
06/23/2022 20:09:08 - INFO - __main__ - Tokenizing Output ...
06/23/2022 20:09:08 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 20:09:09 - INFO - __main__ - Global step 2000 Train loss 0.78 ACC 0.5 on epoch=999
06/23/2022 20:09:09 - INFO - __main__ - save last model!
06/23/2022 20:09:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 20:09:09 - INFO - __main__ - Start tokenizing ... 40430 instances
06/23/2022 20:09:09 - INFO - __main__ - Printing 3 examples
06/23/2022 20:09:09 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/23/2022 20:09:09 - INFO - __main__ - ['not_duplicate']
06/23/2022 20:09:09 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/23/2022 20:09:09 - INFO - __main__ - ['not_duplicate']
06/23/2022 20:09:09 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/23/2022 20:09:09 - INFO - __main__ - ['duplicate']
06/23/2022 20:09:09 - INFO - __main__ - Tokenizing Input ...
06/23/2022 20:09:14 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 20:09:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 20:09:14 - INFO - __main__ - Starting training!
06/23/2022 20:09:30 - INFO - __main__ - Tokenizing Output ...
06/23/2022 20:10:12 - INFO - __main__ - Loaded 40430 examples from test data
06/23/2022 20:51:44 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_100_0.2_8_predictions.txt
06/23/2022 20:51:44 - INFO - __main__ - ACC on test data: 0.3682
06/23/2022 20:51:44 - INFO - __main__ - prefix=glue-qqp_16_100, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.36816720257234725
06/23/2022 20:51:44 - INFO - __main__ - Running ... prefix=glue-qqp_16_13, lr=0.5, bsz=8 ...
06/23/2022 20:51:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 20:51:45 - INFO - __main__ - Printing 3 examples
06/23/2022 20:51:45 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/23/2022 20:51:45 - INFO - __main__ - ['duplicate']
06/23/2022 20:51:45 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/23/2022 20:51:45 - INFO - __main__ - ['duplicate']
06/23/2022 20:51:45 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/23/2022 20:51:45 - INFO - __main__ - ['duplicate']
06/23/2022 20:51:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 20:51:45 - INFO - __main__ - Tokenizing Output ...
06/23/2022 20:51:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 20:51:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 20:51:46 - INFO - __main__ - Printing 3 examples
06/23/2022 20:51:46 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/23/2022 20:51:46 - INFO - __main__ - ['duplicate']
06/23/2022 20:51:46 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/23/2022 20:51:46 - INFO - __main__ - ['duplicate']
06/23/2022 20:51:46 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/23/2022 20:51:46 - INFO - __main__ - ['duplicate']
06/23/2022 20:51:46 - INFO - __main__ - Tokenizing Input ...
06/23/2022 20:51:46 - INFO - __main__ - Tokenizing Output ...
06/23/2022 20:51:46 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 20:51:51 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 20:51:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 20:51:51 - INFO - __main__ - Starting training!
06/23/2022 20:51:53 - INFO - __main__ - Step 10 Global step 10 Train loss 7.19 on epoch=4
06/23/2022 20:51:54 - INFO - __main__ - Step 20 Global step 20 Train loss 7.14 on epoch=9
06/23/2022 20:51:55 - INFO - __main__ - Step 30 Global step 30 Train loss 7.15 on epoch=14
06/23/2022 20:51:57 - INFO - __main__ - Step 40 Global step 40 Train loss 7.09 on epoch=19
06/23/2022 20:51:58 - INFO - __main__ - Step 50 Global step 50 Train loss 7.09 on epoch=24
06/23/2022 20:52:01 - INFO - __main__ - Global step 50 Train loss 7.13 ACC 0.0 on epoch=24
06/23/2022 20:52:01 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 20:52:03 - INFO - __main__ - Step 60 Global step 60 Train loss 7.10 on epoch=29
06/23/2022 20:52:04 - INFO - __main__ - Step 70 Global step 70 Train loss 7.02 on epoch=34
06/23/2022 20:52:05 - INFO - __main__ - Step 80 Global step 80 Train loss 7.02 on epoch=39
06/23/2022 20:52:07 - INFO - __main__ - Step 90 Global step 90 Train loss 7.04 on epoch=44
06/23/2022 20:52:08 - INFO - __main__ - Step 100 Global step 100 Train loss 7.08 on epoch=49
06/23/2022 20:52:11 - INFO - __main__ - Global step 100 Train loss 7.05 ACC 0.0 on epoch=49
06/23/2022 20:52:13 - INFO - __main__ - Step 110 Global step 110 Train loss 6.98 on epoch=54
06/23/2022 20:52:14 - INFO - __main__ - Step 120 Global step 120 Train loss 6.92 on epoch=59
06/23/2022 20:52:15 - INFO - __main__ - Step 130 Global step 130 Train loss 6.88 on epoch=64
06/23/2022 20:52:17 - INFO - __main__ - Step 140 Global step 140 Train loss 6.82 on epoch=69
06/23/2022 20:52:18 - INFO - __main__ - Step 150 Global step 150 Train loss 6.73 on epoch=74
06/23/2022 20:52:24 - INFO - __main__ - Global step 150 Train loss 6.87 ACC 0.0 on epoch=74
06/23/2022 20:52:26 - INFO - __main__ - Step 160 Global step 160 Train loss 6.52 on epoch=79
06/23/2022 20:52:27 - INFO - __main__ - Step 170 Global step 170 Train loss 6.38 on epoch=84
06/23/2022 20:52:28 - INFO - __main__ - Step 180 Global step 180 Train loss 6.26 on epoch=89
06/23/2022 20:52:30 - INFO - __main__ - Step 190 Global step 190 Train loss 5.97 on epoch=94
06/23/2022 20:52:31 - INFO - __main__ - Step 200 Global step 200 Train loss 5.91 on epoch=99
06/23/2022 20:52:34 - INFO - __main__ - Global step 200 Train loss 6.21 ACC 0.0 on epoch=99
06/23/2022 20:52:35 - INFO - __main__ - Step 210 Global step 210 Train loss 5.70 on epoch=104
06/23/2022 20:52:36 - INFO - __main__ - Step 220 Global step 220 Train loss 5.53 on epoch=109
06/23/2022 20:52:38 - INFO - __main__ - Step 230 Global step 230 Train loss 5.50 on epoch=114
06/23/2022 20:52:39 - INFO - __main__ - Step 240 Global step 240 Train loss 5.38 on epoch=119
06/23/2022 20:52:40 - INFO - __main__ - Step 250 Global step 250 Train loss 5.29 on epoch=124
06/23/2022 20:52:47 - INFO - __main__ - Global step 250 Train loss 5.48 ACC 0.0 on epoch=124
06/23/2022 20:52:49 - INFO - __main__ - Step 260 Global step 260 Train loss 5.06 on epoch=129
06/23/2022 20:52:50 - INFO - __main__ - Step 270 Global step 270 Train loss 5.05 on epoch=134
06/23/2022 20:52:52 - INFO - __main__ - Step 280 Global step 280 Train loss 4.89 on epoch=139
06/23/2022 20:52:53 - INFO - __main__ - Step 290 Global step 290 Train loss 4.70 on epoch=144
06/23/2022 20:52:55 - INFO - __main__ - Step 300 Global step 300 Train loss 4.48 on epoch=149
06/23/2022 20:53:03 - INFO - __main__ - Global step 300 Train loss 4.84 ACC 0.0 on epoch=149
06/23/2022 20:53:04 - INFO - __main__ - Step 310 Global step 310 Train loss 4.33 on epoch=154
06/23/2022 20:53:06 - INFO - __main__ - Step 320 Global step 320 Train loss 4.10 on epoch=159
06/23/2022 20:53:08 - INFO - __main__ - Step 330 Global step 330 Train loss 3.97 on epoch=164
06/23/2022 20:53:09 - INFO - __main__ - Step 340 Global step 340 Train loss 3.93 on epoch=169
06/23/2022 20:53:10 - INFO - __main__ - Step 350 Global step 350 Train loss 3.79 on epoch=174
06/23/2022 20:53:21 - INFO - __main__ - Global step 350 Train loss 4.02 ACC 0.0625 on epoch=174
06/23/2022 20:53:21 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=174, global_step=350
06/23/2022 20:53:22 - INFO - __main__ - Step 360 Global step 360 Train loss 3.70 on epoch=179
06/23/2022 20:53:23 - INFO - __main__ - Step 370 Global step 370 Train loss 3.61 on epoch=184
06/23/2022 20:53:25 - INFO - __main__ - Step 380 Global step 380 Train loss 3.48 on epoch=189
06/23/2022 20:53:26 - INFO - __main__ - Step 390 Global step 390 Train loss 3.39 on epoch=194
06/23/2022 20:53:28 - INFO - __main__ - Step 400 Global step 400 Train loss 3.21 on epoch=199
06/23/2022 20:53:31 - INFO - __main__ - Global step 400 Train loss 3.48 ACC 0.0 on epoch=199
06/23/2022 20:53:32 - INFO - __main__ - Step 410 Global step 410 Train loss 3.29 on epoch=204
06/23/2022 20:53:34 - INFO - __main__ - Step 420 Global step 420 Train loss 3.04 on epoch=209
06/23/2022 20:53:35 - INFO - __main__ - Step 430 Global step 430 Train loss 2.98 on epoch=214
06/23/2022 20:53:36 - INFO - __main__ - Step 440 Global step 440 Train loss 2.92 on epoch=219
06/23/2022 20:53:38 - INFO - __main__ - Step 450 Global step 450 Train loss 2.75 on epoch=224
06/23/2022 20:53:41 - INFO - __main__ - Global step 450 Train loss 3.00 ACC 0.4375 on epoch=224
06/23/2022 20:53:41 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.4375 on epoch=224, global_step=450
06/23/2022 20:53:43 - INFO - __main__ - Step 460 Global step 460 Train loss 2.83 on epoch=229
06/23/2022 20:53:44 - INFO - __main__ - Step 470 Global step 470 Train loss 2.69 on epoch=234
06/23/2022 20:53:45 - INFO - __main__ - Step 480 Global step 480 Train loss 2.58 on epoch=239
06/23/2022 20:53:47 - INFO - __main__ - Step 490 Global step 490 Train loss 2.42 on epoch=244
06/23/2022 20:53:48 - INFO - __main__ - Step 500 Global step 500 Train loss 2.30 on epoch=249
06/23/2022 20:53:53 - INFO - __main__ - Global step 500 Train loss 2.56 ACC 0.46875 on epoch=249
06/23/2022 20:53:53 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.46875 on epoch=249, global_step=500
06/23/2022 20:53:54 - INFO - __main__ - Step 510 Global step 510 Train loss 2.15 on epoch=254
06/23/2022 20:53:56 - INFO - __main__ - Step 520 Global step 520 Train loss 2.15 on epoch=259
06/23/2022 20:53:57 - INFO - __main__ - Step 530 Global step 530 Train loss 2.13 on epoch=264
06/23/2022 20:53:58 - INFO - __main__ - Step 540 Global step 540 Train loss 2.00 on epoch=269
06/23/2022 20:54:00 - INFO - __main__ - Step 550 Global step 550 Train loss 1.97 on epoch=274
06/23/2022 20:54:03 - INFO - __main__ - Global step 550 Train loss 2.08 ACC 0.5 on epoch=274
06/23/2022 20:54:03 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=274, global_step=550
06/23/2022 20:54:05 - INFO - __main__ - Step 560 Global step 560 Train loss 1.86 on epoch=279
06/23/2022 20:54:06 - INFO - __main__ - Step 570 Global step 570 Train loss 1.81 on epoch=284
06/23/2022 20:54:08 - INFO - __main__ - Step 580 Global step 580 Train loss 1.72 on epoch=289
06/23/2022 20:54:09 - INFO - __main__ - Step 590 Global step 590 Train loss 1.66 on epoch=294
06/23/2022 20:54:10 - INFO - __main__ - Step 600 Global step 600 Train loss 1.49 on epoch=299
06/23/2022 20:54:12 - INFO - __main__ - Global step 600 Train loss 1.71 ACC 0.5 on epoch=299
06/23/2022 20:54:14 - INFO - __main__ - Step 610 Global step 610 Train loss 1.41 on epoch=304
06/23/2022 20:54:15 - INFO - __main__ - Step 620 Global step 620 Train loss 1.49 on epoch=309
06/23/2022 20:54:17 - INFO - __main__ - Step 630 Global step 630 Train loss 1.39 on epoch=314
06/23/2022 20:54:18 - INFO - __main__ - Step 640 Global step 640 Train loss 1.21 on epoch=319
06/23/2022 20:54:19 - INFO - __main__ - Step 650 Global step 650 Train loss 1.18 on epoch=324
06/23/2022 20:54:22 - INFO - __main__ - Global step 650 Train loss 1.34 ACC 0.5 on epoch=324
06/23/2022 20:54:23 - INFO - __main__ - Step 660 Global step 660 Train loss 1.07 on epoch=329
06/23/2022 20:54:25 - INFO - __main__ - Step 670 Global step 670 Train loss 1.06 on epoch=334
06/23/2022 20:54:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.93 on epoch=339
06/23/2022 20:54:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.92 on epoch=344
06/23/2022 20:54:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.86 on epoch=349
06/23/2022 20:54:32 - INFO - __main__ - Global step 700 Train loss 0.97 ACC 0.5 on epoch=349
06/23/2022 20:54:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.88 on epoch=354
06/23/2022 20:54:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.77 on epoch=359
06/23/2022 20:54:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.81 on epoch=364
06/23/2022 20:54:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.65 on epoch=369
06/23/2022 20:54:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.73 on epoch=374
06/23/2022 20:54:42 - INFO - __main__ - Global step 750 Train loss 0.77 ACC 0.5 on epoch=374
06/23/2022 20:54:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.75 on epoch=379
06/23/2022 20:54:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.75 on epoch=384
06/23/2022 20:54:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.63 on epoch=389
06/23/2022 20:54:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.71 on epoch=394
06/23/2022 20:54:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.59 on epoch=399
06/23/2022 20:54:52 - INFO - __main__ - Global step 800 Train loss 0.69 ACC 0.5 on epoch=399
06/23/2022 20:54:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.62 on epoch=404
06/23/2022 20:54:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.56 on epoch=409
06/23/2022 20:54:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.58 on epoch=414
06/23/2022 20:54:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.62 on epoch=419
06/23/2022 20:54:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.62 on epoch=424
06/23/2022 20:55:02 - INFO - __main__ - Global step 850 Train loss 0.60 ACC 0.5 on epoch=424
06/23/2022 20:55:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.57 on epoch=429
06/23/2022 20:55:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.57 on epoch=434
06/23/2022 20:55:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.55 on epoch=439
06/23/2022 20:55:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.52 on epoch=444
06/23/2022 20:55:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=449
06/23/2022 20:55:12 - INFO - __main__ - Global step 900 Train loss 0.54 ACC 0.5 on epoch=449
06/23/2022 20:55:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.57 on epoch=454
06/23/2022 20:55:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.60 on epoch=459
06/23/2022 20:55:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.54 on epoch=464
06/23/2022 20:55:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.55 on epoch=469
06/23/2022 20:55:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=474
06/23/2022 20:55:22 - INFO - __main__ - Global step 950 Train loss 0.54 ACC 0.5 on epoch=474
06/23/2022 20:55:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=479
06/23/2022 20:55:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=484
06/23/2022 20:55:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.47 on epoch=489
06/23/2022 20:55:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=494
06/23/2022 20:55:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=499
06/23/2022 20:55:32 - INFO - __main__ - Global step 1000 Train loss 0.46 ACC 0.5 on epoch=499
06/23/2022 20:55:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=504
06/23/2022 20:55:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=509
06/23/2022 20:55:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=514
06/23/2022 20:55:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.51 on epoch=519
06/23/2022 20:55:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=524
06/23/2022 20:55:42 - INFO - __main__ - Global step 1050 Train loss 0.45 ACC 0.5 on epoch=524
06/23/2022 20:55:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=529
06/23/2022 20:55:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=534
06/23/2022 20:55:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=539
06/23/2022 20:55:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=544
06/23/2022 20:55:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=549
06/23/2022 20:55:52 - INFO - __main__ - Global step 1100 Train loss 0.42 ACC 0.5 on epoch=549
06/23/2022 20:55:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=554
06/23/2022 20:55:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=559
06/23/2022 20:55:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=564
06/23/2022 20:55:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=569
06/23/2022 20:55:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=574
06/23/2022 20:56:02 - INFO - __main__ - Global step 1150 Train loss 0.43 ACC 0.5 on epoch=574
06/23/2022 20:56:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=579
06/23/2022 20:56:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=584
06/23/2022 20:56:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=589
06/23/2022 20:56:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=594
06/23/2022 20:56:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=599
06/23/2022 20:56:11 - INFO - __main__ - Global step 1200 Train loss 0.44 ACC 0.5 on epoch=599
06/23/2022 20:56:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=604
06/23/2022 20:56:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=609
06/23/2022 20:56:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=614
06/23/2022 20:56:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=619
06/23/2022 20:56:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=624
06/23/2022 20:56:21 - INFO - __main__ - Global step 1250 Train loss 0.41 ACC 0.5 on epoch=624
06/23/2022 20:56:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=629
06/23/2022 20:56:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=634
06/23/2022 20:56:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=639
06/23/2022 20:56:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.43 on epoch=644
06/23/2022 20:56:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=649
06/23/2022 20:56:30 - INFO - __main__ - Global step 1300 Train loss 0.40 ACC 0.5 on epoch=649
06/23/2022 20:56:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=654
06/23/2022 20:56:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=659
06/23/2022 20:56:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=664
06/23/2022 20:56:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=669
06/23/2022 20:56:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=674
06/23/2022 20:56:38 - INFO - __main__ - Global step 1350 Train loss 0.39 ACC 0.5 on epoch=674
06/23/2022 20:56:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=679
06/23/2022 20:56:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=684
06/23/2022 20:56:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.44 on epoch=689
06/23/2022 20:56:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=694
06/23/2022 20:56:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=699
06/23/2022 20:56:47 - INFO - __main__ - Global step 1400 Train loss 0.39 ACC 0.5 on epoch=699
06/23/2022 20:56:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=704
06/23/2022 20:56:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.38 on epoch=709
06/23/2022 20:56:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=714
06/23/2022 20:56:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=719
06/23/2022 20:56:54 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=724
06/23/2022 20:56:56 - INFO - __main__ - Global step 1450 Train loss 0.36 ACC 0.5 on epoch=724
06/23/2022 20:56:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=729
06/23/2022 20:56:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=734
06/23/2022 20:57:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=739
06/23/2022 20:57:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=744
06/23/2022 20:57:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.32 on epoch=749
06/23/2022 20:57:05 - INFO - __main__ - Global step 1500 Train loss 0.36 ACC 0.5 on epoch=749
06/23/2022 20:57:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=754
06/23/2022 20:57:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=759
06/23/2022 20:57:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.33 on epoch=764
06/23/2022 20:57:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.29 on epoch=769
06/23/2022 20:57:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=774
06/23/2022 20:57:13 - INFO - __main__ - Global step 1550 Train loss 0.32 ACC 0.5 on epoch=774
06/23/2022 20:57:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=779
06/23/2022 20:57:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=784
06/23/2022 20:57:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=789
06/23/2022 20:57:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=794
06/23/2022 20:57:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=799
06/23/2022 20:57:22 - INFO - __main__ - Global step 1600 Train loss 0.31 ACC 0.5 on epoch=799
06/23/2022 20:57:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=804
06/23/2022 20:57:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=809
06/23/2022 20:57:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.33 on epoch=814
06/23/2022 20:57:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=819
06/23/2022 20:57:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=824
06/23/2022 20:57:30 - INFO - __main__ - Global step 1650 Train loss 0.35 ACC 0.5 on epoch=824
06/23/2022 20:57:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=829
06/23/2022 20:57:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=834
06/23/2022 20:57:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.29 on epoch=839
06/23/2022 20:57:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=844
06/23/2022 20:57:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.35 on epoch=849
06/23/2022 20:57:37 - INFO - __main__ - Global step 1700 Train loss 0.32 ACC 0.5 on epoch=849
06/23/2022 20:57:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.27 on epoch=854
06/23/2022 20:57:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.30 on epoch=859
06/23/2022 20:57:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=864
06/23/2022 20:57:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=869
06/23/2022 20:57:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=874
06/23/2022 20:57:45 - INFO - __main__ - Global step 1750 Train loss 0.30 ACC 0.5 on epoch=874
06/23/2022 20:57:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=879
06/23/2022 20:57:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.32 on epoch=884
06/23/2022 20:57:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=889
06/23/2022 20:57:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=894
06/23/2022 20:57:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=899
06/23/2022 20:57:52 - INFO - __main__ - Global step 1800 Train loss 0.32 ACC 0.5 on epoch=899
06/23/2022 20:57:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.26 on epoch=904
06/23/2022 20:57:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=909
06/23/2022 20:57:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=914
06/23/2022 20:57:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=919
06/23/2022 20:57:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=924
06/23/2022 20:58:00 - INFO - __main__ - Global step 1850 Train loss 0.32 ACC 0.5 on epoch=924
06/23/2022 20:58:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=929
06/23/2022 20:58:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=934
06/23/2022 20:58:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=939
06/23/2022 20:58:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=944
06/23/2022 20:58:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=949
06/23/2022 20:58:07 - INFO - __main__ - Global step 1900 Train loss 0.30 ACC 0.5 on epoch=949
06/23/2022 20:58:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.30 on epoch=954
06/23/2022 20:58:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
06/23/2022 20:58:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=964
06/23/2022 20:58:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=969
06/23/2022 20:58:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=974
06/23/2022 20:58:15 - INFO - __main__ - Global step 1950 Train loss 0.30 ACC 0.5 on epoch=974
06/23/2022 20:58:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
06/23/2022 20:58:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=984
06/23/2022 20:58:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.30 on epoch=989
06/23/2022 20:58:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=994
06/23/2022 20:58:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=999
06/23/2022 20:58:22 - INFO - __main__ - Global step 2000 Train loss 0.32 ACC 0.5 on epoch=999
06/23/2022 20:58:22 - INFO - __main__ - save last model!
06/23/2022 20:58:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 20:58:22 - INFO - __main__ - Start tokenizing ... 40430 instances
06/23/2022 20:58:22 - INFO - __main__ - Printing 3 examples
06/23/2022 20:58:22 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/23/2022 20:58:22 - INFO - __main__ - ['not_duplicate']
06/23/2022 20:58:22 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/23/2022 20:58:22 - INFO - __main__ - ['not_duplicate']
06/23/2022 20:58:22 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/23/2022 20:58:22 - INFO - __main__ - ['duplicate']
06/23/2022 20:58:22 - INFO - __main__ - Tokenizing Input ...
06/23/2022 20:58:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 20:58:23 - INFO - __main__ - Printing 3 examples
06/23/2022 20:58:23 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/23/2022 20:58:23 - INFO - __main__ - ['duplicate']
06/23/2022 20:58:23 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/23/2022 20:58:23 - INFO - __main__ - ['duplicate']
06/23/2022 20:58:23 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/23/2022 20:58:23 - INFO - __main__ - ['duplicate']
06/23/2022 20:58:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 20:58:23 - INFO - __main__ - Tokenizing Output ...
06/23/2022 20:58:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 20:58:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 20:58:23 - INFO - __main__ - Printing 3 examples
06/23/2022 20:58:23 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/23/2022 20:58:23 - INFO - __main__ - ['duplicate']
06/23/2022 20:58:23 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/23/2022 20:58:23 - INFO - __main__ - ['duplicate']
06/23/2022 20:58:23 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/23/2022 20:58:23 - INFO - __main__ - ['duplicate']
06/23/2022 20:58:23 - INFO - __main__ - Tokenizing Input ...
06/23/2022 20:58:23 - INFO - __main__ - Tokenizing Output ...
06/23/2022 20:58:23 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 20:58:29 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 20:58:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 20:58:30 - INFO - __main__ - Starting training!
06/23/2022 20:58:41 - INFO - __main__ - Tokenizing Output ...
06/23/2022 20:59:22 - INFO - __main__ - Loaded 40430 examples from test data
06/23/2022 21:11:49 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_13_0.5_8_predictions.txt
06/23/2022 21:11:49 - INFO - __main__ - ACC on test data: 0.3823
06/23/2022 21:11:50 - INFO - __main__ - prefix=glue-qqp_16_13, lr=0.5, bsz=8, dev_performance=0.5, test_performance=0.3823151125401929
06/23/2022 21:11:50 - INFO - __main__ - Running ... prefix=glue-qqp_16_13, lr=0.4, bsz=8 ...
06/23/2022 21:11:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 21:11:50 - INFO - __main__ - Printing 3 examples
06/23/2022 21:11:50 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/23/2022 21:11:50 - INFO - __main__ - ['duplicate']
06/23/2022 21:11:50 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/23/2022 21:11:50 - INFO - __main__ - ['duplicate']
06/23/2022 21:11:50 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/23/2022 21:11:50 - INFO - __main__ - ['duplicate']
06/23/2022 21:11:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 21:11:50 - INFO - __main__ - Tokenizing Output ...
06/23/2022 21:11:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 21:11:51 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 21:11:51 - INFO - __main__ - Printing 3 examples
06/23/2022 21:11:51 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/23/2022 21:11:51 - INFO - __main__ - ['duplicate']
06/23/2022 21:11:51 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/23/2022 21:11:51 - INFO - __main__ - ['duplicate']
06/23/2022 21:11:51 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/23/2022 21:11:51 - INFO - __main__ - ['duplicate']
06/23/2022 21:11:51 - INFO - __main__ - Tokenizing Input ...
06/23/2022 21:11:51 - INFO - __main__ - Tokenizing Output ...
06/23/2022 21:11:51 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 21:11:56 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 21:11:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 21:11:56 - INFO - __main__ - Starting training!
06/23/2022 21:11:58 - INFO - __main__ - Step 10 Global step 10 Train loss 7.15 on epoch=4
06/23/2022 21:11:59 - INFO - __main__ - Step 20 Global step 20 Train loss 7.21 on epoch=9
06/23/2022 21:12:01 - INFO - __main__ - Step 30 Global step 30 Train loss 7.17 on epoch=14
06/23/2022 21:12:02 - INFO - __main__ - Step 40 Global step 40 Train loss 7.07 on epoch=19
06/23/2022 21:12:04 - INFO - __main__ - Step 50 Global step 50 Train loss 7.13 on epoch=24
06/23/2022 21:12:05 - INFO - __main__ - Global step 50 Train loss 7.15 ACC 0.0 on epoch=24
06/23/2022 21:12:05 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 21:12:07 - INFO - __main__ - Step 60 Global step 60 Train loss 7.04 on epoch=29
06/23/2022 21:12:09 - INFO - __main__ - Step 70 Global step 70 Train loss 7.05 on epoch=34
06/23/2022 21:12:10 - INFO - __main__ - Step 80 Global step 80 Train loss 7.09 on epoch=39
06/23/2022 21:12:12 - INFO - __main__ - Step 90 Global step 90 Train loss 7.02 on epoch=44
06/23/2022 21:12:13 - INFO - __main__ - Step 100 Global step 100 Train loss 7.07 on epoch=49
06/23/2022 21:12:24 - INFO - __main__ - Global step 100 Train loss 7.05 ACC 0.0 on epoch=49
06/23/2022 21:12:26 - INFO - __main__ - Step 110 Global step 110 Train loss 6.95 on epoch=54
06/23/2022 21:12:27 - INFO - __main__ - Step 120 Global step 120 Train loss 7.00 on epoch=59
06/23/2022 21:12:29 - INFO - __main__ - Step 130 Global step 130 Train loss 6.95 on epoch=64
06/23/2022 21:12:30 - INFO - __main__ - Step 140 Global step 140 Train loss 6.96 on epoch=69
06/23/2022 21:12:31 - INFO - __main__ - Step 150 Global step 150 Train loss 6.89 on epoch=74
06/23/2022 21:12:38 - INFO - __main__ - Global step 150 Train loss 6.95 ACC 0.0 on epoch=74
06/23/2022 21:12:39 - INFO - __main__ - Step 160 Global step 160 Train loss 6.74 on epoch=79
06/23/2022 21:12:40 - INFO - __main__ - Step 170 Global step 170 Train loss 6.74 on epoch=84
06/23/2022 21:12:42 - INFO - __main__ - Step 180 Global step 180 Train loss 6.70 on epoch=89
06/23/2022 21:12:43 - INFO - __main__ - Step 190 Global step 190 Train loss 6.64 on epoch=94
06/23/2022 21:12:45 - INFO - __main__ - Step 200 Global step 200 Train loss 6.55 on epoch=99
06/23/2022 21:12:45 - INFO - __main__ - Global step 200 Train loss 6.67 ACC 0.0 on epoch=99
06/23/2022 21:12:47 - INFO - __main__ - Step 210 Global step 210 Train loss 6.41 on epoch=104
06/23/2022 21:12:48 - INFO - __main__ - Step 220 Global step 220 Train loss 6.22 on epoch=109
06/23/2022 21:12:50 - INFO - __main__ - Step 230 Global step 230 Train loss 6.22 on epoch=114
06/23/2022 21:12:51 - INFO - __main__ - Step 240 Global step 240 Train loss 6.22 on epoch=119
06/23/2022 21:12:53 - INFO - __main__ - Step 250 Global step 250 Train loss 6.13 on epoch=124
06/23/2022 21:13:03 - INFO - __main__ - Global step 250 Train loss 6.24 ACC 0.0 on epoch=124
06/23/2022 21:13:05 - INFO - __main__ - Step 260 Global step 260 Train loss 6.05 on epoch=129
06/23/2022 21:13:06 - INFO - __main__ - Step 270 Global step 270 Train loss 5.97 on epoch=134
06/23/2022 21:13:07 - INFO - __main__ - Step 280 Global step 280 Train loss 5.95 on epoch=139
06/23/2022 21:13:09 - INFO - __main__ - Step 290 Global step 290 Train loss 5.84 on epoch=144
06/23/2022 21:13:10 - INFO - __main__ - Step 300 Global step 300 Train loss 5.74 on epoch=149
06/23/2022 21:13:17 - INFO - __main__ - Global step 300 Train loss 5.91 ACC 0.0 on epoch=149
06/23/2022 21:13:18 - INFO - __main__ - Step 310 Global step 310 Train loss 5.65 on epoch=154
06/23/2022 21:13:20 - INFO - __main__ - Step 320 Global step 320 Train loss 5.65 on epoch=159
06/23/2022 21:13:21 - INFO - __main__ - Step 330 Global step 330 Train loss 5.61 on epoch=164
06/23/2022 21:13:22 - INFO - __main__ - Step 340 Global step 340 Train loss 5.50 on epoch=169
06/23/2022 21:13:24 - INFO - __main__ - Step 350 Global step 350 Train loss 5.46 on epoch=174
06/23/2022 21:13:30 - INFO - __main__ - Global step 350 Train loss 5.57 ACC 0.0 on epoch=174
06/23/2022 21:13:31 - INFO - __main__ - Step 360 Global step 360 Train loss 5.31 on epoch=179
06/23/2022 21:13:32 - INFO - __main__ - Step 370 Global step 370 Train loss 5.20 on epoch=184
06/23/2022 21:13:34 - INFO - __main__ - Step 380 Global step 380 Train loss 5.05 on epoch=189
06/23/2022 21:13:35 - INFO - __main__ - Step 390 Global step 390 Train loss 5.25 on epoch=194
06/23/2022 21:13:37 - INFO - __main__ - Step 400 Global step 400 Train loss 5.00 on epoch=199
06/23/2022 21:13:44 - INFO - __main__ - Global step 400 Train loss 5.16 ACC 0.0 on epoch=199
06/23/2022 21:13:45 - INFO - __main__ - Step 410 Global step 410 Train loss 4.85 on epoch=204
06/23/2022 21:13:46 - INFO - __main__ - Step 420 Global step 420 Train loss 4.81 on epoch=209
06/23/2022 21:13:48 - INFO - __main__ - Step 430 Global step 430 Train loss 4.55 on epoch=214
06/23/2022 21:13:49 - INFO - __main__ - Step 440 Global step 440 Train loss 4.54 on epoch=219
06/23/2022 21:13:50 - INFO - __main__ - Step 450 Global step 450 Train loss 4.42 on epoch=224
06/23/2022 21:13:57 - INFO - __main__ - Global step 450 Train loss 4.63 ACC 0.0 on epoch=224
06/23/2022 21:13:58 - INFO - __main__ - Step 460 Global step 460 Train loss 4.39 on epoch=229
06/23/2022 21:13:59 - INFO - __main__ - Step 470 Global step 470 Train loss 4.39 on epoch=234
06/23/2022 21:14:01 - INFO - __main__ - Step 480 Global step 480 Train loss 4.46 on epoch=239
06/23/2022 21:14:02 - INFO - __main__ - Step 490 Global step 490 Train loss 4.33 on epoch=244
06/23/2022 21:14:03 - INFO - __main__ - Step 500 Global step 500 Train loss 4.37 on epoch=249
06/23/2022 21:14:10 - INFO - __main__ - Global step 500 Train loss 4.39 ACC 0.0 on epoch=249
06/23/2022 21:14:12 - INFO - __main__ - Step 510 Global step 510 Train loss 4.21 on epoch=254
06/23/2022 21:14:13 - INFO - __main__ - Step 520 Global step 520 Train loss 4.16 on epoch=259
06/23/2022 21:14:14 - INFO - __main__ - Step 530 Global step 530 Train loss 4.11 on epoch=264
06/23/2022 21:14:16 - INFO - __main__ - Step 540 Global step 540 Train loss 4.08 on epoch=269
06/23/2022 21:14:17 - INFO - __main__ - Step 550 Global step 550 Train loss 4.13 on epoch=274
06/23/2022 21:14:19 - INFO - __main__ - Global step 550 Train loss 4.14 ACC 0.0 on epoch=274
06/23/2022 21:14:20 - INFO - __main__ - Step 560 Global step 560 Train loss 4.07 on epoch=279
06/23/2022 21:14:21 - INFO - __main__ - Step 570 Global step 570 Train loss 3.99 on epoch=284
06/23/2022 21:14:23 - INFO - __main__ - Step 580 Global step 580 Train loss 3.82 on epoch=289
06/23/2022 21:14:24 - INFO - __main__ - Step 590 Global step 590 Train loss 3.88 on epoch=294
06/23/2022 21:14:26 - INFO - __main__ - Step 600 Global step 600 Train loss 3.84 on epoch=299
06/23/2022 21:14:27 - INFO - __main__ - Global step 600 Train loss 3.92 ACC 0.0 on epoch=299
06/23/2022 21:14:29 - INFO - __main__ - Step 610 Global step 610 Train loss 3.78 on epoch=304
06/23/2022 21:14:30 - INFO - __main__ - Step 620 Global step 620 Train loss 3.87 on epoch=309
06/23/2022 21:14:32 - INFO - __main__ - Step 630 Global step 630 Train loss 3.67 on epoch=314
06/23/2022 21:14:33 - INFO - __main__ - Step 640 Global step 640 Train loss 3.63 on epoch=319
06/23/2022 21:14:34 - INFO - __main__ - Step 650 Global step 650 Train loss 3.61 on epoch=324
06/23/2022 21:14:38 - INFO - __main__ - Global step 650 Train loss 3.71 ACC 0.0 on epoch=324
06/23/2022 21:14:39 - INFO - __main__ - Step 660 Global step 660 Train loss 3.50 on epoch=329
06/23/2022 21:14:41 - INFO - __main__ - Step 670 Global step 670 Train loss 3.27 on epoch=334
06/23/2022 21:14:42 - INFO - __main__ - Step 680 Global step 680 Train loss 3.30 on epoch=339
06/23/2022 21:14:43 - INFO - __main__ - Step 690 Global step 690 Train loss 3.11 on epoch=344
06/23/2022 21:14:45 - INFO - __main__ - Step 700 Global step 700 Train loss 2.93 on epoch=349
06/23/2022 21:14:49 - INFO - __main__ - Global step 700 Train loss 3.22 ACC 0.0 on epoch=349
06/23/2022 21:14:50 - INFO - __main__ - Step 710 Global step 710 Train loss 2.98 on epoch=354
06/23/2022 21:14:52 - INFO - __main__ - Step 720 Global step 720 Train loss 2.89 on epoch=359
06/23/2022 21:14:53 - INFO - __main__ - Step 730 Global step 730 Train loss 2.80 on epoch=364
06/23/2022 21:14:54 - INFO - __main__ - Step 740 Global step 740 Train loss 2.74 on epoch=369
06/23/2022 21:14:56 - INFO - __main__ - Step 750 Global step 750 Train loss 2.82 on epoch=374
06/23/2022 21:15:00 - INFO - __main__ - Global step 750 Train loss 2.85 ACC 0.21875 on epoch=374
06/23/2022 21:15:00 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.21875 on epoch=374, global_step=750
06/23/2022 21:15:01 - INFO - __main__ - Step 760 Global step 760 Train loss 2.56 on epoch=379
06/23/2022 21:15:02 - INFO - __main__ - Step 770 Global step 770 Train loss 2.48 on epoch=384
06/23/2022 21:15:04 - INFO - __main__ - Step 780 Global step 780 Train loss 2.43 on epoch=389
06/23/2022 21:15:05 - INFO - __main__ - Step 790 Global step 790 Train loss 2.37 on epoch=394
06/23/2022 21:15:07 - INFO - __main__ - Step 800 Global step 800 Train loss 2.34 on epoch=399
06/23/2022 21:15:09 - INFO - __main__ - Global step 800 Train loss 2.44 ACC 0.5 on epoch=399
06/23/2022 21:15:09 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.5 on epoch=399, global_step=800
06/23/2022 21:15:11 - INFO - __main__ - Step 810 Global step 810 Train loss 2.20 on epoch=404
06/23/2022 21:15:12 - INFO - __main__ - Step 820 Global step 820 Train loss 2.20 on epoch=409
06/23/2022 21:15:13 - INFO - __main__ - Step 830 Global step 830 Train loss 2.08 on epoch=414
06/23/2022 21:15:15 - INFO - __main__ - Step 840 Global step 840 Train loss 1.95 on epoch=419
06/23/2022 21:15:16 - INFO - __main__ - Step 850 Global step 850 Train loss 1.99 on epoch=424
06/23/2022 21:15:18 - INFO - __main__ - Global step 850 Train loss 2.08 ACC 0.5 on epoch=424
06/23/2022 21:15:20 - INFO - __main__ - Step 860 Global step 860 Train loss 1.77 on epoch=429
06/23/2022 21:15:21 - INFO - __main__ - Step 870 Global step 870 Train loss 1.74 on epoch=434
06/23/2022 21:15:22 - INFO - __main__ - Step 880 Global step 880 Train loss 1.69 on epoch=439
06/23/2022 21:15:24 - INFO - __main__ - Step 890 Global step 890 Train loss 1.68 on epoch=444
06/23/2022 21:15:25 - INFO - __main__ - Step 900 Global step 900 Train loss 1.61 on epoch=449
06/23/2022 21:15:28 - INFO - __main__ - Global step 900 Train loss 1.70 ACC 0.5 on epoch=449
06/23/2022 21:15:29 - INFO - __main__ - Step 910 Global step 910 Train loss 1.53 on epoch=454
06/23/2022 21:15:31 - INFO - __main__ - Step 920 Global step 920 Train loss 1.41 on epoch=459
06/23/2022 21:15:32 - INFO - __main__ - Step 930 Global step 930 Train loss 1.39 on epoch=464
06/23/2022 21:15:33 - INFO - __main__ - Step 940 Global step 940 Train loss 1.28 on epoch=469
06/23/2022 21:15:35 - INFO - __main__ - Step 950 Global step 950 Train loss 1.13 on epoch=474
06/23/2022 21:15:37 - INFO - __main__ - Global step 950 Train loss 1.35 ACC 0.5 on epoch=474
06/23/2022 21:15:39 - INFO - __main__ - Step 960 Global step 960 Train loss 1.28 on epoch=479
06/23/2022 21:15:40 - INFO - __main__ - Step 970 Global step 970 Train loss 1.19 on epoch=484
06/23/2022 21:15:42 - INFO - __main__ - Step 980 Global step 980 Train loss 1.12 on epoch=489
06/23/2022 21:15:43 - INFO - __main__ - Step 990 Global step 990 Train loss 1.18 on epoch=494
06/23/2022 21:15:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.09 on epoch=499
06/23/2022 21:15:47 - INFO - __main__ - Global step 1000 Train loss 1.17 ACC 0.5 on epoch=499
06/23/2022 21:15:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.04 on epoch=504
06/23/2022 21:15:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.01 on epoch=509
06/23/2022 21:15:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.03 on epoch=514
06/23/2022 21:15:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.05 on epoch=519
06/23/2022 21:15:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.90 on epoch=524
06/23/2022 21:15:55 - INFO - __main__ - Global step 1050 Train loss 1.01 ACC 0.5 on epoch=524
06/23/2022 21:15:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.91 on epoch=529
06/23/2022 21:15:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.85 on epoch=534
06/23/2022 21:15:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.87 on epoch=539
06/23/2022 21:16:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.86 on epoch=544
06/23/2022 21:16:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.89 on epoch=549
06/23/2022 21:16:03 - INFO - __main__ - Global step 1100 Train loss 0.88 ACC 0.5 on epoch=549
06/23/2022 21:16:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.84 on epoch=554
06/23/2022 21:16:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.84 on epoch=559
06/23/2022 21:16:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.81 on epoch=564
06/23/2022 21:16:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=569
06/23/2022 21:16:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.77 on epoch=574
06/23/2022 21:16:12 - INFO - __main__ - Global step 1150 Train loss 0.81 ACC 0.5 on epoch=574
06/23/2022 21:16:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.74 on epoch=579
06/23/2022 21:16:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.73 on epoch=584
06/23/2022 21:16:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.73 on epoch=589
06/23/2022 21:16:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.74 on epoch=594
06/23/2022 21:16:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.74 on epoch=599
06/23/2022 21:16:20 - INFO - __main__ - Global step 1200 Train loss 0.74 ACC 0.5 on epoch=599
06/23/2022 21:16:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.73 on epoch=604
06/23/2022 21:16:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.71 on epoch=609
06/23/2022 21:16:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.86 on epoch=614
06/23/2022 21:16:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.73 on epoch=619
06/23/2022 21:16:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.62 on epoch=624
06/23/2022 21:16:28 - INFO - __main__ - Global step 1250 Train loss 0.73 ACC 0.5 on epoch=624
06/23/2022 21:16:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.62 on epoch=629
06/23/2022 21:16:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.64 on epoch=634
06/23/2022 21:16:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.65 on epoch=639
06/23/2022 21:16:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.66 on epoch=644
06/23/2022 21:16:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.60 on epoch=649
06/23/2022 21:16:36 - INFO - __main__ - Global step 1300 Train loss 0.64 ACC 0.5 on epoch=649
06/23/2022 21:16:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.63 on epoch=654
06/23/2022 21:16:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.67 on epoch=659
06/23/2022 21:16:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.64 on epoch=664
06/23/2022 21:16:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.58 on epoch=669
06/23/2022 21:16:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.58 on epoch=674
06/23/2022 21:16:45 - INFO - __main__ - Global step 1350 Train loss 0.62 ACC 0.5 on epoch=674
06/23/2022 21:16:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.56 on epoch=679
06/23/2022 21:16:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.62 on epoch=684
06/23/2022 21:16:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
06/23/2022 21:16:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.55 on epoch=694
06/23/2022 21:16:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.59 on epoch=699
06/23/2022 21:16:53 - INFO - __main__ - Global step 1400 Train loss 0.57 ACC 0.5 on epoch=699
06/23/2022 21:16:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.63 on epoch=704
06/23/2022 21:16:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.61 on epoch=709
06/23/2022 21:16:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.57 on epoch=714
06/23/2022 21:16:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.58 on epoch=719
06/23/2022 21:17:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.53 on epoch=724
06/23/2022 21:17:01 - INFO - __main__ - Global step 1450 Train loss 0.59 ACC 0.5 on epoch=724
06/23/2022 21:17:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.58 on epoch=729
06/23/2022 21:17:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.50 on epoch=734
06/23/2022 21:17:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.64 on epoch=739
06/23/2022 21:17:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.51 on epoch=744
06/23/2022 21:17:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.65 on epoch=749
06/23/2022 21:17:10 - INFO - __main__ - Global step 1500 Train loss 0.58 ACC 0.5 on epoch=749
06/23/2022 21:17:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.51 on epoch=754
06/23/2022 21:17:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.57 on epoch=759
06/23/2022 21:17:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
06/23/2022 21:17:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.49 on epoch=769
06/23/2022 21:17:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.49 on epoch=774
06/23/2022 21:17:18 - INFO - __main__ - Global step 1550 Train loss 0.50 ACC 0.5 on epoch=774
06/23/2022 21:17:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.51 on epoch=779
06/23/2022 21:17:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.49 on epoch=784
06/23/2022 21:17:23 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=789
06/23/2022 21:17:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.49 on epoch=794
06/23/2022 21:17:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=799
06/23/2022 21:17:28 - INFO - __main__ - Global step 1600 Train loss 0.48 ACC 0.5 on epoch=799
06/23/2022 21:17:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=804
06/23/2022 21:17:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=809
06/23/2022 21:17:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.53 on epoch=814
06/23/2022 21:17:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
06/23/2022 21:17:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=824
06/23/2022 21:17:35 - INFO - __main__ - Global step 1650 Train loss 0.45 ACC 0.5 on epoch=824
06/23/2022 21:17:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=829
06/23/2022 21:17:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.53 on epoch=834
06/23/2022 21:17:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.57 on epoch=839
06/23/2022 21:17:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=844
06/23/2022 21:17:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.46 on epoch=849
06/23/2022 21:17:43 - INFO - __main__ - Global step 1700 Train loss 0.49 ACC 0.5 on epoch=849
06/23/2022 21:17:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=854
06/23/2022 21:17:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=859
06/23/2022 21:17:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=864
06/23/2022 21:17:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=869
06/23/2022 21:17:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=874
06/23/2022 21:17:52 - INFO - __main__ - Global step 1750 Train loss 0.42 ACC 0.5 on epoch=874
06/23/2022 21:17:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=879
06/23/2022 21:17:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.51 on epoch=884
06/23/2022 21:17:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=889
06/23/2022 21:17:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
06/23/2022 21:17:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
06/23/2022 21:18:00 - INFO - __main__ - Global step 1800 Train loss 0.41 ACC 0.5 on epoch=899
06/23/2022 21:18:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=904
06/23/2022 21:18:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.46 on epoch=909
06/23/2022 21:18:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=914
06/23/2022 21:18:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=919
06/23/2022 21:18:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=924
06/23/2022 21:18:08 - INFO - __main__ - Global step 1850 Train loss 0.40 ACC 0.5 on epoch=924
06/23/2022 21:18:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=929
06/23/2022 21:18:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
06/23/2022 21:18:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=939
06/23/2022 21:18:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=944
06/23/2022 21:18:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
06/23/2022 21:18:16 - INFO - __main__ - Global step 1900 Train loss 0.40 ACC 0.5 on epoch=949
06/23/2022 21:18:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.44 on epoch=954
06/23/2022 21:18:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=959
06/23/2022 21:18:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.46 on epoch=964
06/23/2022 21:18:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.49 on epoch=969
06/23/2022 21:18:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=974
06/23/2022 21:18:23 - INFO - __main__ - Global step 1950 Train loss 0.43 ACC 0.5 on epoch=974
06/23/2022 21:18:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=979
06/23/2022 21:18:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=984
06/23/2022 21:18:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=989
06/23/2022 21:18:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
06/23/2022 21:18:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=999
06/23/2022 21:18:32 - INFO - __main__ - Global step 2000 Train loss 0.38 ACC 0.5 on epoch=999
06/23/2022 21:18:32 - INFO - __main__ - save last model!
06/23/2022 21:18:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 21:18:32 - INFO - __main__ - Start tokenizing ... 40430 instances
06/23/2022 21:18:32 - INFO - __main__ - Printing 3 examples
06/23/2022 21:18:32 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/23/2022 21:18:32 - INFO - __main__ - ['not_duplicate']
06/23/2022 21:18:32 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/23/2022 21:18:32 - INFO - __main__ - ['not_duplicate']
06/23/2022 21:18:32 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/23/2022 21:18:32 - INFO - __main__ - ['duplicate']
06/23/2022 21:18:32 - INFO - __main__ - Tokenizing Input ...
06/23/2022 21:18:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 21:18:32 - INFO - __main__ - Printing 3 examples
06/23/2022 21:18:32 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/23/2022 21:18:32 - INFO - __main__ - ['duplicate']
06/23/2022 21:18:32 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/23/2022 21:18:32 - INFO - __main__ - ['duplicate']
06/23/2022 21:18:32 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/23/2022 21:18:32 - INFO - __main__ - ['duplicate']
06/23/2022 21:18:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 21:18:32 - INFO - __main__ - Tokenizing Output ...
06/23/2022 21:18:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 21:18:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 21:18:32 - INFO - __main__ - Printing 3 examples
06/23/2022 21:18:32 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/23/2022 21:18:32 - INFO - __main__ - ['duplicate']
06/23/2022 21:18:32 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/23/2022 21:18:32 - INFO - __main__ - ['duplicate']
06/23/2022 21:18:32 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/23/2022 21:18:32 - INFO - __main__ - ['duplicate']
06/23/2022 21:18:32 - INFO - __main__ - Tokenizing Input ...
06/23/2022 21:18:32 - INFO - __main__ - Tokenizing Output ...
06/23/2022 21:18:32 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 21:18:38 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 21:18:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 21:18:39 - INFO - __main__ - Starting training!
06/23/2022 21:18:56 - INFO - __main__ - Tokenizing Output ...
06/23/2022 21:19:39 - INFO - __main__ - Loaded 40430 examples from test data
06/23/2022 21:32:56 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_13_0.4_8_predictions.txt
06/23/2022 21:32:56 - INFO - __main__ - ACC on test data: 0.3682
06/23/2022 21:32:57 - INFO - __main__ - prefix=glue-qqp_16_13, lr=0.4, bsz=8, dev_performance=0.5, test_performance=0.36821667078901804
06/23/2022 21:32:57 - INFO - __main__ - Running ... prefix=glue-qqp_16_13, lr=0.3, bsz=8 ...
06/23/2022 21:32:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 21:32:58 - INFO - __main__ - Printing 3 examples
06/23/2022 21:32:58 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/23/2022 21:32:58 - INFO - __main__ - ['duplicate']
06/23/2022 21:32:58 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/23/2022 21:32:58 - INFO - __main__ - ['duplicate']
06/23/2022 21:32:58 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/23/2022 21:32:58 - INFO - __main__ - ['duplicate']
06/23/2022 21:32:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 21:32:58 - INFO - __main__ - Tokenizing Output ...
06/23/2022 21:32:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 21:32:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 21:32:58 - INFO - __main__ - Printing 3 examples
06/23/2022 21:32:58 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/23/2022 21:32:58 - INFO - __main__ - ['duplicate']
06/23/2022 21:32:58 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/23/2022 21:32:58 - INFO - __main__ - ['duplicate']
06/23/2022 21:32:58 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/23/2022 21:32:58 - INFO - __main__ - ['duplicate']
06/23/2022 21:32:58 - INFO - __main__ - Tokenizing Input ...
06/23/2022 21:32:58 - INFO - __main__ - Tokenizing Output ...
06/23/2022 21:32:58 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 21:33:03 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 21:33:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 21:33:04 - INFO - __main__ - Starting training!
06/23/2022 21:33:05 - INFO - __main__ - Step 10 Global step 10 Train loss 7.29 on epoch=4
06/23/2022 21:33:06 - INFO - __main__ - Step 20 Global step 20 Train loss 7.23 on epoch=9
06/23/2022 21:33:08 - INFO - __main__ - Step 30 Global step 30 Train loss 7.07 on epoch=14
06/23/2022 21:33:09 - INFO - __main__ - Step 40 Global step 40 Train loss 7.15 on epoch=19
06/23/2022 21:33:11 - INFO - __main__ - Step 50 Global step 50 Train loss 7.16 on epoch=24
06/23/2022 21:33:12 - INFO - __main__ - Global step 50 Train loss 7.18 ACC 0.0 on epoch=24
06/23/2022 21:33:12 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 21:33:13 - INFO - __main__ - Step 60 Global step 60 Train loss 7.12 on epoch=29
06/23/2022 21:33:15 - INFO - __main__ - Step 70 Global step 70 Train loss 7.07 on epoch=34
06/23/2022 21:33:16 - INFO - __main__ - Step 80 Global step 80 Train loss 7.08 on epoch=39
06/23/2022 21:33:18 - INFO - __main__ - Step 90 Global step 90 Train loss 7.10 on epoch=44
06/23/2022 21:33:19 - INFO - __main__ - Step 100 Global step 100 Train loss 7.04 on epoch=49
06/23/2022 21:33:23 - INFO - __main__ - Global step 100 Train loss 7.08 ACC 0.0 on epoch=49
06/23/2022 21:33:25 - INFO - __main__ - Step 110 Global step 110 Train loss 7.02 on epoch=54
06/23/2022 21:33:26 - INFO - __main__ - Step 120 Global step 120 Train loss 6.96 on epoch=59
06/23/2022 21:33:28 - INFO - __main__ - Step 130 Global step 130 Train loss 6.98 on epoch=64
06/23/2022 21:33:29 - INFO - __main__ - Step 140 Global step 140 Train loss 6.97 on epoch=69
06/23/2022 21:33:30 - INFO - __main__ - Step 150 Global step 150 Train loss 6.83 on epoch=74
06/23/2022 21:33:32 - INFO - __main__ - Global step 150 Train loss 6.95 ACC 0.0 on epoch=74
06/23/2022 21:33:33 - INFO - __main__ - Step 160 Global step 160 Train loss 6.91 on epoch=79
06/23/2022 21:33:35 - INFO - __main__ - Step 170 Global step 170 Train loss 6.67 on epoch=84
06/23/2022 21:33:36 - INFO - __main__ - Step 180 Global step 180 Train loss 6.63 on epoch=89
06/23/2022 21:33:37 - INFO - __main__ - Step 190 Global step 190 Train loss 6.62 on epoch=94
06/23/2022 21:33:39 - INFO - __main__ - Step 200 Global step 200 Train loss 6.55 on epoch=99
06/23/2022 21:33:42 - INFO - __main__ - Global step 200 Train loss 6.68 ACC 0.0 on epoch=99
06/23/2022 21:33:43 - INFO - __main__ - Step 210 Global step 210 Train loss 6.56 on epoch=104
06/23/2022 21:33:44 - INFO - __main__ - Step 220 Global step 220 Train loss 6.46 on epoch=109
06/23/2022 21:33:46 - INFO - __main__ - Step 230 Global step 230 Train loss 6.30 on epoch=114
06/23/2022 21:33:47 - INFO - __main__ - Step 240 Global step 240 Train loss 6.35 on epoch=119
06/23/2022 21:33:49 - INFO - __main__ - Step 250 Global step 250 Train loss 6.20 on epoch=124
06/23/2022 21:33:53 - INFO - __main__ - Global step 250 Train loss 6.37 ACC 0.0 on epoch=124
06/23/2022 21:33:54 - INFO - __main__ - Step 260 Global step 260 Train loss 6.12 on epoch=129
06/23/2022 21:33:56 - INFO - __main__ - Step 270 Global step 270 Train loss 5.95 on epoch=134
06/23/2022 21:33:57 - INFO - __main__ - Step 280 Global step 280 Train loss 5.80 on epoch=139
06/23/2022 21:33:59 - INFO - __main__ - Step 290 Global step 290 Train loss 5.84 on epoch=144
06/23/2022 21:34:00 - INFO - __main__ - Step 300 Global step 300 Train loss 5.68 on epoch=149
06/23/2022 21:34:03 - INFO - __main__ - Global step 300 Train loss 5.88 ACC 0.0 on epoch=149
06/23/2022 21:34:05 - INFO - __main__ - Step 310 Global step 310 Train loss 5.60 on epoch=154
06/23/2022 21:34:06 - INFO - __main__ - Step 320 Global step 320 Train loss 5.45 on epoch=159
06/23/2022 21:34:08 - INFO - __main__ - Step 330 Global step 330 Train loss 5.38 on epoch=164
06/23/2022 21:34:09 - INFO - __main__ - Step 340 Global step 340 Train loss 5.28 on epoch=169
06/23/2022 21:34:10 - INFO - __main__ - Step 350 Global step 350 Train loss 5.17 on epoch=174
06/23/2022 21:34:16 - INFO - __main__ - Global step 350 Train loss 5.38 ACC 0.0 on epoch=174
06/23/2022 21:34:17 - INFO - __main__ - Step 360 Global step 360 Train loss 5.22 on epoch=179
06/23/2022 21:34:18 - INFO - __main__ - Step 370 Global step 370 Train loss 5.01 on epoch=184
06/23/2022 21:34:20 - INFO - __main__ - Step 380 Global step 380 Train loss 4.94 on epoch=189
06/23/2022 21:34:21 - INFO - __main__ - Step 390 Global step 390 Train loss 4.90 on epoch=194
06/23/2022 21:34:22 - INFO - __main__ - Step 400 Global step 400 Train loss 4.64 on epoch=199
06/23/2022 21:34:29 - INFO - __main__ - Global step 400 Train loss 4.94 ACC 0.0 on epoch=199
06/23/2022 21:34:30 - INFO - __main__ - Step 410 Global step 410 Train loss 4.60 on epoch=204
06/23/2022 21:34:32 - INFO - __main__ - Step 420 Global step 420 Train loss 4.44 on epoch=209
06/23/2022 21:34:33 - INFO - __main__ - Step 430 Global step 430 Train loss 4.26 on epoch=214
06/23/2022 21:34:35 - INFO - __main__ - Step 440 Global step 440 Train loss 4.31 on epoch=219
06/23/2022 21:34:36 - INFO - __main__ - Step 450 Global step 450 Train loss 4.17 on epoch=224
06/23/2022 21:34:40 - INFO - __main__ - Global step 450 Train loss 4.36 ACC 0.0 on epoch=224
06/23/2022 21:34:41 - INFO - __main__ - Step 460 Global step 460 Train loss 4.22 on epoch=229
06/23/2022 21:34:43 - INFO - __main__ - Step 470 Global step 470 Train loss 4.04 on epoch=234
06/23/2022 21:34:44 - INFO - __main__ - Step 480 Global step 480 Train loss 3.86 on epoch=239
06/23/2022 21:34:45 - INFO - __main__ - Step 490 Global step 490 Train loss 3.87 on epoch=244
06/23/2022 21:34:46 - INFO - __main__ - Step 500 Global step 500 Train loss 3.77 on epoch=249
06/23/2022 21:34:50 - INFO - __main__ - Global step 500 Train loss 3.95 ACC 0.03125 on epoch=249
06/23/2022 21:34:50 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=249, global_step=500
06/23/2022 21:34:51 - INFO - __main__ - Step 510 Global step 510 Train loss 3.69 on epoch=254
06/23/2022 21:34:52 - INFO - __main__ - Step 520 Global step 520 Train loss 3.66 on epoch=259
06/23/2022 21:34:54 - INFO - __main__ - Step 530 Global step 530 Train loss 3.55 on epoch=264
06/23/2022 21:34:55 - INFO - __main__ - Step 540 Global step 540 Train loss 3.54 on epoch=269
06/23/2022 21:34:56 - INFO - __main__ - Step 550 Global step 550 Train loss 3.46 on epoch=274
06/23/2022 21:34:59 - INFO - __main__ - Global step 550 Train loss 3.58 ACC 0.40625 on epoch=274
06/23/2022 21:34:59 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.40625 on epoch=274, global_step=550
06/23/2022 21:35:01 - INFO - __main__ - Step 560 Global step 560 Train loss 3.32 on epoch=279
06/23/2022 21:35:02 - INFO - __main__ - Step 570 Global step 570 Train loss 3.39 on epoch=284
06/23/2022 21:35:04 - INFO - __main__ - Step 580 Global step 580 Train loss 3.18 on epoch=289
06/23/2022 21:35:05 - INFO - __main__ - Step 590 Global step 590 Train loss 3.21 on epoch=294
06/23/2022 21:35:06 - INFO - __main__ - Step 600 Global step 600 Train loss 3.01 on epoch=299
06/23/2022 21:35:09 - INFO - __main__ - Global step 600 Train loss 3.22 ACC 0.5 on epoch=299
06/23/2022 21:35:09 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.5 on epoch=299, global_step=600
06/23/2022 21:35:10 - INFO - __main__ - Step 610 Global step 610 Train loss 2.97 on epoch=304
06/23/2022 21:35:12 - INFO - __main__ - Step 620 Global step 620 Train loss 2.86 on epoch=309
06/23/2022 21:35:13 - INFO - __main__ - Step 630 Global step 630 Train loss 2.88 on epoch=314
06/23/2022 21:35:14 - INFO - __main__ - Step 640 Global step 640 Train loss 2.77 on epoch=319
06/23/2022 21:35:16 - INFO - __main__ - Step 650 Global step 650 Train loss 2.80 on epoch=324
06/23/2022 21:35:19 - INFO - __main__ - Global step 650 Train loss 2.86 ACC 0.5 on epoch=324
06/23/2022 21:35:20 - INFO - __main__ - Step 660 Global step 660 Train loss 2.66 on epoch=329
06/23/2022 21:35:21 - INFO - __main__ - Step 670 Global step 670 Train loss 2.64 on epoch=334
06/23/2022 21:35:23 - INFO - __main__ - Step 680 Global step 680 Train loss 2.52 on epoch=339
06/23/2022 21:35:24 - INFO - __main__ - Step 690 Global step 690 Train loss 2.40 on epoch=344
06/23/2022 21:35:25 - INFO - __main__ - Step 700 Global step 700 Train loss 2.27 on epoch=349
06/23/2022 21:35:28 - INFO - __main__ - Global step 700 Train loss 2.50 ACC 0.5 on epoch=349
06/23/2022 21:35:29 - INFO - __main__ - Step 710 Global step 710 Train loss 2.24 on epoch=354
06/23/2022 21:35:31 - INFO - __main__ - Step 720 Global step 720 Train loss 2.23 on epoch=359
06/23/2022 21:35:32 - INFO - __main__ - Step 730 Global step 730 Train loss 2.20 on epoch=364
06/23/2022 21:35:33 - INFO - __main__ - Step 740 Global step 740 Train loss 2.05 on epoch=369
06/23/2022 21:35:35 - INFO - __main__ - Step 750 Global step 750 Train loss 2.05 on epoch=374
06/23/2022 21:35:38 - INFO - __main__ - Global step 750 Train loss 2.16 ACC 0.5 on epoch=374
06/23/2022 21:35:39 - INFO - __main__ - Step 760 Global step 760 Train loss 2.03 on epoch=379
06/23/2022 21:35:41 - INFO - __main__ - Step 770 Global step 770 Train loss 2.00 on epoch=384
06/23/2022 21:35:42 - INFO - __main__ - Step 780 Global step 780 Train loss 2.01 on epoch=389
06/23/2022 21:35:43 - INFO - __main__ - Step 790 Global step 790 Train loss 1.91 on epoch=394
06/23/2022 21:35:45 - INFO - __main__ - Step 800 Global step 800 Train loss 1.98 on epoch=399
06/23/2022 21:35:47 - INFO - __main__ - Global step 800 Train loss 1.99 ACC 0.5 on epoch=399
06/23/2022 21:35:48 - INFO - __main__ - Step 810 Global step 810 Train loss 1.91 on epoch=404
06/23/2022 21:35:49 - INFO - __main__ - Step 820 Global step 820 Train loss 1.65 on epoch=409
06/23/2022 21:35:50 - INFO - __main__ - Step 830 Global step 830 Train loss 1.55 on epoch=414
06/23/2022 21:35:52 - INFO - __main__ - Step 840 Global step 840 Train loss 1.49 on epoch=419
06/23/2022 21:35:53 - INFO - __main__ - Step 850 Global step 850 Train loss 1.62 on epoch=424
06/23/2022 21:35:56 - INFO - __main__ - Global step 850 Train loss 1.65 ACC 0.5 on epoch=424
06/23/2022 21:35:57 - INFO - __main__ - Step 860 Global step 860 Train loss 1.59 on epoch=429
06/23/2022 21:35:58 - INFO - __main__ - Step 870 Global step 870 Train loss 1.51 on epoch=434
06/23/2022 21:36:00 - INFO - __main__ - Step 880 Global step 880 Train loss 1.51 on epoch=439
06/23/2022 21:36:01 - INFO - __main__ - Step 890 Global step 890 Train loss 1.57 on epoch=444
06/23/2022 21:36:02 - INFO - __main__ - Step 900 Global step 900 Train loss 1.41 on epoch=449
06/23/2022 21:36:04 - INFO - __main__ - Global step 900 Train loss 1.52 ACC 0.5 on epoch=449
06/23/2022 21:36:05 - INFO - __main__ - Step 910 Global step 910 Train loss 1.29 on epoch=454
06/23/2022 21:36:07 - INFO - __main__ - Step 920 Global step 920 Train loss 1.34 on epoch=459
06/23/2022 21:36:08 - INFO - __main__ - Step 930 Global step 930 Train loss 1.19 on epoch=464
06/23/2022 21:36:09 - INFO - __main__ - Step 940 Global step 940 Train loss 1.31 on epoch=469
06/23/2022 21:36:11 - INFO - __main__ - Step 950 Global step 950 Train loss 1.15 on epoch=474
06/23/2022 21:36:12 - INFO - __main__ - Global step 950 Train loss 1.26 ACC 0.5 on epoch=474
06/23/2022 21:36:14 - INFO - __main__ - Step 960 Global step 960 Train loss 1.16 on epoch=479
06/23/2022 21:36:15 - INFO - __main__ - Step 970 Global step 970 Train loss 1.10 on epoch=484
06/23/2022 21:36:16 - INFO - __main__ - Step 980 Global step 980 Train loss 1.17 on epoch=489
06/23/2022 21:36:18 - INFO - __main__ - Step 990 Global step 990 Train loss 1.05 on epoch=494
06/23/2022 21:36:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.95 on epoch=499
06/23/2022 21:36:21 - INFO - __main__ - Global step 1000 Train loss 1.09 ACC 0.5 on epoch=499
06/23/2022 21:36:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.95 on epoch=504
06/23/2022 21:36:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.00 on epoch=509
06/23/2022 21:36:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.00 on epoch=514
06/23/2022 21:36:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.02 on epoch=519
06/23/2022 21:36:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.06 on epoch=524
06/23/2022 21:36:30 - INFO - __main__ - Global step 1050 Train loss 1.01 ACC 0.5 on epoch=524
06/23/2022 21:36:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.93 on epoch=529
06/23/2022 21:36:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.87 on epoch=534
06/23/2022 21:36:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.84 on epoch=539
06/23/2022 21:36:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.76 on epoch=544
06/23/2022 21:36:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.87 on epoch=549
06/23/2022 21:36:38 - INFO - __main__ - Global step 1100 Train loss 0.85 ACC 0.5 on epoch=549
06/23/2022 21:36:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.88 on epoch=554
06/23/2022 21:36:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.84 on epoch=559
06/23/2022 21:36:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.73 on epoch=564
06/23/2022 21:36:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.68 on epoch=569
06/23/2022 21:36:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.74 on epoch=574
06/23/2022 21:36:48 - INFO - __main__ - Global step 1150 Train loss 0.77 ACC 0.5 on epoch=574
06/23/2022 21:36:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.73 on epoch=579
06/23/2022 21:36:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.69 on epoch=584
06/23/2022 21:36:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.71 on epoch=589
06/23/2022 21:36:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.61 on epoch=594
06/23/2022 21:36:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.65 on epoch=599
06/23/2022 21:36:57 - INFO - __main__ - Global step 1200 Train loss 0.68 ACC 0.5 on epoch=599
06/23/2022 21:36:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.66 on epoch=604
06/23/2022 21:37:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.74 on epoch=609
06/23/2022 21:37:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.58 on epoch=614
06/23/2022 21:37:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.69 on epoch=619
06/23/2022 21:37:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.62 on epoch=624
06/23/2022 21:37:07 - INFO - __main__ - Global step 1250 Train loss 0.66 ACC 0.5 on epoch=624
06/23/2022 21:37:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.65 on epoch=629
06/23/2022 21:37:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.65 on epoch=634
06/23/2022 21:37:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.64 on epoch=639
06/23/2022 21:37:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.60 on epoch=644
06/23/2022 21:37:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.62 on epoch=649
06/23/2022 21:37:17 - INFO - __main__ - Global step 1300 Train loss 0.63 ACC 0.5 on epoch=649
06/23/2022 21:37:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.60 on epoch=654
06/23/2022 21:37:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.55 on epoch=659
06/23/2022 21:37:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.56 on epoch=664
06/23/2022 21:37:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.58 on epoch=669
06/23/2022 21:37:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.60 on epoch=674
06/23/2022 21:37:27 - INFO - __main__ - Global step 1350 Train loss 0.58 ACC 0.5 on epoch=674
06/23/2022 21:37:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.53 on epoch=679
06/23/2022 21:37:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.57 on epoch=684
06/23/2022 21:37:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
06/23/2022 21:37:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.55 on epoch=694
06/23/2022 21:37:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.59 on epoch=699
06/23/2022 21:37:37 - INFO - __main__ - Global step 1400 Train loss 0.56 ACC 0.5 on epoch=699
06/23/2022 21:37:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.45 on epoch=704
06/23/2022 21:37:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=709
06/23/2022 21:37:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.47 on epoch=714
06/23/2022 21:37:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.49 on epoch=719
06/23/2022 21:37:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.53 on epoch=724
06/23/2022 21:37:48 - INFO - __main__ - Global step 1450 Train loss 0.49 ACC 0.5 on epoch=724
06/23/2022 21:37:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.60 on epoch=729
06/23/2022 21:37:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.57 on epoch=734
06/23/2022 21:37:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.53 on epoch=739
06/23/2022 21:37:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.53 on epoch=744
06/23/2022 21:37:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.49 on epoch=749
06/23/2022 21:37:58 - INFO - __main__ - Global step 1500 Train loss 0.54 ACC 0.5 on epoch=749
06/23/2022 21:37:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.58 on epoch=754
06/23/2022 21:38:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.52 on epoch=759
06/23/2022 21:38:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.55 on epoch=764
06/23/2022 21:38:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.50 on epoch=769
06/23/2022 21:38:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.49 on epoch=774
06/23/2022 21:38:11 - INFO - __main__ - Global step 1550 Train loss 0.53 ACC 0.5 on epoch=774
06/23/2022 21:38:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.47 on epoch=779
06/23/2022 21:38:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=784
06/23/2022 21:38:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.49 on epoch=789
06/23/2022 21:38:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.47 on epoch=794
06/23/2022 21:38:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=799
06/23/2022 21:38:20 - INFO - __main__ - Global step 1600 Train loss 0.47 ACC 0.5 on epoch=799
06/23/2022 21:38:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=804
06/23/2022 21:38:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=809
06/23/2022 21:38:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.45 on epoch=814
06/23/2022 21:38:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=819
06/23/2022 21:38:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=824
06/23/2022 21:38:29 - INFO - __main__ - Global step 1650 Train loss 0.44 ACC 0.5 on epoch=824
06/23/2022 21:38:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=829
06/23/2022 21:38:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=834
06/23/2022 21:38:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=839
06/23/2022 21:38:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=844
06/23/2022 21:38:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.46 on epoch=849
06/23/2022 21:38:37 - INFO - __main__ - Global step 1700 Train loss 0.44 ACC 0.5 on epoch=849
06/23/2022 21:38:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.48 on epoch=854
06/23/2022 21:38:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.49 on epoch=859
06/23/2022 21:38:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=864
06/23/2022 21:38:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=869
06/23/2022 21:38:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=874
06/23/2022 21:38:46 - INFO - __main__ - Global step 1750 Train loss 0.45 ACC 0.5 on epoch=874
06/23/2022 21:38:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=879
06/23/2022 21:38:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=884
06/23/2022 21:38:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=889
06/23/2022 21:38:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
06/23/2022 21:38:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=899
06/23/2022 21:38:55 - INFO - __main__ - Global step 1800 Train loss 0.43 ACC 0.5 on epoch=899
06/23/2022 21:38:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.46 on epoch=904
06/23/2022 21:38:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=909
06/23/2022 21:38:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=914
06/23/2022 21:39:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=919
06/23/2022 21:39:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=924
06/23/2022 21:39:04 - INFO - __main__ - Global step 1850 Train loss 0.40 ACC 0.5 on epoch=924
06/23/2022 21:39:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=929
06/23/2022 21:39:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
06/23/2022 21:39:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=939
06/23/2022 21:39:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.47 on epoch=944
06/23/2022 21:39:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
06/23/2022 21:39:13 - INFO - __main__ - Global step 1900 Train loss 0.41 ACC 0.5 on epoch=949
06/23/2022 21:39:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
06/23/2022 21:39:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
06/23/2022 21:39:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
06/23/2022 21:39:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=969
06/23/2022 21:39:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
06/23/2022 21:39:23 - INFO - __main__ - Global step 1950 Train loss 0.40 ACC 0.5 on epoch=974
06/23/2022 21:39:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=979
06/23/2022 21:39:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=984
06/23/2022 21:39:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=989
06/23/2022 21:39:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=994
06/23/2022 21:39:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=999
06/23/2022 21:39:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 21:39:31 - INFO - __main__ - Printing 3 examples
06/23/2022 21:39:31 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/23/2022 21:39:31 - INFO - __main__ - ['duplicate']
06/23/2022 21:39:31 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/23/2022 21:39:31 - INFO - __main__ - ['duplicate']
06/23/2022 21:39:31 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/23/2022 21:39:31 - INFO - __main__ - ['duplicate']
06/23/2022 21:39:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 21:39:31 - INFO - __main__ - Tokenizing Output ...
06/23/2022 21:39:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 21:39:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 21:39:31 - INFO - __main__ - Printing 3 examples
06/23/2022 21:39:31 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/23/2022 21:39:31 - INFO - __main__ - ['duplicate']
06/23/2022 21:39:31 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/23/2022 21:39:31 - INFO - __main__ - ['duplicate']
06/23/2022 21:39:31 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/23/2022 21:39:31 - INFO - __main__ - ['duplicate']
06/23/2022 21:39:31 - INFO - __main__ - Tokenizing Input ...
06/23/2022 21:39:31 - INFO - __main__ - Tokenizing Output ...
06/23/2022 21:39:31 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 21:39:32 - INFO - __main__ - Global step 2000 Train loss 0.40 ACC 0.5 on epoch=999
06/23/2022 21:39:32 - INFO - __main__ - save last model!
06/23/2022 21:39:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 21:39:32 - INFO - __main__ - Start tokenizing ... 40430 instances
06/23/2022 21:39:32 - INFO - __main__ - Printing 3 examples
06/23/2022 21:39:32 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/23/2022 21:39:32 - INFO - __main__ - ['not_duplicate']
06/23/2022 21:39:32 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/23/2022 21:39:32 - INFO - __main__ - ['not_duplicate']
06/23/2022 21:39:32 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/23/2022 21:39:32 - INFO - __main__ - ['duplicate']
06/23/2022 21:39:32 - INFO - __main__ - Tokenizing Input ...
06/23/2022 21:39:36 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 21:39:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 21:39:37 - INFO - __main__ - Starting training!
06/23/2022 21:39:56 - INFO - __main__ - Tokenizing Output ...
06/23/2022 21:40:49 - INFO - __main__ - Loaded 40430 examples from test data
06/23/2022 22:29:26 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_13_0.3_8_predictions.txt
06/23/2022 22:29:27 - INFO - __main__ - ACC on test data: 0.3683
06/23/2022 22:29:27 - INFO - __main__ - prefix=glue-qqp_16_13, lr=0.3, bsz=8, dev_performance=0.5, test_performance=0.36834034133069504
06/23/2022 22:29:27 - INFO - __main__ - Running ... prefix=glue-qqp_16_13, lr=0.2, bsz=8 ...
06/23/2022 22:29:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 22:29:28 - INFO - __main__ - Printing 3 examples
06/23/2022 22:29:28 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/23/2022 22:29:28 - INFO - __main__ - ['duplicate']
06/23/2022 22:29:28 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/23/2022 22:29:28 - INFO - __main__ - ['duplicate']
06/23/2022 22:29:28 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/23/2022 22:29:28 - INFO - __main__ - ['duplicate']
06/23/2022 22:29:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 22:29:28 - INFO - __main__ - Tokenizing Output ...
06/23/2022 22:29:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 22:29:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 22:29:28 - INFO - __main__ - Printing 3 examples
06/23/2022 22:29:28 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/23/2022 22:29:28 - INFO - __main__ - ['duplicate']
06/23/2022 22:29:28 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/23/2022 22:29:28 - INFO - __main__ - ['duplicate']
06/23/2022 22:29:28 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/23/2022 22:29:28 - INFO - __main__ - ['duplicate']
06/23/2022 22:29:28 - INFO - __main__ - Tokenizing Input ...
06/23/2022 22:29:28 - INFO - __main__ - Tokenizing Output ...
06/23/2022 22:29:28 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 22:29:33 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 22:29:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 22:29:34 - INFO - __main__ - Starting training!
06/23/2022 22:29:35 - INFO - __main__ - Step 10 Global step 10 Train loss 7.14 on epoch=4
06/23/2022 22:29:37 - INFO - __main__ - Step 20 Global step 20 Train loss 7.29 on epoch=9
06/23/2022 22:29:38 - INFO - __main__ - Step 30 Global step 30 Train loss 7.21 on epoch=14
06/23/2022 22:29:39 - INFO - __main__ - Step 40 Global step 40 Train loss 7.14 on epoch=19
06/23/2022 22:29:41 - INFO - __main__ - Step 50 Global step 50 Train loss 7.10 on epoch=24
06/23/2022 22:29:44 - INFO - __main__ - Global step 50 Train loss 7.17 ACC 0.0 on epoch=24
06/23/2022 22:29:44 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 22:29:45 - INFO - __main__ - Step 60 Global step 60 Train loss 7.20 on epoch=29
06/23/2022 22:29:47 - INFO - __main__ - Step 70 Global step 70 Train loss 7.17 on epoch=34
06/23/2022 22:29:48 - INFO - __main__ - Step 80 Global step 80 Train loss 7.17 on epoch=39
06/23/2022 22:29:49 - INFO - __main__ - Step 90 Global step 90 Train loss 7.17 on epoch=44
06/23/2022 22:29:51 - INFO - __main__ - Step 100 Global step 100 Train loss 7.10 on epoch=49
06/23/2022 22:29:52 - INFO - __main__ - Global step 100 Train loss 7.16 ACC 0.0 on epoch=49
06/23/2022 22:29:53 - INFO - __main__ - Step 110 Global step 110 Train loss 7.12 on epoch=54
06/23/2022 22:29:55 - INFO - __main__ - Step 120 Global step 120 Train loss 7.12 on epoch=59
06/23/2022 22:29:56 - INFO - __main__ - Step 130 Global step 130 Train loss 7.04 on epoch=64
06/23/2022 22:29:58 - INFO - __main__ - Step 140 Global step 140 Train loss 7.15 on epoch=69
06/23/2022 22:29:59 - INFO - __main__ - Step 150 Global step 150 Train loss 7.06 on epoch=74
06/23/2022 22:30:05 - INFO - __main__ - Global step 150 Train loss 7.10 ACC 0.0 on epoch=74
06/23/2022 22:30:06 - INFO - __main__ - Step 160 Global step 160 Train loss 7.12 on epoch=79
06/23/2022 22:30:07 - INFO - __main__ - Step 170 Global step 170 Train loss 7.03 on epoch=84
06/23/2022 22:30:09 - INFO - __main__ - Step 180 Global step 180 Train loss 6.94 on epoch=89
06/23/2022 22:30:10 - INFO - __main__ - Step 190 Global step 190 Train loss 6.99 on epoch=94
06/23/2022 22:30:12 - INFO - __main__ - Step 200 Global step 200 Train loss 7.02 on epoch=99
06/23/2022 22:30:13 - INFO - __main__ - Global step 200 Train loss 7.02 ACC 0.0 on epoch=99
06/23/2022 22:30:15 - INFO - __main__ - Step 210 Global step 210 Train loss 7.08 on epoch=104
06/23/2022 22:30:16 - INFO - __main__ - Step 220 Global step 220 Train loss 6.99 on epoch=109
06/23/2022 22:30:18 - INFO - __main__ - Step 230 Global step 230 Train loss 7.01 on epoch=114
06/23/2022 22:30:19 - INFO - __main__ - Step 240 Global step 240 Train loss 6.97 on epoch=119
06/23/2022 22:30:20 - INFO - __main__ - Step 250 Global step 250 Train loss 6.88 on epoch=124
06/23/2022 22:30:26 - INFO - __main__ - Global step 250 Train loss 6.99 ACC 0.0 on epoch=124
06/23/2022 22:30:27 - INFO - __main__ - Step 260 Global step 260 Train loss 6.95 on epoch=129
06/23/2022 22:30:29 - INFO - __main__ - Step 270 Global step 270 Train loss 6.90 on epoch=134
06/23/2022 22:30:30 - INFO - __main__ - Step 280 Global step 280 Train loss 6.93 on epoch=139
06/23/2022 22:30:32 - INFO - __main__ - Step 290 Global step 290 Train loss 6.89 on epoch=144
06/23/2022 22:30:33 - INFO - __main__ - Step 300 Global step 300 Train loss 6.90 on epoch=149
06/23/2022 22:30:36 - INFO - __main__ - Global step 300 Train loss 6.92 ACC 0.0 on epoch=149
06/23/2022 22:30:37 - INFO - __main__ - Step 310 Global step 310 Train loss 6.79 on epoch=154
06/23/2022 22:30:39 - INFO - __main__ - Step 320 Global step 320 Train loss 6.72 on epoch=159
06/23/2022 22:30:40 - INFO - __main__ - Step 330 Global step 330 Train loss 6.75 on epoch=164
06/23/2022 22:30:42 - INFO - __main__ - Step 340 Global step 340 Train loss 6.75 on epoch=169
06/23/2022 22:30:43 - INFO - __main__ - Step 350 Global step 350 Train loss 6.70 on epoch=174
06/23/2022 22:30:49 - INFO - __main__ - Global step 350 Train loss 6.74 ACC 0.0 on epoch=174
06/23/2022 22:30:50 - INFO - __main__ - Step 360 Global step 360 Train loss 6.72 on epoch=179
06/23/2022 22:30:51 - INFO - __main__ - Step 370 Global step 370 Train loss 6.63 on epoch=184
06/23/2022 22:30:53 - INFO - __main__ - Step 380 Global step 380 Train loss 6.58 on epoch=189
06/23/2022 22:30:54 - INFO - __main__ - Step 390 Global step 390 Train loss 6.55 on epoch=194
06/23/2022 22:30:56 - INFO - __main__ - Step 400 Global step 400 Train loss 6.56 on epoch=199
06/23/2022 22:31:02 - INFO - __main__ - Global step 400 Train loss 6.61 ACC 0.0 on epoch=199
06/23/2022 22:31:03 - INFO - __main__ - Step 410 Global step 410 Train loss 6.45 on epoch=204
06/23/2022 22:31:04 - INFO - __main__ - Step 420 Global step 420 Train loss 6.55 on epoch=209
06/23/2022 22:31:06 - INFO - __main__ - Step 430 Global step 430 Train loss 6.48 on epoch=214
06/23/2022 22:31:07 - INFO - __main__ - Step 440 Global step 440 Train loss 6.23 on epoch=219
06/23/2022 22:31:09 - INFO - __main__ - Step 450 Global step 450 Train loss 6.23 on epoch=224
06/23/2022 22:31:15 - INFO - __main__ - Global step 450 Train loss 6.39 ACC 0.0 on epoch=224
06/23/2022 22:31:16 - INFO - __main__ - Step 460 Global step 460 Train loss 6.26 on epoch=229
06/23/2022 22:31:18 - INFO - __main__ - Step 470 Global step 470 Train loss 6.14 on epoch=234
06/23/2022 22:31:19 - INFO - __main__ - Step 480 Global step 480 Train loss 6.15 on epoch=239
06/23/2022 22:31:21 - INFO - __main__ - Step 490 Global step 490 Train loss 6.07 on epoch=244
06/23/2022 22:31:22 - INFO - __main__ - Step 500 Global step 500 Train loss 6.03 on epoch=249
06/23/2022 22:31:25 - INFO - __main__ - Global step 500 Train loss 6.13 ACC 0.0 on epoch=249
06/23/2022 22:31:26 - INFO - __main__ - Step 510 Global step 510 Train loss 5.97 on epoch=254
06/23/2022 22:31:28 - INFO - __main__ - Step 520 Global step 520 Train loss 5.99 on epoch=259
06/23/2022 22:31:29 - INFO - __main__ - Step 530 Global step 530 Train loss 5.86 on epoch=264
06/23/2022 22:31:31 - INFO - __main__ - Step 540 Global step 540 Train loss 5.92 on epoch=269
06/23/2022 22:31:32 - INFO - __main__ - Step 550 Global step 550 Train loss 5.85 on epoch=274
06/23/2022 22:31:35 - INFO - __main__ - Global step 550 Train loss 5.92 ACC 0.0 on epoch=274
06/23/2022 22:31:36 - INFO - __main__ - Step 560 Global step 560 Train loss 5.81 on epoch=279
06/23/2022 22:31:37 - INFO - __main__ - Step 570 Global step 570 Train loss 5.79 on epoch=284
06/23/2022 22:31:39 - INFO - __main__ - Step 580 Global step 580 Train loss 5.78 on epoch=289
06/23/2022 22:31:40 - INFO - __main__ - Step 590 Global step 590 Train loss 5.71 on epoch=294
06/23/2022 22:31:42 - INFO - __main__ - Step 600 Global step 600 Train loss 5.78 on epoch=299
06/23/2022 22:31:43 - INFO - __main__ - Global step 600 Train loss 5.77 ACC 0.0 on epoch=299
06/23/2022 22:31:45 - INFO - __main__ - Step 610 Global step 610 Train loss 5.57 on epoch=304
06/23/2022 22:31:46 - INFO - __main__ - Step 620 Global step 620 Train loss 5.59 on epoch=309
06/23/2022 22:31:47 - INFO - __main__ - Step 630 Global step 630 Train loss 5.58 on epoch=314
06/23/2022 22:31:49 - INFO - __main__ - Step 640 Global step 640 Train loss 5.55 on epoch=319
06/23/2022 22:31:50 - INFO - __main__ - Step 650 Global step 650 Train loss 5.61 on epoch=324
06/23/2022 22:31:54 - INFO - __main__ - Global step 650 Train loss 5.58 ACC 0.0 on epoch=324
06/23/2022 22:31:55 - INFO - __main__ - Step 660 Global step 660 Train loss 5.54 on epoch=329
06/23/2022 22:31:57 - INFO - __main__ - Step 670 Global step 670 Train loss 5.44 on epoch=334
06/23/2022 22:31:58 - INFO - __main__ - Step 680 Global step 680 Train loss 5.53 on epoch=339
06/23/2022 22:31:59 - INFO - __main__ - Step 690 Global step 690 Train loss 5.36 on epoch=344
06/23/2022 22:32:01 - INFO - __main__ - Step 700 Global step 700 Train loss 5.43 on epoch=349
06/23/2022 22:32:04 - INFO - __main__ - Global step 700 Train loss 5.46 ACC 0.0 on epoch=349
06/23/2022 22:32:05 - INFO - __main__ - Step 710 Global step 710 Train loss 5.32 on epoch=354
06/23/2022 22:32:07 - INFO - __main__ - Step 720 Global step 720 Train loss 5.20 on epoch=359
06/23/2022 22:32:08 - INFO - __main__ - Step 730 Global step 730 Train loss 5.10 on epoch=364
06/23/2022 22:32:09 - INFO - __main__ - Step 740 Global step 740 Train loss 5.22 on epoch=369
06/23/2022 22:32:11 - INFO - __main__ - Step 750 Global step 750 Train loss 5.07 on epoch=374
06/23/2022 22:32:14 - INFO - __main__ - Global step 750 Train loss 5.18 ACC 0.0 on epoch=374
06/23/2022 22:32:15 - INFO - __main__ - Step 760 Global step 760 Train loss 5.14 on epoch=379
06/23/2022 22:32:17 - INFO - __main__ - Step 770 Global step 770 Train loss 4.99 on epoch=384
06/23/2022 22:32:18 - INFO - __main__ - Step 780 Global step 780 Train loss 4.85 on epoch=389
06/23/2022 22:32:19 - INFO - __main__ - Step 790 Global step 790 Train loss 4.83 on epoch=394
06/23/2022 22:32:21 - INFO - __main__ - Step 800 Global step 800 Train loss 4.94 on epoch=399
06/23/2022 22:32:28 - INFO - __main__ - Global step 800 Train loss 4.95 ACC 0.0 on epoch=399
06/23/2022 22:32:29 - INFO - __main__ - Step 810 Global step 810 Train loss 4.73 on epoch=404
06/23/2022 22:32:30 - INFO - __main__ - Step 820 Global step 820 Train loss 4.74 on epoch=409
06/23/2022 22:32:32 - INFO - __main__ - Step 830 Global step 830 Train loss 4.69 on epoch=414
06/23/2022 22:32:33 - INFO - __main__ - Step 840 Global step 840 Train loss 4.66 on epoch=419
06/23/2022 22:32:35 - INFO - __main__ - Step 850 Global step 850 Train loss 4.53 on epoch=424
06/23/2022 22:32:37 - INFO - __main__ - Global step 850 Train loss 4.67 ACC 0.0 on epoch=424
06/23/2022 22:32:39 - INFO - __main__ - Step 860 Global step 860 Train loss 4.54 on epoch=429
06/23/2022 22:32:40 - INFO - __main__ - Step 870 Global step 870 Train loss 4.52 on epoch=434
06/23/2022 22:32:42 - INFO - __main__ - Step 880 Global step 880 Train loss 4.65 on epoch=439
06/23/2022 22:32:43 - INFO - __main__ - Step 890 Global step 890 Train loss 4.46 on epoch=444
06/23/2022 22:32:45 - INFO - __main__ - Step 900 Global step 900 Train loss 4.36 on epoch=449
06/23/2022 22:32:55 - INFO - __main__ - Global step 900 Train loss 4.51 ACC 0.0 on epoch=449
06/23/2022 22:32:56 - INFO - __main__ - Step 910 Global step 910 Train loss 4.46 on epoch=454
06/23/2022 22:32:57 - INFO - __main__ - Step 920 Global step 920 Train loss 4.36 on epoch=459
06/23/2022 22:32:59 - INFO - __main__ - Step 930 Global step 930 Train loss 4.24 on epoch=464
06/23/2022 22:33:00 - INFO - __main__ - Step 940 Global step 940 Train loss 4.25 on epoch=469
06/23/2022 22:33:01 - INFO - __main__ - Step 950 Global step 950 Train loss 4.30 on epoch=474
06/23/2022 22:33:08 - INFO - __main__ - Global step 950 Train loss 4.32 ACC 0.0 on epoch=474
06/23/2022 22:33:09 - INFO - __main__ - Step 960 Global step 960 Train loss 4.21 on epoch=479
06/23/2022 22:33:11 - INFO - __main__ - Step 970 Global step 970 Train loss 4.03 on epoch=484
06/23/2022 22:33:12 - INFO - __main__ - Step 980 Global step 980 Train loss 4.07 on epoch=489
06/23/2022 22:33:13 - INFO - __main__ - Step 990 Global step 990 Train loss 4.06 on epoch=494
06/23/2022 22:33:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 4.01 on epoch=499
06/23/2022 22:33:25 - INFO - __main__ - Global step 1000 Train loss 4.08 ACC 0.1875 on epoch=499
06/23/2022 22:33:25 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.1875 on epoch=499, global_step=1000
06/23/2022 22:33:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 4.00 on epoch=504
06/23/2022 22:33:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.99 on epoch=509
06/23/2022 22:33:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.85 on epoch=514
06/23/2022 22:33:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.87 on epoch=519
06/23/2022 22:33:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.87 on epoch=524
06/23/2022 22:33:39 - INFO - __main__ - Global step 1050 Train loss 3.92 ACC 0.34375 on epoch=524
06/23/2022 22:33:39 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.34375 on epoch=524, global_step=1050
06/23/2022 22:33:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.86 on epoch=529
06/23/2022 22:33:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.76 on epoch=534
06/23/2022 22:33:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.78 on epoch=539
06/23/2022 22:33:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.81 on epoch=544
06/23/2022 22:33:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.86 on epoch=549
06/23/2022 22:33:48 - INFO - __main__ - Global step 1100 Train loss 3.81 ACC 0.34375 on epoch=549
06/23/2022 22:33:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.74 on epoch=554
06/23/2022 22:33:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.62 on epoch=559
06/23/2022 22:33:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.75 on epoch=564
06/23/2022 22:33:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.75 on epoch=569
06/23/2022 22:33:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.63 on epoch=574
06/23/2022 22:33:56 - INFO - __main__ - Global step 1150 Train loss 3.69 ACC 0.125 on epoch=574
06/23/2022 22:33:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.67 on epoch=579
06/23/2022 22:33:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.58 on epoch=584
06/23/2022 22:34:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.62 on epoch=589
06/23/2022 22:34:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.52 on epoch=594
06/23/2022 22:34:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.52 on epoch=599
06/23/2022 22:34:05 - INFO - __main__ - Global step 1200 Train loss 3.58 ACC 0.25 on epoch=599
06/23/2022 22:34:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.44 on epoch=604
06/23/2022 22:34:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.47 on epoch=609
06/23/2022 22:34:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.56 on epoch=614
06/23/2022 22:34:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.43 on epoch=619
06/23/2022 22:34:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.45 on epoch=624
06/23/2022 22:34:15 - INFO - __main__ - Global step 1250 Train loss 3.47 ACC 0.1875 on epoch=624
06/23/2022 22:34:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.34 on epoch=629
06/23/2022 22:34:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.33 on epoch=634
06/23/2022 22:34:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.27 on epoch=639
06/23/2022 22:34:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.25 on epoch=644
06/23/2022 22:34:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.21 on epoch=649
06/23/2022 22:34:23 - INFO - __main__ - Global step 1300 Train loss 3.28 ACC 0.5 on epoch=649
06/23/2022 22:34:23 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.5 on epoch=649, global_step=1300
06/23/2022 22:34:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.24 on epoch=654
06/23/2022 22:34:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.30 on epoch=659
06/23/2022 22:34:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 3.18 on epoch=664
06/23/2022 22:34:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 3.06 on epoch=669
06/23/2022 22:34:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 3.10 on epoch=674
06/23/2022 22:34:33 - INFO - __main__ - Global step 1350 Train loss 3.18 ACC 0.46875 on epoch=674
06/23/2022 22:34:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.98 on epoch=679
06/23/2022 22:34:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.98 on epoch=684
06/23/2022 22:34:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.95 on epoch=689
06/23/2022 22:34:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.86 on epoch=694
06/23/2022 22:34:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.92 on epoch=699
06/23/2022 22:34:41 - INFO - __main__ - Global step 1400 Train loss 2.94 ACC 0.5 on epoch=699
06/23/2022 22:34:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.90 on epoch=704
06/23/2022 22:34:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.80 on epoch=709
06/23/2022 22:34:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.87 on epoch=714
06/23/2022 22:34:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.90 on epoch=719
06/23/2022 22:34:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.71 on epoch=724
06/23/2022 22:34:50 - INFO - __main__ - Global step 1450 Train loss 2.83 ACC 0.5 on epoch=724
06/23/2022 22:34:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.94 on epoch=729
06/23/2022 22:34:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.69 on epoch=734
06/23/2022 22:34:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.74 on epoch=739
06/23/2022 22:34:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.68 on epoch=744
06/23/2022 22:34:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.58 on epoch=749
06/23/2022 22:34:59 - INFO - __main__ - Global step 1500 Train loss 2.73 ACC 0.5 on epoch=749
06/23/2022 22:35:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.68 on epoch=754
06/23/2022 22:35:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.66 on epoch=759
06/23/2022 22:35:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 2.46 on epoch=764
06/23/2022 22:35:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 2.60 on epoch=769
06/23/2022 22:35:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 2.57 on epoch=774
06/23/2022 22:35:08 - INFO - __main__ - Global step 1550 Train loss 2.59 ACC 0.5 on epoch=774
06/23/2022 22:35:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 2.42 on epoch=779
06/23/2022 22:35:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 2.46 on epoch=784
06/23/2022 22:35:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 2.32 on epoch=789
06/23/2022 22:35:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 2.31 on epoch=794
06/23/2022 22:35:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 2.33 on epoch=799
06/23/2022 22:35:17 - INFO - __main__ - Global step 1600 Train loss 2.37 ACC 0.5 on epoch=799
06/23/2022 22:35:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 2.33 on epoch=804
06/23/2022 22:35:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.18 on epoch=809
06/23/2022 22:35:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 2.22 on epoch=814
06/23/2022 22:35:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 2.16 on epoch=819
06/23/2022 22:35:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.97 on epoch=824
06/23/2022 22:35:27 - INFO - __main__ - Global step 1650 Train loss 2.17 ACC 0.5 on epoch=824
06/23/2022 22:35:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 2.08 on epoch=829
06/23/2022 22:35:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 2.11 on epoch=834
06/23/2022 22:35:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.98 on epoch=839
06/23/2022 22:35:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 2.02 on epoch=844
06/23/2022 22:35:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.98 on epoch=849
06/23/2022 22:35:35 - INFO - __main__ - Global step 1700 Train loss 2.03 ACC 0.5 on epoch=849
06/23/2022 22:35:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.92 on epoch=854
06/23/2022 22:35:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.91 on epoch=859
06/23/2022 22:35:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.86 on epoch=864
06/23/2022 22:35:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.83 on epoch=869
06/23/2022 22:35:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.71 on epoch=874
06/23/2022 22:35:45 - INFO - __main__ - Global step 1750 Train loss 1.85 ACC 0.5 on epoch=874
06/23/2022 22:35:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.84 on epoch=879
06/23/2022 22:35:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.76 on epoch=884
06/23/2022 22:35:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.70 on epoch=889
06/23/2022 22:35:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.60 on epoch=894
06/23/2022 22:35:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.66 on epoch=899
06/23/2022 22:36:03 - INFO - __main__ - Global step 1800 Train loss 1.71 ACC 0.4375 on epoch=899
06/23/2022 22:36:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.71 on epoch=904
06/23/2022 22:36:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.62 on epoch=909
06/23/2022 22:36:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.63 on epoch=914
06/23/2022 22:36:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.51 on epoch=919
06/23/2022 22:36:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.49 on epoch=924
06/23/2022 22:36:12 - INFO - __main__ - Global step 1850 Train loss 1.59 ACC 0.5 on epoch=924
06/23/2022 22:36:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.56 on epoch=929
06/23/2022 22:36:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.43 on epoch=934
06/23/2022 22:36:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.37 on epoch=939
06/23/2022 22:36:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.45 on epoch=944
06/23/2022 22:36:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.32 on epoch=949
06/23/2022 22:36:25 - INFO - __main__ - Global step 1900 Train loss 1.42 ACC 0.5 on epoch=949
06/23/2022 22:36:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.33 on epoch=954
06/23/2022 22:36:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.31 on epoch=959
06/23/2022 22:36:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.30 on epoch=964
06/23/2022 22:36:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.40 on epoch=969
06/23/2022 22:36:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.39 on epoch=974
06/23/2022 22:36:33 - INFO - __main__ - Global step 1950 Train loss 1.35 ACC 0.5 on epoch=974
06/23/2022 22:36:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.32 on epoch=979
06/23/2022 22:36:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.14 on epoch=984
06/23/2022 22:36:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.17 on epoch=989
06/23/2022 22:36:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.18 on epoch=994
06/23/2022 22:36:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.20 on epoch=999
06/23/2022 22:36:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 22:36:42 - INFO - __main__ - Printing 3 examples
06/23/2022 22:36:42 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/23/2022 22:36:42 - INFO - __main__ - ['not_duplicate']
06/23/2022 22:36:42 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/23/2022 22:36:42 - INFO - __main__ - ['not_duplicate']
06/23/2022 22:36:42 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/23/2022 22:36:42 - INFO - __main__ - ['not_duplicate']
06/23/2022 22:36:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 22:36:42 - INFO - __main__ - Tokenizing Output ...
06/23/2022 22:36:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 22:36:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 22:36:42 - INFO - __main__ - Printing 3 examples
06/23/2022 22:36:42 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/23/2022 22:36:42 - INFO - __main__ - ['not_duplicate']
06/23/2022 22:36:42 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/23/2022 22:36:42 - INFO - __main__ - ['not_duplicate']
06/23/2022 22:36:42 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/23/2022 22:36:42 - INFO - __main__ - ['not_duplicate']
06/23/2022 22:36:42 - INFO - __main__ - Tokenizing Input ...
06/23/2022 22:36:43 - INFO - __main__ - Tokenizing Output ...
06/23/2022 22:36:43 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 22:36:43 - INFO - __main__ - Global step 2000 Train loss 1.20 ACC 0.5 on epoch=999
06/23/2022 22:36:43 - INFO - __main__ - save last model!
06/23/2022 22:36:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 22:36:43 - INFO - __main__ - Start tokenizing ... 40430 instances
06/23/2022 22:36:43 - INFO - __main__ - Printing 3 examples
06/23/2022 22:36:43 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/23/2022 22:36:43 - INFO - __main__ - ['not_duplicate']
06/23/2022 22:36:43 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/23/2022 22:36:43 - INFO - __main__ - ['not_duplicate']
06/23/2022 22:36:43 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/23/2022 22:36:43 - INFO - __main__ - ['duplicate']
06/23/2022 22:36:43 - INFO - __main__ - Tokenizing Input ...
06/23/2022 22:36:49 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 22:36:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 22:36:50 - INFO - __main__ - Starting training!
06/23/2022 22:37:06 - INFO - __main__ - Tokenizing Output ...
06/23/2022 22:37:48 - INFO - __main__ - Loaded 40430 examples from test data
06/23/2022 23:19:50 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_13_0.2_8_predictions.txt
06/23/2022 23:19:50 - INFO - __main__ - ACC on test data: 0.3682
06/23/2022 23:19:50 - INFO - __main__ - prefix=glue-qqp_16_13, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.36816720257234725
06/23/2022 23:19:50 - INFO - __main__ - Running ... prefix=glue-qqp_16_21, lr=0.5, bsz=8 ...
06/23/2022 23:19:51 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 23:19:51 - INFO - __main__ - Printing 3 examples
06/23/2022 23:19:51 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/23/2022 23:19:51 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:19:51 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/23/2022 23:19:51 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:19:51 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/23/2022 23:19:51 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:19:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 23:19:51 - INFO - __main__ - Tokenizing Output ...
06/23/2022 23:19:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 23:19:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 23:19:52 - INFO - __main__ - Printing 3 examples
06/23/2022 23:19:52 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/23/2022 23:19:52 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:19:52 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/23/2022 23:19:52 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:19:52 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/23/2022 23:19:52 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:19:52 - INFO - __main__ - Tokenizing Input ...
06/23/2022 23:19:52 - INFO - __main__ - Tokenizing Output ...
06/23/2022 23:19:52 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 23:19:57 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 23:19:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 23:19:58 - INFO - __main__ - Starting training!
06/23/2022 23:19:59 - INFO - __main__ - Step 10 Global step 10 Train loss 6.84 on epoch=4
06/23/2022 23:20:01 - INFO - __main__ - Step 20 Global step 20 Train loss 6.83 on epoch=9
06/23/2022 23:20:02 - INFO - __main__ - Step 30 Global step 30 Train loss 6.72 on epoch=14
06/23/2022 23:20:03 - INFO - __main__ - Step 40 Global step 40 Train loss 6.75 on epoch=19
06/23/2022 23:20:05 - INFO - __main__ - Step 50 Global step 50 Train loss 6.76 on epoch=24
06/23/2022 23:20:07 - INFO - __main__ - Global step 50 Train loss 6.78 ACC 0.0 on epoch=24
06/23/2022 23:20:07 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 23:20:08 - INFO - __main__ - Step 60 Global step 60 Train loss 6.73 on epoch=29
06/23/2022 23:20:10 - INFO - __main__ - Step 70 Global step 70 Train loss 6.66 on epoch=34
06/23/2022 23:20:11 - INFO - __main__ - Step 80 Global step 80 Train loss 6.67 on epoch=39
06/23/2022 23:20:13 - INFO - __main__ - Step 90 Global step 90 Train loss 6.59 on epoch=44
06/23/2022 23:20:14 - INFO - __main__ - Step 100 Global step 100 Train loss 6.48 on epoch=49
06/23/2022 23:20:18 - INFO - __main__ - Global step 100 Train loss 6.62 ACC 0.0 on epoch=49
06/23/2022 23:20:19 - INFO - __main__ - Step 110 Global step 110 Train loss 6.11 on epoch=54
06/23/2022 23:20:20 - INFO - __main__ - Step 120 Global step 120 Train loss 5.94 on epoch=59
06/23/2022 23:20:22 - INFO - __main__ - Step 130 Global step 130 Train loss 5.92 on epoch=64
06/23/2022 23:20:23 - INFO - __main__ - Step 140 Global step 140 Train loss 5.82 on epoch=69
06/23/2022 23:20:25 - INFO - __main__ - Step 150 Global step 150 Train loss 5.74 on epoch=74
06/23/2022 23:20:28 - INFO - __main__ - Global step 150 Train loss 5.91 ACC 0.0 on epoch=74
06/23/2022 23:20:30 - INFO - __main__ - Step 160 Global step 160 Train loss 5.68 on epoch=79
06/23/2022 23:20:31 - INFO - __main__ - Step 170 Global step 170 Train loss 5.63 on epoch=84
06/23/2022 23:20:33 - INFO - __main__ - Step 180 Global step 180 Train loss 5.55 on epoch=89
06/23/2022 23:20:34 - INFO - __main__ - Step 190 Global step 190 Train loss 5.55 on epoch=94
06/23/2022 23:20:35 - INFO - __main__ - Step 200 Global step 200 Train loss 5.42 on epoch=99
06/23/2022 23:20:38 - INFO - __main__ - Global step 200 Train loss 5.57 ACC 0.0 on epoch=99
06/23/2022 23:20:39 - INFO - __main__ - Step 210 Global step 210 Train loss 5.43 on epoch=104
06/23/2022 23:20:41 - INFO - __main__ - Step 220 Global step 220 Train loss 5.39 on epoch=109
06/23/2022 23:20:42 - INFO - __main__ - Step 230 Global step 230 Train loss 5.24 on epoch=114
06/23/2022 23:20:43 - INFO - __main__ - Step 240 Global step 240 Train loss 5.14 on epoch=119
06/23/2022 23:20:45 - INFO - __main__ - Step 250 Global step 250 Train loss 5.02 on epoch=124
06/23/2022 23:20:49 - INFO - __main__ - Global step 250 Train loss 5.25 ACC 0.0 on epoch=124
06/23/2022 23:20:50 - INFO - __main__ - Step 260 Global step 260 Train loss 4.92 on epoch=129
06/23/2022 23:20:51 - INFO - __main__ - Step 270 Global step 270 Train loss 4.83 on epoch=134
06/23/2022 23:20:53 - INFO - __main__ - Step 280 Global step 280 Train loss 4.77 on epoch=139
06/23/2022 23:20:54 - INFO - __main__ - Step 290 Global step 290 Train loss 4.58 on epoch=144
06/23/2022 23:20:56 - INFO - __main__ - Step 300 Global step 300 Train loss 4.45 on epoch=149
06/23/2022 23:21:07 - INFO - __main__ - Global step 300 Train loss 4.71 ACC 0.0 on epoch=149
06/23/2022 23:21:08 - INFO - __main__ - Step 310 Global step 310 Train loss 4.25 on epoch=154
06/23/2022 23:21:10 - INFO - __main__ - Step 320 Global step 320 Train loss 4.01 on epoch=159
06/23/2022 23:21:11 - INFO - __main__ - Step 330 Global step 330 Train loss 3.87 on epoch=164
06/23/2022 23:21:12 - INFO - __main__ - Step 340 Global step 340 Train loss 3.71 on epoch=169
06/23/2022 23:21:14 - INFO - __main__ - Step 350 Global step 350 Train loss 3.72 on epoch=174
06/23/2022 23:21:26 - INFO - __main__ - Global step 350 Train loss 3.91 ACC 0.0 on epoch=174
06/23/2022 23:21:27 - INFO - __main__ - Step 360 Global step 360 Train loss 3.57 on epoch=179
06/23/2022 23:21:28 - INFO - __main__ - Step 370 Global step 370 Train loss 3.41 on epoch=184
06/23/2022 23:21:30 - INFO - __main__ - Step 380 Global step 380 Train loss 3.52 on epoch=189
06/23/2022 23:21:31 - INFO - __main__ - Step 390 Global step 390 Train loss 3.24 on epoch=194
06/23/2022 23:21:33 - INFO - __main__ - Step 400 Global step 400 Train loss 3.16 on epoch=199
06/23/2022 23:21:39 - INFO - __main__ - Global step 400 Train loss 3.38 ACC 0.0 on epoch=199
06/23/2022 23:21:40 - INFO - __main__ - Step 410 Global step 410 Train loss 3.14 on epoch=204
06/23/2022 23:21:42 - INFO - __main__ - Step 420 Global step 420 Train loss 3.06 on epoch=209
06/23/2022 23:21:43 - INFO - __main__ - Step 430 Global step 430 Train loss 3.07 on epoch=214
06/23/2022 23:21:45 - INFO - __main__ - Step 440 Global step 440 Train loss 2.87 on epoch=219
06/23/2022 23:21:46 - INFO - __main__ - Step 450 Global step 450 Train loss 2.84 on epoch=224
06/23/2022 23:21:47 - INFO - __main__ - Global step 450 Train loss 3.00 ACC 0.375 on epoch=224
06/23/2022 23:21:47 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.375 on epoch=224, global_step=450
06/23/2022 23:21:48 - INFO - __main__ - Step 460 Global step 460 Train loss 2.68 on epoch=229
06/23/2022 23:21:50 - INFO - __main__ - Step 470 Global step 470 Train loss 2.57 on epoch=234
06/23/2022 23:21:51 - INFO - __main__ - Step 480 Global step 480 Train loss 2.46 on epoch=239
06/23/2022 23:21:52 - INFO - __main__ - Step 490 Global step 490 Train loss 2.44 on epoch=244
06/23/2022 23:21:54 - INFO - __main__ - Step 500 Global step 500 Train loss 2.36 on epoch=249
06/23/2022 23:22:00 - INFO - __main__ - Global step 500 Train loss 2.50 ACC 0.46875 on epoch=249
06/23/2022 23:22:00 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.46875 on epoch=249, global_step=500
06/23/2022 23:22:02 - INFO - __main__ - Step 510 Global step 510 Train loss 2.19 on epoch=254
06/23/2022 23:22:03 - INFO - __main__ - Step 520 Global step 520 Train loss 2.16 on epoch=259
06/23/2022 23:22:05 - INFO - __main__ - Step 530 Global step 530 Train loss 2.15 on epoch=264
06/23/2022 23:22:06 - INFO - __main__ - Step 540 Global step 540 Train loss 2.20 on epoch=269
06/23/2022 23:22:07 - INFO - __main__ - Step 550 Global step 550 Train loss 1.90 on epoch=274
06/23/2022 23:22:18 - INFO - __main__ - Global step 550 Train loss 2.12 ACC 0.4375 on epoch=274
06/23/2022 23:22:19 - INFO - __main__ - Step 560 Global step 560 Train loss 1.85 on epoch=279
06/23/2022 23:22:21 - INFO - __main__ - Step 570 Global step 570 Train loss 1.91 on epoch=284
06/23/2022 23:22:22 - INFO - __main__ - Step 580 Global step 580 Train loss 1.81 on epoch=289
06/23/2022 23:22:24 - INFO - __main__ - Step 590 Global step 590 Train loss 1.83 on epoch=294
06/23/2022 23:22:25 - INFO - __main__ - Step 600 Global step 600 Train loss 1.78 on epoch=299
06/23/2022 23:22:32 - INFO - __main__ - Global step 600 Train loss 1.83 ACC 0.5 on epoch=299
06/23/2022 23:22:32 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=299, global_step=600
06/23/2022 23:22:33 - INFO - __main__ - Step 610 Global step 610 Train loss 1.75 on epoch=304
06/23/2022 23:22:35 - INFO - __main__ - Step 620 Global step 620 Train loss 1.59 on epoch=309
06/23/2022 23:22:36 - INFO - __main__ - Step 630 Global step 630 Train loss 1.59 on epoch=314
06/23/2022 23:22:37 - INFO - __main__ - Step 640 Global step 640 Train loss 1.64 on epoch=319
06/23/2022 23:22:39 - INFO - __main__ - Step 650 Global step 650 Train loss 1.54 on epoch=324
06/23/2022 23:22:41 - INFO - __main__ - Global step 650 Train loss 1.62 ACC 0.5 on epoch=324
06/23/2022 23:22:42 - INFO - __main__ - Step 660 Global step 660 Train loss 1.49 on epoch=329
06/23/2022 23:22:44 - INFO - __main__ - Step 670 Global step 670 Train loss 1.44 on epoch=334
06/23/2022 23:22:45 - INFO - __main__ - Step 680 Global step 680 Train loss 1.38 on epoch=339
06/23/2022 23:22:47 - INFO - __main__ - Step 690 Global step 690 Train loss 1.21 on epoch=344
06/23/2022 23:22:48 - INFO - __main__ - Step 700 Global step 700 Train loss 1.26 on epoch=349
06/23/2022 23:22:50 - INFO - __main__ - Global step 700 Train loss 1.36 ACC 0.5 on epoch=349
06/23/2022 23:22:52 - INFO - __main__ - Step 710 Global step 710 Train loss 1.16 on epoch=354
06/23/2022 23:22:53 - INFO - __main__ - Step 720 Global step 720 Train loss 1.17 on epoch=359
06/23/2022 23:22:55 - INFO - __main__ - Step 730 Global step 730 Train loss 1.14 on epoch=364
06/23/2022 23:22:56 - INFO - __main__ - Step 740 Global step 740 Train loss 1.16 on epoch=369
06/23/2022 23:22:58 - INFO - __main__ - Step 750 Global step 750 Train loss 1.10 on epoch=374
06/23/2022 23:23:01 - INFO - __main__ - Global step 750 Train loss 1.15 ACC 0.5 on epoch=374
06/23/2022 23:23:02 - INFO - __main__ - Step 760 Global step 760 Train loss 1.14 on epoch=379
06/23/2022 23:23:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.97 on epoch=384
06/23/2022 23:23:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.93 on epoch=389
06/23/2022 23:23:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.92 on epoch=394
06/23/2022 23:23:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.95 on epoch=399
06/23/2022 23:23:11 - INFO - __main__ - Global step 800 Train loss 0.98 ACC 0.5 on epoch=399
06/23/2022 23:23:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.84 on epoch=404
06/23/2022 23:23:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.85 on epoch=409
06/23/2022 23:23:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.83 on epoch=414
06/23/2022 23:23:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.85 on epoch=419
06/23/2022 23:23:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.79 on epoch=424
06/23/2022 23:23:22 - INFO - __main__ - Global step 850 Train loss 0.83 ACC 0.5 on epoch=424
06/23/2022 23:23:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.81 on epoch=429
06/23/2022 23:23:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.79 on epoch=434
06/23/2022 23:23:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.82 on epoch=439
06/23/2022 23:23:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.77 on epoch=444
06/23/2022 23:23:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.77 on epoch=449
06/23/2022 23:23:31 - INFO - __main__ - Global step 900 Train loss 0.79 ACC 0.5 on epoch=449
06/23/2022 23:23:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.79 on epoch=454
06/23/2022 23:23:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.77 on epoch=459
06/23/2022 23:23:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.73 on epoch=464
06/23/2022 23:23:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.72 on epoch=469
06/23/2022 23:23:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.74 on epoch=474
06/23/2022 23:23:41 - INFO - __main__ - Global step 950 Train loss 0.75 ACC 0.5 on epoch=474
06/23/2022 23:23:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.64 on epoch=479
06/23/2022 23:23:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.63 on epoch=484
06/23/2022 23:23:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.68 on epoch=489
06/23/2022 23:23:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.67 on epoch=494
06/23/2022 23:23:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.67 on epoch=499
06/23/2022 23:23:51 - INFO - __main__ - Global step 1000 Train loss 0.66 ACC 0.5 on epoch=499
06/23/2022 23:23:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.66 on epoch=504
06/23/2022 23:23:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.76 on epoch=509
06/23/2022 23:23:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.62 on epoch=514
06/23/2022 23:23:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.73 on epoch=519
06/23/2022 23:23:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.65 on epoch=524
06/23/2022 23:24:01 - INFO - __main__ - Global step 1050 Train loss 0.69 ACC 0.5 on epoch=524
06/23/2022 23:24:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.64 on epoch=529
06/23/2022 23:24:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.61 on epoch=534
06/23/2022 23:24:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.62 on epoch=539
06/23/2022 23:24:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.72 on epoch=544
06/23/2022 23:24:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.71 on epoch=549
06/23/2022 23:24:11 - INFO - __main__ - Global step 1100 Train loss 0.66 ACC 0.5 on epoch=549
06/23/2022 23:24:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.64 on epoch=554
06/23/2022 23:24:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.54 on epoch=559
06/23/2022 23:24:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.55 on epoch=564
06/23/2022 23:24:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.54 on epoch=569
06/23/2022 23:24:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.58 on epoch=574
06/23/2022 23:24:22 - INFO - __main__ - Global step 1150 Train loss 0.57 ACC 0.5 on epoch=574
06/23/2022 23:24:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.54 on epoch=579
06/23/2022 23:24:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.51 on epoch=584
06/23/2022 23:24:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.49 on epoch=589
06/23/2022 23:24:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.52 on epoch=594
06/23/2022 23:24:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.50 on epoch=599
06/23/2022 23:24:33 - INFO - __main__ - Global step 1200 Train loss 0.51 ACC 0.5 on epoch=599
06/23/2022 23:24:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.51 on epoch=604
06/23/2022 23:24:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.47 on epoch=609
06/23/2022 23:24:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=614
06/23/2022 23:24:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.50 on epoch=619
06/23/2022 23:24:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.50 on epoch=624
06/23/2022 23:24:47 - INFO - __main__ - Global step 1250 Train loss 0.50 ACC 0.5 on epoch=624
06/23/2022 23:24:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=629
06/23/2022 23:24:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=634
06/23/2022 23:24:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
06/23/2022 23:24:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=644
06/23/2022 23:24:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.52 on epoch=649
06/23/2022 23:25:01 - INFO - __main__ - Global step 1300 Train loss 0.49 ACC 0.5 on epoch=649
06/23/2022 23:25:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.46 on epoch=654
06/23/2022 23:25:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.55 on epoch=659
06/23/2022 23:25:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.44 on epoch=664
06/23/2022 23:25:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.43 on epoch=669
06/23/2022 23:25:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.47 on epoch=674
06/23/2022 23:25:13 - INFO - __main__ - Global step 1350 Train loss 0.47 ACC 0.5 on epoch=674
06/23/2022 23:25:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=679
06/23/2022 23:25:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.49 on epoch=684
06/23/2022 23:25:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.49 on epoch=689
06/23/2022 23:25:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=694
06/23/2022 23:25:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=699
06/23/2022 23:25:26 - INFO - __main__ - Global step 1400 Train loss 0.45 ACC 0.5 on epoch=699
06/23/2022 23:25:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=704
06/23/2022 23:25:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=709
06/23/2022 23:25:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.46 on epoch=714
06/23/2022 23:25:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=719
06/23/2022 23:25:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.45 on epoch=724
06/23/2022 23:25:37 - INFO - __main__ - Global step 1450 Train loss 0.45 ACC 0.5 on epoch=724
06/23/2022 23:25:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.39 on epoch=729
06/23/2022 23:25:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=734
06/23/2022 23:25:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
06/23/2022 23:25:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=744
06/23/2022 23:25:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=749
06/23/2022 23:25:50 - INFO - __main__ - Global step 1500 Train loss 0.39 ACC 0.5 on epoch=749
06/23/2022 23:25:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=754
06/23/2022 23:25:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=759
06/23/2022 23:25:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.46 on epoch=764
06/23/2022 23:25:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=769
06/23/2022 23:25:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.44 on epoch=774
06/23/2022 23:26:01 - INFO - __main__ - Global step 1550 Train loss 0.42 ACC 0.5 on epoch=774
06/23/2022 23:26:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=779
06/23/2022 23:26:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=784
06/23/2022 23:26:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=789
06/23/2022 23:26:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=794
06/23/2022 23:26:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=799
06/23/2022 23:26:16 - INFO - __main__ - Global step 1600 Train loss 0.40 ACC 0.5 on epoch=799
06/23/2022 23:26:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=804
06/23/2022 23:26:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=809
06/23/2022 23:26:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=814
06/23/2022 23:26:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=819
06/23/2022 23:26:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=824
06/23/2022 23:26:31 - INFO - __main__ - Global step 1650 Train loss 0.39 ACC 0.5 on epoch=824
06/23/2022 23:26:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.33 on epoch=829
06/23/2022 23:26:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=834
06/23/2022 23:26:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
06/23/2022 23:26:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=844
06/23/2022 23:26:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=849
06/23/2022 23:26:46 - INFO - __main__ - Global step 1700 Train loss 0.38 ACC 0.5 on epoch=849
06/23/2022 23:26:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
06/23/2022 23:26:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=859
06/23/2022 23:26:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=864
06/23/2022 23:26:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=869
06/23/2022 23:26:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
06/23/2022 23:27:02 - INFO - __main__ - Global step 1750 Train loss 0.36 ACC 0.5 on epoch=874
06/23/2022 23:27:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=879
06/23/2022 23:27:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=884
06/23/2022 23:27:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
06/23/2022 23:27:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=894
06/23/2022 23:27:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.31 on epoch=899
06/23/2022 23:27:19 - INFO - __main__ - Global step 1800 Train loss 0.35 ACC 0.5 on epoch=899
06/23/2022 23:27:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=904
06/23/2022 23:27:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=909
06/23/2022 23:27:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=914
06/23/2022 23:27:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=919
06/23/2022 23:27:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=924
06/23/2022 23:27:28 - INFO - __main__ - Global step 1850 Train loss 0.36 ACC 0.5 on epoch=924
06/23/2022 23:27:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=929
06/23/2022 23:27:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=934
06/23/2022 23:27:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=939
06/23/2022 23:27:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=944
06/23/2022 23:27:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
06/23/2022 23:27:37 - INFO - __main__ - Global step 1900 Train loss 0.33 ACC 0.5 on epoch=949
06/23/2022 23:27:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.30 on epoch=954
06/23/2022 23:27:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=959
06/23/2022 23:27:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.34 on epoch=964
06/23/2022 23:27:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=969
06/23/2022 23:27:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=974
06/23/2022 23:27:46 - INFO - __main__ - Global step 1950 Train loss 0.32 ACC 0.5 on epoch=974
06/23/2022 23:27:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=979
06/23/2022 23:27:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.27 on epoch=984
06/23/2022 23:27:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=989
06/23/2022 23:27:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
06/23/2022 23:27:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=999
06/23/2022 23:27:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 23:27:54 - INFO - __main__ - Printing 3 examples
06/23/2022 23:27:54 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/23/2022 23:27:54 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:27:54 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/23/2022 23:27:54 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:27:54 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/23/2022 23:27:54 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:27:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 23:27:54 - INFO - __main__ - Tokenizing Output ...
06/23/2022 23:27:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 23:27:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 23:27:54 - INFO - __main__ - Global step 2000 Train loss 0.34 ACC 0.5 on epoch=999
06/23/2022 23:27:54 - INFO - __main__ - Printing 3 examples
06/23/2022 23:27:54 - INFO - __main__ - save last model!
06/23/2022 23:27:54 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/23/2022 23:27:54 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:27:54 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/23/2022 23:27:54 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:27:54 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/23/2022 23:27:54 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:27:54 - INFO - __main__ - Tokenizing Input ...
06/23/2022 23:27:54 - INFO - __main__ - Tokenizing Output ...
06/23/2022 23:27:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 23:27:54 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 23:27:54 - INFO - __main__ - Start tokenizing ... 40430 instances
06/23/2022 23:27:54 - INFO - __main__ - Printing 3 examples
06/23/2022 23:27:54 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/23/2022 23:27:54 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:27:54 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/23/2022 23:27:54 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:27:54 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/23/2022 23:27:54 - INFO - __main__ - ['duplicate']
06/23/2022 23:27:54 - INFO - __main__ - Tokenizing Input ...
06/23/2022 23:28:00 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 23:28:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 23:28:00 - INFO - __main__ - Starting training!
06/23/2022 23:28:13 - INFO - __main__ - Tokenizing Output ...
06/23/2022 23:28:53 - INFO - __main__ - Loaded 40430 examples from test data
06/23/2022 23:45:11 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_21_0.5_8_predictions.txt
06/23/2022 23:45:11 - INFO - __main__ - ACC on test data: 0.3700
06/23/2022 23:45:11 - INFO - __main__ - prefix=glue-qqp_16_21, lr=0.5, bsz=8, dev_performance=0.5, test_performance=0.3699727924808311
06/23/2022 23:45:11 - INFO - __main__ - Running ... prefix=glue-qqp_16_21, lr=0.4, bsz=8 ...
06/23/2022 23:45:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 23:45:12 - INFO - __main__ - Printing 3 examples
06/23/2022 23:45:12 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/23/2022 23:45:12 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:45:12 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/23/2022 23:45:12 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:45:12 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/23/2022 23:45:12 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:45:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/23/2022 23:45:12 - INFO - __main__ - Tokenizing Output ...
06/23/2022 23:45:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 23:45:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 23:45:13 - INFO - __main__ - Printing 3 examples
06/23/2022 23:45:13 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/23/2022 23:45:13 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:45:13 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/23/2022 23:45:13 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:45:13 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/23/2022 23:45:13 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:45:13 - INFO - __main__ - Tokenizing Input ...
06/23/2022 23:45:13 - INFO - __main__ - Tokenizing Output ...
06/23/2022 23:45:13 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 23:45:19 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 23:45:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 23:45:19 - INFO - __main__ - Starting training!
06/23/2022 23:45:21 - INFO - __main__ - Step 10 Global step 10 Train loss 6.86 on epoch=4
06/23/2022 23:45:22 - INFO - __main__ - Step 20 Global step 20 Train loss 6.86 on epoch=9
06/23/2022 23:45:24 - INFO - __main__ - Step 30 Global step 30 Train loss 6.75 on epoch=14
06/23/2022 23:45:25 - INFO - __main__ - Step 40 Global step 40 Train loss 6.73 on epoch=19
06/23/2022 23:45:26 - INFO - __main__ - Step 50 Global step 50 Train loss 6.74 on epoch=24
06/23/2022 23:45:28 - INFO - __main__ - Global step 50 Train loss 6.79 ACC 0.0 on epoch=24
06/23/2022 23:45:28 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/23/2022 23:45:29 - INFO - __main__ - Step 60 Global step 60 Train loss 6.77 on epoch=29
06/23/2022 23:45:31 - INFO - __main__ - Step 70 Global step 70 Train loss 6.71 on epoch=34
06/23/2022 23:45:32 - INFO - __main__ - Step 80 Global step 80 Train loss 6.69 on epoch=39
06/23/2022 23:45:33 - INFO - __main__ - Step 90 Global step 90 Train loss 6.70 on epoch=44
06/23/2022 23:45:35 - INFO - __main__ - Step 100 Global step 100 Train loss 6.64 on epoch=49
06/23/2022 23:45:41 - INFO - __main__ - Global step 100 Train loss 6.70 ACC 0.0 on epoch=49
06/23/2022 23:45:43 - INFO - __main__ - Step 110 Global step 110 Train loss 6.57 on epoch=54
06/23/2022 23:45:44 - INFO - __main__ - Step 120 Global step 120 Train loss 6.62 on epoch=59
06/23/2022 23:45:46 - INFO - __main__ - Step 130 Global step 130 Train loss 6.59 on epoch=64
06/23/2022 23:45:47 - INFO - __main__ - Step 140 Global step 140 Train loss 6.62 on epoch=69
06/23/2022 23:45:48 - INFO - __main__ - Step 150 Global step 150 Train loss 6.46 on epoch=74
06/23/2022 23:45:53 - INFO - __main__ - Global step 150 Train loss 6.57 ACC 0.0 on epoch=74
06/23/2022 23:45:54 - INFO - __main__ - Step 160 Global step 160 Train loss 6.54 on epoch=79
06/23/2022 23:45:56 - INFO - __main__ - Step 170 Global step 170 Train loss 6.50 on epoch=84
06/23/2022 23:45:57 - INFO - __main__ - Step 180 Global step 180 Train loss 6.52 on epoch=89
06/23/2022 23:45:58 - INFO - __main__ - Step 190 Global step 190 Train loss 6.34 on epoch=94
06/23/2022 23:46:00 - INFO - __main__ - Step 200 Global step 200 Train loss 6.35 on epoch=99
06/23/2022 23:46:01 - INFO - __main__ - Global step 200 Train loss 6.45 ACC 0.0 on epoch=99
06/23/2022 23:46:02 - INFO - __main__ - Step 210 Global step 210 Train loss 6.29 on epoch=104
06/23/2022 23:46:04 - INFO - __main__ - Step 220 Global step 220 Train loss 6.25 on epoch=109
06/23/2022 23:46:05 - INFO - __main__ - Step 230 Global step 230 Train loss 6.18 on epoch=114
06/23/2022 23:46:07 - INFO - __main__ - Step 240 Global step 240 Train loss 6.13 on epoch=119
06/23/2022 23:46:08 - INFO - __main__ - Step 250 Global step 250 Train loss 6.05 on epoch=124
06/23/2022 23:46:10 - INFO - __main__ - Global step 250 Train loss 6.18 ACC 0.0 on epoch=124
06/23/2022 23:46:11 - INFO - __main__ - Step 260 Global step 260 Train loss 5.99 on epoch=129
06/23/2022 23:46:13 - INFO - __main__ - Step 270 Global step 270 Train loss 5.99 on epoch=134
06/23/2022 23:46:14 - INFO - __main__ - Step 280 Global step 280 Train loss 5.86 on epoch=139
06/23/2022 23:46:16 - INFO - __main__ - Step 290 Global step 290 Train loss 5.78 on epoch=144
06/23/2022 23:46:17 - INFO - __main__ - Step 300 Global step 300 Train loss 5.62 on epoch=149
06/23/2022 23:46:19 - INFO - __main__ - Global step 300 Train loss 5.85 ACC 0.0 on epoch=149
06/23/2022 23:46:20 - INFO - __main__ - Step 310 Global step 310 Train loss 5.63 on epoch=154
06/23/2022 23:46:21 - INFO - __main__ - Step 320 Global step 320 Train loss 5.32 on epoch=159
06/23/2022 23:46:23 - INFO - __main__ - Step 330 Global step 330 Train loss 5.20 on epoch=164
06/23/2022 23:46:24 - INFO - __main__ - Step 340 Global step 340 Train loss 5.12 on epoch=169
06/23/2022 23:46:26 - INFO - __main__ - Step 350 Global step 350 Train loss 5.01 on epoch=174
06/23/2022 23:46:37 - INFO - __main__ - Global step 350 Train loss 5.26 ACC 0.0 on epoch=174
06/23/2022 23:46:39 - INFO - __main__ - Step 360 Global step 360 Train loss 5.01 on epoch=179
06/23/2022 23:46:40 - INFO - __main__ - Step 370 Global step 370 Train loss 4.88 on epoch=184
06/23/2022 23:46:42 - INFO - __main__ - Step 380 Global step 380 Train loss 4.81 on epoch=189
06/23/2022 23:46:43 - INFO - __main__ - Step 390 Global step 390 Train loss 4.79 on epoch=194
06/23/2022 23:46:44 - INFO - __main__ - Step 400 Global step 400 Train loss 4.61 on epoch=199
06/23/2022 23:46:52 - INFO - __main__ - Global step 400 Train loss 4.82 ACC 0.0 on epoch=199
06/23/2022 23:46:53 - INFO - __main__ - Step 410 Global step 410 Train loss 4.51 on epoch=204
06/23/2022 23:46:54 - INFO - __main__ - Step 420 Global step 420 Train loss 4.34 on epoch=209
06/23/2022 23:46:56 - INFO - __main__ - Step 430 Global step 430 Train loss 4.46 on epoch=214
06/23/2022 23:46:57 - INFO - __main__ - Step 440 Global step 440 Train loss 4.26 on epoch=219
06/23/2022 23:46:59 - INFO - __main__ - Step 450 Global step 450 Train loss 4.14 on epoch=224
06/23/2022 23:47:05 - INFO - __main__ - Global step 450 Train loss 4.34 ACC 0.0 on epoch=224
06/23/2022 23:47:06 - INFO - __main__ - Step 460 Global step 460 Train loss 4.06 on epoch=229
06/23/2022 23:47:08 - INFO - __main__ - Step 470 Global step 470 Train loss 4.05 on epoch=234
06/23/2022 23:47:09 - INFO - __main__ - Step 480 Global step 480 Train loss 3.88 on epoch=239
06/23/2022 23:47:11 - INFO - __main__ - Step 490 Global step 490 Train loss 3.80 on epoch=244
06/23/2022 23:47:12 - INFO - __main__ - Step 500 Global step 500 Train loss 3.72 on epoch=249
06/23/2022 23:47:15 - INFO - __main__ - Global step 500 Train loss 3.90 ACC 0.03125 on epoch=249
06/23/2022 23:47:15 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=249, global_step=500
06/23/2022 23:47:16 - INFO - __main__ - Step 510 Global step 510 Train loss 3.87 on epoch=254
06/23/2022 23:47:17 - INFO - __main__ - Step 520 Global step 520 Train loss 3.70 on epoch=259
06/23/2022 23:47:19 - INFO - __main__ - Step 530 Global step 530 Train loss 3.61 on epoch=264
06/23/2022 23:47:20 - INFO - __main__ - Step 540 Global step 540 Train loss 3.43 on epoch=269
06/23/2022 23:47:22 - INFO - __main__ - Step 550 Global step 550 Train loss 3.63 on epoch=274
06/23/2022 23:47:23 - INFO - __main__ - Global step 550 Train loss 3.65 ACC 0.5 on epoch=274
06/23/2022 23:47:23 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.5 on epoch=274, global_step=550
06/23/2022 23:47:25 - INFO - __main__ - Step 560 Global step 560 Train loss 3.38 on epoch=279
06/23/2022 23:47:26 - INFO - __main__ - Step 570 Global step 570 Train loss 3.40 on epoch=284
06/23/2022 23:47:27 - INFO - __main__ - Step 580 Global step 580 Train loss 3.29 on epoch=289
06/23/2022 23:47:29 - INFO - __main__ - Step 590 Global step 590 Train loss 3.16 on epoch=294
06/23/2022 23:47:30 - INFO - __main__ - Step 600 Global step 600 Train loss 3.25 on epoch=299
06/23/2022 23:47:32 - INFO - __main__ - Global step 600 Train loss 3.30 ACC 0.5 on epoch=299
06/23/2022 23:47:33 - INFO - __main__ - Step 610 Global step 610 Train loss 3.14 on epoch=304
06/23/2022 23:47:35 - INFO - __main__ - Step 620 Global step 620 Train loss 3.02 on epoch=309
06/23/2022 23:47:36 - INFO - __main__ - Step 630 Global step 630 Train loss 2.99 on epoch=314
06/23/2022 23:47:38 - INFO - __main__ - Step 640 Global step 640 Train loss 2.95 on epoch=319
06/23/2022 23:47:39 - INFO - __main__ - Step 650 Global step 650 Train loss 2.74 on epoch=324
06/23/2022 23:47:41 - INFO - __main__ - Global step 650 Train loss 2.97 ACC 0.5 on epoch=324
06/23/2022 23:47:42 - INFO - __main__ - Step 660 Global step 660 Train loss 2.76 on epoch=329
06/23/2022 23:47:44 - INFO - __main__ - Step 670 Global step 670 Train loss 2.74 on epoch=334
06/23/2022 23:47:45 - INFO - __main__ - Step 680 Global step 680 Train loss 2.56 on epoch=339
06/23/2022 23:47:47 - INFO - __main__ - Step 690 Global step 690 Train loss 2.51 on epoch=344
06/23/2022 23:47:48 - INFO - __main__ - Step 700 Global step 700 Train loss 2.47 on epoch=349
06/23/2022 23:47:50 - INFO - __main__ - Global step 700 Train loss 2.61 ACC 0.5 on epoch=349
06/23/2022 23:47:52 - INFO - __main__ - Step 710 Global step 710 Train loss 2.43 on epoch=354
06/23/2022 23:47:53 - INFO - __main__ - Step 720 Global step 720 Train loss 2.34 on epoch=359
06/23/2022 23:47:54 - INFO - __main__ - Step 730 Global step 730 Train loss 2.23 on epoch=364
06/23/2022 23:47:56 - INFO - __main__ - Step 740 Global step 740 Train loss 2.07 on epoch=369
06/23/2022 23:47:58 - INFO - __main__ - Step 750 Global step 750 Train loss 2.03 on epoch=374
06/23/2022 23:48:10 - INFO - __main__ - Global step 750 Train loss 2.22 ACC 0.09375 on epoch=374
06/23/2022 23:48:12 - INFO - __main__ - Step 760 Global step 760 Train loss 1.83 on epoch=379
06/23/2022 23:48:13 - INFO - __main__ - Step 770 Global step 770 Train loss 1.82 on epoch=384
06/23/2022 23:48:15 - INFO - __main__ - Step 780 Global step 780 Train loss 1.87 on epoch=389
06/23/2022 23:48:16 - INFO - __main__ - Step 790 Global step 790 Train loss 1.70 on epoch=394
06/23/2022 23:48:17 - INFO - __main__ - Step 800 Global step 800 Train loss 1.65 on epoch=399
06/23/2022 23:48:30 - INFO - __main__ - Global step 800 Train loss 1.77 ACC 0.3125 on epoch=399
06/23/2022 23:48:31 - INFO - __main__ - Step 810 Global step 810 Train loss 1.59 on epoch=404
06/23/2022 23:48:32 - INFO - __main__ - Step 820 Global step 820 Train loss 1.45 on epoch=409
06/23/2022 23:48:34 - INFO - __main__ - Step 830 Global step 830 Train loss 1.53 on epoch=414
06/23/2022 23:48:35 - INFO - __main__ - Step 840 Global step 840 Train loss 1.30 on epoch=419
06/23/2022 23:48:37 - INFO - __main__ - Step 850 Global step 850 Train loss 1.47 on epoch=424
06/23/2022 23:48:49 - INFO - __main__ - Global step 850 Train loss 1.47 ACC 0.21875 on epoch=424
06/23/2022 23:48:51 - INFO - __main__ - Step 860 Global step 860 Train loss 1.35 on epoch=429
06/23/2022 23:48:52 - INFO - __main__ - Step 870 Global step 870 Train loss 1.38 on epoch=434
06/23/2022 23:48:53 - INFO - __main__ - Step 880 Global step 880 Train loss 1.17 on epoch=439
06/23/2022 23:48:55 - INFO - __main__ - Step 890 Global step 890 Train loss 1.23 on epoch=444
06/23/2022 23:48:56 - INFO - __main__ - Step 900 Global step 900 Train loss 1.17 on epoch=449
06/23/2022 23:49:07 - INFO - __main__ - Global step 900 Train loss 1.26 ACC 0.46875 on epoch=449
06/23/2022 23:49:08 - INFO - __main__ - Step 910 Global step 910 Train loss 1.22 on epoch=454
06/23/2022 23:49:09 - INFO - __main__ - Step 920 Global step 920 Train loss 1.22 on epoch=459
06/23/2022 23:49:11 - INFO - __main__ - Step 930 Global step 930 Train loss 1.18 on epoch=464
06/23/2022 23:49:12 - INFO - __main__ - Step 940 Global step 940 Train loss 1.05 on epoch=469
06/23/2022 23:49:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.97 on epoch=474
06/23/2022 23:49:16 - INFO - __main__ - Global step 950 Train loss 1.13 ACC 0.5 on epoch=474
06/23/2022 23:49:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.97 on epoch=479
06/23/2022 23:49:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.98 on epoch=484
06/23/2022 23:49:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.98 on epoch=489
06/23/2022 23:49:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.98 on epoch=494
06/23/2022 23:49:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.98 on epoch=499
06/23/2022 23:49:27 - INFO - __main__ - Global step 1000 Train loss 0.98 ACC 0.5 on epoch=499
06/23/2022 23:49:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.85 on epoch=504
06/23/2022 23:49:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.75 on epoch=509
06/23/2022 23:49:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.76 on epoch=514
06/23/2022 23:49:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.80 on epoch=519
06/23/2022 23:49:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.78 on epoch=524
06/23/2022 23:49:36 - INFO - __main__ - Global step 1050 Train loss 0.79 ACC 0.5 on epoch=524
06/23/2022 23:49:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.75 on epoch=529
06/23/2022 23:49:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.71 on epoch=534
06/23/2022 23:49:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.66 on epoch=539
06/23/2022 23:49:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.72 on epoch=544
06/23/2022 23:49:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.71 on epoch=549
06/23/2022 23:49:44 - INFO - __main__ - Global step 1100 Train loss 0.71 ACC 0.5 on epoch=549
06/23/2022 23:49:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.68 on epoch=554
06/23/2022 23:49:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.68 on epoch=559
06/23/2022 23:49:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.62 on epoch=564
06/23/2022 23:49:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.62 on epoch=569
06/23/2022 23:49:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.66 on epoch=574
06/23/2022 23:49:53 - INFO - __main__ - Global step 1150 Train loss 0.65 ACC 0.5 on epoch=574
06/23/2022 23:49:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.75 on epoch=579
06/23/2022 23:49:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.78 on epoch=584
06/23/2022 23:49:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.61 on epoch=589
06/23/2022 23:49:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.57 on epoch=594
06/23/2022 23:50:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.60 on epoch=599
06/23/2022 23:50:03 - INFO - __main__ - Global step 1200 Train loss 0.66 ACC 0.5 on epoch=599
06/23/2022 23:50:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.58 on epoch=604
06/23/2022 23:50:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.64 on epoch=609
06/23/2022 23:50:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.56 on epoch=614
06/23/2022 23:50:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.51 on epoch=619
06/23/2022 23:50:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.56 on epoch=624
06/23/2022 23:50:16 - INFO - __main__ - Global step 1250 Train loss 0.57 ACC 0.5 on epoch=624
06/23/2022 23:50:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=629
06/23/2022 23:50:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.55 on epoch=634
06/23/2022 23:50:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.59 on epoch=639
06/23/2022 23:50:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.59 on epoch=644
06/23/2022 23:50:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.54 on epoch=649
06/23/2022 23:50:25 - INFO - __main__ - Global step 1300 Train loss 0.57 ACC 0.5 on epoch=649
06/23/2022 23:50:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.53 on epoch=654
06/23/2022 23:50:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=659
06/23/2022 23:50:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.53 on epoch=664
06/23/2022 23:50:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.49 on epoch=669
06/23/2022 23:50:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=674
06/23/2022 23:50:35 - INFO - __main__ - Global step 1350 Train loss 0.51 ACC 0.5 on epoch=674
06/23/2022 23:50:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.52 on epoch=679
06/23/2022 23:50:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=684
06/23/2022 23:50:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.44 on epoch=689
06/23/2022 23:50:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.50 on epoch=694
06/23/2022 23:50:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.50 on epoch=699
06/23/2022 23:50:45 - INFO - __main__ - Global step 1400 Train loss 0.48 ACC 0.5 on epoch=699
06/23/2022 23:50:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.54 on epoch=704
06/23/2022 23:50:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=709
06/23/2022 23:50:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=714
06/23/2022 23:50:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=719
06/23/2022 23:50:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=724
06/23/2022 23:50:56 - INFO - __main__ - Global step 1450 Train loss 0.45 ACC 0.5 on epoch=724
06/23/2022 23:50:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=729
06/23/2022 23:50:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=734
06/23/2022 23:51:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.49 on epoch=739
06/23/2022 23:51:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=744
06/23/2022 23:51:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.48 on epoch=749
06/23/2022 23:51:06 - INFO - __main__ - Global step 1500 Train loss 0.45 ACC 0.5 on epoch=749
06/23/2022 23:51:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
06/23/2022 23:51:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=759
06/23/2022 23:51:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.45 on epoch=764
06/23/2022 23:51:12 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=769
06/23/2022 23:51:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=774
06/23/2022 23:51:17 - INFO - __main__ - Global step 1550 Train loss 0.43 ACC 0.5 on epoch=774
06/23/2022 23:51:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=779
06/23/2022 23:51:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.46 on epoch=784
06/23/2022 23:51:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=789
06/23/2022 23:51:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=794
06/23/2022 23:51:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.39 on epoch=799
06/23/2022 23:51:26 - INFO - __main__ - Global step 1600 Train loss 0.42 ACC 0.5 on epoch=799
06/23/2022 23:51:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=804
06/23/2022 23:51:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=809
06/23/2022 23:51:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=814
06/23/2022 23:51:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=819
06/23/2022 23:51:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=824
06/23/2022 23:51:37 - INFO - __main__ - Global step 1650 Train loss 0.40 ACC 0.5 on epoch=824
06/23/2022 23:51:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=829
06/23/2022 23:51:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=834
06/23/2022 23:51:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.36 on epoch=839
06/23/2022 23:51:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=844
06/23/2022 23:51:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=849
06/23/2022 23:51:48 - INFO - __main__ - Global step 1700 Train loss 0.39 ACC 0.5 on epoch=849
06/23/2022 23:51:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=854
06/23/2022 23:51:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=859
06/23/2022 23:51:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.34 on epoch=864
06/23/2022 23:51:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.41 on epoch=869
06/23/2022 23:51:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=874
06/23/2022 23:51:59 - INFO - __main__ - Global step 1750 Train loss 0.36 ACC 0.5 on epoch=874
06/23/2022 23:52:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=879
06/23/2022 23:52:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=884
06/23/2022 23:52:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=889
06/23/2022 23:52:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=894
06/23/2022 23:52:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=899
06/23/2022 23:52:12 - INFO - __main__ - Global step 1800 Train loss 0.37 ACC 0.5 on epoch=899
06/23/2022 23:52:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=904
06/23/2022 23:52:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=909
06/23/2022 23:52:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.29 on epoch=914
06/23/2022 23:52:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=919
06/23/2022 23:52:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=924
06/23/2022 23:52:24 - INFO - __main__ - Global step 1850 Train loss 0.35 ACC 0.5 on epoch=924
06/23/2022 23:52:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=929
06/23/2022 23:52:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
06/23/2022 23:52:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=939
06/23/2022 23:52:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
06/23/2022 23:52:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
06/23/2022 23:52:33 - INFO - __main__ - Global step 1900 Train loss 0.35 ACC 0.5 on epoch=949
06/23/2022 23:52:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.36 on epoch=954
06/23/2022 23:52:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=959
06/23/2022 23:52:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=964
06/23/2022 23:52:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.38 on epoch=969
06/23/2022 23:52:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=974
06/23/2022 23:52:44 - INFO - __main__ - Global step 1950 Train loss 0.36 ACC 0.5 on epoch=974
06/23/2022 23:52:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
06/23/2022 23:52:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=984
06/23/2022 23:52:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=989
06/23/2022 23:52:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=994
06/23/2022 23:52:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=999
06/23/2022 23:52:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 23:52:52 - INFO - __main__ - Printing 3 examples
06/23/2022 23:52:52 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/23/2022 23:52:52 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:52:52 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/23/2022 23:52:52 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:52:52 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/23/2022 23:52:52 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:52:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/23/2022 23:52:52 - INFO - __main__ - Tokenizing Output ...
06/23/2022 23:52:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/23/2022 23:52:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/23/2022 23:52:52 - INFO - __main__ - Printing 3 examples
06/23/2022 23:52:52 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/23/2022 23:52:52 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:52:52 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/23/2022 23:52:52 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:52:52 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/23/2022 23:52:52 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:52:52 - INFO - __main__ - Tokenizing Input ...
06/23/2022 23:52:52 - INFO - __main__ - Tokenizing Output ...
06/23/2022 23:52:52 - INFO - __main__ - Loaded 32 examples from dev data
06/23/2022 23:52:55 - INFO - __main__ - Global step 2000 Train loss 0.34 ACC 0.5 on epoch=999
06/23/2022 23:52:55 - INFO - __main__ - save last model!
06/23/2022 23:52:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/23/2022 23:52:55 - INFO - __main__ - Start tokenizing ... 40430 instances
06/23/2022 23:52:55 - INFO - __main__ - Printing 3 examples
06/23/2022 23:52:55 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/23/2022 23:52:55 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:52:55 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/23/2022 23:52:55 - INFO - __main__ - ['not_duplicate']
06/23/2022 23:52:55 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/23/2022 23:52:55 - INFO - __main__ - ['duplicate']
06/23/2022 23:52:55 - INFO - __main__ - Tokenizing Input ...
06/23/2022 23:52:58 - INFO - __main__ - load prompt embedding from ckpt
06/23/2022 23:52:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/23/2022 23:52:59 - INFO - __main__ - Starting training!
06/23/2022 23:53:15 - INFO - __main__ - Tokenizing Output ...
06/23/2022 23:53:57 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 01:13:53 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_21_0.4_8_predictions.txt
06/24/2022 01:13:54 - INFO - __main__ - ACC on test data: 0.3682
06/24/2022 01:13:54 - INFO - __main__ - prefix=glue-qqp_16_21, lr=0.4, bsz=8, dev_performance=0.5, test_performance=0.36816720257234725
06/24/2022 01:13:54 - INFO - __main__ - Running ... prefix=glue-qqp_16_21, lr=0.3, bsz=8 ...
06/24/2022 01:13:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 01:13:55 - INFO - __main__ - Printing 3 examples
06/24/2022 01:13:55 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/24/2022 01:13:55 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:13:55 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/24/2022 01:13:55 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:13:55 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/24/2022 01:13:55 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:13:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 01:13:55 - INFO - __main__ - Tokenizing Output ...
06/24/2022 01:13:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 01:13:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 01:13:55 - INFO - __main__ - Printing 3 examples
06/24/2022 01:13:55 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/24/2022 01:13:55 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:13:55 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/24/2022 01:13:55 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:13:55 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/24/2022 01:13:55 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:13:55 - INFO - __main__ - Tokenizing Input ...
06/24/2022 01:13:55 - INFO - __main__ - Tokenizing Output ...
06/24/2022 01:13:55 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 01:14:00 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 01:14:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 01:14:00 - INFO - __main__ - Starting training!
06/24/2022 01:14:02 - INFO - __main__ - Step 10 Global step 10 Train loss 6.85 on epoch=4
06/24/2022 01:14:03 - INFO - __main__ - Step 20 Global step 20 Train loss 6.85 on epoch=9
06/24/2022 01:14:05 - INFO - __main__ - Step 30 Global step 30 Train loss 6.79 on epoch=14
06/24/2022 01:14:06 - INFO - __main__ - Step 40 Global step 40 Train loss 6.78 on epoch=19
06/24/2022 01:14:07 - INFO - __main__ - Step 50 Global step 50 Train loss 6.77 on epoch=24
06/24/2022 01:14:11 - INFO - __main__ - Global step 50 Train loss 6.81 ACC 0.0 on epoch=24
06/24/2022 01:14:11 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 01:14:12 - INFO - __main__ - Step 60 Global step 60 Train loss 6.80 on epoch=29
06/24/2022 01:14:14 - INFO - __main__ - Step 70 Global step 70 Train loss 6.82 on epoch=34
06/24/2022 01:14:15 - INFO - __main__ - Step 80 Global step 80 Train loss 6.77 on epoch=39
06/24/2022 01:14:17 - INFO - __main__ - Step 90 Global step 90 Train loss 6.74 on epoch=44
06/24/2022 01:14:18 - INFO - __main__ - Step 100 Global step 100 Train loss 6.70 on epoch=49
06/24/2022 01:14:21 - INFO - __main__ - Global step 100 Train loss 6.77 ACC 0.0 on epoch=49
06/24/2022 01:14:23 - INFO - __main__ - Step 110 Global step 110 Train loss 6.67 on epoch=54
06/24/2022 01:14:24 - INFO - __main__ - Step 120 Global step 120 Train loss 6.71 on epoch=59
06/24/2022 01:14:25 - INFO - __main__ - Step 130 Global step 130 Train loss 6.72 on epoch=64
06/24/2022 01:14:27 - INFO - __main__ - Step 140 Global step 140 Train loss 6.66 on epoch=69
06/24/2022 01:14:28 - INFO - __main__ - Step 150 Global step 150 Train loss 6.68 on epoch=74
06/24/2022 01:14:32 - INFO - __main__ - Global step 150 Train loss 6.69 ACC 0.0 on epoch=74
06/24/2022 01:14:33 - INFO - __main__ - Step 160 Global step 160 Train loss 6.65 on epoch=79
06/24/2022 01:14:35 - INFO - __main__ - Step 170 Global step 170 Train loss 6.59 on epoch=84
06/24/2022 01:14:36 - INFO - __main__ - Step 180 Global step 180 Train loss 6.61 on epoch=89
06/24/2022 01:14:37 - INFO - __main__ - Step 190 Global step 190 Train loss 6.51 on epoch=94
06/24/2022 01:14:39 - INFO - __main__ - Step 200 Global step 200 Train loss 6.50 on epoch=99
06/24/2022 01:14:41 - INFO - __main__ - Global step 200 Train loss 6.57 ACC 0.0 on epoch=99
06/24/2022 01:14:43 - INFO - __main__ - Step 210 Global step 210 Train loss 6.46 on epoch=104
06/24/2022 01:14:44 - INFO - __main__ - Step 220 Global step 220 Train loss 6.42 on epoch=109
06/24/2022 01:14:45 - INFO - __main__ - Step 230 Global step 230 Train loss 6.37 on epoch=114
06/24/2022 01:14:47 - INFO - __main__ - Step 240 Global step 240 Train loss 6.27 on epoch=119
06/24/2022 01:14:48 - INFO - __main__ - Step 250 Global step 250 Train loss 6.28 on epoch=124
06/24/2022 01:14:53 - INFO - __main__ - Global step 250 Train loss 6.36 ACC 0.0 on epoch=124
06/24/2022 01:14:55 - INFO - __main__ - Step 260 Global step 260 Train loss 6.24 on epoch=129
06/24/2022 01:14:57 - INFO - __main__ - Step 270 Global step 270 Train loss 6.17 on epoch=134
06/24/2022 01:14:58 - INFO - __main__ - Step 280 Global step 280 Train loss 6.09 on epoch=139
06/24/2022 01:15:00 - INFO - __main__ - Step 290 Global step 290 Train loss 6.02 on epoch=144
06/24/2022 01:15:02 - INFO - __main__ - Step 300 Global step 300 Train loss 5.94 on epoch=149
06/24/2022 01:15:06 - INFO - __main__ - Global step 300 Train loss 6.09 ACC 0.0 on epoch=149
06/24/2022 01:15:08 - INFO - __main__ - Step 310 Global step 310 Train loss 5.87 on epoch=154
06/24/2022 01:15:09 - INFO - __main__ - Step 320 Global step 320 Train loss 5.83 on epoch=159
06/24/2022 01:15:11 - INFO - __main__ - Step 330 Global step 330 Train loss 5.77 on epoch=164
06/24/2022 01:15:12 - INFO - __main__ - Step 340 Global step 340 Train loss 5.76 on epoch=169
06/24/2022 01:15:13 - INFO - __main__ - Step 350 Global step 350 Train loss 5.69 on epoch=174
06/24/2022 01:15:21 - INFO - __main__ - Global step 350 Train loss 5.78 ACC 0.0 on epoch=174
06/24/2022 01:15:22 - INFO - __main__ - Step 360 Global step 360 Train loss 5.69 on epoch=179
06/24/2022 01:15:24 - INFO - __main__ - Step 370 Global step 370 Train loss 5.58 on epoch=184
06/24/2022 01:15:25 - INFO - __main__ - Step 380 Global step 380 Train loss 5.68 on epoch=189
06/24/2022 01:15:27 - INFO - __main__ - Step 390 Global step 390 Train loss 5.53 on epoch=194
06/24/2022 01:15:28 - INFO - __main__ - Step 400 Global step 400 Train loss 5.50 on epoch=199
06/24/2022 01:15:38 - INFO - __main__ - Global step 400 Train loss 5.60 ACC 0.0 on epoch=199
06/24/2022 01:15:39 - INFO - __main__ - Step 410 Global step 410 Train loss 5.53 on epoch=204
06/24/2022 01:15:41 - INFO - __main__ - Step 420 Global step 420 Train loss 5.45 on epoch=209
06/24/2022 01:15:42 - INFO - __main__ - Step 430 Global step 430 Train loss 5.36 on epoch=214
06/24/2022 01:15:43 - INFO - __main__ - Step 440 Global step 440 Train loss 5.34 on epoch=219
06/24/2022 01:15:45 - INFO - __main__ - Step 450 Global step 450 Train loss 5.33 on epoch=224
06/24/2022 01:15:49 - INFO - __main__ - Global step 450 Train loss 5.40 ACC 0.0 on epoch=224
06/24/2022 01:15:50 - INFO - __main__ - Step 460 Global step 460 Train loss 5.32 on epoch=229
06/24/2022 01:15:52 - INFO - __main__ - Step 470 Global step 470 Train loss 5.29 on epoch=234
06/24/2022 01:15:53 - INFO - __main__ - Step 480 Global step 480 Train loss 5.17 on epoch=239
06/24/2022 01:15:55 - INFO - __main__ - Step 490 Global step 490 Train loss 5.28 on epoch=244
06/24/2022 01:15:56 - INFO - __main__ - Step 500 Global step 500 Train loss 5.13 on epoch=249
06/24/2022 01:16:00 - INFO - __main__ - Global step 500 Train loss 5.24 ACC 0.0 on epoch=249
06/24/2022 01:16:01 - INFO - __main__ - Step 510 Global step 510 Train loss 5.09 on epoch=254
06/24/2022 01:16:03 - INFO - __main__ - Step 520 Global step 520 Train loss 5.10 on epoch=259
06/24/2022 01:16:04 - INFO - __main__ - Step 530 Global step 530 Train loss 4.98 on epoch=264
06/24/2022 01:16:06 - INFO - __main__ - Step 540 Global step 540 Train loss 4.95 on epoch=269
06/24/2022 01:16:07 - INFO - __main__ - Step 550 Global step 550 Train loss 4.89 on epoch=274
06/24/2022 01:16:12 - INFO - __main__ - Global step 550 Train loss 5.00 ACC 0.0 on epoch=274
06/24/2022 01:16:13 - INFO - __main__ - Step 560 Global step 560 Train loss 4.84 on epoch=279
06/24/2022 01:16:14 - INFO - __main__ - Step 570 Global step 570 Train loss 4.83 on epoch=284
06/24/2022 01:16:16 - INFO - __main__ - Step 580 Global step 580 Train loss 4.76 on epoch=289
06/24/2022 01:16:17 - INFO - __main__ - Step 590 Global step 590 Train loss 4.73 on epoch=294
06/24/2022 01:16:19 - INFO - __main__ - Step 600 Global step 600 Train loss 4.79 on epoch=299
06/24/2022 01:16:25 - INFO - __main__ - Global step 600 Train loss 4.79 ACC 0.0 on epoch=299
06/24/2022 01:16:26 - INFO - __main__ - Step 610 Global step 610 Train loss 4.79 on epoch=304
06/24/2022 01:16:28 - INFO - __main__ - Step 620 Global step 620 Train loss 4.54 on epoch=309
06/24/2022 01:16:29 - INFO - __main__ - Step 630 Global step 630 Train loss 4.56 on epoch=314
06/24/2022 01:16:30 - INFO - __main__ - Step 640 Global step 640 Train loss 4.56 on epoch=319
06/24/2022 01:16:32 - INFO - __main__ - Step 650 Global step 650 Train loss 4.46 on epoch=324
06/24/2022 01:16:34 - INFO - __main__ - Global step 650 Train loss 4.58 ACC 0.0 on epoch=324
06/24/2022 01:16:35 - INFO - __main__ - Step 660 Global step 660 Train loss 4.47 on epoch=329
06/24/2022 01:16:37 - INFO - __main__ - Step 670 Global step 670 Train loss 4.38 on epoch=334
06/24/2022 01:16:38 - INFO - __main__ - Step 680 Global step 680 Train loss 4.37 on epoch=339
06/24/2022 01:16:40 - INFO - __main__ - Step 690 Global step 690 Train loss 4.33 on epoch=344
06/24/2022 01:16:41 - INFO - __main__ - Step 700 Global step 700 Train loss 4.22 on epoch=349
06/24/2022 01:16:43 - INFO - __main__ - Global step 700 Train loss 4.35 ACC 0.0 on epoch=349
06/24/2022 01:16:44 - INFO - __main__ - Step 710 Global step 710 Train loss 4.22 on epoch=354
06/24/2022 01:16:46 - INFO - __main__ - Step 720 Global step 720 Train loss 4.19 on epoch=359
06/24/2022 01:16:47 - INFO - __main__ - Step 730 Global step 730 Train loss 4.10 on epoch=364
06/24/2022 01:16:49 - INFO - __main__ - Step 740 Global step 740 Train loss 4.00 on epoch=369
06/24/2022 01:16:50 - INFO - __main__ - Step 750 Global step 750 Train loss 3.99 on epoch=374
06/24/2022 01:16:55 - INFO - __main__ - Global step 750 Train loss 4.10 ACC 0.0 on epoch=374
06/24/2022 01:16:57 - INFO - __main__ - Step 760 Global step 760 Train loss 4.03 on epoch=379
06/24/2022 01:16:58 - INFO - __main__ - Step 770 Global step 770 Train loss 3.91 on epoch=384
06/24/2022 01:17:00 - INFO - __main__ - Step 780 Global step 780 Train loss 3.95 on epoch=389
06/24/2022 01:17:01 - INFO - __main__ - Step 790 Global step 790 Train loss 3.84 on epoch=394
06/24/2022 01:17:02 - INFO - __main__ - Step 800 Global step 800 Train loss 3.83 on epoch=399
06/24/2022 01:17:05 - INFO - __main__ - Global step 800 Train loss 3.91 ACC 0.0 on epoch=399
06/24/2022 01:17:06 - INFO - __main__ - Step 810 Global step 810 Train loss 3.80 on epoch=404
06/24/2022 01:17:07 - INFO - __main__ - Step 820 Global step 820 Train loss 3.70 on epoch=409
06/24/2022 01:17:09 - INFO - __main__ - Step 830 Global step 830 Train loss 3.68 on epoch=414
06/24/2022 01:17:10 - INFO - __main__ - Step 840 Global step 840 Train loss 3.75 on epoch=419
06/24/2022 01:17:12 - INFO - __main__ - Step 850 Global step 850 Train loss 3.67 on epoch=424
06/24/2022 01:17:14 - INFO - __main__ - Global step 850 Train loss 3.72 ACC 0.0 on epoch=424
06/24/2022 01:17:15 - INFO - __main__ - Step 860 Global step 860 Train loss 3.56 on epoch=429
06/24/2022 01:17:17 - INFO - __main__ - Step 870 Global step 870 Train loss 3.48 on epoch=434
06/24/2022 01:17:18 - INFO - __main__ - Step 880 Global step 880 Train loss 3.44 on epoch=439
06/24/2022 01:17:20 - INFO - __main__ - Step 890 Global step 890 Train loss 3.49 on epoch=444
06/24/2022 01:17:21 - INFO - __main__ - Step 900 Global step 900 Train loss 3.46 on epoch=449
06/24/2022 01:17:23 - INFO - __main__ - Global step 900 Train loss 3.48 ACC 0.03125 on epoch=449
06/24/2022 01:17:24 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=449, global_step=900
06/24/2022 01:17:25 - INFO - __main__ - Step 910 Global step 910 Train loss 3.33 on epoch=454
06/24/2022 01:17:26 - INFO - __main__ - Step 920 Global step 920 Train loss 3.38 on epoch=459
06/24/2022 01:17:28 - INFO - __main__ - Step 930 Global step 930 Train loss 3.33 on epoch=464
06/24/2022 01:17:29 - INFO - __main__ - Step 940 Global step 940 Train loss 3.20 on epoch=469
06/24/2022 01:17:30 - INFO - __main__ - Step 950 Global step 950 Train loss 3.17 on epoch=474
06/24/2022 01:17:32 - INFO - __main__ - Global step 950 Train loss 3.28 ACC 0.21875 on epoch=474
06/24/2022 01:17:32 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.21875 on epoch=474, global_step=950
06/24/2022 01:17:33 - INFO - __main__ - Step 960 Global step 960 Train loss 3.17 on epoch=479
06/24/2022 01:17:35 - INFO - __main__ - Step 970 Global step 970 Train loss 3.07 on epoch=484
06/24/2022 01:17:36 - INFO - __main__ - Step 980 Global step 980 Train loss 2.93 on epoch=489
06/24/2022 01:17:38 - INFO - __main__ - Step 990 Global step 990 Train loss 2.88 on epoch=494
06/24/2022 01:17:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.89 on epoch=499
06/24/2022 01:17:44 - INFO - __main__ - Global step 1000 Train loss 2.99 ACC 0.4375 on epoch=499
06/24/2022 01:17:44 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.4375 on epoch=499, global_step=1000
06/24/2022 01:17:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.76 on epoch=504
06/24/2022 01:17:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.76 on epoch=509
06/24/2022 01:17:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.59 on epoch=514
06/24/2022 01:17:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.56 on epoch=519
06/24/2022 01:17:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.55 on epoch=524
06/24/2022 01:17:55 - INFO - __main__ - Global step 1050 Train loss 2.64 ACC 0.4375 on epoch=524
06/24/2022 01:17:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.36 on epoch=529
06/24/2022 01:17:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.32 on epoch=534
06/24/2022 01:17:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.36 on epoch=539
06/24/2022 01:18:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.26 on epoch=544
06/24/2022 01:18:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.18 on epoch=549
06/24/2022 01:18:08 - INFO - __main__ - Global step 1100 Train loss 2.30 ACC 0.4375 on epoch=549
06/24/2022 01:18:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.21 on epoch=554
06/24/2022 01:18:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 2.03 on epoch=559
06/24/2022 01:18:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.07 on epoch=564
06/24/2022 01:18:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.98 on epoch=569
06/24/2022 01:18:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 2.04 on epoch=574
06/24/2022 01:18:21 - INFO - __main__ - Global step 1150 Train loss 2.07 ACC 0.5 on epoch=574
06/24/2022 01:18:21 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=574, global_step=1150
06/24/2022 01:18:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.79 on epoch=579
06/24/2022 01:18:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.91 on epoch=584
06/24/2022 01:18:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.82 on epoch=589
06/24/2022 01:18:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.82 on epoch=594
06/24/2022 01:18:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.79 on epoch=599
06/24/2022 01:18:31 - INFO - __main__ - Global step 1200 Train loss 1.83 ACC 0.5 on epoch=599
06/24/2022 01:18:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.58 on epoch=604
06/24/2022 01:18:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.62 on epoch=609
06/24/2022 01:18:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.50 on epoch=614
06/24/2022 01:18:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.38 on epoch=619
06/24/2022 01:18:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.45 on epoch=624
06/24/2022 01:18:40 - INFO - __main__ - Global step 1250 Train loss 1.51 ACC 0.5 on epoch=624
06/24/2022 01:18:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.46 on epoch=629
06/24/2022 01:18:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.44 on epoch=634
06/24/2022 01:18:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.36 on epoch=639
06/24/2022 01:18:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.28 on epoch=644
06/24/2022 01:18:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.20 on epoch=649
06/24/2022 01:18:49 - INFO - __main__ - Global step 1300 Train loss 1.35 ACC 0.5 on epoch=649
06/24/2022 01:18:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.11 on epoch=654
06/24/2022 01:18:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.12 on epoch=659
06/24/2022 01:18:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.07 on epoch=664
06/24/2022 01:18:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.13 on epoch=669
06/24/2022 01:18:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.02 on epoch=674
06/24/2022 01:18:58 - INFO - __main__ - Global step 1350 Train loss 1.09 ACC 0.5 on epoch=674
06/24/2022 01:19:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.12 on epoch=679
06/24/2022 01:19:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.98 on epoch=684
06/24/2022 01:19:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.96 on epoch=689
06/24/2022 01:19:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.90 on epoch=694
06/24/2022 01:19:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.02 on epoch=699
06/24/2022 01:19:06 - INFO - __main__ - Global step 1400 Train loss 1.00 ACC 0.5 on epoch=699
06/24/2022 01:19:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.97 on epoch=704
06/24/2022 01:19:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.93 on epoch=709
06/24/2022 01:19:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.95 on epoch=714
06/24/2022 01:19:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.91 on epoch=719
06/24/2022 01:19:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.91 on epoch=724
06/24/2022 01:19:14 - INFO - __main__ - Global step 1450 Train loss 0.93 ACC 0.5 on epoch=724
06/24/2022 01:19:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.88 on epoch=729
06/24/2022 01:19:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.80 on epoch=734
06/24/2022 01:19:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.78 on epoch=739
06/24/2022 01:19:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.80 on epoch=744
06/24/2022 01:19:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.76 on epoch=749
06/24/2022 01:19:23 - INFO - __main__ - Global step 1500 Train loss 0.80 ACC 0.5 on epoch=749
06/24/2022 01:19:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.83 on epoch=754
06/24/2022 01:19:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.83 on epoch=759
06/24/2022 01:19:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.74 on epoch=764
06/24/2022 01:19:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.72 on epoch=769
06/24/2022 01:19:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.71 on epoch=774
06/24/2022 01:19:30 - INFO - __main__ - Global step 1550 Train loss 0.77 ACC 0.5 on epoch=774
06/24/2022 01:19:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.73 on epoch=779
06/24/2022 01:19:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.74 on epoch=784
06/24/2022 01:19:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.76 on epoch=789
06/24/2022 01:19:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.64 on epoch=794
06/24/2022 01:19:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.66 on epoch=799
06/24/2022 01:19:38 - INFO - __main__ - Global step 1600 Train loss 0.71 ACC 0.5 on epoch=799
06/24/2022 01:19:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.69 on epoch=804
06/24/2022 01:19:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.71 on epoch=809
06/24/2022 01:19:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.56 on epoch=814
06/24/2022 01:19:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.61 on epoch=819
06/24/2022 01:19:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.59 on epoch=824
06/24/2022 01:19:46 - INFO - __main__ - Global step 1650 Train loss 0.63 ACC 0.5 on epoch=824
06/24/2022 01:19:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.70 on epoch=829
06/24/2022 01:19:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.59 on epoch=834
06/24/2022 01:19:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.65 on epoch=839
06/24/2022 01:19:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.61 on epoch=844
06/24/2022 01:19:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.61 on epoch=849
06/24/2022 01:19:54 - INFO - __main__ - Global step 1700 Train loss 0.63 ACC 0.5 on epoch=849
06/24/2022 01:19:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.58 on epoch=854
06/24/2022 01:19:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.68 on epoch=859
06/24/2022 01:19:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.55 on epoch=864
06/24/2022 01:20:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.60 on epoch=869
06/24/2022 01:20:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.54 on epoch=874
06/24/2022 01:20:02 - INFO - __main__ - Global step 1750 Train loss 0.59 ACC 0.5 on epoch=874
06/24/2022 01:20:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.51 on epoch=879
06/24/2022 01:20:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.54 on epoch=884
06/24/2022 01:20:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.52 on epoch=889
06/24/2022 01:20:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.55 on epoch=894
06/24/2022 01:20:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.56 on epoch=899
06/24/2022 01:20:09 - INFO - __main__ - Global step 1800 Train loss 0.54 ACC 0.5 on epoch=899
06/24/2022 01:20:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.52 on epoch=904
06/24/2022 01:20:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.49 on epoch=909
06/24/2022 01:20:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.55 on epoch=914
06/24/2022 01:20:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.52 on epoch=919
06/24/2022 01:20:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.54 on epoch=924
06/24/2022 01:20:17 - INFO - __main__ - Global step 1850 Train loss 0.52 ACC 0.5 on epoch=924
06/24/2022 01:20:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.50 on epoch=929
06/24/2022 01:20:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.47 on epoch=934
06/24/2022 01:20:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.55 on epoch=939
06/24/2022 01:20:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.47 on epoch=944
06/24/2022 01:20:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=949
06/24/2022 01:20:26 - INFO - __main__ - Global step 1900 Train loss 0.49 ACC 0.5 on epoch=949
06/24/2022 01:20:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.47 on epoch=954
06/24/2022 01:20:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/24/2022 01:20:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.51 on epoch=964
06/24/2022 01:20:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.49 on epoch=969
06/24/2022 01:20:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=974
06/24/2022 01:20:33 - INFO - __main__ - Global step 1950 Train loss 0.48 ACC 0.5 on epoch=974
06/24/2022 01:20:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.52 on epoch=979
06/24/2022 01:20:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.49 on epoch=984
06/24/2022 01:20:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.52 on epoch=989
06/24/2022 01:20:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.59 on epoch=994
06/24/2022 01:20:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=999
06/24/2022 01:20:41 - INFO - __main__ - Global step 2000 Train loss 0.52 ACC 0.5 on epoch=999
06/24/2022 01:20:41 - INFO - __main__ - save last model!
06/24/2022 01:20:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 01:20:41 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 01:20:41 - INFO - __main__ - Printing 3 examples
06/24/2022 01:20:41 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 01:20:41 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:20:41 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 01:20:41 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:20:41 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 01:20:41 - INFO - __main__ - ['duplicate']
06/24/2022 01:20:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 01:20:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 01:20:41 - INFO - __main__ - Printing 3 examples
06/24/2022 01:20:41 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/24/2022 01:20:41 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:20:41 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/24/2022 01:20:41 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:20:41 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/24/2022 01:20:41 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:20:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 01:20:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 01:20:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 01:20:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 01:20:41 - INFO - __main__ - Printing 3 examples
06/24/2022 01:20:41 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/24/2022 01:20:41 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:20:41 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/24/2022 01:20:41 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:20:41 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/24/2022 01:20:41 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:20:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 01:20:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 01:20:41 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 01:20:48 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 01:20:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 01:20:48 - INFO - __main__ - Starting training!
06/24/2022 01:21:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 01:21:56 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 01:34:35 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_21_0.3_8_predictions.txt
06/24/2022 01:34:35 - INFO - __main__ - ACC on test data: 0.3682
06/24/2022 01:34:35 - INFO - __main__ - prefix=glue-qqp_16_21, lr=0.3, bsz=8, dev_performance=0.5, test_performance=0.3681919366806827
06/24/2022 01:34:35 - INFO - __main__ - Running ... prefix=glue-qqp_16_21, lr=0.2, bsz=8 ...
06/24/2022 01:34:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 01:34:36 - INFO - __main__ - Printing 3 examples
06/24/2022 01:34:36 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/24/2022 01:34:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:34:36 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/24/2022 01:34:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:34:36 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/24/2022 01:34:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:34:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 01:34:36 - INFO - __main__ - Tokenizing Output ...
06/24/2022 01:34:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 01:34:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 01:34:36 - INFO - __main__ - Printing 3 examples
06/24/2022 01:34:36 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/24/2022 01:34:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:34:36 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/24/2022 01:34:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:34:36 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/24/2022 01:34:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:34:36 - INFO - __main__ - Tokenizing Input ...
06/24/2022 01:34:36 - INFO - __main__ - Tokenizing Output ...
06/24/2022 01:34:36 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 01:34:42 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 01:34:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 01:34:42 - INFO - __main__ - Starting training!
06/24/2022 01:34:44 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=4
06/24/2022 01:34:45 - INFO - __main__ - Step 20 Global step 20 Train loss 6.86 on epoch=9
06/24/2022 01:34:47 - INFO - __main__ - Step 30 Global step 30 Train loss 6.82 on epoch=14
06/24/2022 01:34:48 - INFO - __main__ - Step 40 Global step 40 Train loss 6.79 on epoch=19
06/24/2022 01:34:49 - INFO - __main__ - Step 50 Global step 50 Train loss 6.85 on epoch=24
06/24/2022 01:34:52 - INFO - __main__ - Global step 50 Train loss 6.84 ACC 0.0 on epoch=24
06/24/2022 01:34:52 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 01:34:53 - INFO - __main__ - Step 60 Global step 60 Train loss 6.85 on epoch=29
06/24/2022 01:34:55 - INFO - __main__ - Step 70 Global step 70 Train loss 6.80 on epoch=34
06/24/2022 01:34:56 - INFO - __main__ - Step 80 Global step 80 Train loss 6.76 on epoch=39
06/24/2022 01:34:58 - INFO - __main__ - Step 90 Global step 90 Train loss 6.81 on epoch=44
06/24/2022 01:34:59 - INFO - __main__ - Step 100 Global step 100 Train loss 6.84 on epoch=49
06/24/2022 01:35:02 - INFO - __main__ - Global step 100 Train loss 6.81 ACC 0.0 on epoch=49
06/24/2022 01:35:04 - INFO - __main__ - Step 110 Global step 110 Train loss 6.80 on epoch=54
06/24/2022 01:35:05 - INFO - __main__ - Step 120 Global step 120 Train loss 6.79 on epoch=59
06/24/2022 01:35:07 - INFO - __main__ - Step 130 Global step 130 Train loss 6.76 on epoch=64
06/24/2022 01:35:08 - INFO - __main__ - Step 140 Global step 140 Train loss 6.78 on epoch=69
06/24/2022 01:35:09 - INFO - __main__ - Step 150 Global step 150 Train loss 6.68 on epoch=74
06/24/2022 01:35:21 - INFO - __main__ - Global step 150 Train loss 6.76 ACC 0.0 on epoch=74
06/24/2022 01:35:22 - INFO - __main__ - Step 160 Global step 160 Train loss 6.71 on epoch=79
06/24/2022 01:35:24 - INFO - __main__ - Step 170 Global step 170 Train loss 6.74 on epoch=84
06/24/2022 01:35:25 - INFO - __main__ - Step 180 Global step 180 Train loss 6.66 on epoch=89
06/24/2022 01:35:27 - INFO - __main__ - Step 190 Global step 190 Train loss 6.67 on epoch=94
06/24/2022 01:35:28 - INFO - __main__ - Step 200 Global step 200 Train loss 6.59 on epoch=99
06/24/2022 01:35:32 - INFO - __main__ - Global step 200 Train loss 6.67 ACC 0.0 on epoch=99
06/24/2022 01:35:34 - INFO - __main__ - Step 210 Global step 210 Train loss 6.57 on epoch=104
06/24/2022 01:35:35 - INFO - __main__ - Step 220 Global step 220 Train loss 6.57 on epoch=109
06/24/2022 01:35:37 - INFO - __main__ - Step 230 Global step 230 Train loss 6.53 on epoch=114
06/24/2022 01:35:38 - INFO - __main__ - Step 240 Global step 240 Train loss 6.46 on epoch=119
06/24/2022 01:35:39 - INFO - __main__ - Step 250 Global step 250 Train loss 6.39 on epoch=124
06/24/2022 01:35:41 - INFO - __main__ - Global step 250 Train loss 6.50 ACC 0.0 on epoch=124
06/24/2022 01:35:42 - INFO - __main__ - Step 260 Global step 260 Train loss 6.30 on epoch=129
06/24/2022 01:35:44 - INFO - __main__ - Step 270 Global step 270 Train loss 6.31 on epoch=134
06/24/2022 01:35:45 - INFO - __main__ - Step 280 Global step 280 Train loss 6.31 on epoch=139
06/24/2022 01:35:47 - INFO - __main__ - Step 290 Global step 290 Train loss 6.19 on epoch=144
06/24/2022 01:35:48 - INFO - __main__ - Step 300 Global step 300 Train loss 6.15 on epoch=149
06/24/2022 01:35:50 - INFO - __main__ - Global step 300 Train loss 6.25 ACC 0.0 on epoch=149
06/24/2022 01:35:52 - INFO - __main__ - Step 310 Global step 310 Train loss 6.11 on epoch=154
06/24/2022 01:35:53 - INFO - __main__ - Step 320 Global step 320 Train loss 6.03 on epoch=159
06/24/2022 01:35:55 - INFO - __main__ - Step 330 Global step 330 Train loss 6.01 on epoch=164
06/24/2022 01:35:56 - INFO - __main__ - Step 340 Global step 340 Train loss 6.03 on epoch=169
06/24/2022 01:35:57 - INFO - __main__ - Step 350 Global step 350 Train loss 5.93 on epoch=174
06/24/2022 01:36:05 - INFO - __main__ - Global step 350 Train loss 6.02 ACC 0.0 on epoch=174
06/24/2022 01:36:06 - INFO - __main__ - Step 360 Global step 360 Train loss 5.80 on epoch=179
06/24/2022 01:36:08 - INFO - __main__ - Step 370 Global step 370 Train loss 5.86 on epoch=184
06/24/2022 01:36:09 - INFO - __main__ - Step 380 Global step 380 Train loss 5.76 on epoch=189
06/24/2022 01:36:10 - INFO - __main__ - Step 390 Global step 390 Train loss 5.72 on epoch=194
06/24/2022 01:36:12 - INFO - __main__ - Step 400 Global step 400 Train loss 5.76 on epoch=199
06/24/2022 01:36:22 - INFO - __main__ - Global step 400 Train loss 5.78 ACC 0.0 on epoch=199
06/24/2022 01:36:23 - INFO - __main__ - Step 410 Global step 410 Train loss 5.70 on epoch=204
06/24/2022 01:36:25 - INFO - __main__ - Step 420 Global step 420 Train loss 5.61 on epoch=209
06/24/2022 01:36:26 - INFO - __main__ - Step 430 Global step 430 Train loss 5.52 on epoch=214
06/24/2022 01:36:27 - INFO - __main__ - Step 440 Global step 440 Train loss 5.48 on epoch=219
06/24/2022 01:36:29 - INFO - __main__ - Step 450 Global step 450 Train loss 5.56 on epoch=224
06/24/2022 01:36:35 - INFO - __main__ - Global step 450 Train loss 5.58 ACC 0.0 on epoch=224
06/24/2022 01:36:36 - INFO - __main__ - Step 460 Global step 460 Train loss 5.45 on epoch=229
06/24/2022 01:36:38 - INFO - __main__ - Step 470 Global step 470 Train loss 5.42 on epoch=234
06/24/2022 01:36:39 - INFO - __main__ - Step 480 Global step 480 Train loss 5.39 on epoch=239
06/24/2022 01:36:40 - INFO - __main__ - Step 490 Global step 490 Train loss 5.22 on epoch=244
06/24/2022 01:36:42 - INFO - __main__ - Step 500 Global step 500 Train loss 5.19 on epoch=249
06/24/2022 01:36:49 - INFO - __main__ - Global step 500 Train loss 5.33 ACC 0.0 on epoch=249
06/24/2022 01:36:51 - INFO - __main__ - Step 510 Global step 510 Train loss 5.16 on epoch=254
06/24/2022 01:36:52 - INFO - __main__ - Step 520 Global step 520 Train loss 5.16 on epoch=259
06/24/2022 01:36:53 - INFO - __main__ - Step 530 Global step 530 Train loss 5.08 on epoch=264
06/24/2022 01:36:55 - INFO - __main__ - Step 540 Global step 540 Train loss 5.13 on epoch=269
06/24/2022 01:36:56 - INFO - __main__ - Step 550 Global step 550 Train loss 5.10 on epoch=274
06/24/2022 01:37:05 - INFO - __main__ - Global step 550 Train loss 5.13 ACC 0.0 on epoch=274
06/24/2022 01:37:06 - INFO - __main__ - Step 560 Global step 560 Train loss 4.98 on epoch=279
06/24/2022 01:37:08 - INFO - __main__ - Step 570 Global step 570 Train loss 4.99 on epoch=284
06/24/2022 01:37:09 - INFO - __main__ - Step 580 Global step 580 Train loss 4.94 on epoch=289
06/24/2022 01:37:11 - INFO - __main__ - Step 590 Global step 590 Train loss 4.91 on epoch=294
06/24/2022 01:37:12 - INFO - __main__ - Step 600 Global step 600 Train loss 4.81 on epoch=299
06/24/2022 01:37:22 - INFO - __main__ - Global step 600 Train loss 4.93 ACC 0.0 on epoch=299
06/24/2022 01:37:23 - INFO - __main__ - Step 610 Global step 610 Train loss 4.90 on epoch=304
06/24/2022 01:37:24 - INFO - __main__ - Step 620 Global step 620 Train loss 4.76 on epoch=309
06/24/2022 01:37:26 - INFO - __main__ - Step 630 Global step 630 Train loss 4.84 on epoch=314
06/24/2022 01:37:27 - INFO - __main__ - Step 640 Global step 640 Train loss 4.62 on epoch=319
06/24/2022 01:37:29 - INFO - __main__ - Step 650 Global step 650 Train loss 4.63 on epoch=324
06/24/2022 01:37:38 - INFO - __main__ - Global step 650 Train loss 4.75 ACC 0.0 on epoch=324
06/24/2022 01:37:39 - INFO - __main__ - Step 660 Global step 660 Train loss 4.56 on epoch=329
06/24/2022 01:37:40 - INFO - __main__ - Step 670 Global step 670 Train loss 4.61 on epoch=334
06/24/2022 01:37:42 - INFO - __main__ - Step 680 Global step 680 Train loss 4.59 on epoch=339
06/24/2022 01:37:43 - INFO - __main__ - Step 690 Global step 690 Train loss 4.52 on epoch=344
06/24/2022 01:37:44 - INFO - __main__ - Step 700 Global step 700 Train loss 4.43 on epoch=349
06/24/2022 01:37:46 - INFO - __main__ - Global step 700 Train loss 4.54 ACC 0.0 on epoch=349
06/24/2022 01:37:47 - INFO - __main__ - Step 710 Global step 710 Train loss 4.35 on epoch=354
06/24/2022 01:37:48 - INFO - __main__ - Step 720 Global step 720 Train loss 4.51 on epoch=359
06/24/2022 01:37:50 - INFO - __main__ - Step 730 Global step 730 Train loss 4.37 on epoch=364
06/24/2022 01:37:51 - INFO - __main__ - Step 740 Global step 740 Train loss 4.30 on epoch=369
06/24/2022 01:37:52 - INFO - __main__ - Step 750 Global step 750 Train loss 4.34 on epoch=374
06/24/2022 01:37:54 - INFO - __main__ - Global step 750 Train loss 4.37 ACC 0.0 on epoch=374
06/24/2022 01:37:55 - INFO - __main__ - Step 760 Global step 760 Train loss 4.33 on epoch=379
06/24/2022 01:37:57 - INFO - __main__ - Step 770 Global step 770 Train loss 4.18 on epoch=384
06/24/2022 01:37:58 - INFO - __main__ - Step 780 Global step 780 Train loss 4.15 on epoch=389
06/24/2022 01:38:00 - INFO - __main__ - Step 790 Global step 790 Train loss 4.18 on epoch=394
06/24/2022 01:38:01 - INFO - __main__ - Step 800 Global step 800 Train loss 4.21 on epoch=399
06/24/2022 01:38:03 - INFO - __main__ - Global step 800 Train loss 4.21 ACC 0.0 on epoch=399
06/24/2022 01:38:05 - INFO - __main__ - Step 810 Global step 810 Train loss 4.10 on epoch=404
06/24/2022 01:38:06 - INFO - __main__ - Step 820 Global step 820 Train loss 4.10 on epoch=409
06/24/2022 01:38:08 - INFO - __main__ - Step 830 Global step 830 Train loss 4.04 on epoch=414
06/24/2022 01:38:09 - INFO - __main__ - Step 840 Global step 840 Train loss 4.04 on epoch=419
06/24/2022 01:38:10 - INFO - __main__ - Step 850 Global step 850 Train loss 3.96 on epoch=424
06/24/2022 01:38:12 - INFO - __main__ - Global step 850 Train loss 4.05 ACC 0.0625 on epoch=424
06/24/2022 01:38:12 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=424, global_step=850
06/24/2022 01:38:14 - INFO - __main__ - Step 860 Global step 860 Train loss 3.96 on epoch=429
06/24/2022 01:38:15 - INFO - __main__ - Step 870 Global step 870 Train loss 3.89 on epoch=434
06/24/2022 01:38:16 - INFO - __main__ - Step 880 Global step 880 Train loss 3.90 on epoch=439
06/24/2022 01:38:18 - INFO - __main__ - Step 890 Global step 890 Train loss 3.71 on epoch=444
06/24/2022 01:38:19 - INFO - __main__ - Step 900 Global step 900 Train loss 3.80 on epoch=449
06/24/2022 01:38:21 - INFO - __main__ - Global step 900 Train loss 3.85 ACC 0.125 on epoch=449
06/24/2022 01:38:21 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.125 on epoch=449, global_step=900
06/24/2022 01:38:23 - INFO - __main__ - Step 910 Global step 910 Train loss 3.81 on epoch=454
06/24/2022 01:38:24 - INFO - __main__ - Step 920 Global step 920 Train loss 3.76 on epoch=459
06/24/2022 01:38:25 - INFO - __main__ - Step 930 Global step 930 Train loss 3.72 on epoch=464
06/24/2022 01:38:27 - INFO - __main__ - Step 940 Global step 940 Train loss 3.73 on epoch=469
06/24/2022 01:38:28 - INFO - __main__ - Step 950 Global step 950 Train loss 3.60 on epoch=474
06/24/2022 01:38:31 - INFO - __main__ - Global step 950 Train loss 3.72 ACC 0.15625 on epoch=474
06/24/2022 01:38:31 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.15625 on epoch=474, global_step=950
06/24/2022 01:38:33 - INFO - __main__ - Step 960 Global step 960 Train loss 3.58 on epoch=479
06/24/2022 01:38:34 - INFO - __main__ - Step 970 Global step 970 Train loss 3.62 on epoch=484
06/24/2022 01:38:36 - INFO - __main__ - Step 980 Global step 980 Train loss 3.48 on epoch=489
06/24/2022 01:38:37 - INFO - __main__ - Step 990 Global step 990 Train loss 3.47 on epoch=494
06/24/2022 01:38:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.45 on epoch=499
06/24/2022 01:38:41 - INFO - __main__ - Global step 1000 Train loss 3.52 ACC 0.21875 on epoch=499
06/24/2022 01:38:41 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.21875 on epoch=499, global_step=1000
06/24/2022 01:38:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.33 on epoch=504
06/24/2022 01:38:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.31 on epoch=509
06/24/2022 01:38:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.14 on epoch=514
06/24/2022 01:38:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.09 on epoch=519
06/24/2022 01:38:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.02 on epoch=524
06/24/2022 01:38:52 - INFO - __main__ - Global step 1050 Train loss 3.18 ACC 0.3125 on epoch=524
06/24/2022 01:38:52 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.3125 on epoch=524, global_step=1050
06/24/2022 01:38:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.95 on epoch=529
06/24/2022 01:38:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.92 on epoch=534
06/24/2022 01:38:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.88 on epoch=539
06/24/2022 01:38:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.76 on epoch=544
06/24/2022 01:38:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.76 on epoch=549
06/24/2022 01:39:02 - INFO - __main__ - Global step 1100 Train loss 2.85 ACC 0.3125 on epoch=549
06/24/2022 01:39:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.64 on epoch=554
06/24/2022 01:39:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 2.55 on epoch=559
06/24/2022 01:39:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.49 on epoch=564
06/24/2022 01:39:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 2.38 on epoch=569
06/24/2022 01:39:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 2.29 on epoch=574
06/24/2022 01:39:12 - INFO - __main__ - Global step 1150 Train loss 2.47 ACC 0.4375 on epoch=574
06/24/2022 01:39:12 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.4375 on epoch=574, global_step=1150
06/24/2022 01:39:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.38 on epoch=579
06/24/2022 01:39:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.34 on epoch=584
06/24/2022 01:39:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 2.36 on epoch=589
06/24/2022 01:39:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.22 on epoch=594
06/24/2022 01:39:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.12 on epoch=599
06/24/2022 01:39:23 - INFO - __main__ - Global step 1200 Train loss 2.29 ACC 0.4375 on epoch=599
06/24/2022 01:39:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.16 on epoch=604
06/24/2022 01:39:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.16 on epoch=609
06/24/2022 01:39:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.15 on epoch=614
06/24/2022 01:39:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.05 on epoch=619
06/24/2022 01:39:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.96 on epoch=624
06/24/2022 01:39:32 - INFO - __main__ - Global step 1250 Train loss 2.09 ACC 0.5 on epoch=624
06/24/2022 01:39:32 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=624, global_step=1250
06/24/2022 01:39:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.99 on epoch=629
06/24/2022 01:39:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.84 on epoch=634
06/24/2022 01:39:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.89 on epoch=639
06/24/2022 01:39:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.90 on epoch=644
06/24/2022 01:39:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.72 on epoch=649
06/24/2022 01:39:42 - INFO - __main__ - Global step 1300 Train loss 1.87 ACC 0.5 on epoch=649
06/24/2022 01:39:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.86 on epoch=654
06/24/2022 01:39:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.81 on epoch=659
06/24/2022 01:39:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.74 on epoch=664
06/24/2022 01:39:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.73 on epoch=669
06/24/2022 01:39:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.80 on epoch=674
06/24/2022 01:39:52 - INFO - __main__ - Global step 1350 Train loss 1.79 ACC 0.5 on epoch=674
06/24/2022 01:39:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.55 on epoch=679
06/24/2022 01:39:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.66 on epoch=684
06/24/2022 01:39:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.64 on epoch=689
06/24/2022 01:39:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.59 on epoch=694
06/24/2022 01:40:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.63 on epoch=699
06/24/2022 01:40:02 - INFO - __main__ - Global step 1400 Train loss 1.61 ACC 0.5 on epoch=699
06/24/2022 01:40:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.68 on epoch=704
06/24/2022 01:40:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.64 on epoch=709
06/24/2022 01:40:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.72 on epoch=714
06/24/2022 01:40:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.53 on epoch=719
06/24/2022 01:40:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.55 on epoch=724
06/24/2022 01:40:12 - INFO - __main__ - Global step 1450 Train loss 1.63 ACC 0.5 on epoch=724
06/24/2022 01:40:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.51 on epoch=729
06/24/2022 01:40:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.49 on epoch=734
06/24/2022 01:40:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.48 on epoch=739
06/24/2022 01:40:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.55 on epoch=744
06/24/2022 01:40:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.55 on epoch=749
06/24/2022 01:40:23 - INFO - __main__ - Global step 1500 Train loss 1.52 ACC 0.5 on epoch=749
06/24/2022 01:40:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.51 on epoch=754
06/24/2022 01:40:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.59 on epoch=759
06/24/2022 01:40:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.37 on epoch=764
06/24/2022 01:40:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.35 on epoch=769
06/24/2022 01:40:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.37 on epoch=774
06/24/2022 01:40:32 - INFO - __main__ - Global step 1550 Train loss 1.44 ACC 0.5 on epoch=774
06/24/2022 01:40:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.34 on epoch=779
06/24/2022 01:40:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.30 on epoch=784
06/24/2022 01:40:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.38 on epoch=789
06/24/2022 01:40:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.28 on epoch=794
06/24/2022 01:40:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.19 on epoch=799
06/24/2022 01:40:41 - INFO - __main__ - Global step 1600 Train loss 1.30 ACC 0.5 on epoch=799
06/24/2022 01:40:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.13 on epoch=804
06/24/2022 01:40:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.32 on epoch=809
06/24/2022 01:40:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.26 on epoch=814
06/24/2022 01:40:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.20 on epoch=819
06/24/2022 01:40:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.22 on epoch=824
06/24/2022 01:40:50 - INFO - __main__ - Global step 1650 Train loss 1.23 ACC 0.5 on epoch=824
06/24/2022 01:40:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.17 on epoch=829
06/24/2022 01:40:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.07 on epoch=834
06/24/2022 01:40:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.11 on epoch=839
06/24/2022 01:40:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.03 on epoch=844
06/24/2022 01:40:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.07 on epoch=849
06/24/2022 01:40:59 - INFO - __main__ - Global step 1700 Train loss 1.09 ACC 0.5 on epoch=849
06/24/2022 01:41:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.99 on epoch=854
06/24/2022 01:41:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.09 on epoch=859
06/24/2022 01:41:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.13 on epoch=864
06/24/2022 01:41:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.13 on epoch=869
06/24/2022 01:41:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.92 on epoch=874
06/24/2022 01:41:08 - INFO - __main__ - Global step 1750 Train loss 1.05 ACC 0.5 on epoch=874
06/24/2022 01:41:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.03 on epoch=879
06/24/2022 01:41:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.03 on epoch=884
06/24/2022 01:41:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.97 on epoch=889
06/24/2022 01:41:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.96 on epoch=894
06/24/2022 01:41:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.90 on epoch=899
06/24/2022 01:41:17 - INFO - __main__ - Global step 1800 Train loss 0.98 ACC 0.5 on epoch=899
06/24/2022 01:41:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.95 on epoch=904
06/24/2022 01:41:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.02 on epoch=909
06/24/2022 01:41:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.86 on epoch=914
06/24/2022 01:41:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.87 on epoch=919
06/24/2022 01:41:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.95 on epoch=924
06/24/2022 01:41:25 - INFO - __main__ - Global step 1850 Train loss 0.93 ACC 0.5 on epoch=924
06/24/2022 01:41:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.85 on epoch=929
06/24/2022 01:41:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.91 on epoch=934
06/24/2022 01:41:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.89 on epoch=939
06/24/2022 01:41:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.91 on epoch=944
06/24/2022 01:41:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.88 on epoch=949
06/24/2022 01:41:33 - INFO - __main__ - Global step 1900 Train loss 0.89 ACC 0.5 on epoch=949
06/24/2022 01:41:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.85 on epoch=954
06/24/2022 01:41:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.84 on epoch=959
06/24/2022 01:41:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.84 on epoch=964
06/24/2022 01:41:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.81 on epoch=969
06/24/2022 01:41:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.86 on epoch=974
06/24/2022 01:41:41 - INFO - __main__ - Global step 1950 Train loss 0.84 ACC 0.5 on epoch=974
06/24/2022 01:41:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.76 on epoch=979
06/24/2022 01:41:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.87 on epoch=984
06/24/2022 01:41:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.85 on epoch=989
06/24/2022 01:41:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.80 on epoch=994
06/24/2022 01:41:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.81 on epoch=999
06/24/2022 01:41:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 01:41:49 - INFO - __main__ - Printing 3 examples
06/24/2022 01:41:49 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 01:41:49 - INFO - __main__ - ['duplicate']
06/24/2022 01:41:49 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 01:41:49 - INFO - __main__ - ['duplicate']
06/24/2022 01:41:49 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 01:41:49 - INFO - __main__ - ['duplicate']
06/24/2022 01:41:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 01:41:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 01:41:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 01:41:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 01:41:49 - INFO - __main__ - Printing 3 examples
06/24/2022 01:41:49 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 01:41:49 - INFO - __main__ - ['duplicate']
06/24/2022 01:41:49 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 01:41:49 - INFO - __main__ - ['duplicate']
06/24/2022 01:41:49 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 01:41:49 - INFO - __main__ - ['duplicate']
06/24/2022 01:41:49 - INFO - __main__ - Tokenizing Input ...
06/24/2022 01:41:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 01:41:50 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 01:41:50 - INFO - __main__ - Global step 2000 Train loss 0.82 ACC 0.5 on epoch=999
06/24/2022 01:41:50 - INFO - __main__ - save last model!
06/24/2022 01:41:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 01:41:50 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 01:41:50 - INFO - __main__ - Printing 3 examples
06/24/2022 01:41:50 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 01:41:50 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:41:50 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 01:41:50 - INFO - __main__ - ['not_duplicate']
06/24/2022 01:41:50 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 01:41:50 - INFO - __main__ - ['duplicate']
06/24/2022 01:41:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 01:41:56 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 01:41:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 01:41:56 - INFO - __main__ - Starting training!
06/24/2022 01:42:10 - INFO - __main__ - Tokenizing Output ...
06/24/2022 01:42:52 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 02:04:48 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_21_0.2_8_predictions.txt
06/24/2022 02:04:48 - INFO - __main__ - ACC on test data: 0.3682
06/24/2022 02:04:48 - INFO - __main__ - prefix=glue-qqp_16_21, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.36816720257234725
06/24/2022 02:04:48 - INFO - __main__ - Running ... prefix=glue-qqp_16_42, lr=0.5, bsz=8 ...
06/24/2022 02:04:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 02:04:49 - INFO - __main__ - Printing 3 examples
06/24/2022 02:04:49 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 02:04:49 - INFO - __main__ - ['duplicate']
06/24/2022 02:04:49 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 02:04:49 - INFO - __main__ - ['duplicate']
06/24/2022 02:04:49 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 02:04:49 - INFO - __main__ - ['duplicate']
06/24/2022 02:04:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 02:04:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 02:04:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 02:04:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 02:04:49 - INFO - __main__ - Printing 3 examples
06/24/2022 02:04:49 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 02:04:49 - INFO - __main__ - ['duplicate']
06/24/2022 02:04:49 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 02:04:49 - INFO - __main__ - ['duplicate']
06/24/2022 02:04:49 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 02:04:49 - INFO - __main__ - ['duplicate']
06/24/2022 02:04:49 - INFO - __main__ - Tokenizing Input ...
06/24/2022 02:04:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 02:04:49 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 02:04:55 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 02:04:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 02:04:56 - INFO - __main__ - Starting training!
06/24/2022 02:04:57 - INFO - __main__ - Step 10 Global step 10 Train loss 7.23 on epoch=4
06/24/2022 02:04:59 - INFO - __main__ - Step 20 Global step 20 Train loss 7.20 on epoch=9
06/24/2022 02:05:00 - INFO - __main__ - Step 30 Global step 30 Train loss 7.15 on epoch=14
06/24/2022 02:05:02 - INFO - __main__ - Step 40 Global step 40 Train loss 7.23 on epoch=19
06/24/2022 02:05:03 - INFO - __main__ - Step 50 Global step 50 Train loss 7.19 on epoch=24
06/24/2022 02:05:11 - INFO - __main__ - Global step 50 Train loss 7.20 ACC 0.0 on epoch=24
06/24/2022 02:05:11 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 02:05:12 - INFO - __main__ - Step 60 Global step 60 Train loss 7.03 on epoch=29
06/24/2022 02:05:14 - INFO - __main__ - Step 70 Global step 70 Train loss 7.06 on epoch=34
06/24/2022 02:05:15 - INFO - __main__ - Step 80 Global step 80 Train loss 7.00 on epoch=39
06/24/2022 02:05:17 - INFO - __main__ - Step 90 Global step 90 Train loss 7.00 on epoch=44
06/24/2022 02:05:18 - INFO - __main__ - Step 100 Global step 100 Train loss 7.02 on epoch=49
06/24/2022 02:05:21 - INFO - __main__ - Global step 100 Train loss 7.02 ACC 0.0 on epoch=49
06/24/2022 02:05:22 - INFO - __main__ - Step 110 Global step 110 Train loss 6.94 on epoch=54
06/24/2022 02:05:24 - INFO - __main__ - Step 120 Global step 120 Train loss 6.86 on epoch=59
06/24/2022 02:05:25 - INFO - __main__ - Step 130 Global step 130 Train loss 6.79 on epoch=64
06/24/2022 02:05:27 - INFO - __main__ - Step 140 Global step 140 Train loss 6.87 on epoch=69
06/24/2022 02:05:29 - INFO - __main__ - Step 150 Global step 150 Train loss 6.83 on epoch=74
06/24/2022 02:05:32 - INFO - __main__ - Global step 150 Train loss 6.86 ACC 0.0 on epoch=74
06/24/2022 02:05:33 - INFO - __main__ - Step 160 Global step 160 Train loss 6.67 on epoch=79
06/24/2022 02:05:34 - INFO - __main__ - Step 170 Global step 170 Train loss 6.69 on epoch=84
06/24/2022 02:05:36 - INFO - __main__ - Step 180 Global step 180 Train loss 6.69 on epoch=89
06/24/2022 02:05:37 - INFO - __main__ - Step 190 Global step 190 Train loss 6.50 on epoch=94
06/24/2022 02:05:39 - INFO - __main__ - Step 200 Global step 200 Train loss 6.45 on epoch=99
06/24/2022 02:05:40 - INFO - __main__ - Global step 200 Train loss 6.60 ACC 0.0 on epoch=99
06/24/2022 02:05:42 - INFO - __main__ - Step 210 Global step 210 Train loss 6.45 on epoch=104
06/24/2022 02:05:43 - INFO - __main__ - Step 220 Global step 220 Train loss 6.39 on epoch=109
06/24/2022 02:05:44 - INFO - __main__ - Step 230 Global step 230 Train loss 6.22 on epoch=114
06/24/2022 02:05:46 - INFO - __main__ - Step 240 Global step 240 Train loss 6.15 on epoch=119
06/24/2022 02:05:47 - INFO - __main__ - Step 250 Global step 250 Train loss 6.02 on epoch=124
06/24/2022 02:05:50 - INFO - __main__ - Global step 250 Train loss 6.25 ACC 0.0 on epoch=124
06/24/2022 02:05:51 - INFO - __main__ - Step 260 Global step 260 Train loss 5.87 on epoch=129
06/24/2022 02:05:52 - INFO - __main__ - Step 270 Global step 270 Train loss 5.61 on epoch=134
06/24/2022 02:05:54 - INFO - __main__ - Step 280 Global step 280 Train loss 5.52 on epoch=139
06/24/2022 02:05:55 - INFO - __main__ - Step 290 Global step 290 Train loss 5.53 on epoch=144
06/24/2022 02:05:57 - INFO - __main__ - Step 300 Global step 300 Train loss 5.41 on epoch=149
06/24/2022 02:05:59 - INFO - __main__ - Global step 300 Train loss 5.59 ACC 0.0 on epoch=149
06/24/2022 02:06:00 - INFO - __main__ - Step 310 Global step 310 Train loss 5.22 on epoch=154
06/24/2022 02:06:02 - INFO - __main__ - Step 320 Global step 320 Train loss 5.08 on epoch=159
06/24/2022 02:06:03 - INFO - __main__ - Step 330 Global step 330 Train loss 5.12 on epoch=164
06/24/2022 02:06:04 - INFO - __main__ - Step 340 Global step 340 Train loss 5.02 on epoch=169
06/24/2022 02:06:06 - INFO - __main__ - Step 350 Global step 350 Train loss 4.94 on epoch=174
06/24/2022 02:06:09 - INFO - __main__ - Global step 350 Train loss 5.08 ACC 0.0 on epoch=174
06/24/2022 02:06:11 - INFO - __main__ - Step 360 Global step 360 Train loss 4.83 on epoch=179
06/24/2022 02:06:12 - INFO - __main__ - Step 370 Global step 370 Train loss 4.77 on epoch=184
06/24/2022 02:06:14 - INFO - __main__ - Step 380 Global step 380 Train loss 4.67 on epoch=189
06/24/2022 02:06:15 - INFO - __main__ - Step 390 Global step 390 Train loss 4.55 on epoch=194
06/24/2022 02:06:17 - INFO - __main__ - Step 400 Global step 400 Train loss 4.42 on epoch=199
06/24/2022 02:06:23 - INFO - __main__ - Global step 400 Train loss 4.65 ACC 0.0 on epoch=199
06/24/2022 02:06:24 - INFO - __main__ - Step 410 Global step 410 Train loss 4.39 on epoch=204
06/24/2022 02:06:26 - INFO - __main__ - Step 420 Global step 420 Train loss 4.25 on epoch=209
06/24/2022 02:06:27 - INFO - __main__ - Step 430 Global step 430 Train loss 4.13 on epoch=214
06/24/2022 02:06:29 - INFO - __main__ - Step 440 Global step 440 Train loss 4.13 on epoch=219
06/24/2022 02:06:30 - INFO - __main__ - Step 450 Global step 450 Train loss 3.99 on epoch=224
06/24/2022 02:06:33 - INFO - __main__ - Global step 450 Train loss 4.18 ACC 0.0 on epoch=224
06/24/2022 02:06:35 - INFO - __main__ - Step 460 Global step 460 Train loss 3.90 on epoch=229
06/24/2022 02:06:36 - INFO - __main__ - Step 470 Global step 470 Train loss 3.78 on epoch=234
06/24/2022 02:06:37 - INFO - __main__ - Step 480 Global step 480 Train loss 3.55 on epoch=239
06/24/2022 02:06:39 - INFO - __main__ - Step 490 Global step 490 Train loss 3.50 on epoch=244
06/24/2022 02:06:40 - INFO - __main__ - Step 500 Global step 500 Train loss 3.33 on epoch=249
06/24/2022 02:06:43 - INFO - __main__ - Global step 500 Train loss 3.61 ACC 0.0 on epoch=249
06/24/2022 02:06:44 - INFO - __main__ - Step 510 Global step 510 Train loss 3.48 on epoch=254
06/24/2022 02:06:46 - INFO - __main__ - Step 520 Global step 520 Train loss 3.27 on epoch=259
06/24/2022 02:06:47 - INFO - __main__ - Step 530 Global step 530 Train loss 3.26 on epoch=264
06/24/2022 02:06:49 - INFO - __main__ - Step 540 Global step 540 Train loss 3.03 on epoch=269
06/24/2022 02:06:50 - INFO - __main__ - Step 550 Global step 550 Train loss 2.84 on epoch=274
06/24/2022 02:06:53 - INFO - __main__ - Global step 550 Train loss 3.17 ACC 0.09375 on epoch=274
06/24/2022 02:06:53 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=274, global_step=550
06/24/2022 02:06:55 - INFO - __main__ - Step 560 Global step 560 Train loss 2.85 on epoch=279
06/24/2022 02:06:56 - INFO - __main__ - Step 570 Global step 570 Train loss 2.65 on epoch=284
06/24/2022 02:06:57 - INFO - __main__ - Step 580 Global step 580 Train loss 2.47 on epoch=289
06/24/2022 02:06:59 - INFO - __main__ - Step 590 Global step 590 Train loss 2.46 on epoch=294
06/24/2022 02:07:00 - INFO - __main__ - Step 600 Global step 600 Train loss 2.26 on epoch=299
06/24/2022 02:07:03 - INFO - __main__ - Global step 600 Train loss 2.54 ACC 0.375 on epoch=299
06/24/2022 02:07:03 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.375 on epoch=299, global_step=600
06/24/2022 02:07:05 - INFO - __main__ - Step 610 Global step 610 Train loss 2.26 on epoch=304
06/24/2022 02:07:06 - INFO - __main__ - Step 620 Global step 620 Train loss 2.20 on epoch=309
06/24/2022 02:07:08 - INFO - __main__ - Step 630 Global step 630 Train loss 2.10 on epoch=314
06/24/2022 02:07:09 - INFO - __main__ - Step 640 Global step 640 Train loss 1.92 on epoch=319
06/24/2022 02:07:10 - INFO - __main__ - Step 650 Global step 650 Train loss 1.78 on epoch=324
06/24/2022 02:07:13 - INFO - __main__ - Global step 650 Train loss 2.05 ACC 0.5 on epoch=324
06/24/2022 02:07:13 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.5 on epoch=324, global_step=650
06/24/2022 02:07:15 - INFO - __main__ - Step 660 Global step 660 Train loss 1.77 on epoch=329
06/24/2022 02:07:16 - INFO - __main__ - Step 670 Global step 670 Train loss 1.61 on epoch=334
06/24/2022 02:07:18 - INFO - __main__ - Step 680 Global step 680 Train loss 1.59 on epoch=339
06/24/2022 02:07:19 - INFO - __main__ - Step 690 Global step 690 Train loss 1.53 on epoch=344
06/24/2022 02:07:20 - INFO - __main__ - Step 700 Global step 700 Train loss 1.57 on epoch=349
06/24/2022 02:07:23 - INFO - __main__ - Global step 700 Train loss 1.62 ACC 0.5 on epoch=349
06/24/2022 02:07:24 - INFO - __main__ - Step 710 Global step 710 Train loss 1.41 on epoch=354
06/24/2022 02:07:26 - INFO - __main__ - Step 720 Global step 720 Train loss 1.38 on epoch=359
06/24/2022 02:07:27 - INFO - __main__ - Step 730 Global step 730 Train loss 1.27 on epoch=364
06/24/2022 02:07:28 - INFO - __main__ - Step 740 Global step 740 Train loss 1.24 on epoch=369
06/24/2022 02:07:30 - INFO - __main__ - Step 750 Global step 750 Train loss 1.18 on epoch=374
06/24/2022 02:07:32 - INFO - __main__ - Global step 750 Train loss 1.30 ACC 0.5 on epoch=374
06/24/2022 02:07:33 - INFO - __main__ - Step 760 Global step 760 Train loss 1.26 on epoch=379
06/24/2022 02:07:35 - INFO - __main__ - Step 770 Global step 770 Train loss 1.20 on epoch=384
06/24/2022 02:07:36 - INFO - __main__ - Step 780 Global step 780 Train loss 1.03 on epoch=389
06/24/2022 02:07:37 - INFO - __main__ - Step 790 Global step 790 Train loss 1.10 on epoch=394
06/24/2022 02:07:39 - INFO - __main__ - Step 800 Global step 800 Train loss 1.02 on epoch=399
06/24/2022 02:07:41 - INFO - __main__ - Global step 800 Train loss 1.12 ACC 0.5 on epoch=399
06/24/2022 02:07:42 - INFO - __main__ - Step 810 Global step 810 Train loss 1.00 on epoch=404
06/24/2022 02:07:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.90 on epoch=409
06/24/2022 02:07:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.91 on epoch=414
06/24/2022 02:07:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.94 on epoch=419
06/24/2022 02:07:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.91 on epoch=424
06/24/2022 02:07:50 - INFO - __main__ - Global step 850 Train loss 0.93 ACC 0.5 on epoch=424
06/24/2022 02:07:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.87 on epoch=429
06/24/2022 02:07:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.72 on epoch=434
06/24/2022 02:07:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.88 on epoch=439
06/24/2022 02:07:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.78 on epoch=444
06/24/2022 02:07:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.73 on epoch=449
06/24/2022 02:08:00 - INFO - __main__ - Global step 900 Train loss 0.80 ACC 0.5 on epoch=449
06/24/2022 02:08:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.73 on epoch=454
06/24/2022 02:08:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.76 on epoch=459
06/24/2022 02:08:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.71 on epoch=464
06/24/2022 02:08:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.60 on epoch=469
06/24/2022 02:08:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.59 on epoch=474
06/24/2022 02:08:09 - INFO - __main__ - Global step 950 Train loss 0.68 ACC 0.5 on epoch=474
06/24/2022 02:08:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.63 on epoch=479
06/24/2022 02:08:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.64 on epoch=484
06/24/2022 02:08:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.63 on epoch=489
06/24/2022 02:08:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.64 on epoch=494
06/24/2022 02:08:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.57 on epoch=499
06/24/2022 02:08:20 - INFO - __main__ - Global step 1000 Train loss 0.62 ACC 0.40625 on epoch=499
06/24/2022 02:08:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.71 on epoch=504
06/24/2022 02:08:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.58 on epoch=509
06/24/2022 02:08:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.61 on epoch=514
06/24/2022 02:08:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.55 on epoch=519
06/24/2022 02:08:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.53 on epoch=524
06/24/2022 02:08:30 - INFO - __main__ - Global step 1050 Train loss 0.60 ACC 0.5 on epoch=524
06/24/2022 02:08:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.56 on epoch=529
06/24/2022 02:08:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.51 on epoch=534
06/24/2022 02:08:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.55 on epoch=539
06/24/2022 02:08:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.53 on epoch=544
06/24/2022 02:08:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=549
06/24/2022 02:08:40 - INFO - __main__ - Global step 1100 Train loss 0.53 ACC 0.5 on epoch=549
06/24/2022 02:08:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=554
06/24/2022 02:08:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=559
06/24/2022 02:08:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.47 on epoch=564
06/24/2022 02:08:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=569
06/24/2022 02:08:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=574
06/24/2022 02:08:50 - INFO - __main__ - Global step 1150 Train loss 0.46 ACC 0.5 on epoch=574
06/24/2022 02:08:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=579
06/24/2022 02:08:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=584
06/24/2022 02:08:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.58 on epoch=589
06/24/2022 02:08:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=594
06/24/2022 02:08:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=599
06/24/2022 02:09:00 - INFO - __main__ - Global step 1200 Train loss 0.47 ACC 0.5 on epoch=599
06/24/2022 02:09:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.51 on epoch=604
06/24/2022 02:09:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=609
06/24/2022 02:09:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=614
06/24/2022 02:09:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=619
06/24/2022 02:09:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=624
06/24/2022 02:09:13 - INFO - __main__ - Global step 1250 Train loss 0.47 ACC 0.65625 on epoch=624
06/24/2022 02:09:13 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.65625 on epoch=624, global_step=1250
06/24/2022 02:09:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=629
06/24/2022 02:09:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=634
06/24/2022 02:09:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=639
06/24/2022 02:09:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=644
06/24/2022 02:09:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=649
06/24/2022 02:09:22 - INFO - __main__ - Global step 1300 Train loss 0.41 ACC 0.53125 on epoch=649
06/24/2022 02:09:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=654
06/24/2022 02:09:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=659
06/24/2022 02:09:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=664
06/24/2022 02:09:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=669
06/24/2022 02:09:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=674
06/24/2022 02:09:39 - INFO - __main__ - Global step 1350 Train loss 0.39 ACC 0.5 on epoch=674
06/24/2022 02:09:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=679
06/24/2022 02:09:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=684
06/24/2022 02:09:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=689
06/24/2022 02:09:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=694
06/24/2022 02:09:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=699
06/24/2022 02:09:53 - INFO - __main__ - Global step 1400 Train loss 0.41 ACC 0.5 on epoch=699
06/24/2022 02:09:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=704
06/24/2022 02:09:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=709
06/24/2022 02:09:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.39 on epoch=714
06/24/2022 02:09:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=719
06/24/2022 02:10:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.35 on epoch=724
06/24/2022 02:10:03 - INFO - __main__ - Global step 1450 Train loss 0.37 ACC 0.46875 on epoch=724
06/24/2022 02:10:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=729
06/24/2022 02:10:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=734
06/24/2022 02:10:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=739
06/24/2022 02:10:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=744
06/24/2022 02:10:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=749
06/24/2022 02:10:14 - INFO - __main__ - Global step 1500 Train loss 0.37 ACC 0.5 on epoch=749
06/24/2022 02:10:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=754
06/24/2022 02:10:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=759
06/24/2022 02:10:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=764
06/24/2022 02:10:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.39 on epoch=769
06/24/2022 02:10:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=774
06/24/2022 02:10:25 - INFO - __main__ - Global step 1550 Train loss 0.38 ACC 0.5 on epoch=774
06/24/2022 02:10:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=779
06/24/2022 02:10:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=784
06/24/2022 02:10:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.33 on epoch=789
06/24/2022 02:10:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.31 on epoch=794
06/24/2022 02:10:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.39 on epoch=799
06/24/2022 02:10:35 - INFO - __main__ - Global step 1600 Train loss 0.36 ACC 0.5625 on epoch=799
06/24/2022 02:10:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=804
06/24/2022 02:10:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.28 on epoch=809
06/24/2022 02:10:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=814
06/24/2022 02:10:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.31 on epoch=819
06/24/2022 02:10:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=824
06/24/2022 02:10:44 - INFO - __main__ - Global step 1650 Train loss 0.35 ACC 0.5 on epoch=824
06/24/2022 02:10:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=829
06/24/2022 02:10:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=834
06/24/2022 02:10:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=839
06/24/2022 02:10:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=844
06/24/2022 02:10:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.32 on epoch=849
06/24/2022 02:10:53 - INFO - __main__ - Global step 1700 Train loss 0.38 ACC 0.53125 on epoch=849
06/24/2022 02:10:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=854
06/24/2022 02:10:55 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=859
06/24/2022 02:10:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.35 on epoch=864
06/24/2022 02:10:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=869
06/24/2022 02:11:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=874
06/24/2022 02:11:02 - INFO - __main__ - Global step 1750 Train loss 0.34 ACC 0.5 on epoch=874
06/24/2022 02:11:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=879
06/24/2022 02:11:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=884
06/24/2022 02:11:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=889
06/24/2022 02:11:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=894
06/24/2022 02:11:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.31 on epoch=899
06/24/2022 02:11:11 - INFO - __main__ - Global step 1800 Train loss 0.33 ACC 0.5 on epoch=899
06/24/2022 02:11:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=904
06/24/2022 02:11:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=909
06/24/2022 02:11:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=914
06/24/2022 02:11:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.29 on epoch=919
06/24/2022 02:11:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.33 on epoch=924
06/24/2022 02:11:20 - INFO - __main__ - Global step 1850 Train loss 0.34 ACC 0.46875 on epoch=924
06/24/2022 02:11:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
06/24/2022 02:11:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=934
06/24/2022 02:11:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=939
06/24/2022 02:11:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.35 on epoch=944
06/24/2022 02:11:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=949
06/24/2022 02:11:28 - INFO - __main__ - Global step 1900 Train loss 0.33 ACC 0.46875 on epoch=949
06/24/2022 02:11:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=954
06/24/2022 02:11:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.34 on epoch=959
06/24/2022 02:11:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.37 on epoch=964
06/24/2022 02:11:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=969
06/24/2022 02:11:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=974
06/24/2022 02:11:36 - INFO - __main__ - Global step 1950 Train loss 0.33 ACC 0.5 on epoch=974
06/24/2022 02:11:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
06/24/2022 02:11:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=984
06/24/2022 02:11:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
06/24/2022 02:11:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=994
06/24/2022 02:11:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=999
06/24/2022 02:11:43 - INFO - __main__ - Global step 2000 Train loss 0.34 ACC 0.5 on epoch=999
06/24/2022 02:11:43 - INFO - __main__ - save last model!
06/24/2022 02:11:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 02:11:44 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 02:11:44 - INFO - __main__ - Printing 3 examples
06/24/2022 02:11:44 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 02:11:44 - INFO - __main__ - ['not_duplicate']
06/24/2022 02:11:44 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 02:11:44 - INFO - __main__ - ['not_duplicate']
06/24/2022 02:11:44 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 02:11:44 - INFO - __main__ - ['duplicate']
06/24/2022 02:11:44 - INFO - __main__ - Tokenizing Input ...
06/24/2022 02:11:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 02:11:44 - INFO - __main__ - Printing 3 examples
06/24/2022 02:11:44 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 02:11:44 - INFO - __main__ - ['duplicate']
06/24/2022 02:11:44 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 02:11:44 - INFO - __main__ - ['duplicate']
06/24/2022 02:11:44 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 02:11:44 - INFO - __main__ - ['duplicate']
06/24/2022 02:11:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 02:11:44 - INFO - __main__ - Tokenizing Output ...
06/24/2022 02:11:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 02:11:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 02:11:44 - INFO - __main__ - Printing 3 examples
06/24/2022 02:11:44 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 02:11:44 - INFO - __main__ - ['duplicate']
06/24/2022 02:11:44 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 02:11:44 - INFO - __main__ - ['duplicate']
06/24/2022 02:11:44 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 02:11:44 - INFO - __main__ - ['duplicate']
06/24/2022 02:11:44 - INFO - __main__ - Tokenizing Input ...
06/24/2022 02:11:44 - INFO - __main__ - Tokenizing Output ...
06/24/2022 02:11:44 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 02:11:50 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 02:11:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 02:11:51 - INFO - __main__ - Starting training!
06/24/2022 02:12:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 02:12:49 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 02:26:50 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_42_0.5_8_predictions.txt
06/24/2022 02:26:50 - INFO - __main__ - ACC on test data: 0.6297
06/24/2022 02:26:50 - INFO - __main__ - prefix=glue-qqp_16_42, lr=0.5, bsz=8, dev_performance=0.65625, test_performance=0.629656195894138
06/24/2022 02:26:50 - INFO - __main__ - Running ... prefix=glue-qqp_16_42, lr=0.4, bsz=8 ...
06/24/2022 02:26:51 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 02:26:51 - INFO - __main__ - Printing 3 examples
06/24/2022 02:26:51 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 02:26:51 - INFO - __main__ - ['duplicate']
06/24/2022 02:26:51 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 02:26:51 - INFO - __main__ - ['duplicate']
06/24/2022 02:26:51 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 02:26:51 - INFO - __main__ - ['duplicate']
06/24/2022 02:26:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 02:26:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 02:26:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 02:26:51 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 02:26:51 - INFO - __main__ - Printing 3 examples
06/24/2022 02:26:51 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 02:26:51 - INFO - __main__ - ['duplicate']
06/24/2022 02:26:51 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 02:26:51 - INFO - __main__ - ['duplicate']
06/24/2022 02:26:51 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 02:26:51 - INFO - __main__ - ['duplicate']
06/24/2022 02:26:51 - INFO - __main__ - Tokenizing Input ...
06/24/2022 02:26:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 02:26:51 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 02:26:59 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 02:26:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 02:26:59 - INFO - __main__ - Starting training!
06/24/2022 02:27:01 - INFO - __main__ - Step 10 Global step 10 Train loss 7.16 on epoch=4
06/24/2022 02:27:02 - INFO - __main__ - Step 20 Global step 20 Train loss 7.15 on epoch=9
06/24/2022 02:27:03 - INFO - __main__ - Step 30 Global step 30 Train loss 7.15 on epoch=14
06/24/2022 02:27:05 - INFO - __main__ - Step 40 Global step 40 Train loss 7.13 on epoch=19
06/24/2022 02:27:06 - INFO - __main__ - Step 50 Global step 50 Train loss 7.12 on epoch=24
06/24/2022 02:27:08 - INFO - __main__ - Global step 50 Train loss 7.14 ACC 0.0 on epoch=24
06/24/2022 02:27:08 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 02:27:09 - INFO - __main__ - Step 60 Global step 60 Train loss 6.98 on epoch=29
06/24/2022 02:27:11 - INFO - __main__ - Step 70 Global step 70 Train loss 6.99 on epoch=34
06/24/2022 02:27:12 - INFO - __main__ - Step 80 Global step 80 Train loss 6.95 on epoch=39
06/24/2022 02:27:13 - INFO - __main__ - Step 90 Global step 90 Train loss 6.98 on epoch=44
06/24/2022 02:27:15 - INFO - __main__ - Step 100 Global step 100 Train loss 7.01 on epoch=49
06/24/2022 02:27:17 - INFO - __main__ - Global step 100 Train loss 6.98 ACC 0.0 on epoch=49
06/24/2022 02:27:18 - INFO - __main__ - Step 110 Global step 110 Train loss 6.85 on epoch=54
06/24/2022 02:27:19 - INFO - __main__ - Step 120 Global step 120 Train loss 6.96 on epoch=59
06/24/2022 02:27:21 - INFO - __main__ - Step 130 Global step 130 Train loss 6.79 on epoch=64
06/24/2022 02:27:22 - INFO - __main__ - Step 140 Global step 140 Train loss 6.83 on epoch=69
06/24/2022 02:27:23 - INFO - __main__ - Step 150 Global step 150 Train loss 6.66 on epoch=74
06/24/2022 02:27:25 - INFO - __main__ - Global step 150 Train loss 6.82 ACC 0.0 on epoch=74
06/24/2022 02:27:27 - INFO - __main__ - Step 160 Global step 160 Train loss 6.61 on epoch=79
06/24/2022 02:27:28 - INFO - __main__ - Step 170 Global step 170 Train loss 6.53 on epoch=84
06/24/2022 02:27:29 - INFO - __main__ - Step 180 Global step 180 Train loss 6.42 on epoch=89
06/24/2022 02:27:31 - INFO - __main__ - Step 190 Global step 190 Train loss 6.32 on epoch=94
06/24/2022 02:27:32 - INFO - __main__ - Step 200 Global step 200 Train loss 6.20 on epoch=99
06/24/2022 02:27:34 - INFO - __main__ - Global step 200 Train loss 6.41 ACC 0.0 on epoch=99
06/24/2022 02:27:36 - INFO - __main__ - Step 210 Global step 210 Train loss 6.24 on epoch=104
06/24/2022 02:27:37 - INFO - __main__ - Step 220 Global step 220 Train loss 6.17 on epoch=109
06/24/2022 02:27:38 - INFO - __main__ - Step 230 Global step 230 Train loss 6.06 on epoch=114
06/24/2022 02:27:40 - INFO - __main__ - Step 240 Global step 240 Train loss 5.97 on epoch=119
06/24/2022 02:27:41 - INFO - __main__ - Step 250 Global step 250 Train loss 5.91 on epoch=124
06/24/2022 02:27:43 - INFO - __main__ - Global step 250 Train loss 6.07 ACC 0.0 on epoch=124
06/24/2022 02:27:44 - INFO - __main__ - Step 260 Global step 260 Train loss 5.84 on epoch=129
06/24/2022 02:27:46 - INFO - __main__ - Step 270 Global step 270 Train loss 5.77 on epoch=134
06/24/2022 02:27:48 - INFO - __main__ - Step 280 Global step 280 Train loss 5.55 on epoch=139
06/24/2022 02:27:49 - INFO - __main__ - Step 290 Global step 290 Train loss 5.51 on epoch=144
06/24/2022 02:27:50 - INFO - __main__ - Step 300 Global step 300 Train loss 5.48 on epoch=149
06/24/2022 02:27:57 - INFO - __main__ - Global step 300 Train loss 5.63 ACC 0.0 on epoch=149
06/24/2022 02:27:59 - INFO - __main__ - Step 310 Global step 310 Train loss 5.50 on epoch=154
06/24/2022 02:28:00 - INFO - __main__ - Step 320 Global step 320 Train loss 5.32 on epoch=159
06/24/2022 02:28:01 - INFO - __main__ - Step 330 Global step 330 Train loss 5.24 on epoch=164
06/24/2022 02:28:03 - INFO - __main__ - Step 340 Global step 340 Train loss 5.19 on epoch=169
06/24/2022 02:28:04 - INFO - __main__ - Step 350 Global step 350 Train loss 5.07 on epoch=174
06/24/2022 02:28:06 - INFO - __main__ - Global step 350 Train loss 5.26 ACC 0.0 on epoch=174
06/24/2022 02:28:08 - INFO - __main__ - Step 360 Global step 360 Train loss 5.02 on epoch=179
06/24/2022 02:28:09 - INFO - __main__ - Step 370 Global step 370 Train loss 4.88 on epoch=184
06/24/2022 02:28:10 - INFO - __main__ - Step 380 Global step 380 Train loss 4.78 on epoch=189
06/24/2022 02:28:12 - INFO - __main__ - Step 390 Global step 390 Train loss 4.63 on epoch=194
06/24/2022 02:28:13 - INFO - __main__ - Step 400 Global step 400 Train loss 4.38 on epoch=199
06/24/2022 02:28:17 - INFO - __main__ - Global step 400 Train loss 4.74 ACC 0.0 on epoch=199
06/24/2022 02:28:19 - INFO - __main__ - Step 410 Global step 410 Train loss 4.31 on epoch=204
06/24/2022 02:28:20 - INFO - __main__ - Step 420 Global step 420 Train loss 4.42 on epoch=209
06/24/2022 02:28:22 - INFO - __main__ - Step 430 Global step 430 Train loss 4.12 on epoch=214
06/24/2022 02:28:24 - INFO - __main__ - Step 440 Global step 440 Train loss 4.00 on epoch=219
06/24/2022 02:28:25 - INFO - __main__ - Step 450 Global step 450 Train loss 3.97 on epoch=224
06/24/2022 02:28:29 - INFO - __main__ - Global step 450 Train loss 4.16 ACC 0.0 on epoch=224
06/24/2022 02:28:31 - INFO - __main__ - Step 460 Global step 460 Train loss 3.94 on epoch=229
06/24/2022 02:28:33 - INFO - __main__ - Step 470 Global step 470 Train loss 3.94 on epoch=234
06/24/2022 02:28:34 - INFO - __main__ - Step 480 Global step 480 Train loss 3.74 on epoch=239
06/24/2022 02:28:36 - INFO - __main__ - Step 490 Global step 490 Train loss 3.75 on epoch=244
06/24/2022 02:28:37 - INFO - __main__ - Step 500 Global step 500 Train loss 4.05 on epoch=249
06/24/2022 02:28:43 - INFO - __main__ - Global step 500 Train loss 3.88 ACC 0.03125 on epoch=249
06/24/2022 02:28:43 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=249, global_step=500
06/24/2022 02:28:45 - INFO - __main__ - Step 510 Global step 510 Train loss 3.87 on epoch=254
06/24/2022 02:28:46 - INFO - __main__ - Step 520 Global step 520 Train loss 3.75 on epoch=259
06/24/2022 02:28:47 - INFO - __main__ - Step 530 Global step 530 Train loss 3.64 on epoch=264
06/24/2022 02:28:49 - INFO - __main__ - Step 540 Global step 540 Train loss 3.70 on epoch=269
06/24/2022 02:28:50 - INFO - __main__ - Step 550 Global step 550 Train loss 3.68 on epoch=274
06/24/2022 02:28:52 - INFO - __main__ - Global step 550 Train loss 3.73 ACC 0.15625 on epoch=274
06/24/2022 02:28:52 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.15625 on epoch=274, global_step=550
06/24/2022 02:28:53 - INFO - __main__ - Step 560 Global step 560 Train loss 3.57 on epoch=279
06/24/2022 02:28:54 - INFO - __main__ - Step 570 Global step 570 Train loss 3.71 on epoch=284
06/24/2022 02:28:56 - INFO - __main__ - Step 580 Global step 580 Train loss 3.38 on epoch=289
06/24/2022 02:28:57 - INFO - __main__ - Step 590 Global step 590 Train loss 3.34 on epoch=294
06/24/2022 02:28:58 - INFO - __main__ - Step 600 Global step 600 Train loss 3.27 on epoch=299
06/24/2022 02:29:01 - INFO - __main__ - Global step 600 Train loss 3.46 ACC 0.46875 on epoch=299
06/24/2022 02:29:01 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.46875 on epoch=299, global_step=600
06/24/2022 02:29:02 - INFO - __main__ - Step 610 Global step 610 Train loss 3.17 on epoch=304
06/24/2022 02:29:03 - INFO - __main__ - Step 620 Global step 620 Train loss 3.03 on epoch=309
06/24/2022 02:29:05 - INFO - __main__ - Step 630 Global step 630 Train loss 2.98 on epoch=314
06/24/2022 02:29:06 - INFO - __main__ - Step 640 Global step 640 Train loss 2.88 on epoch=319
06/24/2022 02:29:07 - INFO - __main__ - Step 650 Global step 650 Train loss 2.86 on epoch=324
06/24/2022 02:29:09 - INFO - __main__ - Global step 650 Train loss 2.98 ACC 0.5 on epoch=324
06/24/2022 02:29:09 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=324, global_step=650
06/24/2022 02:29:11 - INFO - __main__ - Step 660 Global step 660 Train loss 2.61 on epoch=329
06/24/2022 02:29:12 - INFO - __main__ - Step 670 Global step 670 Train loss 2.75 on epoch=334
06/24/2022 02:29:13 - INFO - __main__ - Step 680 Global step 680 Train loss 2.58 on epoch=339
06/24/2022 02:29:15 - INFO - __main__ - Step 690 Global step 690 Train loss 2.55 on epoch=344
06/24/2022 02:29:16 - INFO - __main__ - Step 700 Global step 700 Train loss 2.53 on epoch=349
06/24/2022 02:29:19 - INFO - __main__ - Global step 700 Train loss 2.60 ACC 0.5 on epoch=349
06/24/2022 02:29:20 - INFO - __main__ - Step 710 Global step 710 Train loss 2.44 on epoch=354
06/24/2022 02:29:21 - INFO - __main__ - Step 720 Global step 720 Train loss 2.32 on epoch=359
06/24/2022 02:29:23 - INFO - __main__ - Step 730 Global step 730 Train loss 2.29 on epoch=364
06/24/2022 02:29:24 - INFO - __main__ - Step 740 Global step 740 Train loss 2.22 on epoch=369
06/24/2022 02:29:25 - INFO - __main__ - Step 750 Global step 750 Train loss 2.06 on epoch=374
06/24/2022 02:29:28 - INFO - __main__ - Global step 750 Train loss 2.27 ACC 0.5 on epoch=374
06/24/2022 02:29:29 - INFO - __main__ - Step 760 Global step 760 Train loss 2.19 on epoch=379
06/24/2022 02:29:30 - INFO - __main__ - Step 770 Global step 770 Train loss 2.12 on epoch=384
06/24/2022 02:29:32 - INFO - __main__ - Step 780 Global step 780 Train loss 2.15 on epoch=389
06/24/2022 02:29:33 - INFO - __main__ - Step 790 Global step 790 Train loss 1.87 on epoch=394
06/24/2022 02:29:34 - INFO - __main__ - Step 800 Global step 800 Train loss 1.89 on epoch=399
06/24/2022 02:29:36 - INFO - __main__ - Global step 800 Train loss 2.04 ACC 0.5 on epoch=399
06/24/2022 02:29:37 - INFO - __main__ - Step 810 Global step 810 Train loss 1.81 on epoch=404
06/24/2022 02:29:39 - INFO - __main__ - Step 820 Global step 820 Train loss 1.77 on epoch=409
06/24/2022 02:29:40 - INFO - __main__ - Step 830 Global step 830 Train loss 1.67 on epoch=414
06/24/2022 02:29:41 - INFO - __main__ - Step 840 Global step 840 Train loss 1.65 on epoch=419
06/24/2022 02:29:43 - INFO - __main__ - Step 850 Global step 850 Train loss 1.65 on epoch=424
06/24/2022 02:29:45 - INFO - __main__ - Global step 850 Train loss 1.71 ACC 0.5 on epoch=424
06/24/2022 02:29:46 - INFO - __main__ - Step 860 Global step 860 Train loss 1.50 on epoch=429
06/24/2022 02:29:47 - INFO - __main__ - Step 870 Global step 870 Train loss 1.45 on epoch=434
06/24/2022 02:29:49 - INFO - __main__ - Step 880 Global step 880 Train loss 1.44 on epoch=439
06/24/2022 02:29:50 - INFO - __main__ - Step 890 Global step 890 Train loss 1.37 on epoch=444
06/24/2022 02:29:52 - INFO - __main__ - Step 900 Global step 900 Train loss 1.29 on epoch=449
06/24/2022 02:29:54 - INFO - __main__ - Global step 900 Train loss 1.41 ACC 0.5 on epoch=449
06/24/2022 02:29:55 - INFO - __main__ - Step 910 Global step 910 Train loss 1.30 on epoch=454
06/24/2022 02:29:57 - INFO - __main__ - Step 920 Global step 920 Train loss 1.30 on epoch=459
06/24/2022 02:29:58 - INFO - __main__ - Step 930 Global step 930 Train loss 1.16 on epoch=464
06/24/2022 02:29:59 - INFO - __main__ - Step 940 Global step 940 Train loss 1.19 on epoch=469
06/24/2022 02:30:01 - INFO - __main__ - Step 950 Global step 950 Train loss 1.11 on epoch=474
06/24/2022 02:30:02 - INFO - __main__ - Global step 950 Train loss 1.21 ACC 0.5 on epoch=474
06/24/2022 02:30:04 - INFO - __main__ - Step 960 Global step 960 Train loss 1.03 on epoch=479
06/24/2022 02:30:05 - INFO - __main__ - Step 970 Global step 970 Train loss 1.13 on epoch=484
06/24/2022 02:30:07 - INFO - __main__ - Step 980 Global step 980 Train loss 1.05 on epoch=489
06/24/2022 02:30:08 - INFO - __main__ - Step 990 Global step 990 Train loss 1.13 on epoch=494
06/24/2022 02:30:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.95 on epoch=499
06/24/2022 02:30:12 - INFO - __main__ - Global step 1000 Train loss 1.06 ACC 0.5 on epoch=499
06/24/2022 02:30:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.87 on epoch=504
06/24/2022 02:30:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.91 on epoch=509
06/24/2022 02:30:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.89 on epoch=514
06/24/2022 02:30:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.84 on epoch=519
06/24/2022 02:30:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.95 on epoch=524
06/24/2022 02:30:20 - INFO - __main__ - Global step 1050 Train loss 0.89 ACC 0.5 on epoch=524
06/24/2022 02:30:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.83 on epoch=529
06/24/2022 02:30:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.81 on epoch=534
06/24/2022 02:30:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.87 on epoch=539
06/24/2022 02:30:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.78 on epoch=544
06/24/2022 02:30:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.76 on epoch=549
06/24/2022 02:30:29 - INFO - __main__ - Global step 1100 Train loss 0.81 ACC 0.5 on epoch=549
06/24/2022 02:30:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.74 on epoch=554
06/24/2022 02:30:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.67 on epoch=559
06/24/2022 02:30:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.77 on epoch=564
06/24/2022 02:30:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.69 on epoch=569
06/24/2022 02:30:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.63 on epoch=574
06/24/2022 02:30:39 - INFO - __main__ - Global step 1150 Train loss 0.70 ACC 0.5 on epoch=574
06/24/2022 02:30:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.72 on epoch=579
06/24/2022 02:30:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.67 on epoch=584
06/24/2022 02:30:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.68 on epoch=589
06/24/2022 02:30:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.69 on epoch=594
06/24/2022 02:30:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.72 on epoch=599
06/24/2022 02:30:48 - INFO - __main__ - Global step 1200 Train loss 0.70 ACC 0.5 on epoch=599
06/24/2022 02:30:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.68 on epoch=604
06/24/2022 02:30:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.61 on epoch=609
06/24/2022 02:30:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.65 on epoch=614
06/24/2022 02:30:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.61 on epoch=619
06/24/2022 02:30:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.65 on epoch=624
06/24/2022 02:30:57 - INFO - __main__ - Global step 1250 Train loss 0.64 ACC 0.5 on epoch=624
06/24/2022 02:30:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=629
06/24/2022 02:31:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.59 on epoch=634
06/24/2022 02:31:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.61 on epoch=639
06/24/2022 02:31:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.58 on epoch=644
06/24/2022 02:31:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.60 on epoch=649
06/24/2022 02:31:06 - INFO - __main__ - Global step 1300 Train loss 0.59 ACC 0.5 on epoch=649
06/24/2022 02:31:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=654
06/24/2022 02:31:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.61 on epoch=659
06/24/2022 02:31:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.52 on epoch=664
06/24/2022 02:31:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.50 on epoch=669
06/24/2022 02:31:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.47 on epoch=674
06/24/2022 02:31:15 - INFO - __main__ - Global step 1350 Train loss 0.52 ACC 0.5 on epoch=674
06/24/2022 02:31:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.51 on epoch=679
06/24/2022 02:31:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.49 on epoch=684
06/24/2022 02:31:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=689
06/24/2022 02:31:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.52 on epoch=694
06/24/2022 02:31:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.52 on epoch=699
06/24/2022 02:31:24 - INFO - __main__ - Global step 1400 Train loss 0.50 ACC 0.5 on epoch=699
06/24/2022 02:31:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.48 on epoch=704
06/24/2022 02:31:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=709
06/24/2022 02:31:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.47 on epoch=714
06/24/2022 02:31:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=719
06/24/2022 02:31:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.48 on epoch=724
06/24/2022 02:31:34 - INFO - __main__ - Global step 1450 Train loss 0.47 ACC 0.5 on epoch=724
06/24/2022 02:31:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=729
06/24/2022 02:31:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=734
06/24/2022 02:31:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.45 on epoch=739
06/24/2022 02:31:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=744
06/24/2022 02:31:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.53 on epoch=749
06/24/2022 02:31:44 - INFO - __main__ - Global step 1500 Train loss 0.46 ACC 0.5 on epoch=749
06/24/2022 02:31:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.46 on epoch=754
06/24/2022 02:31:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=759
06/24/2022 02:31:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=764
06/24/2022 02:31:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=769
06/24/2022 02:31:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=774
06/24/2022 02:31:54 - INFO - __main__ - Global step 1550 Train loss 0.44 ACC 0.5 on epoch=774
06/24/2022 02:31:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=779
06/24/2022 02:31:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
06/24/2022 02:31:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=789
06/24/2022 02:32:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=794
06/24/2022 02:32:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.50 on epoch=799
06/24/2022 02:32:04 - INFO - __main__ - Global step 1600 Train loss 0.42 ACC 0.5 on epoch=799
06/24/2022 02:32:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=804
06/24/2022 02:32:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=809
06/24/2022 02:32:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
06/24/2022 02:32:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
06/24/2022 02:32:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=824
06/24/2022 02:32:15 - INFO - __main__ - Global step 1650 Train loss 0.43 ACC 0.5 on epoch=824
06/24/2022 02:32:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.43 on epoch=829
06/24/2022 02:32:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=834
06/24/2022 02:32:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=839
06/24/2022 02:32:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=844
06/24/2022 02:32:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.33 on epoch=849
06/24/2022 02:32:25 - INFO - __main__ - Global step 1700 Train loss 0.39 ACC 0.5 on epoch=849
06/24/2022 02:32:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
06/24/2022 02:32:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=859
06/24/2022 02:32:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=864
06/24/2022 02:32:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=869
06/24/2022 02:32:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
06/24/2022 02:32:35 - INFO - __main__ - Global step 1750 Train loss 0.39 ACC 0.5 on epoch=874
06/24/2022 02:32:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=879
06/24/2022 02:32:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=884
06/24/2022 02:32:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=889
06/24/2022 02:32:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=894
06/24/2022 02:32:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=899
06/24/2022 02:32:45 - INFO - __main__ - Global step 1800 Train loss 0.37 ACC 0.5 on epoch=899
06/24/2022 02:32:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=904
06/24/2022 02:32:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=909
06/24/2022 02:32:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
06/24/2022 02:32:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.39 on epoch=919
06/24/2022 02:32:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.33 on epoch=924
06/24/2022 02:32:55 - INFO - __main__ - Global step 1850 Train loss 0.38 ACC 0.5 on epoch=924
06/24/2022 02:32:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
06/24/2022 02:32:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=934
06/24/2022 02:32:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=939
06/24/2022 02:33:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=944
06/24/2022 02:33:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
06/24/2022 02:33:05 - INFO - __main__ - Global step 1900 Train loss 0.38 ACC 0.5 on epoch=949
06/24/2022 02:33:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=954
06/24/2022 02:33:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
06/24/2022 02:33:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=964
06/24/2022 02:33:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=969
06/24/2022 02:33:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=974
06/24/2022 02:33:17 - INFO - __main__ - Global step 1950 Train loss 0.39 ACC 0.5 on epoch=974
06/24/2022 02:33:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=979
06/24/2022 02:33:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=984
06/24/2022 02:33:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
06/24/2022 02:33:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=994
06/24/2022 02:33:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.42 on epoch=999
06/24/2022 02:33:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 02:33:26 - INFO - __main__ - Printing 3 examples
06/24/2022 02:33:26 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 02:33:26 - INFO - __main__ - ['duplicate']
06/24/2022 02:33:26 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 02:33:26 - INFO - __main__ - ['duplicate']
06/24/2022 02:33:26 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 02:33:26 - INFO - __main__ - ['duplicate']
06/24/2022 02:33:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 02:33:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 02:33:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 02:33:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 02:33:26 - INFO - __main__ - Printing 3 examples
06/24/2022 02:33:26 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 02:33:26 - INFO - __main__ - ['duplicate']
06/24/2022 02:33:26 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 02:33:26 - INFO - __main__ - ['duplicate']
06/24/2022 02:33:26 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 02:33:26 - INFO - __main__ - ['duplicate']
06/24/2022 02:33:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 02:33:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 02:33:26 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 02:33:29 - INFO - __main__ - Global step 2000 Train loss 0.38 ACC 0.5 on epoch=999
06/24/2022 02:33:29 - INFO - __main__ - save last model!
06/24/2022 02:33:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 02:33:29 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 02:33:29 - INFO - __main__ - Printing 3 examples
06/24/2022 02:33:29 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 02:33:29 - INFO - __main__ - ['not_duplicate']
06/24/2022 02:33:29 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 02:33:29 - INFO - __main__ - ['not_duplicate']
06/24/2022 02:33:29 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 02:33:29 - INFO - __main__ - ['duplicate']
06/24/2022 02:33:29 - INFO - __main__ - Tokenizing Input ...
06/24/2022 02:33:32 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 02:33:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 02:33:33 - INFO - __main__ - Starting training!
06/24/2022 02:33:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 02:34:29 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 04:08:06 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_42_0.4_8_predictions.txt
06/24/2022 04:08:06 - INFO - __main__ - ACC on test data: 0.3682
06/24/2022 04:08:06 - INFO - __main__ - prefix=glue-qqp_16_42, lr=0.4, bsz=8, dev_performance=0.5, test_performance=0.36816720257234725
06/24/2022 04:08:06 - INFO - __main__ - Running ... prefix=glue-qqp_16_42, lr=0.3, bsz=8 ...
06/24/2022 04:08:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:08:07 - INFO - __main__ - Printing 3 examples
06/24/2022 04:08:07 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 04:08:07 - INFO - __main__ - ['duplicate']
06/24/2022 04:08:07 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 04:08:07 - INFO - __main__ - ['duplicate']
06/24/2022 04:08:07 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 04:08:07 - INFO - __main__ - ['duplicate']
06/24/2022 04:08:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 04:08:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:08:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 04:08:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:08:07 - INFO - __main__ - Printing 3 examples
06/24/2022 04:08:07 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 04:08:07 - INFO - __main__ - ['duplicate']
06/24/2022 04:08:07 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 04:08:07 - INFO - __main__ - ['duplicate']
06/24/2022 04:08:07 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 04:08:07 - INFO - __main__ - ['duplicate']
06/24/2022 04:08:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:08:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:08:08 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 04:08:14 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 04:08:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 04:08:14 - INFO - __main__ - Starting training!
06/24/2022 04:08:16 - INFO - __main__ - Step 10 Global step 10 Train loss 7.29 on epoch=4
06/24/2022 04:08:17 - INFO - __main__ - Step 20 Global step 20 Train loss 7.15 on epoch=9
06/24/2022 04:08:19 - INFO - __main__ - Step 30 Global step 30 Train loss 7.25 on epoch=14
06/24/2022 04:08:20 - INFO - __main__ - Step 40 Global step 40 Train loss 7.24 on epoch=19
06/24/2022 04:08:22 - INFO - __main__ - Step 50 Global step 50 Train loss 7.15 on epoch=24
06/24/2022 04:08:26 - INFO - __main__ - Global step 50 Train loss 7.22 ACC 0.0 on epoch=24
06/24/2022 04:08:26 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 04:08:28 - INFO - __main__ - Step 60 Global step 60 Train loss 7.11 on epoch=29
06/24/2022 04:08:29 - INFO - __main__ - Step 70 Global step 70 Train loss 7.14 on epoch=34
06/24/2022 04:08:31 - INFO - __main__ - Step 80 Global step 80 Train loss 7.20 on epoch=39
06/24/2022 04:08:32 - INFO - __main__ - Step 90 Global step 90 Train loss 7.08 on epoch=44
06/24/2022 04:08:34 - INFO - __main__ - Step 100 Global step 100 Train loss 7.02 on epoch=49
06/24/2022 04:08:37 - INFO - __main__ - Global step 100 Train loss 7.11 ACC 0.0 on epoch=49
06/24/2022 04:08:38 - INFO - __main__ - Step 110 Global step 110 Train loss 7.05 on epoch=54
06/24/2022 04:08:40 - INFO - __main__ - Step 120 Global step 120 Train loss 7.01 on epoch=59
06/24/2022 04:08:41 - INFO - __main__ - Step 130 Global step 130 Train loss 7.07 on epoch=64
06/24/2022 04:08:42 - INFO - __main__ - Step 140 Global step 140 Train loss 7.04 on epoch=69
06/24/2022 04:08:44 - INFO - __main__ - Step 150 Global step 150 Train loss 6.90 on epoch=74
06/24/2022 04:08:47 - INFO - __main__ - Global step 150 Train loss 7.01 ACC 0.0 on epoch=74
06/24/2022 04:08:48 - INFO - __main__ - Step 160 Global step 160 Train loss 6.94 on epoch=79
06/24/2022 04:08:49 - INFO - __main__ - Step 170 Global step 170 Train loss 6.87 on epoch=84
06/24/2022 04:08:51 - INFO - __main__ - Step 180 Global step 180 Train loss 6.75 on epoch=89
06/24/2022 04:08:52 - INFO - __main__ - Step 190 Global step 190 Train loss 6.62 on epoch=94
06/24/2022 04:08:54 - INFO - __main__ - Step 200 Global step 200 Train loss 6.51 on epoch=99
06/24/2022 04:08:57 - INFO - __main__ - Global step 200 Train loss 6.74 ACC 0.0 on epoch=99
06/24/2022 04:08:58 - INFO - __main__ - Step 210 Global step 210 Train loss 6.44 on epoch=104
06/24/2022 04:09:00 - INFO - __main__ - Step 220 Global step 220 Train loss 6.39 on epoch=109
06/24/2022 04:09:01 - INFO - __main__ - Step 230 Global step 230 Train loss 6.28 on epoch=114
06/24/2022 04:09:03 - INFO - __main__ - Step 240 Global step 240 Train loss 6.32 on epoch=119
06/24/2022 04:09:04 - INFO - __main__ - Step 250 Global step 250 Train loss 6.22 on epoch=124
06/24/2022 04:09:07 - INFO - __main__ - Global step 250 Train loss 6.33 ACC 0.0 on epoch=124
06/24/2022 04:09:09 - INFO - __main__ - Step 260 Global step 260 Train loss 6.14 on epoch=129
06/24/2022 04:09:10 - INFO - __main__ - Step 270 Global step 270 Train loss 6.10 on epoch=134
06/24/2022 04:09:12 - INFO - __main__ - Step 280 Global step 280 Train loss 6.24 on epoch=139
06/24/2022 04:09:13 - INFO - __main__ - Step 290 Global step 290 Train loss 6.03 on epoch=144
06/24/2022 04:09:14 - INFO - __main__ - Step 300 Global step 300 Train loss 6.08 on epoch=149
06/24/2022 04:09:23 - INFO - __main__ - Global step 300 Train loss 6.12 ACC 0.0 on epoch=149
06/24/2022 04:09:24 - INFO - __main__ - Step 310 Global step 310 Train loss 6.01 on epoch=154
06/24/2022 04:09:26 - INFO - __main__ - Step 320 Global step 320 Train loss 5.91 on epoch=159
06/24/2022 04:09:27 - INFO - __main__ - Step 330 Global step 330 Train loss 5.86 on epoch=164
06/24/2022 04:09:28 - INFO - __main__ - Step 340 Global step 340 Train loss 6.00 on epoch=169
06/24/2022 04:09:30 - INFO - __main__ - Step 350 Global step 350 Train loss 5.80 on epoch=174
06/24/2022 04:09:32 - INFO - __main__ - Global step 350 Train loss 5.91 ACC 0.0 on epoch=174
06/24/2022 04:09:34 - INFO - __main__ - Step 360 Global step 360 Train loss 5.85 on epoch=179
06/24/2022 04:09:35 - INFO - __main__ - Step 370 Global step 370 Train loss 5.77 on epoch=184
06/24/2022 04:09:36 - INFO - __main__ - Step 380 Global step 380 Train loss 5.73 on epoch=189
06/24/2022 04:09:38 - INFO - __main__ - Step 390 Global step 390 Train loss 5.66 on epoch=194
06/24/2022 04:09:39 - INFO - __main__ - Step 400 Global step 400 Train loss 5.68 on epoch=199
06/24/2022 04:09:47 - INFO - __main__ - Global step 400 Train loss 5.74 ACC 0.0 on epoch=199
06/24/2022 04:09:49 - INFO - __main__ - Step 410 Global step 410 Train loss 5.64 on epoch=204
06/24/2022 04:09:50 - INFO - __main__ - Step 420 Global step 420 Train loss 5.69 on epoch=209
06/24/2022 04:09:51 - INFO - __main__ - Step 430 Global step 430 Train loss 5.52 on epoch=214
06/24/2022 04:09:53 - INFO - __main__ - Step 440 Global step 440 Train loss 5.55 on epoch=219
06/24/2022 04:09:54 - INFO - __main__ - Step 450 Global step 450 Train loss 5.62 on epoch=224
06/24/2022 04:09:57 - INFO - __main__ - Global step 450 Train loss 5.60 ACC 0.0 on epoch=224
06/24/2022 04:09:58 - INFO - __main__ - Step 460 Global step 460 Train loss 5.49 on epoch=229
06/24/2022 04:09:59 - INFO - __main__ - Step 470 Global step 470 Train loss 5.43 on epoch=234
06/24/2022 04:10:01 - INFO - __main__ - Step 480 Global step 480 Train loss 5.28 on epoch=239
06/24/2022 04:10:02 - INFO - __main__ - Step 490 Global step 490 Train loss 5.24 on epoch=244
06/24/2022 04:10:04 - INFO - __main__ - Step 500 Global step 500 Train loss 5.24 on epoch=249
06/24/2022 04:10:14 - INFO - __main__ - Global step 500 Train loss 5.34 ACC 0.0 on epoch=249
06/24/2022 04:10:16 - INFO - __main__ - Step 510 Global step 510 Train loss 5.23 on epoch=254
06/24/2022 04:10:17 - INFO - __main__ - Step 520 Global step 520 Train loss 5.25 on epoch=259
06/24/2022 04:10:18 - INFO - __main__ - Step 530 Global step 530 Train loss 5.21 on epoch=264
06/24/2022 04:10:20 - INFO - __main__ - Step 540 Global step 540 Train loss 5.00 on epoch=269
06/24/2022 04:10:21 - INFO - __main__ - Step 550 Global step 550 Train loss 4.96 on epoch=274
06/24/2022 04:10:32 - INFO - __main__ - Global step 550 Train loss 5.13 ACC 0.0 on epoch=274
06/24/2022 04:10:33 - INFO - __main__ - Step 560 Global step 560 Train loss 4.92 on epoch=279
06/24/2022 04:10:34 - INFO - __main__ - Step 570 Global step 570 Train loss 4.76 on epoch=284
06/24/2022 04:10:36 - INFO - __main__ - Step 580 Global step 580 Train loss 4.71 on epoch=289
06/24/2022 04:10:37 - INFO - __main__ - Step 590 Global step 590 Train loss 4.68 on epoch=294
06/24/2022 04:10:38 - INFO - __main__ - Step 600 Global step 600 Train loss 4.60 on epoch=299
06/24/2022 04:10:47 - INFO - __main__ - Global step 600 Train loss 4.74 ACC 0.0 on epoch=299
06/24/2022 04:10:48 - INFO - __main__ - Step 610 Global step 610 Train loss 4.43 on epoch=304
06/24/2022 04:10:50 - INFO - __main__ - Step 620 Global step 620 Train loss 4.34 on epoch=309
06/24/2022 04:10:51 - INFO - __main__ - Step 630 Global step 630 Train loss 4.44 on epoch=314
06/24/2022 04:10:52 - INFO - __main__ - Step 640 Global step 640 Train loss 4.31 on epoch=319
06/24/2022 04:10:54 - INFO - __main__ - Step 650 Global step 650 Train loss 4.18 on epoch=324
06/24/2022 04:11:04 - INFO - __main__ - Global step 650 Train loss 4.34 ACC 0.0 on epoch=324
06/24/2022 04:11:05 - INFO - __main__ - Step 660 Global step 660 Train loss 4.07 on epoch=329
06/24/2022 04:11:07 - INFO - __main__ - Step 670 Global step 670 Train loss 4.02 on epoch=334
06/24/2022 04:11:08 - INFO - __main__ - Step 680 Global step 680 Train loss 4.07 on epoch=339
06/24/2022 04:11:10 - INFO - __main__ - Step 690 Global step 690 Train loss 4.09 on epoch=344
06/24/2022 04:11:11 - INFO - __main__ - Step 700 Global step 700 Train loss 3.91 on epoch=349
06/24/2022 04:11:21 - INFO - __main__ - Global step 700 Train loss 4.03 ACC 0.0 on epoch=349
06/24/2022 04:11:22 - INFO - __main__ - Step 710 Global step 710 Train loss 3.81 on epoch=354
06/24/2022 04:11:24 - INFO - __main__ - Step 720 Global step 720 Train loss 3.80 on epoch=359
06/24/2022 04:11:25 - INFO - __main__ - Step 730 Global step 730 Train loss 3.77 on epoch=364
06/24/2022 04:11:27 - INFO - __main__ - Step 740 Global step 740 Train loss 3.77 on epoch=369
06/24/2022 04:11:28 - INFO - __main__ - Step 750 Global step 750 Train loss 3.65 on epoch=374
06/24/2022 04:11:35 - INFO - __main__ - Global step 750 Train loss 3.76 ACC 0.0 on epoch=374
06/24/2022 04:11:36 - INFO - __main__ - Step 760 Global step 760 Train loss 3.47 on epoch=379
06/24/2022 04:11:38 - INFO - __main__ - Step 770 Global step 770 Train loss 3.37 on epoch=384
06/24/2022 04:11:39 - INFO - __main__ - Step 780 Global step 780 Train loss 3.49 on epoch=389
06/24/2022 04:11:41 - INFO - __main__ - Step 790 Global step 790 Train loss 3.22 on epoch=394
06/24/2022 04:11:42 - INFO - __main__ - Step 800 Global step 800 Train loss 3.25 on epoch=399
06/24/2022 04:11:45 - INFO - __main__ - Global step 800 Train loss 3.36 ACC 0.0 on epoch=399
06/24/2022 04:11:46 - INFO - __main__ - Step 810 Global step 810 Train loss 3.25 on epoch=404
06/24/2022 04:11:47 - INFO - __main__ - Step 820 Global step 820 Train loss 3.08 on epoch=409
06/24/2022 04:11:49 - INFO - __main__ - Step 830 Global step 830 Train loss 2.99 on epoch=414
06/24/2022 04:11:50 - INFO - __main__ - Step 840 Global step 840 Train loss 3.01 on epoch=419
06/24/2022 04:11:52 - INFO - __main__ - Step 850 Global step 850 Train loss 2.86 on epoch=424
06/24/2022 04:11:57 - INFO - __main__ - Global step 850 Train loss 3.04 ACC 0.4375 on epoch=424
06/24/2022 04:11:57 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.4375 on epoch=424, global_step=850
06/24/2022 04:11:59 - INFO - __main__ - Step 860 Global step 860 Train loss 2.87 on epoch=429
06/24/2022 04:12:00 - INFO - __main__ - Step 870 Global step 870 Train loss 2.73 on epoch=434
06/24/2022 04:12:02 - INFO - __main__ - Step 880 Global step 880 Train loss 2.72 on epoch=439
06/24/2022 04:12:03 - INFO - __main__ - Step 890 Global step 890 Train loss 2.60 on epoch=444
06/24/2022 04:12:05 - INFO - __main__ - Step 900 Global step 900 Train loss 2.65 on epoch=449
06/24/2022 04:12:08 - INFO - __main__ - Global step 900 Train loss 2.71 ACC 0.46875 on epoch=449
06/24/2022 04:12:08 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.46875 on epoch=449, global_step=900
06/24/2022 04:12:09 - INFO - __main__ - Step 910 Global step 910 Train loss 2.52 on epoch=454
06/24/2022 04:12:11 - INFO - __main__ - Step 920 Global step 920 Train loss 2.54 on epoch=459
06/24/2022 04:12:12 - INFO - __main__ - Step 930 Global step 930 Train loss 2.40 on epoch=464
06/24/2022 04:12:13 - INFO - __main__ - Step 940 Global step 940 Train loss 2.45 on epoch=469
06/24/2022 04:12:15 - INFO - __main__ - Step 950 Global step 950 Train loss 2.33 on epoch=474
06/24/2022 04:12:18 - INFO - __main__ - Global step 950 Train loss 2.45 ACC 0.5 on epoch=474
06/24/2022 04:12:18 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=474, global_step=950
06/24/2022 04:12:19 - INFO - __main__ - Step 960 Global step 960 Train loss 2.36 on epoch=479
06/24/2022 04:12:20 - INFO - __main__ - Step 970 Global step 970 Train loss 2.24 on epoch=484
06/24/2022 04:12:22 - INFO - __main__ - Step 980 Global step 980 Train loss 2.18 on epoch=489
06/24/2022 04:12:23 - INFO - __main__ - Step 990 Global step 990 Train loss 2.11 on epoch=494
06/24/2022 04:12:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.14 on epoch=499
06/24/2022 04:12:27 - INFO - __main__ - Global step 1000 Train loss 2.21 ACC 0.5 on epoch=499
06/24/2022 04:12:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.98 on epoch=504
06/24/2022 04:12:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.05 on epoch=509
06/24/2022 04:12:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.91 on epoch=514
06/24/2022 04:12:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.92 on epoch=519
06/24/2022 04:12:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.95 on epoch=524
06/24/2022 04:12:36 - INFO - __main__ - Global step 1050 Train loss 1.96 ACC 0.5 on epoch=524
06/24/2022 04:12:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.88 on epoch=529
06/24/2022 04:12:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.84 on epoch=534
06/24/2022 04:12:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.67 on epoch=539
06/24/2022 04:12:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.74 on epoch=544
06/24/2022 04:12:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.56 on epoch=549
06/24/2022 04:12:47 - INFO - __main__ - Global step 1100 Train loss 1.74 ACC 0.5 on epoch=549
06/24/2022 04:12:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.52 on epoch=554
06/24/2022 04:12:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.63 on epoch=559
06/24/2022 04:12:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.53 on epoch=564
06/24/2022 04:12:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.56 on epoch=569
06/24/2022 04:12:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.65 on epoch=574
06/24/2022 04:12:57 - INFO - __main__ - Global step 1150 Train loss 1.58 ACC 0.5 on epoch=574
06/24/2022 04:12:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.45 on epoch=579
06/24/2022 04:13:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.40 on epoch=584
06/24/2022 04:13:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.44 on epoch=589
06/24/2022 04:13:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.28 on epoch=594
06/24/2022 04:13:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.37 on epoch=599
06/24/2022 04:13:14 - INFO - __main__ - Global step 1200 Train loss 1.39 ACC 0.5 on epoch=599
06/24/2022 04:13:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.30 on epoch=604
06/24/2022 04:13:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.42 on epoch=609
06/24/2022 04:13:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.33 on epoch=614
06/24/2022 04:13:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.20 on epoch=619
06/24/2022 04:13:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.34 on epoch=624
06/24/2022 04:13:24 - INFO - __main__ - Global step 1250 Train loss 1.32 ACC 0.5 on epoch=624
06/24/2022 04:13:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.12 on epoch=629
06/24/2022 04:13:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.20 on epoch=634
06/24/2022 04:13:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.15 on epoch=639
06/24/2022 04:13:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.14 on epoch=644
06/24/2022 04:13:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.05 on epoch=649
06/24/2022 04:13:35 - INFO - __main__ - Global step 1300 Train loss 1.13 ACC 0.5 on epoch=649
06/24/2022 04:13:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.06 on epoch=654
06/24/2022 04:13:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.15 on epoch=659
06/24/2022 04:13:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.97 on epoch=664
06/24/2022 04:13:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.14 on epoch=669
06/24/2022 04:13:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.14 on epoch=674
06/24/2022 04:13:45 - INFO - __main__ - Global step 1350 Train loss 1.09 ACC 0.5 on epoch=674
06/24/2022 04:13:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.02 on epoch=679
06/24/2022 04:13:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.04 on epoch=684
06/24/2022 04:13:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.97 on epoch=689
06/24/2022 04:13:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.94 on epoch=694
06/24/2022 04:13:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.99 on epoch=699
06/24/2022 04:13:56 - INFO - __main__ - Global step 1400 Train loss 0.99 ACC 0.5 on epoch=699
06/24/2022 04:13:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.06 on epoch=704
06/24/2022 04:13:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.96 on epoch=709
06/24/2022 04:14:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.96 on epoch=714
06/24/2022 04:14:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.90 on epoch=719
06/24/2022 04:14:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.78 on epoch=724
06/24/2022 04:14:06 - INFO - __main__ - Global step 1450 Train loss 0.93 ACC 0.5 on epoch=724
06/24/2022 04:14:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.89 on epoch=729
06/24/2022 04:14:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.81 on epoch=734
06/24/2022 04:14:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.80 on epoch=739
06/24/2022 04:14:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.78 on epoch=744
06/24/2022 04:14:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.77 on epoch=749
06/24/2022 04:14:15 - INFO - __main__ - Global step 1500 Train loss 0.81 ACC 0.5 on epoch=749
06/24/2022 04:14:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.85 on epoch=754
06/24/2022 04:14:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.80 on epoch=759
06/24/2022 04:14:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.70 on epoch=764
06/24/2022 04:14:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.77 on epoch=769
06/24/2022 04:14:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.72 on epoch=774
06/24/2022 04:14:24 - INFO - __main__ - Global step 1550 Train loss 0.77 ACC 0.5 on epoch=774
06/24/2022 04:14:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.68 on epoch=779
06/24/2022 04:14:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.63 on epoch=784
06/24/2022 04:14:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.71 on epoch=789
06/24/2022 04:14:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.69 on epoch=794
06/24/2022 04:14:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.64 on epoch=799
06/24/2022 04:14:34 - INFO - __main__ - Global step 1600 Train loss 0.67 ACC 0.5 on epoch=799
06/24/2022 04:14:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.75 on epoch=804
06/24/2022 04:14:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.70 on epoch=809
06/24/2022 04:14:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.69 on epoch=814
06/24/2022 04:14:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.61 on epoch=819
06/24/2022 04:14:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.61 on epoch=824
06/24/2022 04:14:41 - INFO - __main__ - Global step 1650 Train loss 0.67 ACC 0.5 on epoch=824
06/24/2022 04:14:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.69 on epoch=829
06/24/2022 04:14:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.60 on epoch=834
06/24/2022 04:14:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.63 on epoch=839
06/24/2022 04:14:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.61 on epoch=844
06/24/2022 04:14:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.58 on epoch=849
06/24/2022 04:14:48 - INFO - __main__ - Global step 1700 Train loss 0.62 ACC 0.5 on epoch=849
06/24/2022 04:14:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.62 on epoch=854
06/24/2022 04:14:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.59 on epoch=859
06/24/2022 04:14:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.61 on epoch=864
06/24/2022 04:14:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.53 on epoch=869
06/24/2022 04:14:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.56 on epoch=874
06/24/2022 04:14:56 - INFO - __main__ - Global step 1750 Train loss 0.58 ACC 0.5 on epoch=874
06/24/2022 04:14:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.67 on epoch=879
06/24/2022 04:14:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.58 on epoch=884
06/24/2022 04:15:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=889
06/24/2022 04:15:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.56 on epoch=894
06/24/2022 04:15:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.59 on epoch=899
06/24/2022 04:15:03 - INFO - __main__ - Global step 1800 Train loss 0.57 ACC 0.5 on epoch=899
06/24/2022 04:15:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.62 on epoch=904
06/24/2022 04:15:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.54 on epoch=909
06/24/2022 04:15:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=914
06/24/2022 04:15:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.49 on epoch=919
06/24/2022 04:15:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.57 on epoch=924
06/24/2022 04:15:10 - INFO - __main__ - Global step 1850 Train loss 0.54 ACC 0.5 on epoch=924
06/24/2022 04:15:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.47 on epoch=929
06/24/2022 04:15:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.50 on epoch=934
06/24/2022 04:15:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.47 on epoch=939
06/24/2022 04:15:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.46 on epoch=944
06/24/2022 04:15:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=949
06/24/2022 04:15:17 - INFO - __main__ - Global step 1900 Train loss 0.46 ACC 0.5 on epoch=949
06/24/2022 04:15:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=954
06/24/2022 04:15:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.52 on epoch=959
06/24/2022 04:15:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.50 on epoch=964
06/24/2022 04:15:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.45 on epoch=969
06/24/2022 04:15:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.46 on epoch=974
06/24/2022 04:15:24 - INFO - __main__ - Global step 1950 Train loss 0.48 ACC 0.5 on epoch=974
06/24/2022 04:15:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=979
06/24/2022 04:15:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=984
06/24/2022 04:15:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=989
06/24/2022 04:15:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.52 on epoch=994
06/24/2022 04:15:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.45 on epoch=999
06/24/2022 04:15:31 - INFO - __main__ - Global step 2000 Train loss 0.45 ACC 0.5 on epoch=999
06/24/2022 04:15:31 - INFO - __main__ - save last model!
06/24/2022 04:15:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 04:15:32 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 04:15:32 - INFO - __main__ - Printing 3 examples
06/24/2022 04:15:32 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 04:15:32 - INFO - __main__ - ['not_duplicate']
06/24/2022 04:15:32 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 04:15:32 - INFO - __main__ - ['not_duplicate']
06/24/2022 04:15:32 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 04:15:32 - INFO - __main__ - ['duplicate']
06/24/2022 04:15:32 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:15:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:15:32 - INFO - __main__ - Printing 3 examples
06/24/2022 04:15:32 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 04:15:32 - INFO - __main__ - ['duplicate']
06/24/2022 04:15:32 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 04:15:32 - INFO - __main__ - ['duplicate']
06/24/2022 04:15:32 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 04:15:32 - INFO - __main__ - ['duplicate']
06/24/2022 04:15:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 04:15:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:15:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 04:15:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:15:32 - INFO - __main__ - Printing 3 examples
06/24/2022 04:15:32 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 04:15:32 - INFO - __main__ - ['duplicate']
06/24/2022 04:15:32 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 04:15:32 - INFO - __main__ - ['duplicate']
06/24/2022 04:15:32 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 04:15:32 - INFO - __main__ - ['duplicate']
06/24/2022 04:15:32 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:15:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:15:32 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 04:15:38 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 04:15:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 04:15:38 - INFO - __main__ - Starting training!
06/24/2022 04:15:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:16:32 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 04:28:22 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_42_0.3_8_predictions.txt
06/24/2022 04:28:22 - INFO - __main__ - ACC on test data: 0.3682
06/24/2022 04:28:22 - INFO - __main__ - prefix=glue-qqp_16_42, lr=0.3, bsz=8, dev_performance=0.5, test_performance=0.36816720257234725
06/24/2022 04:28:22 - INFO - __main__ - Running ... prefix=glue-qqp_16_42, lr=0.2, bsz=8 ...
06/24/2022 04:28:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:28:24 - INFO - __main__ - Printing 3 examples
06/24/2022 04:28:24 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 04:28:24 - INFO - __main__ - ['duplicate']
06/24/2022 04:28:24 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 04:28:24 - INFO - __main__ - ['duplicate']
06/24/2022 04:28:24 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 04:28:24 - INFO - __main__ - ['duplicate']
06/24/2022 04:28:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 04:28:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:28:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 04:28:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:28:24 - INFO - __main__ - Printing 3 examples
06/24/2022 04:28:24 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 04:28:24 - INFO - __main__ - ['duplicate']
06/24/2022 04:28:24 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 04:28:24 - INFO - __main__ - ['duplicate']
06/24/2022 04:28:24 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 04:28:24 - INFO - __main__ - ['duplicate']
06/24/2022 04:28:24 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:28:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:28:24 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 04:28:31 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 04:28:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 04:28:31 - INFO - __main__ - Starting training!
06/24/2022 04:28:33 - INFO - __main__ - Step 10 Global step 10 Train loss 7.16 on epoch=4
06/24/2022 04:28:34 - INFO - __main__ - Step 20 Global step 20 Train loss 7.16 on epoch=9
06/24/2022 04:28:36 - INFO - __main__ - Step 30 Global step 30 Train loss 7.12 on epoch=14
06/24/2022 04:28:37 - INFO - __main__ - Step 40 Global step 40 Train loss 7.23 on epoch=19
06/24/2022 04:28:39 - INFO - __main__ - Step 50 Global step 50 Train loss 7.10 on epoch=24
06/24/2022 04:28:43 - INFO - __main__ - Global step 50 Train loss 7.15 ACC 0.0 on epoch=24
06/24/2022 04:28:43 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 04:28:45 - INFO - __main__ - Step 60 Global step 60 Train loss 7.21 on epoch=29
06/24/2022 04:28:46 - INFO - __main__ - Step 70 Global step 70 Train loss 7.14 on epoch=34
06/24/2022 04:28:48 - INFO - __main__ - Step 80 Global step 80 Train loss 7.11 on epoch=39
06/24/2022 04:28:49 - INFO - __main__ - Step 90 Global step 90 Train loss 7.04 on epoch=44
06/24/2022 04:28:51 - INFO - __main__ - Step 100 Global step 100 Train loss 7.17 on epoch=49
06/24/2022 04:28:52 - INFO - __main__ - Global step 100 Train loss 7.14 ACC 0.0 on epoch=49
06/24/2022 04:28:53 - INFO - __main__ - Step 110 Global step 110 Train loss 7.04 on epoch=54
06/24/2022 04:28:55 - INFO - __main__ - Step 120 Global step 120 Train loss 7.06 on epoch=59
06/24/2022 04:28:56 - INFO - __main__ - Step 130 Global step 130 Train loss 7.16 on epoch=64
06/24/2022 04:28:58 - INFO - __main__ - Step 140 Global step 140 Train loss 7.12 on epoch=69
06/24/2022 04:28:59 - INFO - __main__ - Step 150 Global step 150 Train loss 7.09 on epoch=74
06/24/2022 04:29:01 - INFO - __main__ - Global step 150 Train loss 7.10 ACC 0.0 on epoch=74
06/24/2022 04:29:02 - INFO - __main__ - Step 160 Global step 160 Train loss 7.05 on epoch=79
06/24/2022 04:29:03 - INFO - __main__ - Step 170 Global step 170 Train loss 7.04 on epoch=84
06/24/2022 04:29:05 - INFO - __main__ - Step 180 Global step 180 Train loss 7.09 on epoch=89
06/24/2022 04:29:06 - INFO - __main__ - Step 190 Global step 190 Train loss 7.08 on epoch=94
06/24/2022 04:29:08 - INFO - __main__ - Step 200 Global step 200 Train loss 7.12 on epoch=99
06/24/2022 04:29:10 - INFO - __main__ - Global step 200 Train loss 7.07 ACC 0.0 on epoch=99
06/24/2022 04:29:11 - INFO - __main__ - Step 210 Global step 210 Train loss 6.99 on epoch=104
06/24/2022 04:29:13 - INFO - __main__ - Step 220 Global step 220 Train loss 6.93 on epoch=109
06/24/2022 04:29:14 - INFO - __main__ - Step 230 Global step 230 Train loss 6.98 on epoch=114
06/24/2022 04:29:15 - INFO - __main__ - Step 240 Global step 240 Train loss 6.92 on epoch=119
06/24/2022 04:29:17 - INFO - __main__ - Step 250 Global step 250 Train loss 6.98 on epoch=124
06/24/2022 04:29:20 - INFO - __main__ - Global step 250 Train loss 6.96 ACC 0.0 on epoch=124
06/24/2022 04:29:22 - INFO - __main__ - Step 260 Global step 260 Train loss 6.95 on epoch=129
06/24/2022 04:29:23 - INFO - __main__ - Step 270 Global step 270 Train loss 6.99 on epoch=134
06/24/2022 04:29:25 - INFO - __main__ - Step 280 Global step 280 Train loss 6.99 on epoch=139
06/24/2022 04:29:26 - INFO - __main__ - Step 290 Global step 290 Train loss 6.85 on epoch=144
06/24/2022 04:29:28 - INFO - __main__ - Step 300 Global step 300 Train loss 6.88 on epoch=149
06/24/2022 04:29:36 - INFO - __main__ - Global step 300 Train loss 6.93 ACC 0.0 on epoch=149
06/24/2022 04:29:37 - INFO - __main__ - Step 310 Global step 310 Train loss 6.81 on epoch=154
06/24/2022 04:29:39 - INFO - __main__ - Step 320 Global step 320 Train loss 6.86 on epoch=159
06/24/2022 04:29:40 - INFO - __main__ - Step 330 Global step 330 Train loss 6.88 on epoch=164
06/24/2022 04:29:42 - INFO - __main__ - Step 340 Global step 340 Train loss 6.88 on epoch=169
06/24/2022 04:29:43 - INFO - __main__ - Step 350 Global step 350 Train loss 6.83 on epoch=174
06/24/2022 04:29:46 - INFO - __main__ - Global step 350 Train loss 6.85 ACC 0.0 on epoch=174
06/24/2022 04:29:48 - INFO - __main__ - Step 360 Global step 360 Train loss 6.75 on epoch=179
06/24/2022 04:29:49 - INFO - __main__ - Step 370 Global step 370 Train loss 6.82 on epoch=184
06/24/2022 04:29:51 - INFO - __main__ - Step 380 Global step 380 Train loss 6.73 on epoch=189
06/24/2022 04:29:52 - INFO - __main__ - Step 390 Global step 390 Train loss 6.74 on epoch=194
06/24/2022 04:29:54 - INFO - __main__ - Step 400 Global step 400 Train loss 6.71 on epoch=199
06/24/2022 04:29:57 - INFO - __main__ - Global step 400 Train loss 6.75 ACC 0.0 on epoch=199
06/24/2022 04:29:58 - INFO - __main__ - Step 410 Global step 410 Train loss 6.57 on epoch=204
06/24/2022 04:29:59 - INFO - __main__ - Step 420 Global step 420 Train loss 6.68 on epoch=209
06/24/2022 04:30:01 - INFO - __main__ - Step 430 Global step 430 Train loss 6.67 on epoch=214
06/24/2022 04:30:02 - INFO - __main__ - Step 440 Global step 440 Train loss 6.52 on epoch=219
06/24/2022 04:30:04 - INFO - __main__ - Step 450 Global step 450 Train loss 6.39 on epoch=224
06/24/2022 04:30:06 - INFO - __main__ - Global step 450 Train loss 6.57 ACC 0.0 on epoch=224
06/24/2022 04:30:08 - INFO - __main__ - Step 460 Global step 460 Train loss 6.52 on epoch=229
06/24/2022 04:30:09 - INFO - __main__ - Step 470 Global step 470 Train loss 6.37 on epoch=234
06/24/2022 04:30:11 - INFO - __main__ - Step 480 Global step 480 Train loss 6.33 on epoch=239
06/24/2022 04:30:12 - INFO - __main__ - Step 490 Global step 490 Train loss 6.32 on epoch=244
06/24/2022 04:30:14 - INFO - __main__ - Step 500 Global step 500 Train loss 6.21 on epoch=249
06/24/2022 04:30:21 - INFO - __main__ - Global step 500 Train loss 6.35 ACC 0.0 on epoch=249
06/24/2022 04:30:22 - INFO - __main__ - Step 510 Global step 510 Train loss 6.26 on epoch=254
06/24/2022 04:30:24 - INFO - __main__ - Step 520 Global step 520 Train loss 6.17 on epoch=259
06/24/2022 04:30:25 - INFO - __main__ - Step 530 Global step 530 Train loss 6.17 on epoch=264
06/24/2022 04:30:27 - INFO - __main__ - Step 540 Global step 540 Train loss 6.07 on epoch=269
06/24/2022 04:30:28 - INFO - __main__ - Step 550 Global step 550 Train loss 6.05 on epoch=274
06/24/2022 04:30:31 - INFO - __main__ - Global step 550 Train loss 6.14 ACC 0.0 on epoch=274
06/24/2022 04:30:32 - INFO - __main__ - Step 560 Global step 560 Train loss 5.94 on epoch=279
06/24/2022 04:30:34 - INFO - __main__ - Step 570 Global step 570 Train loss 5.99 on epoch=284
06/24/2022 04:30:35 - INFO - __main__ - Step 580 Global step 580 Train loss 5.93 on epoch=289
06/24/2022 04:30:36 - INFO - __main__ - Step 590 Global step 590 Train loss 5.80 on epoch=294
06/24/2022 04:30:38 - INFO - __main__ - Step 600 Global step 600 Train loss 5.90 on epoch=299
06/24/2022 04:30:41 - INFO - __main__ - Global step 600 Train loss 5.91 ACC 0.0 on epoch=299
06/24/2022 04:30:42 - INFO - __main__ - Step 610 Global step 610 Train loss 5.85 on epoch=304
06/24/2022 04:30:44 - INFO - __main__ - Step 620 Global step 620 Train loss 5.79 on epoch=309
06/24/2022 04:30:45 - INFO - __main__ - Step 630 Global step 630 Train loss 5.76 on epoch=314
06/24/2022 04:30:47 - INFO - __main__ - Step 640 Global step 640 Train loss 5.76 on epoch=319
06/24/2022 04:30:48 - INFO - __main__ - Step 650 Global step 650 Train loss 5.62 on epoch=324
06/24/2022 04:30:56 - INFO - __main__ - Global step 650 Train loss 5.75 ACC 0.0 on epoch=324
06/24/2022 04:30:57 - INFO - __main__ - Step 660 Global step 660 Train loss 5.66 on epoch=329
06/24/2022 04:30:59 - INFO - __main__ - Step 670 Global step 670 Train loss 5.49 on epoch=334
06/24/2022 04:31:01 - INFO - __main__ - Step 680 Global step 680 Train loss 5.47 on epoch=339
06/24/2022 04:31:02 - INFO - __main__ - Step 690 Global step 690 Train loss 5.39 on epoch=344
06/24/2022 04:31:04 - INFO - __main__ - Step 700 Global step 700 Train loss 5.55 on epoch=349
06/24/2022 04:31:12 - INFO - __main__ - Global step 700 Train loss 5.51 ACC 0.0 on epoch=349
06/24/2022 04:31:14 - INFO - __main__ - Step 710 Global step 710 Train loss 5.47 on epoch=354
06/24/2022 04:31:15 - INFO - __main__ - Step 720 Global step 720 Train loss 5.37 on epoch=359
06/24/2022 04:31:16 - INFO - __main__ - Step 730 Global step 730 Train loss 5.30 on epoch=364
06/24/2022 04:31:18 - INFO - __main__ - Step 740 Global step 740 Train loss 5.30 on epoch=369
06/24/2022 04:31:19 - INFO - __main__ - Step 750 Global step 750 Train loss 5.33 on epoch=374
06/24/2022 04:31:27 - INFO - __main__ - Global step 750 Train loss 5.36 ACC 0.0 on epoch=374
06/24/2022 04:31:28 - INFO - __main__ - Step 760 Global step 760 Train loss 5.16 on epoch=379
06/24/2022 04:31:29 - INFO - __main__ - Step 770 Global step 770 Train loss 5.17 on epoch=384
06/24/2022 04:31:31 - INFO - __main__ - Step 780 Global step 780 Train loss 5.12 on epoch=389
06/24/2022 04:31:32 - INFO - __main__ - Step 790 Global step 790 Train loss 5.02 on epoch=394
06/24/2022 04:31:34 - INFO - __main__ - Step 800 Global step 800 Train loss 5.15 on epoch=399
06/24/2022 04:31:44 - INFO - __main__ - Global step 800 Train loss 5.12 ACC 0.0 on epoch=399
06/24/2022 04:31:45 - INFO - __main__ - Step 810 Global step 810 Train loss 5.11 on epoch=404
06/24/2022 04:31:47 - INFO - __main__ - Step 820 Global step 820 Train loss 4.92 on epoch=409
06/24/2022 04:31:48 - INFO - __main__ - Step 830 Global step 830 Train loss 4.85 on epoch=414
06/24/2022 04:31:50 - INFO - __main__ - Step 840 Global step 840 Train loss 4.85 on epoch=419
06/24/2022 04:31:51 - INFO - __main__ - Step 850 Global step 850 Train loss 4.75 on epoch=424
06/24/2022 04:32:03 - INFO - __main__ - Global step 850 Train loss 4.90 ACC 0.0 on epoch=424
06/24/2022 04:32:04 - INFO - __main__ - Step 860 Global step 860 Train loss 4.69 on epoch=429
06/24/2022 04:32:05 - INFO - __main__ - Step 870 Global step 870 Train loss 4.62 on epoch=434
06/24/2022 04:32:07 - INFO - __main__ - Step 880 Global step 880 Train loss 4.58 on epoch=439
06/24/2022 04:32:08 - INFO - __main__ - Step 890 Global step 890 Train loss 4.53 on epoch=444
06/24/2022 04:32:10 - INFO - __main__ - Step 900 Global step 900 Train loss 4.38 on epoch=449
06/24/2022 04:32:12 - INFO - __main__ - Global step 900 Train loss 4.56 ACC 0.0 on epoch=449
06/24/2022 04:32:13 - INFO - __main__ - Step 910 Global step 910 Train loss 4.38 on epoch=454
06/24/2022 04:32:14 - INFO - __main__ - Step 920 Global step 920 Train loss 4.38 on epoch=459
06/24/2022 04:32:16 - INFO - __main__ - Step 930 Global step 930 Train loss 4.40 on epoch=464
06/24/2022 04:32:18 - INFO - __main__ - Step 940 Global step 940 Train loss 4.45 on epoch=469
06/24/2022 04:32:19 - INFO - __main__ - Step 950 Global step 950 Train loss 4.24 on epoch=474
06/24/2022 04:32:27 - INFO - __main__ - Global step 950 Train loss 4.37 ACC 0.0 on epoch=474
06/24/2022 04:32:28 - INFO - __main__ - Step 960 Global step 960 Train loss 4.26 on epoch=479
06/24/2022 04:32:30 - INFO - __main__ - Step 970 Global step 970 Train loss 4.22 on epoch=484
06/24/2022 04:32:31 - INFO - __main__ - Step 980 Global step 980 Train loss 4.31 on epoch=489
06/24/2022 04:32:33 - INFO - __main__ - Step 990 Global step 990 Train loss 4.21 on epoch=494
06/24/2022 04:32:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 4.20 on epoch=499
06/24/2022 04:32:37 - INFO - __main__ - Global step 1000 Train loss 4.24 ACC 0.0 on epoch=499
06/24/2022 04:32:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 4.02 on epoch=504
06/24/2022 04:32:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 4.14 on epoch=509
06/24/2022 04:32:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.96 on epoch=514
06/24/2022 04:32:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.92 on epoch=519
06/24/2022 04:32:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.98 on epoch=524
06/24/2022 04:32:52 - INFO - __main__ - Global step 1050 Train loss 4.00 ACC 0.0 on epoch=524
06/24/2022 04:32:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.94 on epoch=529
06/24/2022 04:32:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.89 on epoch=534
06/24/2022 04:32:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.94 on epoch=539
06/24/2022 04:32:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.86 on epoch=544
06/24/2022 04:32:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.86 on epoch=549
06/24/2022 04:33:08 - INFO - __main__ - Global step 1100 Train loss 3.90 ACC 0.09375 on epoch=549
06/24/2022 04:33:08 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=549, global_step=1100
06/24/2022 04:33:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.92 on epoch=554
06/24/2022 04:33:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.72 on epoch=559
06/24/2022 04:33:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.74 on epoch=564
06/24/2022 04:33:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.87 on epoch=569
06/24/2022 04:33:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.68 on epoch=574
06/24/2022 04:33:18 - INFO - __main__ - Global step 1150 Train loss 3.79 ACC 0.03125 on epoch=574
06/24/2022 04:33:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.71 on epoch=579
06/24/2022 04:33:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.69 on epoch=584
06/24/2022 04:33:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.72 on epoch=589
06/24/2022 04:33:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.66 on epoch=594
06/24/2022 04:33:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.73 on epoch=599
06/24/2022 04:33:28 - INFO - __main__ - Global step 1200 Train loss 3.70 ACC 0.09375 on epoch=599
06/24/2022 04:33:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.46 on epoch=604
06/24/2022 04:33:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.71 on epoch=609
06/24/2022 04:33:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.43 on epoch=614
06/24/2022 04:33:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.60 on epoch=619
06/24/2022 04:33:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.53 on epoch=624
06/24/2022 04:33:38 - INFO - __main__ - Global step 1250 Train loss 3.55 ACC 0.09375 on epoch=624
06/24/2022 04:33:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.49 on epoch=629
06/24/2022 04:33:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.40 on epoch=634
06/24/2022 04:33:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.46 on epoch=639
06/24/2022 04:33:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.35 on epoch=644
06/24/2022 04:33:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.29 on epoch=649
06/24/2022 04:33:48 - INFO - __main__ - Global step 1300 Train loss 3.40 ACC 0.25 on epoch=649
06/24/2022 04:33:48 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.25 on epoch=649, global_step=1300
06/24/2022 04:33:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.34 on epoch=654
06/24/2022 04:33:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.31 on epoch=659
06/24/2022 04:33:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 3.36 on epoch=664
06/24/2022 04:33:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 3.25 on epoch=669
06/24/2022 04:33:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 3.10 on epoch=674
06/24/2022 04:33:58 - INFO - __main__ - Global step 1350 Train loss 3.27 ACC 0.46875 on epoch=674
06/24/2022 04:33:58 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.46875 on epoch=674, global_step=1350
06/24/2022 04:33:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 3.26 on epoch=679
06/24/2022 04:34:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 3.21 on epoch=684
06/24/2022 04:34:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 3.24 on epoch=689
06/24/2022 04:34:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 3.20 on epoch=694
06/24/2022 04:34:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 3.20 on epoch=699
06/24/2022 04:34:09 - INFO - __main__ - Global step 1400 Train loss 3.22 ACC 0.28125 on epoch=699
06/24/2022 04:34:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 3.13 on epoch=704
06/24/2022 04:34:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 3.06 on epoch=709
06/24/2022 04:34:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 3.10 on epoch=714
06/24/2022 04:34:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.97 on epoch=719
06/24/2022 04:34:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.88 on epoch=724
06/24/2022 04:34:19 - INFO - __main__ - Global step 1450 Train loss 3.03 ACC 0.4375 on epoch=724
06/24/2022 04:34:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.95 on epoch=729
06/24/2022 04:34:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.88 on epoch=734
06/24/2022 04:34:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.86 on epoch=739
06/24/2022 04:34:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 3.00 on epoch=744
06/24/2022 04:34:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.75 on epoch=749
06/24/2022 04:34:29 - INFO - __main__ - Global step 1500 Train loss 2.89 ACC 0.28125 on epoch=749
06/24/2022 04:34:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.65 on epoch=754
06/24/2022 04:34:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.66 on epoch=759
06/24/2022 04:34:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 2.80 on epoch=764
06/24/2022 04:34:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 2.78 on epoch=769
06/24/2022 04:34:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 2.56 on epoch=774
06/24/2022 04:34:38 - INFO - __main__ - Global step 1550 Train loss 2.69 ACC 0.5 on epoch=774
06/24/2022 04:34:38 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=774, global_step=1550
06/24/2022 04:34:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 2.62 on epoch=779
06/24/2022 04:34:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 2.57 on epoch=784
06/24/2022 04:34:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 2.59 on epoch=789
06/24/2022 04:34:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 2.55 on epoch=794
06/24/2022 04:34:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 2.48 on epoch=799
06/24/2022 04:34:48 - INFO - __main__ - Global step 1600 Train loss 2.56 ACC 0.5 on epoch=799
06/24/2022 04:34:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 2.48 on epoch=804
06/24/2022 04:34:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.39 on epoch=809
06/24/2022 04:34:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 2.41 on epoch=814
06/24/2022 04:34:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 2.29 on epoch=819
06/24/2022 04:34:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 2.38 on epoch=824
06/24/2022 04:34:57 - INFO - __main__ - Global step 1650 Train loss 2.39 ACC 0.5 on epoch=824
06/24/2022 04:34:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 2.23 on epoch=829
06/24/2022 04:35:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 2.23 on epoch=834
06/24/2022 04:35:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 2.15 on epoch=839
06/24/2022 04:35:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 2.10 on epoch=844
06/24/2022 04:35:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 2.03 on epoch=849
06/24/2022 04:35:07 - INFO - __main__ - Global step 1700 Train loss 2.15 ACC 0.5 on epoch=849
06/24/2022 04:35:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 2.00 on epoch=854
06/24/2022 04:35:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.93 on epoch=859
06/24/2022 04:35:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 2.07 on epoch=864
06/24/2022 04:35:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.98 on epoch=869
06/24/2022 04:35:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.92 on epoch=874
06/24/2022 04:35:17 - INFO - __main__ - Global step 1750 Train loss 1.98 ACC 0.5 on epoch=874
06/24/2022 04:35:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.78 on epoch=879
06/24/2022 04:35:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.81 on epoch=884
06/24/2022 04:35:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.75 on epoch=889
06/24/2022 04:35:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.77 on epoch=894
06/24/2022 04:35:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.76 on epoch=899
06/24/2022 04:35:27 - INFO - __main__ - Global step 1800 Train loss 1.77 ACC 0.5 on epoch=899
06/24/2022 04:35:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.69 on epoch=904
06/24/2022 04:35:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.73 on epoch=909
06/24/2022 04:35:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.71 on epoch=914
06/24/2022 04:35:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.57 on epoch=919
06/24/2022 04:35:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.54 on epoch=924
06/24/2022 04:35:36 - INFO - __main__ - Global step 1850 Train loss 1.65 ACC 0.5 on epoch=924
06/24/2022 04:35:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.55 on epoch=929
06/24/2022 04:35:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.56 on epoch=934
06/24/2022 04:35:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.61 on epoch=939
06/24/2022 04:35:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.40 on epoch=944
06/24/2022 04:35:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.51 on epoch=949
06/24/2022 04:35:44 - INFO - __main__ - Global step 1900 Train loss 1.52 ACC 0.5 on epoch=949
06/24/2022 04:35:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.37 on epoch=954
06/24/2022 04:35:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.28 on epoch=959
06/24/2022 04:35:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.42 on epoch=964
06/24/2022 04:35:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.37 on epoch=969
06/24/2022 04:35:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.46 on epoch=974
06/24/2022 04:35:54 - INFO - __main__ - Global step 1950 Train loss 1.38 ACC 0.5 on epoch=974
06/24/2022 04:35:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.37 on epoch=979
06/24/2022 04:35:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.23 on epoch=984
06/24/2022 04:35:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.23 on epoch=989
06/24/2022 04:35:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.39 on epoch=994
06/24/2022 04:36:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.31 on epoch=999
06/24/2022 04:36:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:36:02 - INFO - __main__ - Printing 3 examples
06/24/2022 04:36:02 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 04:36:02 - INFO - __main__ - ['duplicate']
06/24/2022 04:36:02 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 04:36:02 - INFO - __main__ - ['duplicate']
06/24/2022 04:36:02 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 04:36:02 - INFO - __main__ - ['duplicate']
06/24/2022 04:36:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 04:36:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:36:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 04:36:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 04:36:02 - INFO - __main__ - Printing 3 examples
06/24/2022 04:36:02 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 04:36:02 - INFO - __main__ - ['duplicate']
06/24/2022 04:36:02 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 04:36:02 - INFO - __main__ - ['duplicate']
06/24/2022 04:36:02 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 04:36:02 - INFO - __main__ - ['duplicate']
06/24/2022 04:36:02 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:36:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:36:02 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 04:36:03 - INFO - __main__ - Global step 2000 Train loss 1.31 ACC 0.5 on epoch=999
06/24/2022 04:36:03 - INFO - __main__ - save last model!
06/24/2022 04:36:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 04:36:03 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 04:36:03 - INFO - __main__ - Printing 3 examples
06/24/2022 04:36:03 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 04:36:03 - INFO - __main__ - ['not_duplicate']
06/24/2022 04:36:03 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 04:36:03 - INFO - __main__ - ['not_duplicate']
06/24/2022 04:36:03 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 04:36:03 - INFO - __main__ - ['duplicate']
06/24/2022 04:36:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 04:36:09 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 04:36:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 04:36:09 - INFO - __main__ - Starting training!
06/24/2022 04:36:21 - INFO - __main__ - Tokenizing Output ...
06/24/2022 04:37:03 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 05:31:18 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_42_0.2_8_predictions.txt
06/24/2022 05:31:18 - INFO - __main__ - ACC on test data: 0.3682
06/24/2022 05:31:18 - INFO - __main__ - prefix=glue-qqp_16_42, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.36816720257234725
06/24/2022 05:31:18 - INFO - __main__ - Running ... prefix=glue-qqp_16_87, lr=0.5, bsz=8 ...
06/24/2022 05:31:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:31:19 - INFO - __main__ - Printing 3 examples
06/24/2022 05:31:19 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 05:31:19 - INFO - __main__ - ['duplicate']
06/24/2022 05:31:19 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 05:31:19 - INFO - __main__ - ['duplicate']
06/24/2022 05:31:19 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 05:31:19 - INFO - __main__ - ['duplicate']
06/24/2022 05:31:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 05:31:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:31:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 05:31:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:31:19 - INFO - __main__ - Printing 3 examples
06/24/2022 05:31:19 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 05:31:19 - INFO - __main__ - ['duplicate']
06/24/2022 05:31:19 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 05:31:19 - INFO - __main__ - ['duplicate']
06/24/2022 05:31:19 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 05:31:19 - INFO - __main__ - ['duplicate']
06/24/2022 05:31:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:31:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:31:19 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:31:25 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:31:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:31:26 - INFO - __main__ - Starting training!
06/24/2022 05:31:27 - INFO - __main__ - Step 10 Global step 10 Train loss 7.17 on epoch=4
06/24/2022 05:31:29 - INFO - __main__ - Step 20 Global step 20 Train loss 7.23 on epoch=9
06/24/2022 05:31:30 - INFO - __main__ - Step 30 Global step 30 Train loss 7.10 on epoch=14
06/24/2022 05:31:31 - INFO - __main__ - Step 40 Global step 40 Train loss 7.07 on epoch=19
06/24/2022 05:31:33 - INFO - __main__ - Step 50 Global step 50 Train loss 7.16 on epoch=24
06/24/2022 05:31:36 - INFO - __main__ - Global step 50 Train loss 7.14 ACC 0.0 on epoch=24
06/24/2022 05:31:36 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 05:31:37 - INFO - __main__ - Step 60 Global step 60 Train loss 7.12 on epoch=29
06/24/2022 05:31:39 - INFO - __main__ - Step 70 Global step 70 Train loss 7.00 on epoch=34
06/24/2022 05:31:40 - INFO - __main__ - Step 80 Global step 80 Train loss 6.93 on epoch=39
06/24/2022 05:31:41 - INFO - __main__ - Step 90 Global step 90 Train loss 6.81 on epoch=44
06/24/2022 05:31:43 - INFO - __main__ - Step 100 Global step 100 Train loss 6.70 on epoch=49
06/24/2022 05:31:49 - INFO - __main__ - Global step 100 Train loss 6.91 ACC 0.0 on epoch=49
06/24/2022 05:31:50 - INFO - __main__ - Step 110 Global step 110 Train loss 6.62 on epoch=54
06/24/2022 05:31:51 - INFO - __main__ - Step 120 Global step 120 Train loss 6.51 on epoch=59
06/24/2022 05:31:53 - INFO - __main__ - Step 130 Global step 130 Train loss 6.22 on epoch=64
06/24/2022 05:31:54 - INFO - __main__ - Step 140 Global step 140 Train loss 6.03 on epoch=69
06/24/2022 05:31:55 - INFO - __main__ - Step 150 Global step 150 Train loss 5.88 on epoch=74
06/24/2022 05:32:05 - INFO - __main__ - Global step 150 Train loss 6.25 ACC 0.0 on epoch=74
06/24/2022 05:32:07 - INFO - __main__ - Step 160 Global step 160 Train loss 5.57 on epoch=79
06/24/2022 05:32:08 - INFO - __main__ - Step 170 Global step 170 Train loss 5.40 on epoch=84
06/24/2022 05:32:09 - INFO - __main__ - Step 180 Global step 180 Train loss 5.44 on epoch=89
06/24/2022 05:32:11 - INFO - __main__ - Step 190 Global step 190 Train loss 5.29 on epoch=94
06/24/2022 05:32:12 - INFO - __main__ - Step 200 Global step 200 Train loss 5.07 on epoch=99
06/24/2022 05:32:19 - INFO - __main__ - Global step 200 Train loss 5.35 ACC 0.0 on epoch=99
06/24/2022 05:32:20 - INFO - __main__ - Step 210 Global step 210 Train loss 4.90 on epoch=104
06/24/2022 05:32:21 - INFO - __main__ - Step 220 Global step 220 Train loss 4.77 on epoch=109
06/24/2022 05:32:23 - INFO - __main__ - Step 230 Global step 230 Train loss 4.66 on epoch=114
06/24/2022 05:32:24 - INFO - __main__ - Step 240 Global step 240 Train loss 4.58 on epoch=119
06/24/2022 05:32:25 - INFO - __main__ - Step 250 Global step 250 Train loss 4.53 on epoch=124
06/24/2022 05:32:30 - INFO - __main__ - Global step 250 Train loss 4.69 ACC 0.0 on epoch=124
06/24/2022 05:32:31 - INFO - __main__ - Step 260 Global step 260 Train loss 4.43 on epoch=129
06/24/2022 05:32:33 - INFO - __main__ - Step 270 Global step 270 Train loss 4.13 on epoch=134
06/24/2022 05:32:34 - INFO - __main__ - Step 280 Global step 280 Train loss 3.98 on epoch=139
06/24/2022 05:32:35 - INFO - __main__ - Step 290 Global step 290 Train loss 3.77 on epoch=144
06/24/2022 05:32:37 - INFO - __main__ - Step 300 Global step 300 Train loss 3.52 on epoch=149
06/24/2022 05:32:40 - INFO - __main__ - Global step 300 Train loss 3.97 ACC 0.0 on epoch=149
06/24/2022 05:32:41 - INFO - __main__ - Step 310 Global step 310 Train loss 3.25 on epoch=154
06/24/2022 05:32:42 - INFO - __main__ - Step 320 Global step 320 Train loss 3.09 on epoch=159
06/24/2022 05:32:44 - INFO - __main__ - Step 330 Global step 330 Train loss 2.75 on epoch=164
06/24/2022 05:32:45 - INFO - __main__ - Step 340 Global step 340 Train loss 2.57 on epoch=169
06/24/2022 05:32:46 - INFO - __main__ - Step 350 Global step 350 Train loss 2.66 on epoch=174
06/24/2022 05:32:52 - INFO - __main__ - Global step 350 Train loss 2.86 ACC 0.5 on epoch=174
06/24/2022 05:32:52 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.5 on epoch=174, global_step=350
06/24/2022 05:32:54 - INFO - __main__ - Step 360 Global step 360 Train loss 2.30 on epoch=179
06/24/2022 05:32:55 - INFO - __main__ - Step 370 Global step 370 Train loss 2.11 on epoch=184
06/24/2022 05:32:56 - INFO - __main__ - Step 380 Global step 380 Train loss 1.91 on epoch=189
06/24/2022 05:32:58 - INFO - __main__ - Step 390 Global step 390 Train loss 1.81 on epoch=194
06/24/2022 05:32:59 - INFO - __main__ - Step 400 Global step 400 Train loss 1.67 on epoch=199
06/24/2022 05:33:02 - INFO - __main__ - Global step 400 Train loss 1.96 ACC 0.5 on epoch=199
06/24/2022 05:33:03 - INFO - __main__ - Step 410 Global step 410 Train loss 1.62 on epoch=204
06/24/2022 05:33:05 - INFO - __main__ - Step 420 Global step 420 Train loss 1.35 on epoch=209
06/24/2022 05:33:06 - INFO - __main__ - Step 430 Global step 430 Train loss 1.34 on epoch=214
06/24/2022 05:33:07 - INFO - __main__ - Step 440 Global step 440 Train loss 1.41 on epoch=219
06/24/2022 05:33:09 - INFO - __main__ - Step 450 Global step 450 Train loss 1.24 on epoch=224
06/24/2022 05:33:12 - INFO - __main__ - Global step 450 Train loss 1.39 ACC 0.5 on epoch=224
06/24/2022 05:33:13 - INFO - __main__ - Step 460 Global step 460 Train loss 1.13 on epoch=229
06/24/2022 05:33:14 - INFO - __main__ - Step 470 Global step 470 Train loss 1.05 on epoch=234
06/24/2022 05:33:16 - INFO - __main__ - Step 480 Global step 480 Train loss 1.13 on epoch=239
06/24/2022 05:33:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.95 on epoch=244
06/24/2022 05:33:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.90 on epoch=249
06/24/2022 05:33:21 - INFO - __main__ - Global step 500 Train loss 1.03 ACC 0.5 on epoch=249
06/24/2022 05:33:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.91 on epoch=254
06/24/2022 05:33:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.76 on epoch=259
06/24/2022 05:33:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.71 on epoch=264
06/24/2022 05:33:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.83 on epoch=269
06/24/2022 05:33:27 - INFO - __main__ - Step 550 Global step 550 Train loss 1.06 on epoch=274
06/24/2022 05:33:30 - INFO - __main__ - Global step 550 Train loss 0.85 ACC 0.5 on epoch=274
06/24/2022 05:33:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.72 on epoch=279
06/24/2022 05:33:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.82 on epoch=284
06/24/2022 05:33:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.69 on epoch=289
06/24/2022 05:33:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.73 on epoch=294
06/24/2022 05:33:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.63 on epoch=299
06/24/2022 05:33:41 - INFO - __main__ - Global step 600 Train loss 0.72 ACC 0.5 on epoch=299
06/24/2022 05:33:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.59 on epoch=304
06/24/2022 05:33:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.60 on epoch=309
06/24/2022 05:33:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.52 on epoch=314
06/24/2022 05:33:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=319
06/24/2022 05:33:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.58 on epoch=324
06/24/2022 05:33:50 - INFO - __main__ - Global step 650 Train loss 0.57 ACC 0.5 on epoch=324
06/24/2022 05:33:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.53 on epoch=329
06/24/2022 05:33:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=334
06/24/2022 05:33:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.55 on epoch=339
06/24/2022 05:33:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.57 on epoch=344
06/24/2022 05:33:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.61 on epoch=349
06/24/2022 05:33:59 - INFO - __main__ - Global step 700 Train loss 0.55 ACC 0.5 on epoch=349
06/24/2022 05:34:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=354
06/24/2022 05:34:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=359
06/24/2022 05:34:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=364
06/24/2022 05:34:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=369
06/24/2022 05:34:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=374
06/24/2022 05:34:08 - INFO - __main__ - Global step 750 Train loss 0.47 ACC 0.5 on epoch=374
06/24/2022 05:34:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=379
06/24/2022 05:34:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.49 on epoch=384
06/24/2022 05:34:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=389
06/24/2022 05:34:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=394
06/24/2022 05:34:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=399
06/24/2022 05:34:17 - INFO - __main__ - Global step 800 Train loss 0.46 ACC 0.5 on epoch=399
06/24/2022 05:34:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=404
06/24/2022 05:34:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=409
06/24/2022 05:34:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=414
06/24/2022 05:34:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=419
06/24/2022 05:34:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=424
06/24/2022 05:34:26 - INFO - __main__ - Global step 850 Train loss 0.43 ACC 0.5 on epoch=424
06/24/2022 05:34:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=429
06/24/2022 05:34:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=434
06/24/2022 05:34:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=439
06/24/2022 05:34:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=444
06/24/2022 05:34:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=449
06/24/2022 05:34:35 - INFO - __main__ - Global step 900 Train loss 0.42 ACC 0.5 on epoch=449
06/24/2022 05:34:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=454
06/24/2022 05:34:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=459
06/24/2022 05:34:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=464
06/24/2022 05:34:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=469
06/24/2022 05:34:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=474
06/24/2022 05:34:43 - INFO - __main__ - Global step 950 Train loss 0.44 ACC 0.5 on epoch=474
06/24/2022 05:34:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.34 on epoch=479
06/24/2022 05:34:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.53 on epoch=484
06/24/2022 05:34:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=489
06/24/2022 05:34:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=494
06/24/2022 05:34:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=499
06/24/2022 05:34:52 - INFO - __main__ - Global step 1000 Train loss 0.43 ACC 0.5 on epoch=499
06/24/2022 05:34:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=504
06/24/2022 05:34:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=509
06/24/2022 05:34:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=514
06/24/2022 05:34:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=519
06/24/2022 05:34:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=524
06/24/2022 05:35:02 - INFO - __main__ - Global step 1050 Train loss 0.42 ACC 0.5 on epoch=524
06/24/2022 05:35:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=529
06/24/2022 05:35:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=534
06/24/2022 05:35:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=539
06/24/2022 05:35:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=544
06/24/2022 05:35:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=549
06/24/2022 05:35:10 - INFO - __main__ - Global step 1100 Train loss 0.39 ACC 0.5 on epoch=549
06/24/2022 05:35:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=554
06/24/2022 05:35:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=559
06/24/2022 05:35:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=564
06/24/2022 05:35:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=569
06/24/2022 05:35:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=574
06/24/2022 05:35:19 - INFO - __main__ - Global step 1150 Train loss 0.35 ACC 0.5 on epoch=574
06/24/2022 05:35:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=579
06/24/2022 05:35:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=584
06/24/2022 05:35:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=589
06/24/2022 05:35:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=594
06/24/2022 05:35:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=599
06/24/2022 05:35:27 - INFO - __main__ - Global step 1200 Train loss 0.37 ACC 0.53125 on epoch=599
06/24/2022 05:35:27 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=599, global_step=1200
06/24/2022 05:35:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=604
06/24/2022 05:35:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=609
06/24/2022 05:35:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=614
06/24/2022 05:35:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=619
06/24/2022 05:35:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.34 on epoch=624
06/24/2022 05:35:33 - INFO - __main__ - Global step 1250 Train loss 0.33 ACC 0.53125 on epoch=624
06/24/2022 05:35:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=629
06/24/2022 05:35:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=634
06/24/2022 05:35:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=639
06/24/2022 05:35:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=644
06/24/2022 05:35:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=649
06/24/2022 05:35:41 - INFO - __main__ - Global step 1300 Train loss 0.36 ACC 0.5 on epoch=649
06/24/2022 05:35:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=654
06/24/2022 05:35:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=659
06/24/2022 05:35:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
06/24/2022 05:35:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=669
06/24/2022 05:35:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=674
06/24/2022 05:35:49 - INFO - __main__ - Global step 1350 Train loss 0.34 ACC 0.5 on epoch=674
06/24/2022 05:35:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=679
06/24/2022 05:35:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=684
06/24/2022 05:35:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=689
06/24/2022 05:35:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=694
06/24/2022 05:35:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=699
06/24/2022 05:35:56 - INFO - __main__ - Global step 1400 Train loss 0.35 ACC 0.5 on epoch=699
06/24/2022 05:35:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=704
06/24/2022 05:35:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=709
06/24/2022 05:36:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=714
06/24/2022 05:36:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=719
06/24/2022 05:36:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=724
06/24/2022 05:36:03 - INFO - __main__ - Global step 1450 Train loss 0.33 ACC 0.5 on epoch=724
06/24/2022 05:36:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=729
06/24/2022 05:36:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.31 on epoch=734
06/24/2022 05:36:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=739
06/24/2022 05:36:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=744
06/24/2022 05:36:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=749
06/24/2022 05:36:09 - INFO - __main__ - Global step 1500 Train loss 0.34 ACC 0.5 on epoch=749
06/24/2022 05:36:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=754
06/24/2022 05:36:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=759
06/24/2022 05:36:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=764
06/24/2022 05:36:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=769
06/24/2022 05:36:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=774
06/24/2022 05:36:16 - INFO - __main__ - Global step 1550 Train loss 0.31 ACC 0.5 on epoch=774
06/24/2022 05:36:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=779
06/24/2022 05:36:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=784
06/24/2022 05:36:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=789
06/24/2022 05:36:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=794
06/24/2022 05:36:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=799
06/24/2022 05:36:23 - INFO - __main__ - Global step 1600 Train loss 0.35 ACC 0.53125 on epoch=799
06/24/2022 05:36:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=804
06/24/2022 05:36:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=809
06/24/2022 05:36:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.29 on epoch=814
06/24/2022 05:36:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=819
06/24/2022 05:36:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=824
06/24/2022 05:36:30 - INFO - __main__ - Global step 1650 Train loss 0.33 ACC 0.5 on epoch=824
06/24/2022 05:36:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=829
06/24/2022 05:36:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.31 on epoch=834
06/24/2022 05:36:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.35 on epoch=839
06/24/2022 05:36:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=844
06/24/2022 05:36:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.33 on epoch=849
06/24/2022 05:36:36 - INFO - __main__ - Global step 1700 Train loss 0.32 ACC 0.5 on epoch=849
06/24/2022 05:36:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=854
06/24/2022 05:36:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=859
06/24/2022 05:36:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.32 on epoch=864
06/24/2022 05:36:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=869
06/24/2022 05:36:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.29 on epoch=874
06/24/2022 05:36:43 - INFO - __main__ - Global step 1750 Train loss 0.33 ACC 0.5 on epoch=874
06/24/2022 05:36:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.30 on epoch=879
06/24/2022 05:36:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=884
06/24/2022 05:36:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.36 on epoch=889
06/24/2022 05:36:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
06/24/2022 05:36:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
06/24/2022 05:36:50 - INFO - __main__ - Global step 1800 Train loss 0.34 ACC 0.5 on epoch=899
06/24/2022 05:36:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.31 on epoch=904
06/24/2022 05:36:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=909
06/24/2022 05:36:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.29 on epoch=914
06/24/2022 05:36:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=919
06/24/2022 05:36:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
06/24/2022 05:36:57 - INFO - __main__ - Global step 1850 Train loss 0.32 ACC 0.5 on epoch=924
06/24/2022 05:36:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=929
06/24/2022 05:36:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=934
06/24/2022 05:37:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=939
06/24/2022 05:37:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=944
06/24/2022 05:37:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.33 on epoch=949
06/24/2022 05:37:03 - INFO - __main__ - Global step 1900 Train loss 0.32 ACC 0.5 on epoch=949
06/24/2022 05:37:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.29 on epoch=954
06/24/2022 05:37:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=959
06/24/2022 05:37:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=964
06/24/2022 05:37:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=969
06/24/2022 05:37:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=974
06/24/2022 05:37:10 - INFO - __main__ - Global step 1950 Train loss 0.31 ACC 0.5 on epoch=974
06/24/2022 05:37:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.33 on epoch=979
06/24/2022 05:37:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=984
06/24/2022 05:37:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
06/24/2022 05:37:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=994
06/24/2022 05:37:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=999
06/24/2022 05:37:17 - INFO - __main__ - Global step 2000 Train loss 0.31 ACC 0.5 on epoch=999
06/24/2022 05:37:17 - INFO - __main__ - save last model!
06/24/2022 05:37:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:37:17 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 05:37:17 - INFO - __main__ - Printing 3 examples
06/24/2022 05:37:17 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 05:37:17 - INFO - __main__ - ['not_duplicate']
06/24/2022 05:37:17 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 05:37:17 - INFO - __main__ - ['not_duplicate']
06/24/2022 05:37:17 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 05:37:17 - INFO - __main__ - ['duplicate']
06/24/2022 05:37:17 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:37:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:37:17 - INFO - __main__ - Printing 3 examples
06/24/2022 05:37:17 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 05:37:17 - INFO - __main__ - ['duplicate']
06/24/2022 05:37:17 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 05:37:17 - INFO - __main__ - ['duplicate']
06/24/2022 05:37:17 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 05:37:17 - INFO - __main__ - ['duplicate']
06/24/2022 05:37:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 05:37:17 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:37:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 05:37:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:37:17 - INFO - __main__ - Printing 3 examples
06/24/2022 05:37:17 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 05:37:17 - INFO - __main__ - ['duplicate']
06/24/2022 05:37:18 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 05:37:18 - INFO - __main__ - ['duplicate']
06/24/2022 05:37:18 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 05:37:18 - INFO - __main__ - ['duplicate']
06/24/2022 05:37:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:37:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:37:18 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:37:23 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:37:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:37:23 - INFO - __main__ - Starting training!
06/24/2022 05:37:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:38:16 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 05:51:38 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_87_0.5_8_predictions.txt
06/24/2022 05:51:39 - INFO - __main__ - ACC on test data: 0.6082
06/24/2022 05:51:39 - INFO - __main__ - prefix=glue-qqp_16_87, lr=0.5, bsz=8, dev_performance=0.53125, test_performance=0.6081622557506802
06/24/2022 05:51:39 - INFO - __main__ - Running ... prefix=glue-qqp_16_87, lr=0.4, bsz=8 ...
06/24/2022 05:51:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:51:40 - INFO - __main__ - Printing 3 examples
06/24/2022 05:51:40 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 05:51:40 - INFO - __main__ - ['duplicate']
06/24/2022 05:51:40 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 05:51:40 - INFO - __main__ - ['duplicate']
06/24/2022 05:51:40 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 05:51:40 - INFO - __main__ - ['duplicate']
06/24/2022 05:51:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 05:51:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:51:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 05:51:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:51:40 - INFO - __main__ - Printing 3 examples
06/24/2022 05:51:40 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 05:51:40 - INFO - __main__ - ['duplicate']
06/24/2022 05:51:40 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 05:51:40 - INFO - __main__ - ['duplicate']
06/24/2022 05:51:40 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 05:51:40 - INFO - __main__ - ['duplicate']
06/24/2022 05:51:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:51:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:51:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:51:45 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:51:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:51:46 - INFO - __main__ - Starting training!
06/24/2022 05:51:47 - INFO - __main__ - Step 10 Global step 10 Train loss 7.20 on epoch=4
06/24/2022 05:51:49 - INFO - __main__ - Step 20 Global step 20 Train loss 7.10 on epoch=9
06/24/2022 05:51:50 - INFO - __main__ - Step 30 Global step 30 Train loss 7.14 on epoch=14
06/24/2022 05:51:51 - INFO - __main__ - Step 40 Global step 40 Train loss 7.13 on epoch=19
06/24/2022 05:51:52 - INFO - __main__ - Step 50 Global step 50 Train loss 7.06 on epoch=24
06/24/2022 05:51:53 - INFO - __main__ - Global step 50 Train loss 7.13 ACC 0.0 on epoch=24
06/24/2022 05:51:53 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 05:51:55 - INFO - __main__ - Step 60 Global step 60 Train loss 7.11 on epoch=29
06/24/2022 05:51:56 - INFO - __main__ - Step 70 Global step 70 Train loss 7.05 on epoch=34
06/24/2022 05:51:57 - INFO - __main__ - Step 80 Global step 80 Train loss 7.07 on epoch=39
06/24/2022 05:51:58 - INFO - __main__ - Step 90 Global step 90 Train loss 7.00 on epoch=44
06/24/2022 05:51:59 - INFO - __main__ - Step 100 Global step 100 Train loss 6.94 on epoch=49
06/24/2022 05:52:06 - INFO - __main__ - Global step 100 Train loss 7.03 ACC 0.0 on epoch=49
06/24/2022 05:52:07 - INFO - __main__ - Step 110 Global step 110 Train loss 6.97 on epoch=54
06/24/2022 05:52:08 - INFO - __main__ - Step 120 Global step 120 Train loss 6.80 on epoch=59
06/24/2022 05:52:09 - INFO - __main__ - Step 130 Global step 130 Train loss 6.75 on epoch=64
06/24/2022 05:52:11 - INFO - __main__ - Step 140 Global step 140 Train loss 6.70 on epoch=69
06/24/2022 05:52:12 - INFO - __main__ - Step 150 Global step 150 Train loss 6.50 on epoch=74
06/24/2022 05:52:14 - INFO - __main__ - Global step 150 Train loss 6.74 ACC 0.0 on epoch=74
06/24/2022 05:52:15 - INFO - __main__ - Step 160 Global step 160 Train loss 6.46 on epoch=79
06/24/2022 05:52:17 - INFO - __main__ - Step 170 Global step 170 Train loss 6.38 on epoch=84
06/24/2022 05:52:18 - INFO - __main__ - Step 180 Global step 180 Train loss 6.37 on epoch=89
06/24/2022 05:52:19 - INFO - __main__ - Step 190 Global step 190 Train loss 6.29 on epoch=94
06/24/2022 05:52:20 - INFO - __main__ - Step 200 Global step 200 Train loss 6.31 on epoch=99
06/24/2022 05:52:22 - INFO - __main__ - Global step 200 Train loss 6.36 ACC 0.0 on epoch=99
06/24/2022 05:52:24 - INFO - __main__ - Step 210 Global step 210 Train loss 6.16 on epoch=104
06/24/2022 05:52:25 - INFO - __main__ - Step 220 Global step 220 Train loss 6.11 on epoch=109
06/24/2022 05:52:26 - INFO - __main__ - Step 230 Global step 230 Train loss 6.04 on epoch=114
06/24/2022 05:52:27 - INFO - __main__ - Step 240 Global step 240 Train loss 5.97 on epoch=119
06/24/2022 05:52:28 - INFO - __main__ - Step 250 Global step 250 Train loss 5.90 on epoch=124
06/24/2022 05:52:39 - INFO - __main__ - Global step 250 Train loss 6.03 ACC 0.0 on epoch=124
06/24/2022 05:52:40 - INFO - __main__ - Step 260 Global step 260 Train loss 5.88 on epoch=129
06/24/2022 05:52:41 - INFO - __main__ - Step 270 Global step 270 Train loss 5.84 on epoch=134
06/24/2022 05:52:42 - INFO - __main__ - Step 280 Global step 280 Train loss 5.76 on epoch=139
06/24/2022 05:52:44 - INFO - __main__ - Step 290 Global step 290 Train loss 5.66 on epoch=144
06/24/2022 05:52:45 - INFO - __main__ - Step 300 Global step 300 Train loss 5.70 on epoch=149
06/24/2022 05:52:48 - INFO - __main__ - Global step 300 Train loss 5.77 ACC 0.0 on epoch=149
06/24/2022 05:52:49 - INFO - __main__ - Step 310 Global step 310 Train loss 5.63 on epoch=154
06/24/2022 05:52:50 - INFO - __main__ - Step 320 Global step 320 Train loss 5.53 on epoch=159
06/24/2022 05:52:52 - INFO - __main__ - Step 330 Global step 330 Train loss 5.30 on epoch=164
06/24/2022 05:52:53 - INFO - __main__ - Step 340 Global step 340 Train loss 5.34 on epoch=169
06/24/2022 05:52:54 - INFO - __main__ - Step 350 Global step 350 Train loss 5.15 on epoch=174
06/24/2022 05:52:59 - INFO - __main__ - Global step 350 Train loss 5.39 ACC 0.0 on epoch=174
06/24/2022 05:53:00 - INFO - __main__ - Step 360 Global step 360 Train loss 5.03 on epoch=179
06/24/2022 05:53:02 - INFO - __main__ - Step 370 Global step 370 Train loss 4.77 on epoch=184
06/24/2022 05:53:03 - INFO - __main__ - Step 380 Global step 380 Train loss 4.80 on epoch=189
06/24/2022 05:53:04 - INFO - __main__ - Step 390 Global step 390 Train loss 4.48 on epoch=194
06/24/2022 05:53:05 - INFO - __main__ - Step 400 Global step 400 Train loss 4.50 on epoch=199
06/24/2022 05:53:09 - INFO - __main__ - Global step 400 Train loss 4.72 ACC 0.03125 on epoch=199
06/24/2022 05:53:09 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=199, global_step=400
06/24/2022 05:53:10 - INFO - __main__ - Step 410 Global step 410 Train loss 4.53 on epoch=204
06/24/2022 05:53:11 - INFO - __main__ - Step 420 Global step 420 Train loss 4.29 on epoch=209
06/24/2022 05:53:12 - INFO - __main__ - Step 430 Global step 430 Train loss 4.19 on epoch=214
06/24/2022 05:53:14 - INFO - __main__ - Step 440 Global step 440 Train loss 4.24 on epoch=219
06/24/2022 05:53:15 - INFO - __main__ - Step 450 Global step 450 Train loss 4.03 on epoch=224
06/24/2022 05:53:18 - INFO - __main__ - Global step 450 Train loss 4.26 ACC 0.09375 on epoch=224
06/24/2022 05:53:18 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.09375 on epoch=224, global_step=450
06/24/2022 05:53:19 - INFO - __main__ - Step 460 Global step 460 Train loss 4.07 on epoch=229
06/24/2022 05:53:20 - INFO - __main__ - Step 470 Global step 470 Train loss 3.95 on epoch=234
06/24/2022 05:53:21 - INFO - __main__ - Step 480 Global step 480 Train loss 3.76 on epoch=239
06/24/2022 05:53:22 - INFO - __main__ - Step 490 Global step 490 Train loss 3.57 on epoch=244
06/24/2022 05:53:24 - INFO - __main__ - Step 500 Global step 500 Train loss 3.58 on epoch=249
06/24/2022 05:53:28 - INFO - __main__ - Global step 500 Train loss 3.79 ACC 0.21875 on epoch=249
06/24/2022 05:53:29 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.21875 on epoch=249, global_step=500
06/24/2022 05:53:30 - INFO - __main__ - Step 510 Global step 510 Train loss 3.59 on epoch=254
06/24/2022 05:53:31 - INFO - __main__ - Step 520 Global step 520 Train loss 3.62 on epoch=259
06/24/2022 05:53:32 - INFO - __main__ - Step 530 Global step 530 Train loss 3.60 on epoch=264
06/24/2022 05:53:33 - INFO - __main__ - Step 540 Global step 540 Train loss 3.49 on epoch=269
06/24/2022 05:53:35 - INFO - __main__ - Step 550 Global step 550 Train loss 3.24 on epoch=274
06/24/2022 05:53:40 - INFO - __main__ - Global step 550 Train loss 3.51 ACC 0.03125 on epoch=274
06/24/2022 05:53:41 - INFO - __main__ - Step 560 Global step 560 Train loss 3.36 on epoch=279
06/24/2022 05:53:42 - INFO - __main__ - Step 570 Global step 570 Train loss 3.32 on epoch=284
06/24/2022 05:53:43 - INFO - __main__ - Step 580 Global step 580 Train loss 3.24 on epoch=289
06/24/2022 05:53:44 - INFO - __main__ - Step 590 Global step 590 Train loss 3.11 on epoch=294
06/24/2022 05:53:46 - INFO - __main__ - Step 600 Global step 600 Train loss 3.10 on epoch=299
06/24/2022 05:53:49 - INFO - __main__ - Global step 600 Train loss 3.23 ACC 0.0625 on epoch=299
06/24/2022 05:53:50 - INFO - __main__ - Step 610 Global step 610 Train loss 2.99 on epoch=304
06/24/2022 05:53:51 - INFO - __main__ - Step 620 Global step 620 Train loss 2.86 on epoch=309
06/24/2022 05:53:52 - INFO - __main__ - Step 630 Global step 630 Train loss 2.81 on epoch=314
06/24/2022 05:53:53 - INFO - __main__ - Step 640 Global step 640 Train loss 2.75 on epoch=319
06/24/2022 05:53:55 - INFO - __main__ - Step 650 Global step 650 Train loss 2.64 on epoch=324
06/24/2022 05:53:57 - INFO - __main__ - Global step 650 Train loss 2.81 ACC 0.1875 on epoch=324
06/24/2022 05:53:59 - INFO - __main__ - Step 660 Global step 660 Train loss 2.54 on epoch=329
06/24/2022 05:54:00 - INFO - __main__ - Step 670 Global step 670 Train loss 2.30 on epoch=334
06/24/2022 05:54:01 - INFO - __main__ - Step 680 Global step 680 Train loss 2.20 on epoch=339
06/24/2022 05:54:02 - INFO - __main__ - Step 690 Global step 690 Train loss 2.10 on epoch=344
06/24/2022 05:54:03 - INFO - __main__ - Step 700 Global step 700 Train loss 2.13 on epoch=349
06/24/2022 05:54:06 - INFO - __main__ - Global step 700 Train loss 2.25 ACC 0.4375 on epoch=349
06/24/2022 05:54:06 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.4375 on epoch=349, global_step=700
06/24/2022 05:54:08 - INFO - __main__ - Step 710 Global step 710 Train loss 2.12 on epoch=354
06/24/2022 05:54:09 - INFO - __main__ - Step 720 Global step 720 Train loss 2.04 on epoch=359
06/24/2022 05:54:10 - INFO - __main__ - Step 730 Global step 730 Train loss 2.00 on epoch=364
06/24/2022 05:54:11 - INFO - __main__ - Step 740 Global step 740 Train loss 1.87 on epoch=369
06/24/2022 05:54:13 - INFO - __main__ - Step 750 Global step 750 Train loss 1.78 on epoch=374
06/24/2022 05:54:15 - INFO - __main__ - Global step 750 Train loss 1.96 ACC 0.5 on epoch=374
06/24/2022 05:54:15 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=374, global_step=750
06/24/2022 05:54:16 - INFO - __main__ - Step 760 Global step 760 Train loss 1.69 on epoch=379
06/24/2022 05:54:18 - INFO - __main__ - Step 770 Global step 770 Train loss 1.77 on epoch=384
06/24/2022 05:54:19 - INFO - __main__ - Step 780 Global step 780 Train loss 1.70 on epoch=389
06/24/2022 05:54:20 - INFO - __main__ - Step 790 Global step 790 Train loss 1.59 on epoch=394
06/24/2022 05:54:21 - INFO - __main__ - Step 800 Global step 800 Train loss 1.65 on epoch=399
06/24/2022 05:54:24 - INFO - __main__ - Global step 800 Train loss 1.68 ACC 0.5 on epoch=399
06/24/2022 05:54:25 - INFO - __main__ - Step 810 Global step 810 Train loss 1.43 on epoch=404
06/24/2022 05:54:27 - INFO - __main__ - Step 820 Global step 820 Train loss 1.46 on epoch=409
06/24/2022 05:54:28 - INFO - __main__ - Step 830 Global step 830 Train loss 1.63 on epoch=414
06/24/2022 05:54:29 - INFO - __main__ - Step 840 Global step 840 Train loss 1.44 on epoch=419
06/24/2022 05:54:30 - INFO - __main__ - Step 850 Global step 850 Train loss 1.43 on epoch=424
06/24/2022 05:54:33 - INFO - __main__ - Global step 850 Train loss 1.48 ACC 0.5 on epoch=424
06/24/2022 05:54:34 - INFO - __main__ - Step 860 Global step 860 Train loss 1.24 on epoch=429
06/24/2022 05:54:35 - INFO - __main__ - Step 870 Global step 870 Train loss 1.27 on epoch=434
06/24/2022 05:54:36 - INFO - __main__ - Step 880 Global step 880 Train loss 1.25 on epoch=439
06/24/2022 05:54:38 - INFO - __main__ - Step 890 Global step 890 Train loss 1.17 on epoch=444
06/24/2022 05:54:39 - INFO - __main__ - Step 900 Global step 900 Train loss 1.13 on epoch=449
06/24/2022 05:54:41 - INFO - __main__ - Global step 900 Train loss 1.21 ACC 0.5 on epoch=449
06/24/2022 05:54:43 - INFO - __main__ - Step 910 Global step 910 Train loss 1.09 on epoch=454
06/24/2022 05:54:44 - INFO - __main__ - Step 920 Global step 920 Train loss 1.03 on epoch=459
06/24/2022 05:54:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.94 on epoch=464
06/24/2022 05:54:46 - INFO - __main__ - Step 940 Global step 940 Train loss 1.01 on epoch=469
06/24/2022 05:54:48 - INFO - __main__ - Step 950 Global step 950 Train loss 1.05 on epoch=474
06/24/2022 05:54:49 - INFO - __main__ - Global step 950 Train loss 1.02 ACC 0.5 on epoch=474
06/24/2022 05:54:51 - INFO - __main__ - Step 960 Global step 960 Train loss 1.07 on epoch=479
06/24/2022 05:54:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.99 on epoch=484
06/24/2022 05:54:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.99 on epoch=489
06/24/2022 05:54:54 - INFO - __main__ - Step 990 Global step 990 Train loss 1.03 on epoch=494
06/24/2022 05:54:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.91 on epoch=499
06/24/2022 05:54:57 - INFO - __main__ - Global step 1000 Train loss 1.00 ACC 0.5 on epoch=499
06/24/2022 05:54:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.95 on epoch=504
06/24/2022 05:55:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.98 on epoch=509
06/24/2022 05:55:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.84 on epoch=514
06/24/2022 05:55:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.89 on epoch=519
06/24/2022 05:55:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.81 on epoch=524
06/24/2022 05:55:05 - INFO - __main__ - Global step 1050 Train loss 0.89 ACC 0.5 on epoch=524
06/24/2022 05:55:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.86 on epoch=529
06/24/2022 05:55:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.87 on epoch=534
06/24/2022 05:55:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.81 on epoch=539
06/24/2022 05:55:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.01 on epoch=544
06/24/2022 05:55:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.72 on epoch=549
06/24/2022 05:55:13 - INFO - __main__ - Global step 1100 Train loss 0.85 ACC 0.5 on epoch=549
06/24/2022 05:55:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.77 on epoch=554
06/24/2022 05:55:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.74 on epoch=559
06/24/2022 05:55:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.79 on epoch=564
06/24/2022 05:55:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.70 on epoch=569
06/24/2022 05:55:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.70 on epoch=574
06/24/2022 05:55:20 - INFO - __main__ - Global step 1150 Train loss 0.74 ACC 0.5 on epoch=574
06/24/2022 05:55:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.75 on epoch=579
06/24/2022 05:55:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.69 on epoch=584
06/24/2022 05:55:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.70 on epoch=589
06/24/2022 05:55:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.66 on epoch=594
06/24/2022 05:55:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.73 on epoch=599
06/24/2022 05:55:28 - INFO - __main__ - Global step 1200 Train loss 0.71 ACC 0.5 on epoch=599
06/24/2022 05:55:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.78 on epoch=604
06/24/2022 05:55:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.69 on epoch=609
06/24/2022 05:55:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.65 on epoch=614
06/24/2022 05:55:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.65 on epoch=619
06/24/2022 05:55:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.57 on epoch=624
06/24/2022 05:55:37 - INFO - __main__ - Global step 1250 Train loss 0.67 ACC 0.5 on epoch=624
06/24/2022 05:55:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.62 on epoch=629
06/24/2022 05:55:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.68 on epoch=634
06/24/2022 05:55:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.51 on epoch=639
06/24/2022 05:55:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.70 on epoch=644
06/24/2022 05:55:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.58 on epoch=649
06/24/2022 05:55:44 - INFO - __main__ - Global step 1300 Train loss 0.62 ACC 0.5 on epoch=649
06/24/2022 05:55:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.58 on epoch=654
06/24/2022 05:55:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.61 on epoch=659
06/24/2022 05:55:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.54 on epoch=664
06/24/2022 05:55:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.63 on epoch=669
06/24/2022 05:55:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.62 on epoch=674
06/24/2022 05:55:52 - INFO - __main__ - Global step 1350 Train loss 0.59 ACC 0.5 on epoch=674
06/24/2022 05:55:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.60 on epoch=679
06/24/2022 05:55:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.59 on epoch=684
06/24/2022 05:55:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.57 on epoch=689
06/24/2022 05:55:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.55 on epoch=694
06/24/2022 05:55:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.63 on epoch=699
06/24/2022 05:56:00 - INFO - __main__ - Global step 1400 Train loss 0.59 ACC 0.5 on epoch=699
06/24/2022 05:56:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.51 on epoch=704
06/24/2022 05:56:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.58 on epoch=709
06/24/2022 05:56:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.58 on epoch=714
06/24/2022 05:56:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.57 on epoch=719
06/24/2022 05:56:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.56 on epoch=724
06/24/2022 05:56:07 - INFO - __main__ - Global step 1450 Train loss 0.56 ACC 0.5 on epoch=724
06/24/2022 05:56:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.57 on epoch=729
06/24/2022 05:56:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.52 on epoch=734
06/24/2022 05:56:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=739
06/24/2022 05:56:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.50 on epoch=744
06/24/2022 05:56:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.54 on epoch=749
06/24/2022 05:56:15 - INFO - __main__ - Global step 1500 Train loss 0.53 ACC 0.5 on epoch=749
06/24/2022 05:56:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.57 on epoch=754
06/24/2022 05:56:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.59 on epoch=759
06/24/2022 05:56:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.51 on epoch=764
06/24/2022 05:56:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.49 on epoch=769
06/24/2022 05:56:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=774
06/24/2022 05:56:23 - INFO - __main__ - Global step 1550 Train loss 0.52 ACC 0.5 on epoch=774
06/24/2022 05:56:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.53 on epoch=779
06/24/2022 05:56:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.52 on epoch=784
06/24/2022 05:56:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.51 on epoch=789
06/24/2022 05:56:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=794
06/24/2022 05:56:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.55 on epoch=799
06/24/2022 05:56:32 - INFO - __main__ - Global step 1600 Train loss 0.51 ACC 0.5 on epoch=799
06/24/2022 05:56:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.47 on epoch=804
06/24/2022 05:56:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.51 on epoch=809
06/24/2022 05:56:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.46 on epoch=814
06/24/2022 05:56:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
06/24/2022 05:56:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.50 on epoch=824
06/24/2022 05:56:40 - INFO - __main__ - Global step 1650 Train loss 0.47 ACC 0.5 on epoch=824
06/24/2022 05:56:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=829
06/24/2022 05:56:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=834
06/24/2022 05:56:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.48 on epoch=839
06/24/2022 05:56:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.44 on epoch=844
06/24/2022 05:56:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
06/24/2022 05:56:48 - INFO - __main__ - Global step 1700 Train loss 0.43 ACC 0.5 on epoch=849
06/24/2022 05:56:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.48 on epoch=854
06/24/2022 05:56:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.49 on epoch=859
06/24/2022 05:56:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.50 on epoch=864
06/24/2022 05:56:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=869
06/24/2022 05:56:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=874
06/24/2022 05:56:56 - INFO - __main__ - Global step 1750 Train loss 0.48 ACC 0.5 on epoch=874
06/24/2022 05:56:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.43 on epoch=879
06/24/2022 05:56:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=884
06/24/2022 05:57:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.36 on epoch=889
06/24/2022 05:57:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=894
06/24/2022 05:57:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.45 on epoch=899
06/24/2022 05:57:04 - INFO - __main__ - Global step 1800 Train loss 0.41 ACC 0.5 on epoch=899
06/24/2022 05:57:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=904
06/24/2022 05:57:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.46 on epoch=909
06/24/2022 05:57:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=914
06/24/2022 05:57:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=919
06/24/2022 05:57:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=924
06/24/2022 05:57:11 - INFO - __main__ - Global step 1850 Train loss 0.42 ACC 0.5 on epoch=924
06/24/2022 05:57:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=929
06/24/2022 05:57:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=934
06/24/2022 05:57:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
06/24/2022 05:57:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=944
06/24/2022 05:57:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=949
06/24/2022 05:57:19 - INFO - __main__ - Global step 1900 Train loss 0.41 ACC 0.5 on epoch=949
06/24/2022 05:57:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=954
06/24/2022 05:57:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=959
06/24/2022 05:57:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=964
06/24/2022 05:57:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.47 on epoch=969
06/24/2022 05:57:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
06/24/2022 05:57:28 - INFO - __main__ - Global step 1950 Train loss 0.40 ACC 0.5 on epoch=974
06/24/2022 05:57:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
06/24/2022 05:57:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
06/24/2022 05:57:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.34 on epoch=989
06/24/2022 05:57:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=994
06/24/2022 05:57:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=999
06/24/2022 05:57:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:57:35 - INFO - __main__ - Printing 3 examples
06/24/2022 05:57:35 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 05:57:35 - INFO - __main__ - ['duplicate']
06/24/2022 05:57:35 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 05:57:35 - INFO - __main__ - ['duplicate']
06/24/2022 05:57:35 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 05:57:35 - INFO - __main__ - ['duplicate']
06/24/2022 05:57:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 05:57:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:57:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 05:57:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 05:57:35 - INFO - __main__ - Printing 3 examples
06/24/2022 05:57:35 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 05:57:35 - INFO - __main__ - ['duplicate']
06/24/2022 05:57:35 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 05:57:35 - INFO - __main__ - ['duplicate']
06/24/2022 05:57:35 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 05:57:35 - INFO - __main__ - ['duplicate']
06/24/2022 05:57:35 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:57:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:57:35 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 05:57:36 - INFO - __main__ - Global step 2000 Train loss 0.39 ACC 0.5 on epoch=999
06/24/2022 05:57:36 - INFO - __main__ - save last model!
06/24/2022 05:57:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 05:57:36 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 05:57:36 - INFO - __main__ - Printing 3 examples
06/24/2022 05:57:36 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 05:57:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 05:57:36 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 05:57:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 05:57:36 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 05:57:36 - INFO - __main__ - ['duplicate']
06/24/2022 05:57:36 - INFO - __main__ - Tokenizing Input ...
06/24/2022 05:57:41 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 05:57:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 05:57:41 - INFO - __main__ - Starting training!
06/24/2022 05:57:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 05:58:35 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 06:40:51 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_87_0.4_8_predictions.txt
06/24/2022 06:40:51 - INFO - __main__ - ACC on test data: 0.3682
06/24/2022 06:40:52 - INFO - __main__ - prefix=glue-qqp_16_87, lr=0.4, bsz=8, dev_performance=0.5, test_performance=0.36816720257234725
06/24/2022 06:40:52 - INFO - __main__ - Running ... prefix=glue-qqp_16_87, lr=0.3, bsz=8 ...
06/24/2022 06:40:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:40:52 - INFO - __main__ - Printing 3 examples
06/24/2022 06:40:52 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 06:40:52 - INFO - __main__ - ['duplicate']
06/24/2022 06:40:52 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 06:40:52 - INFO - __main__ - ['duplicate']
06/24/2022 06:40:52 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 06:40:52 - INFO - __main__ - ['duplicate']
06/24/2022 06:40:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 06:40:52 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:40:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 06:40:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:40:52 - INFO - __main__ - Printing 3 examples
06/24/2022 06:40:52 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 06:40:52 - INFO - __main__ - ['duplicate']
06/24/2022 06:40:52 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 06:40:52 - INFO - __main__ - ['duplicate']
06/24/2022 06:40:52 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 06:40:52 - INFO - __main__ - ['duplicate']
06/24/2022 06:40:52 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:40:52 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:40:53 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 06:40:58 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 06:40:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 06:40:58 - INFO - __main__ - Starting training!
06/24/2022 06:41:00 - INFO - __main__ - Step 10 Global step 10 Train loss 7.16 on epoch=4
06/24/2022 06:41:01 - INFO - __main__ - Step 20 Global step 20 Train loss 7.20 on epoch=9
06/24/2022 06:41:02 - INFO - __main__ - Step 30 Global step 30 Train loss 7.10 on epoch=14
06/24/2022 06:41:03 - INFO - __main__ - Step 40 Global step 40 Train loss 7.06 on epoch=19
06/24/2022 06:41:05 - INFO - __main__ - Step 50 Global step 50 Train loss 7.13 on epoch=24
06/24/2022 06:41:06 - INFO - __main__ - Global step 50 Train loss 7.13 ACC 0.0 on epoch=24
06/24/2022 06:41:06 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 06:41:08 - INFO - __main__ - Step 60 Global step 60 Train loss 7.09 on epoch=29
06/24/2022 06:41:09 - INFO - __main__ - Step 70 Global step 70 Train loss 7.13 on epoch=34
06/24/2022 06:41:10 - INFO - __main__ - Step 80 Global step 80 Train loss 7.11 on epoch=39
06/24/2022 06:41:11 - INFO - __main__ - Step 90 Global step 90 Train loss 7.08 on epoch=44
06/24/2022 06:41:13 - INFO - __main__ - Step 100 Global step 100 Train loss 7.01 on epoch=49
06/24/2022 06:41:14 - INFO - __main__ - Global step 100 Train loss 7.08 ACC 0.0 on epoch=49
06/24/2022 06:41:15 - INFO - __main__ - Step 110 Global step 110 Train loss 7.04 on epoch=54
06/24/2022 06:41:16 - INFO - __main__ - Step 120 Global step 120 Train loss 6.99 on epoch=59
06/24/2022 06:41:18 - INFO - __main__ - Step 130 Global step 130 Train loss 7.01 on epoch=64
06/24/2022 06:41:19 - INFO - __main__ - Step 140 Global step 140 Train loss 6.95 on epoch=69
06/24/2022 06:41:20 - INFO - __main__ - Step 150 Global step 150 Train loss 6.98 on epoch=74
06/24/2022 06:41:22 - INFO - __main__ - Global step 150 Train loss 6.99 ACC 0.0 on epoch=74
06/24/2022 06:41:23 - INFO - __main__ - Step 160 Global step 160 Train loss 6.92 on epoch=79
06/24/2022 06:41:24 - INFO - __main__ - Step 170 Global step 170 Train loss 6.95 on epoch=84
06/24/2022 06:41:25 - INFO - __main__ - Step 180 Global step 180 Train loss 6.89 on epoch=89
06/24/2022 06:41:27 - INFO - __main__ - Step 190 Global step 190 Train loss 6.89 on epoch=94
06/24/2022 06:41:28 - INFO - __main__ - Step 200 Global step 200 Train loss 6.79 on epoch=99
06/24/2022 06:41:29 - INFO - __main__ - Global step 200 Train loss 6.89 ACC 0.0 on epoch=99
06/24/2022 06:41:30 - INFO - __main__ - Step 210 Global step 210 Train loss 6.78 on epoch=104
06/24/2022 06:41:31 - INFO - __main__ - Step 220 Global step 220 Train loss 6.50 on epoch=109
06/24/2022 06:41:33 - INFO - __main__ - Step 230 Global step 230 Train loss 6.51 on epoch=114
06/24/2022 06:41:34 - INFO - __main__ - Step 240 Global step 240 Train loss 6.54 on epoch=119
06/24/2022 06:41:35 - INFO - __main__ - Step 250 Global step 250 Train loss 6.40 on epoch=124
06/24/2022 06:41:37 - INFO - __main__ - Global step 250 Train loss 6.54 ACC 0.0 on epoch=124
06/24/2022 06:41:38 - INFO - __main__ - Step 260 Global step 260 Train loss 6.37 on epoch=129
06/24/2022 06:41:39 - INFO - __main__ - Step 270 Global step 270 Train loss 6.32 on epoch=134
06/24/2022 06:41:40 - INFO - __main__ - Step 280 Global step 280 Train loss 6.32 on epoch=139
06/24/2022 06:41:42 - INFO - __main__ - Step 290 Global step 290 Train loss 6.13 on epoch=144
06/24/2022 06:41:43 - INFO - __main__ - Step 300 Global step 300 Train loss 5.92 on epoch=149
06/24/2022 06:41:50 - INFO - __main__ - Global step 300 Train loss 6.21 ACC 0.0 on epoch=149
06/24/2022 06:41:51 - INFO - __main__ - Step 310 Global step 310 Train loss 6.08 on epoch=154
06/24/2022 06:41:52 - INFO - __main__ - Step 320 Global step 320 Train loss 5.91 on epoch=159
06/24/2022 06:41:54 - INFO - __main__ - Step 330 Global step 330 Train loss 5.82 on epoch=164
06/24/2022 06:41:55 - INFO - __main__ - Step 340 Global step 340 Train loss 5.83 on epoch=169
06/24/2022 06:41:56 - INFO - __main__ - Step 350 Global step 350 Train loss 5.81 on epoch=174
06/24/2022 06:42:00 - INFO - __main__ - Global step 350 Train loss 5.89 ACC 0.0 on epoch=174
06/24/2022 06:42:01 - INFO - __main__ - Step 360 Global step 360 Train loss 5.70 on epoch=179
06/24/2022 06:42:02 - INFO - __main__ - Step 370 Global step 370 Train loss 5.62 on epoch=184
06/24/2022 06:42:03 - INFO - __main__ - Step 380 Global step 380 Train loss 5.61 on epoch=189
06/24/2022 06:42:05 - INFO - __main__ - Step 390 Global step 390 Train loss 5.52 on epoch=194
06/24/2022 06:42:06 - INFO - __main__ - Step 400 Global step 400 Train loss 5.56 on epoch=199
06/24/2022 06:42:14 - INFO - __main__ - Global step 400 Train loss 5.60 ACC 0.0 on epoch=199
06/24/2022 06:42:15 - INFO - __main__ - Step 410 Global step 410 Train loss 5.59 on epoch=204
06/24/2022 06:42:16 - INFO - __main__ - Step 420 Global step 420 Train loss 5.37 on epoch=209
06/24/2022 06:42:17 - INFO - __main__ - Step 430 Global step 430 Train loss 5.39 on epoch=214
06/24/2022 06:42:19 - INFO - __main__ - Step 440 Global step 440 Train loss 5.26 on epoch=219
06/24/2022 06:42:20 - INFO - __main__ - Step 450 Global step 450 Train loss 5.28 on epoch=224
06/24/2022 06:42:27 - INFO - __main__ - Global step 450 Train loss 5.38 ACC 0.0 on epoch=224
06/24/2022 06:42:29 - INFO - __main__ - Step 460 Global step 460 Train loss 5.06 on epoch=229
06/24/2022 06:42:30 - INFO - __main__ - Step 470 Global step 470 Train loss 5.04 on epoch=234
06/24/2022 06:42:31 - INFO - __main__ - Step 480 Global step 480 Train loss 5.04 on epoch=239
06/24/2022 06:42:32 - INFO - __main__ - Step 490 Global step 490 Train loss 4.96 on epoch=244
06/24/2022 06:42:34 - INFO - __main__ - Step 500 Global step 500 Train loss 4.99 on epoch=249
06/24/2022 06:42:38 - INFO - __main__ - Global step 500 Train loss 5.02 ACC 0.0 on epoch=249
06/24/2022 06:42:39 - INFO - __main__ - Step 510 Global step 510 Train loss 4.85 on epoch=254
06/24/2022 06:42:41 - INFO - __main__ - Step 520 Global step 520 Train loss 4.85 on epoch=259
06/24/2022 06:42:42 - INFO - __main__ - Step 530 Global step 530 Train loss 4.79 on epoch=264
06/24/2022 06:42:43 - INFO - __main__ - Step 540 Global step 540 Train loss 4.73 on epoch=269
06/24/2022 06:42:44 - INFO - __main__ - Step 550 Global step 550 Train loss 4.67 on epoch=274
06/24/2022 06:42:52 - INFO - __main__ - Global step 550 Train loss 4.78 ACC 0.0 on epoch=274
06/24/2022 06:42:53 - INFO - __main__ - Step 560 Global step 560 Train loss 4.54 on epoch=279
06/24/2022 06:42:54 - INFO - __main__ - Step 570 Global step 570 Train loss 4.44 on epoch=284
06/24/2022 06:42:56 - INFO - __main__ - Step 580 Global step 580 Train loss 4.38 on epoch=289
06/24/2022 06:42:57 - INFO - __main__ - Step 590 Global step 590 Train loss 4.35 on epoch=294
06/24/2022 06:42:58 - INFO - __main__ - Step 600 Global step 600 Train loss 4.34 on epoch=299
06/24/2022 06:43:03 - INFO - __main__ - Global step 600 Train loss 4.41 ACC 0.0 on epoch=299
06/24/2022 06:43:04 - INFO - __main__ - Step 610 Global step 610 Train loss 4.16 on epoch=304
06/24/2022 06:43:05 - INFO - __main__ - Step 620 Global step 620 Train loss 4.19 on epoch=309
06/24/2022 06:43:06 - INFO - __main__ - Step 630 Global step 630 Train loss 4.18 on epoch=314
06/24/2022 06:43:08 - INFO - __main__ - Step 640 Global step 640 Train loss 4.05 on epoch=319
06/24/2022 06:43:09 - INFO - __main__ - Step 650 Global step 650 Train loss 4.04 on epoch=324
06/24/2022 06:43:14 - INFO - __main__ - Global step 650 Train loss 4.12 ACC 0.0 on epoch=324
06/24/2022 06:43:16 - INFO - __main__ - Step 660 Global step 660 Train loss 3.93 on epoch=329
06/24/2022 06:43:17 - INFO - __main__ - Step 670 Global step 670 Train loss 3.83 on epoch=334
06/24/2022 06:43:18 - INFO - __main__ - Step 680 Global step 680 Train loss 3.59 on epoch=339
06/24/2022 06:43:19 - INFO - __main__ - Step 690 Global step 690 Train loss 3.61 on epoch=344
06/24/2022 06:43:21 - INFO - __main__ - Step 700 Global step 700 Train loss 3.48 on epoch=349
06/24/2022 06:43:23 - INFO - __main__ - Global step 700 Train loss 3.69 ACC 0.0 on epoch=349
06/24/2022 06:43:24 - INFO - __main__ - Step 710 Global step 710 Train loss 3.57 on epoch=354
06/24/2022 06:43:26 - INFO - __main__ - Step 720 Global step 720 Train loss 3.42 on epoch=359
06/24/2022 06:43:27 - INFO - __main__ - Step 730 Global step 730 Train loss 3.49 on epoch=364
06/24/2022 06:43:28 - INFO - __main__ - Step 740 Global step 740 Train loss 3.26 on epoch=369
06/24/2022 06:43:29 - INFO - __main__ - Step 750 Global step 750 Train loss 3.36 on epoch=374
06/24/2022 06:43:32 - INFO - __main__ - Global step 750 Train loss 3.42 ACC 0.0 on epoch=374
06/24/2022 06:43:34 - INFO - __main__ - Step 760 Global step 760 Train loss 3.25 on epoch=379
06/24/2022 06:43:35 - INFO - __main__ - Step 770 Global step 770 Train loss 3.13 on epoch=384
06/24/2022 06:43:36 - INFO - __main__ - Step 780 Global step 780 Train loss 3.10 on epoch=389
06/24/2022 06:43:38 - INFO - __main__ - Step 790 Global step 790 Train loss 3.01 on epoch=394
06/24/2022 06:43:39 - INFO - __main__ - Step 800 Global step 800 Train loss 2.96 on epoch=399
06/24/2022 06:43:42 - INFO - __main__ - Global step 800 Train loss 3.09 ACC 0.0 on epoch=399
06/24/2022 06:43:44 - INFO - __main__ - Step 810 Global step 810 Train loss 2.74 on epoch=404
06/24/2022 06:43:45 - INFO - __main__ - Step 820 Global step 820 Train loss 2.69 on epoch=409
06/24/2022 06:43:46 - INFO - __main__ - Step 830 Global step 830 Train loss 2.54 on epoch=414
06/24/2022 06:43:48 - INFO - __main__ - Step 840 Global step 840 Train loss 2.43 on epoch=419
06/24/2022 06:43:49 - INFO - __main__ - Step 850 Global step 850 Train loss 2.48 on epoch=424
06/24/2022 06:43:52 - INFO - __main__ - Global step 850 Train loss 2.58 ACC 0.4375 on epoch=424
06/24/2022 06:43:52 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.4375 on epoch=424, global_step=850
06/24/2022 06:43:53 - INFO - __main__ - Step 860 Global step 860 Train loss 2.53 on epoch=429
06/24/2022 06:43:55 - INFO - __main__ - Step 870 Global step 870 Train loss 2.40 on epoch=434
06/24/2022 06:43:56 - INFO - __main__ - Step 880 Global step 880 Train loss 2.21 on epoch=439
06/24/2022 06:43:57 - INFO - __main__ - Step 890 Global step 890 Train loss 2.29 on epoch=444
06/24/2022 06:43:58 - INFO - __main__ - Step 900 Global step 900 Train loss 2.11 on epoch=449
06/24/2022 06:44:01 - INFO - __main__ - Global step 900 Train loss 2.31 ACC 0.5 on epoch=449
06/24/2022 06:44:01 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=449, global_step=900
06/24/2022 06:44:03 - INFO - __main__ - Step 910 Global step 910 Train loss 2.02 on epoch=454
06/24/2022 06:44:04 - INFO - __main__ - Step 920 Global step 920 Train loss 1.92 on epoch=459
06/24/2022 06:44:05 - INFO - __main__ - Step 930 Global step 930 Train loss 1.87 on epoch=464
06/24/2022 06:44:06 - INFO - __main__ - Step 940 Global step 940 Train loss 1.71 on epoch=469
06/24/2022 06:44:08 - INFO - __main__ - Step 950 Global step 950 Train loss 1.54 on epoch=474
06/24/2022 06:44:13 - INFO - __main__ - Global step 950 Train loss 1.81 ACC 0.5 on epoch=474
06/24/2022 06:44:14 - INFO - __main__ - Step 960 Global step 960 Train loss 1.54 on epoch=479
06/24/2022 06:44:15 - INFO - __main__ - Step 970 Global step 970 Train loss 1.42 on epoch=484
06/24/2022 06:44:17 - INFO - __main__ - Step 980 Global step 980 Train loss 1.46 on epoch=489
06/24/2022 06:44:18 - INFO - __main__ - Step 990 Global step 990 Train loss 1.37 on epoch=494
06/24/2022 06:44:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.36 on epoch=499
06/24/2022 06:44:22 - INFO - __main__ - Global step 1000 Train loss 1.43 ACC 0.5 on epoch=499
06/24/2022 06:44:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.29 on epoch=504
06/24/2022 06:44:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.17 on epoch=509
06/24/2022 06:44:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.20 on epoch=514
06/24/2022 06:44:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.14 on epoch=519
06/24/2022 06:44:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.11 on epoch=524
06/24/2022 06:44:30 - INFO - __main__ - Global step 1050 Train loss 1.18 ACC 0.5 on epoch=524
06/24/2022 06:44:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.10 on epoch=529
06/24/2022 06:44:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.06 on epoch=534
06/24/2022 06:44:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.00 on epoch=539
06/24/2022 06:44:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.00 on epoch=544
06/24/2022 06:44:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.00 on epoch=549
06/24/2022 06:44:39 - INFO - __main__ - Global step 1100 Train loss 1.03 ACC 0.5 on epoch=549
06/24/2022 06:44:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.93 on epoch=554
06/24/2022 06:44:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.89 on epoch=559
06/24/2022 06:44:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.93 on epoch=564
06/24/2022 06:44:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.89 on epoch=569
06/24/2022 06:44:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.94 on epoch=574
06/24/2022 06:44:46 - INFO - __main__ - Global step 1150 Train loss 0.91 ACC 0.5 on epoch=574
06/24/2022 06:44:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.98 on epoch=579
06/24/2022 06:44:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.92 on epoch=584
06/24/2022 06:44:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.82 on epoch=589
06/24/2022 06:44:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.87 on epoch=594
06/24/2022 06:44:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.86 on epoch=599
06/24/2022 06:44:53 - INFO - __main__ - Global step 1200 Train loss 0.89 ACC 0.5 on epoch=599
06/24/2022 06:44:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.89 on epoch=604
06/24/2022 06:44:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.79 on epoch=609
06/24/2022 06:44:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.70 on epoch=614
06/24/2022 06:44:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.82 on epoch=619
06/24/2022 06:44:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.85 on epoch=624
06/24/2022 06:45:00 - INFO - __main__ - Global step 1250 Train loss 0.81 ACC 0.5 on epoch=624
06/24/2022 06:45:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.85 on epoch=629
06/24/2022 06:45:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.77 on epoch=634
06/24/2022 06:45:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.84 on epoch=639
06/24/2022 06:45:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.80 on epoch=644
06/24/2022 06:45:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.69 on epoch=649
06/24/2022 06:45:07 - INFO - __main__ - Global step 1300 Train loss 0.79 ACC 0.5 on epoch=649
06/24/2022 06:45:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.75 on epoch=654
06/24/2022 06:45:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.79 on epoch=659
06/24/2022 06:45:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.72 on epoch=664
06/24/2022 06:45:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.65 on epoch=669
06/24/2022 06:45:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.63 on epoch=674
06/24/2022 06:45:13 - INFO - __main__ - Global step 1350 Train loss 0.71 ACC 0.5 on epoch=674
06/24/2022 06:45:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.64 on epoch=679
06/24/2022 06:45:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.60 on epoch=684
06/24/2022 06:45:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.60 on epoch=689
06/24/2022 06:45:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.58 on epoch=694
06/24/2022 06:45:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.64 on epoch=699
06/24/2022 06:45:20 - INFO - __main__ - Global step 1400 Train loss 0.61 ACC 0.5 on epoch=699
06/24/2022 06:45:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.60 on epoch=704
06/24/2022 06:45:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.64 on epoch=709
06/24/2022 06:45:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.67 on epoch=714
06/24/2022 06:45:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.65 on epoch=719
06/24/2022 06:45:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.62 on epoch=724
06/24/2022 06:45:27 - INFO - __main__ - Global step 1450 Train loss 0.64 ACC 0.5 on epoch=724
06/24/2022 06:45:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.61 on epoch=729
06/24/2022 06:45:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.64 on epoch=734
06/24/2022 06:45:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.55 on epoch=739
06/24/2022 06:45:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.65 on epoch=744
06/24/2022 06:45:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.59 on epoch=749
06/24/2022 06:45:34 - INFO - __main__ - Global step 1500 Train loss 0.61 ACC 0.5 on epoch=749
06/24/2022 06:45:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.54 on epoch=754
06/24/2022 06:45:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.56 on epoch=759
06/24/2022 06:45:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.61 on epoch=764
06/24/2022 06:45:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.51 on epoch=769
06/24/2022 06:45:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.56 on epoch=774
06/24/2022 06:45:41 - INFO - __main__ - Global step 1550 Train loss 0.56 ACC 0.5 on epoch=774
06/24/2022 06:45:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.46 on epoch=779
06/24/2022 06:45:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.49 on epoch=784
06/24/2022 06:45:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.46 on epoch=789
06/24/2022 06:45:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.57 on epoch=794
06/24/2022 06:45:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.50 on epoch=799
06/24/2022 06:45:47 - INFO - __main__ - Global step 1600 Train loss 0.49 ACC 0.5 on epoch=799
06/24/2022 06:45:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.51 on epoch=804
06/24/2022 06:45:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.54 on epoch=809
06/24/2022 06:45:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.55 on epoch=814
06/24/2022 06:45:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.48 on epoch=819
06/24/2022 06:45:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=824
06/24/2022 06:45:54 - INFO - __main__ - Global step 1650 Train loss 0.51 ACC 0.5 on epoch=824
06/24/2022 06:45:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=829
06/24/2022 06:45:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.51 on epoch=834
06/24/2022 06:45:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.48 on epoch=839
06/24/2022 06:45:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.55 on epoch=844
06/24/2022 06:46:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.55 on epoch=849
06/24/2022 06:46:01 - INFO - __main__ - Global step 1700 Train loss 0.51 ACC 0.5 on epoch=849
06/24/2022 06:46:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.55 on epoch=854
06/24/2022 06:46:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.54 on epoch=859
06/24/2022 06:46:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.51 on epoch=864
06/24/2022 06:46:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=869
06/24/2022 06:46:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.56 on epoch=874
06/24/2022 06:46:08 - INFO - __main__ - Global step 1750 Train loss 0.52 ACC 0.5 on epoch=874
06/24/2022 06:46:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=879
06/24/2022 06:46:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.47 on epoch=884
06/24/2022 06:46:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
06/24/2022 06:46:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.50 on epoch=894
06/24/2022 06:46:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.45 on epoch=899
06/24/2022 06:46:15 - INFO - __main__ - Global step 1800 Train loss 0.48 ACC 0.5 on epoch=899
06/24/2022 06:46:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.49 on epoch=904
06/24/2022 06:46:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.51 on epoch=909
06/24/2022 06:46:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.47 on epoch=914
06/24/2022 06:46:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.52 on epoch=919
06/24/2022 06:46:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=924
06/24/2022 06:46:22 - INFO - __main__ - Global step 1850 Train loss 0.48 ACC 0.5 on epoch=924
06/24/2022 06:46:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=929
06/24/2022 06:46:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.48 on epoch=934
06/24/2022 06:46:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.49 on epoch=939
06/24/2022 06:46:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=944
06/24/2022 06:46:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.48 on epoch=949
06/24/2022 06:46:28 - INFO - __main__ - Global step 1900 Train loss 0.47 ACC 0.46875 on epoch=949
06/24/2022 06:46:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=954
06/24/2022 06:46:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=959
06/24/2022 06:46:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.48 on epoch=964
06/24/2022 06:46:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.45 on epoch=969
06/24/2022 06:46:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=974
06/24/2022 06:46:35 - INFO - __main__ - Global step 1950 Train loss 0.45 ACC 0.4375 on epoch=974
06/24/2022 06:46:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.47 on epoch=979
06/24/2022 06:46:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.45 on epoch=984
06/24/2022 06:46:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=989
06/24/2022 06:46:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=994
06/24/2022 06:46:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=999
06/24/2022 06:46:42 - INFO - __main__ - Global step 2000 Train loss 0.44 ACC 0.53125 on epoch=999
06/24/2022 06:46:42 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=999, global_step=2000
06/24/2022 06:46:42 - INFO - __main__ - save last model!
06/24/2022 06:46:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 06:46:42 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 06:46:42 - INFO - __main__ - Printing 3 examples
06/24/2022 06:46:42 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 06:46:42 - INFO - __main__ - ['not_duplicate']
06/24/2022 06:46:42 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 06:46:42 - INFO - __main__ - ['not_duplicate']
06/24/2022 06:46:42 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 06:46:42 - INFO - __main__ - ['duplicate']
06/24/2022 06:46:42 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:46:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:46:43 - INFO - __main__ - Printing 3 examples
06/24/2022 06:46:43 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 06:46:43 - INFO - __main__ - ['duplicate']
06/24/2022 06:46:43 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 06:46:43 - INFO - __main__ - ['duplicate']
06/24/2022 06:46:43 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 06:46:43 - INFO - __main__ - ['duplicate']
06/24/2022 06:46:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 06:46:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:46:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 06:46:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 06:46:43 - INFO - __main__ - Printing 3 examples
06/24/2022 06:46:43 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 06:46:43 - INFO - __main__ - ['duplicate']
06/24/2022 06:46:43 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 06:46:43 - INFO - __main__ - ['duplicate']
06/24/2022 06:46:43 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 06:46:43 - INFO - __main__ - ['duplicate']
06/24/2022 06:46:43 - INFO - __main__ - Tokenizing Input ...
06/24/2022 06:46:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:46:43 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 06:46:48 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 06:46:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 06:46:49 - INFO - __main__ - Starting training!
06/24/2022 06:47:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 06:47:41 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 07:00:07 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_87_0.3_8_predictions.txt
06/24/2022 07:00:07 - INFO - __main__ - ACC on test data: 0.3694
06/24/2022 07:00:08 - INFO - __main__ - prefix=glue-qqp_16_87, lr=0.3, bsz=8, dev_performance=0.53125, test_performance=0.3694286420974524
06/24/2022 07:00:08 - INFO - __main__ - Running ... prefix=glue-qqp_16_87, lr=0.2, bsz=8 ...
06/24/2022 07:00:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:00:09 - INFO - __main__ - Printing 3 examples
06/24/2022 07:00:09 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 07:00:09 - INFO - __main__ - ['duplicate']
06/24/2022 07:00:09 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 07:00:09 - INFO - __main__ - ['duplicate']
06/24/2022 07:00:09 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 07:00:09 - INFO - __main__ - ['duplicate']
06/24/2022 07:00:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 07:00:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:00:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 07:00:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:00:09 - INFO - __main__ - Printing 3 examples
06/24/2022 07:00:09 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 07:00:09 - INFO - __main__ - ['duplicate']
06/24/2022 07:00:09 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 07:00:09 - INFO - __main__ - ['duplicate']
06/24/2022 07:00:09 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 07:00:09 - INFO - __main__ - ['duplicate']
06/24/2022 07:00:09 - INFO - __main__ - Tokenizing Input ...
06/24/2022 07:00:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:00:09 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 07:00:14 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 07:00:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 07:00:14 - INFO - __main__ - Starting training!
06/24/2022 07:00:16 - INFO - __main__ - Step 10 Global step 10 Train loss 7.22 on epoch=4
06/24/2022 07:00:17 - INFO - __main__ - Step 20 Global step 20 Train loss 7.22 on epoch=9
06/24/2022 07:00:18 - INFO - __main__ - Step 30 Global step 30 Train loss 7.22 on epoch=14
06/24/2022 07:00:19 - INFO - __main__ - Step 40 Global step 40 Train loss 7.11 on epoch=19
06/24/2022 07:00:21 - INFO - __main__ - Step 50 Global step 50 Train loss 7.04 on epoch=24
06/24/2022 07:00:23 - INFO - __main__ - Global step 50 Train loss 7.16 ACC 0.0 on epoch=24
06/24/2022 07:00:23 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 07:00:24 - INFO - __main__ - Step 60 Global step 60 Train loss 7.10 on epoch=29
06/24/2022 07:00:25 - INFO - __main__ - Step 70 Global step 70 Train loss 7.14 on epoch=34
06/24/2022 07:00:26 - INFO - __main__ - Step 80 Global step 80 Train loss 7.10 on epoch=39
06/24/2022 07:00:28 - INFO - __main__ - Step 90 Global step 90 Train loss 7.14 on epoch=44
06/24/2022 07:00:29 - INFO - __main__ - Step 100 Global step 100 Train loss 7.10 on epoch=49
06/24/2022 07:00:35 - INFO - __main__ - Global step 100 Train loss 7.12 ACC 0.0 on epoch=49
06/24/2022 07:00:37 - INFO - __main__ - Step 110 Global step 110 Train loss 7.05 on epoch=54
06/24/2022 07:00:38 - INFO - __main__ - Step 120 Global step 120 Train loss 7.06 on epoch=59
06/24/2022 07:00:39 - INFO - __main__ - Step 130 Global step 130 Train loss 7.03 on epoch=64
06/24/2022 07:00:40 - INFO - __main__ - Step 140 Global step 140 Train loss 7.00 on epoch=69
06/24/2022 07:00:42 - INFO - __main__ - Step 150 Global step 150 Train loss 6.96 on epoch=74
06/24/2022 07:00:49 - INFO - __main__ - Global step 150 Train loss 7.02 ACC 0.0 on epoch=74
06/24/2022 07:00:50 - INFO - __main__ - Step 160 Global step 160 Train loss 6.99 on epoch=79
06/24/2022 07:00:51 - INFO - __main__ - Step 170 Global step 170 Train loss 7.04 on epoch=84
06/24/2022 07:00:53 - INFO - __main__ - Step 180 Global step 180 Train loss 6.93 on epoch=89
06/24/2022 07:00:54 - INFO - __main__ - Step 190 Global step 190 Train loss 6.95 on epoch=94
06/24/2022 07:00:55 - INFO - __main__ - Step 200 Global step 200 Train loss 6.91 on epoch=99
06/24/2022 07:01:01 - INFO - __main__ - Global step 200 Train loss 6.96 ACC 0.0 on epoch=99
06/24/2022 07:01:03 - INFO - __main__ - Step 210 Global step 210 Train loss 6.97 on epoch=104
06/24/2022 07:01:04 - INFO - __main__ - Step 220 Global step 220 Train loss 6.93 on epoch=109
06/24/2022 07:01:05 - INFO - __main__ - Step 230 Global step 230 Train loss 6.75 on epoch=114
06/24/2022 07:01:06 - INFO - __main__ - Step 240 Global step 240 Train loss 6.81 on epoch=119
06/24/2022 07:01:08 - INFO - __main__ - Step 250 Global step 250 Train loss 6.77 on epoch=124
06/24/2022 07:01:18 - INFO - __main__ - Global step 250 Train loss 6.85 ACC 0.0 on epoch=124
06/24/2022 07:01:19 - INFO - __main__ - Step 260 Global step 260 Train loss 6.75 on epoch=129
06/24/2022 07:01:20 - INFO - __main__ - Step 270 Global step 270 Train loss 6.57 on epoch=134
06/24/2022 07:01:22 - INFO - __main__ - Step 280 Global step 280 Train loss 6.63 on epoch=139
06/24/2022 07:01:23 - INFO - __main__ - Step 290 Global step 290 Train loss 6.52 on epoch=144
06/24/2022 07:01:24 - INFO - __main__ - Step 300 Global step 300 Train loss 6.48 on epoch=149
06/24/2022 07:01:26 - INFO - __main__ - Global step 300 Train loss 6.59 ACC 0.0 on epoch=149
06/24/2022 07:01:28 - INFO - __main__ - Step 310 Global step 310 Train loss 6.39 on epoch=154
06/24/2022 07:01:29 - INFO - __main__ - Step 320 Global step 320 Train loss 6.28 on epoch=159
06/24/2022 07:01:30 - INFO - __main__ - Step 330 Global step 330 Train loss 6.24 on epoch=164
06/24/2022 07:01:31 - INFO - __main__ - Step 340 Global step 340 Train loss 6.20 on epoch=169
06/24/2022 07:01:33 - INFO - __main__ - Step 350 Global step 350 Train loss 6.26 on epoch=174
06/24/2022 07:01:38 - INFO - __main__ - Global step 350 Train loss 6.27 ACC 0.0 on epoch=174
06/24/2022 07:01:40 - INFO - __main__ - Step 360 Global step 360 Train loss 6.16 on epoch=179
06/24/2022 07:01:41 - INFO - __main__ - Step 370 Global step 370 Train loss 6.10 on epoch=184
06/24/2022 07:01:42 - INFO - __main__ - Step 380 Global step 380 Train loss 6.11 on epoch=189
06/24/2022 07:01:43 - INFO - __main__ - Step 390 Global step 390 Train loss 6.01 on epoch=194
06/24/2022 07:01:45 - INFO - __main__ - Step 400 Global step 400 Train loss 6.10 on epoch=199
06/24/2022 07:01:51 - INFO - __main__ - Global step 400 Train loss 6.10 ACC 0.0 on epoch=199
06/24/2022 07:01:52 - INFO - __main__ - Step 410 Global step 410 Train loss 6.00 on epoch=204
06/24/2022 07:01:53 - INFO - __main__ - Step 420 Global step 420 Train loss 6.01 on epoch=209
06/24/2022 07:01:55 - INFO - __main__ - Step 430 Global step 430 Train loss 5.89 on epoch=214
06/24/2022 07:01:56 - INFO - __main__ - Step 440 Global step 440 Train loss 5.90 on epoch=219
06/24/2022 07:01:57 - INFO - __main__ - Step 450 Global step 450 Train loss 5.91 on epoch=224
06/24/2022 07:02:03 - INFO - __main__ - Global step 450 Train loss 5.94 ACC 0.0 on epoch=224
06/24/2022 07:02:04 - INFO - __main__ - Step 460 Global step 460 Train loss 5.84 on epoch=229
06/24/2022 07:02:05 - INFO - __main__ - Step 470 Global step 470 Train loss 5.75 on epoch=234
06/24/2022 07:02:07 - INFO - __main__ - Step 480 Global step 480 Train loss 5.76 on epoch=239
06/24/2022 07:02:08 - INFO - __main__ - Step 490 Global step 490 Train loss 5.77 on epoch=244
06/24/2022 07:02:09 - INFO - __main__ - Step 500 Global step 500 Train loss 5.73 on epoch=249
06/24/2022 07:02:11 - INFO - __main__ - Global step 500 Train loss 5.77 ACC 0.0 on epoch=249
06/24/2022 07:02:12 - INFO - __main__ - Step 510 Global step 510 Train loss 5.72 on epoch=254
06/24/2022 07:02:13 - INFO - __main__ - Step 520 Global step 520 Train loss 5.65 on epoch=259
06/24/2022 07:02:14 - INFO - __main__ - Step 530 Global step 530 Train loss 5.68 on epoch=264
06/24/2022 07:02:16 - INFO - __main__ - Step 540 Global step 540 Train loss 5.69 on epoch=269
06/24/2022 07:02:17 - INFO - __main__ - Step 550 Global step 550 Train loss 5.63 on epoch=274
06/24/2022 07:02:27 - INFO - __main__ - Global step 550 Train loss 5.67 ACC 0.0 on epoch=274
06/24/2022 07:02:28 - INFO - __main__ - Step 560 Global step 560 Train loss 5.61 on epoch=279
06/24/2022 07:02:29 - INFO - __main__ - Step 570 Global step 570 Train loss 5.57 on epoch=284
06/24/2022 07:02:31 - INFO - __main__ - Step 580 Global step 580 Train loss 5.59 on epoch=289
06/24/2022 07:02:32 - INFO - __main__ - Step 590 Global step 590 Train loss 5.54 on epoch=294
06/24/2022 07:02:33 - INFO - __main__ - Step 600 Global step 600 Train loss 5.50 on epoch=299
06/24/2022 07:02:39 - INFO - __main__ - Global step 600 Train loss 5.56 ACC 0.0 on epoch=299
06/24/2022 07:02:40 - INFO - __main__ - Step 610 Global step 610 Train loss 5.43 on epoch=304
06/24/2022 07:02:42 - INFO - __main__ - Step 620 Global step 620 Train loss 5.59 on epoch=309
06/24/2022 07:02:43 - INFO - __main__ - Step 630 Global step 630 Train loss 5.47 on epoch=314
06/24/2022 07:02:44 - INFO - __main__ - Step 640 Global step 640 Train loss 5.42 on epoch=319
06/24/2022 07:02:45 - INFO - __main__ - Step 650 Global step 650 Train loss 5.43 on epoch=324
06/24/2022 07:02:51 - INFO - __main__ - Global step 650 Train loss 5.47 ACC 0.0 on epoch=324
06/24/2022 07:02:52 - INFO - __main__ - Step 660 Global step 660 Train loss 5.36 on epoch=329
06/24/2022 07:02:53 - INFO - __main__ - Step 670 Global step 670 Train loss 5.41 on epoch=334
06/24/2022 07:02:54 - INFO - __main__ - Step 680 Global step 680 Train loss 5.40 on epoch=339
06/24/2022 07:02:56 - INFO - __main__ - Step 690 Global step 690 Train loss 5.29 on epoch=344
06/24/2022 07:02:57 - INFO - __main__ - Step 700 Global step 700 Train loss 5.19 on epoch=349
06/24/2022 07:03:04 - INFO - __main__ - Global step 700 Train loss 5.33 ACC 0.0 on epoch=349
06/24/2022 07:03:06 - INFO - __main__ - Step 710 Global step 710 Train loss 5.25 on epoch=354
06/24/2022 07:03:07 - INFO - __main__ - Step 720 Global step 720 Train loss 5.15 on epoch=359
06/24/2022 07:03:08 - INFO - __main__ - Step 730 Global step 730 Train loss 5.24 on epoch=364
06/24/2022 07:03:09 - INFO - __main__ - Step 740 Global step 740 Train loss 5.07 on epoch=369
06/24/2022 07:03:11 - INFO - __main__ - Step 750 Global step 750 Train loss 5.03 on epoch=374
06/24/2022 07:03:17 - INFO - __main__ - Global step 750 Train loss 5.15 ACC 0.0 on epoch=374
06/24/2022 07:03:18 - INFO - __main__ - Step 760 Global step 760 Train loss 4.97 on epoch=379
06/24/2022 07:03:20 - INFO - __main__ - Step 770 Global step 770 Train loss 5.04 on epoch=384
06/24/2022 07:03:21 - INFO - __main__ - Step 780 Global step 780 Train loss 5.02 on epoch=389
06/24/2022 07:03:22 - INFO - __main__ - Step 790 Global step 790 Train loss 4.87 on epoch=394
06/24/2022 07:03:23 - INFO - __main__ - Step 800 Global step 800 Train loss 4.91 on epoch=399
06/24/2022 07:03:31 - INFO - __main__ - Global step 800 Train loss 4.96 ACC 0.0 on epoch=399
06/24/2022 07:03:33 - INFO - __main__ - Step 810 Global step 810 Train loss 4.87 on epoch=404
06/24/2022 07:03:34 - INFO - __main__ - Step 820 Global step 820 Train loss 4.73 on epoch=409
06/24/2022 07:03:35 - INFO - __main__ - Step 830 Global step 830 Train loss 4.68 on epoch=414
06/24/2022 07:03:36 - INFO - __main__ - Step 840 Global step 840 Train loss 4.61 on epoch=419
06/24/2022 07:03:38 - INFO - __main__ - Step 850 Global step 850 Train loss 4.42 on epoch=424
06/24/2022 07:03:45 - INFO - __main__ - Global step 850 Train loss 4.66 ACC 0.0 on epoch=424
06/24/2022 07:03:46 - INFO - __main__ - Step 860 Global step 860 Train loss 4.55 on epoch=429
06/24/2022 07:03:48 - INFO - __main__ - Step 870 Global step 870 Train loss 4.50 on epoch=434
06/24/2022 07:03:49 - INFO - __main__ - Step 880 Global step 880 Train loss 4.52 on epoch=439
06/24/2022 07:03:50 - INFO - __main__ - Step 890 Global step 890 Train loss 4.45 on epoch=444
06/24/2022 07:03:51 - INFO - __main__ - Step 900 Global step 900 Train loss 4.37 on epoch=449
06/24/2022 07:03:55 - INFO - __main__ - Global step 900 Train loss 4.47 ACC 0.0 on epoch=449
06/24/2022 07:03:56 - INFO - __main__ - Step 910 Global step 910 Train loss 4.39 on epoch=454
06/24/2022 07:03:57 - INFO - __main__ - Step 920 Global step 920 Train loss 4.33 on epoch=459
06/24/2022 07:03:58 - INFO - __main__ - Step 930 Global step 930 Train loss 4.33 on epoch=464
06/24/2022 07:04:00 - INFO - __main__ - Step 940 Global step 940 Train loss 4.18 on epoch=469
06/24/2022 07:04:01 - INFO - __main__ - Step 950 Global step 950 Train loss 4.19 on epoch=474
06/24/2022 07:04:04 - INFO - __main__ - Global step 950 Train loss 4.28 ACC 0.0 on epoch=474
06/24/2022 07:04:05 - INFO - __main__ - Step 960 Global step 960 Train loss 4.21 on epoch=479
06/24/2022 07:04:06 - INFO - __main__ - Step 970 Global step 970 Train loss 4.06 on epoch=484
06/24/2022 07:04:08 - INFO - __main__ - Step 980 Global step 980 Train loss 3.99 on epoch=489
06/24/2022 07:04:09 - INFO - __main__ - Step 990 Global step 990 Train loss 4.01 on epoch=494
06/24/2022 07:04:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.96 on epoch=499
06/24/2022 07:04:12 - INFO - __main__ - Global step 1000 Train loss 4.05 ACC 0.0 on epoch=499
06/24/2022 07:04:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.79 on epoch=504
06/24/2022 07:04:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.88 on epoch=509
06/24/2022 07:04:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.81 on epoch=514
06/24/2022 07:04:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.73 on epoch=519
06/24/2022 07:04:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.91 on epoch=524
06/24/2022 07:04:21 - INFO - __main__ - Global step 1050 Train loss 3.82 ACC 0.0 on epoch=524
06/24/2022 07:04:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.77 on epoch=529
06/24/2022 07:04:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.65 on epoch=534
06/24/2022 07:04:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.70 on epoch=539
06/24/2022 07:04:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.52 on epoch=544
06/24/2022 07:04:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.54 on epoch=549
06/24/2022 07:04:33 - INFO - __main__ - Global step 1100 Train loss 3.64 ACC 0.0 on epoch=549
06/24/2022 07:04:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.50 on epoch=554
06/24/2022 07:04:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.51 on epoch=559
06/24/2022 07:04:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.37 on epoch=564
06/24/2022 07:04:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.38 on epoch=569
06/24/2022 07:04:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.44 on epoch=574
06/24/2022 07:04:44 - INFO - __main__ - Global step 1150 Train loss 3.44 ACC 0.0 on epoch=574
06/24/2022 07:04:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.29 on epoch=579
06/24/2022 07:04:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.27 on epoch=584
06/24/2022 07:04:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.35 on epoch=589
06/24/2022 07:04:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.29 on epoch=594
06/24/2022 07:04:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.00 on epoch=599
06/24/2022 07:04:53 - INFO - __main__ - Global step 1200 Train loss 3.24 ACC 0.0 on epoch=599
06/24/2022 07:04:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.98 on epoch=604
06/24/2022 07:04:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.09 on epoch=609
06/24/2022 07:04:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.99 on epoch=614
06/24/2022 07:04:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.96 on epoch=619
06/24/2022 07:05:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.93 on epoch=624
06/24/2022 07:05:03 - INFO - __main__ - Global step 1250 Train loss 2.99 ACC 0.0 on epoch=624
06/24/2022 07:05:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.90 on epoch=629
06/24/2022 07:05:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.82 on epoch=634
06/24/2022 07:05:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.77 on epoch=639
06/24/2022 07:05:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.54 on epoch=644
06/24/2022 07:05:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.82 on epoch=649
06/24/2022 07:05:13 - INFO - __main__ - Global step 1300 Train loss 2.77 ACC 0.0625 on epoch=649
06/24/2022 07:05:13 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=649, global_step=1300
06/24/2022 07:05:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.62 on epoch=654
06/24/2022 07:05:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.63 on epoch=659
06/24/2022 07:05:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.61 on epoch=664
06/24/2022 07:05:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.55 on epoch=669
06/24/2022 07:05:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.47 on epoch=674
06/24/2022 07:05:29 - INFO - __main__ - Global step 1350 Train loss 2.58 ACC 0.0625 on epoch=674
06/24/2022 07:05:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.44 on epoch=679
06/24/2022 07:05:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.51 on epoch=684
06/24/2022 07:05:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.35 on epoch=689
06/24/2022 07:05:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.37 on epoch=694
06/24/2022 07:05:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.33 on epoch=699
06/24/2022 07:05:46 - INFO - __main__ - Global step 1400 Train loss 2.40 ACC 0.21875 on epoch=699
06/24/2022 07:05:46 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.21875 on epoch=699, global_step=1400
06/24/2022 07:05:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.28 on epoch=704
06/24/2022 07:05:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.29 on epoch=709
06/24/2022 07:05:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.38 on epoch=714
06/24/2022 07:05:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.17 on epoch=719
06/24/2022 07:05:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.20 on epoch=724
06/24/2022 07:06:03 - INFO - __main__ - Global step 1450 Train loss 2.27 ACC 0.375 on epoch=724
06/24/2022 07:06:03 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.375 on epoch=724, global_step=1450
06/24/2022 07:06:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.19 on epoch=729
06/24/2022 07:06:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.07 on epoch=734
06/24/2022 07:06:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.10 on epoch=739
06/24/2022 07:06:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.03 on epoch=744
06/24/2022 07:06:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.08 on epoch=749
06/24/2022 07:06:15 - INFO - __main__ - Global step 1500 Train loss 2.09 ACC 0.4375 on epoch=749
06/24/2022 07:06:15 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.4375 on epoch=749, global_step=1500
06/24/2022 07:06:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.09 on epoch=754
06/24/2022 07:06:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.08 on epoch=759
06/24/2022 07:06:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.98 on epoch=764
06/24/2022 07:06:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.90 on epoch=769
06/24/2022 07:06:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 2.05 on epoch=774
06/24/2022 07:06:32 - INFO - __main__ - Global step 1550 Train loss 2.02 ACC 0.375 on epoch=774
06/24/2022 07:06:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.84 on epoch=779
06/24/2022 07:06:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.91 on epoch=784
06/24/2022 07:06:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.94 on epoch=789
06/24/2022 07:06:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.89 on epoch=794
06/24/2022 07:06:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.80 on epoch=799
06/24/2022 07:06:48 - INFO - __main__ - Global step 1600 Train loss 1.88 ACC 0.40625 on epoch=799
06/24/2022 07:06:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.77 on epoch=804
06/24/2022 07:06:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.84 on epoch=809
06/24/2022 07:06:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.73 on epoch=814
06/24/2022 07:06:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.78 on epoch=819
06/24/2022 07:06:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.69 on epoch=824
06/24/2022 07:07:05 - INFO - __main__ - Global step 1650 Train loss 1.76 ACC 0.4375 on epoch=824
06/24/2022 07:07:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.62 on epoch=829
06/24/2022 07:07:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.66 on epoch=834
06/24/2022 07:07:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.73 on epoch=839
06/24/2022 07:07:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.72 on epoch=844
06/24/2022 07:07:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.64 on epoch=849
06/24/2022 07:07:22 - INFO - __main__ - Global step 1700 Train loss 1.67 ACC 0.40625 on epoch=849
06/24/2022 07:07:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.53 on epoch=854
06/24/2022 07:07:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.52 on epoch=859
06/24/2022 07:07:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.59 on epoch=864
06/24/2022 07:07:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.64 on epoch=869
06/24/2022 07:07:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.52 on epoch=874
06/24/2022 07:07:38 - INFO - __main__ - Global step 1750 Train loss 1.56 ACC 0.4375 on epoch=874
06/24/2022 07:07:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.56 on epoch=879
06/24/2022 07:07:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.44 on epoch=884
06/24/2022 07:07:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.60 on epoch=889
06/24/2022 07:07:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.54 on epoch=894
06/24/2022 07:07:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.51 on epoch=899
06/24/2022 07:07:55 - INFO - __main__ - Global step 1800 Train loss 1.53 ACC 0.4375 on epoch=899
06/24/2022 07:07:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.41 on epoch=904
06/24/2022 07:07:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.38 on epoch=909
06/24/2022 07:07:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.39 on epoch=914
06/24/2022 07:08:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.35 on epoch=919
06/24/2022 07:08:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.31 on epoch=924
06/24/2022 07:08:07 - INFO - __main__ - Global step 1850 Train loss 1.37 ACC 0.5 on epoch=924
06/24/2022 07:08:07 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=924, global_step=1850
06/24/2022 07:08:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.25 on epoch=929
06/24/2022 07:08:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.41 on epoch=934
06/24/2022 07:08:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.19 on epoch=939
06/24/2022 07:08:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.35 on epoch=944
06/24/2022 07:08:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.18 on epoch=949
06/24/2022 07:08:15 - INFO - __main__ - Global step 1900 Train loss 1.28 ACC 0.5 on epoch=949
06/24/2022 07:08:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.26 on epoch=954
06/24/2022 07:08:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.11 on epoch=959
06/24/2022 07:08:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.16 on epoch=964
06/24/2022 07:08:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.23 on epoch=969
06/24/2022 07:08:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.14 on epoch=974
06/24/2022 07:08:27 - INFO - __main__ - Global step 1950 Train loss 1.18 ACC 0.46875 on epoch=974
06/24/2022 07:08:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.14 on epoch=979
06/24/2022 07:08:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.15 on epoch=984
06/24/2022 07:08:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.19 on epoch=989
06/24/2022 07:08:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.09 on epoch=994
06/24/2022 07:08:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.06 on epoch=999
06/24/2022 07:08:35 - INFO - __main__ - Global step 2000 Train loss 1.12 ACC 0.5 on epoch=999
06/24/2022 07:08:35 - INFO - __main__ - save last model!
06/24/2022 07:08:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 07:08:35 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 07:08:35 - INFO - __main__ - Printing 3 examples
06/24/2022 07:08:35 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 07:08:35 - INFO - __main__ - ['not_duplicate']
06/24/2022 07:08:35 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 07:08:35 - INFO - __main__ - ['not_duplicate']
06/24/2022 07:08:35 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 07:08:35 - INFO - __main__ - ['duplicate']
06/24/2022 07:08:35 - INFO - __main__ - Tokenizing Input ...
06/24/2022 07:08:53 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:09:34 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 07:46:14 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/glue-qqp_16_87_0.2_8_predictions.txt
06/24/2022 07:46:15 - INFO - __main__ - ACC on test data: 0.3678
06/24/2022 07:46:15 - INFO - __main__ - prefix=glue-qqp_16_87, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.36777145683898094
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (91026): No such process
Task: medical_questions_pairs, Checkpoint: models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt, Identifier: T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10
06/24/2022 07:46:20 - INFO - __main__ - Namespace(task_dir='data/medical_questions_pairs/', task_name='medical_questions_pairs', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/24/2022 07:46:20 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs
Output directory () already exists and is not empty.
06/24/2022 07:46:20 - INFO - __main__ - Namespace(task_dir='data/medical_questions_pairs/', task_name='medical_questions_pairs', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/24/2022 07:46:20 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs
06/24/2022 07:46:21 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/24/2022 07:46:21 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/24/2022 07:46:21 - INFO - __main__ - args.device: cuda:1
06/24/2022 07:46:21 - INFO - __main__ - args.device: cuda:0
06/24/2022 07:46:21 - INFO - __main__ - Using 2 gpus
06/24/2022 07:46:21 - INFO - __main__ - Using 2 gpus
06/24/2022 07:46:21 - INFO - __main__ - Fine-tuning the following samples: ['medical_questions_pairs_16_100', 'medical_questions_pairs_16_13', 'medical_questions_pairs_16_21', 'medical_questions_pairs_16_42', 'medical_questions_pairs_16_87']
06/24/2022 07:46:21 - INFO - __main__ - Fine-tuning the following samples: ['medical_questions_pairs_16_100', 'medical_questions_pairs_16_13', 'medical_questions_pairs_16_21', 'medical_questions_pairs_16_42', 'medical_questions_pairs_16_87']
06/24/2022 07:46:26 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_100, lr=0.5, bsz=8 ...
06/24/2022 07:46:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:46:27 - INFO - __main__ - Printing 3 examples
06/24/2022 07:46:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 07:46:27 - INFO - __main__ - Printing 3 examples
06/24/2022 07:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 07:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 07:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 07:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 07:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 07:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:46:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 07:46:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 07:46:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:46:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:46:27 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 07:46:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampleruse DistributedSampler

06/24/2022 07:46:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:46:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:46:27 - INFO - __main__ - Printing 3 examples
06/24/2022 07:46:27 - INFO - __main__ - Printing 3 examples
06/24/2022 07:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 07:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 07:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 07:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 07:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 07:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 07:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:46:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 07:46:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 07:46:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:46:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:46:27 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 07:46:27 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 07:46:33 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 07:46:33 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 07:46:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 07:46:33 - INFO - __main__ - Starting training!
06/24/2022 07:46:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 07:46:39 - INFO - __main__ - Starting training!
06/24/2022 07:46:41 - INFO - __main__ - Step 10 Global step 10 Train loss 8.37 on epoch=4
06/24/2022 07:46:42 - INFO - __main__ - Step 20 Global step 20 Train loss 8.30 on epoch=9
06/24/2022 07:46:43 - INFO - __main__ - Step 30 Global step 30 Train loss 8.21 on epoch=14
06/24/2022 07:46:45 - INFO - __main__ - Step 40 Global step 40 Train loss 8.15 on epoch=19
06/24/2022 07:46:46 - INFO - __main__ - Step 50 Global step 50 Train loss 8.16 on epoch=24
06/24/2022 07:46:51 - INFO - __main__ - Global step 50 Train loss 8.24 ACC 0.0 on epoch=24
06/24/2022 07:46:51 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 07:46:53 - INFO - __main__ - Step 60 Global step 60 Train loss 8.18 on epoch=29
06/24/2022 07:46:54 - INFO - __main__ - Step 70 Global step 70 Train loss 8.10 on epoch=34
06/24/2022 07:46:55 - INFO - __main__ - Step 80 Global step 80 Train loss 7.98 on epoch=39
06/24/2022 07:46:56 - INFO - __main__ - Step 90 Global step 90 Train loss 7.92 on epoch=44
06/24/2022 07:46:57 - INFO - __main__ - Step 100 Global step 100 Train loss 7.96 on epoch=49
06/24/2022 07:47:01 - INFO - __main__ - Global step 100 Train loss 8.03 ACC 0.0 on epoch=49
06/24/2022 07:47:03 - INFO - __main__ - Step 110 Global step 110 Train loss 7.93 on epoch=54
06/24/2022 07:47:04 - INFO - __main__ - Step 120 Global step 120 Train loss 7.87 on epoch=59
06/24/2022 07:47:05 - INFO - __main__ - Step 130 Global step 130 Train loss 7.76 on epoch=64
06/24/2022 07:47:06 - INFO - __main__ - Step 140 Global step 140 Train loss 7.71 on epoch=69
06/24/2022 07:47:07 - INFO - __main__ - Step 150 Global step 150 Train loss 7.54 on epoch=74
06/24/2022 07:47:10 - INFO - __main__ - Global step 150 Train loss 7.76 ACC 0.0 on epoch=74
06/24/2022 07:47:11 - INFO - __main__ - Step 160 Global step 160 Train loss 7.54 on epoch=79
06/24/2022 07:47:12 - INFO - __main__ - Step 170 Global step 170 Train loss 7.44 on epoch=84
06/24/2022 07:47:13 - INFO - __main__ - Step 180 Global step 180 Train loss 7.19 on epoch=89
06/24/2022 07:47:14 - INFO - __main__ - Step 190 Global step 190 Train loss 6.87 on epoch=94
06/24/2022 07:47:16 - INFO - __main__ - Step 200 Global step 200 Train loss 6.63 on epoch=99
06/24/2022 07:47:19 - INFO - __main__ - Global step 200 Train loss 7.13 ACC 0.0 on epoch=99
06/24/2022 07:47:20 - INFO - __main__ - Step 210 Global step 210 Train loss 6.43 on epoch=104
06/24/2022 07:47:21 - INFO - __main__ - Step 220 Global step 220 Train loss 6.37 on epoch=109
06/24/2022 07:47:23 - INFO - __main__ - Step 230 Global step 230 Train loss 6.11 on epoch=114
06/24/2022 07:47:24 - INFO - __main__ - Step 240 Global step 240 Train loss 5.83 on epoch=119
06/24/2022 07:47:25 - INFO - __main__ - Step 250 Global step 250 Train loss 5.64 on epoch=124
06/24/2022 07:47:28 - INFO - __main__ - Global step 250 Train loss 6.07 ACC 0.0 on epoch=124
06/24/2022 07:47:29 - INFO - __main__ - Step 260 Global step 260 Train loss 5.59 on epoch=129
06/24/2022 07:47:31 - INFO - __main__ - Step 270 Global step 270 Train loss 5.37 on epoch=134
06/24/2022 07:47:32 - INFO - __main__ - Step 280 Global step 280 Train loss 5.20 on epoch=139
06/24/2022 07:47:33 - INFO - __main__ - Step 290 Global step 290 Train loss 4.89 on epoch=144
06/24/2022 07:47:34 - INFO - __main__ - Step 300 Global step 300 Train loss 4.75 on epoch=149
06/24/2022 07:47:41 - INFO - __main__ - Global step 300 Train loss 5.16 ACC 0.0 on epoch=149
06/24/2022 07:47:42 - INFO - __main__ - Step 310 Global step 310 Train loss 4.49 on epoch=154
06/24/2022 07:47:44 - INFO - __main__ - Step 320 Global step 320 Train loss 4.40 on epoch=159
06/24/2022 07:47:45 - INFO - __main__ - Step 330 Global step 330 Train loss 4.17 on epoch=164
06/24/2022 07:47:46 - INFO - __main__ - Step 340 Global step 340 Train loss 4.02 on epoch=169
06/24/2022 07:47:47 - INFO - __main__ - Step 350 Global step 350 Train loss 3.95 on epoch=174
06/24/2022 07:47:50 - INFO - __main__ - Global step 350 Train loss 4.20 ACC 0.0625 on epoch=174
06/24/2022 07:47:50 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=174, global_step=350
06/24/2022 07:47:51 - INFO - __main__ - Step 360 Global step 360 Train loss 3.68 on epoch=179
06/24/2022 07:47:52 - INFO - __main__ - Step 370 Global step 370 Train loss 3.43 on epoch=184
06/24/2022 07:47:53 - INFO - __main__ - Step 380 Global step 380 Train loss 3.26 on epoch=189
06/24/2022 07:47:54 - INFO - __main__ - Step 390 Global step 390 Train loss 3.29 on epoch=194
06/24/2022 07:47:56 - INFO - __main__ - Step 400 Global step 400 Train loss 2.98 on epoch=199
06/24/2022 07:47:58 - INFO - __main__ - Global step 400 Train loss 3.33 ACC 0.15625 on epoch=199
06/24/2022 07:47:58 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.15625 on epoch=199, global_step=400
06/24/2022 07:47:59 - INFO - __main__ - Step 410 Global step 410 Train loss 2.76 on epoch=204
06/24/2022 07:48:00 - INFO - __main__ - Step 420 Global step 420 Train loss 2.80 on epoch=209
06/24/2022 07:48:01 - INFO - __main__ - Step 430 Global step 430 Train loss 2.68 on epoch=214
06/24/2022 07:48:02 - INFO - __main__ - Step 440 Global step 440 Train loss 2.30 on epoch=219
06/24/2022 07:48:04 - INFO - __main__ - Step 450 Global step 450 Train loss 2.23 on epoch=224
06/24/2022 07:48:13 - INFO - __main__ - Global step 450 Train loss 2.55 ACC 0.4375 on epoch=224
06/24/2022 07:48:13 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.4375 on epoch=224, global_step=450
06/24/2022 07:48:14 - INFO - __main__ - Step 460 Global step 460 Train loss 2.31 on epoch=229
06/24/2022 07:48:15 - INFO - __main__ - Step 470 Global step 470 Train loss 1.97 on epoch=234
06/24/2022 07:48:16 - INFO - __main__ - Step 480 Global step 480 Train loss 1.90 on epoch=239
06/24/2022 07:48:17 - INFO - __main__ - Step 490 Global step 490 Train loss 1.67 on epoch=244
06/24/2022 07:48:19 - INFO - __main__ - Step 500 Global step 500 Train loss 1.77 on epoch=249
06/24/2022 07:48:28 - INFO - __main__ - Global step 500 Train loss 1.93 ACC 0.5 on epoch=249
06/24/2022 07:48:28 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=249, global_step=500
06/24/2022 07:48:29 - INFO - __main__ - Step 510 Global step 510 Train loss 1.80 on epoch=254
06/24/2022 07:48:30 - INFO - __main__ - Step 520 Global step 520 Train loss 1.60 on epoch=259
06/24/2022 07:48:32 - INFO - __main__ - Step 530 Global step 530 Train loss 1.63 on epoch=264
06/24/2022 07:48:33 - INFO - __main__ - Step 540 Global step 540 Train loss 1.41 on epoch=269
06/24/2022 07:48:34 - INFO - __main__ - Step 550 Global step 550 Train loss 1.52 on epoch=274
06/24/2022 07:48:36 - INFO - __main__ - Global step 550 Train loss 1.59 ACC 0.5 on epoch=274
06/24/2022 07:48:37 - INFO - __main__ - Step 560 Global step 560 Train loss 1.69 on epoch=279
06/24/2022 07:48:38 - INFO - __main__ - Step 570 Global step 570 Train loss 1.67 on epoch=284
06/24/2022 07:48:39 - INFO - __main__ - Step 580 Global step 580 Train loss 1.33 on epoch=289
06/24/2022 07:48:40 - INFO - __main__ - Step 590 Global step 590 Train loss 1.37 on epoch=294
06/24/2022 07:48:42 - INFO - __main__ - Step 600 Global step 600 Train loss 1.36 on epoch=299
06/24/2022 07:48:50 - INFO - __main__ - Global step 600 Train loss 1.49 ACC 0.5 on epoch=299
06/24/2022 07:48:51 - INFO - __main__ - Step 610 Global step 610 Train loss 1.26 on epoch=304
06/24/2022 07:48:53 - INFO - __main__ - Step 620 Global step 620 Train loss 1.28 on epoch=309
06/24/2022 07:48:54 - INFO - __main__ - Step 630 Global step 630 Train loss 1.28 on epoch=314
06/24/2022 07:48:55 - INFO - __main__ - Step 640 Global step 640 Train loss 1.16 on epoch=319
06/24/2022 07:48:56 - INFO - __main__ - Step 650 Global step 650 Train loss 1.12 on epoch=324
06/24/2022 07:48:59 - INFO - __main__ - Global step 650 Train loss 1.22 ACC 0.5 on epoch=324
06/24/2022 07:49:00 - INFO - __main__ - Step 660 Global step 660 Train loss 1.01 on epoch=329
06/24/2022 07:49:01 - INFO - __main__ - Step 670 Global step 670 Train loss 1.09 on epoch=334
06/24/2022 07:49:03 - INFO - __main__ - Step 680 Global step 680 Train loss 1.22 on epoch=339
06/24/2022 07:49:04 - INFO - __main__ - Step 690 Global step 690 Train loss 1.09 on epoch=344
06/24/2022 07:49:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.98 on epoch=349
06/24/2022 07:49:14 - INFO - __main__ - Global step 700 Train loss 1.08 ACC 0.5 on epoch=349
06/24/2022 07:49:16 - INFO - __main__ - Step 710 Global step 710 Train loss 1.10 on epoch=354
06/24/2022 07:49:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.90 on epoch=359
06/24/2022 07:49:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.90 on epoch=364
06/24/2022 07:49:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.94 on epoch=369
06/24/2022 07:49:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.92 on epoch=374
06/24/2022 07:49:29 - INFO - __main__ - Global step 750 Train loss 0.95 ACC 0.5 on epoch=374
06/24/2022 07:49:30 - INFO - __main__ - Step 760 Global step 760 Train loss 1.12 on epoch=379
06/24/2022 07:49:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.86 on epoch=384
06/24/2022 07:49:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.89 on epoch=389
06/24/2022 07:49:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.69 on epoch=394
06/24/2022 07:49:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.93 on epoch=399
06/24/2022 07:49:42 - INFO - __main__ - Global step 800 Train loss 0.90 ACC 0.5 on epoch=399
06/24/2022 07:49:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.77 on epoch=404
06/24/2022 07:49:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.69 on epoch=409
06/24/2022 07:49:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.69 on epoch=414
06/24/2022 07:49:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.74 on epoch=419
06/24/2022 07:49:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.71 on epoch=424
06/24/2022 07:49:51 - INFO - __main__ - Global step 850 Train loss 0.72 ACC 0.5 on epoch=424
06/24/2022 07:49:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.78 on epoch=429
06/24/2022 07:49:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.83 on epoch=434
06/24/2022 07:49:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.74 on epoch=439
06/24/2022 07:49:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.77 on epoch=444
06/24/2022 07:49:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.75 on epoch=449
06/24/2022 07:49:59 - INFO - __main__ - Global step 900 Train loss 0.77 ACC 0.5 on epoch=449
06/24/2022 07:50:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.75 on epoch=454
06/24/2022 07:50:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.62 on epoch=459
06/24/2022 07:50:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.67 on epoch=464
06/24/2022 07:50:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.71 on epoch=469
06/24/2022 07:50:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.72 on epoch=474
06/24/2022 07:50:06 - INFO - __main__ - Global step 950 Train loss 0.69 ACC 0.5 on epoch=474
06/24/2022 07:50:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.68 on epoch=479
06/24/2022 07:50:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.70 on epoch=484
06/24/2022 07:50:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.80 on epoch=489
06/24/2022 07:50:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=494
06/24/2022 07:50:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.69 on epoch=499
06/24/2022 07:50:14 - INFO - __main__ - Global step 1000 Train loss 0.72 ACC 0.5 on epoch=499
06/24/2022 07:50:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.62 on epoch=504
06/24/2022 07:50:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.66 on epoch=509
06/24/2022 07:50:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.57 on epoch=514
06/24/2022 07:50:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.64 on epoch=519
06/24/2022 07:50:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.65 on epoch=524
06/24/2022 07:50:22 - INFO - __main__ - Global step 1050 Train loss 0.63 ACC 0.5 on epoch=524
06/24/2022 07:50:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.66 on epoch=529
06/24/2022 07:50:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.79 on epoch=534
06/24/2022 07:50:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.61 on epoch=539
06/24/2022 07:50:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.63 on epoch=544
06/24/2022 07:50:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.59 on epoch=549
06/24/2022 07:50:29 - INFO - __main__ - Global step 1100 Train loss 0.66 ACC 0.5 on epoch=549
06/24/2022 07:50:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.56 on epoch=554
06/24/2022 07:50:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.62 on epoch=559
06/24/2022 07:50:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.63 on epoch=564
06/24/2022 07:50:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.61 on epoch=569
06/24/2022 07:50:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.57 on epoch=574
06/24/2022 07:50:36 - INFO - __main__ - Global step 1150 Train loss 0.60 ACC 0.5 on epoch=574
06/24/2022 07:50:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.65 on epoch=579
06/24/2022 07:50:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.69 on epoch=584
06/24/2022 07:50:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.61 on epoch=589
06/24/2022 07:50:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.67 on epoch=594
06/24/2022 07:50:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.67 on epoch=599
06/24/2022 07:50:43 - INFO - __main__ - Global step 1200 Train loss 0.66 ACC 0.5 on epoch=599
06/24/2022 07:50:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.65 on epoch=604
06/24/2022 07:50:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.62 on epoch=609
06/24/2022 07:50:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.58 on epoch=614
06/24/2022 07:50:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.64 on epoch=619
06/24/2022 07:50:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.55 on epoch=624
06/24/2022 07:50:50 - INFO - __main__ - Global step 1250 Train loss 0.61 ACC 0.5 on epoch=624
06/24/2022 07:50:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.52 on epoch=629
06/24/2022 07:50:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.62 on epoch=634
06/24/2022 07:50:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.51 on epoch=639
06/24/2022 07:50:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.58 on epoch=644
06/24/2022 07:50:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.60 on epoch=649
06/24/2022 07:50:57 - INFO - __main__ - Global step 1300 Train loss 0.56 ACC 0.5 on epoch=649
06/24/2022 07:50:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.54 on epoch=654
06/24/2022 07:50:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.56 on epoch=659
06/24/2022 07:51:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.63 on epoch=664
06/24/2022 07:51:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.49 on epoch=669
06/24/2022 07:51:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=674
06/24/2022 07:51:03 - INFO - __main__ - Global step 1350 Train loss 0.55 ACC 0.5 on epoch=674
06/24/2022 07:51:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.49 on epoch=679
06/24/2022 07:51:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.60 on epoch=684
06/24/2022 07:51:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.62 on epoch=689
06/24/2022 07:51:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.59 on epoch=694
06/24/2022 07:51:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.52 on epoch=699
06/24/2022 07:51:10 - INFO - __main__ - Global step 1400 Train loss 0.56 ACC 0.5 on epoch=699
06/24/2022 07:51:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.56 on epoch=704
06/24/2022 07:51:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=709
06/24/2022 07:51:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.61 on epoch=714
06/24/2022 07:51:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.52 on epoch=719
06/24/2022 07:51:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.51 on epoch=724
06/24/2022 07:51:16 - INFO - __main__ - Global step 1450 Train loss 0.54 ACC 0.5 on epoch=724
06/24/2022 07:51:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=729
06/24/2022 07:51:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.51 on epoch=734
06/24/2022 07:51:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=739
06/24/2022 07:51:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.70 on epoch=744
06/24/2022 07:51:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.54 on epoch=749
06/24/2022 07:51:22 - INFO - __main__ - Global step 1500 Train loss 0.54 ACC 0.5 on epoch=749
06/24/2022 07:51:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=754
06/24/2022 07:51:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.53 on epoch=759
06/24/2022 07:51:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
06/24/2022 07:51:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.58 on epoch=769
06/24/2022 07:51:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.52 on epoch=774
06/24/2022 07:51:29 - INFO - __main__ - Global step 1550 Train loss 0.51 ACC 0.5 on epoch=774
06/24/2022 07:51:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.56 on epoch=779
06/24/2022 07:51:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=784
06/24/2022 07:51:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.55 on epoch=789
06/24/2022 07:51:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.48 on epoch=794
06/24/2022 07:51:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.50 on epoch=799
06/24/2022 07:51:35 - INFO - __main__ - Global step 1600 Train loss 0.53 ACC 0.5 on epoch=799
06/24/2022 07:51:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.55 on epoch=804
06/24/2022 07:51:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.50 on epoch=809
06/24/2022 07:51:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.48 on epoch=814
06/24/2022 07:51:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.59 on epoch=819
06/24/2022 07:51:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=824
06/24/2022 07:51:41 - INFO - __main__ - Global step 1650 Train loss 0.52 ACC 0.5 on epoch=824
06/24/2022 07:51:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=829
06/24/2022 07:51:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.54 on epoch=834
06/24/2022 07:51:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.45 on epoch=839
06/24/2022 07:51:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.51 on epoch=844
06/24/2022 07:51:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=849
06/24/2022 07:51:48 - INFO - __main__ - Global step 1700 Train loss 0.48 ACC 0.5 on epoch=849
06/24/2022 07:51:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=854
06/24/2022 07:51:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.50 on epoch=859
06/24/2022 07:51:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.51 on epoch=864
06/24/2022 07:51:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.47 on epoch=869
06/24/2022 07:51:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.60 on epoch=874
06/24/2022 07:51:54 - INFO - __main__ - Global step 1750 Train loss 0.50 ACC 0.5 on epoch=874
06/24/2022 07:51:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.43 on epoch=879
06/24/2022 07:51:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.46 on epoch=884
06/24/2022 07:51:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.54 on epoch=889
06/24/2022 07:51:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.52 on epoch=894
06/24/2022 07:52:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.61 on epoch=899
06/24/2022 07:52:00 - INFO - __main__ - Global step 1800 Train loss 0.51 ACC 0.5 on epoch=899
06/24/2022 07:52:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.55 on epoch=904
06/24/2022 07:52:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.59 on epoch=909
06/24/2022 07:52:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=914
06/24/2022 07:52:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.50 on epoch=919
06/24/2022 07:52:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=924
06/24/2022 07:52:07 - INFO - __main__ - Global step 1850 Train loss 0.52 ACC 0.5 on epoch=924
06/24/2022 07:52:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=929
06/24/2022 07:52:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.49 on epoch=934
06/24/2022 07:52:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=939
06/24/2022 07:52:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.51 on epoch=944
06/24/2022 07:52:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.56 on epoch=949
06/24/2022 07:52:13 - INFO - __main__ - Global step 1900 Train loss 0.48 ACC 0.5 on epoch=949
06/24/2022 07:52:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=954
06/24/2022 07:52:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.49 on epoch=959
06/24/2022 07:52:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.49 on epoch=964
06/24/2022 07:52:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.46 on epoch=969
06/24/2022 07:52:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.42 on epoch=974
06/24/2022 07:52:19 - INFO - __main__ - Global step 1950 Train loss 0.46 ACC 0.5 on epoch=974
06/24/2022 07:52:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.54 on epoch=979
06/24/2022 07:52:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.45 on epoch=984
06/24/2022 07:52:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=989
06/24/2022 07:52:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=994
06/24/2022 07:52:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.51 on epoch=999
06/24/2022 07:52:26 - INFO - __main__ - Global step 2000 Train loss 0.48 ACC 0.5 on epoch=999
06/24/2022 07:52:26 - INFO - __main__ - save last model!
06/24/2022 07:52:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 07:52:26 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 07:52:26 - INFO - __main__ - Printing 3 examples
06/24/2022 07:52:26 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 07:52:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:26 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 07:52:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:26 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 07:52:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 07:52:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:52:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:52:26 - INFO - __main__ - Printing 3 examples
06/24/2022 07:52:26 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 07:52:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:26 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 07:52:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:26 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 07:52:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 07:52:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:52:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 07:52:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:52:26 - INFO - __main__ - Printing 3 examples
06/24/2022 07:52:26 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 07:52:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:26 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 07:52:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:26 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 07:52:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 07:52:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:52:27 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 07:52:27 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 07:52:32 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 07:52:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 07:52:33 - INFO - __main__ - Starting training!
06/24/2022 07:52:33 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_100_0.5_8_predictions.txt
06/24/2022 07:52:33 - INFO - __main__ - ACC on test data: 0.4836
06/24/2022 07:52:33 - INFO - __main__ - prefix=medical_questions_pairs_16_100, lr=0.5, bsz=8, dev_performance=0.5, test_performance=0.48360655737704916
06/24/2022 07:52:33 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_100, lr=0.4, bsz=8 ...
06/24/2022 07:52:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:52:34 - INFO - __main__ - Printing 3 examples
06/24/2022 07:52:34 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 07:52:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:34 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 07:52:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:34 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 07:52:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 07:52:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:52:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 07:52:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:52:34 - INFO - __main__ - Printing 3 examples
06/24/2022 07:52:34 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 07:52:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:34 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 07:52:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:34 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 07:52:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:52:34 - INFO - __main__ - Tokenizing Input ...
06/24/2022 07:52:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:52:35 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 07:52:40 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 07:52:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 07:52:40 - INFO - __main__ - Starting training!
06/24/2022 07:52:42 - INFO - __main__ - Step 10 Global step 10 Train loss 8.41 on epoch=4
06/24/2022 07:52:43 - INFO - __main__ - Step 20 Global step 20 Train loss 8.33 on epoch=9
06/24/2022 07:52:44 - INFO - __main__ - Step 30 Global step 30 Train loss 8.26 on epoch=14
06/24/2022 07:52:45 - INFO - __main__ - Step 40 Global step 40 Train loss 8.27 on epoch=19
06/24/2022 07:52:46 - INFO - __main__ - Step 50 Global step 50 Train loss 8.24 on epoch=24
06/24/2022 07:52:48 - INFO - __main__ - Global step 50 Train loss 8.30 ACC 0.0 on epoch=24
06/24/2022 07:52:48 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 07:52:49 - INFO - __main__ - Step 60 Global step 60 Train loss 8.11 on epoch=29
06/24/2022 07:52:50 - INFO - __main__ - Step 70 Global step 70 Train loss 8.09 on epoch=34
06/24/2022 07:52:52 - INFO - __main__ - Step 80 Global step 80 Train loss 8.04 on epoch=39
06/24/2022 07:52:53 - INFO - __main__ - Step 90 Global step 90 Train loss 8.04 on epoch=44
06/24/2022 07:52:54 - INFO - __main__ - Step 100 Global step 100 Train loss 8.08 on epoch=49
06/24/2022 07:53:00 - INFO - __main__ - Global step 100 Train loss 8.07 ACC 0.0 on epoch=49
06/24/2022 07:53:01 - INFO - __main__ - Step 110 Global step 110 Train loss 7.89 on epoch=54
06/24/2022 07:53:02 - INFO - __main__ - Step 120 Global step 120 Train loss 7.90 on epoch=59
06/24/2022 07:53:03 - INFO - __main__ - Step 130 Global step 130 Train loss 7.89 on epoch=64
06/24/2022 07:53:05 - INFO - __main__ - Step 140 Global step 140 Train loss 7.96 on epoch=69
06/24/2022 07:53:06 - INFO - __main__ - Step 150 Global step 150 Train loss 7.68 on epoch=74
06/24/2022 07:53:12 - INFO - __main__ - Global step 150 Train loss 7.86 ACC 0.0 on epoch=74
06/24/2022 07:53:13 - INFO - __main__ - Step 160 Global step 160 Train loss 7.76 on epoch=79
06/24/2022 07:53:14 - INFO - __main__ - Step 170 Global step 170 Train loss 7.69 on epoch=84
06/24/2022 07:53:15 - INFO - __main__ - Step 180 Global step 180 Train loss 7.52 on epoch=89
06/24/2022 07:53:17 - INFO - __main__ - Step 190 Global step 190 Train loss 7.51 on epoch=94
06/24/2022 07:53:18 - INFO - __main__ - Step 200 Global step 200 Train loss 7.45 on epoch=99
06/24/2022 07:53:24 - INFO - __main__ - Global step 200 Train loss 7.59 ACC 0.0 on epoch=99
06/24/2022 07:53:25 - INFO - __main__ - Step 210 Global step 210 Train loss 7.09 on epoch=104
06/24/2022 07:53:26 - INFO - __main__ - Step 220 Global step 220 Train loss 7.16 on epoch=109
06/24/2022 07:53:27 - INFO - __main__ - Step 230 Global step 230 Train loss 6.83 on epoch=114
06/24/2022 07:53:29 - INFO - __main__ - Step 240 Global step 240 Train loss 6.83 on epoch=119
06/24/2022 07:53:30 - INFO - __main__ - Step 250 Global step 250 Train loss 6.57 on epoch=124
06/24/2022 07:53:37 - INFO - __main__ - Global step 250 Train loss 6.90 ACC 0.0 on epoch=124
06/24/2022 07:53:38 - INFO - __main__ - Step 260 Global step 260 Train loss 6.44 on epoch=129
06/24/2022 07:53:39 - INFO - __main__ - Step 270 Global step 270 Train loss 6.33 on epoch=134
06/24/2022 07:53:41 - INFO - __main__ - Step 280 Global step 280 Train loss 6.15 on epoch=139
06/24/2022 07:53:42 - INFO - __main__ - Step 290 Global step 290 Train loss 6.12 on epoch=144
06/24/2022 07:53:43 - INFO - __main__ - Step 300 Global step 300 Train loss 6.04 on epoch=149
06/24/2022 07:53:52 - INFO - __main__ - Global step 300 Train loss 6.22 ACC 0.0 on epoch=149
06/24/2022 07:53:54 - INFO - __main__ - Step 310 Global step 310 Train loss 5.79 on epoch=154
06/24/2022 07:53:55 - INFO - __main__ - Step 320 Global step 320 Train loss 5.76 on epoch=159
06/24/2022 07:53:56 - INFO - __main__ - Step 330 Global step 330 Train loss 5.67 on epoch=164
06/24/2022 07:53:57 - INFO - __main__ - Step 340 Global step 340 Train loss 5.54 on epoch=169
06/24/2022 07:53:58 - INFO - __main__ - Step 350 Global step 350 Train loss 5.33 on epoch=174
06/24/2022 07:54:03 - INFO - __main__ - Global step 350 Train loss 5.62 ACC 0.0 on epoch=174
06/24/2022 07:54:05 - INFO - __main__ - Step 360 Global step 360 Train loss 5.25 on epoch=179
06/24/2022 07:54:06 - INFO - __main__ - Step 370 Global step 370 Train loss 5.03 on epoch=184
06/24/2022 07:54:07 - INFO - __main__ - Step 380 Global step 380 Train loss 4.92 on epoch=189
06/24/2022 07:54:08 - INFO - __main__ - Step 390 Global step 390 Train loss 4.67 on epoch=194
06/24/2022 07:54:10 - INFO - __main__ - Step 400 Global step 400 Train loss 4.51 on epoch=199
06/24/2022 07:54:14 - INFO - __main__ - Global step 400 Train loss 4.88 ACC 0.0 on epoch=199
06/24/2022 07:54:16 - INFO - __main__ - Step 410 Global step 410 Train loss 4.17 on epoch=204
06/24/2022 07:54:17 - INFO - __main__ - Step 420 Global step 420 Train loss 4.12 on epoch=209
06/24/2022 07:54:18 - INFO - __main__ - Step 430 Global step 430 Train loss 3.87 on epoch=214
06/24/2022 07:54:20 - INFO - __main__ - Step 440 Global step 440 Train loss 3.76 on epoch=219
06/24/2022 07:54:21 - INFO - __main__ - Step 450 Global step 450 Train loss 3.59 on epoch=224
06/24/2022 07:54:25 - INFO - __main__ - Global step 450 Train loss 3.90 ACC 0.0 on epoch=224
06/24/2022 07:54:27 - INFO - __main__ - Step 460 Global step 460 Train loss 3.49 on epoch=229
06/24/2022 07:54:28 - INFO - __main__ - Step 470 Global step 470 Train loss 3.41 on epoch=234
06/24/2022 07:54:29 - INFO - __main__ - Step 480 Global step 480 Train loss 3.29 on epoch=239
06/24/2022 07:54:30 - INFO - __main__ - Step 490 Global step 490 Train loss 3.17 on epoch=244
06/24/2022 07:54:32 - INFO - __main__ - Step 500 Global step 500 Train loss 3.31 on epoch=249
06/24/2022 07:54:42 - INFO - __main__ - Global step 500 Train loss 3.33 ACC 0.0 on epoch=249
06/24/2022 07:54:43 - INFO - __main__ - Step 510 Global step 510 Train loss 3.20 on epoch=254
06/24/2022 07:54:45 - INFO - __main__ - Step 520 Global step 520 Train loss 3.11 on epoch=259
06/24/2022 07:54:46 - INFO - __main__ - Step 530 Global step 530 Train loss 3.19 on epoch=264
06/24/2022 07:54:47 - INFO - __main__ - Step 540 Global step 540 Train loss 2.95 on epoch=269
06/24/2022 07:54:49 - INFO - __main__ - Step 550 Global step 550 Train loss 2.74 on epoch=274
06/24/2022 07:54:59 - INFO - __main__ - Global step 550 Train loss 3.04 ACC 0.0 on epoch=274
06/24/2022 07:55:01 - INFO - __main__ - Step 560 Global step 560 Train loss 2.77 on epoch=279
06/24/2022 07:55:02 - INFO - __main__ - Step 570 Global step 570 Train loss 2.68 on epoch=284
06/24/2022 07:55:03 - INFO - __main__ - Step 580 Global step 580 Train loss 2.61 on epoch=289
06/24/2022 07:55:04 - INFO - __main__ - Step 590 Global step 590 Train loss 2.82 on epoch=294
06/24/2022 07:55:06 - INFO - __main__ - Step 600 Global step 600 Train loss 2.55 on epoch=299
06/24/2022 07:55:17 - INFO - __main__ - Global step 600 Train loss 2.69 ACC 0.0 on epoch=299
06/24/2022 07:55:19 - INFO - __main__ - Step 610 Global step 610 Train loss 2.63 on epoch=304
06/24/2022 07:55:20 - INFO - __main__ - Step 620 Global step 620 Train loss 2.51 on epoch=309
06/24/2022 07:55:21 - INFO - __main__ - Step 630 Global step 630 Train loss 2.61 on epoch=314
06/24/2022 07:55:23 - INFO - __main__ - Step 640 Global step 640 Train loss 2.51 on epoch=319
06/24/2022 07:55:24 - INFO - __main__ - Step 650 Global step 650 Train loss 2.36 on epoch=324
06/24/2022 07:55:35 - INFO - __main__ - Global step 650 Train loss 2.52 ACC 0.21875 on epoch=324
06/24/2022 07:55:35 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.21875 on epoch=324, global_step=650
06/24/2022 07:55:36 - INFO - __main__ - Step 660 Global step 660 Train loss 2.46 on epoch=329
06/24/2022 07:55:37 - INFO - __main__ - Step 670 Global step 670 Train loss 2.55 on epoch=334
06/24/2022 07:55:39 - INFO - __main__ - Step 680 Global step 680 Train loss 2.32 on epoch=339
06/24/2022 07:55:40 - INFO - __main__ - Step 690 Global step 690 Train loss 2.25 on epoch=344
06/24/2022 07:55:41 - INFO - __main__ - Step 700 Global step 700 Train loss 2.31 on epoch=349
06/24/2022 07:55:47 - INFO - __main__ - Global step 700 Train loss 2.38 ACC 0.4375 on epoch=349
06/24/2022 07:55:47 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.4375 on epoch=349, global_step=700
06/24/2022 07:55:48 - INFO - __main__ - Step 710 Global step 710 Train loss 2.17 on epoch=354
06/24/2022 07:55:50 - INFO - __main__ - Step 720 Global step 720 Train loss 2.22 on epoch=359
06/24/2022 07:55:51 - INFO - __main__ - Step 730 Global step 730 Train loss 2.08 on epoch=364
06/24/2022 07:55:52 - INFO - __main__ - Step 740 Global step 740 Train loss 1.93 on epoch=369
06/24/2022 07:55:54 - INFO - __main__ - Step 750 Global step 750 Train loss 1.91 on epoch=374
06/24/2022 07:56:01 - INFO - __main__ - Global step 750 Train loss 2.06 ACC 0.5 on epoch=374
06/24/2022 07:56:01 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=374, global_step=750
06/24/2022 07:56:02 - INFO - __main__ - Step 760 Global step 760 Train loss 1.86 on epoch=379
06/24/2022 07:56:03 - INFO - __main__ - Step 770 Global step 770 Train loss 1.62 on epoch=384
06/24/2022 07:56:05 - INFO - __main__ - Step 780 Global step 780 Train loss 1.81 on epoch=389
06/24/2022 07:56:06 - INFO - __main__ - Step 790 Global step 790 Train loss 1.77 on epoch=394
06/24/2022 07:56:07 - INFO - __main__ - Step 800 Global step 800 Train loss 1.74 on epoch=399
06/24/2022 07:56:17 - INFO - __main__ - Global step 800 Train loss 1.76 ACC 0.5 on epoch=399
06/24/2022 07:56:18 - INFO - __main__ - Step 810 Global step 810 Train loss 1.75 on epoch=404
06/24/2022 07:56:20 - INFO - __main__ - Step 820 Global step 820 Train loss 1.71 on epoch=409
06/24/2022 07:56:21 - INFO - __main__ - Step 830 Global step 830 Train loss 1.56 on epoch=414
06/24/2022 07:56:22 - INFO - __main__ - Step 840 Global step 840 Train loss 1.50 on epoch=419
06/24/2022 07:56:23 - INFO - __main__ - Step 850 Global step 850 Train loss 1.49 on epoch=424
06/24/2022 07:56:32 - INFO - __main__ - Global step 850 Train loss 1.60 ACC 0.5 on epoch=424
06/24/2022 07:56:33 - INFO - __main__ - Step 860 Global step 860 Train loss 1.35 on epoch=429
06/24/2022 07:56:35 - INFO - __main__ - Step 870 Global step 870 Train loss 1.35 on epoch=434
06/24/2022 07:56:36 - INFO - __main__ - Step 880 Global step 880 Train loss 1.25 on epoch=439
06/24/2022 07:56:37 - INFO - __main__ - Step 890 Global step 890 Train loss 1.43 on epoch=444
06/24/2022 07:56:39 - INFO - __main__ - Step 900 Global step 900 Train loss 1.35 on epoch=449
06/24/2022 07:56:48 - INFO - __main__ - Global step 900 Train loss 1.35 ACC 0.46875 on epoch=449
06/24/2022 07:56:50 - INFO - __main__ - Step 910 Global step 910 Train loss 1.30 on epoch=454
06/24/2022 07:56:51 - INFO - __main__ - Step 920 Global step 920 Train loss 1.47 on epoch=459
06/24/2022 07:56:52 - INFO - __main__ - Step 930 Global step 930 Train loss 1.17 on epoch=464
06/24/2022 07:56:54 - INFO - __main__ - Step 940 Global step 940 Train loss 1.30 on epoch=469
06/24/2022 07:56:55 - INFO - __main__ - Step 950 Global step 950 Train loss 1.22 on epoch=474
06/24/2022 07:57:05 - INFO - __main__ - Global step 950 Train loss 1.29 ACC 0.5 on epoch=474
06/24/2022 07:57:06 - INFO - __main__ - Step 960 Global step 960 Train loss 1.04 on epoch=479
06/24/2022 07:57:08 - INFO - __main__ - Step 970 Global step 970 Train loss 1.10 on epoch=484
06/24/2022 07:57:09 - INFO - __main__ - Step 980 Global step 980 Train loss 1.12 on epoch=489
06/24/2022 07:57:10 - INFO - __main__ - Step 990 Global step 990 Train loss 1.06 on epoch=494
06/24/2022 07:57:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.98 on epoch=499
06/24/2022 07:57:13 - INFO - __main__ - Global step 1000 Train loss 1.06 ACC 0.5 on epoch=499
06/24/2022 07:57:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.13 on epoch=504
06/24/2022 07:57:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.91 on epoch=509
06/24/2022 07:57:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.93 on epoch=514
06/24/2022 07:57:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.94 on epoch=519
06/24/2022 07:57:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.82 on epoch=524
06/24/2022 07:57:25 - INFO - __main__ - Global step 1050 Train loss 0.95 ACC 0.53125 on epoch=524
06/24/2022 07:57:25 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=524, global_step=1050
06/24/2022 07:57:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.89 on epoch=529
06/24/2022 07:57:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.03 on epoch=534
06/24/2022 07:57:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.95 on epoch=539
06/24/2022 07:57:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.90 on epoch=544
06/24/2022 07:57:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.91 on epoch=549
06/24/2022 07:57:31 - INFO - __main__ - Global step 1100 Train loss 0.94 ACC 0.53125 on epoch=549
06/24/2022 07:57:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.90 on epoch=554
06/24/2022 07:57:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.92 on epoch=559
06/24/2022 07:57:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.92 on epoch=564
06/24/2022 07:57:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.76 on epoch=569
06/24/2022 07:57:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.79 on epoch=574
06/24/2022 07:57:38 - INFO - __main__ - Global step 1150 Train loss 0.86 ACC 0.5 on epoch=574
06/24/2022 07:57:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.77 on epoch=579
06/24/2022 07:57:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.90 on epoch=584
06/24/2022 07:57:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.82 on epoch=589
06/24/2022 07:57:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.84 on epoch=594
06/24/2022 07:57:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.85 on epoch=599
06/24/2022 07:57:45 - INFO - __main__ - Global step 1200 Train loss 0.83 ACC 0.53125 on epoch=599
06/24/2022 07:57:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.75 on epoch=604
06/24/2022 07:57:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.82 on epoch=609
06/24/2022 07:57:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.67 on epoch=614
06/24/2022 07:57:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.72 on epoch=619
06/24/2022 07:57:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.62 on epoch=624
06/24/2022 07:57:52 - INFO - __main__ - Global step 1250 Train loss 0.72 ACC 0.5 on epoch=624
06/24/2022 07:57:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.72 on epoch=629
06/24/2022 07:57:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.98 on epoch=634
06/24/2022 07:57:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.65 on epoch=639
06/24/2022 07:57:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.64 on epoch=644
06/24/2022 07:57:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.63 on epoch=649
06/24/2022 07:57:59 - INFO - __main__ - Global step 1300 Train loss 0.72 ACC 0.5 on epoch=649
06/24/2022 07:58:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.73 on epoch=654
06/24/2022 07:58:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.75 on epoch=659
06/24/2022 07:58:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.71 on epoch=664
06/24/2022 07:58:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.70 on epoch=669
06/24/2022 07:58:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.68 on epoch=674
06/24/2022 07:58:06 - INFO - __main__ - Global step 1350 Train loss 0.72 ACC 0.5625 on epoch=674
06/24/2022 07:58:06 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=674, global_step=1350
06/24/2022 07:58:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.69 on epoch=679
06/24/2022 07:58:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.77 on epoch=684
06/24/2022 07:58:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.68 on epoch=689
06/24/2022 07:58:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.63 on epoch=694
06/24/2022 07:58:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.79 on epoch=699
06/24/2022 07:58:13 - INFO - __main__ - Global step 1400 Train loss 0.71 ACC 0.5 on epoch=699
06/24/2022 07:58:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.53 on epoch=704
06/24/2022 07:58:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.71 on epoch=709
06/24/2022 07:58:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.71 on epoch=714
06/24/2022 07:58:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.66 on epoch=719
06/24/2022 07:58:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.68 on epoch=724
06/24/2022 07:58:19 - INFO - __main__ - Global step 1450 Train loss 0.66 ACC 0.5 on epoch=724
06/24/2022 07:58:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.66 on epoch=729
06/24/2022 07:58:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.62 on epoch=734
06/24/2022 07:58:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.86 on epoch=739
06/24/2022 07:58:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.66 on epoch=744
06/24/2022 07:58:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.56 on epoch=749
06/24/2022 07:58:26 - INFO - __main__ - Global step 1500 Train loss 0.67 ACC 0.5 on epoch=749
06/24/2022 07:58:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.66 on epoch=754
06/24/2022 07:58:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.55 on epoch=759
06/24/2022 07:58:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.61 on epoch=764
06/24/2022 07:58:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.61 on epoch=769
06/24/2022 07:58:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.61 on epoch=774
06/24/2022 07:58:33 - INFO - __main__ - Global step 1550 Train loss 0.61 ACC 0.5 on epoch=774
06/24/2022 07:58:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.66 on epoch=779
06/24/2022 07:58:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.57 on epoch=784
06/24/2022 07:58:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.63 on epoch=789
06/24/2022 07:58:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.57 on epoch=794
06/24/2022 07:58:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.61 on epoch=799
06/24/2022 07:58:40 - INFO - __main__ - Global step 1600 Train loss 0.61 ACC 0.53125 on epoch=799
06/24/2022 07:58:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.65 on epoch=804
06/24/2022 07:58:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.64 on epoch=809
06/24/2022 07:58:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.65 on epoch=814
06/24/2022 07:58:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.58 on epoch=819
06/24/2022 07:58:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.60 on epoch=824
06/24/2022 07:58:47 - INFO - __main__ - Global step 1650 Train loss 0.62 ACC 0.5 on epoch=824
06/24/2022 07:58:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.61 on epoch=829
06/24/2022 07:58:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.53 on epoch=834
06/24/2022 07:58:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.55 on epoch=839
06/24/2022 07:58:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.57 on epoch=844
06/24/2022 07:58:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.61 on epoch=849
06/24/2022 07:58:54 - INFO - __main__ - Global step 1700 Train loss 0.57 ACC 0.5 on epoch=849
06/24/2022 07:58:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.58 on epoch=854
06/24/2022 07:58:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.45 on epoch=859
06/24/2022 07:58:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.55 on epoch=864
06/24/2022 07:58:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.55 on epoch=869
06/24/2022 07:59:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.56 on epoch=874
06/24/2022 07:59:01 - INFO - __main__ - Global step 1750 Train loss 0.54 ACC 0.4375 on epoch=874
06/24/2022 07:59:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.67 on epoch=879
06/24/2022 07:59:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.52 on epoch=884
06/24/2022 07:59:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.58 on epoch=889
06/24/2022 07:59:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.70 on epoch=894
06/24/2022 07:59:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.63 on epoch=899
06/24/2022 07:59:07 - INFO - __main__ - Global step 1800 Train loss 0.62 ACC 0.5 on epoch=899
06/24/2022 07:59:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.58 on epoch=904
06/24/2022 07:59:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.54 on epoch=909
06/24/2022 07:59:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.58 on epoch=914
06/24/2022 07:59:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.58 on epoch=919
06/24/2022 07:59:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.55 on epoch=924
06/24/2022 07:59:14 - INFO - __main__ - Global step 1850 Train loss 0.57 ACC 0.5 on epoch=924
06/24/2022 07:59:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.50 on epoch=929
06/24/2022 07:59:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.49 on epoch=934
06/24/2022 07:59:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.73 on epoch=939
06/24/2022 07:59:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.52 on epoch=944
06/24/2022 07:59:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.52 on epoch=949
06/24/2022 07:59:20 - INFO - __main__ - Global step 1900 Train loss 0.55 ACC 0.5 on epoch=949
06/24/2022 07:59:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.54 on epoch=954
06/24/2022 07:59:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.63 on epoch=959
06/24/2022 07:59:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.53 on epoch=964
06/24/2022 07:59:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.52 on epoch=969
06/24/2022 07:59:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=974
06/24/2022 07:59:27 - INFO - __main__ - Global step 1950 Train loss 0.54 ACC 0.5 on epoch=974
06/24/2022 07:59:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.59 on epoch=979
06/24/2022 07:59:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.51 on epoch=984
06/24/2022 07:59:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.48 on epoch=989
06/24/2022 07:59:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.60 on epoch=994
06/24/2022 07:59:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.63 on epoch=999
06/24/2022 07:59:33 - INFO - __main__ - Global step 2000 Train loss 0.56 ACC 0.375 on epoch=999
06/24/2022 07:59:33 - INFO - __main__ - save last model!
06/24/2022 07:59:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 07:59:33 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 07:59:33 - INFO - __main__ - Printing 3 examples
06/24/2022 07:59:33 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 07:59:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:33 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 07:59:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 07:59:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:33 - INFO - __main__ - Tokenizing Input ...
06/24/2022 07:59:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:59:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:59:34 - INFO - __main__ - Printing 3 examples
06/24/2022 07:59:34 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 07:59:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:34 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 07:59:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:34 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 07:59:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 07:59:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:59:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 07:59:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:59:34 - INFO - __main__ - Printing 3 examples
06/24/2022 07:59:34 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 07:59:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:34 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 07:59:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:34 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 07:59:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:34 - INFO - __main__ - Tokenizing Input ...
06/24/2022 07:59:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:59:34 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 07:59:34 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 07:59:39 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 07:59:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 07:59:40 - INFO - __main__ - Starting training!
06/24/2022 07:59:41 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_100_0.4_8_predictions.txt
06/24/2022 07:59:41 - INFO - __main__ - ACC on test data: 0.4984
06/24/2022 07:59:41 - INFO - __main__ - prefix=medical_questions_pairs_16_100, lr=0.4, bsz=8, dev_performance=0.5625, test_performance=0.49836065573770494
06/24/2022 07:59:41 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_100, lr=0.3, bsz=8 ...
06/24/2022 07:59:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:59:42 - INFO - __main__ - Printing 3 examples
06/24/2022 07:59:42 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 07:59:42 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:42 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 07:59:42 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:42 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 07:59:42 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 07:59:42 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:59:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 07:59:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 07:59:42 - INFO - __main__ - Printing 3 examples
06/24/2022 07:59:42 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 07:59:42 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:42 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 07:59:42 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:42 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 07:59:42 - INFO - __main__ - ['Dissimilar']
06/24/2022 07:59:42 - INFO - __main__ - Tokenizing Input ...
06/24/2022 07:59:42 - INFO - __main__ - Tokenizing Output ...
06/24/2022 07:59:42 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 07:59:47 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 07:59:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 07:59:48 - INFO - __main__ - Starting training!
06/24/2022 07:59:49 - INFO - __main__ - Step 10 Global step 10 Train loss 8.41 on epoch=4
06/24/2022 07:59:50 - INFO - __main__ - Step 20 Global step 20 Train loss 8.35 on epoch=9
06/24/2022 07:59:52 - INFO - __main__ - Step 30 Global step 30 Train loss 8.31 on epoch=14
06/24/2022 07:59:53 - INFO - __main__ - Step 40 Global step 40 Train loss 8.25 on epoch=19
06/24/2022 07:59:54 - INFO - __main__ - Step 50 Global step 50 Train loss 8.25 on epoch=24
06/24/2022 07:59:55 - INFO - __main__ - Global step 50 Train loss 8.31 ACC 0.0 on epoch=24
06/24/2022 07:59:55 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 07:59:56 - INFO - __main__ - Step 60 Global step 60 Train loss 8.36 on epoch=29
06/24/2022 07:59:57 - INFO - __main__ - Step 70 Global step 70 Train loss 8.14 on epoch=34
06/24/2022 07:59:59 - INFO - __main__ - Step 80 Global step 80 Train loss 8.13 on epoch=39
06/24/2022 08:00:00 - INFO - __main__ - Step 90 Global step 90 Train loss 8.17 on epoch=44
06/24/2022 08:00:01 - INFO - __main__ - Step 100 Global step 100 Train loss 8.17 on epoch=49
06/24/2022 08:00:03 - INFO - __main__ - Global step 100 Train loss 8.19 ACC 0.0 on epoch=49
06/24/2022 08:00:04 - INFO - __main__ - Step 110 Global step 110 Train loss 8.01 on epoch=54
06/24/2022 08:00:05 - INFO - __main__ - Step 120 Global step 120 Train loss 7.93 on epoch=59
06/24/2022 08:00:06 - INFO - __main__ - Step 130 Global step 130 Train loss 8.07 on epoch=64
06/24/2022 08:00:08 - INFO - __main__ - Step 140 Global step 140 Train loss 8.03 on epoch=69
06/24/2022 08:00:09 - INFO - __main__ - Step 150 Global step 150 Train loss 7.95 on epoch=74
06/24/2022 08:00:11 - INFO - __main__ - Global step 150 Train loss 8.00 ACC 0.0 on epoch=74
06/24/2022 08:00:12 - INFO - __main__ - Step 160 Global step 160 Train loss 7.98 on epoch=79
06/24/2022 08:00:13 - INFO - __main__ - Step 170 Global step 170 Train loss 8.05 on epoch=84
06/24/2022 08:00:15 - INFO - __main__ - Step 180 Global step 180 Train loss 7.91 on epoch=89
06/24/2022 08:00:16 - INFO - __main__ - Step 190 Global step 190 Train loss 7.87 on epoch=94
06/24/2022 08:00:17 - INFO - __main__ - Step 200 Global step 200 Train loss 7.83 on epoch=99
06/24/2022 08:00:18 - INFO - __main__ - Global step 200 Train loss 7.93 ACC 0.0 on epoch=99
06/24/2022 08:00:19 - INFO - __main__ - Step 210 Global step 210 Train loss 7.68 on epoch=104
06/24/2022 08:00:20 - INFO - __main__ - Step 220 Global step 220 Train loss 7.71 on epoch=109
06/24/2022 08:00:22 - INFO - __main__ - Step 230 Global step 230 Train loss 7.49 on epoch=114
06/24/2022 08:00:23 - INFO - __main__ - Step 240 Global step 240 Train loss 7.39 on epoch=119
06/24/2022 08:00:24 - INFO - __main__ - Step 250 Global step 250 Train loss 7.14 on epoch=124
06/24/2022 08:00:26 - INFO - __main__ - Global step 250 Train loss 7.48 ACC 0.0 on epoch=124
06/24/2022 08:00:27 - INFO - __main__ - Step 260 Global step 260 Train loss 7.13 on epoch=129
06/24/2022 08:00:28 - INFO - __main__ - Step 270 Global step 270 Train loss 7.10 on epoch=134
06/24/2022 08:00:30 - INFO - __main__ - Step 280 Global step 280 Train loss 6.90 on epoch=139
06/24/2022 08:00:31 - INFO - __main__ - Step 290 Global step 290 Train loss 6.82 on epoch=144
06/24/2022 08:00:32 - INFO - __main__ - Step 300 Global step 300 Train loss 6.69 on epoch=149
06/24/2022 08:00:34 - INFO - __main__ - Global step 300 Train loss 6.93 ACC 0.0 on epoch=149
06/24/2022 08:00:35 - INFO - __main__ - Step 310 Global step 310 Train loss 6.56 on epoch=154
06/24/2022 08:00:37 - INFO - __main__ - Step 320 Global step 320 Train loss 6.60 on epoch=159
06/24/2022 08:00:38 - INFO - __main__ - Step 330 Global step 330 Train loss 6.44 on epoch=164
06/24/2022 08:00:39 - INFO - __main__ - Step 340 Global step 340 Train loss 6.37 on epoch=169
06/24/2022 08:00:40 - INFO - __main__ - Step 350 Global step 350 Train loss 6.34 on epoch=174
06/24/2022 08:00:43 - INFO - __main__ - Global step 350 Train loss 6.46 ACC 0.0 on epoch=174
06/24/2022 08:00:44 - INFO - __main__ - Step 360 Global step 360 Train loss 6.20 on epoch=179
06/24/2022 08:00:45 - INFO - __main__ - Step 370 Global step 370 Train loss 6.34 on epoch=184
06/24/2022 08:00:47 - INFO - __main__ - Step 380 Global step 380 Train loss 6.07 on epoch=189
06/24/2022 08:00:48 - INFO - __main__ - Step 390 Global step 390 Train loss 6.05 on epoch=194
06/24/2022 08:00:49 - INFO - __main__ - Step 400 Global step 400 Train loss 6.02 on epoch=199
06/24/2022 08:00:53 - INFO - __main__ - Global step 400 Train loss 6.13 ACC 0.0 on epoch=199
06/24/2022 08:00:54 - INFO - __main__ - Step 410 Global step 410 Train loss 6.05 on epoch=204
06/24/2022 08:00:56 - INFO - __main__ - Step 420 Global step 420 Train loss 5.87 on epoch=209
06/24/2022 08:00:57 - INFO - __main__ - Step 430 Global step 430 Train loss 5.84 on epoch=214
06/24/2022 08:00:58 - INFO - __main__ - Step 440 Global step 440 Train loss 5.78 on epoch=219
06/24/2022 08:00:59 - INFO - __main__ - Step 450 Global step 450 Train loss 5.76 on epoch=224
06/24/2022 08:01:01 - INFO - __main__ - Global step 450 Train loss 5.86 ACC 0.0 on epoch=224
06/24/2022 08:01:02 - INFO - __main__ - Step 460 Global step 460 Train loss 5.77 on epoch=229
06/24/2022 08:01:04 - INFO - __main__ - Step 470 Global step 470 Train loss 5.84 on epoch=234
06/24/2022 08:01:05 - INFO - __main__ - Step 480 Global step 480 Train loss 5.79 on epoch=239
06/24/2022 08:01:06 - INFO - __main__ - Step 490 Global step 490 Train loss 5.55 on epoch=244
06/24/2022 08:01:07 - INFO - __main__ - Step 500 Global step 500 Train loss 5.67 on epoch=249
06/24/2022 08:01:09 - INFO - __main__ - Global step 500 Train loss 5.72 ACC 0.0 on epoch=249
06/24/2022 08:01:10 - INFO - __main__ - Step 510 Global step 510 Train loss 5.45 on epoch=254
06/24/2022 08:01:11 - INFO - __main__ - Step 520 Global step 520 Train loss 5.51 on epoch=259
06/24/2022 08:01:12 - INFO - __main__ - Step 530 Global step 530 Train loss 5.62 on epoch=264
06/24/2022 08:01:14 - INFO - __main__ - Step 540 Global step 540 Train loss 5.54 on epoch=269
06/24/2022 08:01:15 - INFO - __main__ - Step 550 Global step 550 Train loss 5.42 on epoch=274
06/24/2022 08:01:23 - INFO - __main__ - Global step 550 Train loss 5.51 ACC 0.0 on epoch=274
06/24/2022 08:01:24 - INFO - __main__ - Step 560 Global step 560 Train loss 5.28 on epoch=279
06/24/2022 08:01:25 - INFO - __main__ - Step 570 Global step 570 Train loss 5.29 on epoch=284
06/24/2022 08:01:26 - INFO - __main__ - Step 580 Global step 580 Train loss 5.21 on epoch=289
06/24/2022 08:01:27 - INFO - __main__ - Step 590 Global step 590 Train loss 5.17 on epoch=294
06/24/2022 08:01:29 - INFO - __main__ - Step 600 Global step 600 Train loss 4.96 on epoch=299
06/24/2022 08:01:30 - INFO - __main__ - Global step 600 Train loss 5.18 ACC 0.0 on epoch=299
06/24/2022 08:01:32 - INFO - __main__ - Step 610 Global step 610 Train loss 4.96 on epoch=304
06/24/2022 08:01:33 - INFO - __main__ - Step 620 Global step 620 Train loss 4.81 on epoch=309
06/24/2022 08:01:34 - INFO - __main__ - Step 630 Global step 630 Train loss 4.91 on epoch=314
06/24/2022 08:01:35 - INFO - __main__ - Step 640 Global step 640 Train loss 4.62 on epoch=319
06/24/2022 08:01:37 - INFO - __main__ - Step 650 Global step 650 Train loss 4.71 on epoch=324
06/24/2022 08:01:43 - INFO - __main__ - Global step 650 Train loss 4.80 ACC 0.0 on epoch=324
06/24/2022 08:01:44 - INFO - __main__ - Step 660 Global step 660 Train loss 4.48 on epoch=329
06/24/2022 08:01:46 - INFO - __main__ - Step 670 Global step 670 Train loss 4.32 on epoch=334
06/24/2022 08:01:47 - INFO - __main__ - Step 680 Global step 680 Train loss 4.32 on epoch=339
06/24/2022 08:01:48 - INFO - __main__ - Step 690 Global step 690 Train loss 4.29 on epoch=344
06/24/2022 08:01:49 - INFO - __main__ - Step 700 Global step 700 Train loss 3.88 on epoch=349
06/24/2022 08:01:51 - INFO - __main__ - Global step 700 Train loss 4.26 ACC 0.0 on epoch=349
06/24/2022 08:01:52 - INFO - __main__ - Step 710 Global step 710 Train loss 3.80 on epoch=354
06/24/2022 08:01:54 - INFO - __main__ - Step 720 Global step 720 Train loss 3.90 on epoch=359
06/24/2022 08:01:55 - INFO - __main__ - Step 730 Global step 730 Train loss 3.68 on epoch=364
06/24/2022 08:01:56 - INFO - __main__ - Step 740 Global step 740 Train loss 3.70 on epoch=369
06/24/2022 08:01:57 - INFO - __main__ - Step 750 Global step 750 Train loss 3.60 on epoch=374
06/24/2022 08:02:05 - INFO - __main__ - Global step 750 Train loss 3.74 ACC 0.5 on epoch=374
06/24/2022 08:02:05 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.5 on epoch=374, global_step=750
06/24/2022 08:02:07 - INFO - __main__ - Step 760 Global step 760 Train loss 3.35 on epoch=379
06/24/2022 08:02:08 - INFO - __main__ - Step 770 Global step 770 Train loss 3.10 on epoch=384
06/24/2022 08:02:09 - INFO - __main__ - Step 780 Global step 780 Train loss 2.99 on epoch=389
06/24/2022 08:02:10 - INFO - __main__ - Step 790 Global step 790 Train loss 3.00 on epoch=394
06/24/2022 08:02:12 - INFO - __main__ - Step 800 Global step 800 Train loss 2.66 on epoch=399
06/24/2022 08:02:21 - INFO - __main__ - Global step 800 Train loss 3.02 ACC 0.5 on epoch=399
06/24/2022 08:02:22 - INFO - __main__ - Step 810 Global step 810 Train loss 2.60 on epoch=404
06/24/2022 08:02:23 - INFO - __main__ - Step 820 Global step 820 Train loss 2.70 on epoch=409
06/24/2022 08:02:25 - INFO - __main__ - Step 830 Global step 830 Train loss 2.55 on epoch=414
06/24/2022 08:02:26 - INFO - __main__ - Step 840 Global step 840 Train loss 2.44 on epoch=419
06/24/2022 08:02:27 - INFO - __main__ - Step 850 Global step 850 Train loss 2.45 on epoch=424
06/24/2022 08:02:36 - INFO - __main__ - Global step 850 Train loss 2.55 ACC 0.5 on epoch=424
06/24/2022 08:02:37 - INFO - __main__ - Step 860 Global step 860 Train loss 2.30 on epoch=429
06/24/2022 08:02:38 - INFO - __main__ - Step 870 Global step 870 Train loss 2.24 on epoch=434
06/24/2022 08:02:40 - INFO - __main__ - Step 880 Global step 880 Train loss 2.17 on epoch=439
06/24/2022 08:02:41 - INFO - __main__ - Step 890 Global step 890 Train loss 1.98 on epoch=444
06/24/2022 08:02:42 - INFO - __main__ - Step 900 Global step 900 Train loss 1.92 on epoch=449
06/24/2022 08:02:50 - INFO - __main__ - Global step 900 Train loss 2.12 ACC 0.5 on epoch=449
06/24/2022 08:02:51 - INFO - __main__ - Step 910 Global step 910 Train loss 1.99 on epoch=454
06/24/2022 08:02:52 - INFO - __main__ - Step 920 Global step 920 Train loss 1.81 on epoch=459
06/24/2022 08:02:53 - INFO - __main__ - Step 930 Global step 930 Train loss 1.71 on epoch=464
06/24/2022 08:02:55 - INFO - __main__ - Step 940 Global step 940 Train loss 1.87 on epoch=469
06/24/2022 08:02:56 - INFO - __main__ - Step 950 Global step 950 Train loss 1.59 on epoch=474
06/24/2022 08:03:05 - INFO - __main__ - Global step 950 Train loss 1.79 ACC 0.5625 on epoch=474
06/24/2022 08:03:05 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=474, global_step=950
06/24/2022 08:03:07 - INFO - __main__ - Step 960 Global step 960 Train loss 1.77 on epoch=479
06/24/2022 08:03:08 - INFO - __main__ - Step 970 Global step 970 Train loss 1.58 on epoch=484
06/24/2022 08:03:09 - INFO - __main__ - Step 980 Global step 980 Train loss 1.56 on epoch=489
06/24/2022 08:03:10 - INFO - __main__ - Step 990 Global step 990 Train loss 1.57 on epoch=494
06/24/2022 08:03:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.55 on epoch=499
06/24/2022 08:03:12 - INFO - __main__ - Global step 1000 Train loss 1.61 ACC 0.5625 on epoch=499
06/24/2022 08:03:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.56 on epoch=504
06/24/2022 08:03:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.45 on epoch=509
06/24/2022 08:03:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.30 on epoch=514
06/24/2022 08:03:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.38 on epoch=519
06/24/2022 08:03:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.20 on epoch=524
06/24/2022 08:03:23 - INFO - __main__ - Global step 1050 Train loss 1.38 ACC 0.53125 on epoch=524
06/24/2022 08:03:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.31 on epoch=529
06/24/2022 08:03:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.38 on epoch=534
06/24/2022 08:03:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.28 on epoch=539
06/24/2022 08:03:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.36 on epoch=544
06/24/2022 08:03:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.37 on epoch=549
06/24/2022 08:03:30 - INFO - __main__ - Global step 1100 Train loss 1.34 ACC 0.4375 on epoch=549
06/24/2022 08:03:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.40 on epoch=554
06/24/2022 08:03:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.36 on epoch=559
06/24/2022 08:03:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.17 on epoch=564
06/24/2022 08:03:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.20 on epoch=569
06/24/2022 08:03:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.12 on epoch=574
06/24/2022 08:03:42 - INFO - __main__ - Global step 1150 Train loss 1.25 ACC 0.59375 on epoch=574
06/24/2022 08:03:42 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=574, global_step=1150
06/24/2022 08:03:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.29 on epoch=579
06/24/2022 08:03:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.05 on epoch=584
06/24/2022 08:03:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.98 on epoch=589
06/24/2022 08:03:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.14 on epoch=594
06/24/2022 08:03:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.12 on epoch=599
06/24/2022 08:03:54 - INFO - __main__ - Global step 1200 Train loss 1.12 ACC 0.5 on epoch=599
06/24/2022 08:03:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.12 on epoch=604
06/24/2022 08:03:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.08 on epoch=609
06/24/2022 08:03:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.02 on epoch=614
06/24/2022 08:03:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.97 on epoch=619
06/24/2022 08:04:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.11 on epoch=624
06/24/2022 08:04:04 - INFO - __main__ - Global step 1250 Train loss 1.06 ACC 0.5 on epoch=624
06/24/2022 08:04:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.92 on epoch=629
06/24/2022 08:04:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.01 on epoch=634
06/24/2022 08:04:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.85 on epoch=639
06/24/2022 08:04:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.02 on epoch=644
06/24/2022 08:04:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.04 on epoch=649
06/24/2022 08:04:13 - INFO - __main__ - Global step 1300 Train loss 0.97 ACC 0.5 on epoch=649
06/24/2022 08:04:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.83 on epoch=654
06/24/2022 08:04:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.93 on epoch=659
06/24/2022 08:04:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.94 on epoch=664
06/24/2022 08:04:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.94 on epoch=669
06/24/2022 08:04:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.94 on epoch=674
06/24/2022 08:04:26 - INFO - __main__ - Global step 1350 Train loss 0.91 ACC 0.5 on epoch=674
06/24/2022 08:04:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.94 on epoch=679
06/24/2022 08:04:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.89 on epoch=684
06/24/2022 08:04:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.84 on epoch=689
06/24/2022 08:04:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.00 on epoch=694
06/24/2022 08:04:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.79 on epoch=699
06/24/2022 08:04:38 - INFO - __main__ - Global step 1400 Train loss 0.89 ACC 0.5 on epoch=699
06/24/2022 08:04:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.81 on epoch=704
06/24/2022 08:04:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.75 on epoch=709
06/24/2022 08:04:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.80 on epoch=714
06/24/2022 08:04:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.84 on epoch=719
06/24/2022 08:04:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.80 on epoch=724
06/24/2022 08:04:46 - INFO - __main__ - Global step 1450 Train loss 0.80 ACC 0.5 on epoch=724
06/24/2022 08:04:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.85 on epoch=729
06/24/2022 08:04:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.76 on epoch=734
06/24/2022 08:04:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.80 on epoch=739
06/24/2022 08:04:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.80 on epoch=744
06/24/2022 08:04:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.74 on epoch=749
06/24/2022 08:04:53 - INFO - __main__ - Global step 1500 Train loss 0.79 ACC 0.5 on epoch=749
06/24/2022 08:04:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.78 on epoch=754
06/24/2022 08:04:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.78 on epoch=759
06/24/2022 08:04:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.72 on epoch=764
06/24/2022 08:04:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.80 on epoch=769
06/24/2022 08:04:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.80 on epoch=774
06/24/2022 08:04:59 - INFO - __main__ - Global step 1550 Train loss 0.78 ACC 0.5 on epoch=774
06/24/2022 08:05:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.74 on epoch=779
06/24/2022 08:05:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.63 on epoch=784
06/24/2022 08:05:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.74 on epoch=789
06/24/2022 08:05:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.74 on epoch=794
06/24/2022 08:05:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.82 on epoch=799
06/24/2022 08:05:06 - INFO - __main__ - Global step 1600 Train loss 0.73 ACC 0.5 on epoch=799
06/24/2022 08:05:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.70 on epoch=804
06/24/2022 08:05:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.64 on epoch=809
06/24/2022 08:05:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.61 on epoch=814
06/24/2022 08:05:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.63 on epoch=819
06/24/2022 08:05:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.68 on epoch=824
06/24/2022 08:05:12 - INFO - __main__ - Global step 1650 Train loss 0.65 ACC 0.5 on epoch=824
06/24/2022 08:05:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.66 on epoch=829
06/24/2022 08:05:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.75 on epoch=834
06/24/2022 08:05:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.74 on epoch=839
06/24/2022 08:05:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.72 on epoch=844
06/24/2022 08:05:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.67 on epoch=849
06/24/2022 08:05:19 - INFO - __main__ - Global step 1700 Train loss 0.71 ACC 0.5 on epoch=849
06/24/2022 08:05:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.64 on epoch=854
06/24/2022 08:05:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.62 on epoch=859
06/24/2022 08:05:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.59 on epoch=864
06/24/2022 08:05:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.71 on epoch=869
06/24/2022 08:05:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.66 on epoch=874
06/24/2022 08:05:26 - INFO - __main__ - Global step 1750 Train loss 0.64 ACC 0.5 on epoch=874
06/24/2022 08:05:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.65 on epoch=879
06/24/2022 08:05:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.71 on epoch=884
06/24/2022 08:05:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.64 on epoch=889
06/24/2022 08:05:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.59 on epoch=894
06/24/2022 08:05:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.62 on epoch=899
06/24/2022 08:05:32 - INFO - __main__ - Global step 1800 Train loss 0.64 ACC 0.5 on epoch=899
06/24/2022 08:05:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.57 on epoch=904
06/24/2022 08:05:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.69 on epoch=909
06/24/2022 08:05:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.56 on epoch=914
06/24/2022 08:05:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.66 on epoch=919
06/24/2022 08:05:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.63 on epoch=924
06/24/2022 08:05:39 - INFO - __main__ - Global step 1850 Train loss 0.62 ACC 0.53125 on epoch=924
06/24/2022 08:05:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.56 on epoch=929
06/24/2022 08:05:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.66 on epoch=934
06/24/2022 08:05:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.57 on epoch=939
06/24/2022 08:05:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.59 on epoch=944
06/24/2022 08:05:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.67 on epoch=949
06/24/2022 08:05:45 - INFO - __main__ - Global step 1900 Train loss 0.61 ACC 0.5625 on epoch=949
06/24/2022 08:05:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.63 on epoch=954
06/24/2022 08:05:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.56 on epoch=959
06/24/2022 08:05:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.62 on epoch=964
06/24/2022 08:05:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.57 on epoch=969
06/24/2022 08:05:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.59 on epoch=974
06/24/2022 08:05:52 - INFO - __main__ - Global step 1950 Train loss 0.59 ACC 0.53125 on epoch=974
06/24/2022 08:05:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.58 on epoch=979
06/24/2022 08:05:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.66 on epoch=984
06/24/2022 08:05:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.58 on epoch=989
06/24/2022 08:05:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.57 on epoch=994
06/24/2022 08:05:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.51 on epoch=999
06/24/2022 08:05:59 - INFO - __main__ - Global step 2000 Train loss 0.58 ACC 0.53125 on epoch=999
06/24/2022 08:05:59 - INFO - __main__ - save last model!
06/24/2022 08:05:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 08:05:59 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 08:05:59 - INFO - __main__ - Printing 3 examples
06/24/2022 08:05:59 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 08:05:59 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:05:59 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 08:05:59 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:05:59 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 08:05:59 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:05:59 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:05:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:06:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:06:00 - INFO - __main__ - Printing 3 examples
06/24/2022 08:06:00 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 08:06:00 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:06:00 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 08:06:00 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:06:00 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 08:06:00 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:06:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 08:06:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:06:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:06:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:06:00 - INFO - __main__ - Printing 3 examples
06/24/2022 08:06:00 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 08:06:00 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:06:00 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 08:06:00 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:06:00 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 08:06:00 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:06:00 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:06:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:06:00 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:06:00 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 08:06:05 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:06:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:06:05 - INFO - __main__ - Starting training!
06/24/2022 08:06:07 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_100_0.3_8_predictions.txt
06/24/2022 08:06:07 - INFO - __main__ - ACC on test data: 0.4754
06/24/2022 08:06:07 - INFO - __main__ - prefix=medical_questions_pairs_16_100, lr=0.3, bsz=8, dev_performance=0.59375, test_performance=0.47540983606557374
06/24/2022 08:06:07 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_100, lr=0.2, bsz=8 ...
06/24/2022 08:06:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:06:08 - INFO - __main__ - Printing 3 examples
06/24/2022 08:06:08 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 08:06:08 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:06:08 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 08:06:08 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:06:08 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 08:06:08 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:06:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 08:06:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:06:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:06:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:06:08 - INFO - __main__ - Printing 3 examples
06/24/2022 08:06:08 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 08:06:08 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:06:08 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 08:06:08 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:06:08 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 08:06:08 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:06:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:06:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:06:08 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:06:15 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:06:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:06:15 - INFO - __main__ - Starting training!
06/24/2022 08:06:16 - INFO - __main__ - Step 10 Global step 10 Train loss 8.41 on epoch=4
06/24/2022 08:06:18 - INFO - __main__ - Step 20 Global step 20 Train loss 8.40 on epoch=9
06/24/2022 08:06:19 - INFO - __main__ - Step 30 Global step 30 Train loss 8.41 on epoch=14
06/24/2022 08:06:20 - INFO - __main__ - Step 40 Global step 40 Train loss 8.36 on epoch=19
06/24/2022 08:06:21 - INFO - __main__ - Step 50 Global step 50 Train loss 8.31 on epoch=24
06/24/2022 08:06:24 - INFO - __main__ - Global step 50 Train loss 8.38 ACC 0.0 on epoch=24
06/24/2022 08:06:24 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 08:06:25 - INFO - __main__ - Step 60 Global step 60 Train loss 8.31 on epoch=29
06/24/2022 08:06:26 - INFO - __main__ - Step 70 Global step 70 Train loss 8.25 on epoch=34
06/24/2022 08:06:28 - INFO - __main__ - Step 80 Global step 80 Train loss 8.27 on epoch=39
06/24/2022 08:06:29 - INFO - __main__ - Step 90 Global step 90 Train loss 8.19 on epoch=44
06/24/2022 08:06:30 - INFO - __main__ - Step 100 Global step 100 Train loss 8.19 on epoch=49
06/24/2022 08:06:37 - INFO - __main__ - Global step 100 Train loss 8.24 ACC 0.0 on epoch=49
06/24/2022 08:06:38 - INFO - __main__ - Step 110 Global step 110 Train loss 8.18 on epoch=54
06/24/2022 08:06:40 - INFO - __main__ - Step 120 Global step 120 Train loss 8.17 on epoch=59
06/24/2022 08:06:41 - INFO - __main__ - Step 130 Global step 130 Train loss 8.14 on epoch=64
06/24/2022 08:06:42 - INFO - __main__ - Step 140 Global step 140 Train loss 8.15 on epoch=69
06/24/2022 08:06:44 - INFO - __main__ - Step 150 Global step 150 Train loss 8.07 on epoch=74
06/24/2022 08:06:45 - INFO - __main__ - Global step 150 Train loss 8.14 ACC 0.0 on epoch=74
06/24/2022 08:06:47 - INFO - __main__ - Step 160 Global step 160 Train loss 8.06 on epoch=79
06/24/2022 08:06:48 - INFO - __main__ - Step 170 Global step 170 Train loss 7.93 on epoch=84
06/24/2022 08:06:49 - INFO - __main__ - Step 180 Global step 180 Train loss 8.05 on epoch=89
06/24/2022 08:06:51 - INFO - __main__ - Step 190 Global step 190 Train loss 7.97 on epoch=94
06/24/2022 08:06:52 - INFO - __main__ - Step 200 Global step 200 Train loss 8.02 on epoch=99
06/24/2022 08:06:58 - INFO - __main__ - Global step 200 Train loss 8.01 ACC 0.0 on epoch=99
06/24/2022 08:06:59 - INFO - __main__ - Step 210 Global step 210 Train loss 7.94 on epoch=104
06/24/2022 08:07:01 - INFO - __main__ - Step 220 Global step 220 Train loss 8.01 on epoch=109
06/24/2022 08:07:02 - INFO - __main__ - Step 230 Global step 230 Train loss 7.97 on epoch=114
06/24/2022 08:07:03 - INFO - __main__ - Step 240 Global step 240 Train loss 7.93 on epoch=119
06/24/2022 08:07:04 - INFO - __main__ - Step 250 Global step 250 Train loss 7.79 on epoch=124
06/24/2022 08:07:08 - INFO - __main__ - Global step 250 Train loss 7.93 ACC 0.0 on epoch=124
06/24/2022 08:07:09 - INFO - __main__ - Step 260 Global step 260 Train loss 7.84 on epoch=129
06/24/2022 08:07:10 - INFO - __main__ - Step 270 Global step 270 Train loss 7.78 on epoch=134
06/24/2022 08:07:12 - INFO - __main__ - Step 280 Global step 280 Train loss 7.83 on epoch=139
06/24/2022 08:07:13 - INFO - __main__ - Step 290 Global step 290 Train loss 7.89 on epoch=144
06/24/2022 08:07:14 - INFO - __main__ - Step 300 Global step 300 Train loss 7.71 on epoch=149
06/24/2022 08:07:17 - INFO - __main__ - Global step 300 Train loss 7.81 ACC 0.0 on epoch=149
06/24/2022 08:07:18 - INFO - __main__ - Step 310 Global step 310 Train loss 7.81 on epoch=154
06/24/2022 08:07:19 - INFO - __main__ - Step 320 Global step 320 Train loss 7.69 on epoch=159
06/24/2022 08:07:21 - INFO - __main__ - Step 330 Global step 330 Train loss 7.64 on epoch=164
06/24/2022 08:07:22 - INFO - __main__ - Step 340 Global step 340 Train loss 7.69 on epoch=169
06/24/2022 08:07:23 - INFO - __main__ - Step 350 Global step 350 Train loss 7.56 on epoch=174
06/24/2022 08:07:27 - INFO - __main__ - Global step 350 Train loss 7.68 ACC 0.0 on epoch=174
06/24/2022 08:07:28 - INFO - __main__ - Step 360 Global step 360 Train loss 7.57 on epoch=179
06/24/2022 08:07:30 - INFO - __main__ - Step 370 Global step 370 Train loss 7.59 on epoch=184
06/24/2022 08:07:31 - INFO - __main__ - Step 380 Global step 380 Train loss 7.55 on epoch=189
06/24/2022 08:07:32 - INFO - __main__ - Step 390 Global step 390 Train loss 7.48 on epoch=194
06/24/2022 08:07:34 - INFO - __main__ - Step 400 Global step 400 Train loss 7.38 on epoch=199
06/24/2022 08:07:38 - INFO - __main__ - Global step 400 Train loss 7.51 ACC 0.0 on epoch=199
06/24/2022 08:07:39 - INFO - __main__ - Step 410 Global step 410 Train loss 7.25 on epoch=204
06/24/2022 08:07:41 - INFO - __main__ - Step 420 Global step 420 Train loss 7.34 on epoch=209
06/24/2022 08:07:42 - INFO - __main__ - Step 430 Global step 430 Train loss 7.32 on epoch=214
06/24/2022 08:07:43 - INFO - __main__ - Step 440 Global step 440 Train loss 7.32 on epoch=219
06/24/2022 08:07:45 - INFO - __main__ - Step 450 Global step 450 Train loss 7.38 on epoch=224
06/24/2022 08:07:51 - INFO - __main__ - Global step 450 Train loss 7.32 ACC 0.0 on epoch=224
06/24/2022 08:07:52 - INFO - __main__ - Step 460 Global step 460 Train loss 7.01 on epoch=229
06/24/2022 08:07:53 - INFO - __main__ - Step 470 Global step 470 Train loss 7.06 on epoch=234
06/24/2022 08:07:54 - INFO - __main__ - Step 480 Global step 480 Train loss 7.10 on epoch=239
06/24/2022 08:07:56 - INFO - __main__ - Step 490 Global step 490 Train loss 7.06 on epoch=244
06/24/2022 08:07:57 - INFO - __main__ - Step 500 Global step 500 Train loss 6.91 on epoch=249
06/24/2022 08:07:59 - INFO - __main__ - Global step 500 Train loss 7.03 ACC 0.0 on epoch=249
06/24/2022 08:08:00 - INFO - __main__ - Step 510 Global step 510 Train loss 7.09 on epoch=254
06/24/2022 08:08:01 - INFO - __main__ - Step 520 Global step 520 Train loss 7.07 on epoch=259
06/24/2022 08:08:03 - INFO - __main__ - Step 530 Global step 530 Train loss 6.95 on epoch=264
06/24/2022 08:08:04 - INFO - __main__ - Step 540 Global step 540 Train loss 6.90 on epoch=269
06/24/2022 08:08:05 - INFO - __main__ - Step 550 Global step 550 Train loss 6.66 on epoch=274
06/24/2022 08:08:09 - INFO - __main__ - Global step 550 Train loss 6.93 ACC 0.0 on epoch=274
06/24/2022 08:08:10 - INFO - __main__ - Step 560 Global step 560 Train loss 6.70 on epoch=279
06/24/2022 08:08:11 - INFO - __main__ - Step 570 Global step 570 Train loss 6.66 on epoch=284
06/24/2022 08:08:13 - INFO - __main__ - Step 580 Global step 580 Train loss 6.56 on epoch=289
06/24/2022 08:08:14 - INFO - __main__ - Step 590 Global step 590 Train loss 6.54 on epoch=294
06/24/2022 08:08:15 - INFO - __main__ - Step 600 Global step 600 Train loss 6.56 on epoch=299
06/24/2022 08:08:22 - INFO - __main__ - Global step 600 Train loss 6.60 ACC 0.0 on epoch=299
06/24/2022 08:08:23 - INFO - __main__ - Step 610 Global step 610 Train loss 6.47 on epoch=304
06/24/2022 08:08:24 - INFO - __main__ - Step 620 Global step 620 Train loss 6.43 on epoch=309
06/24/2022 08:08:25 - INFO - __main__ - Step 630 Global step 630 Train loss 6.29 on epoch=314
06/24/2022 08:08:27 - INFO - __main__ - Step 640 Global step 640 Train loss 6.25 on epoch=319
06/24/2022 08:08:28 - INFO - __main__ - Step 650 Global step 650 Train loss 6.42 on epoch=324
06/24/2022 08:08:32 - INFO - __main__ - Global step 650 Train loss 6.37 ACC 0.0 on epoch=324
06/24/2022 08:08:33 - INFO - __main__ - Step 660 Global step 660 Train loss 6.37 on epoch=329
06/24/2022 08:08:34 - INFO - __main__ - Step 670 Global step 670 Train loss 6.18 on epoch=334
06/24/2022 08:08:36 - INFO - __main__ - Step 680 Global step 680 Train loss 6.24 on epoch=339
06/24/2022 08:08:37 - INFO - __main__ - Step 690 Global step 690 Train loss 6.28 on epoch=344
06/24/2022 08:08:38 - INFO - __main__ - Step 700 Global step 700 Train loss 6.18 on epoch=349
06/24/2022 08:08:43 - INFO - __main__ - Global step 700 Train loss 6.25 ACC 0.0 on epoch=349
06/24/2022 08:08:44 - INFO - __main__ - Step 710 Global step 710 Train loss 6.07 on epoch=354
06/24/2022 08:08:45 - INFO - __main__ - Step 720 Global step 720 Train loss 6.08 on epoch=359
06/24/2022 08:08:47 - INFO - __main__ - Step 730 Global step 730 Train loss 6.21 on epoch=364
06/24/2022 08:08:48 - INFO - __main__ - Step 740 Global step 740 Train loss 6.04 on epoch=369
06/24/2022 08:08:49 - INFO - __main__ - Step 750 Global step 750 Train loss 6.01 on epoch=374
06/24/2022 08:08:54 - INFO - __main__ - Global step 750 Train loss 6.08 ACC 0.0 on epoch=374
06/24/2022 08:08:56 - INFO - __main__ - Step 760 Global step 760 Train loss 5.98 on epoch=379
06/24/2022 08:08:57 - INFO - __main__ - Step 770 Global step 770 Train loss 5.96 on epoch=384
06/24/2022 08:08:58 - INFO - __main__ - Step 780 Global step 780 Train loss 5.91 on epoch=389
06/24/2022 08:08:59 - INFO - __main__ - Step 790 Global step 790 Train loss 5.84 on epoch=394
06/24/2022 08:09:01 - INFO - __main__ - Step 800 Global step 800 Train loss 5.73 on epoch=399
06/24/2022 08:09:08 - INFO - __main__ - Global step 800 Train loss 5.88 ACC 0.0 on epoch=399
06/24/2022 08:09:09 - INFO - __main__ - Step 810 Global step 810 Train loss 5.65 on epoch=404
06/24/2022 08:09:11 - INFO - __main__ - Step 820 Global step 820 Train loss 5.74 on epoch=409
06/24/2022 08:09:12 - INFO - __main__ - Step 830 Global step 830 Train loss 5.69 on epoch=414
06/24/2022 08:09:13 - INFO - __main__ - Step 840 Global step 840 Train loss 5.53 on epoch=419
06/24/2022 08:09:15 - INFO - __main__ - Step 850 Global step 850 Train loss 5.63 on epoch=424
06/24/2022 08:09:18 - INFO - __main__ - Global step 850 Train loss 5.65 ACC 0.0 on epoch=424
06/24/2022 08:09:20 - INFO - __main__ - Step 860 Global step 860 Train loss 5.48 on epoch=429
06/24/2022 08:09:21 - INFO - __main__ - Step 870 Global step 870 Train loss 5.43 on epoch=434
06/24/2022 08:09:22 - INFO - __main__ - Step 880 Global step 880 Train loss 5.34 on epoch=439
06/24/2022 08:09:23 - INFO - __main__ - Step 890 Global step 890 Train loss 5.38 on epoch=444
06/24/2022 08:09:25 - INFO - __main__ - Step 900 Global step 900 Train loss 5.29 on epoch=449
06/24/2022 08:09:35 - INFO - __main__ - Global step 900 Train loss 5.38 ACC 0.0 on epoch=449
06/24/2022 08:09:37 - INFO - __main__ - Step 910 Global step 910 Train loss 5.21 on epoch=454
06/24/2022 08:09:38 - INFO - __main__ - Step 920 Global step 920 Train loss 5.21 on epoch=459
06/24/2022 08:09:39 - INFO - __main__ - Step 930 Global step 930 Train loss 5.11 on epoch=464
06/24/2022 08:09:41 - INFO - __main__ - Step 940 Global step 940 Train loss 5.18 on epoch=469
06/24/2022 08:09:42 - INFO - __main__ - Step 950 Global step 950 Train loss 5.21 on epoch=474
06/24/2022 08:09:53 - INFO - __main__ - Global step 950 Train loss 5.18 ACC 0.0 on epoch=474
06/24/2022 08:09:54 - INFO - __main__ - Step 960 Global step 960 Train loss 4.99 on epoch=479
06/24/2022 08:09:55 - INFO - __main__ - Step 970 Global step 970 Train loss 4.91 on epoch=484
06/24/2022 08:09:57 - INFO - __main__ - Step 980 Global step 980 Train loss 4.82 on epoch=489
06/24/2022 08:09:58 - INFO - __main__ - Step 990 Global step 990 Train loss 4.65 on epoch=494
06/24/2022 08:09:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 4.65 on epoch=499
06/24/2022 08:10:07 - INFO - __main__ - Global step 1000 Train loss 4.80 ACC 0.0 on epoch=499
06/24/2022 08:10:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 4.68 on epoch=504
06/24/2022 08:10:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 4.58 on epoch=509
06/24/2022 08:10:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 4.39 on epoch=514
06/24/2022 08:10:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 4.48 on epoch=519
06/24/2022 08:10:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 4.54 on epoch=524
06/24/2022 08:10:20 - INFO - __main__ - Global step 1050 Train loss 4.53 ACC 0.0 on epoch=524
06/24/2022 08:10:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 4.14 on epoch=529
06/24/2022 08:10:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 4.19 on epoch=534
06/24/2022 08:10:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 4.13 on epoch=539
06/24/2022 08:10:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 4.20 on epoch=544
06/24/2022 08:10:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 4.01 on epoch=549
06/24/2022 08:10:34 - INFO - __main__ - Global step 1100 Train loss 4.13 ACC 0.0 on epoch=549
06/24/2022 08:10:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 4.00 on epoch=554
06/24/2022 08:10:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.89 on epoch=559
06/24/2022 08:10:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.87 on epoch=564
06/24/2022 08:10:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.82 on epoch=569
06/24/2022 08:10:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.84 on epoch=574
06/24/2022 08:10:47 - INFO - __main__ - Global step 1150 Train loss 3.89 ACC 0.0 on epoch=574
06/24/2022 08:10:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.73 on epoch=579
06/24/2022 08:10:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.72 on epoch=584
06/24/2022 08:10:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.77 on epoch=589
06/24/2022 08:10:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.69 on epoch=594
06/24/2022 08:10:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.53 on epoch=599
06/24/2022 08:11:04 - INFO - __main__ - Global step 1200 Train loss 3.69 ACC 0.0625 on epoch=599
06/24/2022 08:11:04 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=599, global_step=1200
06/24/2022 08:11:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.63 on epoch=604
06/24/2022 08:11:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.55 on epoch=609
06/24/2022 08:11:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.34 on epoch=614
06/24/2022 08:11:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.44 on epoch=619
06/24/2022 08:11:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.51 on epoch=624
06/24/2022 08:11:16 - INFO - __main__ - Global step 1250 Train loss 3.49 ACC 0.15625 on epoch=624
06/24/2022 08:11:16 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.15625 on epoch=624, global_step=1250
06/24/2022 08:11:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.50 on epoch=629
06/24/2022 08:11:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.39 on epoch=634
06/24/2022 08:11:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.27 on epoch=639
06/24/2022 08:11:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.41 on epoch=644
06/24/2022 08:11:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.23 on epoch=649
06/24/2022 08:11:29 - INFO - __main__ - Global step 1300 Train loss 3.36 ACC 0.125 on epoch=649
06/24/2022 08:11:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.08 on epoch=654
06/24/2022 08:11:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.05 on epoch=659
06/24/2022 08:11:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 3.14 on epoch=664
06/24/2022 08:11:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.99 on epoch=669
06/24/2022 08:11:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.92 on epoch=674
06/24/2022 08:11:42 - INFO - __main__ - Global step 1350 Train loss 3.04 ACC 0.25 on epoch=674
06/24/2022 08:11:42 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.25 on epoch=674, global_step=1350
06/24/2022 08:11:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 3.09 on epoch=679
06/24/2022 08:11:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 3.17 on epoch=684
06/24/2022 08:11:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.95 on epoch=689
06/24/2022 08:11:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.86 on epoch=694
06/24/2022 08:11:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.82 on epoch=699
06/24/2022 08:11:54 - INFO - __main__ - Global step 1400 Train loss 2.98 ACC 0.3125 on epoch=699
06/24/2022 08:11:54 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.3125 on epoch=699, global_step=1400
06/24/2022 08:11:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.75 on epoch=704
06/24/2022 08:11:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.81 on epoch=709
06/24/2022 08:11:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.87 on epoch=714
06/24/2022 08:12:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.68 on epoch=719
06/24/2022 08:12:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.72 on epoch=724
06/24/2022 08:12:11 - INFO - __main__ - Global step 1450 Train loss 2.77 ACC 0.40625 on epoch=724
06/24/2022 08:12:11 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.40625 on epoch=724, global_step=1450
06/24/2022 08:12:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.55 on epoch=729
06/24/2022 08:12:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.55 on epoch=734
06/24/2022 08:12:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.70 on epoch=739
06/24/2022 08:12:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.62 on epoch=744
06/24/2022 08:12:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.47 on epoch=749
06/24/2022 08:12:28 - INFO - __main__ - Global step 1500 Train loss 2.58 ACC 0.46875 on epoch=749
06/24/2022 08:12:28 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=749, global_step=1500
06/24/2022 08:12:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.58 on epoch=754
06/24/2022 08:12:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.41 on epoch=759
06/24/2022 08:12:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 2.47 on epoch=764
06/24/2022 08:12:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 2.46 on epoch=769
06/24/2022 08:12:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 2.28 on epoch=774
06/24/2022 08:12:45 - INFO - __main__ - Global step 1550 Train loss 2.44 ACC 0.46875 on epoch=774
06/24/2022 08:12:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 2.41 on epoch=779
06/24/2022 08:12:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 2.26 on epoch=784
06/24/2022 08:12:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 2.25 on epoch=789
06/24/2022 08:12:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 2.29 on epoch=794
06/24/2022 08:12:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 2.14 on epoch=799
06/24/2022 08:13:01 - INFO - __main__ - Global step 1600 Train loss 2.27 ACC 0.5 on epoch=799
06/24/2022 08:13:01 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=799, global_step=1600
06/24/2022 08:13:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 2.24 on epoch=804
06/24/2022 08:13:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.23 on epoch=809
06/24/2022 08:13:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 2.20 on epoch=814
06/24/2022 08:13:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 2.26 on epoch=819
06/24/2022 08:13:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 2.20 on epoch=824
06/24/2022 08:13:18 - INFO - __main__ - Global step 1650 Train loss 2.23 ACC 0.5 on epoch=824
06/24/2022 08:13:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 2.05 on epoch=829
06/24/2022 08:13:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 2.25 on epoch=834
06/24/2022 08:13:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 2.02 on epoch=839
06/24/2022 08:13:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 2.16 on epoch=844
06/24/2022 08:13:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.97 on epoch=849
06/24/2022 08:13:34 - INFO - __main__ - Global step 1700 Train loss 2.09 ACC 0.46875 on epoch=849
06/24/2022 08:13:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.93 on epoch=854
06/24/2022 08:13:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 2.05 on epoch=859
06/24/2022 08:13:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 2.08 on epoch=864
06/24/2022 08:13:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.92 on epoch=869
06/24/2022 08:13:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.80 on epoch=874
06/24/2022 08:13:51 - INFO - __main__ - Global step 1750 Train loss 1.96 ACC 0.46875 on epoch=874
06/24/2022 08:13:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.79 on epoch=879
06/24/2022 08:13:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.71 on epoch=884
06/24/2022 08:13:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.86 on epoch=889
06/24/2022 08:13:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.92 on epoch=894
06/24/2022 08:13:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.68 on epoch=899
06/24/2022 08:14:08 - INFO - __main__ - Global step 1800 Train loss 1.79 ACC 0.5 on epoch=899
06/24/2022 08:14:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.81 on epoch=904
06/24/2022 08:14:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.68 on epoch=909
06/24/2022 08:14:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.74 on epoch=914
06/24/2022 08:14:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.74 on epoch=919
06/24/2022 08:14:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.53 on epoch=924
06/24/2022 08:14:24 - INFO - __main__ - Global step 1850 Train loss 1.70 ACC 0.53125 on epoch=924
06/24/2022 08:14:25 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=924, global_step=1850
06/24/2022 08:14:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.68 on epoch=929
06/24/2022 08:14:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.60 on epoch=934
06/24/2022 08:14:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.62 on epoch=939
06/24/2022 08:14:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.39 on epoch=944
06/24/2022 08:14:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.59 on epoch=949
06/24/2022 08:14:41 - INFO - __main__ - Global step 1900 Train loss 1.58 ACC 0.5 on epoch=949
06/24/2022 08:14:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.49 on epoch=954
06/24/2022 08:14:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.43 on epoch=959
06/24/2022 08:14:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.29 on epoch=964
06/24/2022 08:14:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.33 on epoch=969
06/24/2022 08:14:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.29 on epoch=974
06/24/2022 08:14:58 - INFO - __main__ - Global step 1950 Train loss 1.37 ACC 0.4375 on epoch=974
06/24/2022 08:14:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.24 on epoch=979
06/24/2022 08:15:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.31 on epoch=984
06/24/2022 08:15:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.21 on epoch=989
06/24/2022 08:15:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.26 on epoch=994
06/24/2022 08:15:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.21 on epoch=999
06/24/2022 08:15:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:15:05 - INFO - __main__ - Printing 3 examples
06/24/2022 08:15:05 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 08:15:05 - INFO - __main__ - ['Similar']
06/24/2022 08:15:05 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 08:15:05 - INFO - __main__ - ['Similar']
06/24/2022 08:15:05 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 08:15:05 - INFO - __main__ - ['Similar']
06/24/2022 08:15:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 08:15:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:15:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:15:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:15:05 - INFO - __main__ - Printing 3 examples
06/24/2022 08:15:05 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 08:15:05 - INFO - __main__ - ['Similar']
06/24/2022 08:15:05 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 08:15:05 - INFO - __main__ - ['Similar']
06/24/2022 08:15:05 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 08:15:05 - INFO - __main__ - ['Similar']
06/24/2022 08:15:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:15:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:15:05 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:15:10 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:15:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:15:11 - INFO - __main__ - Starting training!
06/24/2022 08:15:14 - INFO - __main__ - Global step 2000 Train loss 1.25 ACC 0.46875 on epoch=999
06/24/2022 08:15:14 - INFO - __main__ - save last model!
06/24/2022 08:15:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 08:15:14 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 08:15:14 - INFO - __main__ - Printing 3 examples
06/24/2022 08:15:14 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 08:15:14 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:15:14 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 08:15:14 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:15:14 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 08:15:14 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:15:14 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:15:15 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:15:15 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 08:18:33 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_100_0.2_8_predictions.txt
06/24/2022 08:18:33 - INFO - __main__ - ACC on test data: 0.4754
06/24/2022 08:18:34 - INFO - __main__ - prefix=medical_questions_pairs_16_100, lr=0.2, bsz=8, dev_performance=0.53125, test_performance=0.47540983606557374
06/24/2022 08:18:34 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_13, lr=0.5, bsz=8 ...
06/24/2022 08:18:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:18:35 - INFO - __main__ - Printing 3 examples
06/24/2022 08:18:35 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 08:18:35 - INFO - __main__ - ['Similar']
06/24/2022 08:18:35 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 08:18:35 - INFO - __main__ - ['Similar']
06/24/2022 08:18:35 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 08:18:35 - INFO - __main__ - ['Similar']
06/24/2022 08:18:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 08:18:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:18:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:18:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:18:35 - INFO - __main__ - Printing 3 examples
06/24/2022 08:18:35 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 08:18:35 - INFO - __main__ - ['Similar']
06/24/2022 08:18:35 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 08:18:35 - INFO - __main__ - ['Similar']
06/24/2022 08:18:35 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 08:18:35 - INFO - __main__ - ['Similar']
06/24/2022 08:18:35 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:18:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:18:35 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:18:41 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:18:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:18:41 - INFO - __main__ - Starting training!
06/24/2022 08:18:43 - INFO - __main__ - Step 10 Global step 10 Train loss 8.59 on epoch=4
06/24/2022 08:18:44 - INFO - __main__ - Step 20 Global step 20 Train loss 8.57 on epoch=9
06/24/2022 08:18:45 - INFO - __main__ - Step 30 Global step 30 Train loss 8.50 on epoch=14
06/24/2022 08:18:47 - INFO - __main__ - Step 40 Global step 40 Train loss 8.40 on epoch=19
06/24/2022 08:18:48 - INFO - __main__ - Step 50 Global step 50 Train loss 8.45 on epoch=24
06/24/2022 08:18:55 - INFO - __main__ - Global step 50 Train loss 8.50 ACC 0.0 on epoch=24
06/24/2022 08:18:55 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 08:18:56 - INFO - __main__ - Step 60 Global step 60 Train loss 8.35 on epoch=29
06/24/2022 08:18:58 - INFO - __main__ - Step 70 Global step 70 Train loss 8.31 on epoch=34
06/24/2022 08:18:59 - INFO - __main__ - Step 80 Global step 80 Train loss 8.38 on epoch=39
06/24/2022 08:19:00 - INFO - __main__ - Step 90 Global step 90 Train loss 8.28 on epoch=44
06/24/2022 08:19:01 - INFO - __main__ - Step 100 Global step 100 Train loss 8.25 on epoch=49
06/24/2022 08:19:03 - INFO - __main__ - Global step 100 Train loss 8.31 ACC 0.0 on epoch=49
06/24/2022 08:19:04 - INFO - __main__ - Step 110 Global step 110 Train loss 8.16 on epoch=54
06/24/2022 08:19:06 - INFO - __main__ - Step 120 Global step 120 Train loss 8.00 on epoch=59
06/24/2022 08:19:07 - INFO - __main__ - Step 130 Global step 130 Train loss 7.94 on epoch=64
06/24/2022 08:19:08 - INFO - __main__ - Step 140 Global step 140 Train loss 8.02 on epoch=69
06/24/2022 08:19:09 - INFO - __main__ - Step 150 Global step 150 Train loss 7.81 on epoch=74
06/24/2022 08:19:15 - INFO - __main__ - Global step 150 Train loss 7.99 ACC 0.0 on epoch=74
06/24/2022 08:19:16 - INFO - __main__ - Step 160 Global step 160 Train loss 7.69 on epoch=79
06/24/2022 08:19:18 - INFO - __main__ - Step 170 Global step 170 Train loss 7.59 on epoch=84
06/24/2022 08:19:19 - INFO - __main__ - Step 180 Global step 180 Train loss 7.47 on epoch=89
06/24/2022 08:19:20 - INFO - __main__ - Step 190 Global step 190 Train loss 7.25 on epoch=94
06/24/2022 08:19:21 - INFO - __main__ - Step 200 Global step 200 Train loss 7.13 on epoch=99
06/24/2022 08:19:26 - INFO - __main__ - Global step 200 Train loss 7.43 ACC 0.0 on epoch=99
06/24/2022 08:19:27 - INFO - __main__ - Step 210 Global step 210 Train loss 6.93 on epoch=104
06/24/2022 08:19:28 - INFO - __main__ - Step 220 Global step 220 Train loss 6.74 on epoch=109
06/24/2022 08:19:29 - INFO - __main__ - Step 230 Global step 230 Train loss 6.76 on epoch=114
06/24/2022 08:19:31 - INFO - __main__ - Step 240 Global step 240 Train loss 6.40 on epoch=119
06/24/2022 08:19:32 - INFO - __main__ - Step 250 Global step 250 Train loss 6.39 on epoch=124
06/24/2022 08:19:37 - INFO - __main__ - Global step 250 Train loss 6.64 ACC 0.0 on epoch=124
06/24/2022 08:19:38 - INFO - __main__ - Step 260 Global step 260 Train loss 6.19 on epoch=129
06/24/2022 08:19:39 - INFO - __main__ - Step 270 Global step 270 Train loss 6.11 on epoch=134
06/24/2022 08:19:40 - INFO - __main__ - Step 280 Global step 280 Train loss 5.90 on epoch=139
06/24/2022 08:19:41 - INFO - __main__ - Step 290 Global step 290 Train loss 5.71 on epoch=144
06/24/2022 08:19:43 - INFO - __main__ - Step 300 Global step 300 Train loss 5.61 on epoch=149
06/24/2022 08:19:50 - INFO - __main__ - Global step 300 Train loss 5.90 ACC 0.0 on epoch=149
06/24/2022 08:19:51 - INFO - __main__ - Step 310 Global step 310 Train loss 5.39 on epoch=154
06/24/2022 08:19:53 - INFO - __main__ - Step 320 Global step 320 Train loss 5.41 on epoch=159
06/24/2022 08:19:54 - INFO - __main__ - Step 330 Global step 330 Train loss 5.45 on epoch=164
06/24/2022 08:19:55 - INFO - __main__ - Step 340 Global step 340 Train loss 5.45 on epoch=169
06/24/2022 08:19:56 - INFO - __main__ - Step 350 Global step 350 Train loss 5.27 on epoch=174
06/24/2022 08:20:04 - INFO - __main__ - Global step 350 Train loss 5.40 ACC 0.0 on epoch=174
06/24/2022 08:20:05 - INFO - __main__ - Step 360 Global step 360 Train loss 5.06 on epoch=179
06/24/2022 08:20:06 - INFO - __main__ - Step 370 Global step 370 Train loss 4.87 on epoch=184
06/24/2022 08:20:08 - INFO - __main__ - Step 380 Global step 380 Train loss 4.58 on epoch=189
06/24/2022 08:20:09 - INFO - __main__ - Step 390 Global step 390 Train loss 4.56 on epoch=194
06/24/2022 08:20:10 - INFO - __main__ - Step 400 Global step 400 Train loss 4.41 on epoch=199
06/24/2022 08:20:15 - INFO - __main__ - Global step 400 Train loss 4.70 ACC 0.0 on epoch=199
06/24/2022 08:20:16 - INFO - __main__ - Step 410 Global step 410 Train loss 4.44 on epoch=204
06/24/2022 08:20:17 - INFO - __main__ - Step 420 Global step 420 Train loss 4.08 on epoch=209
06/24/2022 08:20:19 - INFO - __main__ - Step 430 Global step 430 Train loss 4.12 on epoch=214
06/24/2022 08:20:20 - INFO - __main__ - Step 440 Global step 440 Train loss 4.07 on epoch=219
06/24/2022 08:20:21 - INFO - __main__ - Step 450 Global step 450 Train loss 3.95 on epoch=224
06/24/2022 08:20:31 - INFO - __main__ - Global step 450 Train loss 4.13 ACC 0.0 on epoch=224
06/24/2022 08:20:32 - INFO - __main__ - Step 460 Global step 460 Train loss 3.89 on epoch=229
06/24/2022 08:20:33 - INFO - __main__ - Step 470 Global step 470 Train loss 3.74 on epoch=234
06/24/2022 08:20:35 - INFO - __main__ - Step 480 Global step 480 Train loss 3.62 on epoch=239
06/24/2022 08:20:36 - INFO - __main__ - Step 490 Global step 490 Train loss 3.65 on epoch=244
06/24/2022 08:20:37 - INFO - __main__ - Step 500 Global step 500 Train loss 3.52 on epoch=249
06/24/2022 08:20:44 - INFO - __main__ - Global step 500 Train loss 3.69 ACC 0.0 on epoch=249
06/24/2022 08:20:46 - INFO - __main__ - Step 510 Global step 510 Train loss 3.39 on epoch=254
06/24/2022 08:20:47 - INFO - __main__ - Step 520 Global step 520 Train loss 3.40 on epoch=259
06/24/2022 08:20:48 - INFO - __main__ - Step 530 Global step 530 Train loss 3.26 on epoch=264
06/24/2022 08:20:49 - INFO - __main__ - Step 540 Global step 540 Train loss 3.30 on epoch=269
06/24/2022 08:20:51 - INFO - __main__ - Step 550 Global step 550 Train loss 2.99 on epoch=274
06/24/2022 08:21:01 - INFO - __main__ - Global step 550 Train loss 3.27 ACC 0.0 on epoch=274
06/24/2022 08:21:02 - INFO - __main__ - Step 560 Global step 560 Train loss 3.08 on epoch=279
06/24/2022 08:21:04 - INFO - __main__ - Step 570 Global step 570 Train loss 2.90 on epoch=284
06/24/2022 08:21:05 - INFO - __main__ - Step 580 Global step 580 Train loss 2.87 on epoch=289
06/24/2022 08:21:06 - INFO - __main__ - Step 590 Global step 590 Train loss 2.94 on epoch=294
06/24/2022 08:21:07 - INFO - __main__ - Step 600 Global step 600 Train loss 2.87 on epoch=299
06/24/2022 08:21:13 - INFO - __main__ - Global step 600 Train loss 2.93 ACC 0.03125 on epoch=299
06/24/2022 08:21:13 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=299, global_step=600
06/24/2022 08:21:14 - INFO - __main__ - Step 610 Global step 610 Train loss 2.94 on epoch=304
06/24/2022 08:21:15 - INFO - __main__ - Step 620 Global step 620 Train loss 2.90 on epoch=309
06/24/2022 08:21:16 - INFO - __main__ - Step 630 Global step 630 Train loss 2.70 on epoch=314
06/24/2022 08:21:18 - INFO - __main__ - Step 640 Global step 640 Train loss 2.76 on epoch=319
06/24/2022 08:21:19 - INFO - __main__ - Step 650 Global step 650 Train loss 2.72 on epoch=324
06/24/2022 08:21:29 - INFO - __main__ - Global step 650 Train loss 2.80 ACC 0.0625 on epoch=324
06/24/2022 08:21:29 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.0625 on epoch=324, global_step=650
06/24/2022 08:21:31 - INFO - __main__ - Step 660 Global step 660 Train loss 2.74 on epoch=329
06/24/2022 08:21:32 - INFO - __main__ - Step 670 Global step 670 Train loss 2.67 on epoch=334
06/24/2022 08:21:33 - INFO - __main__ - Step 680 Global step 680 Train loss 2.70 on epoch=339
06/24/2022 08:21:34 - INFO - __main__ - Step 690 Global step 690 Train loss 2.51 on epoch=344
06/24/2022 08:21:36 - INFO - __main__ - Step 700 Global step 700 Train loss 2.61 on epoch=349
06/24/2022 08:21:41 - INFO - __main__ - Global step 700 Train loss 2.65 ACC 0.125 on epoch=349
06/24/2022 08:21:41 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.125 on epoch=349, global_step=700
06/24/2022 08:21:43 - INFO - __main__ - Step 710 Global step 710 Train loss 2.48 on epoch=354
06/24/2022 08:21:44 - INFO - __main__ - Step 720 Global step 720 Train loss 2.35 on epoch=359
06/24/2022 08:21:45 - INFO - __main__ - Step 730 Global step 730 Train loss 2.43 on epoch=364
06/24/2022 08:21:46 - INFO - __main__ - Step 740 Global step 740 Train loss 2.43 on epoch=369
06/24/2022 08:21:47 - INFO - __main__ - Step 750 Global step 750 Train loss 2.30 on epoch=374
06/24/2022 08:21:53 - INFO - __main__ - Global step 750 Train loss 2.40 ACC 0.34375 on epoch=374
06/24/2022 08:21:53 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.34375 on epoch=374, global_step=750
06/24/2022 08:21:55 - INFO - __main__ - Step 760 Global step 760 Train loss 2.33 on epoch=379
06/24/2022 08:21:56 - INFO - __main__ - Step 770 Global step 770 Train loss 2.21 on epoch=384
06/24/2022 08:21:57 - INFO - __main__ - Step 780 Global step 780 Train loss 2.22 on epoch=389
06/24/2022 08:21:59 - INFO - __main__ - Step 790 Global step 790 Train loss 2.30 on epoch=394
06/24/2022 08:22:00 - INFO - __main__ - Step 800 Global step 800 Train loss 2.21 on epoch=399
06/24/2022 08:22:09 - INFO - __main__ - Global step 800 Train loss 2.25 ACC 0.46875 on epoch=399
06/24/2022 08:22:09 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.46875 on epoch=399, global_step=800
06/24/2022 08:22:10 - INFO - __main__ - Step 810 Global step 810 Train loss 2.29 on epoch=404
06/24/2022 08:22:11 - INFO - __main__ - Step 820 Global step 820 Train loss 2.31 on epoch=409
06/24/2022 08:22:12 - INFO - __main__ - Step 830 Global step 830 Train loss 2.37 on epoch=414
06/24/2022 08:22:14 - INFO - __main__ - Step 840 Global step 840 Train loss 2.61 on epoch=419
06/24/2022 08:22:15 - INFO - __main__ - Step 850 Global step 850 Train loss 2.41 on epoch=424
06/24/2022 08:22:22 - INFO - __main__ - Global step 850 Train loss 2.40 ACC 0.46875 on epoch=424
06/24/2022 08:22:23 - INFO - __main__ - Step 860 Global step 860 Train loss 2.30 on epoch=429
06/24/2022 08:22:24 - INFO - __main__ - Step 870 Global step 870 Train loss 2.20 on epoch=434
06/24/2022 08:22:25 - INFO - __main__ - Step 880 Global step 880 Train loss 2.17 on epoch=439
06/24/2022 08:22:27 - INFO - __main__ - Step 890 Global step 890 Train loss 2.25 on epoch=444
06/24/2022 08:22:28 - INFO - __main__ - Step 900 Global step 900 Train loss 1.96 on epoch=449
06/24/2022 08:22:33 - INFO - __main__ - Global step 900 Train loss 2.17 ACC 0.5 on epoch=449
06/24/2022 08:22:33 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=449, global_step=900
06/24/2022 08:22:34 - INFO - __main__ - Step 910 Global step 910 Train loss 2.03 on epoch=454
06/24/2022 08:22:35 - INFO - __main__ - Step 920 Global step 920 Train loss 2.05 on epoch=459
06/24/2022 08:22:37 - INFO - __main__ - Step 930 Global step 930 Train loss 1.88 on epoch=464
06/24/2022 08:22:38 - INFO - __main__ - Step 940 Global step 940 Train loss 1.86 on epoch=469
06/24/2022 08:22:39 - INFO - __main__ - Step 950 Global step 950 Train loss 1.87 on epoch=474
06/24/2022 08:22:49 - INFO - __main__ - Global step 950 Train loss 1.94 ACC 0.5 on epoch=474
06/24/2022 08:22:50 - INFO - __main__ - Step 960 Global step 960 Train loss 1.79 on epoch=479
06/24/2022 08:22:52 - INFO - __main__ - Step 970 Global step 970 Train loss 1.82 on epoch=484
06/24/2022 08:22:53 - INFO - __main__ - Step 980 Global step 980 Train loss 1.66 on epoch=489
06/24/2022 08:22:54 - INFO - __main__ - Step 990 Global step 990 Train loss 1.64 on epoch=494
06/24/2022 08:22:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.57 on epoch=499
06/24/2022 08:22:59 - INFO - __main__ - Global step 1000 Train loss 1.70 ACC 0.5 on epoch=499
06/24/2022 08:23:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.53 on epoch=504
06/24/2022 08:23:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.47 on epoch=509
06/24/2022 08:23:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.37 on epoch=514
06/24/2022 08:23:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.37 on epoch=519
06/24/2022 08:23:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.25 on epoch=524
06/24/2022 08:23:15 - INFO - __main__ - Global step 1050 Train loss 1.40 ACC 0.5 on epoch=524
06/24/2022 08:23:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.23 on epoch=529
06/24/2022 08:23:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.18 on epoch=534
06/24/2022 08:23:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.98 on epoch=539
06/24/2022 08:23:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.05 on epoch=544
06/24/2022 08:23:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.98 on epoch=549
06/24/2022 08:23:24 - INFO - __main__ - Global step 1100 Train loss 1.08 ACC 0.5 on epoch=549
06/24/2022 08:23:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.88 on epoch=554
06/24/2022 08:23:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.04 on epoch=559
06/24/2022 08:23:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.93 on epoch=564
06/24/2022 08:23:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.92 on epoch=569
06/24/2022 08:23:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.91 on epoch=574
06/24/2022 08:23:31 - INFO - __main__ - Global step 1150 Train loss 0.94 ACC 0.5 on epoch=574
06/24/2022 08:23:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.81 on epoch=579
06/24/2022 08:23:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.70 on epoch=584
06/24/2022 08:23:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.78 on epoch=589
06/24/2022 08:23:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.79 on epoch=594
06/24/2022 08:23:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.68 on epoch=599
06/24/2022 08:23:38 - INFO - __main__ - Global step 1200 Train loss 0.75 ACC 0.5 on epoch=599
06/24/2022 08:23:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.72 on epoch=604
06/24/2022 08:23:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.71 on epoch=609
06/24/2022 08:23:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.75 on epoch=614
06/24/2022 08:23:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.69 on epoch=619
06/24/2022 08:23:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.69 on epoch=624
06/24/2022 08:23:45 - INFO - __main__ - Global step 1250 Train loss 0.71 ACC 0.5 on epoch=624
06/24/2022 08:23:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.68 on epoch=629
06/24/2022 08:23:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.70 on epoch=634
06/24/2022 08:23:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.45 on epoch=639
06/24/2022 08:23:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.62 on epoch=644
06/24/2022 08:23:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.65 on epoch=649
06/24/2022 08:23:51 - INFO - __main__ - Global step 1300 Train loss 0.62 ACC 0.5 on epoch=649
06/24/2022 08:23:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.58 on epoch=654
06/24/2022 08:23:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.69 on epoch=659
06/24/2022 08:23:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.70 on epoch=664
06/24/2022 08:23:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.56 on epoch=669
06/24/2022 08:23:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.57 on epoch=674
06/24/2022 08:23:58 - INFO - __main__ - Global step 1350 Train loss 0.62 ACC 0.5 on epoch=674
06/24/2022 08:23:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.57 on epoch=679
06/24/2022 08:24:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.58 on epoch=684
06/24/2022 08:24:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.63 on epoch=689
06/24/2022 08:24:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.64 on epoch=694
06/24/2022 08:24:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.63 on epoch=699
06/24/2022 08:24:04 - INFO - __main__ - Global step 1400 Train loss 0.61 ACC 0.5 on epoch=699
06/24/2022 08:24:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.63 on epoch=704
06/24/2022 08:24:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.67 on epoch=709
06/24/2022 08:24:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.55 on epoch=714
06/24/2022 08:24:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.69 on epoch=719
06/24/2022 08:24:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.58 on epoch=724
06/24/2022 08:24:11 - INFO - __main__ - Global step 1450 Train loss 0.62 ACC 0.5 on epoch=724
06/24/2022 08:24:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.56 on epoch=729
06/24/2022 08:24:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.71 on epoch=734
06/24/2022 08:24:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.63 on epoch=739
06/24/2022 08:24:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.69 on epoch=744
06/24/2022 08:24:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.57 on epoch=749
06/24/2022 08:24:18 - INFO - __main__ - Global step 1500 Train loss 0.63 ACC 0.5 on epoch=749
06/24/2022 08:24:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.63 on epoch=754
06/24/2022 08:24:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.50 on epoch=759
06/24/2022 08:24:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.56 on epoch=764
06/24/2022 08:24:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=769
06/24/2022 08:24:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.56 on epoch=774
06/24/2022 08:24:24 - INFO - __main__ - Global step 1550 Train loss 0.54 ACC 0.5 on epoch=774
06/24/2022 08:24:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.64 on epoch=779
06/24/2022 08:24:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.60 on epoch=784
06/24/2022 08:24:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.65 on epoch=789
06/24/2022 08:24:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.60 on epoch=794
06/24/2022 08:24:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.50 on epoch=799
06/24/2022 08:24:31 - INFO - __main__ - Global step 1600 Train loss 0.60 ACC 0.53125 on epoch=799
06/24/2022 08:24:31 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=799, global_step=1600
06/24/2022 08:24:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.58 on epoch=804
06/24/2022 08:24:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.54 on epoch=809
06/24/2022 08:24:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.61 on epoch=814
06/24/2022 08:24:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.57 on epoch=819
06/24/2022 08:24:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.59 on epoch=824
06/24/2022 08:24:38 - INFO - __main__ - Global step 1650 Train loss 0.58 ACC 0.53125 on epoch=824
06/24/2022 08:24:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.57 on epoch=829
06/24/2022 08:24:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.53 on epoch=834
06/24/2022 08:24:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.56 on epoch=839
06/24/2022 08:24:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=844
06/24/2022 08:24:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.53 on epoch=849
06/24/2022 08:24:44 - INFO - __main__ - Global step 1700 Train loss 0.54 ACC 0.5625 on epoch=849
06/24/2022 08:24:44 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=849, global_step=1700
06/24/2022 08:24:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.54 on epoch=854
06/24/2022 08:24:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.57 on epoch=859
06/24/2022 08:24:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.51 on epoch=864
06/24/2022 08:24:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.52 on epoch=869
06/24/2022 08:24:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.52 on epoch=874
06/24/2022 08:24:51 - INFO - __main__ - Global step 1750 Train loss 0.53 ACC 0.5 on epoch=874
06/24/2022 08:24:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.63 on epoch=879
06/24/2022 08:24:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.50 on epoch=884
06/24/2022 08:24:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.55 on epoch=889
06/24/2022 08:24:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.57 on epoch=894
06/24/2022 08:24:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.54 on epoch=899
06/24/2022 08:24:58 - INFO - __main__ - Global step 1800 Train loss 0.56 ACC 0.53125 on epoch=899
06/24/2022 08:24:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.56 on epoch=904
06/24/2022 08:25:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.48 on epoch=909
06/24/2022 08:25:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.48 on epoch=914
06/24/2022 08:25:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.51 on epoch=919
06/24/2022 08:25:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.48 on epoch=924
06/24/2022 08:25:04 - INFO - __main__ - Global step 1850 Train loss 0.50 ACC 0.5 on epoch=924
06/24/2022 08:25:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.56 on epoch=929
06/24/2022 08:25:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.57 on epoch=934
06/24/2022 08:25:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=939
06/24/2022 08:25:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.52 on epoch=944
06/24/2022 08:25:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=949
06/24/2022 08:25:11 - INFO - __main__ - Global step 1900 Train loss 0.52 ACC 0.5 on epoch=949
06/24/2022 08:25:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.52 on epoch=954
06/24/2022 08:25:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=959
06/24/2022 08:25:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.46 on epoch=964
06/24/2022 08:25:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.52 on epoch=969
06/24/2022 08:25:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.53 on epoch=974
06/24/2022 08:25:18 - INFO - __main__ - Global step 1950 Train loss 0.50 ACC 0.5625 on epoch=974
06/24/2022 08:25:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.54 on epoch=979
06/24/2022 08:25:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.58 on epoch=984
06/24/2022 08:25:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.48 on epoch=989
06/24/2022 08:25:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=994
06/24/2022 08:25:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.57 on epoch=999
06/24/2022 08:25:24 - INFO - __main__ - Global step 2000 Train loss 0.52 ACC 0.4375 on epoch=999
06/24/2022 08:25:24 - INFO - __main__ - save last model!
06/24/2022 08:25:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 08:25:24 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 08:25:24 - INFO - __main__ - Printing 3 examples
06/24/2022 08:25:24 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 08:25:24 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:25:24 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 08:25:24 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:25:24 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 08:25:24 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:25:24 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:25:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:25:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:25:25 - INFO - __main__ - Printing 3 examples
06/24/2022 08:25:25 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 08:25:25 - INFO - __main__ - ['Similar']
06/24/2022 08:25:25 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 08:25:25 - INFO - __main__ - ['Similar']
06/24/2022 08:25:25 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 08:25:25 - INFO - __main__ - ['Similar']
06/24/2022 08:25:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 08:25:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:25:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:25:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:25:25 - INFO - __main__ - Printing 3 examples
06/24/2022 08:25:25 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 08:25:25 - INFO - __main__ - ['Similar']
06/24/2022 08:25:25 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 08:25:25 - INFO - __main__ - ['Similar']
06/24/2022 08:25:25 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 08:25:25 - INFO - __main__ - ['Similar']
06/24/2022 08:25:25 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:25:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:25:25 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:25:25 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 08:25:30 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:25:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:25:30 - INFO - __main__ - Starting training!
06/24/2022 08:25:32 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_13_0.5_8_predictions.txt
06/24/2022 08:25:32 - INFO - __main__ - ACC on test data: 0.4852
06/24/2022 08:25:32 - INFO - __main__ - prefix=medical_questions_pairs_16_13, lr=0.5, bsz=8, dev_performance=0.5625, test_performance=0.4852459016393443
06/24/2022 08:25:32 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_13, lr=0.4, bsz=8 ...
06/24/2022 08:25:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:25:33 - INFO - __main__ - Printing 3 examples
06/24/2022 08:25:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 08:25:33 - INFO - __main__ - ['Similar']
06/24/2022 08:25:33 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 08:25:33 - INFO - __main__ - ['Similar']
06/24/2022 08:25:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 08:25:33 - INFO - __main__ - ['Similar']
06/24/2022 08:25:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 08:25:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:25:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:25:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:25:33 - INFO - __main__ - Printing 3 examples
06/24/2022 08:25:33 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 08:25:33 - INFO - __main__ - ['Similar']
06/24/2022 08:25:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 08:25:33 - INFO - __main__ - ['Similar']
06/24/2022 08:25:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 08:25:33 - INFO - __main__ - ['Similar']
06/24/2022 08:25:33 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:25:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:25:33 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:25:39 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:25:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:25:40 - INFO - __main__ - Starting training!
06/24/2022 08:25:41 - INFO - __main__ - Step 10 Global step 10 Train loss 8.63 on epoch=4
06/24/2022 08:25:42 - INFO - __main__ - Step 20 Global step 20 Train loss 8.49 on epoch=9
06/24/2022 08:25:44 - INFO - __main__ - Step 30 Global step 30 Train loss 8.48 on epoch=14
06/24/2022 08:25:45 - INFO - __main__ - Step 40 Global step 40 Train loss 8.40 on epoch=19
06/24/2022 08:25:46 - INFO - __main__ - Step 50 Global step 50 Train loss 8.39 on epoch=24
06/24/2022 08:25:48 - INFO - __main__ - Global step 50 Train loss 8.48 ACC 0.0 on epoch=24
06/24/2022 08:25:49 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 08:25:50 - INFO - __main__ - Step 60 Global step 60 Train loss 8.29 on epoch=29
06/24/2022 08:25:51 - INFO - __main__ - Step 70 Global step 70 Train loss 8.26 on epoch=34
06/24/2022 08:25:52 - INFO - __main__ - Step 80 Global step 80 Train loss 8.28 on epoch=39
06/24/2022 08:25:54 - INFO - __main__ - Step 90 Global step 90 Train loss 8.32 on epoch=44
06/24/2022 08:25:55 - INFO - __main__ - Step 100 Global step 100 Train loss 8.27 on epoch=49
06/24/2022 08:26:00 - INFO - __main__ - Global step 100 Train loss 8.28 ACC 0.0 on epoch=49
06/24/2022 08:26:02 - INFO - __main__ - Step 110 Global step 110 Train loss 8.16 on epoch=54
06/24/2022 08:26:03 - INFO - __main__ - Step 120 Global step 120 Train loss 8.11 on epoch=59
06/24/2022 08:26:04 - INFO - __main__ - Step 130 Global step 130 Train loss 8.03 on epoch=64
06/24/2022 08:26:05 - INFO - __main__ - Step 140 Global step 140 Train loss 8.09 on epoch=69
06/24/2022 08:26:07 - INFO - __main__ - Step 150 Global step 150 Train loss 8.03 on epoch=74
06/24/2022 08:26:09 - INFO - __main__ - Global step 150 Train loss 8.08 ACC 0.0 on epoch=74
06/24/2022 08:26:10 - INFO - __main__ - Step 160 Global step 160 Train loss 8.07 on epoch=79
06/24/2022 08:26:12 - INFO - __main__ - Step 170 Global step 170 Train loss 8.03 on epoch=84
06/24/2022 08:26:13 - INFO - __main__ - Step 180 Global step 180 Train loss 7.98 on epoch=89
06/24/2022 08:26:14 - INFO - __main__ - Step 190 Global step 190 Train loss 7.88 on epoch=94
06/24/2022 08:26:16 - INFO - __main__ - Step 200 Global step 200 Train loss 7.89 on epoch=99
06/24/2022 08:26:18 - INFO - __main__ - Global step 200 Train loss 7.97 ACC 0.0 on epoch=99
06/24/2022 08:26:19 - INFO - __main__ - Step 210 Global step 210 Train loss 7.88 on epoch=104
06/24/2022 08:26:20 - INFO - __main__ - Step 220 Global step 220 Train loss 7.79 on epoch=109
06/24/2022 08:26:22 - INFO - __main__ - Step 230 Global step 230 Train loss 7.89 on epoch=114
06/24/2022 08:26:23 - INFO - __main__ - Step 240 Global step 240 Train loss 7.79 on epoch=119
06/24/2022 08:26:24 - INFO - __main__ - Step 250 Global step 250 Train loss 7.71 on epoch=124
06/24/2022 08:26:30 - INFO - __main__ - Global step 250 Train loss 7.81 ACC 0.0 on epoch=124
06/24/2022 08:26:32 - INFO - __main__ - Step 260 Global step 260 Train loss 7.71 on epoch=129
06/24/2022 08:26:33 - INFO - __main__ - Step 270 Global step 270 Train loss 7.70 on epoch=134
06/24/2022 08:26:34 - INFO - __main__ - Step 280 Global step 280 Train loss 7.47 on epoch=139
06/24/2022 08:26:36 - INFO - __main__ - Step 290 Global step 290 Train loss 7.50 on epoch=144
06/24/2022 08:26:37 - INFO - __main__ - Step 300 Global step 300 Train loss 7.31 on epoch=149
06/24/2022 08:26:39 - INFO - __main__ - Global step 300 Train loss 7.54 ACC 0.0 on epoch=149
06/24/2022 08:26:40 - INFO - __main__ - Step 310 Global step 310 Train loss 7.26 on epoch=154
06/24/2022 08:26:41 - INFO - __main__ - Step 320 Global step 320 Train loss 7.25 on epoch=159
06/24/2022 08:26:43 - INFO - __main__ - Step 330 Global step 330 Train loss 7.16 on epoch=164
06/24/2022 08:26:44 - INFO - __main__ - Step 340 Global step 340 Train loss 6.94 on epoch=169
06/24/2022 08:26:45 - INFO - __main__ - Step 350 Global step 350 Train loss 6.80 on epoch=174
06/24/2022 08:26:51 - INFO - __main__ - Global step 350 Train loss 7.08 ACC 0.0 on epoch=174
06/24/2022 08:26:52 - INFO - __main__ - Step 360 Global step 360 Train loss 6.74 on epoch=179
06/24/2022 08:26:54 - INFO - __main__ - Step 370 Global step 370 Train loss 6.72 on epoch=184
06/24/2022 08:26:55 - INFO - __main__ - Step 380 Global step 380 Train loss 6.48 on epoch=189
06/24/2022 08:26:56 - INFO - __main__ - Step 390 Global step 390 Train loss 6.42 on epoch=194
06/24/2022 08:26:57 - INFO - __main__ - Step 400 Global step 400 Train loss 6.29 on epoch=199
06/24/2022 08:27:03 - INFO - __main__ - Global step 400 Train loss 6.53 ACC 0.0 on epoch=199
06/24/2022 08:27:05 - INFO - __main__ - Step 410 Global step 410 Train loss 6.26 on epoch=204
06/24/2022 08:27:06 - INFO - __main__ - Step 420 Global step 420 Train loss 6.10 on epoch=209
06/24/2022 08:27:07 - INFO - __main__ - Step 430 Global step 430 Train loss 6.01 on epoch=214
06/24/2022 08:27:08 - INFO - __main__ - Step 440 Global step 440 Train loss 5.89 on epoch=219
06/24/2022 08:27:10 - INFO - __main__ - Step 450 Global step 450 Train loss 5.83 on epoch=224
06/24/2022 08:27:18 - INFO - __main__ - Global step 450 Train loss 6.02 ACC 0.0 on epoch=224
06/24/2022 08:27:20 - INFO - __main__ - Step 460 Global step 460 Train loss 5.80 on epoch=229
06/24/2022 08:27:21 - INFO - __main__ - Step 470 Global step 470 Train loss 5.57 on epoch=234
06/24/2022 08:27:22 - INFO - __main__ - Step 480 Global step 480 Train loss 5.61 on epoch=239
06/24/2022 08:27:23 - INFO - __main__ - Step 490 Global step 490 Train loss 5.45 on epoch=244
06/24/2022 08:27:25 - INFO - __main__ - Step 500 Global step 500 Train loss 5.16 on epoch=249
06/24/2022 08:27:31 - INFO - __main__ - Global step 500 Train loss 5.52 ACC 0.0 on epoch=249
06/24/2022 08:27:32 - INFO - __main__ - Step 510 Global step 510 Train loss 5.28 on epoch=254
06/24/2022 08:27:33 - INFO - __main__ - Step 520 Global step 520 Train loss 5.09 on epoch=259
06/24/2022 08:27:35 - INFO - __main__ - Step 530 Global step 530 Train loss 5.10 on epoch=264
06/24/2022 08:27:36 - INFO - __main__ - Step 540 Global step 540 Train loss 4.98 on epoch=269
06/24/2022 08:27:37 - INFO - __main__ - Step 550 Global step 550 Train loss 4.95 on epoch=274
06/24/2022 08:27:48 - INFO - __main__ - Global step 550 Train loss 5.08 ACC 0.0 on epoch=274
06/24/2022 08:27:49 - INFO - __main__ - Step 560 Global step 560 Train loss 4.72 on epoch=279
06/24/2022 08:27:51 - INFO - __main__ - Step 570 Global step 570 Train loss 4.52 on epoch=284
06/24/2022 08:27:52 - INFO - __main__ - Step 580 Global step 580 Train loss 4.37 on epoch=289
06/24/2022 08:27:53 - INFO - __main__ - Step 590 Global step 590 Train loss 4.41 on epoch=294
06/24/2022 08:27:54 - INFO - __main__ - Step 600 Global step 600 Train loss 4.25 on epoch=299
06/24/2022 08:28:01 - INFO - __main__ - Global step 600 Train loss 4.45 ACC 0.0 on epoch=299
06/24/2022 08:28:02 - INFO - __main__ - Step 610 Global step 610 Train loss 4.16 on epoch=304
06/24/2022 08:28:03 - INFO - __main__ - Step 620 Global step 620 Train loss 3.99 on epoch=309
06/24/2022 08:28:04 - INFO - __main__ - Step 630 Global step 630 Train loss 3.85 on epoch=314
06/24/2022 08:28:06 - INFO - __main__ - Step 640 Global step 640 Train loss 3.69 on epoch=319
06/24/2022 08:28:07 - INFO - __main__ - Step 650 Global step 650 Train loss 3.70 on epoch=324
06/24/2022 08:28:09 - INFO - __main__ - Global step 650 Train loss 3.88 ACC 0.0 on epoch=324
06/24/2022 08:28:10 - INFO - __main__ - Step 660 Global step 660 Train loss 3.37 on epoch=329
06/24/2022 08:28:11 - INFO - __main__ - Step 670 Global step 670 Train loss 3.26 on epoch=334
06/24/2022 08:28:12 - INFO - __main__ - Step 680 Global step 680 Train loss 3.14 on epoch=339
06/24/2022 08:28:14 - INFO - __main__ - Step 690 Global step 690 Train loss 2.96 on epoch=344
06/24/2022 08:28:15 - INFO - __main__ - Step 700 Global step 700 Train loss 3.04 on epoch=349
06/24/2022 08:28:16 - INFO - __main__ - Global step 700 Train loss 3.15 ACC 0.0 on epoch=349
06/24/2022 08:28:17 - INFO - __main__ - Step 710 Global step 710 Train loss 2.86 on epoch=354
06/24/2022 08:28:18 - INFO - __main__ - Step 720 Global step 720 Train loss 2.96 on epoch=359
06/24/2022 08:28:19 - INFO - __main__ - Step 730 Global step 730 Train loss 2.84 on epoch=364
06/24/2022 08:28:21 - INFO - __main__ - Step 740 Global step 740 Train loss 2.70 on epoch=369
06/24/2022 08:28:22 - INFO - __main__ - Step 750 Global step 750 Train loss 2.60 on epoch=374
06/24/2022 08:28:23 - INFO - __main__ - Global step 750 Train loss 2.79 ACC 0.0 on epoch=374
06/24/2022 08:28:24 - INFO - __main__ - Step 760 Global step 760 Train loss 2.70 on epoch=379
06/24/2022 08:28:25 - INFO - __main__ - Step 770 Global step 770 Train loss 2.65 on epoch=384
06/24/2022 08:28:27 - INFO - __main__ - Step 780 Global step 780 Train loss 2.69 on epoch=389
06/24/2022 08:28:28 - INFO - __main__ - Step 790 Global step 790 Train loss 2.34 on epoch=394
06/24/2022 08:28:29 - INFO - __main__ - Step 800 Global step 800 Train loss 2.43 on epoch=399
06/24/2022 08:28:30 - INFO - __main__ - Global step 800 Train loss 2.56 ACC 0.0 on epoch=399
06/24/2022 08:28:31 - INFO - __main__ - Step 810 Global step 810 Train loss 2.42 on epoch=404
06/24/2022 08:28:32 - INFO - __main__ - Step 820 Global step 820 Train loss 2.29 on epoch=409
06/24/2022 08:28:34 - INFO - __main__ - Step 830 Global step 830 Train loss 2.36 on epoch=414
06/24/2022 08:28:35 - INFO - __main__ - Step 840 Global step 840 Train loss 2.17 on epoch=419
06/24/2022 08:28:36 - INFO - __main__ - Step 850 Global step 850 Train loss 2.14 on epoch=424
06/24/2022 08:28:37 - INFO - __main__ - Global step 850 Train loss 2.28 ACC 0.03125 on epoch=424
06/24/2022 08:28:37 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=424, global_step=850
06/24/2022 08:28:38 - INFO - __main__ - Step 860 Global step 860 Train loss 2.05 on epoch=429
06/24/2022 08:28:39 - INFO - __main__ - Step 870 Global step 870 Train loss 2.05 on epoch=434
06/24/2022 08:28:41 - INFO - __main__ - Step 880 Global step 880 Train loss 2.08 on epoch=439
06/24/2022 08:28:42 - INFO - __main__ - Step 890 Global step 890 Train loss 2.01 on epoch=444
06/24/2022 08:28:43 - INFO - __main__ - Step 900 Global step 900 Train loss 1.87 on epoch=449
06/24/2022 08:28:44 - INFO - __main__ - Global step 900 Train loss 2.01 ACC 0.3125 on epoch=449
06/24/2022 08:28:44 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.3125 on epoch=449, global_step=900
06/24/2022 08:28:45 - INFO - __main__ - Step 910 Global step 910 Train loss 2.03 on epoch=454
06/24/2022 08:28:46 - INFO - __main__ - Step 920 Global step 920 Train loss 1.92 on epoch=459
06/24/2022 08:28:48 - INFO - __main__ - Step 930 Global step 930 Train loss 1.91 on epoch=464
06/24/2022 08:28:49 - INFO - __main__ - Step 940 Global step 940 Train loss 2.05 on epoch=469
06/24/2022 08:28:50 - INFO - __main__ - Step 950 Global step 950 Train loss 1.80 on epoch=474
06/24/2022 08:28:51 - INFO - __main__ - Global step 950 Train loss 1.94 ACC 0.5 on epoch=474
06/24/2022 08:28:51 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.5 on epoch=474, global_step=950
06/24/2022 08:28:52 - INFO - __main__ - Step 960 Global step 960 Train loss 1.87 on epoch=479
06/24/2022 08:28:53 - INFO - __main__ - Step 970 Global step 970 Train loss 1.76 on epoch=484
06/24/2022 08:28:54 - INFO - __main__ - Step 980 Global step 980 Train loss 1.89 on epoch=489
06/24/2022 08:28:56 - INFO - __main__ - Step 990 Global step 990 Train loss 1.73 on epoch=494
06/24/2022 08:28:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.57 on epoch=499
06/24/2022 08:28:57 - INFO - __main__ - Global step 1000 Train loss 1.76 ACC 0.5 on epoch=499
06/24/2022 08:28:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.66 on epoch=504
06/24/2022 08:29:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.54 on epoch=509
06/24/2022 08:29:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.59 on epoch=514
06/24/2022 08:29:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.40 on epoch=519
06/24/2022 08:29:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.44 on epoch=524
06/24/2022 08:29:04 - INFO - __main__ - Global step 1050 Train loss 1.53 ACC 0.5 on epoch=524
06/24/2022 08:29:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.37 on epoch=529
06/24/2022 08:29:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.33 on epoch=534
06/24/2022 08:29:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.24 on epoch=539
06/24/2022 08:29:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.33 on epoch=544
06/24/2022 08:29:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.17 on epoch=549
06/24/2022 08:29:11 - INFO - __main__ - Global step 1100 Train loss 1.29 ACC 0.5 on epoch=549
06/24/2022 08:29:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.10 on epoch=554
06/24/2022 08:29:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.99 on epoch=559
06/24/2022 08:29:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.04 on epoch=564
06/24/2022 08:29:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.17 on epoch=569
06/24/2022 08:29:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.14 on epoch=574
06/24/2022 08:29:18 - INFO - __main__ - Global step 1150 Train loss 1.09 ACC 0.5 on epoch=574
06/24/2022 08:29:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.98 on epoch=579
06/24/2022 08:29:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.04 on epoch=584
06/24/2022 08:29:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.07 on epoch=589
06/24/2022 08:29:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.01 on epoch=594
06/24/2022 08:29:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.01 on epoch=599
06/24/2022 08:29:25 - INFO - __main__ - Global step 1200 Train loss 1.02 ACC 0.5 on epoch=599
06/24/2022 08:29:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.90 on epoch=604
06/24/2022 08:29:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.91 on epoch=609
06/24/2022 08:29:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.97 on epoch=614
06/24/2022 08:29:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.01 on epoch=619
06/24/2022 08:29:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.85 on epoch=624
06/24/2022 08:29:32 - INFO - __main__ - Global step 1250 Train loss 0.93 ACC 0.5 on epoch=624
06/24/2022 08:29:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.87 on epoch=629
06/24/2022 08:29:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.87 on epoch=634
06/24/2022 08:29:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.84 on epoch=639
06/24/2022 08:29:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.84 on epoch=644
06/24/2022 08:29:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.76 on epoch=649
06/24/2022 08:29:39 - INFO - __main__ - Global step 1300 Train loss 0.84 ACC 0.5 on epoch=649
06/24/2022 08:29:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.88 on epoch=654
06/24/2022 08:29:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.72 on epoch=659
06/24/2022 08:29:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.75 on epoch=664
06/24/2022 08:29:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.73 on epoch=669
06/24/2022 08:29:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.85 on epoch=674
06/24/2022 08:29:45 - INFO - __main__ - Global step 1350 Train loss 0.79 ACC 0.5 on epoch=674
06/24/2022 08:29:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.84 on epoch=679
06/24/2022 08:29:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.77 on epoch=684
06/24/2022 08:29:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.70 on epoch=689
06/24/2022 08:29:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.69 on epoch=694
06/24/2022 08:29:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.71 on epoch=699
06/24/2022 08:29:52 - INFO - __main__ - Global step 1400 Train loss 0.74 ACC 0.5 on epoch=699
06/24/2022 08:29:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.78 on epoch=704
06/24/2022 08:29:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.65 on epoch=709
06/24/2022 08:29:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.77 on epoch=714
06/24/2022 08:29:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.73 on epoch=719
06/24/2022 08:29:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.64 on epoch=724
06/24/2022 08:29:59 - INFO - __main__ - Global step 1450 Train loss 0.72 ACC 0.5 on epoch=724
06/24/2022 08:30:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.69 on epoch=729
06/24/2022 08:30:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.67 on epoch=734
06/24/2022 08:30:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.77 on epoch=739
06/24/2022 08:30:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.73 on epoch=744
06/24/2022 08:30:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.66 on epoch=749
06/24/2022 08:30:06 - INFO - __main__ - Global step 1500 Train loss 0.71 ACC 0.5 on epoch=749
06/24/2022 08:30:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.54 on epoch=754
06/24/2022 08:30:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.66 on epoch=759
06/24/2022 08:30:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.74 on epoch=764
06/24/2022 08:30:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.59 on epoch=769
06/24/2022 08:30:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.69 on epoch=774
06/24/2022 08:30:13 - INFO - __main__ - Global step 1550 Train loss 0.64 ACC 0.5 on epoch=774
06/24/2022 08:30:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.62 on epoch=779
06/24/2022 08:30:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.61 on epoch=784
06/24/2022 08:30:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.58 on epoch=789
06/24/2022 08:30:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.59 on epoch=794
06/24/2022 08:30:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.59 on epoch=799
06/24/2022 08:30:19 - INFO - __main__ - Global step 1600 Train loss 0.60 ACC 0.5 on epoch=799
06/24/2022 08:30:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.69 on epoch=804
06/24/2022 08:30:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.66 on epoch=809
06/24/2022 08:30:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.66 on epoch=814
06/24/2022 08:30:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.60 on epoch=819
06/24/2022 08:30:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.69 on epoch=824
06/24/2022 08:30:26 - INFO - __main__ - Global step 1650 Train loss 0.66 ACC 0.5 on epoch=824
06/24/2022 08:30:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.68 on epoch=829
06/24/2022 08:30:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.54 on epoch=834
06/24/2022 08:30:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.61 on epoch=839
06/24/2022 08:30:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.62 on epoch=844
06/24/2022 08:30:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.65 on epoch=849
06/24/2022 08:30:33 - INFO - __main__ - Global step 1700 Train loss 0.62 ACC 0.5 on epoch=849
06/24/2022 08:30:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.56 on epoch=854
06/24/2022 08:30:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.60 on epoch=859
06/24/2022 08:30:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.60 on epoch=864
06/24/2022 08:30:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.57 on epoch=869
06/24/2022 08:30:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.68 on epoch=874
06/24/2022 08:30:40 - INFO - __main__ - Global step 1750 Train loss 0.60 ACC 0.5 on epoch=874
06/24/2022 08:30:41 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.54 on epoch=879
06/24/2022 08:30:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.59 on epoch=884
06/24/2022 08:30:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.56 on epoch=889
06/24/2022 08:30:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.58 on epoch=894
06/24/2022 08:30:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.51 on epoch=899
06/24/2022 08:30:47 - INFO - __main__ - Global step 1800 Train loss 0.55 ACC 0.5 on epoch=899
06/24/2022 08:30:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.61 on epoch=904
06/24/2022 08:30:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.64 on epoch=909
06/24/2022 08:30:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.64 on epoch=914
06/24/2022 08:30:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.59 on epoch=919
06/24/2022 08:30:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.59 on epoch=924
06/24/2022 08:30:53 - INFO - __main__ - Global step 1850 Train loss 0.61 ACC 0.5 on epoch=924
06/24/2022 08:30:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.59 on epoch=929
06/24/2022 08:30:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.55 on epoch=934
06/24/2022 08:30:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.68 on epoch=939
06/24/2022 08:30:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.60 on epoch=944
06/24/2022 08:31:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.56 on epoch=949
06/24/2022 08:31:00 - INFO - __main__ - Global step 1900 Train loss 0.60 ACC 0.5 on epoch=949
06/24/2022 08:31:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.59 on epoch=954
06/24/2022 08:31:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.53 on epoch=959
06/24/2022 08:31:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.64 on epoch=964
06/24/2022 08:31:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.59 on epoch=969
06/24/2022 08:31:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.45 on epoch=974
06/24/2022 08:31:07 - INFO - __main__ - Global step 1950 Train loss 0.56 ACC 0.5 on epoch=974
06/24/2022 08:31:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.56 on epoch=979
06/24/2022 08:31:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.60 on epoch=984
06/24/2022 08:31:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.50 on epoch=989
06/24/2022 08:31:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.61 on epoch=994
06/24/2022 08:31:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.66 on epoch=999
06/24/2022 08:31:14 - INFO - __main__ - Global step 2000 Train loss 0.59 ACC 0.5 on epoch=999
06/24/2022 08:31:14 - INFO - __main__ - save last model!
06/24/2022 08:31:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 08:31:14 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 08:31:14 - INFO - __main__ - Printing 3 examples
06/24/2022 08:31:14 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 08:31:14 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:31:14 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 08:31:14 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:31:14 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 08:31:14 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:31:14 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:31:14 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:31:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:31:15 - INFO - __main__ - Printing 3 examples
06/24/2022 08:31:15 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 08:31:15 - INFO - __main__ - ['Similar']
06/24/2022 08:31:15 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 08:31:15 - INFO - __main__ - ['Similar']
06/24/2022 08:31:15 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 08:31:15 - INFO - __main__ - ['Similar']
06/24/2022 08:31:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 08:31:15 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:31:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:31:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:31:15 - INFO - __main__ - Printing 3 examples
06/24/2022 08:31:15 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 08:31:15 - INFO - __main__ - ['Similar']
06/24/2022 08:31:15 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 08:31:15 - INFO - __main__ - ['Similar']
06/24/2022 08:31:15 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 08:31:15 - INFO - __main__ - ['Similar']
06/24/2022 08:31:15 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:31:15 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:31:15 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:31:15 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 08:31:20 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:31:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:31:20 - INFO - __main__ - Starting training!
06/24/2022 08:31:22 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_13_0.4_8_predictions.txt
06/24/2022 08:31:22 - INFO - __main__ - ACC on test data: 0.4836
06/24/2022 08:31:22 - INFO - __main__ - prefix=medical_questions_pairs_16_13, lr=0.4, bsz=8, dev_performance=0.5, test_performance=0.48360655737704916
06/24/2022 08:31:22 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_13, lr=0.3, bsz=8 ...
06/24/2022 08:31:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:31:23 - INFO - __main__ - Printing 3 examples
06/24/2022 08:31:23 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 08:31:23 - INFO - __main__ - ['Similar']
06/24/2022 08:31:23 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 08:31:23 - INFO - __main__ - ['Similar']
06/24/2022 08:31:23 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 08:31:23 - INFO - __main__ - ['Similar']
06/24/2022 08:31:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 08:31:23 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:31:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:31:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:31:23 - INFO - __main__ - Printing 3 examples
06/24/2022 08:31:23 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 08:31:23 - INFO - __main__ - ['Similar']
06/24/2022 08:31:23 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 08:31:23 - INFO - __main__ - ['Similar']
06/24/2022 08:31:23 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 08:31:23 - INFO - __main__ - ['Similar']
06/24/2022 08:31:23 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:31:23 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:31:23 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:31:29 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:31:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:31:29 - INFO - __main__ - Starting training!
06/24/2022 08:31:31 - INFO - __main__ - Step 10 Global step 10 Train loss 8.67 on epoch=4
06/24/2022 08:31:32 - INFO - __main__ - Step 20 Global step 20 Train loss 8.63 on epoch=9
06/24/2022 08:31:33 - INFO - __main__ - Step 30 Global step 30 Train loss 8.41 on epoch=14
06/24/2022 08:31:35 - INFO - __main__ - Step 40 Global step 40 Train loss 8.48 on epoch=19
06/24/2022 08:31:36 - INFO - __main__ - Step 50 Global step 50 Train loss 8.35 on epoch=24
06/24/2022 08:31:38 - INFO - __main__ - Global step 50 Train loss 8.51 ACC 0.0 on epoch=24
06/24/2022 08:31:38 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 08:31:39 - INFO - __main__ - Step 60 Global step 60 Train loss 8.43 on epoch=29
06/24/2022 08:31:41 - INFO - __main__ - Step 70 Global step 70 Train loss 8.39 on epoch=34
06/24/2022 08:31:42 - INFO - __main__ - Step 80 Global step 80 Train loss 8.31 on epoch=39
06/24/2022 08:31:43 - INFO - __main__ - Step 90 Global step 90 Train loss 8.28 on epoch=44
06/24/2022 08:31:45 - INFO - __main__ - Step 100 Global step 100 Train loss 8.42 on epoch=49
06/24/2022 08:31:45 - INFO - __main__ - Global step 100 Train loss 8.37 ACC 0.0 on epoch=49
06/24/2022 08:31:47 - INFO - __main__ - Step 110 Global step 110 Train loss 8.18 on epoch=54
06/24/2022 08:31:48 - INFO - __main__ - Step 120 Global step 120 Train loss 8.15 on epoch=59
06/24/2022 08:31:49 - INFO - __main__ - Step 130 Global step 130 Train loss 8.08 on epoch=64
06/24/2022 08:31:51 - INFO - __main__ - Step 140 Global step 140 Train loss 8.10 on epoch=69
06/24/2022 08:31:52 - INFO - __main__ - Step 150 Global step 150 Train loss 8.10 on epoch=74
06/24/2022 08:31:54 - INFO - __main__ - Global step 150 Train loss 8.12 ACC 0.0 on epoch=74
06/24/2022 08:31:55 - INFO - __main__ - Step 160 Global step 160 Train loss 8.07 on epoch=79
06/24/2022 08:31:56 - INFO - __main__ - Step 170 Global step 170 Train loss 8.11 on epoch=84
06/24/2022 08:31:58 - INFO - __main__ - Step 180 Global step 180 Train loss 8.05 on epoch=89
06/24/2022 08:31:59 - INFO - __main__ - Step 190 Global step 190 Train loss 8.05 on epoch=94
06/24/2022 08:32:00 - INFO - __main__ - Step 200 Global step 200 Train loss 7.96 on epoch=99
06/24/2022 08:32:02 - INFO - __main__ - Global step 200 Train loss 8.05 ACC 0.0 on epoch=99
06/24/2022 08:32:04 - INFO - __main__ - Step 210 Global step 210 Train loss 7.87 on epoch=104
06/24/2022 08:32:05 - INFO - __main__ - Step 220 Global step 220 Train loss 7.87 on epoch=109
06/24/2022 08:32:06 - INFO - __main__ - Step 230 Global step 230 Train loss 7.67 on epoch=114
06/24/2022 08:32:08 - INFO - __main__ - Step 240 Global step 240 Train loss 7.64 on epoch=119
06/24/2022 08:32:09 - INFO - __main__ - Step 250 Global step 250 Train loss 7.74 on epoch=124
06/24/2022 08:32:17 - INFO - __main__ - Global step 250 Train loss 7.76 ACC 0.0 on epoch=124
06/24/2022 08:32:18 - INFO - __main__ - Step 260 Global step 260 Train loss 7.43 on epoch=129
06/24/2022 08:32:20 - INFO - __main__ - Step 270 Global step 270 Train loss 7.49 on epoch=134
06/24/2022 08:32:21 - INFO - __main__ - Step 280 Global step 280 Train loss 7.34 on epoch=139
06/24/2022 08:32:22 - INFO - __main__ - Step 290 Global step 290 Train loss 7.18 on epoch=144
06/24/2022 08:32:23 - INFO - __main__ - Step 300 Global step 300 Train loss 7.03 on epoch=149
06/24/2022 08:32:27 - INFO - __main__ - Global step 300 Train loss 7.29 ACC 0.0 on epoch=149
06/24/2022 08:32:28 - INFO - __main__ - Step 310 Global step 310 Train loss 6.88 on epoch=154
06/24/2022 08:32:29 - INFO - __main__ - Step 320 Global step 320 Train loss 6.93 on epoch=159
06/24/2022 08:32:31 - INFO - __main__ - Step 330 Global step 330 Train loss 6.90 on epoch=164
06/24/2022 08:32:32 - INFO - __main__ - Step 340 Global step 340 Train loss 6.73 on epoch=169
06/24/2022 08:32:33 - INFO - __main__ - Step 350 Global step 350 Train loss 6.58 on epoch=174
06/24/2022 08:32:44 - INFO - __main__ - Global step 350 Train loss 6.80 ACC 0.0 on epoch=174
06/24/2022 08:32:45 - INFO - __main__ - Step 360 Global step 360 Train loss 6.56 on epoch=179
06/24/2022 08:32:47 - INFO - __main__ - Step 370 Global step 370 Train loss 6.57 on epoch=184
06/24/2022 08:32:48 - INFO - __main__ - Step 380 Global step 380 Train loss 6.37 on epoch=189
06/24/2022 08:32:49 - INFO - __main__ - Step 390 Global step 390 Train loss 6.19 on epoch=194
06/24/2022 08:32:50 - INFO - __main__ - Step 400 Global step 400 Train loss 6.29 on epoch=199
06/24/2022 08:32:54 - INFO - __main__ - Global step 400 Train loss 6.40 ACC 0.0 on epoch=199
06/24/2022 08:32:55 - INFO - __main__ - Step 410 Global step 410 Train loss 6.05 on epoch=204
06/24/2022 08:32:57 - INFO - __main__ - Step 420 Global step 420 Train loss 5.97 on epoch=209
06/24/2022 08:32:58 - INFO - __main__ - Step 430 Global step 430 Train loss 5.91 on epoch=214
06/24/2022 08:32:59 - INFO - __main__ - Step 440 Global step 440 Train loss 5.82 on epoch=219
06/24/2022 08:33:01 - INFO - __main__ - Step 450 Global step 450 Train loss 5.89 on epoch=224
06/24/2022 08:33:11 - INFO - __main__ - Global step 450 Train loss 5.93 ACC 0.0 on epoch=224
06/24/2022 08:33:12 - INFO - __main__ - Step 460 Global step 460 Train loss 5.48 on epoch=229
06/24/2022 08:33:14 - INFO - __main__ - Step 470 Global step 470 Train loss 5.50 on epoch=234
06/24/2022 08:33:15 - INFO - __main__ - Step 480 Global step 480 Train loss 5.34 on epoch=239
06/24/2022 08:33:16 - INFO - __main__ - Step 490 Global step 490 Train loss 5.12 on epoch=244
06/24/2022 08:33:18 - INFO - __main__ - Step 500 Global step 500 Train loss 5.04 on epoch=249
06/24/2022 08:33:23 - INFO - __main__ - Global step 500 Train loss 5.30 ACC 0.0 on epoch=249
06/24/2022 08:33:24 - INFO - __main__ - Step 510 Global step 510 Train loss 4.89 on epoch=254
06/24/2022 08:33:26 - INFO - __main__ - Step 520 Global step 520 Train loss 4.77 on epoch=259
06/24/2022 08:33:27 - INFO - __main__ - Step 530 Global step 530 Train loss 4.53 on epoch=264
06/24/2022 08:33:28 - INFO - __main__ - Step 540 Global step 540 Train loss 4.50 on epoch=269
06/24/2022 08:33:30 - INFO - __main__ - Step 550 Global step 550 Train loss 4.37 on epoch=274
06/24/2022 08:33:36 - INFO - __main__ - Global step 550 Train loss 4.61 ACC 0.0 on epoch=274
06/24/2022 08:33:38 - INFO - __main__ - Step 560 Global step 560 Train loss 4.06 on epoch=279
06/24/2022 08:33:39 - INFO - __main__ - Step 570 Global step 570 Train loss 3.98 on epoch=284
06/24/2022 08:33:40 - INFO - __main__ - Step 580 Global step 580 Train loss 3.93 on epoch=289
06/24/2022 08:33:42 - INFO - __main__ - Step 590 Global step 590 Train loss 3.77 on epoch=294
06/24/2022 08:33:43 - INFO - __main__ - Step 600 Global step 600 Train loss 3.68 on epoch=299
06/24/2022 08:33:45 - INFO - __main__ - Global step 600 Train loss 3.88 ACC 0.0 on epoch=299
06/24/2022 08:33:46 - INFO - __main__ - Step 610 Global step 610 Train loss 3.62 on epoch=304
06/24/2022 08:33:47 - INFO - __main__ - Step 620 Global step 620 Train loss 3.66 on epoch=309
06/24/2022 08:33:49 - INFO - __main__ - Step 630 Global step 630 Train loss 3.68 on epoch=314
06/24/2022 08:33:50 - INFO - __main__ - Step 640 Global step 640 Train loss 3.36 on epoch=319
06/24/2022 08:33:51 - INFO - __main__ - Step 650 Global step 650 Train loss 3.35 on epoch=324
06/24/2022 08:33:53 - INFO - __main__ - Global step 650 Train loss 3.53 ACC 0.0 on epoch=324
06/24/2022 08:33:54 - INFO - __main__ - Step 660 Global step 660 Train loss 3.25 on epoch=329
06/24/2022 08:33:55 - INFO - __main__ - Step 670 Global step 670 Train loss 3.24 on epoch=334
06/24/2022 08:33:57 - INFO - __main__ - Step 680 Global step 680 Train loss 3.31 on epoch=339
06/24/2022 08:33:58 - INFO - __main__ - Step 690 Global step 690 Train loss 3.07 on epoch=344
06/24/2022 08:33:59 - INFO - __main__ - Step 700 Global step 700 Train loss 3.32 on epoch=349
06/24/2022 08:34:05 - INFO - __main__ - Global step 700 Train loss 3.24 ACC 0.0 on epoch=349
06/24/2022 08:34:07 - INFO - __main__ - Step 710 Global step 710 Train loss 3.21 on epoch=354
06/24/2022 08:34:08 - INFO - __main__ - Step 720 Global step 720 Train loss 2.90 on epoch=359
06/24/2022 08:34:09 - INFO - __main__ - Step 730 Global step 730 Train loss 2.94 on epoch=364
06/24/2022 08:34:10 - INFO - __main__ - Step 740 Global step 740 Train loss 2.82 on epoch=369
06/24/2022 08:34:12 - INFO - __main__ - Step 750 Global step 750 Train loss 2.79 on epoch=374
06/24/2022 08:34:23 - INFO - __main__ - Global step 750 Train loss 2.93 ACC 0.0 on epoch=374
06/24/2022 08:34:24 - INFO - __main__ - Step 760 Global step 760 Train loss 2.85 on epoch=379
06/24/2022 08:34:25 - INFO - __main__ - Step 770 Global step 770 Train loss 2.83 on epoch=384
06/24/2022 08:34:26 - INFO - __main__ - Step 780 Global step 780 Train loss 2.79 on epoch=389
06/24/2022 08:34:28 - INFO - __main__ - Step 790 Global step 790 Train loss 2.73 on epoch=394
06/24/2022 08:34:29 - INFO - __main__ - Step 800 Global step 800 Train loss 2.66 on epoch=399
06/24/2022 08:34:40 - INFO - __main__ - Global step 800 Train loss 2.77 ACC 0.03125 on epoch=399
06/24/2022 08:34:40 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=399, global_step=800
06/24/2022 08:34:41 - INFO - __main__ - Step 810 Global step 810 Train loss 2.74 on epoch=404
06/24/2022 08:34:42 - INFO - __main__ - Step 820 Global step 820 Train loss 2.52 on epoch=409
06/24/2022 08:34:43 - INFO - __main__ - Step 830 Global step 830 Train loss 2.53 on epoch=414
06/24/2022 08:34:45 - INFO - __main__ - Step 840 Global step 840 Train loss 2.58 on epoch=419
06/24/2022 08:34:46 - INFO - __main__ - Step 850 Global step 850 Train loss 2.48 on epoch=424
06/24/2022 08:34:47 - INFO - __main__ - Global step 850 Train loss 2.57 ACC 0.40625 on epoch=424
06/24/2022 08:34:47 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.40625 on epoch=424, global_step=850
06/24/2022 08:34:48 - INFO - __main__ - Step 860 Global step 860 Train loss 2.44 on epoch=429
06/24/2022 08:34:49 - INFO - __main__ - Step 870 Global step 870 Train loss 2.47 on epoch=434
06/24/2022 08:34:51 - INFO - __main__ - Step 880 Global step 880 Train loss 2.19 on epoch=439
06/24/2022 08:34:52 - INFO - __main__ - Step 890 Global step 890 Train loss 2.27 on epoch=444
06/24/2022 08:34:53 - INFO - __main__ - Step 900 Global step 900 Train loss 2.43 on epoch=449
06/24/2022 08:34:54 - INFO - __main__ - Global step 900 Train loss 2.36 ACC 0.53125 on epoch=449
06/24/2022 08:34:54 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.53125 on epoch=449, global_step=900
06/24/2022 08:34:56 - INFO - __main__ - Step 910 Global step 910 Train loss 2.40 on epoch=454
06/24/2022 08:34:57 - INFO - __main__ - Step 920 Global step 920 Train loss 2.21 on epoch=459
06/24/2022 08:34:58 - INFO - __main__ - Step 930 Global step 930 Train loss 2.09 on epoch=464
06/24/2022 08:34:59 - INFO - __main__ - Step 940 Global step 940 Train loss 2.23 on epoch=469
06/24/2022 08:35:01 - INFO - __main__ - Step 950 Global step 950 Train loss 2.14 on epoch=474
06/24/2022 08:35:11 - INFO - __main__ - Global step 950 Train loss 2.21 ACC 0.46875 on epoch=474
06/24/2022 08:35:13 - INFO - __main__ - Step 960 Global step 960 Train loss 2.02 on epoch=479
06/24/2022 08:35:14 - INFO - __main__ - Step 970 Global step 970 Train loss 2.13 on epoch=484
06/24/2022 08:35:15 - INFO - __main__ - Step 980 Global step 980 Train loss 2.16 on epoch=489
06/24/2022 08:35:16 - INFO - __main__ - Step 990 Global step 990 Train loss 1.96 on epoch=494
06/24/2022 08:35:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.90 on epoch=499
06/24/2022 08:35:19 - INFO - __main__ - Global step 1000 Train loss 2.03 ACC 0.4375 on epoch=499
06/24/2022 08:35:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.88 on epoch=504
06/24/2022 08:35:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.94 on epoch=509
06/24/2022 08:35:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.84 on epoch=514
06/24/2022 08:35:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.79 on epoch=519
06/24/2022 08:35:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.70 on epoch=524
06/24/2022 08:35:36 - INFO - __main__ - Global step 1050 Train loss 1.83 ACC 0.53125 on epoch=524
06/24/2022 08:35:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.90 on epoch=529
06/24/2022 08:35:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.97 on epoch=534
06/24/2022 08:35:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.60 on epoch=539
06/24/2022 08:35:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.66 on epoch=544
06/24/2022 08:35:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.73 on epoch=549
06/24/2022 08:35:53 - INFO - __main__ - Global step 1100 Train loss 1.77 ACC 0.4375 on epoch=549
06/24/2022 08:35:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.51 on epoch=554
06/24/2022 08:35:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.64 on epoch=559
06/24/2022 08:35:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.64 on epoch=564
06/24/2022 08:35:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.57 on epoch=569
06/24/2022 08:35:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.51 on epoch=574
06/24/2022 08:36:06 - INFO - __main__ - Global step 1150 Train loss 1.58 ACC 0.46875 on epoch=574
06/24/2022 08:36:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.49 on epoch=579
06/24/2022 08:36:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.52 on epoch=584
06/24/2022 08:36:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.60 on epoch=589
06/24/2022 08:36:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.47 on epoch=594
06/24/2022 08:36:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.44 on epoch=599
06/24/2022 08:36:16 - INFO - __main__ - Global step 1200 Train loss 1.50 ACC 0.5 on epoch=599
06/24/2022 08:36:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.36 on epoch=604
06/24/2022 08:36:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.36 on epoch=609
06/24/2022 08:36:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.41 on epoch=614
06/24/2022 08:36:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.27 on epoch=619
06/24/2022 08:36:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.37 on epoch=624
06/24/2022 08:36:27 - INFO - __main__ - Global step 1250 Train loss 1.36 ACC 0.5 on epoch=624
06/24/2022 08:36:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.18 on epoch=629
06/24/2022 08:36:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.18 on epoch=634
06/24/2022 08:36:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.25 on epoch=639
06/24/2022 08:36:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.21 on epoch=644
06/24/2022 08:36:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.11 on epoch=649
06/24/2022 08:36:40 - INFO - __main__ - Global step 1300 Train loss 1.19 ACC 0.46875 on epoch=649
06/24/2022 08:36:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.24 on epoch=654
06/24/2022 08:36:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.16 on epoch=659
06/24/2022 08:36:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.09 on epoch=664
06/24/2022 08:36:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.19 on epoch=669
06/24/2022 08:36:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.94 on epoch=674
06/24/2022 08:36:50 - INFO - __main__ - Global step 1350 Train loss 1.12 ACC 0.53125 on epoch=674
06/24/2022 08:36:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.13 on epoch=679
06/24/2022 08:36:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.02 on epoch=684
06/24/2022 08:36:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.03 on epoch=689
06/24/2022 08:36:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.97 on epoch=694
06/24/2022 08:36:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.98 on epoch=699
06/24/2022 08:36:57 - INFO - __main__ - Global step 1400 Train loss 1.03 ACC 0.5 on epoch=699
06/24/2022 08:36:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.99 on epoch=704
06/24/2022 08:37:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.98 on epoch=709
06/24/2022 08:37:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.07 on epoch=714
06/24/2022 08:37:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.99 on epoch=719
06/24/2022 08:37:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.90 on epoch=724
06/24/2022 08:37:04 - INFO - __main__ - Global step 1450 Train loss 0.99 ACC 0.5 on epoch=724
06/24/2022 08:37:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.93 on epoch=729
06/24/2022 08:37:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.84 on epoch=734
06/24/2022 08:37:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.87 on epoch=739
06/24/2022 08:37:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.85 on epoch=744
06/24/2022 08:37:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.69 on epoch=749
06/24/2022 08:37:11 - INFO - __main__ - Global step 1500 Train loss 0.83 ACC 0.5 on epoch=749
06/24/2022 08:37:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.84 on epoch=754
06/24/2022 08:37:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.81 on epoch=759
06/24/2022 08:37:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.90 on epoch=764
06/24/2022 08:37:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.77 on epoch=769
06/24/2022 08:37:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.77 on epoch=774
06/24/2022 08:37:18 - INFO - __main__ - Global step 1550 Train loss 0.82 ACC 0.5 on epoch=774
06/24/2022 08:37:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.80 on epoch=779
06/24/2022 08:37:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.81 on epoch=784
06/24/2022 08:37:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.88 on epoch=789
06/24/2022 08:37:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.89 on epoch=794
06/24/2022 08:37:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.80 on epoch=799
06/24/2022 08:37:26 - INFO - __main__ - Global step 1600 Train loss 0.84 ACC 0.4375 on epoch=799
06/24/2022 08:37:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.70 on epoch=804
06/24/2022 08:37:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.80 on epoch=809
06/24/2022 08:37:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.71 on epoch=814
06/24/2022 08:37:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.64 on epoch=819
06/24/2022 08:37:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.78 on epoch=824
06/24/2022 08:37:34 - INFO - __main__ - Global step 1650 Train loss 0.73 ACC 0.5 on epoch=824
06/24/2022 08:37:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.62 on epoch=829
06/24/2022 08:37:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.60 on epoch=834
06/24/2022 08:37:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.80 on epoch=839
06/24/2022 08:37:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.65 on epoch=844
06/24/2022 08:37:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.71 on epoch=849
06/24/2022 08:37:41 - INFO - __main__ - Global step 1700 Train loss 0.68 ACC 0.5 on epoch=849
06/24/2022 08:37:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.75 on epoch=854
06/24/2022 08:37:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.68 on epoch=859
06/24/2022 08:37:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.77 on epoch=864
06/24/2022 08:37:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.75 on epoch=869
06/24/2022 08:37:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.61 on epoch=874
06/24/2022 08:37:48 - INFO - __main__ - Global step 1750 Train loss 0.71 ACC 0.46875 on epoch=874
06/24/2022 08:37:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.70 on epoch=879
06/24/2022 08:37:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.78 on epoch=884
06/24/2022 08:37:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.59 on epoch=889
06/24/2022 08:37:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.64 on epoch=894
06/24/2022 08:37:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.62 on epoch=899
06/24/2022 08:37:55 - INFO - __main__ - Global step 1800 Train loss 0.67 ACC 0.5 on epoch=899
06/24/2022 08:37:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.59 on epoch=904
06/24/2022 08:37:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.73 on epoch=909
06/24/2022 08:37:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.53 on epoch=914
06/24/2022 08:38:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.67 on epoch=919
06/24/2022 08:38:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.69 on epoch=924
06/24/2022 08:38:02 - INFO - __main__ - Global step 1850 Train loss 0.64 ACC 0.5 on epoch=924
06/24/2022 08:38:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.66 on epoch=929
06/24/2022 08:38:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.68 on epoch=934
06/24/2022 08:38:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.59 on epoch=939
06/24/2022 08:38:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.59 on epoch=944
06/24/2022 08:38:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.72 on epoch=949
06/24/2022 08:38:09 - INFO - __main__ - Global step 1900 Train loss 0.65 ACC 0.5 on epoch=949
06/24/2022 08:38:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.54 on epoch=954
06/24/2022 08:38:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.67 on epoch=959
06/24/2022 08:38:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.73 on epoch=964
06/24/2022 08:38:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.71 on epoch=969
06/24/2022 08:38:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.54 on epoch=974
06/24/2022 08:38:16 - INFO - __main__ - Global step 1950 Train loss 0.64 ACC 0.5 on epoch=974
06/24/2022 08:38:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.59 on epoch=979
06/24/2022 08:38:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.56 on epoch=984
06/24/2022 08:38:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.58 on epoch=989
06/24/2022 08:38:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.70 on epoch=994
06/24/2022 08:38:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.58 on epoch=999
06/24/2022 08:38:23 - INFO - __main__ - Global step 2000 Train loss 0.60 ACC 0.5 on epoch=999
06/24/2022 08:38:23 - INFO - __main__ - save last model!
06/24/2022 08:38:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 08:38:23 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 08:38:23 - INFO - __main__ - Printing 3 examples
06/24/2022 08:38:23 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 08:38:23 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:38:23 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 08:38:23 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:38:23 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 08:38:23 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:38:23 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:38:23 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:38:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:38:23 - INFO - __main__ - Printing 3 examples
06/24/2022 08:38:23 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 08:38:23 - INFO - __main__ - ['Similar']
06/24/2022 08:38:23 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 08:38:23 - INFO - __main__ - ['Similar']
06/24/2022 08:38:23 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 08:38:23 - INFO - __main__ - ['Similar']
06/24/2022 08:38:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 08:38:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:38:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:38:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:38:24 - INFO - __main__ - Printing 3 examples
06/24/2022 08:38:24 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 08:38:24 - INFO - __main__ - ['Similar']
06/24/2022 08:38:24 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 08:38:24 - INFO - __main__ - ['Similar']
06/24/2022 08:38:24 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 08:38:24 - INFO - __main__ - ['Similar']
06/24/2022 08:38:24 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:38:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:38:24 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:38:24 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 08:38:30 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:38:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:38:30 - INFO - __main__ - Starting training!
06/24/2022 08:38:32 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_13_0.3_8_predictions.txt
06/24/2022 08:38:32 - INFO - __main__ - ACC on test data: 0.4836
06/24/2022 08:38:32 - INFO - __main__ - prefix=medical_questions_pairs_16_13, lr=0.3, bsz=8, dev_performance=0.53125, test_performance=0.48360655737704916
06/24/2022 08:38:32 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_13, lr=0.2, bsz=8 ...
06/24/2022 08:38:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:38:33 - INFO - __main__ - Printing 3 examples
06/24/2022 08:38:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 08:38:33 - INFO - __main__ - ['Similar']
06/24/2022 08:38:33 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 08:38:33 - INFO - __main__ - ['Similar']
06/24/2022 08:38:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 08:38:33 - INFO - __main__ - ['Similar']
06/24/2022 08:38:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 08:38:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:38:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:38:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:38:33 - INFO - __main__ - Printing 3 examples
06/24/2022 08:38:33 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 08:38:33 - INFO - __main__ - ['Similar']
06/24/2022 08:38:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 08:38:33 - INFO - __main__ - ['Similar']
06/24/2022 08:38:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 08:38:33 - INFO - __main__ - ['Similar']
06/24/2022 08:38:33 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:38:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:38:33 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:38:39 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:38:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:38:39 - INFO - __main__ - Starting training!
06/24/2022 08:38:41 - INFO - __main__ - Step 10 Global step 10 Train loss 8.57 on epoch=4
06/24/2022 08:38:42 - INFO - __main__ - Step 20 Global step 20 Train loss 8.57 on epoch=9
06/24/2022 08:38:43 - INFO - __main__ - Step 30 Global step 30 Train loss 8.52 on epoch=14
06/24/2022 08:38:45 - INFO - __main__ - Step 40 Global step 40 Train loss 8.52 on epoch=19
06/24/2022 08:38:46 - INFO - __main__ - Step 50 Global step 50 Train loss 8.45 on epoch=24
06/24/2022 08:38:48 - INFO - __main__ - Global step 50 Train loss 8.53 ACC 0.0 on epoch=24
06/24/2022 08:38:48 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 08:38:49 - INFO - __main__ - Step 60 Global step 60 Train loss 8.53 on epoch=29
06/24/2022 08:38:51 - INFO - __main__ - Step 70 Global step 70 Train loss 8.46 on epoch=34
06/24/2022 08:38:52 - INFO - __main__ - Step 80 Global step 80 Train loss 8.50 on epoch=39
06/24/2022 08:38:53 - INFO - __main__ - Step 90 Global step 90 Train loss 8.43 on epoch=44
06/24/2022 08:38:55 - INFO - __main__ - Step 100 Global step 100 Train loss 8.36 on epoch=49
06/24/2022 08:38:56 - INFO - __main__ - Global step 100 Train loss 8.46 ACC 0.0 on epoch=49
06/24/2022 08:38:58 - INFO - __main__ - Step 110 Global step 110 Train loss 8.31 on epoch=54
06/24/2022 08:38:59 - INFO - __main__ - Step 120 Global step 120 Train loss 8.29 on epoch=59
06/24/2022 08:39:00 - INFO - __main__ - Step 130 Global step 130 Train loss 8.33 on epoch=64
06/24/2022 08:39:01 - INFO - __main__ - Step 140 Global step 140 Train loss 8.29 on epoch=69
06/24/2022 08:39:03 - INFO - __main__ - Step 150 Global step 150 Train loss 8.25 on epoch=74
06/24/2022 08:39:09 - INFO - __main__ - Global step 150 Train loss 8.29 ACC 0.0 on epoch=74
06/24/2022 08:39:11 - INFO - __main__ - Step 160 Global step 160 Train loss 8.26 on epoch=79
06/24/2022 08:39:12 - INFO - __main__ - Step 170 Global step 170 Train loss 8.30 on epoch=84
06/24/2022 08:39:13 - INFO - __main__ - Step 180 Global step 180 Train loss 8.20 on epoch=89
06/24/2022 08:39:14 - INFO - __main__ - Step 190 Global step 190 Train loss 8.16 on epoch=94
06/24/2022 08:39:16 - INFO - __main__ - Step 200 Global step 200 Train loss 8.19 on epoch=99
06/24/2022 08:39:18 - INFO - __main__ - Global step 200 Train loss 8.23 ACC 0.0 on epoch=99
06/24/2022 08:39:20 - INFO - __main__ - Step 210 Global step 210 Train loss 8.17 on epoch=104
06/24/2022 08:39:21 - INFO - __main__ - Step 220 Global step 220 Train loss 8.15 on epoch=109
06/24/2022 08:39:22 - INFO - __main__ - Step 230 Global step 230 Train loss 8.14 on epoch=114
06/24/2022 08:39:23 - INFO - __main__ - Step 240 Global step 240 Train loss 8.00 on epoch=119
06/24/2022 08:39:25 - INFO - __main__ - Step 250 Global step 250 Train loss 7.99 on epoch=124
06/24/2022 08:39:28 - INFO - __main__ - Global step 250 Train loss 8.09 ACC 0.0 on epoch=124
06/24/2022 08:39:29 - INFO - __main__ - Step 260 Global step 260 Train loss 8.09 on epoch=129
06/24/2022 08:39:31 - INFO - __main__ - Step 270 Global step 270 Train loss 8.11 on epoch=134
06/24/2022 08:39:32 - INFO - __main__ - Step 280 Global step 280 Train loss 8.01 on epoch=139
06/24/2022 08:39:33 - INFO - __main__ - Step 290 Global step 290 Train loss 7.95 on epoch=144
06/24/2022 08:39:34 - INFO - __main__ - Step 300 Global step 300 Train loss 7.91 on epoch=149
06/24/2022 08:39:42 - INFO - __main__ - Global step 300 Train loss 8.01 ACC 0.0 on epoch=149
06/24/2022 08:39:44 - INFO - __main__ - Step 310 Global step 310 Train loss 7.82 on epoch=154
06/24/2022 08:39:45 - INFO - __main__ - Step 320 Global step 320 Train loss 7.85 on epoch=159
06/24/2022 08:39:46 - INFO - __main__ - Step 330 Global step 330 Train loss 7.76 on epoch=164
06/24/2022 08:39:48 - INFO - __main__ - Step 340 Global step 340 Train loss 7.73 on epoch=169
06/24/2022 08:39:49 - INFO - __main__ - Step 350 Global step 350 Train loss 7.73 on epoch=174
06/24/2022 08:40:00 - INFO - __main__ - Global step 350 Train loss 7.78 ACC 0.0 on epoch=174
06/24/2022 08:40:01 - INFO - __main__ - Step 360 Global step 360 Train loss 7.67 on epoch=179
06/24/2022 08:40:02 - INFO - __main__ - Step 370 Global step 370 Train loss 7.52 on epoch=184
06/24/2022 08:40:03 - INFO - __main__ - Step 380 Global step 380 Train loss 7.49 on epoch=189
06/24/2022 08:40:05 - INFO - __main__ - Step 390 Global step 390 Train loss 7.36 on epoch=194
06/24/2022 08:40:06 - INFO - __main__ - Step 400 Global step 400 Train loss 7.20 on epoch=199
06/24/2022 08:40:13 - INFO - __main__ - Global step 400 Train loss 7.45 ACC 0.0 on epoch=199
06/24/2022 08:40:14 - INFO - __main__ - Step 410 Global step 410 Train loss 7.16 on epoch=204
06/24/2022 08:40:16 - INFO - __main__ - Step 420 Global step 420 Train loss 7.19 on epoch=209
06/24/2022 08:40:17 - INFO - __main__ - Step 430 Global step 430 Train loss 7.07 on epoch=214
06/24/2022 08:40:18 - INFO - __main__ - Step 440 Global step 440 Train loss 6.99 on epoch=219
06/24/2022 08:40:20 - INFO - __main__ - Step 450 Global step 450 Train loss 6.98 on epoch=224
06/24/2022 08:40:24 - INFO - __main__ - Global step 450 Train loss 7.08 ACC 0.0 on epoch=224
06/24/2022 08:40:25 - INFO - __main__ - Step 460 Global step 460 Train loss 6.90 on epoch=229
06/24/2022 08:40:27 - INFO - __main__ - Step 470 Global step 470 Train loss 6.64 on epoch=234
06/24/2022 08:40:28 - INFO - __main__ - Step 480 Global step 480 Train loss 6.77 on epoch=239
06/24/2022 08:40:29 - INFO - __main__ - Step 490 Global step 490 Train loss 6.68 on epoch=244
06/24/2022 08:40:31 - INFO - __main__ - Step 500 Global step 500 Train loss 6.60 on epoch=249
06/24/2022 08:40:38 - INFO - __main__ - Global step 500 Train loss 6.72 ACC 0.0 on epoch=249
06/24/2022 08:40:39 - INFO - __main__ - Step 510 Global step 510 Train loss 6.56 on epoch=254
06/24/2022 08:40:41 - INFO - __main__ - Step 520 Global step 520 Train loss 6.47 on epoch=259
06/24/2022 08:40:42 - INFO - __main__ - Step 530 Global step 530 Train loss 6.36 on epoch=264
06/24/2022 08:40:43 - INFO - __main__ - Step 540 Global step 540 Train loss 6.30 on epoch=269
06/24/2022 08:40:45 - INFO - __main__ - Step 550 Global step 550 Train loss 6.33 on epoch=274
06/24/2022 08:40:48 - INFO - __main__ - Global step 550 Train loss 6.40 ACC 0.0 on epoch=274
06/24/2022 08:40:49 - INFO - __main__ - Step 560 Global step 560 Train loss 6.11 on epoch=279
06/24/2022 08:40:51 - INFO - __main__ - Step 570 Global step 570 Train loss 6.14 on epoch=284
06/24/2022 08:40:52 - INFO - __main__ - Step 580 Global step 580 Train loss 6.09 on epoch=289
06/24/2022 08:40:53 - INFO - __main__ - Step 590 Global step 590 Train loss 5.99 on epoch=294
06/24/2022 08:40:54 - INFO - __main__ - Step 600 Global step 600 Train loss 5.94 on epoch=299
06/24/2022 08:41:02 - INFO - __main__ - Global step 600 Train loss 6.05 ACC 0.0 on epoch=299
06/24/2022 08:41:03 - INFO - __main__ - Step 610 Global step 610 Train loss 5.91 on epoch=304
06/24/2022 08:41:04 - INFO - __main__ - Step 620 Global step 620 Train loss 5.87 on epoch=309
06/24/2022 08:41:06 - INFO - __main__ - Step 630 Global step 630 Train loss 5.85 on epoch=314
06/24/2022 08:41:07 - INFO - __main__ - Step 640 Global step 640 Train loss 5.82 on epoch=319
06/24/2022 08:41:08 - INFO - __main__ - Step 650 Global step 650 Train loss 5.68 on epoch=324
06/24/2022 08:41:16 - INFO - __main__ - Global step 650 Train loss 5.83 ACC 0.0 on epoch=324
06/24/2022 08:41:17 - INFO - __main__ - Step 660 Global step 660 Train loss 5.70 on epoch=329
06/24/2022 08:41:18 - INFO - __main__ - Step 670 Global step 670 Train loss 5.71 on epoch=334
06/24/2022 08:41:20 - INFO - __main__ - Step 680 Global step 680 Train loss 5.46 on epoch=339
06/24/2022 08:41:21 - INFO - __main__ - Step 690 Global step 690 Train loss 5.46 on epoch=344
06/24/2022 08:41:22 - INFO - __main__ - Step 700 Global step 700 Train loss 5.39 on epoch=349
06/24/2022 08:41:27 - INFO - __main__ - Global step 700 Train loss 5.54 ACC 0.0 on epoch=349
06/24/2022 08:41:29 - INFO - __main__ - Step 710 Global step 710 Train loss 5.23 on epoch=354
06/24/2022 08:41:30 - INFO - __main__ - Step 720 Global step 720 Train loss 5.19 on epoch=359
06/24/2022 08:41:31 - INFO - __main__ - Step 730 Global step 730 Train loss 5.19 on epoch=364
06/24/2022 08:41:33 - INFO - __main__ - Step 740 Global step 740 Train loss 5.10 on epoch=369
06/24/2022 08:41:34 - INFO - __main__ - Step 750 Global step 750 Train loss 4.98 on epoch=374
06/24/2022 08:41:44 - INFO - __main__ - Global step 750 Train loss 5.14 ACC 0.0625 on epoch=374
06/24/2022 08:41:45 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=374, global_step=750
06/24/2022 08:41:46 - INFO - __main__ - Step 760 Global step 760 Train loss 4.91 on epoch=379
06/24/2022 08:41:47 - INFO - __main__ - Step 770 Global step 770 Train loss 4.80 on epoch=384
06/24/2022 08:41:48 - INFO - __main__ - Step 780 Global step 780 Train loss 4.72 on epoch=389
06/24/2022 08:41:50 - INFO - __main__ - Step 790 Global step 790 Train loss 4.79 on epoch=394
06/24/2022 08:41:51 - INFO - __main__ - Step 800 Global step 800 Train loss 4.50 on epoch=399
06/24/2022 08:42:02 - INFO - __main__ - Global step 800 Train loss 4.74 ACC 0.125 on epoch=399
06/24/2022 08:42:02 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.125 on epoch=399, global_step=800
06/24/2022 08:42:03 - INFO - __main__ - Step 810 Global step 810 Train loss 4.55 on epoch=404
06/24/2022 08:42:05 - INFO - __main__ - Step 820 Global step 820 Train loss 4.47 on epoch=409
06/24/2022 08:42:06 - INFO - __main__ - Step 830 Global step 830 Train loss 4.28 on epoch=414
06/24/2022 08:42:07 - INFO - __main__ - Step 840 Global step 840 Train loss 4.09 on epoch=419
06/24/2022 08:42:08 - INFO - __main__ - Step 850 Global step 850 Train loss 4.33 on epoch=424
06/24/2022 08:42:19 - INFO - __main__ - Global step 850 Train loss 4.34 ACC 0.40625 on epoch=424
06/24/2022 08:42:19 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.40625 on epoch=424, global_step=850
06/24/2022 08:42:20 - INFO - __main__ - Step 860 Global step 860 Train loss 3.99 on epoch=429
06/24/2022 08:42:22 - INFO - __main__ - Step 870 Global step 870 Train loss 3.90 on epoch=434
06/24/2022 08:42:23 - INFO - __main__ - Step 880 Global step 880 Train loss 3.78 on epoch=439
06/24/2022 08:42:24 - INFO - __main__ - Step 890 Global step 890 Train loss 3.89 on epoch=444
06/24/2022 08:42:26 - INFO - __main__ - Step 900 Global step 900 Train loss 3.66 on epoch=449
06/24/2022 08:42:36 - INFO - __main__ - Global step 900 Train loss 3.85 ACC 0.5 on epoch=449
06/24/2022 08:42:36 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.5 on epoch=449, global_step=900
06/24/2022 08:42:38 - INFO - __main__ - Step 910 Global step 910 Train loss 3.61 on epoch=454
06/24/2022 08:42:39 - INFO - __main__ - Step 920 Global step 920 Train loss 3.43 on epoch=459
06/24/2022 08:42:40 - INFO - __main__ - Step 930 Global step 930 Train loss 3.47 on epoch=464
06/24/2022 08:42:42 - INFO - __main__ - Step 940 Global step 940 Train loss 3.45 on epoch=469
06/24/2022 08:42:43 - INFO - __main__ - Step 950 Global step 950 Train loss 3.25 on epoch=474
06/24/2022 08:42:53 - INFO - __main__ - Global step 950 Train loss 3.44 ACC 0.5 on epoch=474
06/24/2022 08:42:54 - INFO - __main__ - Step 960 Global step 960 Train loss 3.20 on epoch=479
06/24/2022 08:42:55 - INFO - __main__ - Step 970 Global step 970 Train loss 2.94 on epoch=484
06/24/2022 08:42:56 - INFO - __main__ - Step 980 Global step 980 Train loss 2.95 on epoch=489
06/24/2022 08:42:58 - INFO - __main__ - Step 990 Global step 990 Train loss 2.80 on epoch=494
06/24/2022 08:42:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.92 on epoch=499
06/24/2022 08:43:08 - INFO - __main__ - Global step 1000 Train loss 2.96 ACC 0.5 on epoch=499
06/24/2022 08:43:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.68 on epoch=504
06/24/2022 08:43:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.65 on epoch=509
06/24/2022 08:43:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.53 on epoch=514
06/24/2022 08:43:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.50 on epoch=519
06/24/2022 08:43:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.35 on epoch=524
06/24/2022 08:43:24 - INFO - __main__ - Global step 1050 Train loss 2.54 ACC 0.5 on epoch=524
06/24/2022 08:43:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.50 on epoch=529
06/24/2022 08:43:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.58 on epoch=534
06/24/2022 08:43:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.49 on epoch=539
06/24/2022 08:43:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.16 on epoch=544
06/24/2022 08:43:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.17 on epoch=549
06/24/2022 08:43:36 - INFO - __main__ - Global step 1100 Train loss 2.38 ACC 0.5 on epoch=549
06/24/2022 08:43:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.99 on epoch=554
06/24/2022 08:43:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 2.04 on epoch=559
06/24/2022 08:43:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.09 on epoch=564
06/24/2022 08:43:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 2.07 on epoch=569
06/24/2022 08:43:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.89 on epoch=574
06/24/2022 08:43:53 - INFO - __main__ - Global step 1150 Train loss 2.02 ACC 0.5 on epoch=574
06/24/2022 08:43:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.00 on epoch=579
06/24/2022 08:43:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.83 on epoch=584
06/24/2022 08:43:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.81 on epoch=589
06/24/2022 08:43:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.88 on epoch=594
06/24/2022 08:43:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.70 on epoch=599
06/24/2022 08:44:07 - INFO - __main__ - Global step 1200 Train loss 1.85 ACC 0.5 on epoch=599
06/24/2022 08:44:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.59 on epoch=604
06/24/2022 08:44:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.65 on epoch=609
06/24/2022 08:44:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.59 on epoch=614
06/24/2022 08:44:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.59 on epoch=619
06/24/2022 08:44:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.56 on epoch=624
06/24/2022 08:44:21 - INFO - __main__ - Global step 1250 Train loss 1.60 ACC 0.46875 on epoch=624
06/24/2022 08:44:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.53 on epoch=629
06/24/2022 08:44:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.51 on epoch=634
06/24/2022 08:44:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.43 on epoch=639
06/24/2022 08:44:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.48 on epoch=644
06/24/2022 08:44:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.37 on epoch=649
06/24/2022 08:44:35 - INFO - __main__ - Global step 1300 Train loss 1.46 ACC 0.5 on epoch=649
06/24/2022 08:44:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.45 on epoch=654
06/24/2022 08:44:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.47 on epoch=659
06/24/2022 08:44:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.40 on epoch=664
06/24/2022 08:44:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.40 on epoch=669
06/24/2022 08:44:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.18 on epoch=674
06/24/2022 08:44:46 - INFO - __main__ - Global step 1350 Train loss 1.38 ACC 0.59375 on epoch=674
06/24/2022 08:44:46 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=674, global_step=1350
06/24/2022 08:44:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.24 on epoch=679
06/24/2022 08:44:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.18 on epoch=684
06/24/2022 08:44:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.19 on epoch=689
06/24/2022 08:44:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.29 on epoch=694
06/24/2022 08:44:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.18 on epoch=699
06/24/2022 08:44:56 - INFO - __main__ - Global step 1400 Train loss 1.22 ACC 0.5 on epoch=699
06/24/2022 08:44:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.12 on epoch=704
06/24/2022 08:44:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.09 on epoch=709
06/24/2022 08:45:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.15 on epoch=714
06/24/2022 08:45:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.06 on epoch=719
06/24/2022 08:45:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.04 on epoch=724
06/24/2022 08:45:06 - INFO - __main__ - Global step 1450 Train loss 1.09 ACC 0.5 on epoch=724
06/24/2022 08:45:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.01 on epoch=729
06/24/2022 08:45:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.18 on epoch=734
06/24/2022 08:45:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.18 on epoch=739
06/24/2022 08:45:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.15 on epoch=744
06/24/2022 08:45:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.11 on epoch=749
06/24/2022 08:45:16 - INFO - __main__ - Global step 1500 Train loss 1.13 ACC 0.5 on epoch=749
06/24/2022 08:45:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.02 on epoch=754
06/24/2022 08:45:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.08 on epoch=759
06/24/2022 08:45:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.02 on epoch=764
06/24/2022 08:45:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.12 on epoch=769
06/24/2022 08:45:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.10 on epoch=774
06/24/2022 08:45:23 - INFO - __main__ - Global step 1550 Train loss 1.07 ACC 0.5 on epoch=774
06/24/2022 08:45:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.01 on epoch=779
06/24/2022 08:45:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.95 on epoch=784
06/24/2022 08:45:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.00 on epoch=789
06/24/2022 08:45:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.85 on epoch=794
06/24/2022 08:45:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.92 on epoch=799
06/24/2022 08:45:30 - INFO - __main__ - Global step 1600 Train loss 0.95 ACC 0.5 on epoch=799
06/24/2022 08:45:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.95 on epoch=804
06/24/2022 08:45:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.81 on epoch=809
06/24/2022 08:45:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.10 on epoch=814
06/24/2022 08:45:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.81 on epoch=819
06/24/2022 08:45:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.92 on epoch=824
06/24/2022 08:45:37 - INFO - __main__ - Global step 1650 Train loss 0.92 ACC 0.5 on epoch=824
06/24/2022 08:45:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.85 on epoch=829
06/24/2022 08:45:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.02 on epoch=834
06/24/2022 08:45:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.90 on epoch=839
06/24/2022 08:45:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.92 on epoch=844
06/24/2022 08:45:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.82 on epoch=849
06/24/2022 08:45:44 - INFO - __main__ - Global step 1700 Train loss 0.90 ACC 0.5 on epoch=849
06/24/2022 08:45:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.80 on epoch=854
06/24/2022 08:45:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.99 on epoch=859
06/24/2022 08:45:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.86 on epoch=864
06/24/2022 08:45:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.84 on epoch=869
06/24/2022 08:45:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.73 on epoch=874
06/24/2022 08:45:51 - INFO - __main__ - Global step 1750 Train loss 0.84 ACC 0.5 on epoch=874
06/24/2022 08:45:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.90 on epoch=879
06/24/2022 08:45:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.87 on epoch=884
06/24/2022 08:45:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.79 on epoch=889
06/24/2022 08:45:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.86 on epoch=894
06/24/2022 08:45:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.71 on epoch=899
06/24/2022 08:45:58 - INFO - __main__ - Global step 1800 Train loss 0.83 ACC 0.5 on epoch=899
06/24/2022 08:46:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.83 on epoch=904
06/24/2022 08:46:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.71 on epoch=909
06/24/2022 08:46:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.86 on epoch=914
06/24/2022 08:46:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.73 on epoch=919
06/24/2022 08:46:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.86 on epoch=924
06/24/2022 08:46:05 - INFO - __main__ - Global step 1850 Train loss 0.80 ACC 0.5 on epoch=924
06/24/2022 08:46:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.81 on epoch=929
06/24/2022 08:46:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.71 on epoch=934
06/24/2022 08:46:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.63 on epoch=939
06/24/2022 08:46:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.75 on epoch=944
06/24/2022 08:46:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.64 on epoch=949
06/24/2022 08:46:13 - INFO - __main__ - Global step 1900 Train loss 0.71 ACC 0.5 on epoch=949
06/24/2022 08:46:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.72 on epoch=954
06/24/2022 08:46:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.70 on epoch=959
06/24/2022 08:46:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.78 on epoch=964
06/24/2022 08:46:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.66 on epoch=969
06/24/2022 08:46:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.70 on epoch=974
06/24/2022 08:46:19 - INFO - __main__ - Global step 1950 Train loss 0.71 ACC 0.5 on epoch=974
06/24/2022 08:46:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.65 on epoch=979
06/24/2022 08:46:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.79 on epoch=984
06/24/2022 08:46:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.60 on epoch=989
06/24/2022 08:46:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.72 on epoch=994
06/24/2022 08:46:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.79 on epoch=999
06/24/2022 08:46:26 - INFO - __main__ - Global step 2000 Train loss 0.71 ACC 0.5 on epoch=999
06/24/2022 08:46:26 - INFO - __main__ - save last model!
06/24/2022 08:46:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 08:46:27 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 08:46:27 - INFO - __main__ - Printing 3 examples
06/24/2022 08:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 08:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 08:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 08:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:46:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:46:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:46:27 - INFO - __main__ - Printing 3 examples
06/24/2022 08:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 08:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 08:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 08:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 08:46:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:46:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:46:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:46:27 - INFO - __main__ - Printing 3 examples
06/24/2022 08:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 08:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 08:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 08:46:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:46:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:46:27 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:46:27 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 08:46:32 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:46:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:46:33 - INFO - __main__ - Starting training!
06/24/2022 08:46:37 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_13_0.2_8_predictions.txt
06/24/2022 08:46:37 - INFO - __main__ - ACC on test data: 0.4836
06/24/2022 08:46:38 - INFO - __main__ - prefix=medical_questions_pairs_16_13, lr=0.2, bsz=8, dev_performance=0.59375, test_performance=0.48360655737704916
06/24/2022 08:46:38 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_21, lr=0.5, bsz=8 ...
06/24/2022 08:46:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:46:39 - INFO - __main__ - Printing 3 examples
06/24/2022 08:46:39 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 08:46:39 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:39 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 08:46:39 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:39 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 08:46:39 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 08:46:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:46:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:46:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:46:39 - INFO - __main__ - Printing 3 examples
06/24/2022 08:46:39 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 08:46:39 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:39 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 08:46:39 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:39 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 08:46:39 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:46:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:46:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:46:39 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:46:45 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:46:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:46:45 - INFO - __main__ - Starting training!
06/24/2022 08:46:47 - INFO - __main__ - Step 10 Global step 10 Train loss 8.39 on epoch=4
06/24/2022 08:46:48 - INFO - __main__ - Step 20 Global step 20 Train loss 8.32 on epoch=9
06/24/2022 08:46:49 - INFO - __main__ - Step 30 Global step 30 Train loss 8.26 on epoch=14
06/24/2022 08:46:50 - INFO - __main__ - Step 40 Global step 40 Train loss 8.22 on epoch=19
06/24/2022 08:46:52 - INFO - __main__ - Step 50 Global step 50 Train loss 8.23 on epoch=24
06/24/2022 08:46:53 - INFO - __main__ - Global step 50 Train loss 8.28 ACC 0.0 on epoch=24
06/24/2022 08:46:53 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 08:46:55 - INFO - __main__ - Step 60 Global step 60 Train loss 8.14 on epoch=29
06/24/2022 08:46:56 - INFO - __main__ - Step 70 Global step 70 Train loss 8.05 on epoch=34
06/24/2022 08:46:57 - INFO - __main__ - Step 80 Global step 80 Train loss 8.06 on epoch=39
06/24/2022 08:46:59 - INFO - __main__ - Step 90 Global step 90 Train loss 7.93 on epoch=44
06/24/2022 08:47:00 - INFO - __main__ - Step 100 Global step 100 Train loss 7.76 on epoch=49
06/24/2022 08:47:04 - INFO - __main__ - Global step 100 Train loss 7.99 ACC 0.0 on epoch=49
06/24/2022 08:47:06 - INFO - __main__ - Step 110 Global step 110 Train loss 7.82 on epoch=54
06/24/2022 08:47:07 - INFO - __main__ - Step 120 Global step 120 Train loss 7.83 on epoch=59
06/24/2022 08:47:08 - INFO - __main__ - Step 130 Global step 130 Train loss 7.67 on epoch=64
06/24/2022 08:47:10 - INFO - __main__ - Step 140 Global step 140 Train loss 7.65 on epoch=69
06/24/2022 08:47:11 - INFO - __main__ - Step 150 Global step 150 Train loss 7.57 on epoch=74
06/24/2022 08:47:17 - INFO - __main__ - Global step 150 Train loss 7.71 ACC 0.0 on epoch=74
06/24/2022 08:47:18 - INFO - __main__ - Step 160 Global step 160 Train loss 7.62 on epoch=79
06/24/2022 08:47:20 - INFO - __main__ - Step 170 Global step 170 Train loss 7.36 on epoch=84
06/24/2022 08:47:21 - INFO - __main__ - Step 180 Global step 180 Train loss 7.24 on epoch=89
06/24/2022 08:47:22 - INFO - __main__ - Step 190 Global step 190 Train loss 7.10 on epoch=94
06/24/2022 08:47:24 - INFO - __main__ - Step 200 Global step 200 Train loss 7.04 on epoch=99
06/24/2022 08:47:27 - INFO - __main__ - Global step 200 Train loss 7.27 ACC 0.0 on epoch=99
06/24/2022 08:47:28 - INFO - __main__ - Step 210 Global step 210 Train loss 6.86 on epoch=104
06/24/2022 08:47:30 - INFO - __main__ - Step 220 Global step 220 Train loss 6.70 on epoch=109
06/24/2022 08:47:31 - INFO - __main__ - Step 230 Global step 230 Train loss 6.47 on epoch=114
06/24/2022 08:47:32 - INFO - __main__ - Step 240 Global step 240 Train loss 6.45 on epoch=119
06/24/2022 08:47:34 - INFO - __main__ - Step 250 Global step 250 Train loss 6.37 on epoch=124
06/24/2022 08:47:38 - INFO - __main__ - Global step 250 Train loss 6.57 ACC 0.0 on epoch=124
06/24/2022 08:47:39 - INFO - __main__ - Step 260 Global step 260 Train loss 6.14 on epoch=129
06/24/2022 08:47:41 - INFO - __main__ - Step 270 Global step 270 Train loss 6.13 on epoch=134
06/24/2022 08:47:42 - INFO - __main__ - Step 280 Global step 280 Train loss 6.04 on epoch=139
06/24/2022 08:47:43 - INFO - __main__ - Step 290 Global step 290 Train loss 5.98 on epoch=144
06/24/2022 08:47:44 - INFO - __main__ - Step 300 Global step 300 Train loss 5.93 on epoch=149
06/24/2022 08:47:54 - INFO - __main__ - Global step 300 Train loss 6.04 ACC 0.0 on epoch=149
06/24/2022 08:47:55 - INFO - __main__ - Step 310 Global step 310 Train loss 5.79 on epoch=154
06/24/2022 08:47:56 - INFO - __main__ - Step 320 Global step 320 Train loss 5.67 on epoch=159
06/24/2022 08:47:58 - INFO - __main__ - Step 330 Global step 330 Train loss 5.48 on epoch=164
06/24/2022 08:47:59 - INFO - __main__ - Step 340 Global step 340 Train loss 5.38 on epoch=169
06/24/2022 08:48:00 - INFO - __main__ - Step 350 Global step 350 Train loss 5.24 on epoch=174
06/24/2022 08:48:06 - INFO - __main__ - Global step 350 Train loss 5.51 ACC 0.0 on epoch=174
06/24/2022 08:48:08 - INFO - __main__ - Step 360 Global step 360 Train loss 5.17 on epoch=179
06/24/2022 08:48:09 - INFO - __main__ - Step 370 Global step 370 Train loss 5.13 on epoch=184
06/24/2022 08:48:10 - INFO - __main__ - Step 380 Global step 380 Train loss 5.02 on epoch=189
06/24/2022 08:48:11 - INFO - __main__ - Step 390 Global step 390 Train loss 4.82 on epoch=194
06/24/2022 08:48:13 - INFO - __main__ - Step 400 Global step 400 Train loss 4.70 on epoch=199
06/24/2022 08:48:21 - INFO - __main__ - Global step 400 Train loss 4.97 ACC 0.0 on epoch=199
06/24/2022 08:48:22 - INFO - __main__ - Step 410 Global step 410 Train loss 4.46 on epoch=204
06/24/2022 08:48:23 - INFO - __main__ - Step 420 Global step 420 Train loss 4.31 on epoch=209
06/24/2022 08:48:25 - INFO - __main__ - Step 430 Global step 430 Train loss 4.16 on epoch=214
06/24/2022 08:48:26 - INFO - __main__ - Step 440 Global step 440 Train loss 4.07 on epoch=219
06/24/2022 08:48:27 - INFO - __main__ - Step 450 Global step 450 Train loss 3.74 on epoch=224
06/24/2022 08:48:34 - INFO - __main__ - Global step 450 Train loss 4.15 ACC 0.0 on epoch=224
06/24/2022 08:48:35 - INFO - __main__ - Step 460 Global step 460 Train loss 3.69 on epoch=229
06/24/2022 08:48:36 - INFO - __main__ - Step 470 Global step 470 Train loss 3.64 on epoch=234
06/24/2022 08:48:38 - INFO - __main__ - Step 480 Global step 480 Train loss 3.42 on epoch=239
06/24/2022 08:48:39 - INFO - __main__ - Step 490 Global step 490 Train loss 3.37 on epoch=244
06/24/2022 08:48:40 - INFO - __main__ - Step 500 Global step 500 Train loss 3.32 on epoch=249
06/24/2022 08:48:44 - INFO - __main__ - Global step 500 Train loss 3.49 ACC 0.0 on epoch=249
06/24/2022 08:48:45 - INFO - __main__ - Step 510 Global step 510 Train loss 3.21 on epoch=254
06/24/2022 08:48:47 - INFO - __main__ - Step 520 Global step 520 Train loss 3.22 on epoch=259
06/24/2022 08:48:48 - INFO - __main__ - Step 530 Global step 530 Train loss 3.24 on epoch=264
06/24/2022 08:48:49 - INFO - __main__ - Step 540 Global step 540 Train loss 3.22 on epoch=269
06/24/2022 08:48:51 - INFO - __main__ - Step 550 Global step 550 Train loss 3.05 on epoch=274
06/24/2022 08:49:02 - INFO - __main__ - Global step 550 Train loss 3.19 ACC 0.0 on epoch=274
06/24/2022 08:49:03 - INFO - __main__ - Step 560 Global step 560 Train loss 3.11 on epoch=279
06/24/2022 08:49:04 - INFO - __main__ - Step 570 Global step 570 Train loss 2.96 on epoch=284
06/24/2022 08:49:05 - INFO - __main__ - Step 580 Global step 580 Train loss 2.84 on epoch=289
06/24/2022 08:49:07 - INFO - __main__ - Step 590 Global step 590 Train loss 2.90 on epoch=294
06/24/2022 08:49:08 - INFO - __main__ - Step 600 Global step 600 Train loss 2.94 on epoch=299
06/24/2022 08:49:19 - INFO - __main__ - Global step 600 Train loss 2.95 ACC 0.0 on epoch=299
06/24/2022 08:49:20 - INFO - __main__ - Step 610 Global step 610 Train loss 2.69 on epoch=304
06/24/2022 08:49:21 - INFO - __main__ - Step 620 Global step 620 Train loss 2.79 on epoch=309
06/24/2022 08:49:22 - INFO - __main__ - Step 630 Global step 630 Train loss 2.72 on epoch=314
06/24/2022 08:49:24 - INFO - __main__ - Step 640 Global step 640 Train loss 2.65 on epoch=319
06/24/2022 08:49:25 - INFO - __main__ - Step 650 Global step 650 Train loss 2.62 on epoch=324
06/24/2022 08:49:37 - INFO - __main__ - Global step 650 Train loss 2.70 ACC 0.0 on epoch=324
06/24/2022 08:49:38 - INFO - __main__ - Step 660 Global step 660 Train loss 2.51 on epoch=329
06/24/2022 08:49:39 - INFO - __main__ - Step 670 Global step 670 Train loss 2.60 on epoch=334
06/24/2022 08:49:41 - INFO - __main__ - Step 680 Global step 680 Train loss 2.63 on epoch=339
06/24/2022 08:49:42 - INFO - __main__ - Step 690 Global step 690 Train loss 2.28 on epoch=344
06/24/2022 08:49:43 - INFO - __main__ - Step 700 Global step 700 Train loss 2.29 on epoch=349
06/24/2022 08:49:54 - INFO - __main__ - Global step 700 Train loss 2.46 ACC 0.03125 on epoch=349
06/24/2022 08:49:54 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=349, global_step=700
06/24/2022 08:49:55 - INFO - __main__ - Step 710 Global step 710 Train loss 2.34 on epoch=354
06/24/2022 08:49:57 - INFO - __main__ - Step 720 Global step 720 Train loss 2.29 on epoch=359
06/24/2022 08:49:58 - INFO - __main__ - Step 730 Global step 730 Train loss 2.25 on epoch=364
06/24/2022 08:49:59 - INFO - __main__ - Step 740 Global step 740 Train loss 2.37 on epoch=369
06/24/2022 08:50:00 - INFO - __main__ - Step 750 Global step 750 Train loss 2.25 on epoch=374
06/24/2022 08:50:01 - INFO - __main__ - Global step 750 Train loss 2.30 ACC 0.375 on epoch=374
06/24/2022 08:50:01 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.375 on epoch=374, global_step=750
06/24/2022 08:50:03 - INFO - __main__ - Step 760 Global step 760 Train loss 2.11 on epoch=379
06/24/2022 08:50:04 - INFO - __main__ - Step 770 Global step 770 Train loss 2.11 on epoch=384
06/24/2022 08:50:05 - INFO - __main__ - Step 780 Global step 780 Train loss 2.13 on epoch=389
06/24/2022 08:50:07 - INFO - __main__ - Step 790 Global step 790 Train loss 1.92 on epoch=394
06/24/2022 08:50:08 - INFO - __main__ - Step 800 Global step 800 Train loss 2.15 on epoch=399
06/24/2022 08:50:08 - INFO - __main__ - Global step 800 Train loss 2.08 ACC 0.5 on epoch=399
06/24/2022 08:50:08 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.5 on epoch=399, global_step=800
06/24/2022 08:50:10 - INFO - __main__ - Step 810 Global step 810 Train loss 1.90 on epoch=404
06/24/2022 08:50:11 - INFO - __main__ - Step 820 Global step 820 Train loss 1.91 on epoch=409
06/24/2022 08:50:12 - INFO - __main__ - Step 830 Global step 830 Train loss 1.84 on epoch=414
06/24/2022 08:50:14 - INFO - __main__ - Step 840 Global step 840 Train loss 1.87 on epoch=419
06/24/2022 08:50:15 - INFO - __main__ - Step 850 Global step 850 Train loss 1.76 on epoch=424
06/24/2022 08:50:15 - INFO - __main__ - Global step 850 Train loss 1.85 ACC 0.4375 on epoch=424
06/24/2022 08:50:17 - INFO - __main__ - Step 860 Global step 860 Train loss 1.84 on epoch=429
06/24/2022 08:50:18 - INFO - __main__ - Step 870 Global step 870 Train loss 1.79 on epoch=434
06/24/2022 08:50:19 - INFO - __main__ - Step 880 Global step 880 Train loss 1.90 on epoch=439
06/24/2022 08:50:21 - INFO - __main__ - Step 890 Global step 890 Train loss 1.62 on epoch=444
06/24/2022 08:50:22 - INFO - __main__ - Step 900 Global step 900 Train loss 1.55 on epoch=449
06/24/2022 08:50:22 - INFO - __main__ - Global step 900 Train loss 1.74 ACC 0.46875 on epoch=449
06/24/2022 08:50:24 - INFO - __main__ - Step 910 Global step 910 Train loss 1.50 on epoch=454
06/24/2022 08:50:25 - INFO - __main__ - Step 920 Global step 920 Train loss 1.61 on epoch=459
06/24/2022 08:50:26 - INFO - __main__ - Step 930 Global step 930 Train loss 1.45 on epoch=464
06/24/2022 08:50:28 - INFO - __main__ - Step 940 Global step 940 Train loss 1.55 on epoch=469
06/24/2022 08:50:29 - INFO - __main__ - Step 950 Global step 950 Train loss 1.31 on epoch=474
06/24/2022 08:50:29 - INFO - __main__ - Global step 950 Train loss 1.48 ACC 0.5 on epoch=474
06/24/2022 08:50:31 - INFO - __main__ - Step 960 Global step 960 Train loss 1.38 on epoch=479
06/24/2022 08:50:32 - INFO - __main__ - Step 970 Global step 970 Train loss 1.26 on epoch=484
06/24/2022 08:50:33 - INFO - __main__ - Step 980 Global step 980 Train loss 1.13 on epoch=489
06/24/2022 08:50:35 - INFO - __main__ - Step 990 Global step 990 Train loss 1.18 on epoch=494
06/24/2022 08:50:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.19 on epoch=499
06/24/2022 08:50:36 - INFO - __main__ - Global step 1000 Train loss 1.23 ACC 0.5 on epoch=499
06/24/2022 08:50:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.30 on epoch=504
06/24/2022 08:50:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.13 on epoch=509
06/24/2022 08:50:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.15 on epoch=514
06/24/2022 08:50:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.11 on epoch=519
06/24/2022 08:50:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.00 on epoch=524
06/24/2022 08:50:43 - INFO - __main__ - Global step 1050 Train loss 1.14 ACC 0.5 on epoch=524
06/24/2022 08:50:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.10 on epoch=529
06/24/2022 08:50:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.03 on epoch=534
06/24/2022 08:50:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.02 on epoch=539
06/24/2022 08:50:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.12 on epoch=544
06/24/2022 08:50:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.00 on epoch=549
06/24/2022 08:50:50 - INFO - __main__ - Global step 1100 Train loss 1.06 ACC 0.5 on epoch=549
06/24/2022 08:50:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.08 on epoch=554
06/24/2022 08:50:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.14 on epoch=559
06/24/2022 08:50:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.89 on epoch=564
06/24/2022 08:50:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.01 on epoch=569
06/24/2022 08:50:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.93 on epoch=574
06/24/2022 08:50:57 - INFO - __main__ - Global step 1150 Train loss 1.01 ACC 0.5 on epoch=574
06/24/2022 08:50:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.84 on epoch=579
06/24/2022 08:51:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.87 on epoch=584
06/24/2022 08:51:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.98 on epoch=589
06/24/2022 08:51:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.80 on epoch=594
06/24/2022 08:51:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.00 on epoch=599
06/24/2022 08:51:04 - INFO - __main__ - Global step 1200 Train loss 0.90 ACC 0.5 on epoch=599
06/24/2022 08:51:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.82 on epoch=604
06/24/2022 08:51:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.99 on epoch=609
06/24/2022 08:51:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.81 on epoch=614
06/24/2022 08:51:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.78 on epoch=619
06/24/2022 08:51:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.02 on epoch=624
06/24/2022 08:51:11 - INFO - __main__ - Global step 1250 Train loss 0.89 ACC 0.5 on epoch=624
06/24/2022 08:51:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.75 on epoch=629
06/24/2022 08:51:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.71 on epoch=634
06/24/2022 08:51:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.74 on epoch=639
06/24/2022 08:51:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.83 on epoch=644
06/24/2022 08:51:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.83 on epoch=649
06/24/2022 08:51:18 - INFO - __main__ - Global step 1300 Train loss 0.77 ACC 0.5 on epoch=649
06/24/2022 08:51:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.69 on epoch=654
06/24/2022 08:51:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.82 on epoch=659
06/24/2022 08:51:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.79 on epoch=664
06/24/2022 08:51:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.78 on epoch=669
06/24/2022 08:51:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.66 on epoch=674
06/24/2022 08:51:25 - INFO - __main__ - Global step 1350 Train loss 0.75 ACC 0.5 on epoch=674
06/24/2022 08:51:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.81 on epoch=679
06/24/2022 08:51:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.74 on epoch=684
06/24/2022 08:51:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.83 on epoch=689
06/24/2022 08:51:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.86 on epoch=694
06/24/2022 08:51:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.85 on epoch=699
06/24/2022 08:51:31 - INFO - __main__ - Global step 1400 Train loss 0.82 ACC 0.5 on epoch=699
06/24/2022 08:51:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.76 on epoch=704
06/24/2022 08:51:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.80 on epoch=709
06/24/2022 08:51:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.73 on epoch=714
06/24/2022 08:51:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.68 on epoch=719
06/24/2022 08:51:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.73 on epoch=724
06/24/2022 08:51:38 - INFO - __main__ - Global step 1450 Train loss 0.74 ACC 0.5 on epoch=724
06/24/2022 08:51:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.71 on epoch=729
06/24/2022 08:51:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.68 on epoch=734
06/24/2022 08:51:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.71 on epoch=739
06/24/2022 08:51:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.73 on epoch=744
06/24/2022 08:51:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.74 on epoch=749
06/24/2022 08:51:45 - INFO - __main__ - Global step 1500 Train loss 0.72 ACC 0.5 on epoch=749
06/24/2022 08:51:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.76 on epoch=754
06/24/2022 08:51:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.70 on epoch=759
06/24/2022 08:51:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.64 on epoch=764
06/24/2022 08:51:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.72 on epoch=769
06/24/2022 08:51:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.63 on epoch=774
06/24/2022 08:51:52 - INFO - __main__ - Global step 1550 Train loss 0.69 ACC 0.5 on epoch=774
06/24/2022 08:51:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.68 on epoch=779
06/24/2022 08:51:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.70 on epoch=784
06/24/2022 08:51:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.72 on epoch=789
06/24/2022 08:51:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.73 on epoch=794
06/24/2022 08:51:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.77 on epoch=799
06/24/2022 08:51:59 - INFO - __main__ - Global step 1600 Train loss 0.72 ACC 0.53125 on epoch=799
06/24/2022 08:51:59 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=799, global_step=1600
06/24/2022 08:52:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.76 on epoch=804
06/24/2022 08:52:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.63 on epoch=809
06/24/2022 08:52:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.79 on epoch=814
06/24/2022 08:52:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.67 on epoch=819
06/24/2022 08:52:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.71 on epoch=824
06/24/2022 08:52:06 - INFO - __main__ - Global step 1650 Train loss 0.71 ACC 0.5 on epoch=824
06/24/2022 08:52:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.60 on epoch=829
06/24/2022 08:52:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.76 on epoch=834
06/24/2022 08:52:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.60 on epoch=839
06/24/2022 08:52:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.70 on epoch=844
06/24/2022 08:52:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.76 on epoch=849
06/24/2022 08:52:13 - INFO - __main__ - Global step 1700 Train loss 0.68 ACC 0.5 on epoch=849
06/24/2022 08:52:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.69 on epoch=854
06/24/2022 08:52:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.70 on epoch=859
06/24/2022 08:52:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.76 on epoch=864
06/24/2022 08:52:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.63 on epoch=869
06/24/2022 08:52:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.69 on epoch=874
06/24/2022 08:52:19 - INFO - __main__ - Global step 1750 Train loss 0.69 ACC 0.5 on epoch=874
06/24/2022 08:52:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.66 on epoch=879
06/24/2022 08:52:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.52 on epoch=884
06/24/2022 08:52:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.74 on epoch=889
06/24/2022 08:52:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.74 on epoch=894
06/24/2022 08:52:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.66 on epoch=899
06/24/2022 08:52:26 - INFO - __main__ - Global step 1800 Train loss 0.66 ACC 0.5 on epoch=899
06/24/2022 08:52:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.60 on epoch=904
06/24/2022 08:52:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.58 on epoch=909
06/24/2022 08:52:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.76 on epoch=914
06/24/2022 08:52:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.66 on epoch=919
06/24/2022 08:52:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.57 on epoch=924
06/24/2022 08:52:33 - INFO - __main__ - Global step 1850 Train loss 0.63 ACC 0.53125 on epoch=924
06/24/2022 08:52:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.59 on epoch=929
06/24/2022 08:52:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.59 on epoch=934
06/24/2022 08:52:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.66 on epoch=939
06/24/2022 08:52:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.57 on epoch=944
06/24/2022 08:52:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.60 on epoch=949
06/24/2022 08:52:40 - INFO - __main__ - Global step 1900 Train loss 0.60 ACC 0.5 on epoch=949
06/24/2022 08:52:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.67 on epoch=954
06/24/2022 08:52:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.61 on epoch=959
06/24/2022 08:52:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.67 on epoch=964
06/24/2022 08:52:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.66 on epoch=969
06/24/2022 08:52:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.61 on epoch=974
06/24/2022 08:52:47 - INFO - __main__ - Global step 1950 Train loss 0.64 ACC 0.5 on epoch=974
06/24/2022 08:52:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.61 on epoch=979
06/24/2022 08:52:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.62 on epoch=984
06/24/2022 08:52:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.55 on epoch=989
06/24/2022 08:52:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.64 on epoch=994
06/24/2022 08:52:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.59 on epoch=999
06/24/2022 08:52:54 - INFO - __main__ - Global step 2000 Train loss 0.60 ACC 0.5 on epoch=999
06/24/2022 08:52:54 - INFO - __main__ - save last model!
06/24/2022 08:52:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 08:52:54 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 08:52:54 - INFO - __main__ - Printing 3 examples
06/24/2022 08:52:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 08:52:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:52:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 08:52:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:52:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 08:52:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:52:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:52:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:52:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:52:54 - INFO - __main__ - Printing 3 examples
06/24/2022 08:52:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 08:52:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:52:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 08:52:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:52:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 08:52:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:52:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 08:52:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:52:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:52:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:52:54 - INFO - __main__ - Printing 3 examples
06/24/2022 08:52:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 08:52:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:52:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 08:52:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:52:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 08:52:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:52:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:52:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:52:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:52:55 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 08:53:01 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:53:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:53:01 - INFO - __main__ - Starting training!
06/24/2022 08:53:02 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_21_0.5_8_predictions.txt
06/24/2022 08:53:02 - INFO - __main__ - ACC on test data: 0.4836
06/24/2022 08:53:02 - INFO - __main__ - prefix=medical_questions_pairs_16_21, lr=0.5, bsz=8, dev_performance=0.53125, test_performance=0.48360655737704916
06/24/2022 08:53:02 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_21, lr=0.4, bsz=8 ...
06/24/2022 08:53:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:53:03 - INFO - __main__ - Printing 3 examples
06/24/2022 08:53:03 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 08:53:03 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:53:03 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 08:53:03 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:53:03 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 08:53:03 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:53:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 08:53:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:53:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:53:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:53:03 - INFO - __main__ - Printing 3 examples
06/24/2022 08:53:03 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 08:53:03 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:53:03 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 08:53:03 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:53:03 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 08:53:03 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:53:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:53:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:53:03 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:53:09 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:53:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:53:09 - INFO - __main__ - Starting training!
06/24/2022 08:53:11 - INFO - __main__ - Step 10 Global step 10 Train loss 8.47 on epoch=4
06/24/2022 08:53:12 - INFO - __main__ - Step 20 Global step 20 Train loss 8.45 on epoch=9
06/24/2022 08:53:13 - INFO - __main__ - Step 30 Global step 30 Train loss 8.15 on epoch=14
06/24/2022 08:53:15 - INFO - __main__ - Step 40 Global step 40 Train loss 8.22 on epoch=19
06/24/2022 08:53:16 - INFO - __main__ - Step 50 Global step 50 Train loss 8.20 on epoch=24
06/24/2022 08:53:17 - INFO - __main__ - Global step 50 Train loss 8.30 ACC 0.0 on epoch=24
06/24/2022 08:53:17 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 08:53:18 - INFO - __main__ - Step 60 Global step 60 Train loss 8.18 on epoch=29
06/24/2022 08:53:20 - INFO - __main__ - Step 70 Global step 70 Train loss 8.10 on epoch=34
06/24/2022 08:53:21 - INFO - __main__ - Step 80 Global step 80 Train loss 8.17 on epoch=39
06/24/2022 08:53:22 - INFO - __main__ - Step 90 Global step 90 Train loss 8.01 on epoch=44
06/24/2022 08:53:23 - INFO - __main__ - Step 100 Global step 100 Train loss 8.00 on epoch=49
06/24/2022 08:53:24 - INFO - __main__ - Global step 100 Train loss 8.09 ACC 0.0 on epoch=49
06/24/2022 08:53:26 - INFO - __main__ - Step 110 Global step 110 Train loss 8.02 on epoch=54
06/24/2022 08:53:27 - INFO - __main__ - Step 120 Global step 120 Train loss 7.93 on epoch=59
06/24/2022 08:53:28 - INFO - __main__ - Step 130 Global step 130 Train loss 7.89 on epoch=64
06/24/2022 08:53:30 - INFO - __main__ - Step 140 Global step 140 Train loss 7.71 on epoch=69
06/24/2022 08:53:31 - INFO - __main__ - Step 150 Global step 150 Train loss 7.65 on epoch=74
06/24/2022 08:53:33 - INFO - __main__ - Global step 150 Train loss 7.84 ACC 0.0 on epoch=74
06/24/2022 08:53:34 - INFO - __main__ - Step 160 Global step 160 Train loss 7.71 on epoch=79
06/24/2022 08:53:35 - INFO - __main__ - Step 170 Global step 170 Train loss 7.65 on epoch=84
06/24/2022 08:53:37 - INFO - __main__ - Step 180 Global step 180 Train loss 7.65 on epoch=89
06/24/2022 08:53:38 - INFO - __main__ - Step 190 Global step 190 Train loss 7.47 on epoch=94
06/24/2022 08:53:39 - INFO - __main__ - Step 200 Global step 200 Train loss 7.52 on epoch=99
06/24/2022 08:53:44 - INFO - __main__ - Global step 200 Train loss 7.60 ACC 0.0 on epoch=99
06/24/2022 08:53:45 - INFO - __main__ - Step 210 Global step 210 Train loss 7.34 on epoch=104
06/24/2022 08:53:46 - INFO - __main__ - Step 220 Global step 220 Train loss 7.21 on epoch=109
06/24/2022 08:53:47 - INFO - __main__ - Step 230 Global step 230 Train loss 6.94 on epoch=114
06/24/2022 08:53:49 - INFO - __main__ - Step 240 Global step 240 Train loss 6.79 on epoch=119
06/24/2022 08:53:50 - INFO - __main__ - Step 250 Global step 250 Train loss 6.66 on epoch=124
06/24/2022 08:53:52 - INFO - __main__ - Global step 250 Train loss 6.99 ACC 0.0 on epoch=124
06/24/2022 08:53:54 - INFO - __main__ - Step 260 Global step 260 Train loss 6.48 on epoch=129
06/24/2022 08:53:55 - INFO - __main__ - Step 270 Global step 270 Train loss 6.41 on epoch=134
06/24/2022 08:53:56 - INFO - __main__ - Step 280 Global step 280 Train loss 6.26 on epoch=139
06/24/2022 08:53:57 - INFO - __main__ - Step 290 Global step 290 Train loss 6.19 on epoch=144
06/24/2022 08:53:59 - INFO - __main__ - Step 300 Global step 300 Train loss 5.96 on epoch=149
06/24/2022 08:54:07 - INFO - __main__ - Global step 300 Train loss 6.26 ACC 0.0 on epoch=149
06/24/2022 08:54:08 - INFO - __main__ - Step 310 Global step 310 Train loss 5.80 on epoch=154
06/24/2022 08:54:10 - INFO - __main__ - Step 320 Global step 320 Train loss 5.59 on epoch=159
06/24/2022 08:54:11 - INFO - __main__ - Step 330 Global step 330 Train loss 5.56 on epoch=164
06/24/2022 08:54:12 - INFO - __main__ - Step 340 Global step 340 Train loss 5.40 on epoch=169
06/24/2022 08:54:14 - INFO - __main__ - Step 350 Global step 350 Train loss 5.34 on epoch=174
06/24/2022 08:54:21 - INFO - __main__ - Global step 350 Train loss 5.54 ACC 0.0 on epoch=174
06/24/2022 08:54:22 - INFO - __main__ - Step 360 Global step 360 Train loss 5.18 on epoch=179
06/24/2022 08:54:24 - INFO - __main__ - Step 370 Global step 370 Train loss 5.19 on epoch=184
06/24/2022 08:54:25 - INFO - __main__ - Step 380 Global step 380 Train loss 4.92 on epoch=189
06/24/2022 08:54:26 - INFO - __main__ - Step 390 Global step 390 Train loss 4.75 on epoch=194
06/24/2022 08:54:27 - INFO - __main__ - Step 400 Global step 400 Train loss 4.60 on epoch=199
06/24/2022 08:54:31 - INFO - __main__ - Global step 400 Train loss 4.93 ACC 0.0 on epoch=199
06/24/2022 08:54:32 - INFO - __main__ - Step 410 Global step 410 Train loss 4.37 on epoch=204
06/24/2022 08:54:33 - INFO - __main__ - Step 420 Global step 420 Train loss 4.28 on epoch=209
06/24/2022 08:54:34 - INFO - __main__ - Step 430 Global step 430 Train loss 4.11 on epoch=214
06/24/2022 08:54:36 - INFO - __main__ - Step 440 Global step 440 Train loss 4.02 on epoch=219
06/24/2022 08:54:37 - INFO - __main__ - Step 450 Global step 450 Train loss 3.86 on epoch=224
06/24/2022 08:54:40 - INFO - __main__ - Global step 450 Train loss 4.13 ACC 0.0 on epoch=224
06/24/2022 08:54:42 - INFO - __main__ - Step 460 Global step 460 Train loss 3.34 on epoch=229
06/24/2022 08:54:43 - INFO - __main__ - Step 470 Global step 470 Train loss 3.17 on epoch=234
06/24/2022 08:54:44 - INFO - __main__ - Step 480 Global step 480 Train loss 3.02 on epoch=239
06/24/2022 08:54:45 - INFO - __main__ - Step 490 Global step 490 Train loss 2.88 on epoch=244
06/24/2022 08:54:47 - INFO - __main__ - Step 500 Global step 500 Train loss 2.85 on epoch=249
06/24/2022 08:54:57 - INFO - __main__ - Global step 500 Train loss 3.05 ACC 0.0 on epoch=249
06/24/2022 08:54:59 - INFO - __main__ - Step 510 Global step 510 Train loss 2.95 on epoch=254
06/24/2022 08:55:00 - INFO - __main__ - Step 520 Global step 520 Train loss 2.73 on epoch=259
06/24/2022 08:55:01 - INFO - __main__ - Step 530 Global step 530 Train loss 2.58 on epoch=264
06/24/2022 08:55:03 - INFO - __main__ - Step 540 Global step 540 Train loss 2.55 on epoch=269
06/24/2022 08:55:04 - INFO - __main__ - Step 550 Global step 550 Train loss 2.43 on epoch=274
06/24/2022 08:55:05 - INFO - __main__ - Global step 550 Train loss 2.65 ACC 0.03125 on epoch=274
06/24/2022 08:55:05 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=274, global_step=550
06/24/2022 08:55:06 - INFO - __main__ - Step 560 Global step 560 Train loss 2.23 on epoch=279
06/24/2022 08:55:08 - INFO - __main__ - Step 570 Global step 570 Train loss 2.60 on epoch=284
06/24/2022 08:55:09 - INFO - __main__ - Step 580 Global step 580 Train loss 2.38 on epoch=289
06/24/2022 08:55:10 - INFO - __main__ - Step 590 Global step 590 Train loss 2.56 on epoch=294
06/24/2022 08:55:11 - INFO - __main__ - Step 600 Global step 600 Train loss 2.30 on epoch=299
06/24/2022 08:55:17 - INFO - __main__ - Global step 600 Train loss 2.42 ACC 0.28125 on epoch=299
06/24/2022 08:55:17 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.28125 on epoch=299, global_step=600
06/24/2022 08:55:18 - INFO - __main__ - Step 610 Global step 610 Train loss 2.15 on epoch=304
06/24/2022 08:55:20 - INFO - __main__ - Step 620 Global step 620 Train loss 2.05 on epoch=309
06/24/2022 08:55:21 - INFO - __main__ - Step 630 Global step 630 Train loss 2.14 on epoch=314
06/24/2022 08:55:22 - INFO - __main__ - Step 640 Global step 640 Train loss 2.04 on epoch=319
06/24/2022 08:55:23 - INFO - __main__ - Step 650 Global step 650 Train loss 2.18 on epoch=324
06/24/2022 08:55:24 - INFO - __main__ - Global step 650 Train loss 2.11 ACC 0.5 on epoch=324
06/24/2022 08:55:24 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.5 on epoch=324, global_step=650
06/24/2022 08:55:25 - INFO - __main__ - Step 660 Global step 660 Train loss 2.14 on epoch=329
06/24/2022 08:55:27 - INFO - __main__ - Step 670 Global step 670 Train loss 1.93 on epoch=334
06/24/2022 08:55:28 - INFO - __main__ - Step 680 Global step 680 Train loss 1.90 on epoch=339
06/24/2022 08:55:29 - INFO - __main__ - Step 690 Global step 690 Train loss 1.95 on epoch=344
06/24/2022 08:55:30 - INFO - __main__ - Step 700 Global step 700 Train loss 1.86 on epoch=349
06/24/2022 08:55:31 - INFO - __main__ - Global step 700 Train loss 1.96 ACC 0.5 on epoch=349
06/24/2022 08:55:32 - INFO - __main__ - Step 710 Global step 710 Train loss 1.88 on epoch=354
06/24/2022 08:55:34 - INFO - __main__ - Step 720 Global step 720 Train loss 1.79 on epoch=359
06/24/2022 08:55:35 - INFO - __main__ - Step 730 Global step 730 Train loss 1.75 on epoch=364
06/24/2022 08:55:36 - INFO - __main__ - Step 740 Global step 740 Train loss 1.73 on epoch=369
06/24/2022 08:55:37 - INFO - __main__ - Step 750 Global step 750 Train loss 1.74 on epoch=374
06/24/2022 08:55:38 - INFO - __main__ - Global step 750 Train loss 1.78 ACC 0.6875 on epoch=374
06/24/2022 08:55:38 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.6875 on epoch=374, global_step=750
06/24/2022 08:55:39 - INFO - __main__ - Step 760 Global step 760 Train loss 1.62 on epoch=379
06/24/2022 08:55:41 - INFO - __main__ - Step 770 Global step 770 Train loss 1.64 on epoch=384
06/24/2022 08:55:42 - INFO - __main__ - Step 780 Global step 780 Train loss 1.62 on epoch=389
06/24/2022 08:55:43 - INFO - __main__ - Step 790 Global step 790 Train loss 1.43 on epoch=394
06/24/2022 08:55:45 - INFO - __main__ - Step 800 Global step 800 Train loss 1.53 on epoch=399
06/24/2022 08:55:45 - INFO - __main__ - Global step 800 Train loss 1.57 ACC 0.5 on epoch=399
06/24/2022 08:55:46 - INFO - __main__ - Step 810 Global step 810 Train loss 1.48 on epoch=404
06/24/2022 08:55:48 - INFO - __main__ - Step 820 Global step 820 Train loss 1.37 on epoch=409
06/24/2022 08:55:49 - INFO - __main__ - Step 830 Global step 830 Train loss 1.41 on epoch=414
06/24/2022 08:55:50 - INFO - __main__ - Step 840 Global step 840 Train loss 1.41 on epoch=419
06/24/2022 08:55:51 - INFO - __main__ - Step 850 Global step 850 Train loss 1.29 on epoch=424
06/24/2022 08:55:52 - INFO - __main__ - Global step 850 Train loss 1.39 ACC 0.53125 on epoch=424
06/24/2022 08:55:53 - INFO - __main__ - Step 860 Global step 860 Train loss 1.23 on epoch=429
06/24/2022 08:55:55 - INFO - __main__ - Step 870 Global step 870 Train loss 1.29 on epoch=434
06/24/2022 08:55:56 - INFO - __main__ - Step 880 Global step 880 Train loss 1.19 on epoch=439
06/24/2022 08:55:57 - INFO - __main__ - Step 890 Global step 890 Train loss 1.23 on epoch=444
06/24/2022 08:55:58 - INFO - __main__ - Step 900 Global step 900 Train loss 1.14 on epoch=449
06/24/2022 08:55:59 - INFO - __main__ - Global step 900 Train loss 1.22 ACC 0.5625 on epoch=449
06/24/2022 08:56:00 - INFO - __main__ - Step 910 Global step 910 Train loss 1.23 on epoch=454
06/24/2022 08:56:02 - INFO - __main__ - Step 920 Global step 920 Train loss 1.22 on epoch=459
06/24/2022 08:56:03 - INFO - __main__ - Step 930 Global step 930 Train loss 1.00 on epoch=464
06/24/2022 08:56:04 - INFO - __main__ - Step 940 Global step 940 Train loss 1.06 on epoch=469
06/24/2022 08:56:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.99 on epoch=474
06/24/2022 08:56:06 - INFO - __main__ - Global step 950 Train loss 1.10 ACC 0.40625 on epoch=474
06/24/2022 08:56:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.88 on epoch=479
06/24/2022 08:56:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.99 on epoch=484
06/24/2022 08:56:10 - INFO - __main__ - Step 980 Global step 980 Train loss 1.01 on epoch=489
06/24/2022 08:56:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.92 on epoch=494
06/24/2022 08:56:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.84 on epoch=499
06/24/2022 08:56:13 - INFO - __main__ - Global step 1000 Train loss 0.93 ACC 0.625 on epoch=499
06/24/2022 08:56:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.04 on epoch=504
06/24/2022 08:56:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.90 on epoch=509
06/24/2022 08:56:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.82 on epoch=514
06/24/2022 08:56:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.86 on epoch=519
06/24/2022 08:56:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.74 on epoch=524
06/24/2022 08:56:20 - INFO - __main__ - Global step 1050 Train loss 0.87 ACC 0.53125 on epoch=524
06/24/2022 08:56:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.95 on epoch=529
06/24/2022 08:56:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.86 on epoch=534
06/24/2022 08:56:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.90 on epoch=539
06/24/2022 08:56:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.84 on epoch=544
06/24/2022 08:56:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.83 on epoch=549
06/24/2022 08:56:26 - INFO - __main__ - Global step 1100 Train loss 0.87 ACC 0.59375 on epoch=549
06/24/2022 08:56:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.93 on epoch=554
06/24/2022 08:56:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.86 on epoch=559
06/24/2022 08:56:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.74 on epoch=564
06/24/2022 08:56:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.81 on epoch=569
06/24/2022 08:56:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.77 on epoch=574
06/24/2022 08:56:33 - INFO - __main__ - Global step 1150 Train loss 0.82 ACC 0.53125 on epoch=574
06/24/2022 08:56:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.61 on epoch=579
06/24/2022 08:56:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.83 on epoch=584
06/24/2022 08:56:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.75 on epoch=589
06/24/2022 08:56:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.78 on epoch=594
06/24/2022 08:56:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.75 on epoch=599
06/24/2022 08:56:40 - INFO - __main__ - Global step 1200 Train loss 0.74 ACC 0.46875 on epoch=599
06/24/2022 08:56:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.68 on epoch=604
06/24/2022 08:56:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.70 on epoch=609
06/24/2022 08:56:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.82 on epoch=614
06/24/2022 08:56:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.87 on epoch=619
06/24/2022 08:56:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.65 on epoch=624
06/24/2022 08:56:47 - INFO - __main__ - Global step 1250 Train loss 0.74 ACC 0.53125 on epoch=624
06/24/2022 08:56:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.83 on epoch=629
06/24/2022 08:56:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.67 on epoch=634
06/24/2022 08:56:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.68 on epoch=639
06/24/2022 08:56:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.69 on epoch=644
06/24/2022 08:56:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.72 on epoch=649
06/24/2022 08:56:54 - INFO - __main__ - Global step 1300 Train loss 0.72 ACC 0.59375 on epoch=649
06/24/2022 08:56:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.85 on epoch=654
06/24/2022 08:56:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.82 on epoch=659
06/24/2022 08:56:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.67 on epoch=664
06/24/2022 08:56:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.66 on epoch=669
06/24/2022 08:57:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.64 on epoch=674
06/24/2022 08:57:01 - INFO - __main__ - Global step 1350 Train loss 0.73 ACC 0.46875 on epoch=674
06/24/2022 08:57:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.60 on epoch=679
06/24/2022 08:57:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.71 on epoch=684
06/24/2022 08:57:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.74 on epoch=689
06/24/2022 08:57:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.64 on epoch=694
06/24/2022 08:57:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.71 on epoch=699
06/24/2022 08:57:08 - INFO - __main__ - Global step 1400 Train loss 0.68 ACC 0.46875 on epoch=699
06/24/2022 08:57:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.67 on epoch=704
06/24/2022 08:57:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.71 on epoch=709
06/24/2022 08:57:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.70 on epoch=714
06/24/2022 08:57:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.78 on epoch=719
06/24/2022 08:57:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.80 on epoch=724
06/24/2022 08:57:14 - INFO - __main__ - Global step 1450 Train loss 0.73 ACC 0.5 on epoch=724
06/24/2022 08:57:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.71 on epoch=729
06/24/2022 08:57:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.71 on epoch=734
06/24/2022 08:57:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.85 on epoch=739
06/24/2022 08:57:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.72 on epoch=744
06/24/2022 08:57:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.68 on epoch=749
06/24/2022 08:57:21 - INFO - __main__ - Global step 1500 Train loss 0.73 ACC 0.53125 on epoch=749
06/24/2022 08:57:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.77 on epoch=754
06/24/2022 08:57:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.70 on epoch=759
06/24/2022 08:57:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.65 on epoch=764
06/24/2022 08:57:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.59 on epoch=769
06/24/2022 08:57:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.81 on epoch=774
06/24/2022 08:57:28 - INFO - __main__ - Global step 1550 Train loss 0.70 ACC 0.5 on epoch=774
06/24/2022 08:57:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.80 on epoch=779
06/24/2022 08:57:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.67 on epoch=784
06/24/2022 08:57:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.62 on epoch=789
06/24/2022 08:57:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.60 on epoch=794
06/24/2022 08:57:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.73 on epoch=799
06/24/2022 08:57:35 - INFO - __main__ - Global step 1600 Train loss 0.69 ACC 0.5 on epoch=799
06/24/2022 08:57:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.72 on epoch=804
06/24/2022 08:57:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.69 on epoch=809
06/24/2022 08:57:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.60 on epoch=814
06/24/2022 08:57:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.59 on epoch=819
06/24/2022 08:57:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.65 on epoch=824
06/24/2022 08:57:42 - INFO - __main__ - Global step 1650 Train loss 0.65 ACC 0.5 on epoch=824
06/24/2022 08:57:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.65 on epoch=829
06/24/2022 08:57:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.58 on epoch=834
06/24/2022 08:57:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.66 on epoch=839
06/24/2022 08:57:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.79 on epoch=844
06/24/2022 08:57:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.67 on epoch=849
06/24/2022 08:57:49 - INFO - __main__ - Global step 1700 Train loss 0.67 ACC 0.5 on epoch=849
06/24/2022 08:57:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.62 on epoch=854
06/24/2022 08:57:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.67 on epoch=859
06/24/2022 08:57:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.57 on epoch=864
06/24/2022 08:57:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.64 on epoch=869
06/24/2022 08:57:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.74 on epoch=874
06/24/2022 08:57:56 - INFO - __main__ - Global step 1750 Train loss 0.65 ACC 0.46875 on epoch=874
06/24/2022 08:57:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.61 on epoch=879
06/24/2022 08:57:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.62 on epoch=884
06/24/2022 08:57:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.64 on epoch=889
06/24/2022 08:58:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.62 on epoch=894
06/24/2022 08:58:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.53 on epoch=899
06/24/2022 08:58:02 - INFO - __main__ - Global step 1800 Train loss 0.60 ACC 0.5 on epoch=899
06/24/2022 08:58:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.60 on epoch=904
06/24/2022 08:58:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.63 on epoch=909
06/24/2022 08:58:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.58 on epoch=914
06/24/2022 08:58:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.65 on epoch=919
06/24/2022 08:58:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.67 on epoch=924
06/24/2022 08:58:09 - INFO - __main__ - Global step 1850 Train loss 0.63 ACC 0.5 on epoch=924
06/24/2022 08:58:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.70 on epoch=929
06/24/2022 08:58:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.61 on epoch=934
06/24/2022 08:58:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.59 on epoch=939
06/24/2022 08:58:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.56 on epoch=944
06/24/2022 08:58:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.63 on epoch=949
06/24/2022 08:58:16 - INFO - __main__ - Global step 1900 Train loss 0.62 ACC 0.5 on epoch=949
06/24/2022 08:58:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.58 on epoch=954
06/24/2022 08:58:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.70 on epoch=959
06/24/2022 08:58:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.53 on epoch=964
06/24/2022 08:58:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.56 on epoch=969
06/24/2022 08:58:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.53 on epoch=974
06/24/2022 08:58:23 - INFO - __main__ - Global step 1950 Train loss 0.58 ACC 0.5 on epoch=974
06/24/2022 08:58:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.57 on epoch=979
06/24/2022 08:58:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.51 on epoch=984
06/24/2022 08:58:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.60 on epoch=989
06/24/2022 08:58:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.60 on epoch=994
06/24/2022 08:58:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.67 on epoch=999
06/24/2022 08:58:29 - INFO - __main__ - Global step 2000 Train loss 0.59 ACC 0.5 on epoch=999
06/24/2022 08:58:29 - INFO - __main__ - save last model!
06/24/2022 08:58:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 08:58:30 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 08:58:30 - INFO - __main__ - Printing 3 examples
06/24/2022 08:58:30 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 08:58:30 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:30 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 08:58:30 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:30 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 08:58:30 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:30 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:58:30 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:58:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:58:30 - INFO - __main__ - Printing 3 examples
06/24/2022 08:58:30 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 08:58:30 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:30 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 08:58:30 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:30 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 08:58:30 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 08:58:30 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:58:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:58:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:58:30 - INFO - __main__ - Printing 3 examples
06/24/2022 08:58:30 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 08:58:30 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:30 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 08:58:30 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:30 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 08:58:30 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:30 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:58:30 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:58:30 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:58:30 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 08:58:36 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:58:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:58:36 - INFO - __main__ - Starting training!
06/24/2022 08:58:37 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_21_0.4_8_predictions.txt
06/24/2022 08:58:37 - INFO - __main__ - ACC on test data: 0.4836
06/24/2022 08:58:37 - INFO - __main__ - prefix=medical_questions_pairs_16_21, lr=0.4, bsz=8, dev_performance=0.6875, test_performance=0.48360655737704916
06/24/2022 08:58:37 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_21, lr=0.3, bsz=8 ...
06/24/2022 08:58:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:58:38 - INFO - __main__ - Printing 3 examples
06/24/2022 08:58:38 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 08:58:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:38 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 08:58:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:38 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 08:58:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 08:58:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:58:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 08:58:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 08:58:38 - INFO - __main__ - Printing 3 examples
06/24/2022 08:58:38 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 08:58:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 08:58:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 08:58:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 08:58:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 08:58:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 08:58:38 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 08:58:44 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 08:58:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 08:58:44 - INFO - __main__ - Starting training!
06/24/2022 08:58:45 - INFO - __main__ - Step 10 Global step 10 Train loss 8.36 on epoch=4
06/24/2022 08:58:47 - INFO - __main__ - Step 20 Global step 20 Train loss 8.41 on epoch=9
06/24/2022 08:58:48 - INFO - __main__ - Step 30 Global step 30 Train loss 8.25 on epoch=14
06/24/2022 08:58:49 - INFO - __main__ - Step 40 Global step 40 Train loss 8.26 on epoch=19
06/24/2022 08:58:50 - INFO - __main__ - Step 50 Global step 50 Train loss 8.28 on epoch=24
06/24/2022 08:58:52 - INFO - __main__ - Global step 50 Train loss 8.31 ACC 0.0 on epoch=24
06/24/2022 08:58:52 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 08:58:53 - INFO - __main__ - Step 60 Global step 60 Train loss 8.26 on epoch=29
06/24/2022 08:58:54 - INFO - __main__ - Step 70 Global step 70 Train loss 8.17 on epoch=34
06/24/2022 08:58:56 - INFO - __main__ - Step 80 Global step 80 Train loss 8.17 on epoch=39
06/24/2022 08:58:57 - INFO - __main__ - Step 90 Global step 90 Train loss 8.20 on epoch=44
06/24/2022 08:58:58 - INFO - __main__ - Step 100 Global step 100 Train loss 8.20 on epoch=49
06/24/2022 08:59:02 - INFO - __main__ - Global step 100 Train loss 8.20 ACC 0.0 on epoch=49
06/24/2022 08:59:03 - INFO - __main__ - Step 110 Global step 110 Train loss 8.26 on epoch=54
06/24/2022 08:59:05 - INFO - __main__ - Step 120 Global step 120 Train loss 7.93 on epoch=59
06/24/2022 08:59:06 - INFO - __main__ - Step 130 Global step 130 Train loss 8.09 on epoch=64
06/24/2022 08:59:07 - INFO - __main__ - Step 140 Global step 140 Train loss 8.15 on epoch=69
06/24/2022 08:59:08 - INFO - __main__ - Step 150 Global step 150 Train loss 7.96 on epoch=74
06/24/2022 08:59:11 - INFO - __main__ - Global step 150 Train loss 8.08 ACC 0.0 on epoch=74
06/24/2022 08:59:13 - INFO - __main__ - Step 160 Global step 160 Train loss 8.04 on epoch=79
06/24/2022 08:59:14 - INFO - __main__ - Step 170 Global step 170 Train loss 8.07 on epoch=84
06/24/2022 08:59:15 - INFO - __main__ - Step 180 Global step 180 Train loss 7.96 on epoch=89
06/24/2022 08:59:16 - INFO - __main__ - Step 190 Global step 190 Train loss 8.00 on epoch=94
06/24/2022 08:59:18 - INFO - __main__ - Step 200 Global step 200 Train loss 7.92 on epoch=99
06/24/2022 08:59:22 - INFO - __main__ - Global step 200 Train loss 8.00 ACC 0.0 on epoch=99
06/24/2022 08:59:23 - INFO - __main__ - Step 210 Global step 210 Train loss 7.85 on epoch=104
06/24/2022 08:59:24 - INFO - __main__ - Step 220 Global step 220 Train loss 7.96 on epoch=109
06/24/2022 08:59:26 - INFO - __main__ - Step 230 Global step 230 Train loss 7.84 on epoch=114
06/24/2022 08:59:27 - INFO - __main__ - Step 240 Global step 240 Train loss 7.95 on epoch=119
06/24/2022 08:59:28 - INFO - __main__ - Step 250 Global step 250 Train loss 7.77 on epoch=124
06/24/2022 08:59:31 - INFO - __main__ - Global step 250 Train loss 7.87 ACC 0.0 on epoch=124
06/24/2022 08:59:32 - INFO - __main__ - Step 260 Global step 260 Train loss 7.91 on epoch=129
06/24/2022 08:59:34 - INFO - __main__ - Step 270 Global step 270 Train loss 7.77 on epoch=134
06/24/2022 08:59:35 - INFO - __main__ - Step 280 Global step 280 Train loss 7.79 on epoch=139
06/24/2022 08:59:36 - INFO - __main__ - Step 290 Global step 290 Train loss 7.80 on epoch=144
06/24/2022 08:59:37 - INFO - __main__ - Step 300 Global step 300 Train loss 7.59 on epoch=149
06/24/2022 08:59:42 - INFO - __main__ - Global step 300 Train loss 7.77 ACC 0.0 on epoch=149
06/24/2022 08:59:43 - INFO - __main__ - Step 310 Global step 310 Train loss 7.59 on epoch=154
06/24/2022 08:59:44 - INFO - __main__ - Step 320 Global step 320 Train loss 7.54 on epoch=159
06/24/2022 08:59:46 - INFO - __main__ - Step 330 Global step 330 Train loss 7.50 on epoch=164
06/24/2022 08:59:47 - INFO - __main__ - Step 340 Global step 340 Train loss 7.42 on epoch=169
06/24/2022 08:59:48 - INFO - __main__ - Step 350 Global step 350 Train loss 7.40 on epoch=174
06/24/2022 08:59:52 - INFO - __main__ - Global step 350 Train loss 7.49 ACC 0.0 on epoch=174
06/24/2022 08:59:54 - INFO - __main__ - Step 360 Global step 360 Train loss 7.22 on epoch=179
06/24/2022 08:59:55 - INFO - __main__ - Step 370 Global step 370 Train loss 7.13 on epoch=184
06/24/2022 08:59:56 - INFO - __main__ - Step 380 Global step 380 Train loss 7.00 on epoch=189
06/24/2022 08:59:57 - INFO - __main__ - Step 390 Global step 390 Train loss 6.95 on epoch=194
06/24/2022 08:59:59 - INFO - __main__ - Step 400 Global step 400 Train loss 6.71 on epoch=199
06/24/2022 09:00:00 - INFO - __main__ - Global step 400 Train loss 7.00 ACC 0.0 on epoch=199
06/24/2022 09:00:01 - INFO - __main__ - Step 410 Global step 410 Train loss 6.73 on epoch=204
06/24/2022 09:00:02 - INFO - __main__ - Step 420 Global step 420 Train loss 6.64 on epoch=209
06/24/2022 09:00:04 - INFO - __main__ - Step 430 Global step 430 Train loss 6.51 on epoch=214
06/24/2022 09:00:05 - INFO - __main__ - Step 440 Global step 440 Train loss 6.43 on epoch=219
06/24/2022 09:00:06 - INFO - __main__ - Step 450 Global step 450 Train loss 6.42 on epoch=224
06/24/2022 09:00:09 - INFO - __main__ - Global step 450 Train loss 6.55 ACC 0.0 on epoch=224
06/24/2022 09:00:11 - INFO - __main__ - Step 460 Global step 460 Train loss 6.26 on epoch=229
06/24/2022 09:00:12 - INFO - __main__ - Step 470 Global step 470 Train loss 6.12 on epoch=234
06/24/2022 09:00:13 - INFO - __main__ - Step 480 Global step 480 Train loss 6.06 on epoch=239
06/24/2022 09:00:14 - INFO - __main__ - Step 490 Global step 490 Train loss 6.10 on epoch=244
06/24/2022 09:00:15 - INFO - __main__ - Step 500 Global step 500 Train loss 5.85 on epoch=249
06/24/2022 09:00:21 - INFO - __main__ - Global step 500 Train loss 6.08 ACC 0.0 on epoch=249
06/24/2022 09:00:22 - INFO - __main__ - Step 510 Global step 510 Train loss 5.83 on epoch=254
06/24/2022 09:00:23 - INFO - __main__ - Step 520 Global step 520 Train loss 5.72 on epoch=259
06/24/2022 09:00:24 - INFO - __main__ - Step 530 Global step 530 Train loss 5.64 on epoch=264
06/24/2022 09:00:26 - INFO - __main__ - Step 540 Global step 540 Train loss 5.65 on epoch=269
06/24/2022 09:00:27 - INFO - __main__ - Step 550 Global step 550 Train loss 5.46 on epoch=274
06/24/2022 09:00:34 - INFO - __main__ - Global step 550 Train loss 5.66 ACC 0.0 on epoch=274
06/24/2022 09:00:35 - INFO - __main__ - Step 560 Global step 560 Train loss 5.50 on epoch=279
06/24/2022 09:00:36 - INFO - __main__ - Step 570 Global step 570 Train loss 5.47 on epoch=284
06/24/2022 09:00:37 - INFO - __main__ - Step 580 Global step 580 Train loss 5.31 on epoch=289
06/24/2022 09:00:38 - INFO - __main__ - Step 590 Global step 590 Train loss 5.11 on epoch=294
06/24/2022 09:00:40 - INFO - __main__ - Step 600 Global step 600 Train loss 5.08 on epoch=299
06/24/2022 09:00:41 - INFO - __main__ - Global step 600 Train loss 5.29 ACC 0.0 on epoch=299
06/24/2022 09:00:43 - INFO - __main__ - Step 610 Global step 610 Train loss 4.81 on epoch=304
06/24/2022 09:00:44 - INFO - __main__ - Step 620 Global step 620 Train loss 4.78 on epoch=309
06/24/2022 09:00:45 - INFO - __main__ - Step 630 Global step 630 Train loss 4.78 on epoch=314
06/24/2022 09:00:46 - INFO - __main__ - Step 640 Global step 640 Train loss 4.54 on epoch=319
06/24/2022 09:00:48 - INFO - __main__ - Step 650 Global step 650 Train loss 4.45 on epoch=324
06/24/2022 09:00:49 - INFO - __main__ - Global step 650 Train loss 4.67 ACC 0.0 on epoch=324
06/24/2022 09:00:50 - INFO - __main__ - Step 660 Global step 660 Train loss 4.19 on epoch=329
06/24/2022 09:00:52 - INFO - __main__ - Step 670 Global step 670 Train loss 4.27 on epoch=334
06/24/2022 09:00:53 - INFO - __main__ - Step 680 Global step 680 Train loss 3.87 on epoch=339
06/24/2022 09:00:54 - INFO - __main__ - Step 690 Global step 690 Train loss 3.74 on epoch=344
06/24/2022 09:00:55 - INFO - __main__ - Step 700 Global step 700 Train loss 3.71 on epoch=349
06/24/2022 09:00:57 - INFO - __main__ - Global step 700 Train loss 3.95 ACC 0.0 on epoch=349
06/24/2022 09:00:58 - INFO - __main__ - Step 710 Global step 710 Train loss 3.62 on epoch=354
06/24/2022 09:00:59 - INFO - __main__ - Step 720 Global step 720 Train loss 3.42 on epoch=359
06/24/2022 09:01:01 - INFO - __main__ - Step 730 Global step 730 Train loss 3.44 on epoch=364
06/24/2022 09:01:02 - INFO - __main__ - Step 740 Global step 740 Train loss 3.20 on epoch=369
06/24/2022 09:01:03 - INFO - __main__ - Step 750 Global step 750 Train loss 3.28 on epoch=374
06/24/2022 09:01:04 - INFO - __main__ - Global step 750 Train loss 3.39 ACC 0.0 on epoch=374
06/24/2022 09:01:06 - INFO - __main__ - Step 760 Global step 760 Train loss 3.24 on epoch=379
06/24/2022 09:01:07 - INFO - __main__ - Step 770 Global step 770 Train loss 3.14 on epoch=384
06/24/2022 09:01:08 - INFO - __main__ - Step 780 Global step 780 Train loss 3.26 on epoch=389
06/24/2022 09:01:09 - INFO - __main__ - Step 790 Global step 790 Train loss 2.94 on epoch=394
06/24/2022 09:01:11 - INFO - __main__ - Step 800 Global step 800 Train loss 2.90 on epoch=399
06/24/2022 09:01:12 - INFO - __main__ - Global step 800 Train loss 3.09 ACC 0.0 on epoch=399
06/24/2022 09:01:13 - INFO - __main__ - Step 810 Global step 810 Train loss 3.06 on epoch=404
06/24/2022 09:01:14 - INFO - __main__ - Step 820 Global step 820 Train loss 2.90 on epoch=409
06/24/2022 09:01:16 - INFO - __main__ - Step 830 Global step 830 Train loss 3.01 on epoch=414
06/24/2022 09:01:17 - INFO - __main__ - Step 840 Global step 840 Train loss 2.92 on epoch=419
06/24/2022 09:01:18 - INFO - __main__ - Step 850 Global step 850 Train loss 2.81 on epoch=424
06/24/2022 09:01:19 - INFO - __main__ - Global step 850 Train loss 2.94 ACC 0.0 on epoch=424
06/24/2022 09:01:21 - INFO - __main__ - Step 860 Global step 860 Train loss 2.92 on epoch=429
06/24/2022 09:01:22 - INFO - __main__ - Step 870 Global step 870 Train loss 2.81 on epoch=434
06/24/2022 09:01:23 - INFO - __main__ - Step 880 Global step 880 Train loss 2.77 on epoch=439
06/24/2022 09:01:24 - INFO - __main__ - Step 890 Global step 890 Train loss 2.88 on epoch=444
06/24/2022 09:01:26 - INFO - __main__ - Step 900 Global step 900 Train loss 2.67 on epoch=449
06/24/2022 09:01:27 - INFO - __main__ - Global step 900 Train loss 2.81 ACC 0.0 on epoch=449
06/24/2022 09:01:29 - INFO - __main__ - Step 910 Global step 910 Train loss 2.53 on epoch=454
06/24/2022 09:01:30 - INFO - __main__ - Step 920 Global step 920 Train loss 2.55 on epoch=459
06/24/2022 09:01:31 - INFO - __main__ - Step 930 Global step 930 Train loss 2.55 on epoch=464
06/24/2022 09:01:32 - INFO - __main__ - Step 940 Global step 940 Train loss 2.47 on epoch=469
06/24/2022 09:01:34 - INFO - __main__ - Step 950 Global step 950 Train loss 2.49 on epoch=474
06/24/2022 09:01:36 - INFO - __main__ - Global step 950 Train loss 2.52 ACC 0.0 on epoch=474
06/24/2022 09:01:37 - INFO - __main__ - Step 960 Global step 960 Train loss 2.59 on epoch=479
06/24/2022 09:01:38 - INFO - __main__ - Step 970 Global step 970 Train loss 2.49 on epoch=484
06/24/2022 09:01:39 - INFO - __main__ - Step 980 Global step 980 Train loss 2.48 on epoch=489
06/24/2022 09:01:41 - INFO - __main__ - Step 990 Global step 990 Train loss 2.36 on epoch=494
06/24/2022 09:01:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.33 on epoch=499
06/24/2022 09:01:43 - INFO - __main__ - Global step 1000 Train loss 2.45 ACC 0.0 on epoch=499
06/24/2022 09:01:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.46 on epoch=504
06/24/2022 09:01:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.46 on epoch=509
06/24/2022 09:01:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.40 on epoch=514
06/24/2022 09:01:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.42 on epoch=519
06/24/2022 09:01:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.29 on epoch=524
06/24/2022 09:01:51 - INFO - __main__ - Global step 1050 Train loss 2.40 ACC 0.0 on epoch=524
06/24/2022 09:01:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.24 on epoch=529
06/24/2022 09:01:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.30 on epoch=534
06/24/2022 09:01:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.27 on epoch=539
06/24/2022 09:01:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.31 on epoch=544
06/24/2022 09:01:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.34 on epoch=549
06/24/2022 09:01:59 - INFO - __main__ - Global step 1100 Train loss 2.29 ACC 0.0625 on epoch=549
06/24/2022 09:01:59 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=549, global_step=1100
06/24/2022 09:02:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.26 on epoch=554
06/24/2022 09:02:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 2.07 on epoch=559
06/24/2022 09:02:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.20 on epoch=564
06/24/2022 09:02:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 2.12 on epoch=569
06/24/2022 09:02:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 2.08 on epoch=574
06/24/2022 09:02:07 - INFO - __main__ - Global step 1150 Train loss 2.15 ACC 0.28125 on epoch=574
06/24/2022 09:02:07 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.28125 on epoch=574, global_step=1150
06/24/2022 09:02:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.01 on epoch=579
06/24/2022 09:02:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.12 on epoch=584
06/24/2022 09:02:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 2.04 on epoch=589
06/24/2022 09:02:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.08 on epoch=594
06/24/2022 09:02:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.01 on epoch=599
06/24/2022 09:02:14 - INFO - __main__ - Global step 1200 Train loss 2.05 ACC 0.46875 on epoch=599
06/24/2022 09:02:14 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.46875 on epoch=599, global_step=1200
06/24/2022 09:02:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.02 on epoch=604
06/24/2022 09:02:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.08 on epoch=609
06/24/2022 09:02:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.89 on epoch=614
06/24/2022 09:02:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.86 on epoch=619
06/24/2022 09:02:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.84 on epoch=624
06/24/2022 09:02:22 - INFO - __main__ - Global step 1250 Train loss 1.94 ACC 0.5 on epoch=624
06/24/2022 09:02:22 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=624, global_step=1250
06/24/2022 09:02:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.96 on epoch=629
06/24/2022 09:02:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.88 on epoch=634
06/24/2022 09:02:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.07 on epoch=639
06/24/2022 09:02:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.87 on epoch=644
06/24/2022 09:02:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.85 on epoch=649
06/24/2022 09:02:28 - INFO - __main__ - Global step 1300 Train loss 1.93 ACC 0.5 on epoch=649
06/24/2022 09:02:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.86 on epoch=654
06/24/2022 09:02:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.78 on epoch=659
06/24/2022 09:02:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.86 on epoch=664
06/24/2022 09:02:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.70 on epoch=669
06/24/2022 09:02:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.85 on epoch=674
06/24/2022 09:02:35 - INFO - __main__ - Global step 1350 Train loss 1.81 ACC 0.5 on epoch=674
06/24/2022 09:02:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.93 on epoch=679
06/24/2022 09:02:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.80 on epoch=684
06/24/2022 09:02:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.75 on epoch=689
06/24/2022 09:02:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.06 on epoch=694
06/24/2022 09:02:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.94 on epoch=699
06/24/2022 09:02:42 - INFO - __main__ - Global step 1400 Train loss 1.89 ACC 0.46875 on epoch=699
06/24/2022 09:02:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.91 on epoch=704
06/24/2022 09:02:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.89 on epoch=709
06/24/2022 09:02:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.84 on epoch=714
06/24/2022 09:02:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.76 on epoch=719
06/24/2022 09:02:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.83 on epoch=724
06/24/2022 09:02:49 - INFO - __main__ - Global step 1450 Train loss 1.84 ACC 0.5 on epoch=724
06/24/2022 09:02:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.85 on epoch=729
06/24/2022 09:02:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.67 on epoch=734
06/24/2022 09:02:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.61 on epoch=739
06/24/2022 09:02:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.71 on epoch=744
06/24/2022 09:02:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.71 on epoch=749
06/24/2022 09:03:03 - INFO - __main__ - Global step 1500 Train loss 1.71 ACC 0.5 on epoch=749
06/24/2022 09:03:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.64 on epoch=754
06/24/2022 09:03:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.58 on epoch=759
06/24/2022 09:03:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.69 on epoch=764
06/24/2022 09:03:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.53 on epoch=769
06/24/2022 09:03:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.47 on epoch=774
06/24/2022 09:03:12 - INFO - __main__ - Global step 1550 Train loss 1.58 ACC 0.5 on epoch=774
06/24/2022 09:03:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.31 on epoch=779
06/24/2022 09:03:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.46 on epoch=784
06/24/2022 09:03:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.52 on epoch=789
06/24/2022 09:03:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.57 on epoch=794
06/24/2022 09:03:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.45 on epoch=799
06/24/2022 09:03:23 - INFO - __main__ - Global step 1600 Train loss 1.46 ACC 0.5 on epoch=799
06/24/2022 09:03:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.48 on epoch=804
06/24/2022 09:03:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.47 on epoch=809
06/24/2022 09:03:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.45 on epoch=814
06/24/2022 09:03:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.27 on epoch=819
06/24/2022 09:03:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.46 on epoch=824
06/24/2022 09:03:35 - INFO - __main__ - Global step 1650 Train loss 1.43 ACC 0.46875 on epoch=824
06/24/2022 09:03:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.47 on epoch=829
06/24/2022 09:03:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.26 on epoch=834
06/24/2022 09:03:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.30 on epoch=839
06/24/2022 09:03:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.33 on epoch=844
06/24/2022 09:03:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.34 on epoch=849
06/24/2022 09:03:50 - INFO - __main__ - Global step 1700 Train loss 1.34 ACC 0.5 on epoch=849
06/24/2022 09:03:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.24 on epoch=854
06/24/2022 09:03:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.14 on epoch=859
06/24/2022 09:03:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.20 on epoch=864
06/24/2022 09:03:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.20 on epoch=869
06/24/2022 09:03:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.15 on epoch=874
06/24/2022 09:04:04 - INFO - __main__ - Global step 1750 Train loss 1.19 ACC 0.5 on epoch=874
06/24/2022 09:04:05 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.24 on epoch=879
06/24/2022 09:04:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.14 on epoch=884
06/24/2022 09:04:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.31 on epoch=889
06/24/2022 09:04:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.10 on epoch=894
06/24/2022 09:04:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.00 on epoch=899
06/24/2022 09:04:14 - INFO - __main__ - Global step 1800 Train loss 1.16 ACC 0.5625 on epoch=899
06/24/2022 09:04:14 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=899, global_step=1800
06/24/2022 09:04:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.18 on epoch=904
06/24/2022 09:04:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.18 on epoch=909
06/24/2022 09:04:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.07 on epoch=914
06/24/2022 09:04:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.09 on epoch=919
06/24/2022 09:04:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.04 on epoch=924
06/24/2022 09:04:26 - INFO - __main__ - Global step 1850 Train loss 1.11 ACC 0.5 on epoch=924
06/24/2022 09:04:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.11 on epoch=929
06/24/2022 09:04:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.01 on epoch=934
06/24/2022 09:04:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.90 on epoch=939
06/24/2022 09:04:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.98 on epoch=944
06/24/2022 09:04:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.11 on epoch=949
06/24/2022 09:04:41 - INFO - __main__ - Global step 1900 Train loss 1.02 ACC 0.65625 on epoch=949
06/24/2022 09:04:41 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.65625 on epoch=949, global_step=1900
06/24/2022 09:04:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.91 on epoch=954
06/24/2022 09:04:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.90 on epoch=959
06/24/2022 09:04:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.96 on epoch=964
06/24/2022 09:04:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.89 on epoch=969
06/24/2022 09:04:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.96 on epoch=974
06/24/2022 09:04:50 - INFO - __main__ - Global step 1950 Train loss 0.93 ACC 0.53125 on epoch=974
06/24/2022 09:04:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.86 on epoch=979
06/24/2022 09:04:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.95 on epoch=984
06/24/2022 09:04:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.93 on epoch=989
06/24/2022 09:04:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.89 on epoch=994
06/24/2022 09:04:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.96 on epoch=999
06/24/2022 09:04:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:04:58 - INFO - __main__ - Printing 3 examples
06/24/2022 09:04:58 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 09:04:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:04:58 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 09:04:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:04:58 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 09:04:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:04:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 09:04:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:04:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:04:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:04:58 - INFO - __main__ - Printing 3 examples
06/24/2022 09:04:58 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 09:04:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:04:58 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 09:04:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:04:58 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 09:04:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:04:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:04:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:04:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:05:01 - INFO - __main__ - Global step 2000 Train loss 0.92 ACC 0.5 on epoch=999
06/24/2022 09:05:01 - INFO - __main__ - save last model!
06/24/2022 09:05:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 09:05:01 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 09:05:01 - INFO - __main__ - Printing 3 examples
06/24/2022 09:05:01 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 09:05:01 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:05:01 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 09:05:01 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:05:01 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 09:05:01 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:05:01 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:05:01 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:05:02 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 09:05:03 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:05:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:05:04 - INFO - __main__ - Starting training!
06/24/2022 09:06:45 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_21_0.3_8_predictions.txt
06/24/2022 09:06:45 - INFO - __main__ - ACC on test data: 0.4951
06/24/2022 09:06:45 - INFO - __main__ - prefix=medical_questions_pairs_16_21, lr=0.3, bsz=8, dev_performance=0.65625, test_performance=0.49508196721311476
06/24/2022 09:06:45 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_21, lr=0.2, bsz=8 ...
06/24/2022 09:06:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:06:46 - INFO - __main__ - Printing 3 examples
06/24/2022 09:06:46 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 09:06:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:06:46 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 09:06:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:06:46 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 09:06:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:06:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 09:06:46 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:06:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:06:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:06:46 - INFO - __main__ - Printing 3 examples
06/24/2022 09:06:46 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 09:06:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:06:46 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 09:06:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:06:46 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 09:06:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:06:46 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:06:46 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:06:46 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:06:52 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:06:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:06:52 - INFO - __main__ - Starting training!
06/24/2022 09:06:53 - INFO - __main__ - Step 10 Global step 10 Train loss 8.43 on epoch=4
06/24/2022 09:06:55 - INFO - __main__ - Step 20 Global step 20 Train loss 8.33 on epoch=9
06/24/2022 09:06:56 - INFO - __main__ - Step 30 Global step 30 Train loss 8.35 on epoch=14
06/24/2022 09:06:57 - INFO - __main__ - Step 40 Global step 40 Train loss 8.30 on epoch=19
06/24/2022 09:06:59 - INFO - __main__ - Step 50 Global step 50 Train loss 8.35 on epoch=24
06/24/2022 09:07:01 - INFO - __main__ - Global step 50 Train loss 8.35 ACC 0.0 on epoch=24
06/24/2022 09:07:01 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 09:07:02 - INFO - __main__ - Step 60 Global step 60 Train loss 8.31 on epoch=29
06/24/2022 09:07:04 - INFO - __main__ - Step 70 Global step 70 Train loss 8.27 on epoch=34
06/24/2022 09:07:05 - INFO - __main__ - Step 80 Global step 80 Train loss 8.22 on epoch=39
06/24/2022 09:07:06 - INFO - __main__ - Step 90 Global step 90 Train loss 8.20 on epoch=44
06/24/2022 09:07:07 - INFO - __main__ - Step 100 Global step 100 Train loss 8.29 on epoch=49
06/24/2022 09:07:09 - INFO - __main__ - Global step 100 Train loss 8.26 ACC 0.0 on epoch=49
06/24/2022 09:07:11 - INFO - __main__ - Step 110 Global step 110 Train loss 8.31 on epoch=54
06/24/2022 09:07:12 - INFO - __main__ - Step 120 Global step 120 Train loss 8.27 on epoch=59
06/24/2022 09:07:13 - INFO - __main__ - Step 130 Global step 130 Train loss 8.16 on epoch=64
06/24/2022 09:07:14 - INFO - __main__ - Step 140 Global step 140 Train loss 8.25 on epoch=69
06/24/2022 09:07:16 - INFO - __main__ - Step 150 Global step 150 Train loss 8.29 on epoch=74
06/24/2022 09:07:19 - INFO - __main__ - Global step 150 Train loss 8.26 ACC 0.0 on epoch=74
06/24/2022 09:07:21 - INFO - __main__ - Step 160 Global step 160 Train loss 8.11 on epoch=79
06/24/2022 09:07:22 - INFO - __main__ - Step 170 Global step 170 Train loss 8.06 on epoch=84
06/24/2022 09:07:23 - INFO - __main__ - Step 180 Global step 180 Train loss 8.15 on epoch=89
06/24/2022 09:07:24 - INFO - __main__ - Step 190 Global step 190 Train loss 8.15 on epoch=94
06/24/2022 09:07:26 - INFO - __main__ - Step 200 Global step 200 Train loss 8.09 on epoch=99
06/24/2022 09:07:29 - INFO - __main__ - Global step 200 Train loss 8.11 ACC 0.0 on epoch=99
06/24/2022 09:07:30 - INFO - __main__ - Step 210 Global step 210 Train loss 8.09 on epoch=104
06/24/2022 09:07:31 - INFO - __main__ - Step 220 Global step 220 Train loss 7.95 on epoch=109
06/24/2022 09:07:32 - INFO - __main__ - Step 230 Global step 230 Train loss 8.00 on epoch=114
06/24/2022 09:07:34 - INFO - __main__ - Step 240 Global step 240 Train loss 7.99 on epoch=119
06/24/2022 09:07:35 - INFO - __main__ - Step 250 Global step 250 Train loss 7.92 on epoch=124
06/24/2022 09:07:40 - INFO - __main__ - Global step 250 Train loss 7.99 ACC 0.0 on epoch=124
06/24/2022 09:07:42 - INFO - __main__ - Step 260 Global step 260 Train loss 7.93 on epoch=129
06/24/2022 09:07:43 - INFO - __main__ - Step 270 Global step 270 Train loss 8.02 on epoch=134
06/24/2022 09:07:44 - INFO - __main__ - Step 280 Global step 280 Train loss 7.91 on epoch=139
06/24/2022 09:07:46 - INFO - __main__ - Step 290 Global step 290 Train loss 7.86 on epoch=144
06/24/2022 09:07:47 - INFO - __main__ - Step 300 Global step 300 Train loss 7.83 on epoch=149
06/24/2022 09:07:49 - INFO - __main__ - Global step 300 Train loss 7.91 ACC 0.0 on epoch=149
06/24/2022 09:07:50 - INFO - __main__ - Step 310 Global step 310 Train loss 7.93 on epoch=154
06/24/2022 09:07:51 - INFO - __main__ - Step 320 Global step 320 Train loss 7.91 on epoch=159
06/24/2022 09:07:52 - INFO - __main__ - Step 330 Global step 330 Train loss 7.87 on epoch=164
06/24/2022 09:07:54 - INFO - __main__ - Step 340 Global step 340 Train loss 7.80 on epoch=169
06/24/2022 09:07:55 - INFO - __main__ - Step 350 Global step 350 Train loss 7.79 on epoch=174
06/24/2022 09:07:56 - INFO - __main__ - Global step 350 Train loss 7.86 ACC 0.0 on epoch=174
06/24/2022 09:07:57 - INFO - __main__ - Step 360 Global step 360 Train loss 7.81 on epoch=179
06/24/2022 09:07:59 - INFO - __main__ - Step 370 Global step 370 Train loss 7.62 on epoch=184
06/24/2022 09:08:00 - INFO - __main__ - Step 380 Global step 380 Train loss 7.83 on epoch=189
06/24/2022 09:08:01 - INFO - __main__ - Step 390 Global step 390 Train loss 7.70 on epoch=194
06/24/2022 09:08:03 - INFO - __main__ - Step 400 Global step 400 Train loss 7.58 on epoch=199
06/24/2022 09:08:04 - INFO - __main__ - Global step 400 Train loss 7.71 ACC 0.0 on epoch=199
06/24/2022 09:08:06 - INFO - __main__ - Step 410 Global step 410 Train loss 7.64 on epoch=204
06/24/2022 09:08:07 - INFO - __main__ - Step 420 Global step 420 Train loss 7.52 on epoch=209
06/24/2022 09:08:08 - INFO - __main__ - Step 430 Global step 430 Train loss 7.46 on epoch=214
06/24/2022 09:08:09 - INFO - __main__ - Step 440 Global step 440 Train loss 7.56 on epoch=219
06/24/2022 09:08:11 - INFO - __main__ - Step 450 Global step 450 Train loss 7.47 on epoch=224
06/24/2022 09:08:17 - INFO - __main__ - Global step 450 Train loss 7.53 ACC 0.0 on epoch=224
06/24/2022 09:08:18 - INFO - __main__ - Step 460 Global step 460 Train loss 7.35 on epoch=229
06/24/2022 09:08:20 - INFO - __main__ - Step 470 Global step 470 Train loss 7.31 on epoch=234
06/24/2022 09:08:21 - INFO - __main__ - Step 480 Global step 480 Train loss 7.35 on epoch=239
06/24/2022 09:08:22 - INFO - __main__ - Step 490 Global step 490 Train loss 7.39 on epoch=244
06/24/2022 09:08:24 - INFO - __main__ - Step 500 Global step 500 Train loss 7.20 on epoch=249
06/24/2022 09:08:27 - INFO - __main__ - Global step 500 Train loss 7.32 ACC 0.0 on epoch=249
06/24/2022 09:08:28 - INFO - __main__ - Step 510 Global step 510 Train loss 7.23 on epoch=254
06/24/2022 09:08:30 - INFO - __main__ - Step 520 Global step 520 Train loss 7.12 on epoch=259
06/24/2022 09:08:31 - INFO - __main__ - Step 530 Global step 530 Train loss 7.01 on epoch=264
06/24/2022 09:08:32 - INFO - __main__ - Step 540 Global step 540 Train loss 7.01 on epoch=269
06/24/2022 09:08:34 - INFO - __main__ - Step 550 Global step 550 Train loss 6.75 on epoch=274
06/24/2022 09:08:37 - INFO - __main__ - Global step 550 Train loss 7.02 ACC 0.0 on epoch=274
06/24/2022 09:08:38 - INFO - __main__ - Step 560 Global step 560 Train loss 6.90 on epoch=279
06/24/2022 09:08:40 - INFO - __main__ - Step 570 Global step 570 Train loss 6.73 on epoch=284
06/24/2022 09:08:41 - INFO - __main__ - Step 580 Global step 580 Train loss 6.75 on epoch=289
06/24/2022 09:08:42 - INFO - __main__ - Step 590 Global step 590 Train loss 6.69 on epoch=294
06/24/2022 09:08:44 - INFO - __main__ - Step 600 Global step 600 Train loss 6.50 on epoch=299
06/24/2022 09:08:48 - INFO - __main__ - Global step 600 Train loss 6.71 ACC 0.0 on epoch=299
06/24/2022 09:08:50 - INFO - __main__ - Step 610 Global step 610 Train loss 6.30 on epoch=304
06/24/2022 09:08:51 - INFO - __main__ - Step 620 Global step 620 Train loss 6.41 on epoch=309
06/24/2022 09:08:52 - INFO - __main__ - Step 630 Global step 630 Train loss 6.38 on epoch=314
06/24/2022 09:08:54 - INFO - __main__ - Step 640 Global step 640 Train loss 6.34 on epoch=319
06/24/2022 09:08:55 - INFO - __main__ - Step 650 Global step 650 Train loss 6.10 on epoch=324
06/24/2022 09:08:58 - INFO - __main__ - Global step 650 Train loss 6.31 ACC 0.0 on epoch=324
06/24/2022 09:08:59 - INFO - __main__ - Step 660 Global step 660 Train loss 6.14 on epoch=329
06/24/2022 09:09:00 - INFO - __main__ - Step 670 Global step 670 Train loss 6.10 on epoch=334
06/24/2022 09:09:01 - INFO - __main__ - Step 680 Global step 680 Train loss 6.04 on epoch=339
06/24/2022 09:09:03 - INFO - __main__ - Step 690 Global step 690 Train loss 5.88 on epoch=344
06/24/2022 09:09:04 - INFO - __main__ - Step 700 Global step 700 Train loss 5.72 on epoch=349
06/24/2022 09:09:06 - INFO - __main__ - Global step 700 Train loss 5.97 ACC 0.0 on epoch=349
06/24/2022 09:09:07 - INFO - __main__ - Step 710 Global step 710 Train loss 5.67 on epoch=354
06/24/2022 09:09:08 - INFO - __main__ - Step 720 Global step 720 Train loss 5.50 on epoch=359
06/24/2022 09:09:10 - INFO - __main__ - Step 730 Global step 730 Train loss 5.56 on epoch=364
06/24/2022 09:09:11 - INFO - __main__ - Step 740 Global step 740 Train loss 5.36 on epoch=369
06/24/2022 09:09:12 - INFO - __main__ - Step 750 Global step 750 Train loss 5.42 on epoch=374
06/24/2022 09:09:14 - INFO - __main__ - Global step 750 Train loss 5.50 ACC 0.0 on epoch=374
06/24/2022 09:09:15 - INFO - __main__ - Step 760 Global step 760 Train loss 5.21 on epoch=379
06/24/2022 09:09:16 - INFO - __main__ - Step 770 Global step 770 Train loss 5.09 on epoch=384
06/24/2022 09:09:18 - INFO - __main__ - Step 780 Global step 780 Train loss 5.20 on epoch=389
06/24/2022 09:09:19 - INFO - __main__ - Step 790 Global step 790 Train loss 5.22 on epoch=394
06/24/2022 09:09:20 - INFO - __main__ - Step 800 Global step 800 Train loss 5.21 on epoch=399
06/24/2022 09:09:23 - INFO - __main__ - Global step 800 Train loss 5.19 ACC 0.0 on epoch=399
06/24/2022 09:09:24 - INFO - __main__ - Step 810 Global step 810 Train loss 4.92 on epoch=404
06/24/2022 09:09:26 - INFO - __main__ - Step 820 Global step 820 Train loss 4.99 on epoch=409
06/24/2022 09:09:27 - INFO - __main__ - Step 830 Global step 830 Train loss 4.81 on epoch=414
06/24/2022 09:09:28 - INFO - __main__ - Step 840 Global step 840 Train loss 4.85 on epoch=419
06/24/2022 09:09:29 - INFO - __main__ - Step 850 Global step 850 Train loss 4.74 on epoch=424
06/24/2022 09:09:32 - INFO - __main__ - Global step 850 Train loss 4.86 ACC 0.0 on epoch=424
06/24/2022 09:09:33 - INFO - __main__ - Step 860 Global step 860 Train loss 4.61 on epoch=429
06/24/2022 09:09:34 - INFO - __main__ - Step 870 Global step 870 Train loss 4.77 on epoch=434
06/24/2022 09:09:36 - INFO - __main__ - Step 880 Global step 880 Train loss 4.56 on epoch=439
06/24/2022 09:09:37 - INFO - __main__ - Step 890 Global step 890 Train loss 4.55 on epoch=444
06/24/2022 09:09:38 - INFO - __main__ - Step 900 Global step 900 Train loss 4.61 on epoch=449
06/24/2022 09:09:43 - INFO - __main__ - Global step 900 Train loss 4.62 ACC 0.0 on epoch=449
06/24/2022 09:09:44 - INFO - __main__ - Step 910 Global step 910 Train loss 4.46 on epoch=454
06/24/2022 09:09:45 - INFO - __main__ - Step 920 Global step 920 Train loss 4.58 on epoch=459
06/24/2022 09:09:47 - INFO - __main__ - Step 930 Global step 930 Train loss 4.24 on epoch=464
06/24/2022 09:09:48 - INFO - __main__ - Step 940 Global step 940 Train loss 4.42 on epoch=469
06/24/2022 09:09:49 - INFO - __main__ - Step 950 Global step 950 Train loss 4.27 on epoch=474
06/24/2022 09:09:51 - INFO - __main__ - Global step 950 Train loss 4.39 ACC 0.0 on epoch=474
06/24/2022 09:09:52 - INFO - __main__ - Step 960 Global step 960 Train loss 4.18 on epoch=479
06/24/2022 09:09:53 - INFO - __main__ - Step 970 Global step 970 Train loss 4.19 on epoch=484
06/24/2022 09:09:55 - INFO - __main__ - Step 980 Global step 980 Train loss 4.24 on epoch=489
06/24/2022 09:09:56 - INFO - __main__ - Step 990 Global step 990 Train loss 4.32 on epoch=494
06/24/2022 09:09:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 4.23 on epoch=499
06/24/2022 09:10:00 - INFO - __main__ - Global step 1000 Train loss 4.23 ACC 0.0 on epoch=499
06/24/2022 09:10:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 4.12 on epoch=504
06/24/2022 09:10:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 4.04 on epoch=509
06/24/2022 09:10:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 4.34 on epoch=514
06/24/2022 09:10:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.83 on epoch=519
06/24/2022 09:10:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.86 on epoch=524
06/24/2022 09:10:09 - INFO - __main__ - Global step 1050 Train loss 4.04 ACC 0.0 on epoch=524
06/24/2022 09:10:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.91 on epoch=529
06/24/2022 09:10:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.83 on epoch=534
06/24/2022 09:10:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.80 on epoch=539
06/24/2022 09:10:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.76 on epoch=544
06/24/2022 09:10:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.89 on epoch=549
06/24/2022 09:10:17 - INFO - __main__ - Global step 1100 Train loss 3.84 ACC 0.0 on epoch=549
06/24/2022 09:10:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.78 on epoch=554
06/24/2022 09:10:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.72 on epoch=559
06/24/2022 09:10:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.81 on epoch=564
06/24/2022 09:10:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.70 on epoch=569
06/24/2022 09:10:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.62 on epoch=574
06/24/2022 09:10:28 - INFO - __main__ - Global step 1150 Train loss 3.72 ACC 0.0 on epoch=574
06/24/2022 09:10:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.53 on epoch=579
06/24/2022 09:10:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.57 on epoch=584
06/24/2022 09:10:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.53 on epoch=589
06/24/2022 09:10:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.45 on epoch=594
06/24/2022 09:10:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.47 on epoch=599
06/24/2022 09:10:35 - INFO - __main__ - Global step 1200 Train loss 3.51 ACC 0.0 on epoch=599
06/24/2022 09:10:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.35 on epoch=604
06/24/2022 09:10:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.39 on epoch=609
06/24/2022 09:10:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.47 on epoch=614
06/24/2022 09:10:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.34 on epoch=619
06/24/2022 09:10:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.37 on epoch=624
06/24/2022 09:10:43 - INFO - __main__ - Global step 1250 Train loss 3.39 ACC 0.0 on epoch=624
06/24/2022 09:10:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.40 on epoch=629
06/24/2022 09:10:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.20 on epoch=634
06/24/2022 09:10:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.23 on epoch=639
06/24/2022 09:10:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.24 on epoch=644
06/24/2022 09:10:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.02 on epoch=649
06/24/2022 09:10:51 - INFO - __main__ - Global step 1300 Train loss 3.22 ACC 0.0 on epoch=649
06/24/2022 09:10:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.04 on epoch=654
06/24/2022 09:10:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.05 on epoch=659
06/24/2022 09:10:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.92 on epoch=664
06/24/2022 09:10:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 3.06 on epoch=669
06/24/2022 09:10:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 3.06 on epoch=674
06/24/2022 09:10:59 - INFO - __main__ - Global step 1350 Train loss 3.03 ACC 0.0 on epoch=674
06/24/2022 09:11:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.90 on epoch=679
06/24/2022 09:11:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.79 on epoch=684
06/24/2022 09:11:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.90 on epoch=689
06/24/2022 09:11:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.91 on epoch=694
06/24/2022 09:11:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.79 on epoch=699
06/24/2022 09:11:06 - INFO - __main__ - Global step 1400 Train loss 2.86 ACC 0.0625 on epoch=699
06/24/2022 09:11:06 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=699, global_step=1400
06/24/2022 09:11:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.84 on epoch=704
06/24/2022 09:11:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.94 on epoch=709
06/24/2022 09:11:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.75 on epoch=714
06/24/2022 09:11:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.79 on epoch=719
06/24/2022 09:11:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.68 on epoch=724
06/24/2022 09:11:13 - INFO - __main__ - Global step 1450 Train loss 2.80 ACC 0.5 on epoch=724
06/24/2022 09:11:13 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.5 on epoch=724, global_step=1450
06/24/2022 09:11:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.57 on epoch=729
06/24/2022 09:11:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.63 on epoch=734
06/24/2022 09:11:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.63 on epoch=739
06/24/2022 09:11:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.55 on epoch=744
06/24/2022 09:11:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.52 on epoch=749
06/24/2022 09:11:20 - INFO - __main__ - Global step 1500 Train loss 2.58 ACC 0.46875 on epoch=749
06/24/2022 09:11:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.44 on epoch=754
06/24/2022 09:11:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.47 on epoch=759
06/24/2022 09:11:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 2.46 on epoch=764
06/24/2022 09:11:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 2.26 on epoch=769
06/24/2022 09:11:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 2.28 on epoch=774
06/24/2022 09:11:27 - INFO - __main__ - Global step 1550 Train loss 2.38 ACC 0.5 on epoch=774
06/24/2022 09:11:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 2.19 on epoch=779
06/24/2022 09:11:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 2.32 on epoch=784
06/24/2022 09:11:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 2.25 on epoch=789
06/24/2022 09:11:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 2.33 on epoch=794
06/24/2022 09:11:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 2.10 on epoch=799
06/24/2022 09:11:43 - INFO - __main__ - Global step 1600 Train loss 2.24 ACC 0.5 on epoch=799
06/24/2022 09:11:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 2.20 on epoch=804
06/24/2022 09:11:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.13 on epoch=809
06/24/2022 09:11:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 2.05 on epoch=814
06/24/2022 09:11:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 2.01 on epoch=819
06/24/2022 09:11:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 2.06 on epoch=824
06/24/2022 09:12:00 - INFO - __main__ - Global step 1650 Train loss 2.09 ACC 0.5 on epoch=824
06/24/2022 09:12:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.94 on epoch=829
06/24/2022 09:12:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.95 on epoch=834
06/24/2022 09:12:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 2.13 on epoch=839
06/24/2022 09:12:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 2.02 on epoch=844
06/24/2022 09:12:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.85 on epoch=849
06/24/2022 09:12:16 - INFO - __main__ - Global step 1700 Train loss 1.98 ACC 0.5 on epoch=849
06/24/2022 09:12:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.83 on epoch=854
06/24/2022 09:12:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.84 on epoch=859
06/24/2022 09:12:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.63 on epoch=864
06/24/2022 09:12:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.82 on epoch=869
06/24/2022 09:12:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.51 on epoch=874
06/24/2022 09:12:31 - INFO - __main__ - Global step 1750 Train loss 1.73 ACC 0.5 on epoch=874
06/24/2022 09:12:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.44 on epoch=879
06/24/2022 09:12:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.71 on epoch=884
06/24/2022 09:12:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.62 on epoch=889
06/24/2022 09:12:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.35 on epoch=894
06/24/2022 09:12:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.48 on epoch=899
06/24/2022 09:12:47 - INFO - __main__ - Global step 1800 Train loss 1.52 ACC 0.5 on epoch=899
06/24/2022 09:12:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.33 on epoch=904
06/24/2022 09:12:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.64 on epoch=909
06/24/2022 09:12:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.43 on epoch=914
06/24/2022 09:12:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.35 on epoch=919
06/24/2022 09:12:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.32 on epoch=924
06/24/2022 09:13:02 - INFO - __main__ - Global step 1850 Train loss 1.41 ACC 0.5 on epoch=924
06/24/2022 09:13:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.32 on epoch=929
06/24/2022 09:13:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.33 on epoch=934
06/24/2022 09:13:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.33 on epoch=939
06/24/2022 09:13:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.10 on epoch=944
06/24/2022 09:13:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.20 on epoch=949
06/24/2022 09:13:18 - INFO - __main__ - Global step 1900 Train loss 1.25 ACC 0.5 on epoch=949
06/24/2022 09:13:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.20 on epoch=954
06/24/2022 09:13:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.23 on epoch=959
06/24/2022 09:13:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.17 on epoch=964
06/24/2022 09:13:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.19 on epoch=969
06/24/2022 09:13:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.34 on epoch=974
06/24/2022 09:13:31 - INFO - __main__ - Global step 1950 Train loss 1.23 ACC 0.5 on epoch=974
06/24/2022 09:13:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.15 on epoch=979
06/24/2022 09:13:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.27 on epoch=984
06/24/2022 09:13:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.04 on epoch=989
06/24/2022 09:13:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.02 on epoch=994
06/24/2022 09:13:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.07 on epoch=999
06/24/2022 09:13:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:13:38 - INFO - __main__ - Printing 3 examples
06/24/2022 09:13:38 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 09:13:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:13:38 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 09:13:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:13:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 09:13:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:13:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 09:13:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:13:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:13:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:13:38 - INFO - __main__ - Printing 3 examples
06/24/2022 09:13:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 09:13:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:13:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 09:13:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:13:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 09:13:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:13:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:13:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:13:38 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:13:44 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:13:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:13:44 - INFO - __main__ - Starting training!
06/24/2022 09:13:46 - INFO - __main__ - Global step 2000 Train loss 1.11 ACC 0.46875 on epoch=999
06/24/2022 09:13:46 - INFO - __main__ - save last model!
06/24/2022 09:13:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 09:13:46 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 09:13:46 - INFO - __main__ - Printing 3 examples
06/24/2022 09:13:46 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 09:13:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:13:46 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 09:13:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:13:46 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 09:13:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:13:46 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:13:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:13:47 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 09:15:40 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_21_0.2_8_predictions.txt
06/24/2022 09:15:40 - INFO - __main__ - ACC on test data: 0.5164
06/24/2022 09:15:40 - INFO - __main__ - prefix=medical_questions_pairs_16_21, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.5163934426229508
06/24/2022 09:15:40 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_42, lr=0.5, bsz=8 ...
06/24/2022 09:15:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:15:41 - INFO - __main__ - Printing 3 examples
06/24/2022 09:15:41 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 09:15:41 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:15:41 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 09:15:41 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:15:41 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 09:15:41 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:15:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 09:15:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:15:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:15:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:15:41 - INFO - __main__ - Printing 3 examples
06/24/2022 09:15:41 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 09:15:41 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:15:41 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 09:15:41 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:15:41 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 09:15:41 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:15:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:15:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:15:41 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:15:47 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:15:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:15:47 - INFO - __main__ - Starting training!
06/24/2022 09:15:49 - INFO - __main__ - Step 10 Global step 10 Train loss 8.36 on epoch=4
06/24/2022 09:15:50 - INFO - __main__ - Step 20 Global step 20 Train loss 8.37 on epoch=9
06/24/2022 09:15:51 - INFO - __main__ - Step 30 Global step 30 Train loss 8.31 on epoch=14
06/24/2022 09:15:52 - INFO - __main__ - Step 40 Global step 40 Train loss 8.32 on epoch=19
06/24/2022 09:15:54 - INFO - __main__ - Step 50 Global step 50 Train loss 8.27 on epoch=24
06/24/2022 09:16:00 - INFO - __main__ - Global step 50 Train loss 8.33 ACC 0.0 on epoch=24
06/24/2022 09:16:00 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 09:16:02 - INFO - __main__ - Step 60 Global step 60 Train loss 8.21 on epoch=29
06/24/2022 09:16:03 - INFO - __main__ - Step 70 Global step 70 Train loss 8.13 on epoch=34
06/24/2022 09:16:04 - INFO - __main__ - Step 80 Global step 80 Train loss 8.19 on epoch=39
06/24/2022 09:16:05 - INFO - __main__ - Step 90 Global step 90 Train loss 8.14 on epoch=44
06/24/2022 09:16:07 - INFO - __main__ - Step 100 Global step 100 Train loss 7.93 on epoch=49
06/24/2022 09:16:09 - INFO - __main__ - Global step 100 Train loss 8.12 ACC 0.0 on epoch=49
06/24/2022 09:16:11 - INFO - __main__ - Step 110 Global step 110 Train loss 7.99 on epoch=54
06/24/2022 09:16:12 - INFO - __main__ - Step 120 Global step 120 Train loss 8.00 on epoch=59
06/24/2022 09:16:13 - INFO - __main__ - Step 130 Global step 130 Train loss 7.95 on epoch=64
06/24/2022 09:16:14 - INFO - __main__ - Step 140 Global step 140 Train loss 7.93 on epoch=69
06/24/2022 09:16:16 - INFO - __main__ - Step 150 Global step 150 Train loss 7.95 on epoch=74
06/24/2022 09:16:18 - INFO - __main__ - Global step 150 Train loss 7.96 ACC 0.0 on epoch=74
06/24/2022 09:16:20 - INFO - __main__ - Step 160 Global step 160 Train loss 7.81 on epoch=79
06/24/2022 09:16:21 - INFO - __main__ - Step 170 Global step 170 Train loss 7.58 on epoch=84
06/24/2022 09:16:22 - INFO - __main__ - Step 180 Global step 180 Train loss 7.50 on epoch=89
06/24/2022 09:16:24 - INFO - __main__ - Step 190 Global step 190 Train loss 7.34 on epoch=94
06/24/2022 09:16:25 - INFO - __main__ - Step 200 Global step 200 Train loss 7.21 on epoch=99
06/24/2022 09:16:28 - INFO - __main__ - Global step 200 Train loss 7.49 ACC 0.0 on epoch=99
06/24/2022 09:16:29 - INFO - __main__ - Step 210 Global step 210 Train loss 7.24 on epoch=104
06/24/2022 09:16:30 - INFO - __main__ - Step 220 Global step 220 Train loss 6.80 on epoch=109
06/24/2022 09:16:32 - INFO - __main__ - Step 230 Global step 230 Train loss 6.94 on epoch=114
06/24/2022 09:16:33 - INFO - __main__ - Step 240 Global step 240 Train loss 6.56 on epoch=119
06/24/2022 09:16:34 - INFO - __main__ - Step 250 Global step 250 Train loss 6.50 on epoch=124
06/24/2022 09:16:43 - INFO - __main__ - Global step 250 Train loss 6.81 ACC 0.0 on epoch=124
06/24/2022 09:16:44 - INFO - __main__ - Step 260 Global step 260 Train loss 6.39 on epoch=129
06/24/2022 09:16:46 - INFO - __main__ - Step 270 Global step 270 Train loss 6.11 on epoch=134
06/24/2022 09:16:47 - INFO - __main__ - Step 280 Global step 280 Train loss 6.06 on epoch=139
06/24/2022 09:16:48 - INFO - __main__ - Step 290 Global step 290 Train loss 6.15 on epoch=144
06/24/2022 09:16:49 - INFO - __main__ - Step 300 Global step 300 Train loss 6.12 on epoch=149
06/24/2022 09:16:56 - INFO - __main__ - Global step 300 Train loss 6.16 ACC 0.0 on epoch=149
06/24/2022 09:16:57 - INFO - __main__ - Step 310 Global step 310 Train loss 6.05 on epoch=154
06/24/2022 09:16:58 - INFO - __main__ - Step 320 Global step 320 Train loss 5.78 on epoch=159
06/24/2022 09:17:00 - INFO - __main__ - Step 330 Global step 330 Train loss 5.84 on epoch=164
06/24/2022 09:17:01 - INFO - __main__ - Step 340 Global step 340 Train loss 5.66 on epoch=169
06/24/2022 09:17:02 - INFO - __main__ - Step 350 Global step 350 Train loss 5.49 on epoch=174
06/24/2022 09:17:04 - INFO - __main__ - Global step 350 Train loss 5.76 ACC 0.0 on epoch=174
06/24/2022 09:17:06 - INFO - __main__ - Step 360 Global step 360 Train loss 5.37 on epoch=179
06/24/2022 09:17:07 - INFO - __main__ - Step 370 Global step 370 Train loss 5.19 on epoch=184
06/24/2022 09:17:08 - INFO - __main__ - Step 380 Global step 380 Train loss 5.06 on epoch=189
06/24/2022 09:17:09 - INFO - __main__ - Step 390 Global step 390 Train loss 4.82 on epoch=194
06/24/2022 09:17:11 - INFO - __main__ - Step 400 Global step 400 Train loss 4.95 on epoch=199
06/24/2022 09:17:15 - INFO - __main__ - Global step 400 Train loss 5.08 ACC 0.0 on epoch=199
06/24/2022 09:17:16 - INFO - __main__ - Step 410 Global step 410 Train loss 4.92 on epoch=204
06/24/2022 09:17:18 - INFO - __main__ - Step 420 Global step 420 Train loss 4.61 on epoch=209
06/24/2022 09:17:19 - INFO - __main__ - Step 430 Global step 430 Train loss 4.73 on epoch=214
06/24/2022 09:17:20 - INFO - __main__ - Step 440 Global step 440 Train loss 4.46 on epoch=219
06/24/2022 09:17:22 - INFO - __main__ - Step 450 Global step 450 Train loss 4.45 on epoch=224
06/24/2022 09:17:24 - INFO - __main__ - Global step 450 Train loss 4.64 ACC 0.0 on epoch=224
06/24/2022 09:17:26 - INFO - __main__ - Step 460 Global step 460 Train loss 4.34 on epoch=229
06/24/2022 09:17:27 - INFO - __main__ - Step 470 Global step 470 Train loss 4.04 on epoch=234
06/24/2022 09:17:28 - INFO - __main__ - Step 480 Global step 480 Train loss 4.14 on epoch=239
06/24/2022 09:17:30 - INFO - __main__ - Step 490 Global step 490 Train loss 4.10 on epoch=244
06/24/2022 09:17:31 - INFO - __main__ - Step 500 Global step 500 Train loss 3.99 on epoch=249
06/24/2022 09:17:39 - INFO - __main__ - Global step 500 Train loss 4.12 ACC 0.0 on epoch=249
06/24/2022 09:17:40 - INFO - __main__ - Step 510 Global step 510 Train loss 4.02 on epoch=254
06/24/2022 09:17:41 - INFO - __main__ - Step 520 Global step 520 Train loss 3.89 on epoch=259
06/24/2022 09:17:43 - INFO - __main__ - Step 530 Global step 530 Train loss 3.94 on epoch=264
06/24/2022 09:17:44 - INFO - __main__ - Step 540 Global step 540 Train loss 3.79 on epoch=269
06/24/2022 09:17:45 - INFO - __main__ - Step 550 Global step 550 Train loss 3.98 on epoch=274
06/24/2022 09:17:51 - INFO - __main__ - Global step 550 Train loss 3.92 ACC 0.0 on epoch=274
06/24/2022 09:17:52 - INFO - __main__ - Step 560 Global step 560 Train loss 3.83 on epoch=279
06/24/2022 09:17:53 - INFO - __main__ - Step 570 Global step 570 Train loss 3.53 on epoch=284
06/24/2022 09:17:55 - INFO - __main__ - Step 580 Global step 580 Train loss 3.69 on epoch=289
06/24/2022 09:17:56 - INFO - __main__ - Step 590 Global step 590 Train loss 3.56 on epoch=294
06/24/2022 09:17:57 - INFO - __main__ - Step 600 Global step 600 Train loss 3.57 on epoch=299
06/24/2022 09:18:04 - INFO - __main__ - Global step 600 Train loss 3.64 ACC 0.0 on epoch=299
06/24/2022 09:18:05 - INFO - __main__ - Step 610 Global step 610 Train loss 3.56 on epoch=304
06/24/2022 09:18:06 - INFO - __main__ - Step 620 Global step 620 Train loss 3.35 on epoch=309
06/24/2022 09:18:08 - INFO - __main__ - Step 630 Global step 630 Train loss 3.30 on epoch=314
06/24/2022 09:18:09 - INFO - __main__ - Step 640 Global step 640 Train loss 3.26 on epoch=319
06/24/2022 09:18:10 - INFO - __main__ - Step 650 Global step 650 Train loss 3.22 on epoch=324
06/24/2022 09:18:20 - INFO - __main__ - Global step 650 Train loss 3.34 ACC 0.0 on epoch=324
06/24/2022 09:18:22 - INFO - __main__ - Step 660 Global step 660 Train loss 3.17 on epoch=329
06/24/2022 09:18:23 - INFO - __main__ - Step 670 Global step 670 Train loss 3.12 on epoch=334
06/24/2022 09:18:24 - INFO - __main__ - Step 680 Global step 680 Train loss 3.30 on epoch=339
06/24/2022 09:18:26 - INFO - __main__ - Step 690 Global step 690 Train loss 3.23 on epoch=344
06/24/2022 09:18:27 - INFO - __main__ - Step 700 Global step 700 Train loss 3.08 on epoch=349
06/24/2022 09:18:37 - INFO - __main__ - Global step 700 Train loss 3.18 ACC 0.03125 on epoch=349
06/24/2022 09:18:37 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=349, global_step=700
06/24/2022 09:18:39 - INFO - __main__ - Step 710 Global step 710 Train loss 2.85 on epoch=354
06/24/2022 09:18:40 - INFO - __main__ - Step 720 Global step 720 Train loss 2.94 on epoch=359
06/24/2022 09:18:41 - INFO - __main__ - Step 730 Global step 730 Train loss 2.86 on epoch=364
06/24/2022 09:18:42 - INFO - __main__ - Step 740 Global step 740 Train loss 2.87 on epoch=369
06/24/2022 09:18:44 - INFO - __main__ - Step 750 Global step 750 Train loss 2.69 on epoch=374
06/24/2022 09:18:49 - INFO - __main__ - Global step 750 Train loss 2.84 ACC 0.34375 on epoch=374
06/24/2022 09:18:49 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.34375 on epoch=374, global_step=750
06/24/2022 09:18:51 - INFO - __main__ - Step 760 Global step 760 Train loss 2.77 on epoch=379
06/24/2022 09:18:52 - INFO - __main__ - Step 770 Global step 770 Train loss 2.76 on epoch=384
06/24/2022 09:18:53 - INFO - __main__ - Step 780 Global step 780 Train loss 2.76 on epoch=389
06/24/2022 09:18:54 - INFO - __main__ - Step 790 Global step 790 Train loss 2.67 on epoch=394
06/24/2022 09:18:56 - INFO - __main__ - Step 800 Global step 800 Train loss 2.43 on epoch=399
06/24/2022 09:19:01 - INFO - __main__ - Global step 800 Train loss 2.68 ACC 0.46875 on epoch=399
06/24/2022 09:19:01 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.46875 on epoch=399, global_step=800
06/24/2022 09:19:02 - INFO - __main__ - Step 810 Global step 810 Train loss 2.46 on epoch=404
06/24/2022 09:19:04 - INFO - __main__ - Step 820 Global step 820 Train loss 2.60 on epoch=409
06/24/2022 09:19:05 - INFO - __main__ - Step 830 Global step 830 Train loss 2.57 on epoch=414
06/24/2022 09:19:06 - INFO - __main__ - Step 840 Global step 840 Train loss 2.38 on epoch=419
06/24/2022 09:19:07 - INFO - __main__ - Step 850 Global step 850 Train loss 2.28 on epoch=424
06/24/2022 09:19:18 - INFO - __main__ - Global step 850 Train loss 2.46 ACC 0.46875 on epoch=424
06/24/2022 09:19:19 - INFO - __main__ - Step 860 Global step 860 Train loss 2.40 on epoch=429
06/24/2022 09:19:20 - INFO - __main__ - Step 870 Global step 870 Train loss 2.15 on epoch=434
06/24/2022 09:19:21 - INFO - __main__ - Step 880 Global step 880 Train loss 2.21 on epoch=439
06/24/2022 09:19:23 - INFO - __main__ - Step 890 Global step 890 Train loss 2.31 on epoch=444
06/24/2022 09:19:24 - INFO - __main__ - Step 900 Global step 900 Train loss 2.13 on epoch=449
06/24/2022 09:19:34 - INFO - __main__ - Global step 900 Train loss 2.24 ACC 0.46875 on epoch=449
06/24/2022 09:19:35 - INFO - __main__ - Step 910 Global step 910 Train loss 2.19 on epoch=454
06/24/2022 09:19:36 - INFO - __main__ - Step 920 Global step 920 Train loss 1.98 on epoch=459
06/24/2022 09:19:37 - INFO - __main__ - Step 930 Global step 930 Train loss 2.13 on epoch=464
06/24/2022 09:19:39 - INFO - __main__ - Step 940 Global step 940 Train loss 1.96 on epoch=469
06/24/2022 09:19:40 - INFO - __main__ - Step 950 Global step 950 Train loss 2.05 on epoch=474
06/24/2022 09:19:47 - INFO - __main__ - Global step 950 Train loss 2.06 ACC 0.5 on epoch=474
06/24/2022 09:19:47 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=474, global_step=950
06/24/2022 09:19:48 - INFO - __main__ - Step 960 Global step 960 Train loss 1.91 on epoch=479
06/24/2022 09:19:49 - INFO - __main__ - Step 970 Global step 970 Train loss 2.06 on epoch=484
06/24/2022 09:19:51 - INFO - __main__ - Step 980 Global step 980 Train loss 2.01 on epoch=489
06/24/2022 09:19:52 - INFO - __main__ - Step 990 Global step 990 Train loss 1.92 on epoch=494
06/24/2022 09:19:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.88 on epoch=499
06/24/2022 09:20:03 - INFO - __main__ - Global step 1000 Train loss 1.96 ACC 0.625 on epoch=499
06/24/2022 09:20:04 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.625 on epoch=499, global_step=1000
06/24/2022 09:20:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.92 on epoch=504
06/24/2022 09:20:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.70 on epoch=509
06/24/2022 09:20:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.76 on epoch=514
06/24/2022 09:20:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.66 on epoch=519
06/24/2022 09:20:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.65 on epoch=524
06/24/2022 09:20:19 - INFO - __main__ - Global step 1050 Train loss 1.74 ACC 0.4375 on epoch=524
06/24/2022 09:20:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.74 on epoch=529
06/24/2022 09:20:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.67 on epoch=534
06/24/2022 09:20:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.58 on epoch=539
06/24/2022 09:20:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.52 on epoch=544
06/24/2022 09:20:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.43 on epoch=549
06/24/2022 09:20:35 - INFO - __main__ - Global step 1100 Train loss 1.59 ACC 0.5 on epoch=549
06/24/2022 09:20:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.43 on epoch=554
06/24/2022 09:20:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.42 on epoch=559
06/24/2022 09:20:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.37 on epoch=564
06/24/2022 09:20:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.42 on epoch=569
06/24/2022 09:20:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.39 on epoch=574
06/24/2022 09:20:47 - INFO - __main__ - Global step 1150 Train loss 1.41 ACC 0.53125 on epoch=574
06/24/2022 09:20:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.27 on epoch=579
06/24/2022 09:20:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.29 on epoch=584
06/24/2022 09:20:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.22 on epoch=589
06/24/2022 09:20:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.19 on epoch=594
06/24/2022 09:20:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.26 on epoch=599
06/24/2022 09:20:58 - INFO - __main__ - Global step 1200 Train loss 1.24 ACC 0.5 on epoch=599
06/24/2022 09:20:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.08 on epoch=604
06/24/2022 09:21:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.10 on epoch=609
06/24/2022 09:21:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.08 on epoch=614
06/24/2022 09:21:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.06 on epoch=619
06/24/2022 09:21:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.10 on epoch=624
06/24/2022 09:21:08 - INFO - __main__ - Global step 1250 Train loss 1.09 ACC 0.5625 on epoch=624
06/24/2022 09:21:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.08 on epoch=629
06/24/2022 09:21:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.91 on epoch=634
06/24/2022 09:21:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.08 on epoch=639
06/24/2022 09:21:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.06 on epoch=644
06/24/2022 09:21:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.88 on epoch=649
06/24/2022 09:21:19 - INFO - __main__ - Global step 1300 Train loss 1.00 ACC 0.53125 on epoch=649
06/24/2022 09:21:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.04 on epoch=654
06/24/2022 09:21:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.79 on epoch=659
06/24/2022 09:21:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.99 on epoch=664
06/24/2022 09:21:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.91 on epoch=669
06/24/2022 09:21:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.88 on epoch=674
06/24/2022 09:21:26 - INFO - __main__ - Global step 1350 Train loss 0.92 ACC 0.40625 on epoch=674
06/24/2022 09:21:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.85 on epoch=679
06/24/2022 09:21:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.81 on epoch=684
06/24/2022 09:21:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.86 on epoch=689
06/24/2022 09:21:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.86 on epoch=694
06/24/2022 09:21:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.83 on epoch=699
06/24/2022 09:21:33 - INFO - __main__ - Global step 1400 Train loss 0.84 ACC 0.46875 on epoch=699
06/24/2022 09:21:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.72 on epoch=704
06/24/2022 09:21:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.80 on epoch=709
06/24/2022 09:21:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.79 on epoch=714
06/24/2022 09:21:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.72 on epoch=719
06/24/2022 09:21:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.69 on epoch=724
06/24/2022 09:21:40 - INFO - __main__ - Global step 1450 Train loss 0.74 ACC 0.53125 on epoch=724
06/24/2022 09:21:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.84 on epoch=729
06/24/2022 09:21:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.84 on epoch=734
06/24/2022 09:21:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.71 on epoch=739
06/24/2022 09:21:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.78 on epoch=744
06/24/2022 09:21:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.77 on epoch=749
06/24/2022 09:21:47 - INFO - __main__ - Global step 1500 Train loss 0.79 ACC 0.53125 on epoch=749
06/24/2022 09:21:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.63 on epoch=754
06/24/2022 09:21:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.70 on epoch=759
06/24/2022 09:21:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.65 on epoch=764
06/24/2022 09:21:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.73 on epoch=769
06/24/2022 09:21:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.74 on epoch=774
06/24/2022 09:21:54 - INFO - __main__ - Global step 1550 Train loss 0.69 ACC 0.5625 on epoch=774
06/24/2022 09:21:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.78 on epoch=779
06/24/2022 09:21:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.85 on epoch=784
06/24/2022 09:21:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.79 on epoch=789
06/24/2022 09:21:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.64 on epoch=794
06/24/2022 09:22:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.78 on epoch=799
06/24/2022 09:22:01 - INFO - __main__ - Global step 1600 Train loss 0.77 ACC 0.65625 on epoch=799
06/24/2022 09:22:01 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=799, global_step=1600
06/24/2022 09:22:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.77 on epoch=804
06/24/2022 09:22:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.68 on epoch=809
06/24/2022 09:22:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.75 on epoch=814
06/24/2022 09:22:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.68 on epoch=819
06/24/2022 09:22:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.50 on epoch=824
06/24/2022 09:22:08 - INFO - __main__ - Global step 1650 Train loss 0.68 ACC 0.5 on epoch=824
06/24/2022 09:22:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.67 on epoch=829
06/24/2022 09:22:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.77 on epoch=834
06/24/2022 09:22:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.62 on epoch=839
06/24/2022 09:22:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.72 on epoch=844
06/24/2022 09:22:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.61 on epoch=849
06/24/2022 09:22:15 - INFO - __main__ - Global step 1700 Train loss 0.68 ACC 0.4375 on epoch=849
06/24/2022 09:22:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.59 on epoch=854
06/24/2022 09:22:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.55 on epoch=859
06/24/2022 09:22:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.68 on epoch=864
06/24/2022 09:22:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.66 on epoch=869
06/24/2022 09:22:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.52 on epoch=874
06/24/2022 09:22:22 - INFO - __main__ - Global step 1750 Train loss 0.60 ACC 0.53125 on epoch=874
06/24/2022 09:22:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.59 on epoch=879
06/24/2022 09:22:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.66 on epoch=884
06/24/2022 09:22:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.52 on epoch=889
06/24/2022 09:22:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.72 on epoch=894
06/24/2022 09:22:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.67 on epoch=899
06/24/2022 09:22:29 - INFO - __main__ - Global step 1800 Train loss 0.63 ACC 0.5 on epoch=899
06/24/2022 09:22:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.59 on epoch=904
06/24/2022 09:22:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.65 on epoch=909
06/24/2022 09:22:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.53 on epoch=914
06/24/2022 09:22:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.50 on epoch=919
06/24/2022 09:22:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.68 on epoch=924
06/24/2022 09:22:35 - INFO - __main__ - Global step 1850 Train loss 0.59 ACC 0.34375 on epoch=924
06/24/2022 09:22:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.65 on epoch=929
06/24/2022 09:22:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.58 on epoch=934
06/24/2022 09:22:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.61 on epoch=939
06/24/2022 09:22:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.57 on epoch=944
06/24/2022 09:22:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.52 on epoch=949
06/24/2022 09:22:42 - INFO - __main__ - Global step 1900 Train loss 0.59 ACC 0.40625 on epoch=949
06/24/2022 09:22:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.57 on epoch=954
06/24/2022 09:22:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.58 on epoch=959
06/24/2022 09:22:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.58 on epoch=964
06/24/2022 09:22:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.70 on epoch=969
06/24/2022 09:22:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.65 on epoch=974
06/24/2022 09:22:49 - INFO - __main__ - Global step 1950 Train loss 0.61 ACC 0.53125 on epoch=974
06/24/2022 09:22:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.56 on epoch=979
06/24/2022 09:22:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.63 on epoch=984
06/24/2022 09:22:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.57 on epoch=989
06/24/2022 09:22:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.63 on epoch=994
06/24/2022 09:22:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.66 on epoch=999
06/24/2022 09:22:56 - INFO - __main__ - Global step 2000 Train loss 0.61 ACC 0.4375 on epoch=999
06/24/2022 09:22:56 - INFO - __main__ - save last model!
06/24/2022 09:22:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 09:22:56 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 09:22:56 - INFO - __main__ - Printing 3 examples
06/24/2022 09:22:56 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 09:22:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:22:56 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 09:22:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:22:56 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 09:22:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:22:56 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:22:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:22:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:22:56 - INFO - __main__ - Printing 3 examples
06/24/2022 09:22:56 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 09:22:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:22:56 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 09:22:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:22:56 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 09:22:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:22:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 09:22:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:22:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:22:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:22:56 - INFO - __main__ - Printing 3 examples
06/24/2022 09:22:56 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 09:22:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:22:56 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 09:22:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:22:56 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 09:22:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:22:56 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:22:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:22:57 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:22:57 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 09:23:03 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:23:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:23:03 - INFO - __main__ - Starting training!
06/24/2022 09:23:05 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_42_0.5_8_predictions.txt
06/24/2022 09:23:05 - INFO - __main__ - ACC on test data: 0.5246
06/24/2022 09:23:05 - INFO - __main__ - prefix=medical_questions_pairs_16_42, lr=0.5, bsz=8, dev_performance=0.65625, test_performance=0.5245901639344263
06/24/2022 09:23:05 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_42, lr=0.4, bsz=8 ...
06/24/2022 09:23:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:23:06 - INFO - __main__ - Printing 3 examples
06/24/2022 09:23:06 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 09:23:06 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:23:06 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 09:23:06 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:23:06 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 09:23:06 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:23:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 09:23:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:23:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:23:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:23:06 - INFO - __main__ - Printing 3 examples
06/24/2022 09:23:06 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 09:23:06 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:23:06 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 09:23:06 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:23:06 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 09:23:06 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:23:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:23:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:23:07 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:23:13 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:23:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:23:13 - INFO - __main__ - Starting training!
06/24/2022 09:23:15 - INFO - __main__ - Step 10 Global step 10 Train loss 8.39 on epoch=4
06/24/2022 09:23:16 - INFO - __main__ - Step 20 Global step 20 Train loss 8.34 on epoch=9
06/24/2022 09:23:17 - INFO - __main__ - Step 30 Global step 30 Train loss 8.27 on epoch=14
06/24/2022 09:23:18 - INFO - __main__ - Step 40 Global step 40 Train loss 8.23 on epoch=19
06/24/2022 09:23:20 - INFO - __main__ - Step 50 Global step 50 Train loss 8.25 on epoch=24
06/24/2022 09:23:21 - INFO - __main__ - Global step 50 Train loss 8.30 ACC 0.0 on epoch=24
06/24/2022 09:23:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 09:23:23 - INFO - __main__ - Step 60 Global step 60 Train loss 8.17 on epoch=29
06/24/2022 09:23:24 - INFO - __main__ - Step 70 Global step 70 Train loss 8.24 on epoch=34
06/24/2022 09:23:25 - INFO - __main__ - Step 80 Global step 80 Train loss 8.05 on epoch=39
06/24/2022 09:23:26 - INFO - __main__ - Step 90 Global step 90 Train loss 8.14 on epoch=44
06/24/2022 09:23:28 - INFO - __main__ - Step 100 Global step 100 Train loss 8.02 on epoch=49
06/24/2022 09:23:30 - INFO - __main__ - Global step 100 Train loss 8.12 ACC 0.0 on epoch=49
06/24/2022 09:23:32 - INFO - __main__ - Step 110 Global step 110 Train loss 8.09 on epoch=54
06/24/2022 09:23:33 - INFO - __main__ - Step 120 Global step 120 Train loss 8.05 on epoch=59
06/24/2022 09:23:34 - INFO - __main__ - Step 130 Global step 130 Train loss 7.98 on epoch=64
06/24/2022 09:23:35 - INFO - __main__ - Step 140 Global step 140 Train loss 7.91 on epoch=69
06/24/2022 09:23:37 - INFO - __main__ - Step 150 Global step 150 Train loss 7.93 on epoch=74
06/24/2022 09:23:39 - INFO - __main__ - Global step 150 Train loss 7.99 ACC 0.0 on epoch=74
06/24/2022 09:23:41 - INFO - __main__ - Step 160 Global step 160 Train loss 7.95 on epoch=79
06/24/2022 09:23:42 - INFO - __main__ - Step 170 Global step 170 Train loss 7.97 on epoch=84
06/24/2022 09:23:43 - INFO - __main__ - Step 180 Global step 180 Train loss 7.82 on epoch=89
06/24/2022 09:23:44 - INFO - __main__ - Step 190 Global step 190 Train loss 7.87 on epoch=94
06/24/2022 09:23:46 - INFO - __main__ - Step 200 Global step 200 Train loss 7.81 on epoch=99
06/24/2022 09:23:53 - INFO - __main__ - Global step 200 Train loss 7.88 ACC 0.0 on epoch=99
06/24/2022 09:23:55 - INFO - __main__ - Step 210 Global step 210 Train loss 7.81 on epoch=104
06/24/2022 09:23:56 - INFO - __main__ - Step 220 Global step 220 Train loss 7.76 on epoch=109
06/24/2022 09:23:57 - INFO - __main__ - Step 230 Global step 230 Train loss 7.74 on epoch=114
06/24/2022 09:23:58 - INFO - __main__ - Step 240 Global step 240 Train loss 7.71 on epoch=119
06/24/2022 09:24:00 - INFO - __main__ - Step 250 Global step 250 Train loss 7.64 on epoch=124
06/24/2022 09:24:06 - INFO - __main__ - Global step 250 Train loss 7.73 ACC 0.0 on epoch=124
06/24/2022 09:24:07 - INFO - __main__ - Step 260 Global step 260 Train loss 7.58 on epoch=129
06/24/2022 09:24:08 - INFO - __main__ - Step 270 Global step 270 Train loss 7.70 on epoch=134
06/24/2022 09:24:10 - INFO - __main__ - Step 280 Global step 280 Train loss 7.68 on epoch=139
06/24/2022 09:24:11 - INFO - __main__ - Step 290 Global step 290 Train loss 7.53 on epoch=144
06/24/2022 09:24:12 - INFO - __main__ - Step 300 Global step 300 Train loss 7.49 on epoch=149
06/24/2022 09:24:13 - INFO - __main__ - Global step 300 Train loss 7.60 ACC 0.0 on epoch=149
06/24/2022 09:24:14 - INFO - __main__ - Step 310 Global step 310 Train loss 7.43 on epoch=154
06/24/2022 09:24:15 - INFO - __main__ - Step 320 Global step 320 Train loss 7.51 on epoch=159
06/24/2022 09:24:17 - INFO - __main__ - Step 330 Global step 330 Train loss 7.29 on epoch=164
06/24/2022 09:24:18 - INFO - __main__ - Step 340 Global step 340 Train loss 7.23 on epoch=169
06/24/2022 09:24:19 - INFO - __main__ - Step 350 Global step 350 Train loss 7.13 on epoch=174
06/24/2022 09:24:25 - INFO - __main__ - Global step 350 Train loss 7.32 ACC 0.0 on epoch=174
06/24/2022 09:24:27 - INFO - __main__ - Step 360 Global step 360 Train loss 6.97 on epoch=179
06/24/2022 09:24:28 - INFO - __main__ - Step 370 Global step 370 Train loss 6.90 on epoch=184
06/24/2022 09:24:29 - INFO - __main__ - Step 380 Global step 380 Train loss 6.68 on epoch=189
06/24/2022 09:24:30 - INFO - __main__ - Step 390 Global step 390 Train loss 6.33 on epoch=194
06/24/2022 09:24:32 - INFO - __main__ - Step 400 Global step 400 Train loss 6.04 on epoch=199
06/24/2022 09:24:35 - INFO - __main__ - Global step 400 Train loss 6.58 ACC 0.0 on epoch=199
06/24/2022 09:24:36 - INFO - __main__ - Step 410 Global step 410 Train loss 5.85 on epoch=204
06/24/2022 09:24:37 - INFO - __main__ - Step 420 Global step 420 Train loss 5.68 on epoch=209
06/24/2022 09:24:38 - INFO - __main__ - Step 430 Global step 430 Train loss 5.28 on epoch=214
06/24/2022 09:24:40 - INFO - __main__ - Step 440 Global step 440 Train loss 5.20 on epoch=219
06/24/2022 09:24:41 - INFO - __main__ - Step 450 Global step 450 Train loss 5.11 on epoch=224
06/24/2022 09:24:46 - INFO - __main__ - Global step 450 Train loss 5.42 ACC 0.0 on epoch=224
06/24/2022 09:24:48 - INFO - __main__ - Step 460 Global step 460 Train loss 5.08 on epoch=229
06/24/2022 09:24:49 - INFO - __main__ - Step 470 Global step 470 Train loss 4.97 on epoch=234
06/24/2022 09:24:50 - INFO - __main__ - Step 480 Global step 480 Train loss 5.00 on epoch=239
06/24/2022 09:24:52 - INFO - __main__ - Step 490 Global step 490 Train loss 4.82 on epoch=244
06/24/2022 09:24:53 - INFO - __main__ - Step 500 Global step 500 Train loss 4.93 on epoch=249
06/24/2022 09:24:59 - INFO - __main__ - Global step 500 Train loss 4.96 ACC 0.0 on epoch=249
06/24/2022 09:25:00 - INFO - __main__ - Step 510 Global step 510 Train loss 4.90 on epoch=254
06/24/2022 09:25:01 - INFO - __main__ - Step 520 Global step 520 Train loss 4.74 on epoch=259
06/24/2022 09:25:03 - INFO - __main__ - Step 530 Global step 530 Train loss 4.58 on epoch=264
06/24/2022 09:25:04 - INFO - __main__ - Step 540 Global step 540 Train loss 4.52 on epoch=269
06/24/2022 09:25:05 - INFO - __main__ - Step 550 Global step 550 Train loss 4.46 on epoch=274
06/24/2022 09:25:11 - INFO - __main__ - Global step 550 Train loss 4.64 ACC 0.0 on epoch=274
06/24/2022 09:25:12 - INFO - __main__ - Step 560 Global step 560 Train loss 4.30 on epoch=279
06/24/2022 09:25:13 - INFO - __main__ - Step 570 Global step 570 Train loss 4.27 on epoch=284
06/24/2022 09:25:15 - INFO - __main__ - Step 580 Global step 580 Train loss 4.17 on epoch=289
06/24/2022 09:25:16 - INFO - __main__ - Step 590 Global step 590 Train loss 4.05 on epoch=294
06/24/2022 09:25:17 - INFO - __main__ - Step 600 Global step 600 Train loss 4.03 on epoch=299
06/24/2022 09:25:21 - INFO - __main__ - Global step 600 Train loss 4.16 ACC 0.0 on epoch=299
06/24/2022 09:25:22 - INFO - __main__ - Step 610 Global step 610 Train loss 3.93 on epoch=304
06/24/2022 09:25:23 - INFO - __main__ - Step 620 Global step 620 Train loss 3.78 on epoch=309
06/24/2022 09:25:24 - INFO - __main__ - Step 630 Global step 630 Train loss 3.69 on epoch=314
06/24/2022 09:25:26 - INFO - __main__ - Step 640 Global step 640 Train loss 3.83 on epoch=319
06/24/2022 09:25:27 - INFO - __main__ - Step 650 Global step 650 Train loss 3.67 on epoch=324
06/24/2022 09:25:29 - INFO - __main__ - Global step 650 Train loss 3.78 ACC 0.0 on epoch=324
06/24/2022 09:25:30 - INFO - __main__ - Step 660 Global step 660 Train loss 3.75 on epoch=329
06/24/2022 09:25:31 - INFO - __main__ - Step 670 Global step 670 Train loss 3.54 on epoch=334
06/24/2022 09:25:33 - INFO - __main__ - Step 680 Global step 680 Train loss 3.45 on epoch=339
06/24/2022 09:25:34 - INFO - __main__ - Step 690 Global step 690 Train loss 3.51 on epoch=344
06/24/2022 09:25:35 - INFO - __main__ - Step 700 Global step 700 Train loss 3.41 on epoch=349
06/24/2022 09:25:43 - INFO - __main__ - Global step 700 Train loss 3.53 ACC 0.0 on epoch=349
06/24/2022 09:25:44 - INFO - __main__ - Step 710 Global step 710 Train loss 3.27 on epoch=354
06/24/2022 09:25:45 - INFO - __main__ - Step 720 Global step 720 Train loss 3.36 on epoch=359
06/24/2022 09:25:46 - INFO - __main__ - Step 730 Global step 730 Train loss 3.23 on epoch=364
06/24/2022 09:25:48 - INFO - __main__ - Step 740 Global step 740 Train loss 3.32 on epoch=369
06/24/2022 09:25:49 - INFO - __main__ - Step 750 Global step 750 Train loss 3.27 on epoch=374
06/24/2022 09:25:51 - INFO - __main__ - Global step 750 Train loss 3.29 ACC 0.0 on epoch=374
06/24/2022 09:25:52 - INFO - __main__ - Step 760 Global step 760 Train loss 3.19 on epoch=379
06/24/2022 09:25:53 - INFO - __main__ - Step 770 Global step 770 Train loss 3.15 on epoch=384
06/24/2022 09:25:54 - INFO - __main__ - Step 780 Global step 780 Train loss 3.18 on epoch=389
06/24/2022 09:25:56 - INFO - __main__ - Step 790 Global step 790 Train loss 3.14 on epoch=394
06/24/2022 09:25:57 - INFO - __main__ - Step 800 Global step 800 Train loss 3.14 on epoch=399
06/24/2022 09:26:00 - INFO - __main__ - Global step 800 Train loss 3.16 ACC 0.0 on epoch=399
06/24/2022 09:26:01 - INFO - __main__ - Step 810 Global step 810 Train loss 3.02 on epoch=404
06/24/2022 09:26:02 - INFO - __main__ - Step 820 Global step 820 Train loss 3.07 on epoch=409
06/24/2022 09:26:04 - INFO - __main__ - Step 830 Global step 830 Train loss 3.11 on epoch=414
06/24/2022 09:26:05 - INFO - __main__ - Step 840 Global step 840 Train loss 3.20 on epoch=419
06/24/2022 09:26:06 - INFO - __main__ - Step 850 Global step 850 Train loss 3.08 on epoch=424
06/24/2022 09:26:17 - INFO - __main__ - Global step 850 Train loss 3.09 ACC 0.0 on epoch=424
06/24/2022 09:26:18 - INFO - __main__ - Step 860 Global step 860 Train loss 2.89 on epoch=429
06/24/2022 09:26:20 - INFO - __main__ - Step 870 Global step 870 Train loss 2.89 on epoch=434
06/24/2022 09:26:21 - INFO - __main__ - Step 880 Global step 880 Train loss 2.74 on epoch=439
06/24/2022 09:26:22 - INFO - __main__ - Step 890 Global step 890 Train loss 2.66 on epoch=444
06/24/2022 09:26:23 - INFO - __main__ - Step 900 Global step 900 Train loss 2.72 on epoch=449
06/24/2022 09:26:27 - INFO - __main__ - Global step 900 Train loss 2.78 ACC 0.375 on epoch=449
06/24/2022 09:26:27 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.375 on epoch=449, global_step=900
06/24/2022 09:26:28 - INFO - __main__ - Step 910 Global step 910 Train loss 2.73 on epoch=454
06/24/2022 09:26:29 - INFO - __main__ - Step 920 Global step 920 Train loss 2.47 on epoch=459
06/24/2022 09:26:31 - INFO - __main__ - Step 930 Global step 930 Train loss 2.56 on epoch=464
06/24/2022 09:26:32 - INFO - __main__ - Step 940 Global step 940 Train loss 2.36 on epoch=469
06/24/2022 09:26:33 - INFO - __main__ - Step 950 Global step 950 Train loss 2.35 on epoch=474
06/24/2022 09:26:43 - INFO - __main__ - Global step 950 Train loss 2.49 ACC 0.5 on epoch=474
06/24/2022 09:26:43 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.5 on epoch=474, global_step=950
06/24/2022 09:26:44 - INFO - __main__ - Step 960 Global step 960 Train loss 2.41 on epoch=479
06/24/2022 09:26:45 - INFO - __main__ - Step 970 Global step 970 Train loss 2.26 on epoch=484
06/24/2022 09:26:47 - INFO - __main__ - Step 980 Global step 980 Train loss 2.44 on epoch=489
06/24/2022 09:26:48 - INFO - __main__ - Step 990 Global step 990 Train loss 2.48 on epoch=494
06/24/2022 09:26:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.27 on epoch=499
06/24/2022 09:26:56 - INFO - __main__ - Global step 1000 Train loss 2.37 ACC 0.5 on epoch=499
06/24/2022 09:26:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.23 on epoch=504
06/24/2022 09:26:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.19 on epoch=509
06/24/2022 09:27:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.03 on epoch=514
06/24/2022 09:27:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.05 on epoch=519
06/24/2022 09:27:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.06 on epoch=524
06/24/2022 09:27:13 - INFO - __main__ - Global step 1050 Train loss 2.11 ACC 0.5 on epoch=524
06/24/2022 09:27:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.14 on epoch=529
06/24/2022 09:27:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.07 on epoch=534
06/24/2022 09:27:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.98 on epoch=539
06/24/2022 09:27:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.97 on epoch=544
06/24/2022 09:27:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.81 on epoch=549
06/24/2022 09:27:29 - INFO - __main__ - Global step 1100 Train loss 1.99 ACC 0.5 on epoch=549
06/24/2022 09:27:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.89 on epoch=554
06/24/2022 09:27:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.76 on epoch=559
06/24/2022 09:27:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.80 on epoch=564
06/24/2022 09:27:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.61 on epoch=569
06/24/2022 09:27:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.87 on epoch=574
06/24/2022 09:27:45 - INFO - __main__ - Global step 1150 Train loss 1.79 ACC 0.5 on epoch=574
06/24/2022 09:27:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.76 on epoch=579
06/24/2022 09:27:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.63 on epoch=584
06/24/2022 09:27:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.73 on epoch=589
06/24/2022 09:27:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.67 on epoch=594
06/24/2022 09:27:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.53 on epoch=599
06/24/2022 09:28:02 - INFO - __main__ - Global step 1200 Train loss 1.66 ACC 0.5 on epoch=599
06/24/2022 09:28:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.61 on epoch=604
06/24/2022 09:28:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.46 on epoch=609
06/24/2022 09:28:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.45 on epoch=614
06/24/2022 09:28:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.54 on epoch=619
06/24/2022 09:28:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.37 on epoch=624
06/24/2022 09:28:18 - INFO - __main__ - Global step 1250 Train loss 1.48 ACC 0.5 on epoch=624
06/24/2022 09:28:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.33 on epoch=629
06/24/2022 09:28:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.35 on epoch=634
06/24/2022 09:28:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.35 on epoch=639
06/24/2022 09:28:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.20 on epoch=644
06/24/2022 09:28:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.32 on epoch=649
06/24/2022 09:28:31 - INFO - __main__ - Global step 1300 Train loss 1.31 ACC 0.5 on epoch=649
06/24/2022 09:28:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.30 on epoch=654
06/24/2022 09:28:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.35 on epoch=659
06/24/2022 09:28:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.21 on epoch=664
06/24/2022 09:28:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.31 on epoch=669
06/24/2022 09:28:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.31 on epoch=674
06/24/2022 09:28:45 - INFO - __main__ - Global step 1350 Train loss 1.29 ACC 0.5 on epoch=674
06/24/2022 09:28:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.17 on epoch=679
06/24/2022 09:28:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.24 on epoch=684
06/24/2022 09:28:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.08 on epoch=689
06/24/2022 09:28:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.31 on epoch=694
06/24/2022 09:28:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.23 on epoch=699
06/24/2022 09:29:01 - INFO - __main__ - Global step 1400 Train loss 1.20 ACC 0.5 on epoch=699
06/24/2022 09:29:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.06 on epoch=704
06/24/2022 09:29:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.97 on epoch=709
06/24/2022 09:29:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.10 on epoch=714
06/24/2022 09:29:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.07 on epoch=719
06/24/2022 09:29:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.00 on epoch=724
06/24/2022 09:29:15 - INFO - __main__ - Global step 1450 Train loss 1.04 ACC 0.5 on epoch=724
06/24/2022 09:29:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.05 on epoch=729
06/24/2022 09:29:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.02 on epoch=734
06/24/2022 09:29:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.98 on epoch=739
06/24/2022 09:29:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.04 on epoch=744
06/24/2022 09:29:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.91 on epoch=749
06/24/2022 09:29:30 - INFO - __main__ - Global step 1500 Train loss 1.00 ACC 0.5 on epoch=749
06/24/2022 09:29:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.92 on epoch=754
06/24/2022 09:29:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.99 on epoch=759
06/24/2022 09:29:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.87 on epoch=764
06/24/2022 09:29:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.89 on epoch=769
06/24/2022 09:29:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.88 on epoch=774
06/24/2022 09:29:43 - INFO - __main__ - Global step 1550 Train loss 0.91 ACC 0.46875 on epoch=774
06/24/2022 09:29:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.94 on epoch=779
06/24/2022 09:29:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.77 on epoch=784
06/24/2022 09:29:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.76 on epoch=789
06/24/2022 09:29:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.94 on epoch=794
06/24/2022 09:29:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.81 on epoch=799
06/24/2022 09:29:53 - INFO - __main__ - Global step 1600 Train loss 0.84 ACC 0.5 on epoch=799
06/24/2022 09:29:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.79 on epoch=804
06/24/2022 09:29:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.77 on epoch=809
06/24/2022 09:29:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.83 on epoch=814
06/24/2022 09:29:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.92 on epoch=819
06/24/2022 09:30:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.75 on epoch=824
06/24/2022 09:30:08 - INFO - __main__ - Global step 1650 Train loss 0.81 ACC 0.46875 on epoch=824
06/24/2022 09:30:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.70 on epoch=829
06/24/2022 09:30:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.67 on epoch=834
06/24/2022 09:30:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.74 on epoch=839
06/24/2022 09:30:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.71 on epoch=844
06/24/2022 09:30:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.65 on epoch=849
06/24/2022 09:30:23 - INFO - __main__ - Global step 1700 Train loss 0.70 ACC 0.53125 on epoch=849
06/24/2022 09:30:23 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=849, global_step=1700
06/24/2022 09:30:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.92 on epoch=854
06/24/2022 09:30:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.77 on epoch=859
06/24/2022 09:30:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.63 on epoch=864
06/24/2022 09:30:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.71 on epoch=869
06/24/2022 09:30:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.79 on epoch=874
06/24/2022 09:30:31 - INFO - __main__ - Global step 1750 Train loss 0.77 ACC 0.40625 on epoch=874
06/24/2022 09:30:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.67 on epoch=879
06/24/2022 09:30:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.66 on epoch=884
06/24/2022 09:30:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.69 on epoch=889
06/24/2022 09:30:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.59 on epoch=894
06/24/2022 09:30:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.62 on epoch=899
06/24/2022 09:30:39 - INFO - __main__ - Global step 1800 Train loss 0.65 ACC 0.5 on epoch=899
06/24/2022 09:30:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.74 on epoch=904
06/24/2022 09:30:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.74 on epoch=909
06/24/2022 09:30:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.70 on epoch=914
06/24/2022 09:30:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.68 on epoch=919
06/24/2022 09:30:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.59 on epoch=924
06/24/2022 09:30:47 - INFO - __main__ - Global step 1850 Train loss 0.69 ACC 0.53125 on epoch=924
06/24/2022 09:30:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.70 on epoch=929
06/24/2022 09:30:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.74 on epoch=934
06/24/2022 09:30:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.70 on epoch=939
06/24/2022 09:30:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.69 on epoch=944
06/24/2022 09:30:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.68 on epoch=949
06/24/2022 09:30:55 - INFO - __main__ - Global step 1900 Train loss 0.70 ACC 0.5 on epoch=949
06/24/2022 09:30:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.74 on epoch=954
06/24/2022 09:30:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.64 on epoch=959
06/24/2022 09:30:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.66 on epoch=964
06/24/2022 09:31:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.74 on epoch=969
06/24/2022 09:31:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.57 on epoch=974
06/24/2022 09:31:04 - INFO - __main__ - Global step 1950 Train loss 0.67 ACC 0.46875 on epoch=974
06/24/2022 09:31:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.50 on epoch=979
06/24/2022 09:31:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.65 on epoch=984
06/24/2022 09:31:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.70 on epoch=989
06/24/2022 09:31:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.74 on epoch=994
06/24/2022 09:31:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.43 on epoch=999
06/24/2022 09:31:11 - INFO - __main__ - Global step 2000 Train loss 0.61 ACC 0.46875 on epoch=999
06/24/2022 09:31:11 - INFO - __main__ - save last model!
06/24/2022 09:31:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 09:31:11 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 09:31:11 - INFO - __main__ - Printing 3 examples
06/24/2022 09:31:11 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 09:31:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:11 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 09:31:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:11 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 09:31:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:11 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:31:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:31:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:31:12 - INFO - __main__ - Printing 3 examples
06/24/2022 09:31:12 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 09:31:12 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:12 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 09:31:12 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:12 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 09:31:12 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 09:31:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:31:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:31:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:31:12 - INFO - __main__ - Printing 3 examples
06/24/2022 09:31:12 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 09:31:12 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:12 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 09:31:12 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:12 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 09:31:12 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:31:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:31:12 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:31:12 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 09:31:17 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:31:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:31:17 - INFO - __main__ - Starting training!
06/24/2022 09:31:27 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_42_0.4_8_predictions.txt
06/24/2022 09:31:27 - INFO - __main__ - ACC on test data: 0.5082
06/24/2022 09:31:27 - INFO - __main__ - prefix=medical_questions_pairs_16_42, lr=0.4, bsz=8, dev_performance=0.53125, test_performance=0.5081967213114754
06/24/2022 09:31:27 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_42, lr=0.3, bsz=8 ...
06/24/2022 09:31:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:31:28 - INFO - __main__ - Printing 3 examples
06/24/2022 09:31:28 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 09:31:28 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:28 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 09:31:28 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:28 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 09:31:28 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 09:31:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:31:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:31:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:31:28 - INFO - __main__ - Printing 3 examples
06/24/2022 09:31:28 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 09:31:28 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:28 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 09:31:28 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:28 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 09:31:28 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:31:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:31:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:31:28 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:31:35 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:31:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:31:35 - INFO - __main__ - Starting training!
06/24/2022 09:31:36 - INFO - __main__ - Step 10 Global step 10 Train loss 8.31 on epoch=4
06/24/2022 09:31:38 - INFO - __main__ - Step 20 Global step 20 Train loss 8.35 on epoch=9
06/24/2022 09:31:39 - INFO - __main__ - Step 30 Global step 30 Train loss 8.31 on epoch=14
06/24/2022 09:31:40 - INFO - __main__ - Step 40 Global step 40 Train loss 8.36 on epoch=19
06/24/2022 09:31:41 - INFO - __main__ - Step 50 Global step 50 Train loss 8.29 on epoch=24
06/24/2022 09:31:43 - INFO - __main__ - Global step 50 Train loss 8.32 ACC 0.0 on epoch=24
06/24/2022 09:31:43 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 09:31:44 - INFO - __main__ - Step 60 Global step 60 Train loss 8.41 on epoch=29
06/24/2022 09:31:45 - INFO - __main__ - Step 70 Global step 70 Train loss 8.19 on epoch=34
06/24/2022 09:31:47 - INFO - __main__ - Step 80 Global step 80 Train loss 8.25 on epoch=39
06/24/2022 09:31:48 - INFO - __main__ - Step 90 Global step 90 Train loss 8.08 on epoch=44
06/24/2022 09:31:49 - INFO - __main__ - Step 100 Global step 100 Train loss 8.14 on epoch=49
06/24/2022 09:31:51 - INFO - __main__ - Global step 100 Train loss 8.21 ACC 0.0 on epoch=49
06/24/2022 09:31:52 - INFO - __main__ - Step 110 Global step 110 Train loss 8.07 on epoch=54
06/24/2022 09:31:53 - INFO - __main__ - Step 120 Global step 120 Train loss 8.04 on epoch=59
06/24/2022 09:31:55 - INFO - __main__ - Step 130 Global step 130 Train loss 8.11 on epoch=64
06/24/2022 09:31:56 - INFO - __main__ - Step 140 Global step 140 Train loss 8.13 on epoch=69
06/24/2022 09:31:57 - INFO - __main__ - Step 150 Global step 150 Train loss 8.01 on epoch=74
06/24/2022 09:31:58 - INFO - __main__ - Global step 150 Train loss 8.07 ACC 0.0 on epoch=74
06/24/2022 09:32:00 - INFO - __main__ - Step 160 Global step 160 Train loss 8.03 on epoch=79
06/24/2022 09:32:01 - INFO - __main__ - Step 170 Global step 170 Train loss 7.82 on epoch=84
06/24/2022 09:32:02 - INFO - __main__ - Step 180 Global step 180 Train loss 7.91 on epoch=89
06/24/2022 09:32:03 - INFO - __main__ - Step 190 Global step 190 Train loss 7.79 on epoch=94
06/24/2022 09:32:05 - INFO - __main__ - Step 200 Global step 200 Train loss 7.67 on epoch=99
06/24/2022 09:32:06 - INFO - __main__ - Global step 200 Train loss 7.85 ACC 0.0 on epoch=99
06/24/2022 09:32:07 - INFO - __main__ - Step 210 Global step 210 Train loss 7.72 on epoch=104
06/24/2022 09:32:09 - INFO - __main__ - Step 220 Global step 220 Train loss 7.60 on epoch=109
06/24/2022 09:32:10 - INFO - __main__ - Step 230 Global step 230 Train loss 7.60 on epoch=114
06/24/2022 09:32:11 - INFO - __main__ - Step 240 Global step 240 Train loss 7.48 on epoch=119
06/24/2022 09:32:13 - INFO - __main__ - Step 250 Global step 250 Train loss 7.38 on epoch=124
06/24/2022 09:32:15 - INFO - __main__ - Global step 250 Train loss 7.55 ACC 0.0 on epoch=124
06/24/2022 09:32:16 - INFO - __main__ - Step 260 Global step 260 Train loss 7.33 on epoch=129
06/24/2022 09:32:18 - INFO - __main__ - Step 270 Global step 270 Train loss 7.11 on epoch=134
06/24/2022 09:32:19 - INFO - __main__ - Step 280 Global step 280 Train loss 7.28 on epoch=139
06/24/2022 09:32:20 - INFO - __main__ - Step 290 Global step 290 Train loss 7.06 on epoch=144
06/24/2022 09:32:22 - INFO - __main__ - Step 300 Global step 300 Train loss 7.03 on epoch=149
06/24/2022 09:32:24 - INFO - __main__ - Global step 300 Train loss 7.16 ACC 0.0 on epoch=149
06/24/2022 09:32:25 - INFO - __main__ - Step 310 Global step 310 Train loss 6.80 on epoch=154
06/24/2022 09:32:26 - INFO - __main__ - Step 320 Global step 320 Train loss 6.81 on epoch=159
06/24/2022 09:32:27 - INFO - __main__ - Step 330 Global step 330 Train loss 6.69 on epoch=164
06/24/2022 09:32:29 - INFO - __main__ - Step 340 Global step 340 Train loss 6.53 on epoch=169
06/24/2022 09:32:30 - INFO - __main__ - Step 350 Global step 350 Train loss 6.41 on epoch=174
06/24/2022 09:32:34 - INFO - __main__ - Global step 350 Train loss 6.65 ACC 0.0 on epoch=174
06/24/2022 09:32:35 - INFO - __main__ - Step 360 Global step 360 Train loss 6.53 on epoch=179
06/24/2022 09:32:36 - INFO - __main__ - Step 370 Global step 370 Train loss 6.39 on epoch=184
06/24/2022 09:32:38 - INFO - __main__ - Step 380 Global step 380 Train loss 6.25 on epoch=189
06/24/2022 09:32:39 - INFO - __main__ - Step 390 Global step 390 Train loss 6.21 on epoch=194
06/24/2022 09:32:40 - INFO - __main__ - Step 400 Global step 400 Train loss 6.06 on epoch=199
06/24/2022 09:32:41 - INFO - __main__ - Global step 400 Train loss 6.29 ACC 0.0 on epoch=199
06/24/2022 09:32:43 - INFO - __main__ - Step 410 Global step 410 Train loss 6.13 on epoch=204
06/24/2022 09:32:44 - INFO - __main__ - Step 420 Global step 420 Train loss 5.95 on epoch=209
06/24/2022 09:32:45 - INFO - __main__ - Step 430 Global step 430 Train loss 5.84 on epoch=214
06/24/2022 09:32:46 - INFO - __main__ - Step 440 Global step 440 Train loss 5.64 on epoch=219
06/24/2022 09:32:48 - INFO - __main__ - Step 450 Global step 450 Train loss 5.68 on epoch=224
06/24/2022 09:32:59 - INFO - __main__ - Global step 450 Train loss 5.85 ACC 0.0 on epoch=224
06/24/2022 09:33:00 - INFO - __main__ - Step 460 Global step 460 Train loss 5.64 on epoch=229
06/24/2022 09:33:01 - INFO - __main__ - Step 470 Global step 470 Train loss 5.39 on epoch=234
06/24/2022 09:33:02 - INFO - __main__ - Step 480 Global step 480 Train loss 5.33 on epoch=239
06/24/2022 09:33:04 - INFO - __main__ - Step 490 Global step 490 Train loss 5.32 on epoch=244
06/24/2022 09:33:05 - INFO - __main__ - Step 500 Global step 500 Train loss 5.11 on epoch=249
06/24/2022 09:33:07 - INFO - __main__ - Global step 500 Train loss 5.36 ACC 0.0 on epoch=249
06/24/2022 09:33:08 - INFO - __main__ - Step 510 Global step 510 Train loss 4.99 on epoch=254
06/24/2022 09:33:09 - INFO - __main__ - Step 520 Global step 520 Train loss 4.79 on epoch=259
06/24/2022 09:33:10 - INFO - __main__ - Step 530 Global step 530 Train loss 4.85 on epoch=264
06/24/2022 09:33:12 - INFO - __main__ - Step 540 Global step 540 Train loss 4.76 on epoch=269
06/24/2022 09:33:13 - INFO - __main__ - Step 550 Global step 550 Train loss 4.39 on epoch=274
06/24/2022 09:33:14 - INFO - __main__ - Global step 550 Train loss 4.76 ACC 0.0 on epoch=274
06/24/2022 09:33:16 - INFO - __main__ - Step 560 Global step 560 Train loss 4.38 on epoch=279
06/24/2022 09:33:17 - INFO - __main__ - Step 570 Global step 570 Train loss 3.99 on epoch=284
06/24/2022 09:33:18 - INFO - __main__ - Step 580 Global step 580 Train loss 4.00 on epoch=289
06/24/2022 09:33:20 - INFO - __main__ - Step 590 Global step 590 Train loss 4.09 on epoch=294
06/24/2022 09:33:21 - INFO - __main__ - Step 600 Global step 600 Train loss 3.93 on epoch=299
06/24/2022 09:33:22 - INFO - __main__ - Global step 600 Train loss 4.08 ACC 0.0 on epoch=299
06/24/2022 09:33:24 - INFO - __main__ - Step 610 Global step 610 Train loss 3.65 on epoch=304
06/24/2022 09:33:25 - INFO - __main__ - Step 620 Global step 620 Train loss 3.61 on epoch=309
06/24/2022 09:33:26 - INFO - __main__ - Step 630 Global step 630 Train loss 3.69 on epoch=314
06/24/2022 09:33:27 - INFO - __main__ - Step 640 Global step 640 Train loss 3.57 on epoch=319
06/24/2022 09:33:29 - INFO - __main__ - Step 650 Global step 650 Train loss 3.59 on epoch=324
06/24/2022 09:33:30 - INFO - __main__ - Global step 650 Train loss 3.62 ACC 0.0 on epoch=324
06/24/2022 09:33:31 - INFO - __main__ - Step 660 Global step 660 Train loss 3.42 on epoch=329
06/24/2022 09:33:33 - INFO - __main__ - Step 670 Global step 670 Train loss 3.31 on epoch=334
06/24/2022 09:33:34 - INFO - __main__ - Step 680 Global step 680 Train loss 3.23 on epoch=339
06/24/2022 09:33:35 - INFO - __main__ - Step 690 Global step 690 Train loss 3.12 on epoch=344
06/24/2022 09:33:36 - INFO - __main__ - Step 700 Global step 700 Train loss 3.16 on epoch=349
06/24/2022 09:33:42 - INFO - __main__ - Global step 700 Train loss 3.25 ACC 0.0 on epoch=349
06/24/2022 09:33:43 - INFO - __main__ - Step 710 Global step 710 Train loss 3.31 on epoch=354
06/24/2022 09:33:45 - INFO - __main__ - Step 720 Global step 720 Train loss 3.16 on epoch=359
06/24/2022 09:33:46 - INFO - __main__ - Step 730 Global step 730 Train loss 3.00 on epoch=364
06/24/2022 09:33:47 - INFO - __main__ - Step 740 Global step 740 Train loss 3.04 on epoch=369
06/24/2022 09:33:48 - INFO - __main__ - Step 750 Global step 750 Train loss 3.04 on epoch=374
06/24/2022 09:33:50 - INFO - __main__ - Global step 750 Train loss 3.11 ACC 0.0 on epoch=374
06/24/2022 09:33:51 - INFO - __main__ - Step 760 Global step 760 Train loss 2.92 on epoch=379
06/24/2022 09:33:52 - INFO - __main__ - Step 770 Global step 770 Train loss 2.78 on epoch=384
06/24/2022 09:33:53 - INFO - __main__ - Step 780 Global step 780 Train loss 2.84 on epoch=389
06/24/2022 09:33:55 - INFO - __main__ - Step 790 Global step 790 Train loss 2.81 on epoch=394
06/24/2022 09:33:56 - INFO - __main__ - Step 800 Global step 800 Train loss 2.74 on epoch=399
06/24/2022 09:34:03 - INFO - __main__ - Global step 800 Train loss 2.82 ACC 0.0 on epoch=399
06/24/2022 09:34:04 - INFO - __main__ - Step 810 Global step 810 Train loss 2.72 on epoch=404
06/24/2022 09:34:05 - INFO - __main__ - Step 820 Global step 820 Train loss 2.71 on epoch=409
06/24/2022 09:34:06 - INFO - __main__ - Step 830 Global step 830 Train loss 2.71 on epoch=414
06/24/2022 09:34:08 - INFO - __main__ - Step 840 Global step 840 Train loss 2.53 on epoch=419
06/24/2022 09:34:09 - INFO - __main__ - Step 850 Global step 850 Train loss 2.62 on epoch=424
06/24/2022 09:34:16 - INFO - __main__ - Global step 850 Train loss 2.66 ACC 0.0 on epoch=424
06/24/2022 09:34:17 - INFO - __main__ - Step 860 Global step 860 Train loss 2.61 on epoch=429
06/24/2022 09:34:18 - INFO - __main__ - Step 870 Global step 870 Train loss 2.40 on epoch=434
06/24/2022 09:34:20 - INFO - __main__ - Step 880 Global step 880 Train loss 2.47 on epoch=439
06/24/2022 09:34:21 - INFO - __main__ - Step 890 Global step 890 Train loss 2.39 on epoch=444
06/24/2022 09:34:22 - INFO - __main__ - Step 900 Global step 900 Train loss 2.33 on epoch=449
06/24/2022 09:34:24 - INFO - __main__ - Global step 900 Train loss 2.44 ACC 0.0 on epoch=449
06/24/2022 09:34:25 - INFO - __main__ - Step 910 Global step 910 Train loss 2.56 on epoch=454
06/24/2022 09:34:26 - INFO - __main__ - Step 920 Global step 920 Train loss 2.45 on epoch=459
06/24/2022 09:34:27 - INFO - __main__ - Step 930 Global step 930 Train loss 2.39 on epoch=464
06/24/2022 09:34:29 - INFO - __main__ - Step 940 Global step 940 Train loss 2.36 on epoch=469
06/24/2022 09:34:30 - INFO - __main__ - Step 950 Global step 950 Train loss 2.38 on epoch=474
06/24/2022 09:34:31 - INFO - __main__ - Global step 950 Train loss 2.43 ACC 0.0 on epoch=474
06/24/2022 09:34:32 - INFO - __main__ - Step 960 Global step 960 Train loss 2.19 on epoch=479
06/24/2022 09:34:34 - INFO - __main__ - Step 970 Global step 970 Train loss 2.25 on epoch=484
06/24/2022 09:34:35 - INFO - __main__ - Step 980 Global step 980 Train loss 2.29 on epoch=489
06/24/2022 09:34:36 - INFO - __main__ - Step 990 Global step 990 Train loss 2.31 on epoch=494
06/24/2022 09:34:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.22 on epoch=499
06/24/2022 09:34:38 - INFO - __main__ - Global step 1000 Train loss 2.25 ACC 0.03125 on epoch=499
06/24/2022 09:34:38 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=499, global_step=1000
06/24/2022 09:34:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.03 on epoch=504
06/24/2022 09:34:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.32 on epoch=509
06/24/2022 09:34:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.08 on epoch=514
06/24/2022 09:34:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.06 on epoch=519
06/24/2022 09:34:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.12 on epoch=524
06/24/2022 09:34:50 - INFO - __main__ - Global step 1050 Train loss 2.12 ACC 0.46875 on epoch=524
06/24/2022 09:34:50 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.46875 on epoch=524, global_step=1050
06/24/2022 09:34:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.93 on epoch=529
06/24/2022 09:34:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.02 on epoch=534
06/24/2022 09:34:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.77 on epoch=539
06/24/2022 09:34:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.86 on epoch=544
06/24/2022 09:34:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.85 on epoch=549
06/24/2022 09:35:02 - INFO - __main__ - Global step 1100 Train loss 1.89 ACC 0.46875 on epoch=549
06/24/2022 09:35:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.87 on epoch=554
06/24/2022 09:35:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.73 on epoch=559
06/24/2022 09:35:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.76 on epoch=564
06/24/2022 09:35:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.62 on epoch=569
06/24/2022 09:35:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.56 on epoch=574
06/24/2022 09:35:20 - INFO - __main__ - Global step 1150 Train loss 1.71 ACC 0.46875 on epoch=574
06/24/2022 09:35:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.67 on epoch=579
06/24/2022 09:35:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.65 on epoch=584
06/24/2022 09:35:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.59 on epoch=589
06/24/2022 09:35:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.65 on epoch=594
06/24/2022 09:35:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.53 on epoch=599
06/24/2022 09:35:34 - INFO - __main__ - Global step 1200 Train loss 1.62 ACC 0.46875 on epoch=599
06/24/2022 09:35:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.43 on epoch=604
06/24/2022 09:35:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.37 on epoch=609
06/24/2022 09:35:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.53 on epoch=614
06/24/2022 09:35:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.39 on epoch=619
06/24/2022 09:35:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.41 on epoch=624
06/24/2022 09:35:44 - INFO - __main__ - Global step 1250 Train loss 1.43 ACC 0.65625 on epoch=624
06/24/2022 09:35:44 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.65625 on epoch=624, global_step=1250
06/24/2022 09:35:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.33 on epoch=629
06/24/2022 09:35:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.43 on epoch=634
06/24/2022 09:35:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.31 on epoch=639
06/24/2022 09:35:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.22 on epoch=644
06/24/2022 09:35:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.25 on epoch=649
06/24/2022 09:36:01 - INFO - __main__ - Global step 1300 Train loss 1.31 ACC 0.40625 on epoch=649
06/24/2022 09:36:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.28 on epoch=654
06/24/2022 09:36:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.27 on epoch=659
06/24/2022 09:36:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.29 on epoch=664
06/24/2022 09:36:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.36 on epoch=669
06/24/2022 09:36:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.13 on epoch=674
06/24/2022 09:36:18 - INFO - __main__ - Global step 1350 Train loss 1.27 ACC 0.5 on epoch=674
06/24/2022 09:36:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.20 on epoch=679
06/24/2022 09:36:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.08 on epoch=684
06/24/2022 09:36:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.11 on epoch=689
06/24/2022 09:36:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.11 on epoch=694
06/24/2022 09:36:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.10 on epoch=699
06/24/2022 09:36:35 - INFO - __main__ - Global step 1400 Train loss 1.12 ACC 0.59375 on epoch=699
06/24/2022 09:36:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.11 on epoch=704
06/24/2022 09:36:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.99 on epoch=709
06/24/2022 09:36:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.17 on epoch=714
06/24/2022 09:36:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.95 on epoch=719
06/24/2022 09:36:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.01 on epoch=724
06/24/2022 09:36:51 - INFO - __main__ - Global step 1450 Train loss 1.05 ACC 0.5 on epoch=724
06/24/2022 09:36:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.86 on epoch=729
06/24/2022 09:36:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.95 on epoch=734
06/24/2022 09:36:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.97 on epoch=739
06/24/2022 09:36:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.94 on epoch=744
06/24/2022 09:36:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.93 on epoch=749
06/24/2022 09:37:01 - INFO - __main__ - Global step 1500 Train loss 0.93 ACC 0.5625 on epoch=749
06/24/2022 09:37:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.85 on epoch=754
06/24/2022 09:37:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.77 on epoch=759
06/24/2022 09:37:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.84 on epoch=764
06/24/2022 09:37:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.85 on epoch=769
06/24/2022 09:37:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.84 on epoch=774
06/24/2022 09:37:17 - INFO - __main__ - Global step 1550 Train loss 0.83 ACC 0.46875 on epoch=774
06/24/2022 09:37:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.87 on epoch=779
06/24/2022 09:37:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.97 on epoch=784
06/24/2022 09:37:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.97 on epoch=789
06/24/2022 09:37:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.73 on epoch=794
06/24/2022 09:37:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.82 on epoch=799
06/24/2022 09:37:24 - INFO - __main__ - Global step 1600 Train loss 0.87 ACC 0.5 on epoch=799
06/24/2022 09:37:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.90 on epoch=804
06/24/2022 09:37:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.81 on epoch=809
06/24/2022 09:37:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.81 on epoch=814
06/24/2022 09:37:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.81 on epoch=819
06/24/2022 09:37:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.83 on epoch=824
06/24/2022 09:37:31 - INFO - __main__ - Global step 1650 Train loss 0.83 ACC 0.5 on epoch=824
06/24/2022 09:37:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.63 on epoch=829
06/24/2022 09:37:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.71 on epoch=834
06/24/2022 09:37:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.77 on epoch=839
06/24/2022 09:37:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.70 on epoch=844
06/24/2022 09:37:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.75 on epoch=849
06/24/2022 09:37:40 - INFO - __main__ - Global step 1700 Train loss 0.71 ACC 0.5 on epoch=849
06/24/2022 09:37:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.87 on epoch=854
06/24/2022 09:37:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.69 on epoch=859
06/24/2022 09:37:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.74 on epoch=864
06/24/2022 09:37:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.79 on epoch=869
06/24/2022 09:37:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.67 on epoch=874
06/24/2022 09:37:47 - INFO - __main__ - Global step 1750 Train loss 0.75 ACC 0.5 on epoch=874
06/24/2022 09:37:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.64 on epoch=879
06/24/2022 09:37:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.79 on epoch=884
06/24/2022 09:37:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.71 on epoch=889
06/24/2022 09:37:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.72 on epoch=894
06/24/2022 09:37:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.74 on epoch=899
06/24/2022 09:37:54 - INFO - __main__ - Global step 1800 Train loss 0.72 ACC 0.46875 on epoch=899
06/24/2022 09:37:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.67 on epoch=904
06/24/2022 09:37:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.68 on epoch=909
06/24/2022 09:37:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.75 on epoch=914
06/24/2022 09:37:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.57 on epoch=919
06/24/2022 09:38:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.70 on epoch=924
06/24/2022 09:38:01 - INFO - __main__ - Global step 1850 Train loss 0.67 ACC 0.53125 on epoch=924
06/24/2022 09:38:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.59 on epoch=929
06/24/2022 09:38:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.67 on epoch=934
06/24/2022 09:38:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.65 on epoch=939
06/24/2022 09:38:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.79 on epoch=944
06/24/2022 09:38:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.62 on epoch=949
06/24/2022 09:38:08 - INFO - __main__ - Global step 1900 Train loss 0.67 ACC 0.5 on epoch=949
06/24/2022 09:38:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.56 on epoch=954
06/24/2022 09:38:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.68 on epoch=959
06/24/2022 09:38:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.58 on epoch=964
06/24/2022 09:38:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.59 on epoch=969
06/24/2022 09:38:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.69 on epoch=974
06/24/2022 09:38:15 - INFO - __main__ - Global step 1950 Train loss 0.62 ACC 0.5 on epoch=974
06/24/2022 09:38:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.65 on epoch=979
06/24/2022 09:38:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.52 on epoch=984
06/24/2022 09:38:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.65 on epoch=989
06/24/2022 09:38:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.71 on epoch=994
06/24/2022 09:38:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.67 on epoch=999
06/24/2022 09:38:22 - INFO - __main__ - Global step 2000 Train loss 0.64 ACC 0.5 on epoch=999
06/24/2022 09:38:22 - INFO - __main__ - save last model!
06/24/2022 09:38:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 09:38:22 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 09:38:22 - INFO - __main__ - Printing 3 examples
06/24/2022 09:38:22 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 09:38:22 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:22 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 09:38:22 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:22 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 09:38:22 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:22 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:38:22 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:38:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:38:22 - INFO - __main__ - Printing 3 examples
06/24/2022 09:38:22 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 09:38:22 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:22 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 09:38:22 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:22 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 09:38:22 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 09:38:22 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:38:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:38:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:38:22 - INFO - __main__ - Printing 3 examples
06/24/2022 09:38:22 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 09:38:22 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:22 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 09:38:22 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:22 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 09:38:22 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:22 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:38:22 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:38:22 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:38:23 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 09:38:27 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:38:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:38:28 - INFO - __main__ - Starting training!
06/24/2022 09:38:35 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_42_0.3_8_predictions.txt
06/24/2022 09:38:35 - INFO - __main__ - ACC on test data: 0.4836
06/24/2022 09:38:35 - INFO - __main__ - prefix=medical_questions_pairs_16_42, lr=0.3, bsz=8, dev_performance=0.65625, test_performance=0.48360655737704916
06/24/2022 09:38:35 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_42, lr=0.2, bsz=8 ...
06/24/2022 09:38:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:38:36 - INFO - __main__ - Printing 3 examples
06/24/2022 09:38:36 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 09:38:36 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:36 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 09:38:36 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:36 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 09:38:36 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 09:38:36 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:38:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:38:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:38:36 - INFO - __main__ - Printing 3 examples
06/24/2022 09:38:36 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 09:38:36 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:36 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 09:38:36 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:36 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 09:38:36 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:38:36 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:38:36 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:38:36 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:38:42 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:38:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:38:43 - INFO - __main__ - Starting training!
06/24/2022 09:38:44 - INFO - __main__ - Step 10 Global step 10 Train loss 8.36 on epoch=4
06/24/2022 09:38:46 - INFO - __main__ - Step 20 Global step 20 Train loss 8.40 on epoch=9
06/24/2022 09:38:47 - INFO - __main__ - Step 30 Global step 30 Train loss 8.40 on epoch=14
06/24/2022 09:38:48 - INFO - __main__ - Step 40 Global step 40 Train loss 8.36 on epoch=19
06/24/2022 09:38:49 - INFO - __main__ - Step 50 Global step 50 Train loss 8.27 on epoch=24
06/24/2022 09:38:51 - INFO - __main__ - Global step 50 Train loss 8.36 ACC 0.0 on epoch=24
06/24/2022 09:38:51 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 09:38:53 - INFO - __main__ - Step 60 Global step 60 Train loss 8.22 on epoch=29
06/24/2022 09:38:54 - INFO - __main__ - Step 70 Global step 70 Train loss 8.32 on epoch=34
06/24/2022 09:38:55 - INFO - __main__ - Step 80 Global step 80 Train loss 8.22 on epoch=39
06/24/2022 09:38:57 - INFO - __main__ - Step 90 Global step 90 Train loss 8.19 on epoch=44
06/24/2022 09:38:58 - INFO - __main__ - Step 100 Global step 100 Train loss 8.24 on epoch=49
06/24/2022 09:38:59 - INFO - __main__ - Global step 100 Train loss 8.24 ACC 0.0 on epoch=49
06/24/2022 09:39:01 - INFO - __main__ - Step 110 Global step 110 Train loss 8.22 on epoch=54
06/24/2022 09:39:02 - INFO - __main__ - Step 120 Global step 120 Train loss 8.11 on epoch=59
06/24/2022 09:39:03 - INFO - __main__ - Step 130 Global step 130 Train loss 8.18 on epoch=64
06/24/2022 09:39:04 - INFO - __main__ - Step 140 Global step 140 Train loss 8.07 on epoch=69
06/24/2022 09:39:06 - INFO - __main__ - Step 150 Global step 150 Train loss 8.14 on epoch=74
06/24/2022 09:39:08 - INFO - __main__ - Global step 150 Train loss 8.14 ACC 0.0 on epoch=74
06/24/2022 09:39:10 - INFO - __main__ - Step 160 Global step 160 Train loss 8.20 on epoch=79
06/24/2022 09:39:11 - INFO - __main__ - Step 170 Global step 170 Train loss 8.02 on epoch=84
06/24/2022 09:39:12 - INFO - __main__ - Step 180 Global step 180 Train loss 8.09 on epoch=89
06/24/2022 09:39:14 - INFO - __main__ - Step 190 Global step 190 Train loss 8.04 on epoch=94
06/24/2022 09:39:15 - INFO - __main__ - Step 200 Global step 200 Train loss 8.03 on epoch=99
06/24/2022 09:39:21 - INFO - __main__ - Global step 200 Train loss 8.08 ACC 0.0 on epoch=99
06/24/2022 09:39:22 - INFO - __main__ - Step 210 Global step 210 Train loss 8.07 on epoch=104
06/24/2022 09:39:23 - INFO - __main__ - Step 220 Global step 220 Train loss 8.06 on epoch=109
06/24/2022 09:39:24 - INFO - __main__ - Step 230 Global step 230 Train loss 7.98 on epoch=114
06/24/2022 09:39:26 - INFO - __main__ - Step 240 Global step 240 Train loss 7.93 on epoch=119
06/24/2022 09:39:27 - INFO - __main__ - Step 250 Global step 250 Train loss 7.98 on epoch=124
06/24/2022 09:39:31 - INFO - __main__ - Global step 250 Train loss 8.01 ACC 0.0 on epoch=124
06/24/2022 09:39:32 - INFO - __main__ - Step 260 Global step 260 Train loss 7.95 on epoch=129
06/24/2022 09:39:34 - INFO - __main__ - Step 270 Global step 270 Train loss 7.92 on epoch=134
06/24/2022 09:39:35 - INFO - __main__ - Step 280 Global step 280 Train loss 7.88 on epoch=139
06/24/2022 09:39:36 - INFO - __main__ - Step 290 Global step 290 Train loss 7.75 on epoch=144
06/24/2022 09:39:37 - INFO - __main__ - Step 300 Global step 300 Train loss 7.98 on epoch=149
06/24/2022 09:39:45 - INFO - __main__ - Global step 300 Train loss 7.90 ACC 0.0 on epoch=149
06/24/2022 09:39:46 - INFO - __main__ - Step 310 Global step 310 Train loss 7.88 on epoch=154
06/24/2022 09:39:48 - INFO - __main__ - Step 320 Global step 320 Train loss 7.80 on epoch=159
06/24/2022 09:39:49 - INFO - __main__ - Step 330 Global step 330 Train loss 7.76 on epoch=164
06/24/2022 09:39:50 - INFO - __main__ - Step 340 Global step 340 Train loss 7.83 on epoch=169
06/24/2022 09:39:52 - INFO - __main__ - Step 350 Global step 350 Train loss 7.87 on epoch=174
06/24/2022 09:39:56 - INFO - __main__ - Global step 350 Train loss 7.83 ACC 0.0 on epoch=174
06/24/2022 09:39:58 - INFO - __main__ - Step 360 Global step 360 Train loss 7.73 on epoch=179
06/24/2022 09:39:59 - INFO - __main__ - Step 370 Global step 370 Train loss 7.75 on epoch=184
06/24/2022 09:40:00 - INFO - __main__ - Step 380 Global step 380 Train loss 7.75 on epoch=189
06/24/2022 09:40:01 - INFO - __main__ - Step 390 Global step 390 Train loss 7.60 on epoch=194
06/24/2022 09:40:03 - INFO - __main__ - Step 400 Global step 400 Train loss 7.73 on epoch=199
06/24/2022 09:40:11 - INFO - __main__ - Global step 400 Train loss 7.71 ACC 0.0 on epoch=199
06/24/2022 09:40:12 - INFO - __main__ - Step 410 Global step 410 Train loss 7.63 on epoch=204
06/24/2022 09:40:14 - INFO - __main__ - Step 420 Global step 420 Train loss 7.58 on epoch=209
06/24/2022 09:40:15 - INFO - __main__ - Step 430 Global step 430 Train loss 7.64 on epoch=214
06/24/2022 09:40:16 - INFO - __main__ - Step 440 Global step 440 Train loss 7.68 on epoch=219
06/24/2022 09:40:18 - INFO - __main__ - Step 450 Global step 450 Train loss 7.49 on epoch=224
06/24/2022 09:40:24 - INFO - __main__ - Global step 450 Train loss 7.60 ACC 0.0 on epoch=224
06/24/2022 09:40:26 - INFO - __main__ - Step 460 Global step 460 Train loss 7.52 on epoch=229
06/24/2022 09:40:27 - INFO - __main__ - Step 470 Global step 470 Train loss 7.53 on epoch=234
06/24/2022 09:40:28 - INFO - __main__ - Step 480 Global step 480 Train loss 7.49 on epoch=239
06/24/2022 09:40:30 - INFO - __main__ - Step 490 Global step 490 Train loss 7.44 on epoch=244
06/24/2022 09:40:31 - INFO - __main__ - Step 500 Global step 500 Train loss 7.45 on epoch=249
06/24/2022 09:40:35 - INFO - __main__ - Global step 500 Train loss 7.49 ACC 0.0 on epoch=249
06/24/2022 09:40:37 - INFO - __main__ - Step 510 Global step 510 Train loss 7.47 on epoch=254
06/24/2022 09:40:38 - INFO - __main__ - Step 520 Global step 520 Train loss 7.40 on epoch=259
06/24/2022 09:40:39 - INFO - __main__ - Step 530 Global step 530 Train loss 7.30 on epoch=264
06/24/2022 09:40:40 - INFO - __main__ - Step 540 Global step 540 Train loss 7.17 on epoch=269
06/24/2022 09:40:42 - INFO - __main__ - Step 550 Global step 550 Train loss 7.10 on epoch=274
06/24/2022 09:40:48 - INFO - __main__ - Global step 550 Train loss 7.29 ACC 0.0 on epoch=274
06/24/2022 09:40:49 - INFO - __main__ - Step 560 Global step 560 Train loss 7.05 on epoch=279
06/24/2022 09:40:51 - INFO - __main__ - Step 570 Global step 570 Train loss 7.03 on epoch=284
06/24/2022 09:40:52 - INFO - __main__ - Step 580 Global step 580 Train loss 6.97 on epoch=289
06/24/2022 09:40:53 - INFO - __main__ - Step 590 Global step 590 Train loss 6.78 on epoch=294
06/24/2022 09:40:55 - INFO - __main__ - Step 600 Global step 600 Train loss 6.73 on epoch=299
06/24/2022 09:41:05 - INFO - __main__ - Global step 600 Train loss 6.91 ACC 0.0 on epoch=299
06/24/2022 09:41:07 - INFO - __main__ - Step 610 Global step 610 Train loss 6.68 on epoch=304
06/24/2022 09:41:08 - INFO - __main__ - Step 620 Global step 620 Train loss 6.52 on epoch=309
06/24/2022 09:41:09 - INFO - __main__ - Step 630 Global step 630 Train loss 6.37 on epoch=314
06/24/2022 09:41:10 - INFO - __main__ - Step 640 Global step 640 Train loss 6.31 on epoch=319
06/24/2022 09:41:12 - INFO - __main__ - Step 650 Global step 650 Train loss 6.27 on epoch=324
06/24/2022 09:41:19 - INFO - __main__ - Global step 650 Train loss 6.43 ACC 0.0 on epoch=324
06/24/2022 09:41:21 - INFO - __main__ - Step 660 Global step 660 Train loss 6.41 on epoch=329
06/24/2022 09:41:22 - INFO - __main__ - Step 670 Global step 670 Train loss 6.29 on epoch=334
06/24/2022 09:41:23 - INFO - __main__ - Step 680 Global step 680 Train loss 6.28 on epoch=339
06/24/2022 09:41:24 - INFO - __main__ - Step 690 Global step 690 Train loss 6.17 on epoch=344
06/24/2022 09:41:26 - INFO - __main__ - Step 700 Global step 700 Train loss 6.15 on epoch=349
06/24/2022 09:41:28 - INFO - __main__ - Global step 700 Train loss 6.26 ACC 0.0 on epoch=349
06/24/2022 09:41:29 - INFO - __main__ - Step 710 Global step 710 Train loss 5.94 on epoch=354
06/24/2022 09:41:31 - INFO - __main__ - Step 720 Global step 720 Train loss 5.87 on epoch=359
06/24/2022 09:41:32 - INFO - __main__ - Step 730 Global step 730 Train loss 5.81 on epoch=364
06/24/2022 09:41:33 - INFO - __main__ - Step 740 Global step 740 Train loss 5.56 on epoch=369
06/24/2022 09:41:35 - INFO - __main__ - Step 750 Global step 750 Train loss 5.70 on epoch=374
06/24/2022 09:41:42 - INFO - __main__ - Global step 750 Train loss 5.78 ACC 0.0 on epoch=374
06/24/2022 09:41:44 - INFO - __main__ - Step 760 Global step 760 Train loss 5.59 on epoch=379
06/24/2022 09:41:45 - INFO - __main__ - Step 770 Global step 770 Train loss 5.68 on epoch=384
06/24/2022 09:41:46 - INFO - __main__ - Step 780 Global step 780 Train loss 5.56 on epoch=389
06/24/2022 09:41:47 - INFO - __main__ - Step 790 Global step 790 Train loss 5.53 on epoch=394
06/24/2022 09:41:49 - INFO - __main__ - Step 800 Global step 800 Train loss 5.21 on epoch=399
06/24/2022 09:41:55 - INFO - __main__ - Global step 800 Train loss 5.51 ACC 0.0 on epoch=399
06/24/2022 09:41:57 - INFO - __main__ - Step 810 Global step 810 Train loss 5.28 on epoch=404
06/24/2022 09:41:58 - INFO - __main__ - Step 820 Global step 820 Train loss 5.19 on epoch=409
06/24/2022 09:41:59 - INFO - __main__ - Step 830 Global step 830 Train loss 5.22 on epoch=414
06/24/2022 09:42:00 - INFO - __main__ - Step 840 Global step 840 Train loss 5.33 on epoch=419
06/24/2022 09:42:02 - INFO - __main__ - Step 850 Global step 850 Train loss 5.11 on epoch=424
06/24/2022 09:42:12 - INFO - __main__ - Global step 850 Train loss 5.22 ACC 0.0 on epoch=424
06/24/2022 09:42:14 - INFO - __main__ - Step 860 Global step 860 Train loss 5.14 on epoch=429
06/24/2022 09:42:15 - INFO - __main__ - Step 870 Global step 870 Train loss 5.06 on epoch=434
06/24/2022 09:42:16 - INFO - __main__ - Step 880 Global step 880 Train loss 5.02 on epoch=439
06/24/2022 09:42:17 - INFO - __main__ - Step 890 Global step 890 Train loss 4.92 on epoch=444
06/24/2022 09:42:19 - INFO - __main__ - Step 900 Global step 900 Train loss 4.81 on epoch=449
06/24/2022 09:42:25 - INFO - __main__ - Global step 900 Train loss 4.99 ACC 0.0 on epoch=449
06/24/2022 09:42:26 - INFO - __main__ - Step 910 Global step 910 Train loss 4.78 on epoch=454
06/24/2022 09:42:28 - INFO - __main__ - Step 920 Global step 920 Train loss 4.78 on epoch=459
06/24/2022 09:42:29 - INFO - __main__ - Step 930 Global step 930 Train loss 4.79 on epoch=464
06/24/2022 09:42:30 - INFO - __main__ - Step 940 Global step 940 Train loss 4.66 on epoch=469
06/24/2022 09:42:31 - INFO - __main__ - Step 950 Global step 950 Train loss 4.56 on epoch=474
06/24/2022 09:42:42 - INFO - __main__ - Global step 950 Train loss 4.71 ACC 0.0 on epoch=474
06/24/2022 09:42:43 - INFO - __main__ - Step 960 Global step 960 Train loss 4.62 on epoch=479
06/24/2022 09:42:44 - INFO - __main__ - Step 970 Global step 970 Train loss 4.58 on epoch=484
06/24/2022 09:42:46 - INFO - __main__ - Step 980 Global step 980 Train loss 4.75 on epoch=489
06/24/2022 09:42:47 - INFO - __main__ - Step 990 Global step 990 Train loss 4.63 on epoch=494
06/24/2022 09:42:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 4.55 on epoch=499
06/24/2022 09:42:54 - INFO - __main__ - Global step 1000 Train loss 4.62 ACC 0.0 on epoch=499
06/24/2022 09:42:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 4.39 on epoch=504
06/24/2022 09:42:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 4.52 on epoch=509
06/24/2022 09:42:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 4.36 on epoch=514
06/24/2022 09:43:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 4.30 on epoch=519
06/24/2022 09:43:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 4.31 on epoch=524
06/24/2022 09:43:05 - INFO - __main__ - Global step 1050 Train loss 4.38 ACC 0.0 on epoch=524
06/24/2022 09:43:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 4.29 on epoch=529
06/24/2022 09:43:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 4.12 on epoch=534
06/24/2022 09:43:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 4.15 on epoch=539
06/24/2022 09:43:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 4.15 on epoch=544
06/24/2022 09:43:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 4.16 on epoch=549
06/24/2022 09:43:22 - INFO - __main__ - Global step 1100 Train loss 4.17 ACC 0.0 on epoch=549
06/24/2022 09:43:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.99 on epoch=554
06/24/2022 09:43:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 4.01 on epoch=559
06/24/2022 09:43:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 4.08 on epoch=564
06/24/2022 09:43:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 4.04 on epoch=569
06/24/2022 09:43:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.83 on epoch=574
06/24/2022 09:43:35 - INFO - __main__ - Global step 1150 Train loss 3.99 ACC 0.0 on epoch=574
06/24/2022 09:43:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.94 on epoch=579
06/24/2022 09:43:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.83 on epoch=584
06/24/2022 09:43:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.83 on epoch=589
06/24/2022 09:43:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.68 on epoch=594
06/24/2022 09:43:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.69 on epoch=599
06/24/2022 09:43:47 - INFO - __main__ - Global step 1200 Train loss 3.79 ACC 0.0 on epoch=599
06/24/2022 09:43:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.72 on epoch=604
06/24/2022 09:43:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.69 on epoch=609
06/24/2022 09:43:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.55 on epoch=614
06/24/2022 09:43:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.59 on epoch=619
06/24/2022 09:43:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.48 on epoch=624
06/24/2022 09:44:00 - INFO - __main__ - Global step 1250 Train loss 3.60 ACC 0.0 on epoch=624
06/24/2022 09:44:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.56 on epoch=629
06/24/2022 09:44:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.24 on epoch=634
06/24/2022 09:44:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.60 on epoch=639
06/24/2022 09:44:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.51 on epoch=644
06/24/2022 09:44:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.46 on epoch=649
06/24/2022 09:44:13 - INFO - __main__ - Global step 1300 Train loss 3.47 ACC 0.0 on epoch=649
06/24/2022 09:44:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.34 on epoch=654
06/24/2022 09:44:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.31 on epoch=659
06/24/2022 09:44:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 3.20 on epoch=664
06/24/2022 09:44:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 3.25 on epoch=669
06/24/2022 09:44:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 3.29 on epoch=674
06/24/2022 09:44:30 - INFO - __main__ - Global step 1350 Train loss 3.28 ACC 0.0 on epoch=674
06/24/2022 09:44:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 3.22 on epoch=679
06/24/2022 09:44:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 3.19 on epoch=684
06/24/2022 09:44:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 3.19 on epoch=689
06/24/2022 09:44:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 3.29 on epoch=694
06/24/2022 09:44:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 3.10 on epoch=699
06/24/2022 09:44:41 - INFO - __main__ - Global step 1400 Train loss 3.20 ACC 0.0 on epoch=699
06/24/2022 09:44:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 3.09 on epoch=704
06/24/2022 09:44:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 3.00 on epoch=709
06/24/2022 09:44:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.97 on epoch=714
06/24/2022 09:44:46 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.91 on epoch=719
06/24/2022 09:44:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.75 on epoch=724
06/24/2022 09:44:53 - INFO - __main__ - Global step 1450 Train loss 2.94 ACC 0.0 on epoch=724
06/24/2022 09:44:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.83 on epoch=729
06/24/2022 09:44:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.78 on epoch=734
06/24/2022 09:44:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.78 on epoch=739
06/24/2022 09:44:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.68 on epoch=744
06/24/2022 09:44:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.65 on epoch=749
06/24/2022 09:45:10 - INFO - __main__ - Global step 1500 Train loss 2.75 ACC 0.0 on epoch=749
06/24/2022 09:45:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.63 on epoch=754
06/24/2022 09:45:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.72 on epoch=759
06/24/2022 09:45:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 2.75 on epoch=764
06/24/2022 09:45:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 2.73 on epoch=769
06/24/2022 09:45:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 2.52 on epoch=774
06/24/2022 09:45:27 - INFO - __main__ - Global step 1550 Train loss 2.67 ACC 0.03125 on epoch=774
06/24/2022 09:45:27 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=774, global_step=1550
06/24/2022 09:45:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 2.64 on epoch=779
06/24/2022 09:45:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 2.74 on epoch=784
06/24/2022 09:45:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 2.51 on epoch=789
06/24/2022 09:45:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 2.50 on epoch=794
06/24/2022 09:45:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 2.52 on epoch=799
06/24/2022 09:45:40 - INFO - __main__ - Global step 1600 Train loss 2.58 ACC 0.0625 on epoch=799
06/24/2022 09:45:40 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.0625 on epoch=799, global_step=1600
06/24/2022 09:45:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 2.45 on epoch=804
06/24/2022 09:45:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.43 on epoch=809
06/24/2022 09:45:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 2.48 on epoch=814
06/24/2022 09:45:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 2.63 on epoch=819
06/24/2022 09:45:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 2.67 on epoch=824
06/24/2022 09:45:57 - INFO - __main__ - Global step 1650 Train loss 2.53 ACC 0.3125 on epoch=824
06/24/2022 09:45:57 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.3125 on epoch=824, global_step=1650
06/24/2022 09:45:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 2.56 on epoch=829
06/24/2022 09:45:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 2.69 on epoch=834
06/24/2022 09:46:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 2.39 on epoch=839
06/24/2022 09:46:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 2.47 on epoch=844
06/24/2022 09:46:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 2.22 on epoch=849
06/24/2022 09:46:04 - INFO - __main__ - Global step 1700 Train loss 2.47 ACC 0.4375 on epoch=849
06/24/2022 09:46:04 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.4375 on epoch=849, global_step=1700
06/24/2022 09:46:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 2.39 on epoch=854
06/24/2022 09:46:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 2.35 on epoch=859
06/24/2022 09:46:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 2.11 on epoch=864
06/24/2022 09:46:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 2.13 on epoch=869
06/24/2022 09:46:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 2.19 on epoch=874
06/24/2022 09:46:21 - INFO - __main__ - Global step 1750 Train loss 2.23 ACC 0.46875 on epoch=874
06/24/2022 09:46:21 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.46875 on epoch=874, global_step=1750
06/24/2022 09:46:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 2.17 on epoch=879
06/24/2022 09:46:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 2.12 on epoch=884
06/24/2022 09:46:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 2.04 on epoch=889
06/24/2022 09:46:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 2.10 on epoch=894
06/24/2022 09:46:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 2.20 on epoch=899
06/24/2022 09:46:28 - INFO - __main__ - Global step 1800 Train loss 2.13 ACC 0.4375 on epoch=899
06/24/2022 09:46:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 2.14 on epoch=904
06/24/2022 09:46:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 2.20 on epoch=909
06/24/2022 09:46:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.93 on epoch=914
06/24/2022 09:46:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 2.04 on epoch=919
06/24/2022 09:46:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.98 on epoch=924
06/24/2022 09:46:42 - INFO - __main__ - Global step 1850 Train loss 2.06 ACC 0.46875 on epoch=924
06/24/2022 09:46:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 2.03 on epoch=929
06/24/2022 09:46:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.97 on epoch=934
06/24/2022 09:46:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.94 on epoch=939
06/24/2022 09:46:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.89 on epoch=944
06/24/2022 09:46:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 2.08 on epoch=949
06/24/2022 09:46:51 - INFO - __main__ - Global step 1900 Train loss 1.98 ACC 0.40625 on epoch=949
06/24/2022 09:46:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.96 on epoch=954
06/24/2022 09:46:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.83 on epoch=959
06/24/2022 09:46:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.83 on epoch=964
06/24/2022 09:46:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 2.00 on epoch=969
06/24/2022 09:46:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.90 on epoch=974
06/24/2022 09:47:08 - INFO - __main__ - Global step 1950 Train loss 1.91 ACC 0.46875 on epoch=974
06/24/2022 09:47:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.64 on epoch=979
06/24/2022 09:47:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.68 on epoch=984
06/24/2022 09:47:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.71 on epoch=989
06/24/2022 09:47:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.67 on epoch=994
06/24/2022 09:47:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.74 on epoch=999
06/24/2022 09:47:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:47:16 - INFO - __main__ - Printing 3 examples
06/24/2022 09:47:16 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 09:47:16 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:47:16 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 09:47:16 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:47:16 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 09:47:16 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:47:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 09:47:16 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:47:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:47:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:47:16 - INFO - __main__ - Printing 3 examples
06/24/2022 09:47:16 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 09:47:16 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:47:16 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 09:47:16 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:47:16 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 09:47:16 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:47:16 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:47:16 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:47:16 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:47:20 - INFO - __main__ - Global step 2000 Train loss 1.69 ACC 0.5 on epoch=999
06/24/2022 09:47:20 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=999, global_step=2000
06/24/2022 09:47:20 - INFO - __main__ - save last model!
06/24/2022 09:47:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 09:47:20 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 09:47:20 - INFO - __main__ - Printing 3 examples
06/24/2022 09:47:20 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 09:47:20 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:47:20 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 09:47:20 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:47:20 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 09:47:20 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:47:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:47:21 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:47:21 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:47:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:47:21 - INFO - __main__ - Starting training!
06/24/2022 09:47:21 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 09:49:26 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_42_0.2_8_predictions.txt
06/24/2022 09:49:26 - INFO - __main__ - ACC on test data: 0.4770
06/24/2022 09:49:26 - INFO - __main__ - prefix=medical_questions_pairs_16_42, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.47704918032786886
06/24/2022 09:49:26 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_87, lr=0.5, bsz=8 ...
06/24/2022 09:49:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:49:27 - INFO - __main__ - Printing 3 examples
06/24/2022 09:49:27 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 09:49:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:49:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 09:49:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:49:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 09:49:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:49:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 09:49:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:49:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:49:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:49:27 - INFO - __main__ - Printing 3 examples
06/24/2022 09:49:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 09:49:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:49:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 09:49:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:49:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 09:49:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:49:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:49:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:49:27 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:49:32 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:49:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:49:33 - INFO - __main__ - Starting training!
06/24/2022 09:49:34 - INFO - __main__ - Step 10 Global step 10 Train loss 8.34 on epoch=4
06/24/2022 09:49:35 - INFO - __main__ - Step 20 Global step 20 Train loss 8.31 on epoch=9
06/24/2022 09:49:37 - INFO - __main__ - Step 30 Global step 30 Train loss 8.18 on epoch=14
06/24/2022 09:49:38 - INFO - __main__ - Step 40 Global step 40 Train loss 8.35 on epoch=19
06/24/2022 09:49:39 - INFO - __main__ - Step 50 Global step 50 Train loss 8.24 on epoch=24
06/24/2022 09:49:41 - INFO - __main__ - Global step 50 Train loss 8.29 ACC 0.0 on epoch=24
06/24/2022 09:49:41 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 09:49:42 - INFO - __main__ - Step 60 Global step 60 Train loss 8.20 on epoch=29
06/24/2022 09:49:44 - INFO - __main__ - Step 70 Global step 70 Train loss 8.05 on epoch=34
06/24/2022 09:49:45 - INFO - __main__ - Step 80 Global step 80 Train loss 8.16 on epoch=39
06/24/2022 09:49:46 - INFO - __main__ - Step 90 Global step 90 Train loss 8.05 on epoch=44
06/24/2022 09:49:47 - INFO - __main__ - Step 100 Global step 100 Train loss 8.04 on epoch=49
06/24/2022 09:49:54 - INFO - __main__ - Global step 100 Train loss 8.10 ACC 0.0 on epoch=49
06/24/2022 09:49:55 - INFO - __main__ - Step 110 Global step 110 Train loss 8.01 on epoch=54
06/24/2022 09:49:56 - INFO - __main__ - Step 120 Global step 120 Train loss 7.89 on epoch=59
06/24/2022 09:49:58 - INFO - __main__ - Step 130 Global step 130 Train loss 7.74 on epoch=64
06/24/2022 09:49:59 - INFO - __main__ - Step 140 Global step 140 Train loss 7.41 on epoch=69
06/24/2022 09:50:00 - INFO - __main__ - Step 150 Global step 150 Train loss 7.27 on epoch=74
06/24/2022 09:50:08 - INFO - __main__ - Global step 150 Train loss 7.66 ACC 0.0 on epoch=74
06/24/2022 09:50:09 - INFO - __main__ - Step 160 Global step 160 Train loss 6.97 on epoch=79
06/24/2022 09:50:11 - INFO - __main__ - Step 170 Global step 170 Train loss 6.77 on epoch=84
06/24/2022 09:50:12 - INFO - __main__ - Step 180 Global step 180 Train loss 6.62 on epoch=89
06/24/2022 09:50:13 - INFO - __main__ - Step 190 Global step 190 Train loss 6.56 on epoch=94
06/24/2022 09:50:14 - INFO - __main__ - Step 200 Global step 200 Train loss 6.41 on epoch=99
06/24/2022 09:50:17 - INFO - __main__ - Global step 200 Train loss 6.67 ACC 0.0 on epoch=99
06/24/2022 09:50:18 - INFO - __main__ - Step 210 Global step 210 Train loss 6.26 on epoch=104
06/24/2022 09:50:19 - INFO - __main__ - Step 220 Global step 220 Train loss 6.06 on epoch=109
06/24/2022 09:50:20 - INFO - __main__ - Step 230 Global step 230 Train loss 5.78 on epoch=114
06/24/2022 09:50:22 - INFO - __main__ - Step 240 Global step 240 Train loss 5.58 on epoch=119
06/24/2022 09:50:23 - INFO - __main__ - Step 250 Global step 250 Train loss 5.35 on epoch=124
06/24/2022 09:50:25 - INFO - __main__ - Global step 250 Train loss 5.81 ACC 0.0 on epoch=124
06/24/2022 09:50:27 - INFO - __main__ - Step 260 Global step 260 Train loss 5.16 on epoch=129
06/24/2022 09:50:28 - INFO - __main__ - Step 270 Global step 270 Train loss 4.79 on epoch=134
06/24/2022 09:50:29 - INFO - __main__ - Step 280 Global step 280 Train loss 4.29 on epoch=139
06/24/2022 09:50:30 - INFO - __main__ - Step 290 Global step 290 Train loss 3.87 on epoch=144
06/24/2022 09:50:32 - INFO - __main__ - Step 300 Global step 300 Train loss 3.70 on epoch=149
06/24/2022 09:50:33 - INFO - __main__ - Global step 300 Train loss 4.36 ACC 0.0 on epoch=149
06/24/2022 09:50:34 - INFO - __main__ - Step 310 Global step 310 Train loss 3.55 on epoch=154
06/24/2022 09:50:36 - INFO - __main__ - Step 320 Global step 320 Train loss 3.40 on epoch=159
06/24/2022 09:50:37 - INFO - __main__ - Step 330 Global step 330 Train loss 3.32 on epoch=164
06/24/2022 09:50:38 - INFO - __main__ - Step 340 Global step 340 Train loss 2.96 on epoch=169
06/24/2022 09:50:39 - INFO - __main__ - Step 350 Global step 350 Train loss 3.02 on epoch=174
06/24/2022 09:50:42 - INFO - __main__ - Global step 350 Train loss 3.25 ACC 0.0 on epoch=174
06/24/2022 09:50:43 - INFO - __main__ - Step 360 Global step 360 Train loss 2.97 on epoch=179
06/24/2022 09:50:44 - INFO - __main__ - Step 370 Global step 370 Train loss 2.76 on epoch=184
06/24/2022 09:50:46 - INFO - __main__ - Step 380 Global step 380 Train loss 2.65 on epoch=189
06/24/2022 09:50:47 - INFO - __main__ - Step 390 Global step 390 Train loss 2.66 on epoch=194
06/24/2022 09:50:48 - INFO - __main__ - Step 400 Global step 400 Train loss 2.43 on epoch=199
06/24/2022 09:50:59 - INFO - __main__ - Global step 400 Train loss 2.70 ACC 0.375 on epoch=199
06/24/2022 09:50:59 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.375 on epoch=199, global_step=400
06/24/2022 09:51:00 - INFO - __main__ - Step 410 Global step 410 Train loss 2.52 on epoch=204
06/24/2022 09:51:01 - INFO - __main__ - Step 420 Global step 420 Train loss 2.39 on epoch=209
06/24/2022 09:51:03 - INFO - __main__ - Step 430 Global step 430 Train loss 2.12 on epoch=214
06/24/2022 09:51:04 - INFO - __main__ - Step 440 Global step 440 Train loss 2.09 on epoch=219
06/24/2022 09:51:05 - INFO - __main__ - Step 450 Global step 450 Train loss 2.19 on epoch=224
06/24/2022 09:51:06 - INFO - __main__ - Global step 450 Train loss 2.26 ACC 0.5 on epoch=224
06/24/2022 09:51:06 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.5 on epoch=224, global_step=450
06/24/2022 09:51:08 - INFO - __main__ - Step 460 Global step 460 Train loss 1.96 on epoch=229
06/24/2022 09:51:09 - INFO - __main__ - Step 470 Global step 470 Train loss 2.01 on epoch=234
06/24/2022 09:51:10 - INFO - __main__ - Step 480 Global step 480 Train loss 1.89 on epoch=239
06/24/2022 09:51:11 - INFO - __main__ - Step 490 Global step 490 Train loss 1.82 on epoch=244
06/24/2022 09:51:13 - INFO - __main__ - Step 500 Global step 500 Train loss 1.64 on epoch=249
06/24/2022 09:51:20 - INFO - __main__ - Global step 500 Train loss 1.86 ACC 0.5 on epoch=249
06/24/2022 09:51:22 - INFO - __main__ - Step 510 Global step 510 Train loss 1.56 on epoch=254
06/24/2022 09:51:23 - INFO - __main__ - Step 520 Global step 520 Train loss 1.72 on epoch=259
06/24/2022 09:51:24 - INFO - __main__ - Step 530 Global step 530 Train loss 1.58 on epoch=264
06/24/2022 09:51:25 - INFO - __main__ - Step 540 Global step 540 Train loss 1.37 on epoch=269
06/24/2022 09:51:27 - INFO - __main__ - Step 550 Global step 550 Train loss 1.34 on epoch=274
06/24/2022 09:51:36 - INFO - __main__ - Global step 550 Train loss 1.51 ACC 0.5 on epoch=274
06/24/2022 09:51:37 - INFO - __main__ - Step 560 Global step 560 Train loss 1.45 on epoch=279
06/24/2022 09:51:39 - INFO - __main__ - Step 570 Global step 570 Train loss 1.25 on epoch=284
06/24/2022 09:51:40 - INFO - __main__ - Step 580 Global step 580 Train loss 1.33 on epoch=289
06/24/2022 09:51:41 - INFO - __main__ - Step 590 Global step 590 Train loss 1.12 on epoch=294
06/24/2022 09:51:42 - INFO - __main__ - Step 600 Global step 600 Train loss 1.15 on epoch=299
06/24/2022 09:51:49 - INFO - __main__ - Global step 600 Train loss 1.26 ACC 0.5 on epoch=299
06/24/2022 09:51:50 - INFO - __main__ - Step 610 Global step 610 Train loss 1.30 on epoch=304
06/24/2022 09:51:52 - INFO - __main__ - Step 620 Global step 620 Train loss 1.19 on epoch=309
06/24/2022 09:51:53 - INFO - __main__ - Step 630 Global step 630 Train loss 1.10 on epoch=314
06/24/2022 09:51:54 - INFO - __main__ - Step 640 Global step 640 Train loss 1.11 on epoch=319
06/24/2022 09:51:55 - INFO - __main__ - Step 650 Global step 650 Train loss 1.08 on epoch=324
06/24/2022 09:51:57 - INFO - __main__ - Global step 650 Train loss 1.16 ACC 0.5 on epoch=324
06/24/2022 09:51:58 - INFO - __main__ - Step 660 Global step 660 Train loss 1.04 on epoch=329
06/24/2022 09:51:59 - INFO - __main__ - Step 670 Global step 670 Train loss 1.08 on epoch=334
06/24/2022 09:52:00 - INFO - __main__ - Step 680 Global step 680 Train loss 1.00 on epoch=339
06/24/2022 09:52:02 - INFO - __main__ - Step 690 Global step 690 Train loss 1.03 on epoch=344
06/24/2022 09:52:03 - INFO - __main__ - Step 700 Global step 700 Train loss 1.01 on epoch=349
06/24/2022 09:52:03 - INFO - __main__ - Global step 700 Train loss 1.03 ACC 0.46875 on epoch=349
06/24/2022 09:52:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.90 on epoch=354
06/24/2022 09:52:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.97 on epoch=359
06/24/2022 09:52:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.88 on epoch=364
06/24/2022 09:52:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.88 on epoch=369
06/24/2022 09:52:10 - INFO - __main__ - Step 750 Global step 750 Train loss 1.02 on epoch=374
06/24/2022 09:52:10 - INFO - __main__ - Global step 750 Train loss 0.93 ACC 0.46875 on epoch=374
06/24/2022 09:52:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.92 on epoch=379
06/24/2022 09:52:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.82 on epoch=384
06/24/2022 09:52:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.69 on epoch=389
06/24/2022 09:52:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.67 on epoch=394
06/24/2022 09:52:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.81 on epoch=399
06/24/2022 09:52:17 - INFO - __main__ - Global step 800 Train loss 0.78 ACC 0.5 on epoch=399
06/24/2022 09:52:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.79 on epoch=404
06/24/2022 09:52:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.84 on epoch=409
06/24/2022 09:52:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.81 on epoch=414
06/24/2022 09:52:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.74 on epoch=419
06/24/2022 09:52:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.74 on epoch=424
06/24/2022 09:52:24 - INFO - __main__ - Global step 850 Train loss 0.78 ACC 0.5 on epoch=424
06/24/2022 09:52:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.79 on epoch=429
06/24/2022 09:52:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.59 on epoch=434
06/24/2022 09:52:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.80 on epoch=439
06/24/2022 09:52:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.68 on epoch=444
06/24/2022 09:52:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.68 on epoch=449
06/24/2022 09:52:31 - INFO - __main__ - Global step 900 Train loss 0.71 ACC 0.5625 on epoch=449
06/24/2022 09:52:31 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=449, global_step=900
06/24/2022 09:52:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.71 on epoch=454
06/24/2022 09:52:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.62 on epoch=459
06/24/2022 09:52:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.60 on epoch=464
06/24/2022 09:52:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.72 on epoch=469
06/24/2022 09:52:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.68 on epoch=474
06/24/2022 09:52:38 - INFO - __main__ - Global step 950 Train loss 0.67 ACC 0.53125 on epoch=474
06/24/2022 09:52:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.66 on epoch=479
06/24/2022 09:52:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.56 on epoch=484
06/24/2022 09:52:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.57 on epoch=489
06/24/2022 09:52:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.68 on epoch=494
06/24/2022 09:52:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.69 on epoch=499
06/24/2022 09:52:44 - INFO - __main__ - Global step 1000 Train loss 0.63 ACC 0.5625 on epoch=499
06/24/2022 09:52:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.60 on epoch=504
06/24/2022 09:52:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.66 on epoch=509
06/24/2022 09:52:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.52 on epoch=514
06/24/2022 09:52:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.64 on epoch=519
06/24/2022 09:52:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.71 on epoch=524
06/24/2022 09:52:51 - INFO - __main__ - Global step 1050 Train loss 0.63 ACC 0.59375 on epoch=524
06/24/2022 09:52:51 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=524, global_step=1050
06/24/2022 09:52:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.59 on epoch=529
06/24/2022 09:52:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.65 on epoch=534
06/24/2022 09:52:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.65 on epoch=539
06/24/2022 09:52:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.65 on epoch=544
06/24/2022 09:52:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.60 on epoch=549
06/24/2022 09:52:58 - INFO - __main__ - Global step 1100 Train loss 0.63 ACC 0.53125 on epoch=549
06/24/2022 09:52:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.57 on epoch=554
06/24/2022 09:53:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.66 on epoch=559
06/24/2022 09:53:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.59 on epoch=564
06/24/2022 09:53:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.50 on epoch=569
06/24/2022 09:53:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.64 on epoch=574
06/24/2022 09:53:04 - INFO - __main__ - Global step 1150 Train loss 0.59 ACC 0.46875 on epoch=574
06/24/2022 09:53:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.69 on epoch=579
06/24/2022 09:53:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.71 on epoch=584
06/24/2022 09:53:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.51 on epoch=589
06/24/2022 09:53:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.65 on epoch=594
06/24/2022 09:53:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=599
06/24/2022 09:53:11 - INFO - __main__ - Global step 1200 Train loss 0.60 ACC 0.5 on epoch=599
06/24/2022 09:53:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.60 on epoch=604
06/24/2022 09:53:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.58 on epoch=609
06/24/2022 09:53:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.57 on epoch=614
06/24/2022 09:53:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=619
06/24/2022 09:53:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.58 on epoch=624
06/24/2022 09:53:18 - INFO - __main__ - Global step 1250 Train loss 0.55 ACC 0.5 on epoch=624
06/24/2022 09:53:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.56 on epoch=629
06/24/2022 09:53:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.63 on epoch=634
06/24/2022 09:53:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.54 on epoch=639
06/24/2022 09:53:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.59 on epoch=644
06/24/2022 09:53:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=649
06/24/2022 09:53:24 - INFO - __main__ - Global step 1300 Train loss 0.56 ACC 0.5 on epoch=649
06/24/2022 09:53:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.54 on epoch=654
06/24/2022 09:53:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=659
06/24/2022 09:53:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.53 on epoch=664
06/24/2022 09:53:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.60 on epoch=669
06/24/2022 09:53:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=674
06/24/2022 09:53:31 - INFO - __main__ - Global step 1350 Train loss 0.53 ACC 0.59375 on epoch=674
06/24/2022 09:53:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.58 on epoch=679
06/24/2022 09:53:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.61 on epoch=684
06/24/2022 09:53:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.59 on epoch=689
06/24/2022 09:53:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.57 on epoch=694
06/24/2022 09:53:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.55 on epoch=699
06/24/2022 09:53:38 - INFO - __main__ - Global step 1400 Train loss 0.58 ACC 0.46875 on epoch=699
06/24/2022 09:53:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.58 on epoch=704
06/24/2022 09:53:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=709
06/24/2022 09:53:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.51 on epoch=714
06/24/2022 09:53:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.62 on epoch=719
06/24/2022 09:53:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.60 on epoch=724
06/24/2022 09:53:44 - INFO - __main__ - Global step 1450 Train loss 0.56 ACC 0.5 on epoch=724
06/24/2022 09:53:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=729
06/24/2022 09:53:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.57 on epoch=734
06/24/2022 09:53:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.61 on epoch=739
06/24/2022 09:53:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.56 on epoch=744
06/24/2022 09:53:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.58 on epoch=749
06/24/2022 09:53:51 - INFO - __main__ - Global step 1500 Train loss 0.57 ACC 0.5 on epoch=749
06/24/2022 09:53:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.52 on epoch=754
06/24/2022 09:53:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.58 on epoch=759
06/24/2022 09:53:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=764
06/24/2022 09:53:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.57 on epoch=769
06/24/2022 09:53:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.50 on epoch=774
06/24/2022 09:53:57 - INFO - __main__ - Global step 1550 Train loss 0.52 ACC 0.5 on epoch=774
06/24/2022 09:53:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=779
06/24/2022 09:54:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=784
06/24/2022 09:54:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.65 on epoch=789
06/24/2022 09:54:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=794
06/24/2022 09:54:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.54 on epoch=799
06/24/2022 09:54:04 - INFO - __main__ - Global step 1600 Train loss 0.53 ACC 0.5 on epoch=799
06/24/2022 09:54:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=804
06/24/2022 09:54:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.55 on epoch=809
06/24/2022 09:54:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.61 on epoch=814
06/24/2022 09:54:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.49 on epoch=819
06/24/2022 09:54:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=824
06/24/2022 09:54:11 - INFO - __main__ - Global step 1650 Train loss 0.51 ACC 0.5 on epoch=824
06/24/2022 09:54:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.55 on epoch=829
06/24/2022 09:54:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.58 on epoch=834
06/24/2022 09:54:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.54 on epoch=839
06/24/2022 09:54:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.52 on epoch=844
06/24/2022 09:54:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.50 on epoch=849
06/24/2022 09:54:17 - INFO - __main__ - Global step 1700 Train loss 0.54 ACC 0.5 on epoch=849
06/24/2022 09:54:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.50 on epoch=854
06/24/2022 09:54:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.56 on epoch=859
06/24/2022 09:54:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.58 on epoch=864
06/24/2022 09:54:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.46 on epoch=869
06/24/2022 09:54:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.47 on epoch=874
06/24/2022 09:54:24 - INFO - __main__ - Global step 1750 Train loss 0.51 ACC 0.5 on epoch=874
06/24/2022 09:54:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=879
06/24/2022 09:54:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.55 on epoch=884
06/24/2022 09:54:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=889
06/24/2022 09:54:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.49 on epoch=894
06/24/2022 09:54:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.49 on epoch=899
06/24/2022 09:54:31 - INFO - __main__ - Global step 1800 Train loss 0.49 ACC 0.5 on epoch=899
06/24/2022 09:54:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.49 on epoch=904
06/24/2022 09:54:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=909
06/24/2022 09:54:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=914
06/24/2022 09:54:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.50 on epoch=919
06/24/2022 09:54:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.59 on epoch=924
06/24/2022 09:54:37 - INFO - __main__ - Global step 1850 Train loss 0.50 ACC 0.5 on epoch=924
06/24/2022 09:54:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.58 on epoch=929
06/24/2022 09:54:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.48 on epoch=934
06/24/2022 09:54:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=939
06/24/2022 09:54:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.47 on epoch=944
06/24/2022 09:54:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.50 on epoch=949
06/24/2022 09:54:44 - INFO - __main__ - Global step 1900 Train loss 0.50 ACC 0.5 on epoch=949
06/24/2022 09:54:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.51 on epoch=954
06/24/2022 09:54:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.50 on epoch=959
06/24/2022 09:54:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=964
06/24/2022 09:54:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.48 on epoch=969
06/24/2022 09:54:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.56 on epoch=974
06/24/2022 09:54:51 - INFO - __main__ - Global step 1950 Train loss 0.50 ACC 0.59375 on epoch=974
06/24/2022 09:54:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.50 on epoch=979
06/24/2022 09:54:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=984
06/24/2022 09:54:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=989
06/24/2022 09:54:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=994
06/24/2022 09:54:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=999
06/24/2022 09:54:57 - INFO - __main__ - Global step 2000 Train loss 0.45 ACC 0.5 on epoch=999
06/24/2022 09:54:57 - INFO - __main__ - save last model!
06/24/2022 09:54:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 09:54:57 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 09:54:57 - INFO - __main__ - Printing 3 examples
06/24/2022 09:54:57 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 09:54:57 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:54:57 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 09:54:57 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:54:57 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 09:54:57 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:54:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:54:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:54:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:54:58 - INFO - __main__ - Printing 3 examples
06/24/2022 09:54:58 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 09:54:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:54:58 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 09:54:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:54:58 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 09:54:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:54:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 09:54:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:54:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:54:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:54:58 - INFO - __main__ - Printing 3 examples
06/24/2022 09:54:58 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 09:54:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:54:58 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 09:54:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:54:58 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 09:54:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:54:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:54:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:54:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:54:58 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 09:55:04 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:55:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:55:04 - INFO - __main__ - Starting training!
06/24/2022 09:55:05 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_87_0.5_8_predictions.txt
06/24/2022 09:55:05 - INFO - __main__ - ACC on test data: 0.5197
06/24/2022 09:55:06 - INFO - __main__ - prefix=medical_questions_pairs_16_87, lr=0.5, bsz=8, dev_performance=0.59375, test_performance=0.519672131147541
06/24/2022 09:55:06 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_87, lr=0.4, bsz=8 ...
06/24/2022 09:55:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:55:07 - INFO - __main__ - Printing 3 examples
06/24/2022 09:55:07 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 09:55:07 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:55:07 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 09:55:07 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:55:07 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 09:55:07 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:55:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 09:55:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:55:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 09:55:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 09:55:07 - INFO - __main__ - Printing 3 examples
06/24/2022 09:55:07 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 09:55:07 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:55:07 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 09:55:07 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:55:07 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 09:55:07 - INFO - __main__ - ['Dissimilar']
06/24/2022 09:55:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 09:55:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 09:55:07 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 09:55:12 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 09:55:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 09:55:12 - INFO - __main__ - Starting training!
06/24/2022 09:55:14 - INFO - __main__ - Step 10 Global step 10 Train loss 8.34 on epoch=4
06/24/2022 09:55:15 - INFO - __main__ - Step 20 Global step 20 Train loss 8.43 on epoch=9
06/24/2022 09:55:16 - INFO - __main__ - Step 30 Global step 30 Train loss 8.25 on epoch=14
06/24/2022 09:55:17 - INFO - __main__ - Step 40 Global step 40 Train loss 8.31 on epoch=19
06/24/2022 09:55:19 - INFO - __main__ - Step 50 Global step 50 Train loss 8.29 on epoch=24
06/24/2022 09:55:21 - INFO - __main__ - Global step 50 Train loss 8.32 ACC 0.0 on epoch=24
06/24/2022 09:55:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 09:55:22 - INFO - __main__ - Step 60 Global step 60 Train loss 8.27 on epoch=29
06/24/2022 09:55:23 - INFO - __main__ - Step 70 Global step 70 Train loss 8.22 on epoch=34
06/24/2022 09:55:24 - INFO - __main__ - Step 80 Global step 80 Train loss 8.18 on epoch=39
06/24/2022 09:55:25 - INFO - __main__ - Step 90 Global step 90 Train loss 8.03 on epoch=44
06/24/2022 09:55:27 - INFO - __main__ - Step 100 Global step 100 Train loss 8.13 on epoch=49
06/24/2022 09:55:28 - INFO - __main__ - Global step 100 Train loss 8.16 ACC 0.0 on epoch=49
06/24/2022 09:55:29 - INFO - __main__ - Step 110 Global step 110 Train loss 8.06 on epoch=54
06/24/2022 09:55:30 - INFO - __main__ - Step 120 Global step 120 Train loss 8.10 on epoch=59
06/24/2022 09:55:32 - INFO - __main__ - Step 130 Global step 130 Train loss 7.93 on epoch=64
06/24/2022 09:55:33 - INFO - __main__ - Step 140 Global step 140 Train loss 7.82 on epoch=69
06/24/2022 09:55:34 - INFO - __main__ - Step 150 Global step 150 Train loss 7.89 on epoch=74
06/24/2022 09:55:36 - INFO - __main__ - Global step 150 Train loss 7.96 ACC 0.0 on epoch=74
06/24/2022 09:55:37 - INFO - __main__ - Step 160 Global step 160 Train loss 7.75 on epoch=79
06/24/2022 09:55:38 - INFO - __main__ - Step 170 Global step 170 Train loss 7.72 on epoch=84
06/24/2022 09:55:39 - INFO - __main__ - Step 180 Global step 180 Train loss 7.65 on epoch=89
06/24/2022 09:55:40 - INFO - __main__ - Step 190 Global step 190 Train loss 7.65 on epoch=94
06/24/2022 09:55:42 - INFO - __main__ - Step 200 Global step 200 Train loss 7.54 on epoch=99
06/24/2022 09:55:45 - INFO - __main__ - Global step 200 Train loss 7.66 ACC 0.0 on epoch=99
06/24/2022 09:55:46 - INFO - __main__ - Step 210 Global step 210 Train loss 7.42 on epoch=104
06/24/2022 09:55:47 - INFO - __main__ - Step 220 Global step 220 Train loss 7.34 on epoch=109
06/24/2022 09:55:48 - INFO - __main__ - Step 230 Global step 230 Train loss 7.31 on epoch=114
06/24/2022 09:55:50 - INFO - __main__ - Step 240 Global step 240 Train loss 7.17 on epoch=119
06/24/2022 09:55:51 - INFO - __main__ - Step 250 Global step 250 Train loss 7.05 on epoch=124
06/24/2022 09:55:57 - INFO - __main__ - Global step 250 Train loss 7.26 ACC 0.0 on epoch=124
06/24/2022 09:55:58 - INFO - __main__ - Step 260 Global step 260 Train loss 6.92 on epoch=129
06/24/2022 09:55:59 - INFO - __main__ - Step 270 Global step 270 Train loss 6.68 on epoch=134
06/24/2022 09:56:01 - INFO - __main__ - Step 280 Global step 280 Train loss 6.75 on epoch=139
06/24/2022 09:56:02 - INFO - __main__ - Step 290 Global step 290 Train loss 6.58 on epoch=144
06/24/2022 09:56:03 - INFO - __main__ - Step 300 Global step 300 Train loss 6.40 on epoch=149
06/24/2022 09:56:06 - INFO - __main__ - Global step 300 Train loss 6.66 ACC 0.0 on epoch=149
06/24/2022 09:56:07 - INFO - __main__ - Step 310 Global step 310 Train loss 6.28 on epoch=154
06/24/2022 09:56:09 - INFO - __main__ - Step 320 Global step 320 Train loss 6.17 on epoch=159
06/24/2022 09:56:10 - INFO - __main__ - Step 330 Global step 330 Train loss 6.17 on epoch=164
06/24/2022 09:56:11 - INFO - __main__ - Step 340 Global step 340 Train loss 6.02 on epoch=169
06/24/2022 09:56:12 - INFO - __main__ - Step 350 Global step 350 Train loss 5.85 on epoch=174
06/24/2022 09:56:22 - INFO - __main__ - Global step 350 Train loss 6.10 ACC 0.0 on epoch=174
06/24/2022 09:56:23 - INFO - __main__ - Step 360 Global step 360 Train loss 5.78 on epoch=179
06/24/2022 09:56:24 - INFO - __main__ - Step 370 Global step 370 Train loss 5.59 on epoch=184
06/24/2022 09:56:25 - INFO - __main__ - Step 380 Global step 380 Train loss 5.55 on epoch=189
06/24/2022 09:56:26 - INFO - __main__ - Step 390 Global step 390 Train loss 5.29 on epoch=194
06/24/2022 09:56:28 - INFO - __main__ - Step 400 Global step 400 Train loss 5.45 on epoch=199
06/24/2022 09:56:35 - INFO - __main__ - Global step 400 Train loss 5.53 ACC 0.0 on epoch=199
06/24/2022 09:56:36 - INFO - __main__ - Step 410 Global step 410 Train loss 5.35 on epoch=204
06/24/2022 09:56:37 - INFO - __main__ - Step 420 Global step 420 Train loss 5.23 on epoch=209
06/24/2022 09:56:39 - INFO - __main__ - Step 430 Global step 430 Train loss 5.12 on epoch=214
06/24/2022 09:56:40 - INFO - __main__ - Step 440 Global step 440 Train loss 4.90 on epoch=219
06/24/2022 09:56:41 - INFO - __main__ - Step 450 Global step 450 Train loss 4.73 on epoch=224
06/24/2022 09:56:43 - INFO - __main__ - Global step 450 Train loss 5.06 ACC 0.0 on epoch=224
06/24/2022 09:56:45 - INFO - __main__ - Step 460 Global step 460 Train loss 4.68 on epoch=229
06/24/2022 09:56:46 - INFO - __main__ - Step 470 Global step 470 Train loss 4.54 on epoch=234
06/24/2022 09:56:47 - INFO - __main__ - Step 480 Global step 480 Train loss 4.19 on epoch=239
06/24/2022 09:56:48 - INFO - __main__ - Step 490 Global step 490 Train loss 4.22 on epoch=244
06/24/2022 09:56:49 - INFO - __main__ - Step 500 Global step 500 Train loss 4.14 on epoch=249
06/24/2022 09:56:55 - INFO - __main__ - Global step 500 Train loss 4.35 ACC 0.25 on epoch=249
06/24/2022 09:56:55 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.25 on epoch=249, global_step=500
06/24/2022 09:56:57 - INFO - __main__ - Step 510 Global step 510 Train loss 3.96 on epoch=254
06/24/2022 09:56:58 - INFO - __main__ - Step 520 Global step 520 Train loss 3.91 on epoch=259
06/24/2022 09:56:59 - INFO - __main__ - Step 530 Global step 530 Train loss 3.65 on epoch=264
06/24/2022 09:57:00 - INFO - __main__ - Step 540 Global step 540 Train loss 3.42 on epoch=269
06/24/2022 09:57:01 - INFO - __main__ - Step 550 Global step 550 Train loss 3.35 on epoch=274
06/24/2022 09:57:12 - INFO - __main__ - Global step 550 Train loss 3.66 ACC 0.46875 on epoch=274
06/24/2022 09:57:12 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.46875 on epoch=274, global_step=550
06/24/2022 09:57:13 - INFO - __main__ - Step 560 Global step 560 Train loss 3.12 on epoch=279
06/24/2022 09:57:15 - INFO - __main__ - Step 570 Global step 570 Train loss 2.89 on epoch=284
06/24/2022 09:57:16 - INFO - __main__ - Step 580 Global step 580 Train loss 3.04 on epoch=289
06/24/2022 09:57:17 - INFO - __main__ - Step 590 Global step 590 Train loss 2.58 on epoch=294
06/24/2022 09:57:18 - INFO - __main__ - Step 600 Global step 600 Train loss 2.58 on epoch=299
06/24/2022 09:57:29 - INFO - __main__ - Global step 600 Train loss 2.84 ACC 0.46875 on epoch=299
06/24/2022 09:57:30 - INFO - __main__ - Step 610 Global step 610 Train loss 2.40 on epoch=304
06/24/2022 09:57:31 - INFO - __main__ - Step 620 Global step 620 Train loss 2.26 on epoch=309
06/24/2022 09:57:32 - INFO - __main__ - Step 630 Global step 630 Train loss 1.98 on epoch=314
06/24/2022 09:57:34 - INFO - __main__ - Step 640 Global step 640 Train loss 1.87 on epoch=319
06/24/2022 09:57:35 - INFO - __main__ - Step 650 Global step 650 Train loss 2.09 on epoch=324
06/24/2022 09:57:44 - INFO - __main__ - Global step 650 Train loss 2.12 ACC 0.5 on epoch=324
06/24/2022 09:57:44 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=324, global_step=650
06/24/2022 09:57:46 - INFO - __main__ - Step 660 Global step 660 Train loss 1.86 on epoch=329
06/24/2022 09:57:47 - INFO - __main__ - Step 670 Global step 670 Train loss 1.82 on epoch=334
06/24/2022 09:57:48 - INFO - __main__ - Step 680 Global step 680 Train loss 1.59 on epoch=339
06/24/2022 09:57:49 - INFO - __main__ - Step 690 Global step 690 Train loss 1.54 on epoch=344
06/24/2022 09:57:50 - INFO - __main__ - Step 700 Global step 700 Train loss 1.44 on epoch=349
06/24/2022 09:58:00 - INFO - __main__ - Global step 700 Train loss 1.65 ACC 0.5 on epoch=349
06/24/2022 09:58:01 - INFO - __main__ - Step 710 Global step 710 Train loss 1.39 on epoch=354
06/24/2022 09:58:02 - INFO - __main__ - Step 720 Global step 720 Train loss 1.54 on epoch=359
06/24/2022 09:58:04 - INFO - __main__ - Step 730 Global step 730 Train loss 1.38 on epoch=364
06/24/2022 09:58:05 - INFO - __main__ - Step 740 Global step 740 Train loss 1.34 on epoch=369
06/24/2022 09:58:06 - INFO - __main__ - Step 750 Global step 750 Train loss 1.51 on epoch=374
06/24/2022 09:58:12 - INFO - __main__ - Global step 750 Train loss 1.43 ACC 0.53125 on epoch=374
06/24/2022 09:58:12 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=374, global_step=750
06/24/2022 09:58:13 - INFO - __main__ - Step 760 Global step 760 Train loss 1.42 on epoch=379
06/24/2022 09:58:14 - INFO - __main__ - Step 770 Global step 770 Train loss 1.04 on epoch=384
06/24/2022 09:58:15 - INFO - __main__ - Step 780 Global step 780 Train loss 1.06 on epoch=389
06/24/2022 09:58:16 - INFO - __main__ - Step 790 Global step 790 Train loss 1.29 on epoch=394
06/24/2022 09:58:18 - INFO - __main__ - Step 800 Global step 800 Train loss 1.22 on epoch=399
06/24/2022 09:58:27 - INFO - __main__ - Global step 800 Train loss 1.21 ACC 0.5 on epoch=399
06/24/2022 09:58:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.90 on epoch=404
06/24/2022 09:58:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.99 on epoch=409
06/24/2022 09:58:30 - INFO - __main__ - Step 830 Global step 830 Train loss 1.13 on epoch=414
06/24/2022 09:58:32 - INFO - __main__ - Step 840 Global step 840 Train loss 1.03 on epoch=419
06/24/2022 09:58:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.94 on epoch=424
06/24/2022 09:58:42 - INFO - __main__ - Global step 850 Train loss 1.00 ACC 0.46875 on epoch=424
06/24/2022 09:58:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.90 on epoch=429
06/24/2022 09:58:45 - INFO - __main__ - Step 870 Global step 870 Train loss 1.03 on epoch=434
06/24/2022 09:58:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.79 on epoch=439
06/24/2022 09:58:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.78 on epoch=444
06/24/2022 09:58:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.85 on epoch=449
06/24/2022 09:58:53 - INFO - __main__ - Global step 900 Train loss 0.87 ACC 0.5625 on epoch=449
06/24/2022 09:58:53 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=449, global_step=900
06/24/2022 09:58:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.84 on epoch=454
06/24/2022 09:58:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.91 on epoch=459
06/24/2022 09:58:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.79 on epoch=464
06/24/2022 09:58:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.88 on epoch=469
06/24/2022 09:58:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.90 on epoch=474
06/24/2022 09:59:05 - INFO - __main__ - Global step 950 Train loss 0.86 ACC 0.5 on epoch=474
06/24/2022 09:59:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.97 on epoch=479
06/24/2022 09:59:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.77 on epoch=484
06/24/2022 09:59:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.81 on epoch=489
06/24/2022 09:59:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.83 on epoch=494
06/24/2022 09:59:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.94 on epoch=499
06/24/2022 09:59:15 - INFO - __main__ - Global step 1000 Train loss 0.86 ACC 0.53125 on epoch=499
06/24/2022 09:59:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.82 on epoch=504
06/24/2022 09:59:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.68 on epoch=509
06/24/2022 09:59:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.67 on epoch=514
06/24/2022 09:59:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.70 on epoch=519
06/24/2022 09:59:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.60 on epoch=524
06/24/2022 09:59:22 - INFO - __main__ - Global step 1050 Train loss 0.69 ACC 0.5 on epoch=524
06/24/2022 09:59:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.69 on epoch=529
06/24/2022 09:59:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.75 on epoch=534
06/24/2022 09:59:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.76 on epoch=539
06/24/2022 09:59:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.59 on epoch=544
06/24/2022 09:59:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.82 on epoch=549
06/24/2022 09:59:30 - INFO - __main__ - Global step 1100 Train loss 0.72 ACC 0.5 on epoch=549
06/24/2022 09:59:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.62 on epoch=554
06/24/2022 09:59:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.58 on epoch=559
06/24/2022 09:59:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.65 on epoch=564
06/24/2022 09:59:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.79 on epoch=569
06/24/2022 09:59:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.76 on epoch=574
06/24/2022 09:59:36 - INFO - __main__ - Global step 1150 Train loss 0.68 ACC 0.5 on epoch=574
06/24/2022 09:59:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.65 on epoch=579
06/24/2022 09:59:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.66 on epoch=584
06/24/2022 09:59:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.53 on epoch=589
06/24/2022 09:59:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.71 on epoch=594
06/24/2022 09:59:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.64 on epoch=599
06/24/2022 09:59:42 - INFO - __main__ - Global step 1200 Train loss 0.64 ACC 0.5 on epoch=599
06/24/2022 09:59:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.74 on epoch=604
06/24/2022 09:59:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.74 on epoch=609
06/24/2022 09:59:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.62 on epoch=614
06/24/2022 09:59:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.58 on epoch=619
06/24/2022 09:59:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.63 on epoch=624
06/24/2022 09:59:49 - INFO - __main__ - Global step 1250 Train loss 0.66 ACC 0.46875 on epoch=624
06/24/2022 09:59:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.63 on epoch=629
06/24/2022 09:59:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.57 on epoch=634
06/24/2022 09:59:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.63 on epoch=639
06/24/2022 09:59:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.63 on epoch=644
06/24/2022 09:59:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.64 on epoch=649
06/24/2022 09:59:55 - INFO - __main__ - Global step 1300 Train loss 0.62 ACC 0.5 on epoch=649
06/24/2022 09:59:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.58 on epoch=654
06/24/2022 09:59:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.59 on epoch=659
06/24/2022 09:59:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.56 on epoch=664
06/24/2022 10:00:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.59 on epoch=669
06/24/2022 10:00:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.68 on epoch=674
06/24/2022 10:00:01 - INFO - __main__ - Global step 1350 Train loss 0.60 ACC 0.5 on epoch=674
06/24/2022 10:00:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.57 on epoch=679
06/24/2022 10:00:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.57 on epoch=684
06/24/2022 10:00:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.60 on epoch=689
06/24/2022 10:00:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.65 on epoch=694
06/24/2022 10:00:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.58 on epoch=699
06/24/2022 10:00:08 - INFO - __main__ - Global step 1400 Train loss 0.59 ACC 0.5 on epoch=699
06/24/2022 10:00:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.68 on epoch=704
06/24/2022 10:00:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.53 on epoch=709
06/24/2022 10:00:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.60 on epoch=714
06/24/2022 10:00:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.52 on epoch=719
06/24/2022 10:00:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.49 on epoch=724
06/24/2022 10:00:14 - INFO - __main__ - Global step 1450 Train loss 0.56 ACC 0.5 on epoch=724
06/24/2022 10:00:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.51 on epoch=729
06/24/2022 10:00:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.64 on epoch=734
06/24/2022 10:00:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.65 on epoch=739
06/24/2022 10:00:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.53 on epoch=744
06/24/2022 10:00:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.49 on epoch=749
06/24/2022 10:00:20 - INFO - __main__ - Global step 1500 Train loss 0.56 ACC 0.5 on epoch=749
06/24/2022 10:00:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.58 on epoch=754
06/24/2022 10:00:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.69 on epoch=759
06/24/2022 10:00:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.53 on epoch=764
06/24/2022 10:00:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.54 on epoch=769
06/24/2022 10:00:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.54 on epoch=774
06/24/2022 10:00:27 - INFO - __main__ - Global step 1550 Train loss 0.58 ACC 0.5 on epoch=774
06/24/2022 10:00:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.51 on epoch=779
06/24/2022 10:00:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.54 on epoch=784
06/24/2022 10:00:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.60 on epoch=789
06/24/2022 10:00:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.58 on epoch=794
06/24/2022 10:00:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.57 on epoch=799
06/24/2022 10:00:33 - INFO - __main__ - Global step 1600 Train loss 0.56 ACC 0.46875 on epoch=799
06/24/2022 10:00:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.52 on epoch=804
06/24/2022 10:00:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.59 on epoch=809
06/24/2022 10:00:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.61 on epoch=814
06/24/2022 10:00:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.51 on epoch=819
06/24/2022 10:00:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.56 on epoch=824
06/24/2022 10:00:40 - INFO - __main__ - Global step 1650 Train loss 0.56 ACC 0.5 on epoch=824
06/24/2022 10:00:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=829
06/24/2022 10:00:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=834
06/24/2022 10:00:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.48 on epoch=839
06/24/2022 10:00:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.52 on epoch=844
06/24/2022 10:00:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.50 on epoch=849
06/24/2022 10:00:46 - INFO - __main__ - Global step 1700 Train loss 0.49 ACC 0.5 on epoch=849
06/24/2022 10:00:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.50 on epoch=854
06/24/2022 10:00:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.50 on epoch=859
06/24/2022 10:00:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.58 on epoch=864
06/24/2022 10:00:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.59 on epoch=869
06/24/2022 10:00:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.57 on epoch=874
06/24/2022 10:00:52 - INFO - __main__ - Global step 1750 Train loss 0.55 ACC 0.5 on epoch=874
06/24/2022 10:00:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.61 on epoch=879
06/24/2022 10:00:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.50 on epoch=884
06/24/2022 10:00:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.51 on epoch=889
06/24/2022 10:00:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.51 on epoch=894
06/24/2022 10:00:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.61 on epoch=899
06/24/2022 10:00:59 - INFO - __main__ - Global step 1800 Train loss 0.55 ACC 0.5 on epoch=899
06/24/2022 10:01:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.48 on epoch=904
06/24/2022 10:01:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.47 on epoch=909
06/24/2022 10:01:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.55 on epoch=914
06/24/2022 10:01:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.58 on epoch=919
06/24/2022 10:01:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.47 on epoch=924
06/24/2022 10:01:05 - INFO - __main__ - Global step 1850 Train loss 0.51 ACC 0.4375 on epoch=924
06/24/2022 10:01:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.66 on epoch=929
06/24/2022 10:01:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.47 on epoch=934
06/24/2022 10:01:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.57 on epoch=939
06/24/2022 10:01:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.51 on epoch=944
06/24/2022 10:01:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.55 on epoch=949
06/24/2022 10:01:11 - INFO - __main__ - Global step 1900 Train loss 0.55 ACC 0.40625 on epoch=949
06/24/2022 10:01:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.59 on epoch=954
06/24/2022 10:01:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.57 on epoch=959
06/24/2022 10:01:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.48 on epoch=964
06/24/2022 10:01:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.64 on epoch=969
06/24/2022 10:01:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=974
06/24/2022 10:01:18 - INFO - __main__ - Global step 1950 Train loss 0.54 ACC 0.53125 on epoch=974
06/24/2022 10:01:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.50 on epoch=979
06/24/2022 10:01:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.48 on epoch=984
06/24/2022 10:01:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.56 on epoch=989
06/24/2022 10:01:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.47 on epoch=994
06/24/2022 10:01:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=999
06/24/2022 10:01:24 - INFO - __main__ - Global step 2000 Train loss 0.50 ACC 0.53125 on epoch=999
06/24/2022 10:01:24 - INFO - __main__ - save last model!
06/24/2022 10:01:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 10:01:24 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 10:01:24 - INFO - __main__ - Printing 3 examples
06/24/2022 10:01:24 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 10:01:24 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:24 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 10:01:24 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:24 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 10:01:24 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:24 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:01:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:01:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:01:25 - INFO - __main__ - Printing 3 examples
06/24/2022 10:01:25 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 10:01:25 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:25 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 10:01:25 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:25 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 10:01:25 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 10:01:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:01:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:01:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:01:25 - INFO - __main__ - Printing 3 examples
06/24/2022 10:01:25 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 10:01:25 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:25 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 10:01:25 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:25 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 10:01:25 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:25 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:01:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:01:25 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:01:25 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 10:01:31 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:01:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:01:31 - INFO - __main__ - Starting training!
06/24/2022 10:01:32 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_87_0.4_8_predictions.txt
06/24/2022 10:01:32 - INFO - __main__ - ACC on test data: 0.5066
06/24/2022 10:01:32 - INFO - __main__ - prefix=medical_questions_pairs_16_87, lr=0.4, bsz=8, dev_performance=0.5625, test_performance=0.5065573770491804
06/24/2022 10:01:32 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_87, lr=0.3, bsz=8 ...
06/24/2022 10:01:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:01:33 - INFO - __main__ - Printing 3 examples
06/24/2022 10:01:33 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 10:01:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 10:01:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:33 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 10:01:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 10:01:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:01:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:01:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:01:33 - INFO - __main__ - Printing 3 examples
06/24/2022 10:01:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 10:01:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:33 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 10:01:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:33 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 10:01:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:01:33 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:01:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:01:33 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:01:38 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:01:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:01:39 - INFO - __main__ - Starting training!
06/24/2022 10:01:40 - INFO - __main__ - Step 10 Global step 10 Train loss 8.47 on epoch=4
06/24/2022 10:01:42 - INFO - __main__ - Step 20 Global step 20 Train loss 8.42 on epoch=9
06/24/2022 10:01:43 - INFO - __main__ - Step 30 Global step 30 Train loss 8.38 on epoch=14
06/24/2022 10:01:44 - INFO - __main__ - Step 40 Global step 40 Train loss 8.30 on epoch=19
06/24/2022 10:01:45 - INFO - __main__ - Step 50 Global step 50 Train loss 8.33 on epoch=24
06/24/2022 10:01:52 - INFO - __main__ - Global step 50 Train loss 8.38 ACC 0.0 on epoch=24
06/24/2022 10:01:52 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 10:01:53 - INFO - __main__ - Step 60 Global step 60 Train loss 8.25 on epoch=29
06/24/2022 10:01:54 - INFO - __main__ - Step 70 Global step 70 Train loss 8.23 on epoch=34
06/24/2022 10:01:55 - INFO - __main__ - Step 80 Global step 80 Train loss 8.27 on epoch=39
06/24/2022 10:01:57 - INFO - __main__ - Step 90 Global step 90 Train loss 8.15 on epoch=44
06/24/2022 10:01:58 - INFO - __main__ - Step 100 Global step 100 Train loss 8.13 on epoch=49
06/24/2022 10:02:06 - INFO - __main__ - Global step 100 Train loss 8.21 ACC 0.0 on epoch=49
06/24/2022 10:02:07 - INFO - __main__ - Step 110 Global step 110 Train loss 8.05 on epoch=54
06/24/2022 10:02:09 - INFO - __main__ - Step 120 Global step 120 Train loss 8.07 on epoch=59
06/24/2022 10:02:10 - INFO - __main__ - Step 130 Global step 130 Train loss 8.05 on epoch=64
06/24/2022 10:02:11 - INFO - __main__ - Step 140 Global step 140 Train loss 7.89 on epoch=69
06/24/2022 10:02:12 - INFO - __main__ - Step 150 Global step 150 Train loss 8.05 on epoch=74
06/24/2022 10:02:22 - INFO - __main__ - Global step 150 Train loss 8.03 ACC 0.0 on epoch=74
06/24/2022 10:02:24 - INFO - __main__ - Step 160 Global step 160 Train loss 8.01 on epoch=79
06/24/2022 10:02:25 - INFO - __main__ - Step 170 Global step 170 Train loss 7.96 on epoch=84
06/24/2022 10:02:26 - INFO - __main__ - Step 180 Global step 180 Train loss 7.88 on epoch=89
06/24/2022 10:02:27 - INFO - __main__ - Step 190 Global step 190 Train loss 7.85 on epoch=94
06/24/2022 10:02:28 - INFO - __main__ - Step 200 Global step 200 Train loss 7.94 on epoch=99
06/24/2022 10:02:36 - INFO - __main__ - Global step 200 Train loss 7.93 ACC 0.0 on epoch=99
06/24/2022 10:02:37 - INFO - __main__ - Step 210 Global step 210 Train loss 7.85 on epoch=104
06/24/2022 10:02:38 - INFO - __main__ - Step 220 Global step 220 Train loss 7.78 on epoch=109
06/24/2022 10:02:39 - INFO - __main__ - Step 230 Global step 230 Train loss 7.79 on epoch=114
06/24/2022 10:02:41 - INFO - __main__ - Step 240 Global step 240 Train loss 7.77 on epoch=119
06/24/2022 10:02:42 - INFO - __main__ - Step 250 Global step 250 Train loss 7.64 on epoch=124
06/24/2022 10:02:47 - INFO - __main__ - Global step 250 Train loss 7.77 ACC 0.0 on epoch=124
06/24/2022 10:02:49 - INFO - __main__ - Step 260 Global step 260 Train loss 7.53 on epoch=129
06/24/2022 10:02:50 - INFO - __main__ - Step 270 Global step 270 Train loss 7.58 on epoch=134
06/24/2022 10:02:51 - INFO - __main__ - Step 280 Global step 280 Train loss 7.50 on epoch=139
06/24/2022 10:02:52 - INFO - __main__ - Step 290 Global step 290 Train loss 7.34 on epoch=144
06/24/2022 10:02:53 - INFO - __main__ - Step 300 Global step 300 Train loss 7.23 on epoch=149
06/24/2022 10:03:03 - INFO - __main__ - Global step 300 Train loss 7.44 ACC 0.0 on epoch=149
06/24/2022 10:03:04 - INFO - __main__ - Step 310 Global step 310 Train loss 7.16 on epoch=154
06/24/2022 10:03:06 - INFO - __main__ - Step 320 Global step 320 Train loss 7.19 on epoch=159
06/24/2022 10:03:07 - INFO - __main__ - Step 330 Global step 330 Train loss 7.01 on epoch=164
06/24/2022 10:03:08 - INFO - __main__ - Step 340 Global step 340 Train loss 6.87 on epoch=169
06/24/2022 10:03:09 - INFO - __main__ - Step 350 Global step 350 Train loss 6.76 on epoch=174
06/24/2022 10:03:16 - INFO - __main__ - Global step 350 Train loss 7.00 ACC 0.0 on epoch=174
06/24/2022 10:03:18 - INFO - __main__ - Step 360 Global step 360 Train loss 6.60 on epoch=179
06/24/2022 10:03:19 - INFO - __main__ - Step 370 Global step 370 Train loss 6.67 on epoch=184
06/24/2022 10:03:20 - INFO - __main__ - Step 380 Global step 380 Train loss 6.58 on epoch=189
06/24/2022 10:03:21 - INFO - __main__ - Step 390 Global step 390 Train loss 6.37 on epoch=194
06/24/2022 10:03:22 - INFO - __main__ - Step 400 Global step 400 Train loss 6.32 on epoch=199
06/24/2022 10:03:30 - INFO - __main__ - Global step 400 Train loss 6.51 ACC 0.0 on epoch=199
06/24/2022 10:03:31 - INFO - __main__ - Step 410 Global step 410 Train loss 6.23 on epoch=204
06/24/2022 10:03:32 - INFO - __main__ - Step 420 Global step 420 Train loss 6.13 on epoch=209
06/24/2022 10:03:34 - INFO - __main__ - Step 430 Global step 430 Train loss 5.93 on epoch=214
06/24/2022 10:03:35 - INFO - __main__ - Step 440 Global step 440 Train loss 6.12 on epoch=219
06/24/2022 10:03:36 - INFO - __main__ - Step 450 Global step 450 Train loss 5.85 on epoch=224
06/24/2022 10:03:46 - INFO - __main__ - Global step 450 Train loss 6.05 ACC 0.0 on epoch=224
06/24/2022 10:03:48 - INFO - __main__ - Step 460 Global step 460 Train loss 5.78 on epoch=229
06/24/2022 10:03:49 - INFO - __main__ - Step 470 Global step 470 Train loss 5.69 on epoch=234
06/24/2022 10:03:50 - INFO - __main__ - Step 480 Global step 480 Train loss 5.71 on epoch=239
06/24/2022 10:03:51 - INFO - __main__ - Step 490 Global step 490 Train loss 5.39 on epoch=244
06/24/2022 10:03:52 - INFO - __main__ - Step 500 Global step 500 Train loss 5.36 on epoch=249
06/24/2022 10:03:57 - INFO - __main__ - Global step 500 Train loss 5.59 ACC 0.0 on epoch=249
06/24/2022 10:03:58 - INFO - __main__ - Step 510 Global step 510 Train loss 5.21 on epoch=254
06/24/2022 10:04:00 - INFO - __main__ - Step 520 Global step 520 Train loss 5.18 on epoch=259
06/24/2022 10:04:01 - INFO - __main__ - Step 530 Global step 530 Train loss 5.07 on epoch=264
06/24/2022 10:04:02 - INFO - __main__ - Step 540 Global step 540 Train loss 4.84 on epoch=269
06/24/2022 10:04:03 - INFO - __main__ - Step 550 Global step 550 Train loss 4.80 on epoch=274
06/24/2022 10:04:08 - INFO - __main__ - Global step 550 Train loss 5.02 ACC 0.0 on epoch=274
06/24/2022 10:04:09 - INFO - __main__ - Step 560 Global step 560 Train loss 4.75 on epoch=279
06/24/2022 10:04:11 - INFO - __main__ - Step 570 Global step 570 Train loss 4.65 on epoch=284
06/24/2022 10:04:12 - INFO - __main__ - Step 580 Global step 580 Train loss 4.51 on epoch=289
06/24/2022 10:04:13 - INFO - __main__ - Step 590 Global step 590 Train loss 4.35 on epoch=294
06/24/2022 10:04:14 - INFO - __main__ - Step 600 Global step 600 Train loss 4.31 on epoch=299
06/24/2022 10:04:25 - INFO - __main__ - Global step 600 Train loss 4.51 ACC 0.0 on epoch=299
06/24/2022 10:04:27 - INFO - __main__ - Step 610 Global step 610 Train loss 4.15 on epoch=304
06/24/2022 10:04:28 - INFO - __main__ - Step 620 Global step 620 Train loss 4.07 on epoch=309
06/24/2022 10:04:29 - INFO - __main__ - Step 630 Global step 630 Train loss 4.03 on epoch=314
06/24/2022 10:04:30 - INFO - __main__ - Step 640 Global step 640 Train loss 3.88 on epoch=319
06/24/2022 10:04:31 - INFO - __main__ - Step 650 Global step 650 Train loss 3.81 on epoch=324
06/24/2022 10:04:37 - INFO - __main__ - Global step 650 Train loss 3.99 ACC 0.0 on epoch=324
06/24/2022 10:04:38 - INFO - __main__ - Step 660 Global step 660 Train loss 3.70 on epoch=329
06/24/2022 10:04:40 - INFO - __main__ - Step 670 Global step 670 Train loss 3.61 on epoch=334
06/24/2022 10:04:41 - INFO - __main__ - Step 680 Global step 680 Train loss 3.37 on epoch=339
06/24/2022 10:04:42 - INFO - __main__ - Step 690 Global step 690 Train loss 3.33 on epoch=344
06/24/2022 10:04:43 - INFO - __main__ - Step 700 Global step 700 Train loss 3.33 on epoch=349
06/24/2022 10:04:46 - INFO - __main__ - Global step 700 Train loss 3.47 ACC 0.15625 on epoch=349
06/24/2022 10:04:46 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.15625 on epoch=349, global_step=700
06/24/2022 10:04:47 - INFO - __main__ - Step 710 Global step 710 Train loss 3.19 on epoch=354
06/24/2022 10:04:48 - INFO - __main__ - Step 720 Global step 720 Train loss 3.24 on epoch=359
06/24/2022 10:04:49 - INFO - __main__ - Step 730 Global step 730 Train loss 2.93 on epoch=364
06/24/2022 10:04:51 - INFO - __main__ - Step 740 Global step 740 Train loss 2.89 on epoch=369
06/24/2022 10:04:52 - INFO - __main__ - Step 750 Global step 750 Train loss 2.83 on epoch=374
06/24/2022 10:04:56 - INFO - __main__ - Global step 750 Train loss 3.01 ACC 0.375 on epoch=374
06/24/2022 10:04:56 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.375 on epoch=374, global_step=750
06/24/2022 10:04:57 - INFO - __main__ - Step 760 Global step 760 Train loss 2.80 on epoch=379
06/24/2022 10:04:58 - INFO - __main__ - Step 770 Global step 770 Train loss 2.63 on epoch=384
06/24/2022 10:04:59 - INFO - __main__ - Step 780 Global step 780 Train loss 2.43 on epoch=389
06/24/2022 10:05:00 - INFO - __main__ - Step 790 Global step 790 Train loss 2.60 on epoch=394
06/24/2022 10:05:02 - INFO - __main__ - Step 800 Global step 800 Train loss 2.31 on epoch=399
06/24/2022 10:05:11 - INFO - __main__ - Global step 800 Train loss 2.55 ACC 0.46875 on epoch=399
06/24/2022 10:05:11 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.46875 on epoch=399, global_step=800
06/24/2022 10:05:12 - INFO - __main__ - Step 810 Global step 810 Train loss 2.41 on epoch=404
06/24/2022 10:05:13 - INFO - __main__ - Step 820 Global step 820 Train loss 2.31 on epoch=409
06/24/2022 10:05:15 - INFO - __main__ - Step 830 Global step 830 Train loss 2.39 on epoch=414
06/24/2022 10:05:16 - INFO - __main__ - Step 840 Global step 840 Train loss 2.36 on epoch=419
06/24/2022 10:05:17 - INFO - __main__ - Step 850 Global step 850 Train loss 2.20 on epoch=424
06/24/2022 10:05:28 - INFO - __main__ - Global step 850 Train loss 2.34 ACC 0.5 on epoch=424
06/24/2022 10:05:28 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=424, global_step=850
06/24/2022 10:05:29 - INFO - __main__ - Step 860 Global step 860 Train loss 2.18 on epoch=429
06/24/2022 10:05:30 - INFO - __main__ - Step 870 Global step 870 Train loss 2.08 on epoch=434
06/24/2022 10:05:31 - INFO - __main__ - Step 880 Global step 880 Train loss 2.03 on epoch=439
06/24/2022 10:05:32 - INFO - __main__ - Step 890 Global step 890 Train loss 2.01 on epoch=444
06/24/2022 10:05:34 - INFO - __main__ - Step 900 Global step 900 Train loss 1.92 on epoch=449
06/24/2022 10:05:44 - INFO - __main__ - Global step 900 Train loss 2.05 ACC 0.46875 on epoch=449
06/24/2022 10:05:45 - INFO - __main__ - Step 910 Global step 910 Train loss 1.90 on epoch=454
06/24/2022 10:05:46 - INFO - __main__ - Step 920 Global step 920 Train loss 1.77 on epoch=459
06/24/2022 10:05:47 - INFO - __main__ - Step 930 Global step 930 Train loss 1.66 on epoch=464
06/24/2022 10:05:49 - INFO - __main__ - Step 940 Global step 940 Train loss 1.83 on epoch=469
06/24/2022 10:05:50 - INFO - __main__ - Step 950 Global step 950 Train loss 1.78 on epoch=474
06/24/2022 10:06:00 - INFO - __main__ - Global step 950 Train loss 1.79 ACC 0.5 on epoch=474
06/24/2022 10:06:01 - INFO - __main__ - Step 960 Global step 960 Train loss 1.63 on epoch=479
06/24/2022 10:06:02 - INFO - __main__ - Step 970 Global step 970 Train loss 1.51 on epoch=484
06/24/2022 10:06:03 - INFO - __main__ - Step 980 Global step 980 Train loss 1.56 on epoch=489
06/24/2022 10:06:04 - INFO - __main__ - Step 990 Global step 990 Train loss 1.48 on epoch=494
06/24/2022 10:06:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.53 on epoch=499
06/24/2022 10:06:16 - INFO - __main__ - Global step 1000 Train loss 1.54 ACC 0.5 on epoch=499
06/24/2022 10:06:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.41 on epoch=504
06/24/2022 10:06:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.24 on epoch=509
06/24/2022 10:06:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.39 on epoch=514
06/24/2022 10:06:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.31 on epoch=519
06/24/2022 10:06:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.31 on epoch=524
06/24/2022 10:06:31 - INFO - __main__ - Global step 1050 Train loss 1.33 ACC 0.5 on epoch=524
06/24/2022 10:06:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.24 on epoch=529
06/24/2022 10:06:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.29 on epoch=534
06/24/2022 10:06:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.14 on epoch=539
06/24/2022 10:06:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.24 on epoch=544
06/24/2022 10:06:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.18 on epoch=549
06/24/2022 10:06:47 - INFO - __main__ - Global step 1100 Train loss 1.22 ACC 0.53125 on epoch=549
06/24/2022 10:06:47 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=549, global_step=1100
06/24/2022 10:06:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.14 on epoch=554
06/24/2022 10:06:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.08 on epoch=559
06/24/2022 10:06:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.16 on epoch=564
06/24/2022 10:06:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.97 on epoch=569
06/24/2022 10:06:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.11 on epoch=574
06/24/2022 10:06:57 - INFO - __main__ - Global step 1150 Train loss 1.09 ACC 0.5 on epoch=574
06/24/2022 10:06:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.91 on epoch=579
06/24/2022 10:07:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.04 on epoch=584
06/24/2022 10:07:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.90 on epoch=589
06/24/2022 10:07:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.87 on epoch=594
06/24/2022 10:07:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.96 on epoch=599
06/24/2022 10:07:11 - INFO - __main__ - Global step 1200 Train loss 0.93 ACC 0.4375 on epoch=599
06/24/2022 10:07:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.83 on epoch=604
06/24/2022 10:07:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=609
06/24/2022 10:07:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.86 on epoch=614
06/24/2022 10:07:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.93 on epoch=619
06/24/2022 10:07:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.81 on epoch=624
06/24/2022 10:07:27 - INFO - __main__ - Global step 1250 Train loss 0.85 ACC 0.46875 on epoch=624
06/24/2022 10:07:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.89 on epoch=629
06/24/2022 10:07:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.82 on epoch=634
06/24/2022 10:07:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.79 on epoch=639
06/24/2022 10:07:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.99 on epoch=644
06/24/2022 10:07:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.78 on epoch=649
06/24/2022 10:07:37 - INFO - __main__ - Global step 1300 Train loss 0.86 ACC 0.5 on epoch=649
06/24/2022 10:07:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.72 on epoch=654
06/24/2022 10:07:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.78 on epoch=659
06/24/2022 10:07:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.72 on epoch=664
06/24/2022 10:07:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.82 on epoch=669
06/24/2022 10:07:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.76 on epoch=674
06/24/2022 10:07:50 - INFO - __main__ - Global step 1350 Train loss 0.76 ACC 0.5 on epoch=674
06/24/2022 10:07:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.83 on epoch=679
06/24/2022 10:07:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.79 on epoch=684
06/24/2022 10:07:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.72 on epoch=689
06/24/2022 10:07:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.82 on epoch=694
06/24/2022 10:07:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.65 on epoch=699
06/24/2022 10:07:59 - INFO - __main__ - Global step 1400 Train loss 0.76 ACC 0.5 on epoch=699
06/24/2022 10:08:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.68 on epoch=704
06/24/2022 10:08:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.79 on epoch=709
06/24/2022 10:08:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.69 on epoch=714
06/24/2022 10:08:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.64 on epoch=719
06/24/2022 10:08:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.64 on epoch=724
06/24/2022 10:08:09 - INFO - __main__ - Global step 1450 Train loss 0.69 ACC 0.46875 on epoch=724
06/24/2022 10:08:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.63 on epoch=729
06/24/2022 10:08:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.62 on epoch=734
06/24/2022 10:08:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.73 on epoch=739
06/24/2022 10:08:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.71 on epoch=744
06/24/2022 10:08:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.60 on epoch=749
06/24/2022 10:08:19 - INFO - __main__ - Global step 1500 Train loss 0.66 ACC 0.46875 on epoch=749
06/24/2022 10:08:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.65 on epoch=754
06/24/2022 10:08:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.64 on epoch=759
06/24/2022 10:08:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.57 on epoch=764
06/24/2022 10:08:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.65 on epoch=769
06/24/2022 10:08:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.67 on epoch=774
06/24/2022 10:08:27 - INFO - __main__ - Global step 1550 Train loss 0.64 ACC 0.34375 on epoch=774
06/24/2022 10:08:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.67 on epoch=779
06/24/2022 10:08:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.63 on epoch=784
06/24/2022 10:08:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.55 on epoch=789
06/24/2022 10:08:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.71 on epoch=794
06/24/2022 10:08:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.64 on epoch=799
06/24/2022 10:08:35 - INFO - __main__ - Global step 1600 Train loss 0.64 ACC 0.4375 on epoch=799
06/24/2022 10:08:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.69 on epoch=804
06/24/2022 10:08:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.67 on epoch=809
06/24/2022 10:08:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.54 on epoch=814
06/24/2022 10:08:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.60 on epoch=819
06/24/2022 10:08:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.59 on epoch=824
06/24/2022 10:08:43 - INFO - __main__ - Global step 1650 Train loss 0.62 ACC 0.5 on epoch=824
06/24/2022 10:08:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.58 on epoch=829
06/24/2022 10:08:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.57 on epoch=834
06/24/2022 10:08:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.67 on epoch=839
06/24/2022 10:08:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.60 on epoch=844
06/24/2022 10:08:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.69 on epoch=849
06/24/2022 10:08:50 - INFO - __main__ - Global step 1700 Train loss 0.62 ACC 0.5 on epoch=849
06/24/2022 10:08:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.53 on epoch=854
06/24/2022 10:08:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.52 on epoch=859
06/24/2022 10:08:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.56 on epoch=864
06/24/2022 10:08:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.57 on epoch=869
06/24/2022 10:08:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.64 on epoch=874
06/24/2022 10:09:06 - INFO - __main__ - Global step 1750 Train loss 0.56 ACC 0.53125 on epoch=874
06/24/2022 10:09:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.54 on epoch=879
06/24/2022 10:09:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.62 on epoch=884
06/24/2022 10:09:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.60 on epoch=889
06/24/2022 10:09:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.55 on epoch=894
06/24/2022 10:09:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.45 on epoch=899
06/24/2022 10:09:12 - INFO - __main__ - Global step 1800 Train loss 0.55 ACC 0.4375 on epoch=899
06/24/2022 10:09:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.60 on epoch=904
06/24/2022 10:09:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.52 on epoch=909
06/24/2022 10:09:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.56 on epoch=914
06/24/2022 10:09:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.63 on epoch=919
06/24/2022 10:09:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.63 on epoch=924
06/24/2022 10:09:18 - INFO - __main__ - Global step 1850 Train loss 0.59 ACC 0.4375 on epoch=924
06/24/2022 10:09:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.54 on epoch=929
06/24/2022 10:09:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.58 on epoch=934
06/24/2022 10:09:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.60 on epoch=939
06/24/2022 10:09:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.53 on epoch=944
06/24/2022 10:09:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.60 on epoch=949
06/24/2022 10:09:25 - INFO - __main__ - Global step 1900 Train loss 0.57 ACC 0.40625 on epoch=949
06/24/2022 10:09:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.57 on epoch=954
06/24/2022 10:09:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.55 on epoch=959
06/24/2022 10:09:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.59 on epoch=964
06/24/2022 10:09:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.54 on epoch=969
06/24/2022 10:09:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=974
06/24/2022 10:09:31 - INFO - __main__ - Global step 1950 Train loss 0.55 ACC 0.5 on epoch=974
06/24/2022 10:09:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.54 on epoch=979
06/24/2022 10:09:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.50 on epoch=984
06/24/2022 10:09:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=989
06/24/2022 10:09:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.61 on epoch=994
06/24/2022 10:09:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.53 on epoch=999
06/24/2022 10:09:38 - INFO - __main__ - Global step 2000 Train loss 0.52 ACC 0.5 on epoch=999
06/24/2022 10:09:38 - INFO - __main__ - save last model!
06/24/2022 10:09:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 10:09:38 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 10:09:38 - INFO - __main__ - Printing 3 examples
06/24/2022 10:09:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 10:09:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 10:09:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:38 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 10:09:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:09:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:09:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:09:38 - INFO - __main__ - Printing 3 examples
06/24/2022 10:09:38 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 10:09:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:38 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 10:09:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 10:09:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 10:09:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:09:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:09:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:09:38 - INFO - __main__ - Printing 3 examples
06/24/2022 10:09:38 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 10:09:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 10:09:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 10:09:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:09:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:09:38 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:09:39 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 10:09:44 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:09:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:09:44 - INFO - __main__ - Starting training!
06/24/2022 10:09:46 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_87_0.3_8_predictions.txt
06/24/2022 10:09:46 - INFO - __main__ - ACC on test data: 0.5115
06/24/2022 10:09:46 - INFO - __main__ - prefix=medical_questions_pairs_16_87, lr=0.3, bsz=8, dev_performance=0.53125, test_performance=0.5114754098360655
06/24/2022 10:09:46 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_87, lr=0.2, bsz=8 ...
06/24/2022 10:09:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:09:47 - INFO - __main__ - Printing 3 examples
06/24/2022 10:09:47 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 10:09:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:47 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 10:09:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:47 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 10:09:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 10:09:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:09:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:09:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:09:47 - INFO - __main__ - Printing 3 examples
06/24/2022 10:09:47 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 10:09:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:47 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 10:09:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:47 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 10:09:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:09:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:09:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:09:47 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:09:53 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:09:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:09:53 - INFO - __main__ - Starting training!
06/24/2022 10:09:55 - INFO - __main__ - Step 10 Global step 10 Train loss 8.47 on epoch=4
06/24/2022 10:09:56 - INFO - __main__ - Step 20 Global step 20 Train loss 8.35 on epoch=9
06/24/2022 10:09:57 - INFO - __main__ - Step 30 Global step 30 Train loss 8.41 on epoch=14
06/24/2022 10:09:59 - INFO - __main__ - Step 40 Global step 40 Train loss 8.29 on epoch=19
06/24/2022 10:10:00 - INFO - __main__ - Step 50 Global step 50 Train loss 8.24 on epoch=24
06/24/2022 10:10:06 - INFO - __main__ - Global step 50 Train loss 8.35 ACC 0.0 on epoch=24
06/24/2022 10:10:06 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 10:10:07 - INFO - __main__ - Step 60 Global step 60 Train loss 8.27 on epoch=29
06/24/2022 10:10:08 - INFO - __main__ - Step 70 Global step 70 Train loss 8.28 on epoch=34
06/24/2022 10:10:10 - INFO - __main__ - Step 80 Global step 80 Train loss 8.27 on epoch=39
06/24/2022 10:10:11 - INFO - __main__ - Step 90 Global step 90 Train loss 8.31 on epoch=44
06/24/2022 10:10:12 - INFO - __main__ - Step 100 Global step 100 Train loss 8.31 on epoch=49
06/24/2022 10:10:14 - INFO - __main__ - Global step 100 Train loss 8.29 ACC 0.0 on epoch=49
06/24/2022 10:10:15 - INFO - __main__ - Step 110 Global step 110 Train loss 8.26 on epoch=54
06/24/2022 10:10:17 - INFO - __main__ - Step 120 Global step 120 Train loss 8.20 on epoch=59
06/24/2022 10:10:18 - INFO - __main__ - Step 130 Global step 130 Train loss 8.26 on epoch=64
06/24/2022 10:10:19 - INFO - __main__ - Step 140 Global step 140 Train loss 8.13 on epoch=69
06/24/2022 10:10:21 - INFO - __main__ - Step 150 Global step 150 Train loss 8.07 on epoch=74
06/24/2022 10:10:24 - INFO - __main__ - Global step 150 Train loss 8.19 ACC 0.0 on epoch=74
06/24/2022 10:10:25 - INFO - __main__ - Step 160 Global step 160 Train loss 8.13 on epoch=79
06/24/2022 10:10:27 - INFO - __main__ - Step 170 Global step 170 Train loss 8.06 on epoch=84
06/24/2022 10:10:28 - INFO - __main__ - Step 180 Global step 180 Train loss 8.05 on epoch=89
06/24/2022 10:10:29 - INFO - __main__ - Step 190 Global step 190 Train loss 8.04 on epoch=94
06/24/2022 10:10:30 - INFO - __main__ - Step 200 Global step 200 Train loss 8.06 on epoch=99
06/24/2022 10:10:35 - INFO - __main__ - Global step 200 Train loss 8.07 ACC 0.0 on epoch=99
06/24/2022 10:10:37 - INFO - __main__ - Step 210 Global step 210 Train loss 8.06 on epoch=104
06/24/2022 10:10:38 - INFO - __main__ - Step 220 Global step 220 Train loss 7.99 on epoch=109
06/24/2022 10:10:39 - INFO - __main__ - Step 230 Global step 230 Train loss 7.90 on epoch=114
06/24/2022 10:10:41 - INFO - __main__ - Step 240 Global step 240 Train loss 7.83 on epoch=119
06/24/2022 10:10:42 - INFO - __main__ - Step 250 Global step 250 Train loss 7.96 on epoch=124
06/24/2022 10:10:46 - INFO - __main__ - Global step 250 Train loss 7.95 ACC 0.0 on epoch=124
06/24/2022 10:10:48 - INFO - __main__ - Step 260 Global step 260 Train loss 8.00 on epoch=129
06/24/2022 10:10:49 - INFO - __main__ - Step 270 Global step 270 Train loss 7.90 on epoch=134
06/24/2022 10:10:50 - INFO - __main__ - Step 280 Global step 280 Train loss 7.94 on epoch=139
06/24/2022 10:10:51 - INFO - __main__ - Step 290 Global step 290 Train loss 7.81 on epoch=144
06/24/2022 10:10:53 - INFO - __main__ - Step 300 Global step 300 Train loss 7.74 on epoch=149
06/24/2022 10:10:57 - INFO - __main__ - Global step 300 Train loss 7.88 ACC 0.0 on epoch=149
06/24/2022 10:10:59 - INFO - __main__ - Step 310 Global step 310 Train loss 7.62 on epoch=154
06/24/2022 10:11:00 - INFO - __main__ - Step 320 Global step 320 Train loss 7.70 on epoch=159
06/24/2022 10:11:01 - INFO - __main__ - Step 330 Global step 330 Train loss 7.61 on epoch=164
06/24/2022 10:11:02 - INFO - __main__ - Step 340 Global step 340 Train loss 7.59 on epoch=169
06/24/2022 10:11:04 - INFO - __main__ - Step 350 Global step 350 Train loss 7.61 on epoch=174
06/24/2022 10:11:09 - INFO - __main__ - Global step 350 Train loss 7.63 ACC 0.0 on epoch=174
06/24/2022 10:11:11 - INFO - __main__ - Step 360 Global step 360 Train loss 7.56 on epoch=179
06/24/2022 10:11:12 - INFO - __main__ - Step 370 Global step 370 Train loss 7.32 on epoch=184
06/24/2022 10:11:13 - INFO - __main__ - Step 380 Global step 380 Train loss 7.36 on epoch=189
06/24/2022 10:11:15 - INFO - __main__ - Step 390 Global step 390 Train loss 7.37 on epoch=194
06/24/2022 10:11:16 - INFO - __main__ - Step 400 Global step 400 Train loss 7.38 on epoch=199
06/24/2022 10:11:25 - INFO - __main__ - Global step 400 Train loss 7.40 ACC 0.0 on epoch=199
06/24/2022 10:11:26 - INFO - __main__ - Step 410 Global step 410 Train loss 7.19 on epoch=204
06/24/2022 10:11:28 - INFO - __main__ - Step 420 Global step 420 Train loss 7.20 on epoch=209
06/24/2022 10:11:29 - INFO - __main__ - Step 430 Global step 430 Train loss 7.13 on epoch=214
06/24/2022 10:11:30 - INFO - __main__ - Step 440 Global step 440 Train loss 6.88 on epoch=219
06/24/2022 10:11:32 - INFO - __main__ - Step 450 Global step 450 Train loss 6.90 on epoch=224
06/24/2022 10:11:42 - INFO - __main__ - Global step 450 Train loss 7.06 ACC 0.0 on epoch=224
06/24/2022 10:11:44 - INFO - __main__ - Step 460 Global step 460 Train loss 6.70 on epoch=229
06/24/2022 10:11:45 - INFO - __main__ - Step 470 Global step 470 Train loss 6.73 on epoch=234
06/24/2022 10:11:46 - INFO - __main__ - Step 480 Global step 480 Train loss 6.72 on epoch=239
06/24/2022 10:11:47 - INFO - __main__ - Step 490 Global step 490 Train loss 6.56 on epoch=244
06/24/2022 10:11:49 - INFO - __main__ - Step 500 Global step 500 Train loss 6.48 on epoch=249
06/24/2022 10:11:59 - INFO - __main__ - Global step 500 Train loss 6.64 ACC 0.0 on epoch=249
06/24/2022 10:12:01 - INFO - __main__ - Step 510 Global step 510 Train loss 6.34 on epoch=254
06/24/2022 10:12:02 - INFO - __main__ - Step 520 Global step 520 Train loss 6.27 on epoch=259
06/24/2022 10:12:03 - INFO - __main__ - Step 530 Global step 530 Train loss 6.04 on epoch=264
06/24/2022 10:12:05 - INFO - __main__ - Step 540 Global step 540 Train loss 5.97 on epoch=269
06/24/2022 10:12:06 - INFO - __main__ - Step 550 Global step 550 Train loss 5.89 on epoch=274
06/24/2022 10:12:16 - INFO - __main__ - Global step 550 Train loss 6.10 ACC 0.0 on epoch=274
06/24/2022 10:12:17 - INFO - __main__ - Step 560 Global step 560 Train loss 5.80 on epoch=279
06/24/2022 10:12:18 - INFO - __main__ - Step 570 Global step 570 Train loss 5.53 on epoch=284
06/24/2022 10:12:20 - INFO - __main__ - Step 580 Global step 580 Train loss 5.35 on epoch=289
06/24/2022 10:12:21 - INFO - __main__ - Step 590 Global step 590 Train loss 5.34 on epoch=294
06/24/2022 10:12:22 - INFO - __main__ - Step 600 Global step 600 Train loss 5.46 on epoch=299
06/24/2022 10:12:29 - INFO - __main__ - Global step 600 Train loss 5.50 ACC 0.0 on epoch=299
06/24/2022 10:12:30 - INFO - __main__ - Step 610 Global step 610 Train loss 5.28 on epoch=304
06/24/2022 10:12:31 - INFO - __main__ - Step 620 Global step 620 Train loss 5.00 on epoch=309
06/24/2022 10:12:33 - INFO - __main__ - Step 630 Global step 630 Train loss 5.13 on epoch=314
06/24/2022 10:12:34 - INFO - __main__ - Step 640 Global step 640 Train loss 5.17 on epoch=319
06/24/2022 10:12:35 - INFO - __main__ - Step 650 Global step 650 Train loss 5.03 on epoch=324
06/24/2022 10:12:44 - INFO - __main__ - Global step 650 Train loss 5.12 ACC 0.0 on epoch=324
06/24/2022 10:12:45 - INFO - __main__ - Step 660 Global step 660 Train loss 5.01 on epoch=329
06/24/2022 10:12:47 - INFO - __main__ - Step 670 Global step 670 Train loss 4.82 on epoch=334
06/24/2022 10:12:48 - INFO - __main__ - Step 680 Global step 680 Train loss 4.64 on epoch=339
06/24/2022 10:12:49 - INFO - __main__ - Step 690 Global step 690 Train loss 4.47 on epoch=344
06/24/2022 10:12:51 - INFO - __main__ - Step 700 Global step 700 Train loss 4.49 on epoch=349
06/24/2022 10:12:59 - INFO - __main__ - Global step 700 Train loss 4.69 ACC 0.0 on epoch=349
06/24/2022 10:13:00 - INFO - __main__ - Step 710 Global step 710 Train loss 4.46 on epoch=354
06/24/2022 10:13:02 - INFO - __main__ - Step 720 Global step 720 Train loss 4.39 on epoch=359
06/24/2022 10:13:03 - INFO - __main__ - Step 730 Global step 730 Train loss 4.39 on epoch=364
06/24/2022 10:13:04 - INFO - __main__ - Step 740 Global step 740 Train loss 4.42 on epoch=369
06/24/2022 10:13:06 - INFO - __main__ - Step 750 Global step 750 Train loss 4.42 on epoch=374
06/24/2022 10:13:16 - INFO - __main__ - Global step 750 Train loss 4.42 ACC 0.0 on epoch=374
06/24/2022 10:13:17 - INFO - __main__ - Step 760 Global step 760 Train loss 4.10 on epoch=379
06/24/2022 10:13:18 - INFO - __main__ - Step 770 Global step 770 Train loss 4.23 on epoch=384
06/24/2022 10:13:20 - INFO - __main__ - Step 780 Global step 780 Train loss 4.17 on epoch=389
06/24/2022 10:13:21 - INFO - __main__ - Step 790 Global step 790 Train loss 4.07 on epoch=394
06/24/2022 10:13:22 - INFO - __main__ - Step 800 Global step 800 Train loss 4.09 on epoch=399
06/24/2022 10:13:33 - INFO - __main__ - Global step 800 Train loss 4.13 ACC 0.0 on epoch=399
06/24/2022 10:13:34 - INFO - __main__ - Step 810 Global step 810 Train loss 4.17 on epoch=404
06/24/2022 10:13:35 - INFO - __main__ - Step 820 Global step 820 Train loss 4.12 on epoch=409
06/24/2022 10:13:37 - INFO - __main__ - Step 830 Global step 830 Train loss 4.11 on epoch=414
06/24/2022 10:13:38 - INFO - __main__ - Step 840 Global step 840 Train loss 4.01 on epoch=419
06/24/2022 10:13:39 - INFO - __main__ - Step 850 Global step 850 Train loss 4.07 on epoch=424
06/24/2022 10:13:49 - INFO - __main__ - Global step 850 Train loss 4.10 ACC 0.0 on epoch=424
06/24/2022 10:13:50 - INFO - __main__ - Step 860 Global step 860 Train loss 3.88 on epoch=429
06/24/2022 10:13:52 - INFO - __main__ - Step 870 Global step 870 Train loss 3.85 on epoch=434
06/24/2022 10:13:53 - INFO - __main__ - Step 880 Global step 880 Train loss 3.84 on epoch=439
06/24/2022 10:13:54 - INFO - __main__ - Step 890 Global step 890 Train loss 3.92 on epoch=444
06/24/2022 10:13:56 - INFO - __main__ - Step 900 Global step 900 Train loss 3.80 on epoch=449
06/24/2022 10:14:05 - INFO - __main__ - Global step 900 Train loss 3.86 ACC 0.0 on epoch=449
06/24/2022 10:14:06 - INFO - __main__ - Step 910 Global step 910 Train loss 3.74 on epoch=454
06/24/2022 10:14:08 - INFO - __main__ - Step 920 Global step 920 Train loss 3.64 on epoch=459
06/24/2022 10:14:09 - INFO - __main__ - Step 930 Global step 930 Train loss 3.59 on epoch=464
06/24/2022 10:14:10 - INFO - __main__ - Step 940 Global step 940 Train loss 3.54 on epoch=469
06/24/2022 10:14:11 - INFO - __main__ - Step 950 Global step 950 Train loss 3.49 on epoch=474
06/24/2022 10:14:21 - INFO - __main__ - Global step 950 Train loss 3.60 ACC 0.0 on epoch=474
06/24/2022 10:14:22 - INFO - __main__ - Step 960 Global step 960 Train loss 3.69 on epoch=479
06/24/2022 10:14:23 - INFO - __main__ - Step 970 Global step 970 Train loss 3.46 on epoch=484
06/24/2022 10:14:25 - INFO - __main__ - Step 980 Global step 980 Train loss 3.54 on epoch=489
06/24/2022 10:14:26 - INFO - __main__ - Step 990 Global step 990 Train loss 3.51 on epoch=494
06/24/2022 10:14:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.34 on epoch=499
06/24/2022 10:14:36 - INFO - __main__ - Global step 1000 Train loss 3.51 ACC 0.0 on epoch=499
06/24/2022 10:14:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.40 on epoch=504
06/24/2022 10:14:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.33 on epoch=509
06/24/2022 10:14:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.27 on epoch=514
06/24/2022 10:14:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.24 on epoch=519
06/24/2022 10:14:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.15 on epoch=524
06/24/2022 10:14:48 - INFO - __main__ - Global step 1050 Train loss 3.28 ACC 0.0 on epoch=524
06/24/2022 10:14:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.24 on epoch=529
06/24/2022 10:14:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.26 on epoch=534
06/24/2022 10:14:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.23 on epoch=539
06/24/2022 10:14:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.00 on epoch=544
06/24/2022 10:14:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.20 on epoch=549
06/24/2022 10:15:04 - INFO - __main__ - Global step 1100 Train loss 3.19 ACC 0.0 on epoch=549
06/24/2022 10:15:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.12 on epoch=554
06/24/2022 10:15:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.16 on epoch=559
06/24/2022 10:15:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.03 on epoch=564
06/24/2022 10:15:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.20 on epoch=569
06/24/2022 10:15:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.23 on epoch=574
06/24/2022 10:15:18 - INFO - __main__ - Global step 1150 Train loss 3.15 ACC 0.0 on epoch=574
06/24/2022 10:15:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.13 on epoch=579
06/24/2022 10:15:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.18 on epoch=584
06/24/2022 10:15:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.20 on epoch=589
06/24/2022 10:15:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.89 on epoch=594
06/24/2022 10:15:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.09 on epoch=599
06/24/2022 10:15:35 - INFO - __main__ - Global step 1200 Train loss 3.10 ACC 0.0 on epoch=599
06/24/2022 10:15:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.83 on epoch=604
06/24/2022 10:15:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.95 on epoch=609
06/24/2022 10:15:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.82 on epoch=614
06/24/2022 10:15:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.80 on epoch=619
06/24/2022 10:15:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.85 on epoch=624
06/24/2022 10:15:50 - INFO - __main__ - Global step 1250 Train loss 2.85 ACC 0.0 on epoch=624
06/24/2022 10:15:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.79 on epoch=629
06/24/2022 10:15:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.90 on epoch=634
06/24/2022 10:15:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.63 on epoch=639
06/24/2022 10:15:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.76 on epoch=644
06/24/2022 10:15:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.63 on epoch=649
06/24/2022 10:16:06 - INFO - __main__ - Global step 1300 Train loss 2.74 ACC 0.0625 on epoch=649
06/24/2022 10:16:06 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=649, global_step=1300
06/24/2022 10:16:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.63 on epoch=654
06/24/2022 10:16:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.57 on epoch=659
06/24/2022 10:16:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.61 on epoch=664
06/24/2022 10:16:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.59 on epoch=669
06/24/2022 10:16:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.46 on epoch=674
06/24/2022 10:16:20 - INFO - __main__ - Global step 1350 Train loss 2.57 ACC 0.34375 on epoch=674
06/24/2022 10:16:20 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.34375 on epoch=674, global_step=1350
06/24/2022 10:16:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.58 on epoch=679
06/24/2022 10:16:23 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.43 on epoch=684
06/24/2022 10:16:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.40 on epoch=689
06/24/2022 10:16:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.40 on epoch=694
06/24/2022 10:16:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.27 on epoch=699
06/24/2022 10:16:37 - INFO - __main__ - Global step 1400 Train loss 2.41 ACC 0.46875 on epoch=699
06/24/2022 10:16:37 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.46875 on epoch=699, global_step=1400
06/24/2022 10:16:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.44 on epoch=704
06/24/2022 10:16:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.18 on epoch=709
06/24/2022 10:16:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.13 on epoch=714
06/24/2022 10:16:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.12 on epoch=719
06/24/2022 10:16:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.08 on epoch=724
06/24/2022 10:16:55 - INFO - __main__ - Global step 1450 Train loss 2.19 ACC 0.4375 on epoch=724
06/24/2022 10:16:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.10 on epoch=729
06/24/2022 10:16:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.00 on epoch=734
06/24/2022 10:16:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.26 on epoch=739
06/24/2022 10:17:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.88 on epoch=744
06/24/2022 10:17:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.21 on epoch=749
06/24/2022 10:17:12 - INFO - __main__ - Global step 1500 Train loss 2.09 ACC 0.5 on epoch=749
06/24/2022 10:17:12 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=749, global_step=1500
06/24/2022 10:17:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.16 on epoch=754
06/24/2022 10:17:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.01 on epoch=759
06/24/2022 10:17:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 2.01 on epoch=764
06/24/2022 10:17:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.84 on epoch=769
06/24/2022 10:17:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.89 on epoch=774
06/24/2022 10:17:29 - INFO - __main__ - Global step 1550 Train loss 1.98 ACC 0.5 on epoch=774
06/24/2022 10:17:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.77 on epoch=779
06/24/2022 10:17:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.88 on epoch=784
06/24/2022 10:17:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.73 on epoch=789
06/24/2022 10:17:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.81 on epoch=794
06/24/2022 10:17:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.75 on epoch=799
06/24/2022 10:17:46 - INFO - __main__ - Global step 1600 Train loss 1.79 ACC 0.5 on epoch=799
06/24/2022 10:17:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.70 on epoch=804
06/24/2022 10:17:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.67 on epoch=809
06/24/2022 10:17:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.73 on epoch=814
06/24/2022 10:17:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.70 on epoch=819
06/24/2022 10:17:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.51 on epoch=824
06/24/2022 10:18:02 - INFO - __main__ - Global step 1650 Train loss 1.66 ACC 0.5 on epoch=824
06/24/2022 10:18:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.54 on epoch=829
06/24/2022 10:18:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.48 on epoch=834
06/24/2022 10:18:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.56 on epoch=839
06/24/2022 10:18:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.48 on epoch=844
06/24/2022 10:18:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.51 on epoch=849
06/24/2022 10:18:19 - INFO - __main__ - Global step 1700 Train loss 1.52 ACC 0.5 on epoch=849
06/24/2022 10:18:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.39 on epoch=854
06/24/2022 10:18:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.16 on epoch=859
06/24/2022 10:18:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.43 on epoch=864
06/24/2022 10:18:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.27 on epoch=869
06/24/2022 10:18:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.28 on epoch=874
06/24/2022 10:18:36 - INFO - __main__ - Global step 1750 Train loss 1.30 ACC 0.5 on epoch=874
06/24/2022 10:18:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.40 on epoch=879
06/24/2022 10:18:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.24 on epoch=884
06/24/2022 10:18:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.31 on epoch=889
06/24/2022 10:18:41 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.20 on epoch=894
06/24/2022 10:18:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.22 on epoch=899
06/24/2022 10:18:53 - INFO - __main__ - Global step 1800 Train loss 1.27 ACC 0.5 on epoch=899
06/24/2022 10:18:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.22 on epoch=904
06/24/2022 10:18:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.13 on epoch=909
06/24/2022 10:18:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.23 on epoch=914
06/24/2022 10:18:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.26 on epoch=919
06/24/2022 10:18:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.09 on epoch=924
06/24/2022 10:19:09 - INFO - __main__ - Global step 1850 Train loss 1.19 ACC 0.5 on epoch=924
06/24/2022 10:19:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.15 on epoch=929
06/24/2022 10:19:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.13 on epoch=934
06/24/2022 10:19:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.13 on epoch=939
06/24/2022 10:19:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.33 on epoch=944
06/24/2022 10:19:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.06 on epoch=949
06/24/2022 10:19:26 - INFO - __main__ - Global step 1900 Train loss 1.16 ACC 0.46875 on epoch=949
06/24/2022 10:19:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.12 on epoch=954
06/24/2022 10:19:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.14 on epoch=959
06/24/2022 10:19:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.07 on epoch=964
06/24/2022 10:19:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.01 on epoch=969
06/24/2022 10:19:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.09 on epoch=974
06/24/2022 10:19:43 - INFO - __main__ - Global step 1950 Train loss 1.09 ACC 0.5 on epoch=974
06/24/2022 10:19:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.99 on epoch=979
06/24/2022 10:19:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.90 on epoch=984
06/24/2022 10:19:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.96 on epoch=989
06/24/2022 10:19:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.96 on epoch=994
06/24/2022 10:19:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.87 on epoch=999
06/24/2022 10:20:00 - INFO - __main__ - Global step 2000 Train loss 0.94 ACC 0.5 on epoch=999
06/24/2022 10:20:00 - INFO - __main__ - save last model!
06/24/2022 10:20:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 10:20:00 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 10:20:00 - INFO - __main__ - Printing 3 examples
06/24/2022 10:20:00 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 10:20:00 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:20:00 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 10:20:00 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:20:00 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 10:20:00 - INFO - __main__ - ['Dissimilar']
06/24/2022 10:20:00 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:20:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:20:01 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 10:23:21 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/medical_questions_pairs_16_87_0.2_8_predictions.txt
06/24/2022 10:23:21 - INFO - __main__ - ACC on test data: 0.5148
06/24/2022 10:23:21 - INFO - __main__ - prefix=medical_questions_pairs_16_87, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.5147540983606558
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (91053): No such process
Task: paws, Checkpoint: models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt, Identifier: T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10
Output directory () already exists and is not empty.
06/24/2022 10:23:26 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/24/2022 10:23:26 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws
06/24/2022 10:23:26 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/24/2022 10:23:26 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws
06/24/2022 10:23:28 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/24/2022 10:23:28 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/24/2022 10:23:28 - INFO - __main__ - args.device: cuda:1
06/24/2022 10:23:28 - INFO - __main__ - args.device: cuda:0
06/24/2022 10:23:28 - INFO - __main__ - Using 2 gpus
06/24/2022 10:23:28 - INFO - __main__ - Using 2 gpus
06/24/2022 10:23:28 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/24/2022 10:23:28 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/24/2022 10:23:33 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.5, bsz=8 ...
06/24/2022 10:23:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:23:34 - INFO - __main__ - Printing 3 examples
06/24/2022 10:23:34 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 10:23:34 - INFO - __main__ - ['1']
06/24/2022 10:23:34 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 10:23:34 - INFO - __main__ - ['1']
06/24/2022 10:23:34 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 10:23:34 - INFO - __main__ - ['1']
06/24/2022 10:23:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 10:23:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:23:34 - INFO - __main__ - Printing 3 examples
06/24/2022 10:23:34 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 10:23:34 - INFO - __main__ - ['1']
06/24/2022 10:23:34 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 10:23:34 - INFO - __main__ - ['1']
06/24/2022 10:23:34 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 10:23:34 - INFO - __main__ - ['1']
06/24/2022 10:23:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:23:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 10:23:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:23:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:23:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:23:34 - INFO - __main__ - Printing 3 examples
06/24/2022 10:23:34 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 10:23:34 - INFO - __main__ - ['1']
06/24/2022 10:23:34 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 10:23:34 - INFO - __main__ - ['1']
06/24/2022 10:23:34 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 10:23:34 - INFO - __main__ - ['1']
06/24/2022 10:23:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:23:34 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:23:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:23:34 - INFO - __main__ - Printing 3 examples
06/24/2022 10:23:34 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 10:23:34 - INFO - __main__ - ['1']
06/24/2022 10:23:34 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 10:23:34 - INFO - __main__ - ['1']
06/24/2022 10:23:34 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 10:23:34 - INFO - __main__ - ['1']
06/24/2022 10:23:34 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:23:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:23:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:23:34 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:23:34 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:23:40 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:23:40 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:23:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:23:40 - INFO - __main__ - Starting training!
06/24/2022 10:23:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:23:45 - INFO - __main__ - Starting training!
06/24/2022 10:23:47 - INFO - __main__ - Step 10 Global step 10 Train loss 6.05 on epoch=4
06/24/2022 10:23:48 - INFO - __main__ - Step 20 Global step 20 Train loss 5.99 on epoch=9
06/24/2022 10:23:50 - INFO - __main__ - Step 30 Global step 30 Train loss 5.95 on epoch=14
06/24/2022 10:23:51 - INFO - __main__ - Step 40 Global step 40 Train loss 5.92 on epoch=19
06/24/2022 10:23:52 - INFO - __main__ - Step 50 Global step 50 Train loss 5.97 on epoch=24
06/24/2022 10:23:54 - INFO - __main__ - Global step 50 Train loss 5.98 Classification-F1 0.0 on epoch=24
06/24/2022 10:23:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 10:23:55 - INFO - __main__ - Step 60 Global step 60 Train loss 5.95 on epoch=29
06/24/2022 10:23:56 - INFO - __main__ - Step 70 Global step 70 Train loss 5.85 on epoch=34
06/24/2022 10:23:57 - INFO - __main__ - Step 80 Global step 80 Train loss 5.77 on epoch=39
06/24/2022 10:23:59 - INFO - __main__ - Step 90 Global step 90 Train loss 5.66 on epoch=44
06/24/2022 10:24:00 - INFO - __main__ - Step 100 Global step 100 Train loss 5.70 on epoch=49
06/24/2022 10:24:01 - INFO - __main__ - Global step 100 Train loss 5.79 Classification-F1 0.0 on epoch=49
06/24/2022 10:24:02 - INFO - __main__ - Step 110 Global step 110 Train loss 5.76 on epoch=54
06/24/2022 10:24:03 - INFO - __main__ - Step 120 Global step 120 Train loss 5.62 on epoch=59
06/24/2022 10:24:04 - INFO - __main__ - Step 130 Global step 130 Train loss 5.75 on epoch=64
06/24/2022 10:24:05 - INFO - __main__ - Step 140 Global step 140 Train loss 5.69 on epoch=69
06/24/2022 10:24:06 - INFO - __main__ - Step 150 Global step 150 Train loss 5.68 on epoch=74
06/24/2022 10:24:07 - INFO - __main__ - Global step 150 Train loss 5.70 Classification-F1 0.0 on epoch=74
06/24/2022 10:24:08 - INFO - __main__ - Step 160 Global step 160 Train loss 5.54 on epoch=79
06/24/2022 10:24:10 - INFO - __main__ - Step 170 Global step 170 Train loss 5.47 on epoch=84
06/24/2022 10:24:11 - INFO - __main__ - Step 180 Global step 180 Train loss 5.43 on epoch=89
06/24/2022 10:24:12 - INFO - __main__ - Step 190 Global step 190 Train loss 5.39 on epoch=94
06/24/2022 10:24:13 - INFO - __main__ - Step 200 Global step 200 Train loss 5.47 on epoch=99
06/24/2022 10:24:14 - INFO - __main__ - Global step 200 Train loss 5.46 Classification-F1 0.0 on epoch=99
06/24/2022 10:24:15 - INFO - __main__ - Step 210 Global step 210 Train loss 5.41 on epoch=104
06/24/2022 10:24:17 - INFO - __main__ - Step 220 Global step 220 Train loss 5.34 on epoch=109
06/24/2022 10:24:18 - INFO - __main__ - Step 230 Global step 230 Train loss 5.30 on epoch=114
06/24/2022 10:24:19 - INFO - __main__ - Step 240 Global step 240 Train loss 5.30 on epoch=119
06/24/2022 10:24:20 - INFO - __main__ - Step 250 Global step 250 Train loss 5.28 on epoch=124
06/24/2022 10:24:27 - INFO - __main__ - Global step 250 Train loss 5.33 Classification-F1 0.0 on epoch=124
06/24/2022 10:24:28 - INFO - __main__ - Step 260 Global step 260 Train loss 5.18 on epoch=129
06/24/2022 10:24:29 - INFO - __main__ - Step 270 Global step 270 Train loss 5.06 on epoch=134
06/24/2022 10:24:31 - INFO - __main__ - Step 280 Global step 280 Train loss 5.01 on epoch=139
06/24/2022 10:24:32 - INFO - __main__ - Step 290 Global step 290 Train loss 4.95 on epoch=144
06/24/2022 10:24:33 - INFO - __main__ - Step 300 Global step 300 Train loss 4.95 on epoch=149
06/24/2022 10:24:35 - INFO - __main__ - Global step 300 Train loss 5.03 Classification-F1 0.0 on epoch=149
06/24/2022 10:24:36 - INFO - __main__ - Step 310 Global step 310 Train loss 4.94 on epoch=154
06/24/2022 10:24:37 - INFO - __main__ - Step 320 Global step 320 Train loss 4.88 on epoch=159
06/24/2022 10:24:39 - INFO - __main__ - Step 330 Global step 330 Train loss 4.82 on epoch=164
06/24/2022 10:24:40 - INFO - __main__ - Step 340 Global step 340 Train loss 4.85 on epoch=169
06/24/2022 10:24:41 - INFO - __main__ - Step 350 Global step 350 Train loss 4.55 on epoch=174
06/24/2022 10:24:52 - INFO - __main__ - Global step 350 Train loss 4.81 Classification-F1 0.0 on epoch=174
06/24/2022 10:24:53 - INFO - __main__ - Step 360 Global step 360 Train loss 4.51 on epoch=179
06/24/2022 10:24:54 - INFO - __main__ - Step 370 Global step 370 Train loss 4.54 on epoch=184
06/24/2022 10:24:55 - INFO - __main__ - Step 380 Global step 380 Train loss 4.43 on epoch=189
06/24/2022 10:24:56 - INFO - __main__ - Step 390 Global step 390 Train loss 4.32 on epoch=194
06/24/2022 10:24:58 - INFO - __main__ - Step 400 Global step 400 Train loss 4.27 on epoch=199
06/24/2022 10:25:00 - INFO - __main__ - Global step 400 Train loss 4.42 Classification-F1 0.0 on epoch=199
06/24/2022 10:25:01 - INFO - __main__ - Step 410 Global step 410 Train loss 4.32 on epoch=204
06/24/2022 10:25:02 - INFO - __main__ - Step 420 Global step 420 Train loss 4.25 on epoch=209
06/24/2022 10:25:03 - INFO - __main__ - Step 430 Global step 430 Train loss 4.00 on epoch=214
06/24/2022 10:25:04 - INFO - __main__ - Step 440 Global step 440 Train loss 4.18 on epoch=219
06/24/2022 10:25:06 - INFO - __main__ - Step 450 Global step 450 Train loss 4.13 on epoch=224
06/24/2022 10:25:16 - INFO - __main__ - Global step 450 Train loss 4.18 Classification-F1 0.0 on epoch=224
06/24/2022 10:25:17 - INFO - __main__ - Step 460 Global step 460 Train loss 3.91 on epoch=229
06/24/2022 10:25:18 - INFO - __main__ - Step 470 Global step 470 Train loss 3.96 on epoch=234
06/24/2022 10:25:20 - INFO - __main__ - Step 480 Global step 480 Train loss 3.88 on epoch=239
06/24/2022 10:25:21 - INFO - __main__ - Step 490 Global step 490 Train loss 3.86 on epoch=244
06/24/2022 10:25:22 - INFO - __main__ - Step 500 Global step 500 Train loss 3.76 on epoch=249
06/24/2022 10:25:24 - INFO - __main__ - Global step 500 Train loss 3.87 Classification-F1 0.0 on epoch=249
06/24/2022 10:25:25 - INFO - __main__ - Step 510 Global step 510 Train loss 3.69 on epoch=254
06/24/2022 10:25:26 - INFO - __main__ - Step 520 Global step 520 Train loss 3.55 on epoch=259
06/24/2022 10:25:27 - INFO - __main__ - Step 530 Global step 530 Train loss 3.46 on epoch=264
06/24/2022 10:25:28 - INFO - __main__ - Step 540 Global step 540 Train loss 3.54 on epoch=269
06/24/2022 10:25:29 - INFO - __main__ - Step 550 Global step 550 Train loss 3.43 on epoch=274
06/24/2022 10:25:36 - INFO - __main__ - Global step 550 Train loss 3.53 Classification-F1 0.0 on epoch=274
06/24/2022 10:25:37 - INFO - __main__ - Step 560 Global step 560 Train loss 3.25 on epoch=279
06/24/2022 10:25:38 - INFO - __main__ - Step 570 Global step 570 Train loss 3.29 on epoch=284
06/24/2022 10:25:39 - INFO - __main__ - Step 580 Global step 580 Train loss 3.27 on epoch=289
06/24/2022 10:25:40 - INFO - __main__ - Step 590 Global step 590 Train loss 3.24 on epoch=294
06/24/2022 10:25:42 - INFO - __main__ - Step 600 Global step 600 Train loss 3.22 on epoch=299
06/24/2022 10:25:49 - INFO - __main__ - Global step 600 Train loss 3.25 Classification-F1 0.0 on epoch=299
06/24/2022 10:25:50 - INFO - __main__ - Step 610 Global step 610 Train loss 3.11 on epoch=304
06/24/2022 10:25:51 - INFO - __main__ - Step 620 Global step 620 Train loss 3.10 on epoch=309
06/24/2022 10:25:52 - INFO - __main__ - Step 630 Global step 630 Train loss 3.02 on epoch=314
06/24/2022 10:25:53 - INFO - __main__ - Step 640 Global step 640 Train loss 2.92 on epoch=319
06/24/2022 10:25:55 - INFO - __main__ - Step 650 Global step 650 Train loss 2.93 on epoch=324
06/24/2022 10:25:57 - INFO - __main__ - Global step 650 Train loss 3.02 Classification-F1 0.061728395061728385 on epoch=324
06/24/2022 10:25:57 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.061728395061728385 on epoch=324, global_step=650
06/24/2022 10:25:58 - INFO - __main__ - Step 660 Global step 660 Train loss 2.66 on epoch=329
06/24/2022 10:25:59 - INFO - __main__ - Step 670 Global step 670 Train loss 2.73 on epoch=334
06/24/2022 10:26:00 - INFO - __main__ - Step 680 Global step 680 Train loss 2.63 on epoch=339
06/24/2022 10:26:02 - INFO - __main__ - Step 690 Global step 690 Train loss 2.64 on epoch=344
06/24/2022 10:26:03 - INFO - __main__ - Step 700 Global step 700 Train loss 2.45 on epoch=349
06/24/2022 10:26:05 - INFO - __main__ - Global step 700 Train loss 2.62 Classification-F1 0.20155038759689922 on epoch=349
06/24/2022 10:26:05 - INFO - __main__ - Saving model with best Classification-F1: 0.061728395061728385 -> 0.20155038759689922 on epoch=349, global_step=700
06/24/2022 10:26:06 - INFO - __main__ - Step 710 Global step 710 Train loss 2.53 on epoch=354
06/24/2022 10:26:07 - INFO - __main__ - Step 720 Global step 720 Train loss 2.52 on epoch=359
06/24/2022 10:26:08 - INFO - __main__ - Step 730 Global step 730 Train loss 2.46 on epoch=364
06/24/2022 10:26:09 - INFO - __main__ - Step 740 Global step 740 Train loss 2.27 on epoch=369
06/24/2022 10:26:11 - INFO - __main__ - Step 750 Global step 750 Train loss 2.26 on epoch=374
06/24/2022 10:26:13 - INFO - __main__ - Global step 750 Train loss 2.41 Classification-F1 0.3333333333333333 on epoch=374
06/24/2022 10:26:14 - INFO - __main__ - Saving model with best Classification-F1: 0.20155038759689922 -> 0.3333333333333333 on epoch=374, global_step=750
06/24/2022 10:26:15 - INFO - __main__ - Step 760 Global step 760 Train loss 2.33 on epoch=379
06/24/2022 10:26:16 - INFO - __main__ - Step 770 Global step 770 Train loss 2.17 on epoch=384
06/24/2022 10:26:17 - INFO - __main__ - Step 780 Global step 780 Train loss 2.10 on epoch=389
06/24/2022 10:26:18 - INFO - __main__ - Step 790 Global step 790 Train loss 2.04 on epoch=394
06/24/2022 10:26:19 - INFO - __main__ - Step 800 Global step 800 Train loss 1.96 on epoch=399
06/24/2022 10:26:21 - INFO - __main__ - Global step 800 Train loss 2.12 Classification-F1 0.43529411764705883 on epoch=399
06/24/2022 10:26:21 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.43529411764705883 on epoch=399, global_step=800
06/24/2022 10:26:22 - INFO - __main__ - Step 810 Global step 810 Train loss 1.91 on epoch=404
06/24/2022 10:26:23 - INFO - __main__ - Step 820 Global step 820 Train loss 1.89 on epoch=409
06/24/2022 10:26:25 - INFO - __main__ - Step 830 Global step 830 Train loss 1.70 on epoch=414
06/24/2022 10:26:26 - INFO - __main__ - Step 840 Global step 840 Train loss 1.77 on epoch=419
06/24/2022 10:26:27 - INFO - __main__ - Step 850 Global step 850 Train loss 1.77 on epoch=424
06/24/2022 10:26:27 - INFO - __main__ - Global step 850 Train loss 1.81 Classification-F1 0.3992490613266583 on epoch=424
06/24/2022 10:26:29 - INFO - __main__ - Step 860 Global step 860 Train loss 1.54 on epoch=429
06/24/2022 10:26:30 - INFO - __main__ - Step 870 Global step 870 Train loss 1.76 on epoch=434
06/24/2022 10:26:31 - INFO - __main__ - Step 880 Global step 880 Train loss 1.50 on epoch=439
06/24/2022 10:26:32 - INFO - __main__ - Step 890 Global step 890 Train loss 1.51 on epoch=444
06/24/2022 10:26:33 - INFO - __main__ - Step 900 Global step 900 Train loss 1.48 on epoch=449
06/24/2022 10:26:34 - INFO - __main__ - Global step 900 Train loss 1.56 Classification-F1 0.3333333333333333 on epoch=449
06/24/2022 10:26:35 - INFO - __main__ - Step 910 Global step 910 Train loss 1.23 on epoch=454
06/24/2022 10:26:37 - INFO - __main__ - Step 920 Global step 920 Train loss 1.25 on epoch=459
06/24/2022 10:26:38 - INFO - __main__ - Step 930 Global step 930 Train loss 1.21 on epoch=464
06/24/2022 10:26:39 - INFO - __main__ - Step 940 Global step 940 Train loss 1.04 on epoch=469
06/24/2022 10:26:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.91 on epoch=474
06/24/2022 10:26:40 - INFO - __main__ - Global step 950 Train loss 1.13 Classification-F1 0.3992490613266583 on epoch=474
06/24/2022 10:26:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.95 on epoch=479
06/24/2022 10:26:43 - INFO - __main__ - Step 970 Global step 970 Train loss 1.02 on epoch=484
06/24/2022 10:26:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.94 on epoch=489
06/24/2022 10:26:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.82 on epoch=494
06/24/2022 10:26:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.94 on epoch=499
06/24/2022 10:26:47 - INFO - __main__ - Global step 1000 Train loss 0.93 Classification-F1 0.3333333333333333 on epoch=499
06/24/2022 10:26:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.83 on epoch=504
06/24/2022 10:26:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.89 on epoch=509
06/24/2022 10:26:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.87 on epoch=514
06/24/2022 10:26:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.94 on epoch=519
06/24/2022 10:26:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.89 on epoch=524
06/24/2022 10:26:53 - INFO - __main__ - Global step 1050 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=524
06/24/2022 10:26:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.78 on epoch=529
06/24/2022 10:26:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.85 on epoch=534
06/24/2022 10:26:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.77 on epoch=539
06/24/2022 10:26:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.75 on epoch=544
06/24/2022 10:26:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.77 on epoch=549
06/24/2022 10:26:59 - INFO - __main__ - Global step 1100 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=549
06/24/2022 10:27:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.76 on epoch=554
06/24/2022 10:27:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.74 on epoch=559
06/24/2022 10:27:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.82 on epoch=564
06/24/2022 10:27:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.76 on epoch=569
06/24/2022 10:27:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.72 on epoch=574
06/24/2022 10:27:06 - INFO - __main__ - Global step 1150 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=574
06/24/2022 10:27:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.69 on epoch=579
06/24/2022 10:27:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.76 on epoch=584
06/24/2022 10:27:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.70 on epoch=589
06/24/2022 10:27:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.67 on epoch=594
06/24/2022 10:27:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.74 on epoch=599
06/24/2022 10:27:12 - INFO - __main__ - Global step 1200 Train loss 0.71 Classification-F1 0.3191489361702127 on epoch=599
06/24/2022 10:27:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.70 on epoch=604
06/24/2022 10:27:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.70 on epoch=609
06/24/2022 10:27:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.71 on epoch=614
06/24/2022 10:27:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.65 on epoch=619
06/24/2022 10:27:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.71 on epoch=624
06/24/2022 10:27:18 - INFO - __main__ - Global step 1250 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=624
06/24/2022 10:27:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.77 on epoch=629
06/24/2022 10:27:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.68 on epoch=634
06/24/2022 10:27:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.73 on epoch=639
06/24/2022 10:27:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.73 on epoch=644
06/24/2022 10:27:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.63 on epoch=649
06/24/2022 10:27:25 - INFO - __main__ - Global step 1300 Train loss 0.70 Classification-F1 0.3333333333333333 on epoch=649
06/24/2022 10:27:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.73 on epoch=654
06/24/2022 10:27:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.67 on epoch=659
06/24/2022 10:27:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.68 on epoch=664
06/24/2022 10:27:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.65 on epoch=669
06/24/2022 10:27:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.72 on epoch=674
06/24/2022 10:27:32 - INFO - __main__ - Global step 1350 Train loss 0.69 Classification-F1 0.3454545454545454 on epoch=674
06/24/2022 10:27:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.67 on epoch=679
06/24/2022 10:27:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.64 on epoch=684
06/24/2022 10:27:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.55 on epoch=689
06/24/2022 10:27:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.65 on epoch=694
06/24/2022 10:27:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.69 on epoch=699
06/24/2022 10:27:38 - INFO - __main__ - Global step 1400 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=699
06/24/2022 10:27:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.64 on epoch=704
06/24/2022 10:27:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.73 on epoch=709
06/24/2022 10:27:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.72 on epoch=714
06/24/2022 10:27:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.74 on epoch=719
06/24/2022 10:27:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.66 on epoch=724
06/24/2022 10:27:45 - INFO - __main__ - Global step 1450 Train loss 0.70 Classification-F1 0.3333333333333333 on epoch=724
06/24/2022 10:27:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.70 on epoch=729
06/24/2022 10:27:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.66 on epoch=734
06/24/2022 10:27:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.66 on epoch=739
06/24/2022 10:27:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.58 on epoch=744
06/24/2022 10:27:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.66 on epoch=749
06/24/2022 10:27:51 - INFO - __main__ - Global step 1500 Train loss 0.65 Classification-F1 0.3992490613266583 on epoch=749
06/24/2022 10:27:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.53 on epoch=754
06/24/2022 10:27:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.68 on epoch=759
06/24/2022 10:27:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.64 on epoch=764
06/24/2022 10:27:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.63 on epoch=769
06/24/2022 10:27:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.62 on epoch=774
06/24/2022 10:27:58 - INFO - __main__ - Global step 1550 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=774
06/24/2022 10:27:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.63 on epoch=779
06/24/2022 10:28:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.54 on epoch=784
06/24/2022 10:28:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.68 on epoch=789
06/24/2022 10:28:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.56 on epoch=794
06/24/2022 10:28:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.60 on epoch=799
06/24/2022 10:28:05 - INFO - __main__ - Global step 1600 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=799
06/24/2022 10:28:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.57 on epoch=804
06/24/2022 10:28:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.62 on epoch=809
06/24/2022 10:28:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.57 on epoch=814
06/24/2022 10:28:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.58 on epoch=819
06/24/2022 10:28:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.62 on epoch=824
06/24/2022 10:28:11 - INFO - __main__ - Global step 1650 Train loss 0.59 Classification-F1 0.3191489361702127 on epoch=824
06/24/2022 10:28:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.57 on epoch=829
06/24/2022 10:28:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.60 on epoch=834
06/24/2022 10:28:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.58 on epoch=839
06/24/2022 10:28:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.64 on epoch=844
06/24/2022 10:28:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.52 on epoch=849
06/24/2022 10:28:18 - INFO - __main__ - Global step 1700 Train loss 0.58 Classification-F1 0.3191489361702127 on epoch=849
06/24/2022 10:28:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.60 on epoch=854
06/24/2022 10:28:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.61 on epoch=859
06/24/2022 10:28:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.70 on epoch=864
06/24/2022 10:28:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.52 on epoch=869
06/24/2022 10:28:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.55 on epoch=874
06/24/2022 10:28:25 - INFO - __main__ - Global step 1750 Train loss 0.59 Classification-F1 0.4385964912280702 on epoch=874
06/24/2022 10:28:25 - INFO - __main__ - Saving model with best Classification-F1: 0.43529411764705883 -> 0.4385964912280702 on epoch=874, global_step=1750
06/24/2022 10:28:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.55 on epoch=879
06/24/2022 10:28:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.54 on epoch=884
06/24/2022 10:28:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.63 on epoch=889
06/24/2022 10:28:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.55 on epoch=894
06/24/2022 10:28:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.48 on epoch=899
06/24/2022 10:28:31 - INFO - __main__ - Global step 1800 Train loss 0.55 Classification-F1 0.39999999999999997 on epoch=899
06/24/2022 10:28:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.54 on epoch=904
06/24/2022 10:28:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.52 on epoch=909
06/24/2022 10:28:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.50 on epoch=914
06/24/2022 10:28:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.45 on epoch=919
06/24/2022 10:28:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.56 on epoch=924
06/24/2022 10:28:38 - INFO - __main__ - Global step 1850 Train loss 0.51 Classification-F1 0.4909862142099682 on epoch=924
06/24/2022 10:28:38 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.4909862142099682 on epoch=924, global_step=1850
06/24/2022 10:28:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.62 on epoch=929
06/24/2022 10:28:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.57 on epoch=934
06/24/2022 10:28:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.53 on epoch=939
06/24/2022 10:28:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=944
06/24/2022 10:28:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.55 on epoch=949
06/24/2022 10:28:44 - INFO - __main__ - Global step 1900 Train loss 0.55 Classification-F1 0.5270935960591133 on epoch=949
06/24/2022 10:28:44 - INFO - __main__ - Saving model with best Classification-F1: 0.4909862142099682 -> 0.5270935960591133 on epoch=949, global_step=1900
06/24/2022 10:28:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.60 on epoch=954
06/24/2022 10:28:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.55 on epoch=959
06/24/2022 10:28:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.49 on epoch=964
06/24/2022 10:28:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.59 on epoch=969
06/24/2022 10:28:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=974
06/24/2022 10:28:51 - INFO - __main__ - Global step 1950 Train loss 0.54 Classification-F1 0.3764102564102564 on epoch=974
06/24/2022 10:28:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.57 on epoch=979
06/24/2022 10:28:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.54 on epoch=984
06/24/2022 10:28:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.55 on epoch=989
06/24/2022 10:28:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.49 on epoch=994
06/24/2022 10:28:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=999
06/24/2022 10:28:58 - INFO - __main__ - Global step 2000 Train loss 0.52 Classification-F1 0.3992490613266583 on epoch=999
06/24/2022 10:28:58 - INFO - __main__ - save last model!
06/24/2022 10:28:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 10:28:58 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 10:28:58 - INFO - __main__ - Printing 3 examples
06/24/2022 10:28:58 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 10:28:58 - INFO - __main__ - ['0']
06/24/2022 10:28:58 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 10:28:58 - INFO - __main__ - ['1']
06/24/2022 10:28:58 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 10:28:58 - INFO - __main__ - ['1']
06/24/2022 10:28:58 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 10:28:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:28:58 - INFO - __main__ - Printing 3 examples
06/24/2022 10:28:58 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 10:28:58 - INFO - __main__ - ['1']
06/24/2022 10:28:58 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 10:28:58 - INFO - __main__ - ['1']
06/24/2022 10:28:58 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 10:28:58 - INFO - __main__ - ['1']
06/24/2022 10:28:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 10:28:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:28:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:28:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:28:58 - INFO - __main__ - Printing 3 examples
06/24/2022 10:28:58 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 10:28:58 - INFO - __main__ - ['1']
06/24/2022 10:28:58 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 10:28:58 - INFO - __main__ - ['1']
06/24/2022 10:28:58 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 10:28:58 - INFO - __main__ - ['1']
06/24/2022 10:28:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:28:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:28:59 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:29:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:29:04 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:29:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:29:04 - INFO - __main__ - Starting training!
06/24/2022 10:29:10 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 10:30:34 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_100_0.5_8_predictions.txt
06/24/2022 10:30:34 - INFO - __main__ - Classification-F1 on test data: 0.3288
06/24/2022 10:30:34 - INFO - __main__ - prefix=paws_16_100, lr=0.5, bsz=8, dev_performance=0.5270935960591133, test_performance=0.32878269385106845
06/24/2022 10:30:34 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.4, bsz=8 ...
06/24/2022 10:30:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:30:35 - INFO - __main__ - Printing 3 examples
06/24/2022 10:30:35 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 10:30:35 - INFO - __main__ - ['1']
06/24/2022 10:30:35 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 10:30:35 - INFO - __main__ - ['1']
06/24/2022 10:30:35 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 10:30:35 - INFO - __main__ - ['1']
06/24/2022 10:30:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 10:30:36 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:30:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:30:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:30:36 - INFO - __main__ - Printing 3 examples
06/24/2022 10:30:36 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 10:30:36 - INFO - __main__ - ['1']
06/24/2022 10:30:36 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 10:30:36 - INFO - __main__ - ['1']
06/24/2022 10:30:36 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 10:30:36 - INFO - __main__ - ['1']
06/24/2022 10:30:36 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:30:36 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:30:36 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:30:42 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:30:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:30:42 - INFO - __main__ - Starting training!
06/24/2022 10:30:44 - INFO - __main__ - Step 10 Global step 10 Train loss 6.01 on epoch=4
06/24/2022 10:30:45 - INFO - __main__ - Step 20 Global step 20 Train loss 5.92 on epoch=9
06/24/2022 10:30:46 - INFO - __main__ - Step 30 Global step 30 Train loss 5.89 on epoch=14
06/24/2022 10:30:47 - INFO - __main__ - Step 40 Global step 40 Train loss 5.90 on epoch=19
06/24/2022 10:30:49 - INFO - __main__ - Step 50 Global step 50 Train loss 5.96 on epoch=24
06/24/2022 10:30:51 - INFO - __main__ - Global step 50 Train loss 5.94 Classification-F1 0.0 on epoch=24
06/24/2022 10:30:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 10:30:52 - INFO - __main__ - Step 60 Global step 60 Train loss 5.81 on epoch=29
06/24/2022 10:30:53 - INFO - __main__ - Step 70 Global step 70 Train loss 5.79 on epoch=34
06/24/2022 10:30:55 - INFO - __main__ - Step 80 Global step 80 Train loss 5.85 on epoch=39
06/24/2022 10:30:56 - INFO - __main__ - Step 90 Global step 90 Train loss 5.85 on epoch=44
06/24/2022 10:30:57 - INFO - __main__ - Step 100 Global step 100 Train loss 5.70 on epoch=49
06/24/2022 10:30:59 - INFO - __main__ - Global step 100 Train loss 5.80 Classification-F1 0.0 on epoch=49
06/24/2022 10:31:00 - INFO - __main__ - Step 110 Global step 110 Train loss 5.80 on epoch=54
06/24/2022 10:31:02 - INFO - __main__ - Step 120 Global step 120 Train loss 5.77 on epoch=59
06/24/2022 10:31:03 - INFO - __main__ - Step 130 Global step 130 Train loss 5.74 on epoch=64
06/24/2022 10:31:04 - INFO - __main__ - Step 140 Global step 140 Train loss 5.65 on epoch=69
06/24/2022 10:31:05 - INFO - __main__ - Step 150 Global step 150 Train loss 5.64 on epoch=74
06/24/2022 10:31:06 - INFO - __main__ - Global step 150 Train loss 5.72 Classification-F1 0.0 on epoch=74
06/24/2022 10:31:08 - INFO - __main__ - Step 160 Global step 160 Train loss 5.69 on epoch=79
06/24/2022 10:31:09 - INFO - __main__ - Step 170 Global step 170 Train loss 5.66 on epoch=84
06/24/2022 10:31:10 - INFO - __main__ - Step 180 Global step 180 Train loss 5.49 on epoch=89
06/24/2022 10:31:11 - INFO - __main__ - Step 190 Global step 190 Train loss 5.48 on epoch=94
06/24/2022 10:31:13 - INFO - __main__ - Step 200 Global step 200 Train loss 5.41 on epoch=99
06/24/2022 10:31:19 - INFO - __main__ - Global step 200 Train loss 5.55 Classification-F1 0.0 on epoch=99
06/24/2022 10:31:20 - INFO - __main__ - Step 210 Global step 210 Train loss 5.42 on epoch=104
06/24/2022 10:31:21 - INFO - __main__ - Step 220 Global step 220 Train loss 5.26 on epoch=109
06/24/2022 10:31:22 - INFO - __main__ - Step 230 Global step 230 Train loss 5.28 on epoch=114
06/24/2022 10:31:24 - INFO - __main__ - Step 240 Global step 240 Train loss 5.19 on epoch=119
06/24/2022 10:31:25 - INFO - __main__ - Step 250 Global step 250 Train loss 5.13 on epoch=124
06/24/2022 10:31:35 - INFO - __main__ - Global step 250 Train loss 5.26 Classification-F1 0.0 on epoch=124
06/24/2022 10:31:36 - INFO - __main__ - Step 260 Global step 260 Train loss 5.07 on epoch=129
06/24/2022 10:31:38 - INFO - __main__ - Step 270 Global step 270 Train loss 4.94 on epoch=134
06/24/2022 10:31:39 - INFO - __main__ - Step 280 Global step 280 Train loss 5.06 on epoch=139
06/24/2022 10:31:40 - INFO - __main__ - Step 290 Global step 290 Train loss 4.91 on epoch=144
06/24/2022 10:31:41 - INFO - __main__ - Step 300 Global step 300 Train loss 4.88 on epoch=149
06/24/2022 10:31:53 - INFO - __main__ - Global step 300 Train loss 4.97 Classification-F1 0.0 on epoch=149
06/24/2022 10:31:54 - INFO - __main__ - Step 310 Global step 310 Train loss 4.78 on epoch=154
06/24/2022 10:31:55 - INFO - __main__ - Step 320 Global step 320 Train loss 4.59 on epoch=159
06/24/2022 10:31:57 - INFO - __main__ - Step 330 Global step 330 Train loss 4.62 on epoch=164
06/24/2022 10:31:58 - INFO - __main__ - Step 340 Global step 340 Train loss 4.56 on epoch=169
06/24/2022 10:31:59 - INFO - __main__ - Step 350 Global step 350 Train loss 4.37 on epoch=174
06/24/2022 10:32:10 - INFO - __main__ - Global step 350 Train loss 4.58 Classification-F1 0.0 on epoch=174
06/24/2022 10:32:11 - INFO - __main__ - Step 360 Global step 360 Train loss 4.40 on epoch=179
06/24/2022 10:32:13 - INFO - __main__ - Step 370 Global step 370 Train loss 4.30 on epoch=184
06/24/2022 10:32:14 - INFO - __main__ - Step 380 Global step 380 Train loss 4.17 on epoch=189
06/24/2022 10:32:15 - INFO - __main__ - Step 390 Global step 390 Train loss 4.21 on epoch=194
06/24/2022 10:32:17 - INFO - __main__ - Step 400 Global step 400 Train loss 4.15 on epoch=199
06/24/2022 10:32:28 - INFO - __main__ - Global step 400 Train loss 4.24 Classification-F1 0.0 on epoch=199
06/24/2022 10:32:30 - INFO - __main__ - Step 410 Global step 410 Train loss 4.16 on epoch=204
06/24/2022 10:32:31 - INFO - __main__ - Step 420 Global step 420 Train loss 4.00 on epoch=209
06/24/2022 10:32:32 - INFO - __main__ - Step 430 Global step 430 Train loss 4.00 on epoch=214
06/24/2022 10:32:33 - INFO - __main__ - Step 440 Global step 440 Train loss 3.88 on epoch=219
06/24/2022 10:32:35 - INFO - __main__ - Step 450 Global step 450 Train loss 3.89 on epoch=224
06/24/2022 10:32:46 - INFO - __main__ - Global step 450 Train loss 3.99 Classification-F1 0.0 on epoch=224
06/24/2022 10:32:48 - INFO - __main__ - Step 460 Global step 460 Train loss 3.74 on epoch=229
06/24/2022 10:32:49 - INFO - __main__ - Step 470 Global step 470 Train loss 3.69 on epoch=234
06/24/2022 10:32:50 - INFO - __main__ - Step 480 Global step 480 Train loss 3.55 on epoch=239
06/24/2022 10:32:51 - INFO - __main__ - Step 490 Global step 490 Train loss 3.60 on epoch=244
06/24/2022 10:32:53 - INFO - __main__ - Step 500 Global step 500 Train loss 3.43 on epoch=249
06/24/2022 10:33:04 - INFO - __main__ - Global step 500 Train loss 3.60 Classification-F1 0.0 on epoch=249
06/24/2022 10:33:05 - INFO - __main__ - Step 510 Global step 510 Train loss 3.38 on epoch=254
06/24/2022 10:33:06 - INFO - __main__ - Step 520 Global step 520 Train loss 3.35 on epoch=259
06/24/2022 10:33:07 - INFO - __main__ - Step 530 Global step 530 Train loss 3.37 on epoch=264
06/24/2022 10:33:09 - INFO - __main__ - Step 540 Global step 540 Train loss 3.30 on epoch=269
06/24/2022 10:33:10 - INFO - __main__ - Step 550 Global step 550 Train loss 3.45 on epoch=274
06/24/2022 10:33:22 - INFO - __main__ - Global step 550 Train loss 3.37 Classification-F1 0.0 on epoch=274
06/24/2022 10:33:23 - INFO - __main__ - Step 560 Global step 560 Train loss 3.42 on epoch=279
06/24/2022 10:33:24 - INFO - __main__ - Step 570 Global step 570 Train loss 3.27 on epoch=284
06/24/2022 10:33:25 - INFO - __main__ - Step 580 Global step 580 Train loss 3.20 on epoch=289
06/24/2022 10:33:27 - INFO - __main__ - Step 590 Global step 590 Train loss 3.15 on epoch=294
06/24/2022 10:33:28 - INFO - __main__ - Step 600 Global step 600 Train loss 2.98 on epoch=299
06/24/2022 10:33:30 - INFO - __main__ - Global step 600 Train loss 3.20 Classification-F1 0.1 on epoch=299
06/24/2022 10:33:30 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.1 on epoch=299, global_step=600
06/24/2022 10:33:31 - INFO - __main__ - Step 610 Global step 610 Train loss 2.96 on epoch=304
06/24/2022 10:33:33 - INFO - __main__ - Step 620 Global step 620 Train loss 2.95 on epoch=309
06/24/2022 10:33:34 - INFO - __main__ - Step 630 Global step 630 Train loss 2.71 on epoch=314
06/24/2022 10:33:35 - INFO - __main__ - Step 640 Global step 640 Train loss 2.89 on epoch=319
06/24/2022 10:33:36 - INFO - __main__ - Step 650 Global step 650 Train loss 2.63 on epoch=324
06/24/2022 10:33:47 - INFO - __main__ - Global step 650 Train loss 2.83 Classification-F1 0.16666666666666669 on epoch=324
06/24/2022 10:33:47 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.16666666666666669 on epoch=324, global_step=650
06/24/2022 10:33:48 - INFO - __main__ - Step 660 Global step 660 Train loss 2.70 on epoch=329
06/24/2022 10:33:49 - INFO - __main__ - Step 670 Global step 670 Train loss 2.68 on epoch=334
06/24/2022 10:33:50 - INFO - __main__ - Step 680 Global step 680 Train loss 2.45 on epoch=339
06/24/2022 10:33:52 - INFO - __main__ - Step 690 Global step 690 Train loss 2.49 on epoch=344
06/24/2022 10:33:53 - INFO - __main__ - Step 700 Global step 700 Train loss 2.41 on epoch=349
06/24/2022 10:33:55 - INFO - __main__ - Global step 700 Train loss 2.55 Classification-F1 0.3333333333333333 on epoch=349
06/24/2022 10:33:55 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666669 -> 0.3333333333333333 on epoch=349, global_step=700
06/24/2022 10:33:56 - INFO - __main__ - Step 710 Global step 710 Train loss 2.40 on epoch=354
06/24/2022 10:33:57 - INFO - __main__ - Step 720 Global step 720 Train loss 2.42 on epoch=359
06/24/2022 10:33:58 - INFO - __main__ - Step 730 Global step 730 Train loss 2.42 on epoch=364
06/24/2022 10:34:00 - INFO - __main__ - Step 740 Global step 740 Train loss 2.28 on epoch=369
06/24/2022 10:34:01 - INFO - __main__ - Step 750 Global step 750 Train loss 2.22 on epoch=374
06/24/2022 10:34:02 - INFO - __main__ - Global step 750 Train loss 2.35 Classification-F1 0.4385964912280702 on epoch=374
06/24/2022 10:34:02 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4385964912280702 on epoch=374, global_step=750
06/24/2022 10:34:04 - INFO - __main__ - Step 760 Global step 760 Train loss 2.24 on epoch=379
06/24/2022 10:34:05 - INFO - __main__ - Step 770 Global step 770 Train loss 2.17 on epoch=384
06/24/2022 10:34:06 - INFO - __main__ - Step 780 Global step 780 Train loss 2.20 on epoch=389
06/24/2022 10:34:08 - INFO - __main__ - Step 790 Global step 790 Train loss 2.01 on epoch=394
06/24/2022 10:34:09 - INFO - __main__ - Step 800 Global step 800 Train loss 2.15 on epoch=399
06/24/2022 10:34:10 - INFO - __main__ - Global step 800 Train loss 2.15 Classification-F1 0.30158730158730157 on epoch=399
06/24/2022 10:34:11 - INFO - __main__ - Step 810 Global step 810 Train loss 2.10 on epoch=404
06/24/2022 10:34:13 - INFO - __main__ - Step 820 Global step 820 Train loss 2.03 on epoch=409
06/24/2022 10:34:14 - INFO - __main__ - Step 830 Global step 830 Train loss 1.88 on epoch=414
06/24/2022 10:34:15 - INFO - __main__ - Step 840 Global step 840 Train loss 1.94 on epoch=419
06/24/2022 10:34:17 - INFO - __main__ - Step 850 Global step 850 Train loss 1.90 on epoch=424
06/24/2022 10:34:17 - INFO - __main__ - Global step 850 Train loss 1.97 Classification-F1 0.4682306940371457 on epoch=424
06/24/2022 10:34:17 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.4682306940371457 on epoch=424, global_step=850
06/24/2022 10:34:19 - INFO - __main__ - Step 860 Global step 860 Train loss 1.86 on epoch=429
06/24/2022 10:34:20 - INFO - __main__ - Step 870 Global step 870 Train loss 1.92 on epoch=434
06/24/2022 10:34:21 - INFO - __main__ - Step 880 Global step 880 Train loss 1.83 on epoch=439
06/24/2022 10:34:22 - INFO - __main__ - Step 890 Global step 890 Train loss 1.79 on epoch=444
06/24/2022 10:34:24 - INFO - __main__ - Step 900 Global step 900 Train loss 1.74 on epoch=449
06/24/2022 10:34:25 - INFO - __main__ - Global step 900 Train loss 1.83 Classification-F1 0.4458874458874459 on epoch=449
06/24/2022 10:34:26 - INFO - __main__ - Step 910 Global step 910 Train loss 1.88 on epoch=454
06/24/2022 10:34:28 - INFO - __main__ - Step 920 Global step 920 Train loss 1.76 on epoch=459
06/24/2022 10:34:29 - INFO - __main__ - Step 930 Global step 930 Train loss 1.60 on epoch=464
06/24/2022 10:34:30 - INFO - __main__ - Step 940 Global step 940 Train loss 1.50 on epoch=469
06/24/2022 10:34:31 - INFO - __main__ - Step 950 Global step 950 Train loss 1.55 on epoch=474
06/24/2022 10:34:32 - INFO - __main__ - Global step 950 Train loss 1.66 Classification-F1 0.3816425120772947 on epoch=474
06/24/2022 10:34:34 - INFO - __main__ - Step 960 Global step 960 Train loss 1.65 on epoch=479
06/24/2022 10:34:35 - INFO - __main__ - Step 970 Global step 970 Train loss 1.44 on epoch=484
06/24/2022 10:34:36 - INFO - __main__ - Step 980 Global step 980 Train loss 1.52 on epoch=489
06/24/2022 10:34:37 - INFO - __main__ - Step 990 Global step 990 Train loss 1.57 on epoch=494
06/24/2022 10:34:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.49 on epoch=499
06/24/2022 10:34:39 - INFO - __main__ - Global step 1000 Train loss 1.54 Classification-F1 0.4181818181818182 on epoch=499
06/24/2022 10:34:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.50 on epoch=504
06/24/2022 10:34:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.43 on epoch=509
06/24/2022 10:34:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.44 on epoch=514
06/24/2022 10:34:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.31 on epoch=519
06/24/2022 10:34:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.38 on epoch=524
06/24/2022 10:34:46 - INFO - __main__ - Global step 1050 Train loss 1.41 Classification-F1 0.3333333333333333 on epoch=524
06/24/2022 10:34:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.42 on epoch=529
06/24/2022 10:34:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.17 on epoch=534
06/24/2022 10:34:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.19 on epoch=539
06/24/2022 10:34:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.14 on epoch=544
06/24/2022 10:34:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.30 on epoch=549
06/24/2022 10:34:53 - INFO - __main__ - Global step 1100 Train loss 1.24 Classification-F1 0.539313399778516 on epoch=549
06/24/2022 10:34:53 - INFO - __main__ - Saving model with best Classification-F1: 0.4682306940371457 -> 0.539313399778516 on epoch=549, global_step=1100
06/24/2022 10:34:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.24 on epoch=554
06/24/2022 10:34:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.22 on epoch=559
06/24/2022 10:34:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.20 on epoch=564
06/24/2022 10:34:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.21 on epoch=569
06/24/2022 10:34:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.04 on epoch=574
06/24/2022 10:35:00 - INFO - __main__ - Global step 1150 Train loss 1.18 Classification-F1 0.3816425120772947 on epoch=574
06/24/2022 10:35:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.08 on epoch=579
06/24/2022 10:35:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.97 on epoch=584
06/24/2022 10:35:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.01 on epoch=589
06/24/2022 10:35:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.06 on epoch=594
06/24/2022 10:35:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.16 on epoch=599
06/24/2022 10:35:06 - INFO - __main__ - Global step 1200 Train loss 1.06 Classification-F1 0.5134502923976608 on epoch=599
06/24/2022 10:35:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.16 on epoch=604
06/24/2022 10:35:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.13 on epoch=609
06/24/2022 10:35:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.03 on epoch=614
06/24/2022 10:35:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.91 on epoch=619
06/24/2022 10:35:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.00 on epoch=624
06/24/2022 10:35:13 - INFO - __main__ - Global step 1250 Train loss 1.04 Classification-F1 0.3992490613266583 on epoch=624
06/24/2022 10:35:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.03 on epoch=629
06/24/2022 10:35:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.98 on epoch=634
06/24/2022 10:35:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.98 on epoch=639
06/24/2022 10:35:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.98 on epoch=644
06/24/2022 10:35:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.92 on epoch=649
06/24/2022 10:35:20 - INFO - __main__ - Global step 1300 Train loss 0.98 Classification-F1 0.5555555555555556 on epoch=649
06/24/2022 10:35:20 - INFO - __main__ - Saving model with best Classification-F1: 0.539313399778516 -> 0.5555555555555556 on epoch=649, global_step=1300
06/24/2022 10:35:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.03 on epoch=654
06/24/2022 10:35:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.95 on epoch=659
06/24/2022 10:35:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.94 on epoch=664
06/24/2022 10:35:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.94 on epoch=669
06/24/2022 10:35:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.88 on epoch=674
06/24/2022 10:35:27 - INFO - __main__ - Global step 1350 Train loss 0.95 Classification-F1 0.5076923076923077 on epoch=674
06/24/2022 10:35:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.88 on epoch=679
06/24/2022 10:35:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.91 on epoch=684
06/24/2022 10:35:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.92 on epoch=689
06/24/2022 10:35:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.85 on epoch=694
06/24/2022 10:35:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.85 on epoch=699
06/24/2022 10:35:33 - INFO - __main__ - Global step 1400 Train loss 0.88 Classification-F1 0.4009852216748768 on epoch=699
06/24/2022 10:35:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.76 on epoch=704
06/24/2022 10:35:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.94 on epoch=709
06/24/2022 10:35:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.83 on epoch=714
06/24/2022 10:35:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.76 on epoch=719
06/24/2022 10:35:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.75 on epoch=724
06/24/2022 10:35:40 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.464039408866995 on epoch=724
06/24/2022 10:35:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.87 on epoch=729
06/24/2022 10:35:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.73 on epoch=734
06/24/2022 10:35:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.88 on epoch=739
06/24/2022 10:35:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.86 on epoch=744
06/24/2022 10:35:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.87 on epoch=749
06/24/2022 10:35:47 - INFO - __main__ - Global step 1500 Train loss 0.84 Classification-F1 0.49090909090909085 on epoch=749
06/24/2022 10:35:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.79 on epoch=754
06/24/2022 10:35:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.78 on epoch=759
06/24/2022 10:35:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.81 on epoch=764
06/24/2022 10:35:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.83 on epoch=769
06/24/2022 10:35:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.74 on epoch=774
06/24/2022 10:35:53 - INFO - __main__ - Global step 1550 Train loss 0.79 Classification-F1 0.5195195195195195 on epoch=774
06/24/2022 10:35:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.73 on epoch=779
06/24/2022 10:35:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.67 on epoch=784
06/24/2022 10:35:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.68 on epoch=789
06/24/2022 10:35:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.77 on epoch=794
06/24/2022 10:36:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.81 on epoch=799
06/24/2022 10:36:00 - INFO - __main__ - Global step 1600 Train loss 0.73 Classification-F1 0.3454545454545454 on epoch=799
06/24/2022 10:36:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.73 on epoch=804
06/24/2022 10:36:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.76 on epoch=809
06/24/2022 10:36:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.81 on epoch=814
06/24/2022 10:36:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.69 on epoch=819
06/24/2022 10:36:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.75 on epoch=824
06/24/2022 10:36:07 - INFO - __main__ - Global step 1650 Train loss 0.75 Classification-F1 0.5844155844155844 on epoch=824
06/24/2022 10:36:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5555555555555556 -> 0.5844155844155844 on epoch=824, global_step=1650
06/24/2022 10:36:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.66 on epoch=829
06/24/2022 10:36:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.69 on epoch=834
06/24/2022 10:36:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.68 on epoch=839
06/24/2022 10:36:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.70 on epoch=844
06/24/2022 10:36:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.66 on epoch=849
06/24/2022 10:36:13 - INFO - __main__ - Global step 1700 Train loss 0.68 Classification-F1 0.3043478260869565 on epoch=849
06/24/2022 10:36:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.73 on epoch=854
06/24/2022 10:36:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.75 on epoch=859
06/24/2022 10:36:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.70 on epoch=864
06/24/2022 10:36:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.69 on epoch=869
06/24/2022 10:36:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.70 on epoch=874
06/24/2022 10:36:20 - INFO - __main__ - Global step 1750 Train loss 0.71 Classification-F1 0.3266888150609081 on epoch=874
06/24/2022 10:36:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.71 on epoch=879
06/24/2022 10:36:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.70 on epoch=884
06/24/2022 10:36:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.71 on epoch=889
06/24/2022 10:36:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.78 on epoch=894
06/24/2022 10:36:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.62 on epoch=899
06/24/2022 10:36:27 - INFO - __main__ - Global step 1800 Train loss 0.70 Classification-F1 0.4666666666666667 on epoch=899
06/24/2022 10:36:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.62 on epoch=904
06/24/2022 10:36:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.67 on epoch=909
06/24/2022 10:36:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.59 on epoch=914
06/24/2022 10:36:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.73 on epoch=919
06/24/2022 10:36:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.75 on epoch=924
06/24/2022 10:36:33 - INFO - __main__ - Global step 1850 Train loss 0.67 Classification-F1 0.5076923076923077 on epoch=924
06/24/2022 10:36:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.61 on epoch=929
06/24/2022 10:36:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.68 on epoch=934
06/24/2022 10:36:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.62 on epoch=939
06/24/2022 10:36:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.77 on epoch=944
06/24/2022 10:36:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.70 on epoch=949
06/24/2022 10:36:40 - INFO - __main__ - Global step 1900 Train loss 0.68 Classification-F1 0.3552492046659597 on epoch=949
06/24/2022 10:36:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.64 on epoch=954
06/24/2022 10:36:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.65 on epoch=959
06/24/2022 10:36:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.69 on epoch=964
06/24/2022 10:36:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.68 on epoch=969
06/24/2022 10:36:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.58 on epoch=974
06/24/2022 10:36:47 - INFO - __main__ - Global step 1950 Train loss 0.65 Classification-F1 0.6000000000000001 on epoch=974
06/24/2022 10:36:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5844155844155844 -> 0.6000000000000001 on epoch=974, global_step=1950
06/24/2022 10:36:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.65 on epoch=979
06/24/2022 10:36:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.64 on epoch=984
06/24/2022 10:36:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.60 on epoch=989
06/24/2022 10:36:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.73 on epoch=994
06/24/2022 10:36:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.71 on epoch=999
06/24/2022 10:36:53 - INFO - __main__ - Global step 2000 Train loss 0.67 Classification-F1 0.3266888150609081 on epoch=999
06/24/2022 10:36:53 - INFO - __main__ - save last model!
06/24/2022 10:36:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 10:36:53 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 10:36:53 - INFO - __main__ - Printing 3 examples
06/24/2022 10:36:53 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 10:36:53 - INFO - __main__ - ['0']
06/24/2022 10:36:53 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 10:36:53 - INFO - __main__ - ['1']
06/24/2022 10:36:53 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 10:36:53 - INFO - __main__ - ['1']
06/24/2022 10:36:53 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 10:36:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:36:54 - INFO - __main__ - Printing 3 examples
06/24/2022 10:36:54 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 10:36:54 - INFO - __main__ - ['1']
06/24/2022 10:36:54 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 10:36:54 - INFO - __main__ - ['1']
06/24/2022 10:36:54 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 10:36:54 - INFO - __main__ - ['1']
06/24/2022 10:36:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 10:36:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:36:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:36:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:36:54 - INFO - __main__ - Printing 3 examples
06/24/2022 10:36:54 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 10:36:54 - INFO - __main__ - ['1']
06/24/2022 10:36:54 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 10:36:54 - INFO - __main__ - ['1']
06/24/2022 10:36:54 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 10:36:54 - INFO - __main__ - ['1']
06/24/2022 10:36:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:36:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:36:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:36:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:37:00 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:37:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:37:01 - INFO - __main__ - Starting training!
06/24/2022 10:37:05 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 10:38:31 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_100_0.4_8_predictions.txt
06/24/2022 10:38:31 - INFO - __main__ - Classification-F1 on test data: 0.4090
06/24/2022 10:38:31 - INFO - __main__ - prefix=paws_16_100, lr=0.4, bsz=8, dev_performance=0.6000000000000001, test_performance=0.4089811058093652
06/24/2022 10:38:31 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.3, bsz=8 ...
06/24/2022 10:38:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:38:32 - INFO - __main__ - Printing 3 examples
06/24/2022 10:38:32 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 10:38:32 - INFO - __main__ - ['1']
06/24/2022 10:38:32 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 10:38:32 - INFO - __main__ - ['1']
06/24/2022 10:38:32 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 10:38:32 - INFO - __main__ - ['1']
06/24/2022 10:38:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 10:38:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:38:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:38:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:38:32 - INFO - __main__ - Printing 3 examples
06/24/2022 10:38:32 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 10:38:32 - INFO - __main__ - ['1']
06/24/2022 10:38:32 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 10:38:32 - INFO - __main__ - ['1']
06/24/2022 10:38:32 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 10:38:32 - INFO - __main__ - ['1']
06/24/2022 10:38:32 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:38:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:38:32 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:38:38 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:38:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:38:39 - INFO - __main__ - Starting training!
06/24/2022 10:38:40 - INFO - __main__ - Step 10 Global step 10 Train loss 5.99 on epoch=4
06/24/2022 10:38:41 - INFO - __main__ - Step 20 Global step 20 Train loss 6.05 on epoch=9
06/24/2022 10:38:43 - INFO - __main__ - Step 30 Global step 30 Train loss 5.94 on epoch=14
06/24/2022 10:38:44 - INFO - __main__ - Step 40 Global step 40 Train loss 5.89 on epoch=19
06/24/2022 10:38:45 - INFO - __main__ - Step 50 Global step 50 Train loss 5.94 on epoch=24
06/24/2022 10:38:46 - INFO - __main__ - Global step 50 Train loss 5.96 Classification-F1 0.0 on epoch=24
06/24/2022 10:38:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 10:38:47 - INFO - __main__ - Step 60 Global step 60 Train loss 5.91 on epoch=29
06/24/2022 10:38:49 - INFO - __main__ - Step 70 Global step 70 Train loss 5.86 on epoch=34
06/24/2022 10:38:50 - INFO - __main__ - Step 80 Global step 80 Train loss 5.91 on epoch=39
06/24/2022 10:38:51 - INFO - __main__ - Step 90 Global step 90 Train loss 5.75 on epoch=44
06/24/2022 10:38:52 - INFO - __main__ - Step 100 Global step 100 Train loss 5.83 on epoch=49
06/24/2022 10:38:54 - INFO - __main__ - Global step 100 Train loss 5.85 Classification-F1 0.0 on epoch=49
06/24/2022 10:38:56 - INFO - __main__ - Step 110 Global step 110 Train loss 5.82 on epoch=54
06/24/2022 10:38:57 - INFO - __main__ - Step 120 Global step 120 Train loss 5.80 on epoch=59
06/24/2022 10:38:58 - INFO - __main__ - Step 130 Global step 130 Train loss 5.71 on epoch=64
06/24/2022 10:38:59 - INFO - __main__ - Step 140 Global step 140 Train loss 5.73 on epoch=69
06/24/2022 10:39:01 - INFO - __main__ - Step 150 Global step 150 Train loss 5.72 on epoch=74
06/24/2022 10:39:03 - INFO - __main__ - Global step 150 Train loss 5.76 Classification-F1 0.0 on epoch=74
06/24/2022 10:39:04 - INFO - __main__ - Step 160 Global step 160 Train loss 5.62 on epoch=79
06/24/2022 10:39:05 - INFO - __main__ - Step 170 Global step 170 Train loss 5.71 on epoch=84
06/24/2022 10:39:07 - INFO - __main__ - Step 180 Global step 180 Train loss 5.65 on epoch=89
06/24/2022 10:39:08 - INFO - __main__ - Step 190 Global step 190 Train loss 5.59 on epoch=94
06/24/2022 10:39:09 - INFO - __main__ - Step 200 Global step 200 Train loss 5.61 on epoch=99
06/24/2022 10:39:11 - INFO - __main__ - Global step 200 Train loss 5.64 Classification-F1 0.0 on epoch=99
06/24/2022 10:39:12 - INFO - __main__ - Step 210 Global step 210 Train loss 5.56 on epoch=104
06/24/2022 10:39:13 - INFO - __main__ - Step 220 Global step 220 Train loss 5.48 on epoch=109
06/24/2022 10:39:14 - INFO - __main__ - Step 230 Global step 230 Train loss 5.57 on epoch=114
06/24/2022 10:39:16 - INFO - __main__ - Step 240 Global step 240 Train loss 5.36 on epoch=119
06/24/2022 10:39:17 - INFO - __main__ - Step 250 Global step 250 Train loss 5.45 on epoch=124
06/24/2022 10:39:18 - INFO - __main__ - Global step 250 Train loss 5.48 Classification-F1 0.0 on epoch=124
06/24/2022 10:39:19 - INFO - __main__ - Step 260 Global step 260 Train loss 5.51 on epoch=129
06/24/2022 10:39:21 - INFO - __main__ - Step 270 Global step 270 Train loss 5.48 on epoch=134
06/24/2022 10:39:22 - INFO - __main__ - Step 280 Global step 280 Train loss 5.45 on epoch=139
06/24/2022 10:39:23 - INFO - __main__ - Step 290 Global step 290 Train loss 5.47 on epoch=144
06/24/2022 10:39:24 - INFO - __main__ - Step 300 Global step 300 Train loss 5.46 on epoch=149
06/24/2022 10:39:26 - INFO - __main__ - Global step 300 Train loss 5.47 Classification-F1 0.0 on epoch=149
06/24/2022 10:39:27 - INFO - __main__ - Step 310 Global step 310 Train loss 5.39 on epoch=154
06/24/2022 10:39:29 - INFO - __main__ - Step 320 Global step 320 Train loss 5.29 on epoch=159
06/24/2022 10:39:30 - INFO - __main__ - Step 330 Global step 330 Train loss 5.26 on epoch=164
06/24/2022 10:39:31 - INFO - __main__ - Step 340 Global step 340 Train loss 5.27 on epoch=169
06/24/2022 10:39:32 - INFO - __main__ - Step 350 Global step 350 Train loss 5.18 on epoch=174
06/24/2022 10:39:36 - INFO - __main__ - Global step 350 Train loss 5.28 Classification-F1 0.0 on epoch=174
06/24/2022 10:39:37 - INFO - __main__ - Step 360 Global step 360 Train loss 5.22 on epoch=179
06/24/2022 10:39:38 - INFO - __main__ - Step 370 Global step 370 Train loss 5.21 on epoch=184
06/24/2022 10:39:40 - INFO - __main__ - Step 380 Global step 380 Train loss 5.09 on epoch=189
06/24/2022 10:39:41 - INFO - __main__ - Step 390 Global step 390 Train loss 4.94 on epoch=194
06/24/2022 10:39:42 - INFO - __main__ - Step 400 Global step 400 Train loss 4.76 on epoch=199
06/24/2022 10:39:48 - INFO - __main__ - Global step 400 Train loss 5.05 Classification-F1 0.0 on epoch=199
06/24/2022 10:39:50 - INFO - __main__ - Step 410 Global step 410 Train loss 4.70 on epoch=204
06/24/2022 10:39:51 - INFO - __main__ - Step 420 Global step 420 Train loss 4.57 on epoch=209
06/24/2022 10:39:52 - INFO - __main__ - Step 430 Global step 430 Train loss 4.53 on epoch=214
06/24/2022 10:39:53 - INFO - __main__ - Step 440 Global step 440 Train loss 4.37 on epoch=219
06/24/2022 10:39:55 - INFO - __main__ - Step 450 Global step 450 Train loss 4.32 on epoch=224
06/24/2022 10:39:57 - INFO - __main__ - Global step 450 Train loss 4.50 Classification-F1 0.0 on epoch=224
06/24/2022 10:39:58 - INFO - __main__ - Step 460 Global step 460 Train loss 4.22 on epoch=229
06/24/2022 10:39:59 - INFO - __main__ - Step 470 Global step 470 Train loss 4.15 on epoch=234
06/24/2022 10:40:01 - INFO - __main__ - Step 480 Global step 480 Train loss 4.26 on epoch=239
06/24/2022 10:40:02 - INFO - __main__ - Step 490 Global step 490 Train loss 4.10 on epoch=244
06/24/2022 10:40:03 - INFO - __main__ - Step 500 Global step 500 Train loss 4.03 on epoch=249
06/24/2022 10:40:10 - INFO - __main__ - Global step 500 Train loss 4.15 Classification-F1 0.0 on epoch=249
06/24/2022 10:40:11 - INFO - __main__ - Step 510 Global step 510 Train loss 3.93 on epoch=254
06/24/2022 10:40:12 - INFO - __main__ - Step 520 Global step 520 Train loss 3.86 on epoch=259
06/24/2022 10:40:14 - INFO - __main__ - Step 530 Global step 530 Train loss 3.81 on epoch=264
06/24/2022 10:40:15 - INFO - __main__ - Step 540 Global step 540 Train loss 3.69 on epoch=269
06/24/2022 10:40:16 - INFO - __main__ - Step 550 Global step 550 Train loss 3.67 on epoch=274
06/24/2022 10:40:23 - INFO - __main__ - Global step 550 Train loss 3.79 Classification-F1 0.0 on epoch=274
06/24/2022 10:40:24 - INFO - __main__ - Step 560 Global step 560 Train loss 3.68 on epoch=279
06/24/2022 10:40:26 - INFO - __main__ - Step 570 Global step 570 Train loss 3.57 on epoch=284
06/24/2022 10:40:27 - INFO - __main__ - Step 580 Global step 580 Train loss 3.42 on epoch=289
06/24/2022 10:40:28 - INFO - __main__ - Step 590 Global step 590 Train loss 3.62 on epoch=294
06/24/2022 10:40:29 - INFO - __main__ - Step 600 Global step 600 Train loss 3.45 on epoch=299
06/24/2022 10:40:36 - INFO - __main__ - Global step 600 Train loss 3.55 Classification-F1 0.0 on epoch=299
06/24/2022 10:40:38 - INFO - __main__ - Step 610 Global step 610 Train loss 3.25 on epoch=304
06/24/2022 10:40:39 - INFO - __main__ - Step 620 Global step 620 Train loss 3.34 on epoch=309
06/24/2022 10:40:40 - INFO - __main__ - Step 630 Global step 630 Train loss 3.32 on epoch=314
06/24/2022 10:40:41 - INFO - __main__ - Step 640 Global step 640 Train loss 3.27 on epoch=319
06/24/2022 10:40:43 - INFO - __main__ - Step 650 Global step 650 Train loss 3.17 on epoch=324
06/24/2022 10:40:49 - INFO - __main__ - Global step 650 Train loss 3.27 Classification-F1 0.0 on epoch=324
06/24/2022 10:40:50 - INFO - __main__ - Step 660 Global step 660 Train loss 3.33 on epoch=329
06/24/2022 10:40:52 - INFO - __main__ - Step 670 Global step 670 Train loss 3.07 on epoch=334
06/24/2022 10:40:53 - INFO - __main__ - Step 680 Global step 680 Train loss 3.18 on epoch=339
06/24/2022 10:40:54 - INFO - __main__ - Step 690 Global step 690 Train loss 3.20 on epoch=344
06/24/2022 10:40:55 - INFO - __main__ - Step 700 Global step 700 Train loss 3.13 on epoch=349
06/24/2022 10:41:07 - INFO - __main__ - Global step 700 Train loss 3.18 Classification-F1 0.014652014652014652 on epoch=349
06/24/2022 10:41:07 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.014652014652014652 on epoch=349, global_step=700
06/24/2022 10:41:08 - INFO - __main__ - Step 710 Global step 710 Train loss 2.98 on epoch=354
06/24/2022 10:41:09 - INFO - __main__ - Step 720 Global step 720 Train loss 2.94 on epoch=359
06/24/2022 10:41:11 - INFO - __main__ - Step 730 Global step 730 Train loss 2.99 on epoch=364
06/24/2022 10:41:12 - INFO - __main__ - Step 740 Global step 740 Train loss 3.00 on epoch=369
06/24/2022 10:41:13 - INFO - __main__ - Step 750 Global step 750 Train loss 3.02 on epoch=374
06/24/2022 10:41:20 - INFO - __main__ - Global step 750 Train loss 2.99 Classification-F1 0.06707317073170732 on epoch=374
06/24/2022 10:41:20 - INFO - __main__ - Saving model with best Classification-F1: 0.014652014652014652 -> 0.06707317073170732 on epoch=374, global_step=750
06/24/2022 10:41:22 - INFO - __main__ - Step 760 Global step 760 Train loss 3.02 on epoch=379
06/24/2022 10:41:23 - INFO - __main__ - Step 770 Global step 770 Train loss 3.00 on epoch=384
06/24/2022 10:41:24 - INFO - __main__ - Step 780 Global step 780 Train loss 2.74 on epoch=389
06/24/2022 10:41:25 - INFO - __main__ - Step 790 Global step 790 Train loss 2.94 on epoch=394
06/24/2022 10:41:27 - INFO - __main__ - Step 800 Global step 800 Train loss 2.66 on epoch=399
06/24/2022 10:41:33 - INFO - __main__ - Global step 800 Train loss 2.87 Classification-F1 0.15555555555555556 on epoch=399
06/24/2022 10:41:33 - INFO - __main__ - Saving model with best Classification-F1: 0.06707317073170732 -> 0.15555555555555556 on epoch=399, global_step=800
06/24/2022 10:41:35 - INFO - __main__ - Step 810 Global step 810 Train loss 2.66 on epoch=404
06/24/2022 10:41:36 - INFO - __main__ - Step 820 Global step 820 Train loss 2.59 on epoch=409
06/24/2022 10:41:37 - INFO - __main__ - Step 830 Global step 830 Train loss 2.64 on epoch=414
06/24/2022 10:41:38 - INFO - __main__ - Step 840 Global step 840 Train loss 2.35 on epoch=419
06/24/2022 10:41:40 - INFO - __main__ - Step 850 Global step 850 Train loss 2.54 on epoch=424
06/24/2022 10:41:42 - INFO - __main__ - Global step 850 Train loss 2.56 Classification-F1 0.3992490613266583 on epoch=424
06/24/2022 10:41:42 - INFO - __main__ - Saving model with best Classification-F1: 0.15555555555555556 -> 0.3992490613266583 on epoch=424, global_step=850
06/24/2022 10:41:43 - INFO - __main__ - Step 860 Global step 860 Train loss 2.39 on epoch=429
06/24/2022 10:41:44 - INFO - __main__ - Step 870 Global step 870 Train loss 2.42 on epoch=434
06/24/2022 10:41:46 - INFO - __main__ - Step 880 Global step 880 Train loss 2.29 on epoch=439
06/24/2022 10:41:47 - INFO - __main__ - Step 890 Global step 890 Train loss 2.21 on epoch=444
06/24/2022 10:41:48 - INFO - __main__ - Step 900 Global step 900 Train loss 2.17 on epoch=449
06/24/2022 10:41:50 - INFO - __main__ - Global step 900 Train loss 2.29 Classification-F1 0.5607843137254902 on epoch=449
06/24/2022 10:41:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5607843137254902 on epoch=449, global_step=900
06/24/2022 10:41:51 - INFO - __main__ - Step 910 Global step 910 Train loss 1.96 on epoch=454
06/24/2022 10:41:52 - INFO - __main__ - Step 920 Global step 920 Train loss 2.19 on epoch=459
06/24/2022 10:41:54 - INFO - __main__ - Step 930 Global step 930 Train loss 2.14 on epoch=464
06/24/2022 10:41:55 - INFO - __main__ - Step 940 Global step 940 Train loss 1.94 on epoch=469
06/24/2022 10:41:56 - INFO - __main__ - Step 950 Global step 950 Train loss 2.01 on epoch=474
06/24/2022 10:41:58 - INFO - __main__ - Global step 950 Train loss 2.05 Classification-F1 0.3191489361702127 on epoch=474
06/24/2022 10:41:59 - INFO - __main__ - Step 960 Global step 960 Train loss 2.03 on epoch=479
06/24/2022 10:42:00 - INFO - __main__ - Step 970 Global step 970 Train loss 2.03 on epoch=484
06/24/2022 10:42:02 - INFO - __main__ - Step 980 Global step 980 Train loss 1.86 on epoch=489
06/24/2022 10:42:03 - INFO - __main__ - Step 990 Global step 990 Train loss 1.98 on epoch=494
06/24/2022 10:42:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.82 on epoch=499
06/24/2022 10:42:05 - INFO - __main__ - Global step 1000 Train loss 1.94 Classification-F1 0.3191489361702127 on epoch=499
06/24/2022 10:42:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.73 on epoch=504
06/24/2022 10:42:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.65 on epoch=509
06/24/2022 10:42:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.71 on epoch=514
06/24/2022 10:42:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.61 on epoch=519
06/24/2022 10:42:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.59 on epoch=524
06/24/2022 10:42:11 - INFO - __main__ - Global step 1050 Train loss 1.66 Classification-F1 0.3333333333333333 on epoch=524
06/24/2022 10:42:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.63 on epoch=529
06/24/2022 10:42:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.62 on epoch=534
06/24/2022 10:42:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.57 on epoch=539
06/24/2022 10:42:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.42 on epoch=544
06/24/2022 10:42:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.44 on epoch=549
06/24/2022 10:42:18 - INFO - __main__ - Global step 1100 Train loss 1.54 Classification-F1 0.3992490613266583 on epoch=549
06/24/2022 10:42:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.41 on epoch=554
06/24/2022 10:42:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.37 on epoch=559
06/24/2022 10:42:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.37 on epoch=564
06/24/2022 10:42:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.31 on epoch=569
06/24/2022 10:42:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.28 on epoch=574
06/24/2022 10:42:25 - INFO - __main__ - Global step 1150 Train loss 1.35 Classification-F1 0.3992490613266583 on epoch=574
06/24/2022 10:42:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.31 on epoch=579
06/24/2022 10:42:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.23 on epoch=584
06/24/2022 10:42:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.30 on epoch=589
06/24/2022 10:42:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.16 on epoch=594
06/24/2022 10:42:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.26 on epoch=599
06/24/2022 10:42:32 - INFO - __main__ - Global step 1200 Train loss 1.25 Classification-F1 0.3191489361702127 on epoch=599
06/24/2022 10:42:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.23 on epoch=604
06/24/2022 10:42:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.16 on epoch=609
06/24/2022 10:42:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.22 on epoch=614
06/24/2022 10:42:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.19 on epoch=619
06/24/2022 10:42:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.20 on epoch=624
06/24/2022 10:42:38 - INFO - __main__ - Global step 1250 Train loss 1.20 Classification-F1 0.3992490613266583 on epoch=624
06/24/2022 10:42:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.14 on epoch=629
06/24/2022 10:42:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.12 on epoch=634
06/24/2022 10:42:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.02 on epoch=639
06/24/2022 10:42:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.10 on epoch=644
06/24/2022 10:42:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.08 on epoch=649
06/24/2022 10:42:45 - INFO - __main__ - Global step 1300 Train loss 1.09 Classification-F1 0.3191489361702127 on epoch=649
06/24/2022 10:42:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.01 on epoch=654
06/24/2022 10:42:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.12 on epoch=659
06/24/2022 10:42:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.96 on epoch=664
06/24/2022 10:42:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.07 on epoch=669
06/24/2022 10:42:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.01 on epoch=674
06/24/2022 10:42:52 - INFO - __main__ - Global step 1350 Train loss 1.04 Classification-F1 0.3816425120772947 on epoch=674
06/24/2022 10:42:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.09 on epoch=679
06/24/2022 10:42:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.04 on epoch=684
06/24/2022 10:42:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.00 on epoch=689
06/24/2022 10:42:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.94 on epoch=694
06/24/2022 10:42:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.93 on epoch=699
06/24/2022 10:42:58 - INFO - __main__ - Global step 1400 Train loss 1.00 Classification-F1 0.3992490613266583 on epoch=699
06/24/2022 10:43:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.98 on epoch=704
06/24/2022 10:43:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.95 on epoch=709
06/24/2022 10:43:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.96 on epoch=714
06/24/2022 10:43:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.92 on epoch=719
06/24/2022 10:43:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.03 on epoch=724
06/24/2022 10:43:05 - INFO - __main__ - Global step 1450 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=724
06/24/2022 10:43:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.90 on epoch=729
06/24/2022 10:43:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.88 on epoch=734
06/24/2022 10:43:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.81 on epoch=739
06/24/2022 10:43:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.84 on epoch=744
06/24/2022 10:43:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.90 on epoch=749
06/24/2022 10:43:12 - INFO - __main__ - Global step 1500 Train loss 0.87 Classification-F1 0.3333333333333333 on epoch=749
06/24/2022 10:43:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.92 on epoch=754
06/24/2022 10:43:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.91 on epoch=759
06/24/2022 10:43:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.80 on epoch=764
06/24/2022 10:43:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.86 on epoch=769
06/24/2022 10:43:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.81 on epoch=774
06/24/2022 10:43:19 - INFO - __main__ - Global step 1550 Train loss 0.86 Classification-F1 0.39139139139139134 on epoch=774
06/24/2022 10:43:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.79 on epoch=779
06/24/2022 10:43:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.93 on epoch=784
06/24/2022 10:43:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.83 on epoch=789
06/24/2022 10:43:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.80 on epoch=794
06/24/2022 10:43:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.76 on epoch=799
06/24/2022 10:43:25 - INFO - __main__ - Global step 1600 Train loss 0.82 Classification-F1 0.5076923076923077 on epoch=799
06/24/2022 10:43:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.81 on epoch=804
06/24/2022 10:43:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.95 on epoch=809
06/24/2022 10:43:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.80 on epoch=814
06/24/2022 10:43:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.77 on epoch=819
06/24/2022 10:43:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.75 on epoch=824
06/24/2022 10:43:32 - INFO - __main__ - Global step 1650 Train loss 0.82 Classification-F1 0.3816425120772947 on epoch=824
06/24/2022 10:43:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.84 on epoch=829
06/24/2022 10:43:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.80 on epoch=834
06/24/2022 10:43:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.74 on epoch=839
06/24/2022 10:43:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.77 on epoch=844
06/24/2022 10:43:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.84 on epoch=849
06/24/2022 10:43:39 - INFO - __main__ - Global step 1700 Train loss 0.80 Classification-F1 0.3191489361702127 on epoch=849
06/24/2022 10:43:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.74 on epoch=854
06/24/2022 10:43:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.81 on epoch=859
06/24/2022 10:43:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.72 on epoch=864
06/24/2022 10:43:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.82 on epoch=869
06/24/2022 10:43:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.79 on epoch=874
06/24/2022 10:43:45 - INFO - __main__ - Global step 1750 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=874
06/24/2022 10:43:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.67 on epoch=879
06/24/2022 10:43:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.73 on epoch=884
06/24/2022 10:43:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.63 on epoch=889
06/24/2022 10:43:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.82 on epoch=894
06/24/2022 10:43:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.72 on epoch=899
06/24/2022 10:43:52 - INFO - __main__ - Global step 1800 Train loss 0.71 Classification-F1 0.3816425120772947 on epoch=899
06/24/2022 10:43:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.74 on epoch=904
06/24/2022 10:43:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.69 on epoch=909
06/24/2022 10:43:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.73 on epoch=914
06/24/2022 10:43:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.71 on epoch=919
06/24/2022 10:43:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.79 on epoch=924
06/24/2022 10:43:59 - INFO - __main__ - Global step 1850 Train loss 0.73 Classification-F1 0.36374269005847953 on epoch=924
06/24/2022 10:44:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.74 on epoch=929
06/24/2022 10:44:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.74 on epoch=934
06/24/2022 10:44:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.76 on epoch=939
06/24/2022 10:44:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.73 on epoch=944
06/24/2022 10:44:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.66 on epoch=949
06/24/2022 10:44:05 - INFO - __main__ - Global step 1900 Train loss 0.73 Classification-F1 0.39999999999999997 on epoch=949
06/24/2022 10:44:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.66 on epoch=954
06/24/2022 10:44:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.75 on epoch=959
06/24/2022 10:44:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.79 on epoch=964
06/24/2022 10:44:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.72 on epoch=969
06/24/2022 10:44:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.59 on epoch=974
06/24/2022 10:44:12 - INFO - __main__ - Global step 1950 Train loss 0.70 Classification-F1 0.3992490613266583 on epoch=974
06/24/2022 10:44:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.69 on epoch=979
06/24/2022 10:44:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.71 on epoch=984
06/24/2022 10:44:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.66 on epoch=989
06/24/2022 10:44:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.66 on epoch=994
06/24/2022 10:44:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.73 on epoch=999
06/24/2022 10:44:19 - INFO - __main__ - Global step 2000 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=999
06/24/2022 10:44:19 - INFO - __main__ - save last model!
06/24/2022 10:44:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 10:44:19 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 10:44:19 - INFO - __main__ - Printing 3 examples
06/24/2022 10:44:19 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 10:44:19 - INFO - __main__ - ['0']
06/24/2022 10:44:19 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 10:44:19 - INFO - __main__ - ['1']
06/24/2022 10:44:19 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 10:44:19 - INFO - __main__ - ['1']
06/24/2022 10:44:19 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 10:44:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:44:19 - INFO - __main__ - Printing 3 examples
06/24/2022 10:44:19 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 10:44:19 - INFO - __main__ - ['1']
06/24/2022 10:44:19 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 10:44:19 - INFO - __main__ - ['1']
06/24/2022 10:44:19 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 10:44:19 - INFO - __main__ - ['1']
06/24/2022 10:44:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 10:44:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:44:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:44:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:44:19 - INFO - __main__ - Printing 3 examples
06/24/2022 10:44:19 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 10:44:19 - INFO - __main__ - ['1']
06/24/2022 10:44:19 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 10:44:20 - INFO - __main__ - ['1']
06/24/2022 10:44:20 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 10:44:20 - INFO - __main__ - ['1']
06/24/2022 10:44:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:44:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:44:20 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:44:23 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:44:25 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:44:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:44:25 - INFO - __main__ - Starting training!
06/24/2022 10:44:31 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 10:45:54 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_100_0.3_8_predictions.txt
06/24/2022 10:45:54 - INFO - __main__ - Classification-F1 on test data: 0.3194
06/24/2022 10:45:54 - INFO - __main__ - prefix=paws_16_100, lr=0.3, bsz=8, dev_performance=0.5607843137254902, test_performance=0.31940841732188124
06/24/2022 10:45:54 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.2, bsz=8 ...
06/24/2022 10:45:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:45:55 - INFO - __main__ - Printing 3 examples
06/24/2022 10:45:55 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 10:45:55 - INFO - __main__ - ['1']
06/24/2022 10:45:55 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 10:45:55 - INFO - __main__ - ['1']
06/24/2022 10:45:55 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 10:45:55 - INFO - __main__ - ['1']
06/24/2022 10:45:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 10:45:55 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:45:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:45:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:45:55 - INFO - __main__ - Printing 3 examples
06/24/2022 10:45:55 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 10:45:55 - INFO - __main__ - ['1']
06/24/2022 10:45:55 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 10:45:55 - INFO - __main__ - ['1']
06/24/2022 10:45:55 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 10:45:55 - INFO - __main__ - ['1']
06/24/2022 10:45:55 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:45:55 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:45:55 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:46:01 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:46:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:46:02 - INFO - __main__ - Starting training!
06/24/2022 10:46:03 - INFO - __main__ - Step 10 Global step 10 Train loss 5.95 on epoch=4
06/24/2022 10:46:04 - INFO - __main__ - Step 20 Global step 20 Train loss 6.00 on epoch=9
06/24/2022 10:46:06 - INFO - __main__ - Step 30 Global step 30 Train loss 5.97 on epoch=14
06/24/2022 10:46:07 - INFO - __main__ - Step 40 Global step 40 Train loss 5.87 on epoch=19
06/24/2022 10:46:08 - INFO - __main__ - Step 50 Global step 50 Train loss 5.90 on epoch=24
06/24/2022 10:46:11 - INFO - __main__ - Global step 50 Train loss 5.94 Classification-F1 0.0 on epoch=24
06/24/2022 10:46:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 10:46:12 - INFO - __main__ - Step 60 Global step 60 Train loss 5.99 on epoch=29
06/24/2022 10:46:13 - INFO - __main__ - Step 70 Global step 70 Train loss 5.94 on epoch=34
06/24/2022 10:46:15 - INFO - __main__ - Step 80 Global step 80 Train loss 5.93 on epoch=39
06/24/2022 10:46:16 - INFO - __main__ - Step 90 Global step 90 Train loss 5.97 on epoch=44
06/24/2022 10:46:17 - INFO - __main__ - Step 100 Global step 100 Train loss 5.87 on epoch=49
06/24/2022 10:46:18 - INFO - __main__ - Global step 100 Train loss 5.94 Classification-F1 0.0 on epoch=49
06/24/2022 10:46:19 - INFO - __main__ - Step 110 Global step 110 Train loss 5.88 on epoch=54
06/24/2022 10:46:20 - INFO - __main__ - Step 120 Global step 120 Train loss 5.86 on epoch=59
06/24/2022 10:46:22 - INFO - __main__ - Step 130 Global step 130 Train loss 5.78 on epoch=64
06/24/2022 10:46:23 - INFO - __main__ - Step 140 Global step 140 Train loss 5.97 on epoch=69
06/24/2022 10:46:24 - INFO - __main__ - Step 150 Global step 150 Train loss 5.70 on epoch=74
06/24/2022 10:46:30 - INFO - __main__ - Global step 150 Train loss 5.84 Classification-F1 0.0 on epoch=74
06/24/2022 10:46:31 - INFO - __main__ - Step 160 Global step 160 Train loss 5.82 on epoch=79
06/24/2022 10:46:32 - INFO - __main__ - Step 170 Global step 170 Train loss 5.84 on epoch=84
06/24/2022 10:46:33 - INFO - __main__ - Step 180 Global step 180 Train loss 5.80 on epoch=89
06/24/2022 10:46:35 - INFO - __main__ - Step 190 Global step 190 Train loss 5.85 on epoch=94
06/24/2022 10:46:36 - INFO - __main__ - Step 200 Global step 200 Train loss 5.77 on epoch=99
06/24/2022 10:46:41 - INFO - __main__ - Global step 200 Train loss 5.82 Classification-F1 0.0 on epoch=99
06/24/2022 10:46:43 - INFO - __main__ - Step 210 Global step 210 Train loss 5.74 on epoch=104
06/24/2022 10:46:44 - INFO - __main__ - Step 220 Global step 220 Train loss 5.81 on epoch=109
06/24/2022 10:46:45 - INFO - __main__ - Step 230 Global step 230 Train loss 5.82 on epoch=114
06/24/2022 10:46:46 - INFO - __main__ - Step 240 Global step 240 Train loss 5.77 on epoch=119
06/24/2022 10:46:48 - INFO - __main__ - Step 250 Global step 250 Train loss 5.84 on epoch=124
06/24/2022 10:46:49 - INFO - __main__ - Global step 250 Train loss 5.79 Classification-F1 0.0 on epoch=124
06/24/2022 10:46:50 - INFO - __main__ - Step 260 Global step 260 Train loss 5.76 on epoch=129
06/24/2022 10:46:52 - INFO - __main__ - Step 270 Global step 270 Train loss 5.84 on epoch=134
06/24/2022 10:46:53 - INFO - __main__ - Step 280 Global step 280 Train loss 5.69 on epoch=139
06/24/2022 10:46:54 - INFO - __main__ - Step 290 Global step 290 Train loss 5.78 on epoch=144
06/24/2022 10:46:55 - INFO - __main__ - Step 300 Global step 300 Train loss 5.72 on epoch=149
06/24/2022 10:46:57 - INFO - __main__ - Global step 300 Train loss 5.76 Classification-F1 0.0 on epoch=149
06/24/2022 10:46:58 - INFO - __main__ - Step 310 Global step 310 Train loss 5.71 on epoch=154
06/24/2022 10:46:59 - INFO - __main__ - Step 320 Global step 320 Train loss 5.63 on epoch=159
06/24/2022 10:47:00 - INFO - __main__ - Step 330 Global step 330 Train loss 5.74 on epoch=164
06/24/2022 10:47:02 - INFO - __main__ - Step 340 Global step 340 Train loss 5.63 on epoch=169
06/24/2022 10:47:03 - INFO - __main__ - Step 350 Global step 350 Train loss 5.66 on epoch=174
06/24/2022 10:47:13 - INFO - __main__ - Global step 350 Train loss 5.67 Classification-F1 0.0 on epoch=174
06/24/2022 10:47:14 - INFO - __main__ - Step 360 Global step 360 Train loss 5.70 on epoch=179
06/24/2022 10:47:16 - INFO - __main__ - Step 370 Global step 370 Train loss 5.62 on epoch=184
06/24/2022 10:47:17 - INFO - __main__ - Step 380 Global step 380 Train loss 5.71 on epoch=189
06/24/2022 10:47:18 - INFO - __main__ - Step 390 Global step 390 Train loss 5.55 on epoch=194
06/24/2022 10:47:19 - INFO - __main__ - Step 400 Global step 400 Train loss 5.56 on epoch=199
06/24/2022 10:47:29 - INFO - __main__ - Global step 400 Train loss 5.63 Classification-F1 0.0 on epoch=199
06/24/2022 10:47:31 - INFO - __main__ - Step 410 Global step 410 Train loss 5.49 on epoch=204
06/24/2022 10:47:32 - INFO - __main__ - Step 420 Global step 420 Train loss 5.59 on epoch=209
06/24/2022 10:47:33 - INFO - __main__ - Step 430 Global step 430 Train loss 5.49 on epoch=214
06/24/2022 10:47:34 - INFO - __main__ - Step 440 Global step 440 Train loss 5.51 on epoch=219
06/24/2022 10:47:36 - INFO - __main__ - Step 450 Global step 450 Train loss 5.51 on epoch=224
06/24/2022 10:47:36 - INFO - __main__ - Global step 450 Train loss 5.52 Classification-F1 0.0 on epoch=224
06/24/2022 10:47:38 - INFO - __main__ - Step 460 Global step 460 Train loss 5.42 on epoch=229
06/24/2022 10:47:39 - INFO - __main__ - Step 470 Global step 470 Train loss 5.45 on epoch=234
06/24/2022 10:47:40 - INFO - __main__ - Step 480 Global step 480 Train loss 5.45 on epoch=239
06/24/2022 10:47:41 - INFO - __main__ - Step 490 Global step 490 Train loss 5.42 on epoch=244
06/24/2022 10:47:43 - INFO - __main__ - Step 500 Global step 500 Train loss 5.42 on epoch=249
06/24/2022 10:47:44 - INFO - __main__ - Global step 500 Train loss 5.43 Classification-F1 0.0 on epoch=249
06/24/2022 10:47:45 - INFO - __main__ - Step 510 Global step 510 Train loss 5.31 on epoch=254
06/24/2022 10:47:47 - INFO - __main__ - Step 520 Global step 520 Train loss 5.27 on epoch=259
06/24/2022 10:47:48 - INFO - __main__ - Step 530 Global step 530 Train loss 5.31 on epoch=264
06/24/2022 10:47:49 - INFO - __main__ - Step 540 Global step 540 Train loss 5.29 on epoch=269
06/24/2022 10:47:50 - INFO - __main__ - Step 550 Global step 550 Train loss 5.37 on epoch=274
06/24/2022 10:47:51 - INFO - __main__ - Global step 550 Train loss 5.31 Classification-F1 0.0 on epoch=274
06/24/2022 10:47:52 - INFO - __main__ - Step 560 Global step 560 Train loss 5.23 on epoch=279
06/24/2022 10:47:54 - INFO - __main__ - Step 570 Global step 570 Train loss 5.24 on epoch=284
06/24/2022 10:47:55 - INFO - __main__ - Step 580 Global step 580 Train loss 5.20 on epoch=289
06/24/2022 10:47:56 - INFO - __main__ - Step 590 Global step 590 Train loss 5.26 on epoch=294
06/24/2022 10:47:58 - INFO - __main__ - Step 600 Global step 600 Train loss 5.09 on epoch=299
06/24/2022 10:48:04 - INFO - __main__ - Global step 600 Train loss 5.20 Classification-F1 0.0 on epoch=299
06/24/2022 10:48:05 - INFO - __main__ - Step 610 Global step 610 Train loss 5.12 on epoch=304
06/24/2022 10:48:06 - INFO - __main__ - Step 620 Global step 620 Train loss 5.18 on epoch=309
06/24/2022 10:48:07 - INFO - __main__ - Step 630 Global step 630 Train loss 5.10 on epoch=314
06/24/2022 10:48:09 - INFO - __main__ - Step 640 Global step 640 Train loss 5.08 on epoch=319
06/24/2022 10:48:10 - INFO - __main__ - Step 650 Global step 650 Train loss 5.07 on epoch=324
06/24/2022 10:48:21 - INFO - __main__ - Global step 650 Train loss 5.11 Classification-F1 0.0 on epoch=324
06/24/2022 10:48:22 - INFO - __main__ - Step 660 Global step 660 Train loss 4.98 on epoch=329
06/24/2022 10:48:23 - INFO - __main__ - Step 670 Global step 670 Train loss 5.00 on epoch=334
06/24/2022 10:48:25 - INFO - __main__ - Step 680 Global step 680 Train loss 4.89 on epoch=339
06/24/2022 10:48:26 - INFO - __main__ - Step 690 Global step 690 Train loss 4.86 on epoch=344
06/24/2022 10:48:27 - INFO - __main__ - Step 700 Global step 700 Train loss 4.80 on epoch=349
06/24/2022 10:48:38 - INFO - __main__ - Global step 700 Train loss 4.91 Classification-F1 0.0 on epoch=349
06/24/2022 10:48:39 - INFO - __main__ - Step 710 Global step 710 Train loss 4.71 on epoch=354
06/24/2022 10:48:41 - INFO - __main__ - Step 720 Global step 720 Train loss 4.56 on epoch=359
06/24/2022 10:48:42 - INFO - __main__ - Step 730 Global step 730 Train loss 4.61 on epoch=364
06/24/2022 10:48:43 - INFO - __main__ - Step 740 Global step 740 Train loss 4.68 on epoch=369
06/24/2022 10:48:45 - INFO - __main__ - Step 750 Global step 750 Train loss 4.66 on epoch=374
06/24/2022 10:48:56 - INFO - __main__ - Global step 750 Train loss 4.64 Classification-F1 0.0 on epoch=374
06/24/2022 10:48:57 - INFO - __main__ - Step 760 Global step 760 Train loss 4.47 on epoch=379
06/24/2022 10:48:58 - INFO - __main__ - Step 770 Global step 770 Train loss 4.57 on epoch=384
06/24/2022 10:49:00 - INFO - __main__ - Step 780 Global step 780 Train loss 4.44 on epoch=389
06/24/2022 10:49:01 - INFO - __main__ - Step 790 Global step 790 Train loss 4.40 on epoch=394
06/24/2022 10:49:02 - INFO - __main__ - Step 800 Global step 800 Train loss 4.33 on epoch=399
06/24/2022 10:49:14 - INFO - __main__ - Global step 800 Train loss 4.44 Classification-F1 0.0 on epoch=399
06/24/2022 10:49:16 - INFO - __main__ - Step 810 Global step 810 Train loss 4.30 on epoch=404
06/24/2022 10:49:17 - INFO - __main__ - Step 820 Global step 820 Train loss 4.27 on epoch=409
06/24/2022 10:49:18 - INFO - __main__ - Step 830 Global step 830 Train loss 4.23 on epoch=414
06/24/2022 10:49:19 - INFO - __main__ - Step 840 Global step 840 Train loss 4.26 on epoch=419
06/24/2022 10:49:21 - INFO - __main__ - Step 850 Global step 850 Train loss 4.30 on epoch=424
06/24/2022 10:49:33 - INFO - __main__ - Global step 850 Train loss 4.27 Classification-F1 0.0 on epoch=424
06/24/2022 10:49:34 - INFO - __main__ - Step 860 Global step 860 Train loss 4.12 on epoch=429
06/24/2022 10:49:35 - INFO - __main__ - Step 870 Global step 870 Train loss 4.13 on epoch=434
06/24/2022 10:49:36 - INFO - __main__ - Step 880 Global step 880 Train loss 4.11 on epoch=439
06/24/2022 10:49:38 - INFO - __main__ - Step 890 Global step 890 Train loss 4.11 on epoch=444
06/24/2022 10:49:39 - INFO - __main__ - Step 900 Global step 900 Train loss 4.20 on epoch=449
06/24/2022 10:49:51 - INFO - __main__ - Global step 900 Train loss 4.13 Classification-F1 0.0 on epoch=449
06/24/2022 10:49:52 - INFO - __main__ - Step 910 Global step 910 Train loss 4.02 on epoch=454
06/24/2022 10:49:53 - INFO - __main__ - Step 920 Global step 920 Train loss 4.12 on epoch=459
06/24/2022 10:49:55 - INFO - __main__ - Step 930 Global step 930 Train loss 4.11 on epoch=464
06/24/2022 10:49:56 - INFO - __main__ - Step 940 Global step 940 Train loss 3.97 on epoch=469
06/24/2022 10:49:57 - INFO - __main__ - Step 950 Global step 950 Train loss 3.85 on epoch=474
06/24/2022 10:50:09 - INFO - __main__ - Global step 950 Train loss 4.02 Classification-F1 0.0 on epoch=474
06/24/2022 10:50:10 - INFO - __main__ - Step 960 Global step 960 Train loss 3.94 on epoch=479
06/24/2022 10:50:12 - INFO - __main__ - Step 970 Global step 970 Train loss 3.94 on epoch=484
06/24/2022 10:50:13 - INFO - __main__ - Step 980 Global step 980 Train loss 3.94 on epoch=489
06/24/2022 10:50:14 - INFO - __main__ - Step 990 Global step 990 Train loss 3.89 on epoch=494
06/24/2022 10:50:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.91 on epoch=499
06/24/2022 10:50:26 - INFO - __main__ - Global step 1000 Train loss 3.93 Classification-F1 0.0 on epoch=499
06/24/2022 10:50:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.93 on epoch=504
06/24/2022 10:50:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.94 on epoch=509
06/24/2022 10:50:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.71 on epoch=514
06/24/2022 10:50:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.64 on epoch=519
06/24/2022 10:50:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.82 on epoch=524
06/24/2022 10:50:44 - INFO - __main__ - Global step 1050 Train loss 3.81 Classification-F1 0.0 on epoch=524
06/24/2022 10:50:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.76 on epoch=529
06/24/2022 10:50:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.81 on epoch=534
06/24/2022 10:50:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.82 on epoch=539
06/24/2022 10:50:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.71 on epoch=544
06/24/2022 10:50:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.69 on epoch=549
06/24/2022 10:51:02 - INFO - __main__ - Global step 1100 Train loss 3.76 Classification-F1 0.0 on epoch=549
06/24/2022 10:51:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.72 on epoch=554
06/24/2022 10:51:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.68 on epoch=559
06/24/2022 10:51:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.70 on epoch=564
06/24/2022 10:51:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.68 on epoch=569
06/24/2022 10:51:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.72 on epoch=574
06/24/2022 10:51:21 - INFO - __main__ - Global step 1150 Train loss 3.70 Classification-F1 0.0 on epoch=574
06/24/2022 10:51:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.62 on epoch=579
06/24/2022 10:51:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.61 on epoch=584
06/24/2022 10:51:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.76 on epoch=589
06/24/2022 10:51:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.59 on epoch=594
06/24/2022 10:51:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.66 on epoch=599
06/24/2022 10:51:39 - INFO - __main__ - Global step 1200 Train loss 3.65 Classification-F1 0.0 on epoch=599
06/24/2022 10:51:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.46 on epoch=604
06/24/2022 10:51:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.53 on epoch=609
06/24/2022 10:51:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.74 on epoch=614
06/24/2022 10:51:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.60 on epoch=619
06/24/2022 10:51:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.51 on epoch=624
06/24/2022 10:51:57 - INFO - __main__ - Global step 1250 Train loss 3.57 Classification-F1 0.0 on epoch=624
06/24/2022 10:51:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.59 on epoch=629
06/24/2022 10:52:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.61 on epoch=634
06/24/2022 10:52:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.51 on epoch=639
06/24/2022 10:52:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.53 on epoch=644
06/24/2022 10:52:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.51 on epoch=649
06/24/2022 10:52:16 - INFO - __main__ - Global step 1300 Train loss 3.55 Classification-F1 0.0 on epoch=649
06/24/2022 10:52:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.47 on epoch=654
06/24/2022 10:52:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.42 on epoch=659
06/24/2022 10:52:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 3.50 on epoch=664
06/24/2022 10:52:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 3.46 on epoch=669
06/24/2022 10:52:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 3.35 on epoch=674
06/24/2022 10:52:33 - INFO - __main__ - Global step 1350 Train loss 3.44 Classification-F1 0.0 on epoch=674
06/24/2022 10:52:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 3.34 on epoch=679
06/24/2022 10:52:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 3.41 on epoch=684
06/24/2022 10:52:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 3.42 on epoch=689
06/24/2022 10:52:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 3.31 on epoch=694
06/24/2022 10:52:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 3.31 on epoch=699
06/24/2022 10:52:51 - INFO - __main__ - Global step 1400 Train loss 3.36 Classification-F1 0.0 on epoch=699
06/24/2022 10:52:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 3.19 on epoch=704
06/24/2022 10:52:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 3.52 on epoch=709
06/24/2022 10:52:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 3.31 on epoch=714
06/24/2022 10:52:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 3.42 on epoch=719
06/24/2022 10:52:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 3.30 on epoch=724
06/24/2022 10:53:09 - INFO - __main__ - Global step 1450 Train loss 3.35 Classification-F1 0.0 on epoch=724
06/24/2022 10:53:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 3.14 on epoch=729
06/24/2022 10:53:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 3.15 on epoch=734
06/24/2022 10:53:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 3.26 on epoch=739
06/24/2022 10:53:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 3.15 on epoch=744
06/24/2022 10:53:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 3.12 on epoch=749
06/24/2022 10:53:27 - INFO - __main__ - Global step 1500 Train loss 3.17 Classification-F1 0.006578947368421052 on epoch=749
06/24/2022 10:53:27 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.006578947368421052 on epoch=749, global_step=1500
06/24/2022 10:53:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 3.03 on epoch=754
06/24/2022 10:53:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 3.24 on epoch=759
06/24/2022 10:53:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 3.17 on epoch=764
06/24/2022 10:53:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 3.16 on epoch=769
06/24/2022 10:53:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 3.04 on epoch=774
06/24/2022 10:53:43 - INFO - __main__ - Global step 1550 Train loss 3.13 Classification-F1 0.030769230769230767 on epoch=774
06/24/2022 10:53:43 - INFO - __main__ - Saving model with best Classification-F1: 0.006578947368421052 -> 0.030769230769230767 on epoch=774, global_step=1550
06/24/2022 10:53:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 3.02 on epoch=779
06/24/2022 10:53:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 3.09 on epoch=784
06/24/2022 10:53:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 3.00 on epoch=789
06/24/2022 10:53:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 2.94 on epoch=794
06/24/2022 10:53:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 3.06 on epoch=799
06/24/2022 10:53:57 - INFO - __main__ - Global step 1600 Train loss 3.02 Classification-F1 0.1590909090909091 on epoch=799
06/24/2022 10:53:57 - INFO - __main__ - Saving model with best Classification-F1: 0.030769230769230767 -> 0.1590909090909091 on epoch=799, global_step=1600
06/24/2022 10:53:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 2.99 on epoch=804
06/24/2022 10:54:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.94 on epoch=809
06/24/2022 10:54:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 2.87 on epoch=814
06/24/2022 10:54:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 2.91 on epoch=819
06/24/2022 10:54:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 2.87 on epoch=824
06/24/2022 10:54:10 - INFO - __main__ - Global step 1650 Train loss 2.92 Classification-F1 0.07926829268292683 on epoch=824
06/24/2022 10:54:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 3.00 on epoch=829
06/24/2022 10:54:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 2.98 on epoch=834
06/24/2022 10:54:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 2.72 on epoch=839
06/24/2022 10:54:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 2.94 on epoch=844
06/24/2022 10:54:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 2.80 on epoch=849
06/24/2022 10:54:18 - INFO - __main__ - Global step 1700 Train loss 2.89 Classification-F1 0.09401709401709403 on epoch=849
06/24/2022 10:54:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 2.77 on epoch=854
06/24/2022 10:54:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 2.76 on epoch=859
06/24/2022 10:54:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 2.63 on epoch=864
06/24/2022 10:54:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 2.74 on epoch=869
06/24/2022 10:54:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 2.69 on epoch=874
06/24/2022 10:54:26 - INFO - __main__ - Global step 1750 Train loss 2.72 Classification-F1 0.13333333333333336 on epoch=874
06/24/2022 10:54:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 2.72 on epoch=879
06/24/2022 10:54:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 2.78 on epoch=884
06/24/2022 10:54:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 2.64 on epoch=889
06/24/2022 10:54:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 2.68 on epoch=894
06/24/2022 10:54:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 2.67 on epoch=899
06/24/2022 10:54:33 - INFO - __main__ - Global step 1800 Train loss 2.70 Classification-F1 0.27109974424552435 on epoch=899
06/24/2022 10:54:33 - INFO - __main__ - Saving model with best Classification-F1: 0.1590909090909091 -> 0.27109974424552435 on epoch=899, global_step=1800
06/24/2022 10:54:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 2.61 on epoch=904
06/24/2022 10:54:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 2.46 on epoch=909
06/24/2022 10:54:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 2.63 on epoch=914
06/24/2022 10:54:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 2.48 on epoch=919
06/24/2022 10:54:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 2.38 on epoch=924
06/24/2022 10:54:41 - INFO - __main__ - Global step 1850 Train loss 2.51 Classification-F1 0.12631578947368421 on epoch=924
06/24/2022 10:54:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 2.38 on epoch=929
06/24/2022 10:54:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 2.42 on epoch=934
06/24/2022 10:54:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 2.29 on epoch=939
06/24/2022 10:54:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 2.30 on epoch=944
06/24/2022 10:54:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 2.50 on epoch=949
06/24/2022 10:54:50 - INFO - __main__ - Global step 1900 Train loss 2.38 Classification-F1 0.20148148148148148 on epoch=949
06/24/2022 10:54:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 2.21 on epoch=954
06/24/2022 10:54:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 2.27 on epoch=959
06/24/2022 10:54:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 2.06 on epoch=964
06/24/2022 10:54:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.99 on epoch=969
06/24/2022 10:54:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.94 on epoch=974
06/24/2022 10:54:58 - INFO - __main__ - Global step 1950 Train loss 2.09 Classification-F1 0.4458874458874459 on epoch=974
06/24/2022 10:54:58 - INFO - __main__ - Saving model with best Classification-F1: 0.27109974424552435 -> 0.4458874458874459 on epoch=974, global_step=1950
06/24/2022 10:55:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 2.12 on epoch=979
06/24/2022 10:55:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 2.07 on epoch=984
06/24/2022 10:55:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.98 on epoch=989
06/24/2022 10:55:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.91 on epoch=994
06/24/2022 10:55:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.83 on epoch=999
06/24/2022 10:55:05 - INFO - __main__ - Global step 2000 Train loss 1.98 Classification-F1 0.36374269005847953 on epoch=999
06/24/2022 10:55:05 - INFO - __main__ - save last model!
06/24/2022 10:55:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 10:55:05 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 10:55:05 - INFO - __main__ - Printing 3 examples
06/24/2022 10:55:05 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 10:55:05 - INFO - __main__ - ['0']
06/24/2022 10:55:05 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 10:55:05 - INFO - __main__ - ['1']
06/24/2022 10:55:05 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 10:55:05 - INFO - __main__ - ['1']
06/24/2022 10:55:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 10:55:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:55:06 - INFO - __main__ - Printing 3 examples
06/24/2022 10:55:06 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 10:55:06 - INFO - __main__ - ['1']
06/24/2022 10:55:06 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 10:55:06 - INFO - __main__ - ['1']
06/24/2022 10:55:06 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 10:55:06 - INFO - __main__ - ['1']
06/24/2022 10:55:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 10:55:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:55:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:55:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:55:06 - INFO - __main__ - Printing 3 examples
06/24/2022 10:55:06 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 10:55:06 - INFO - __main__ - ['1']
06/24/2022 10:55:06 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 10:55:06 - INFO - __main__ - ['1']
06/24/2022 10:55:06 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 10:55:06 - INFO - __main__ - ['1']
06/24/2022 10:55:06 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:55:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:55:06 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:55:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:55:12 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:55:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:55:12 - INFO - __main__ - Starting training!
06/24/2022 10:55:17 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 10:57:01 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_100_0.2_8_predictions.txt
06/24/2022 10:57:01 - INFO - __main__ - Classification-F1 on test data: 0.2554
06/24/2022 10:57:01 - INFO - __main__ - prefix=paws_16_100, lr=0.2, bsz=8, dev_performance=0.4458874458874459, test_performance=0.255418618844434
06/24/2022 10:57:01 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.5, bsz=8 ...
06/24/2022 10:57:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:57:02 - INFO - __main__ - Printing 3 examples
06/24/2022 10:57:02 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 10:57:02 - INFO - __main__ - ['1']
06/24/2022 10:57:02 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 10:57:02 - INFO - __main__ - ['1']
06/24/2022 10:57:02 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 10:57:02 - INFO - __main__ - ['1']
06/24/2022 10:57:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 10:57:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:57:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 10:57:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 10:57:02 - INFO - __main__ - Printing 3 examples
06/24/2022 10:57:02 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 10:57:02 - INFO - __main__ - ['1']
06/24/2022 10:57:02 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 10:57:02 - INFO - __main__ - ['1']
06/24/2022 10:57:02 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 10:57:02 - INFO - __main__ - ['1']
06/24/2022 10:57:02 - INFO - __main__ - Tokenizing Input ...
06/24/2022 10:57:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 10:57:02 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 10:57:08 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 10:57:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 10:57:08 - INFO - __main__ - Starting training!
06/24/2022 10:57:10 - INFO - __main__ - Step 10 Global step 10 Train loss 5.94 on epoch=4
06/24/2022 10:57:11 - INFO - __main__ - Step 20 Global step 20 Train loss 5.91 on epoch=9
06/24/2022 10:57:12 - INFO - __main__ - Step 30 Global step 30 Train loss 5.84 on epoch=14
06/24/2022 10:57:13 - INFO - __main__ - Step 40 Global step 40 Train loss 5.85 on epoch=19
06/24/2022 10:57:15 - INFO - __main__ - Step 50 Global step 50 Train loss 5.91 on epoch=24
06/24/2022 10:57:16 - INFO - __main__ - Global step 50 Train loss 5.89 Classification-F1 0.0 on epoch=24
06/24/2022 10:57:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 10:57:17 - INFO - __main__ - Step 60 Global step 60 Train loss 5.80 on epoch=29
06/24/2022 10:57:19 - INFO - __main__ - Step 70 Global step 70 Train loss 5.76 on epoch=34
06/24/2022 10:57:20 - INFO - __main__ - Step 80 Global step 80 Train loss 5.77 on epoch=39
06/24/2022 10:57:21 - INFO - __main__ - Step 90 Global step 90 Train loss 5.74 on epoch=44
06/24/2022 10:57:22 - INFO - __main__ - Step 100 Global step 100 Train loss 5.68 on epoch=49
06/24/2022 10:57:24 - INFO - __main__ - Global step 100 Train loss 5.75 Classification-F1 0.0 on epoch=49
06/24/2022 10:57:25 - INFO - __main__ - Step 110 Global step 110 Train loss 5.70 on epoch=54
06/24/2022 10:57:27 - INFO - __main__ - Step 120 Global step 120 Train loss 5.72 on epoch=59
06/24/2022 10:57:28 - INFO - __main__ - Step 130 Global step 130 Train loss 5.57 on epoch=64
06/24/2022 10:57:29 - INFO - __main__ - Step 140 Global step 140 Train loss 5.69 on epoch=69
06/24/2022 10:57:30 - INFO - __main__ - Step 150 Global step 150 Train loss 5.47 on epoch=74
06/24/2022 10:57:32 - INFO - __main__ - Global step 150 Train loss 5.63 Classification-F1 0.0 on epoch=74
06/24/2022 10:57:33 - INFO - __main__ - Step 160 Global step 160 Train loss 5.55 on epoch=79
06/24/2022 10:57:34 - INFO - __main__ - Step 170 Global step 170 Train loss 5.54 on epoch=84
06/24/2022 10:57:35 - INFO - __main__ - Step 180 Global step 180 Train loss 5.54 on epoch=89
06/24/2022 10:57:37 - INFO - __main__ - Step 190 Global step 190 Train loss 5.50 on epoch=94
06/24/2022 10:57:38 - INFO - __main__ - Step 200 Global step 200 Train loss 5.35 on epoch=99
06/24/2022 10:57:41 - INFO - __main__ - Global step 200 Train loss 5.50 Classification-F1 0.0 on epoch=99
06/24/2022 10:57:42 - INFO - __main__ - Step 210 Global step 210 Train loss 5.22 on epoch=104
06/24/2022 10:57:43 - INFO - __main__ - Step 220 Global step 220 Train loss 5.04 on epoch=109
06/24/2022 10:57:45 - INFO - __main__ - Step 230 Global step 230 Train loss 4.84 on epoch=114
06/24/2022 10:57:46 - INFO - __main__ - Step 240 Global step 240 Train loss 4.57 on epoch=119
06/24/2022 10:57:47 - INFO - __main__ - Step 250 Global step 250 Train loss 4.29 on epoch=124
06/24/2022 10:57:53 - INFO - __main__ - Global step 250 Train loss 4.79 Classification-F1 0.0 on epoch=124
06/24/2022 10:57:55 - INFO - __main__ - Step 260 Global step 260 Train loss 4.17 on epoch=129
06/24/2022 10:57:56 - INFO - __main__ - Step 270 Global step 270 Train loss 4.01 on epoch=134
06/24/2022 10:57:57 - INFO - __main__ - Step 280 Global step 280 Train loss 4.42 on epoch=139
06/24/2022 10:57:58 - INFO - __main__ - Step 290 Global step 290 Train loss 4.29 on epoch=144
06/24/2022 10:58:00 - INFO - __main__ - Step 300 Global step 300 Train loss 4.20 on epoch=149
06/24/2022 10:58:08 - INFO - __main__ - Global step 300 Train loss 4.22 Classification-F1 0.0 on epoch=149
06/24/2022 10:58:09 - INFO - __main__ - Step 310 Global step 310 Train loss 4.18 on epoch=154
06/24/2022 10:58:11 - INFO - __main__ - Step 320 Global step 320 Train loss 3.97 on epoch=159
06/24/2022 10:58:12 - INFO - __main__ - Step 330 Global step 330 Train loss 3.80 on epoch=164
06/24/2022 10:58:13 - INFO - __main__ - Step 340 Global step 340 Train loss 3.65 on epoch=169
06/24/2022 10:58:14 - INFO - __main__ - Step 350 Global step 350 Train loss 3.64 on epoch=174
06/24/2022 10:58:25 - INFO - __main__ - Global step 350 Train loss 3.85 Classification-F1 0.0 on epoch=174
06/24/2022 10:58:26 - INFO - __main__ - Step 360 Global step 360 Train loss 3.66 on epoch=179
06/24/2022 10:58:27 - INFO - __main__ - Step 370 Global step 370 Train loss 3.73 on epoch=184
06/24/2022 10:58:29 - INFO - __main__ - Step 380 Global step 380 Train loss 3.56 on epoch=189
06/24/2022 10:58:30 - INFO - __main__ - Step 390 Global step 390 Train loss 3.52 on epoch=194
06/24/2022 10:58:31 - INFO - __main__ - Step 400 Global step 400 Train loss 3.48 on epoch=199
06/24/2022 10:58:42 - INFO - __main__ - Global step 400 Train loss 3.59 Classification-F1 0.0 on epoch=199
06/24/2022 10:58:43 - INFO - __main__ - Step 410 Global step 410 Train loss 3.49 on epoch=204
06/24/2022 10:58:44 - INFO - __main__ - Step 420 Global step 420 Train loss 3.34 on epoch=209
06/24/2022 10:58:46 - INFO - __main__ - Step 430 Global step 430 Train loss 3.41 on epoch=214
06/24/2022 10:58:47 - INFO - __main__ - Step 440 Global step 440 Train loss 3.36 on epoch=219
06/24/2022 10:58:48 - INFO - __main__ - Step 450 Global step 450 Train loss 3.29 on epoch=224
06/24/2022 10:58:59 - INFO - __main__ - Global step 450 Train loss 3.38 Classification-F1 0.0 on epoch=224
06/24/2022 10:59:00 - INFO - __main__ - Step 460 Global step 460 Train loss 3.21 on epoch=229
06/24/2022 10:59:01 - INFO - __main__ - Step 470 Global step 470 Train loss 3.16 on epoch=234
06/24/2022 10:59:02 - INFO - __main__ - Step 480 Global step 480 Train loss 3.17 on epoch=239
06/24/2022 10:59:04 - INFO - __main__ - Step 490 Global step 490 Train loss 3.20 on epoch=244
06/24/2022 10:59:05 - INFO - __main__ - Step 500 Global step 500 Train loss 2.93 on epoch=249
06/24/2022 10:59:12 - INFO - __main__ - Global step 500 Train loss 3.13 Classification-F1 0.07407407407407408 on epoch=249
06/24/2022 10:59:12 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.07407407407407408 on epoch=249, global_step=500
06/24/2022 10:59:13 - INFO - __main__ - Step 510 Global step 510 Train loss 2.81 on epoch=254
06/24/2022 10:59:14 - INFO - __main__ - Step 520 Global step 520 Train loss 2.81 on epoch=259
06/24/2022 10:59:15 - INFO - __main__ - Step 530 Global step 530 Train loss 2.80 on epoch=264
06/24/2022 10:59:17 - INFO - __main__ - Step 540 Global step 540 Train loss 2.65 on epoch=269
06/24/2022 10:59:18 - INFO - __main__ - Step 550 Global step 550 Train loss 2.41 on epoch=274
06/24/2022 10:59:23 - INFO - __main__ - Global step 550 Train loss 2.70 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 10:59:23 - INFO - __main__ - Saving model with best Classification-F1: 0.07407407407407408 -> 0.3333333333333333 on epoch=274, global_step=550
06/24/2022 10:59:24 - INFO - __main__ - Step 560 Global step 560 Train loss 2.49 on epoch=279
06/24/2022 10:59:26 - INFO - __main__ - Step 570 Global step 570 Train loss 2.50 on epoch=284
06/24/2022 10:59:27 - INFO - __main__ - Step 580 Global step 580 Train loss 2.44 on epoch=289
06/24/2022 10:59:28 - INFO - __main__ - Step 590 Global step 590 Train loss 2.21 on epoch=294
06/24/2022 10:59:29 - INFO - __main__ - Step 600 Global step 600 Train loss 2.27 on epoch=299
06/24/2022 10:59:30 - INFO - __main__ - Global step 600 Train loss 2.38 Classification-F1 0.37662337662337664 on epoch=299
06/24/2022 10:59:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.37662337662337664 on epoch=299, global_step=600
06/24/2022 10:59:31 - INFO - __main__ - Step 610 Global step 610 Train loss 2.10 on epoch=304
06/24/2022 10:59:32 - INFO - __main__ - Step 620 Global step 620 Train loss 1.94 on epoch=309
06/24/2022 10:59:34 - INFO - __main__ - Step 630 Global step 630 Train loss 1.96 on epoch=314
06/24/2022 10:59:35 - INFO - __main__ - Step 640 Global step 640 Train loss 1.79 on epoch=319
06/24/2022 10:59:36 - INFO - __main__ - Step 650 Global step 650 Train loss 1.74 on epoch=324
06/24/2022 10:59:37 - INFO - __main__ - Global step 650 Train loss 1.91 Classification-F1 0.3333333333333333 on epoch=324
06/24/2022 10:59:38 - INFO - __main__ - Step 660 Global step 660 Train loss 1.36 on epoch=329
06/24/2022 10:59:39 - INFO - __main__ - Step 670 Global step 670 Train loss 1.42 on epoch=334
06/24/2022 10:59:40 - INFO - __main__ - Step 680 Global step 680 Train loss 1.54 on epoch=339
06/24/2022 10:59:42 - INFO - __main__ - Step 690 Global step 690 Train loss 1.30 on epoch=344
06/24/2022 10:59:43 - INFO - __main__ - Step 700 Global step 700 Train loss 1.37 on epoch=349
06/24/2022 10:59:43 - INFO - __main__ - Global step 700 Train loss 1.40 Classification-F1 0.539313399778516 on epoch=349
06/24/2022 10:59:43 - INFO - __main__ - Saving model with best Classification-F1: 0.37662337662337664 -> 0.539313399778516 on epoch=349, global_step=700
06/24/2022 10:59:45 - INFO - __main__ - Step 710 Global step 710 Train loss 1.35 on epoch=354
06/24/2022 10:59:46 - INFO - __main__ - Step 720 Global step 720 Train loss 1.36 on epoch=359
06/24/2022 10:59:47 - INFO - __main__ - Step 730 Global step 730 Train loss 1.19 on epoch=364
06/24/2022 10:59:48 - INFO - __main__ - Step 740 Global step 740 Train loss 1.05 on epoch=369
06/24/2022 10:59:50 - INFO - __main__ - Step 750 Global step 750 Train loss 1.27 on epoch=374
06/24/2022 10:59:50 - INFO - __main__ - Global step 750 Train loss 1.24 Classification-F1 0.6825396825396826 on epoch=374
06/24/2022 10:59:50 - INFO - __main__ - Saving model with best Classification-F1: 0.539313399778516 -> 0.6825396825396826 on epoch=374, global_step=750
06/24/2022 10:59:51 - INFO - __main__ - Step 760 Global step 760 Train loss 1.15 on epoch=379
06/24/2022 10:59:53 - INFO - __main__ - Step 770 Global step 770 Train loss 1.10 on epoch=384
06/24/2022 10:59:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.99 on epoch=389
06/24/2022 10:59:55 - INFO - __main__ - Step 790 Global step 790 Train loss 1.08 on epoch=394
06/24/2022 10:59:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.99 on epoch=399
06/24/2022 10:59:57 - INFO - __main__ - Global step 800 Train loss 1.06 Classification-F1 0.3266888150609081 on epoch=399
06/24/2022 10:59:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.94 on epoch=404
06/24/2022 10:59:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.99 on epoch=409
06/24/2022 11:00:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.90 on epoch=414
06/24/2022 11:00:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.90 on epoch=419
06/24/2022 11:00:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.90 on epoch=424
06/24/2022 11:00:03 - INFO - __main__ - Global step 850 Train loss 0.92 Classification-F1 0.3191489361702127 on epoch=424
06/24/2022 11:00:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.77 on epoch=429
06/24/2022 11:00:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.85 on epoch=434
06/24/2022 11:00:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.84 on epoch=439
06/24/2022 11:00:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.89 on epoch=444
06/24/2022 11:00:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.83 on epoch=449
06/24/2022 11:00:10 - INFO - __main__ - Global step 900 Train loss 0.84 Classification-F1 0.3191489361702127 on epoch=449
06/24/2022 11:00:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.78 on epoch=454
06/24/2022 11:00:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.80 on epoch=459
06/24/2022 11:00:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.83 on epoch=464
06/24/2022 11:00:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.81 on epoch=469
06/24/2022 11:00:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.74 on epoch=474
06/24/2022 11:00:17 - INFO - __main__ - Global step 950 Train loss 0.79 Classification-F1 0.3191489361702127 on epoch=474
06/24/2022 11:00:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.72 on epoch=479
06/24/2022 11:00:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.84 on epoch=484
06/24/2022 11:00:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.84 on epoch=489
06/24/2022 11:00:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.75 on epoch=494
06/24/2022 11:00:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.80 on epoch=499
06/24/2022 11:00:23 - INFO - __main__ - Global step 1000 Train loss 0.79 Classification-F1 0.3816425120772947 on epoch=499
06/24/2022 11:00:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.69 on epoch=504
06/24/2022 11:00:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.84 on epoch=509
06/24/2022 11:00:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.70 on epoch=514
06/24/2022 11:00:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.59 on epoch=519
06/24/2022 11:00:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.67 on epoch=524
06/24/2022 11:00:30 - INFO - __main__ - Global step 1050 Train loss 0.70 Classification-F1 0.3992490613266583 on epoch=524
06/24/2022 11:00:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.69 on epoch=529
06/24/2022 11:00:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.70 on epoch=534
06/24/2022 11:00:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.60 on epoch=539
06/24/2022 11:00:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.68 on epoch=544
06/24/2022 11:00:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.64 on epoch=549
06/24/2022 11:00:37 - INFO - __main__ - Global step 1100 Train loss 0.66 Classification-F1 0.539313399778516 on epoch=549
06/24/2022 11:00:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.61 on epoch=554
06/24/2022 11:00:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.69 on epoch=559
06/24/2022 11:00:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.65 on epoch=564
06/24/2022 11:00:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.69 on epoch=569
06/24/2022 11:00:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.66 on epoch=574
06/24/2022 11:00:44 - INFO - __main__ - Global step 1150 Train loss 0.66 Classification-F1 0.3816425120772947 on epoch=574
06/24/2022 11:00:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.65 on epoch=579
06/24/2022 11:00:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.66 on epoch=584
06/24/2022 11:00:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.68 on epoch=589
06/24/2022 11:00:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.58 on epoch=594
06/24/2022 11:00:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.72 on epoch=599
06/24/2022 11:00:50 - INFO - __main__ - Global step 1200 Train loss 0.66 Classification-F1 0.4666666666666667 on epoch=599
06/24/2022 11:00:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.68 on epoch=604
06/24/2022 11:00:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.59 on epoch=609
06/24/2022 11:00:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.60 on epoch=614
06/24/2022 11:00:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.57 on epoch=619
06/24/2022 11:00:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.63 on epoch=624
06/24/2022 11:00:57 - INFO - __main__ - Global step 1250 Train loss 0.62 Classification-F1 0.36374269005847953 on epoch=624
06/24/2022 11:00:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=629
06/24/2022 11:00:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.65 on epoch=634
06/24/2022 11:01:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.64 on epoch=639
06/24/2022 11:01:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.64 on epoch=644
06/24/2022 11:01:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.62 on epoch=649
06/24/2022 11:01:04 - INFO - __main__ - Global step 1300 Train loss 0.61 Classification-F1 0.4554554554554554 on epoch=649
06/24/2022 11:01:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.62 on epoch=654
06/24/2022 11:01:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.70 on epoch=659
06/24/2022 11:01:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.65 on epoch=664
06/24/2022 11:01:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.56 on epoch=669
06/24/2022 11:01:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.52 on epoch=674
06/24/2022 11:01:10 - INFO - __main__ - Global step 1350 Train loss 0.61 Classification-F1 0.37662337662337664 on epoch=674
06/24/2022 11:01:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.66 on epoch=679
06/24/2022 11:01:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.55 on epoch=684
06/24/2022 11:01:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.65 on epoch=689
06/24/2022 11:01:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.60 on epoch=694
06/24/2022 11:01:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.68 on epoch=699
06/24/2022 11:01:17 - INFO - __main__ - Global step 1400 Train loss 0.63 Classification-F1 0.4285714285714286 on epoch=699
06/24/2022 11:01:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.55 on epoch=704
06/24/2022 11:01:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.55 on epoch=709
06/24/2022 11:01:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.64 on epoch=714
06/24/2022 11:01:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.62 on epoch=719
06/24/2022 11:01:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.59 on epoch=724
06/24/2022 11:01:24 - INFO - __main__ - Global step 1450 Train loss 0.59 Classification-F1 0.4909862142099682 on epoch=724
06/24/2022 11:01:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.53 on epoch=729
06/24/2022 11:01:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.58 on epoch=734
06/24/2022 11:01:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.60 on epoch=739
06/24/2022 11:01:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.59 on epoch=744
06/24/2022 11:01:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.49 on epoch=749
06/24/2022 11:01:30 - INFO - __main__ - Global step 1500 Train loss 0.56 Classification-F1 0.3816425120772947 on epoch=749
06/24/2022 11:01:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.55 on epoch=754
06/24/2022 11:01:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.62 on epoch=759
06/24/2022 11:01:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=764
06/24/2022 11:01:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.60 on epoch=769
06/24/2022 11:01:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.54 on epoch=774
06/24/2022 11:01:37 - INFO - __main__ - Global step 1550 Train loss 0.56 Classification-F1 0.28888888888888886 on epoch=774
06/24/2022 11:01:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.51 on epoch=779
06/24/2022 11:01:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.59 on epoch=784
06/24/2022 11:01:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.59 on epoch=789
06/24/2022 11:01:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.62 on epoch=794
06/24/2022 11:01:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=799
06/24/2022 11:01:44 - INFO - __main__ - Global step 1600 Train loss 0.56 Classification-F1 0.33793103448275863 on epoch=799
06/24/2022 11:01:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.55 on epoch=804
06/24/2022 11:01:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.50 on epoch=809
06/24/2022 11:01:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.54 on epoch=814
06/24/2022 11:01:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.64 on epoch=819
06/24/2022 11:01:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.50 on epoch=824
06/24/2022 11:01:50 - INFO - __main__ - Global step 1650 Train loss 0.55 Classification-F1 0.4554554554554554 on epoch=824
06/24/2022 11:01:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=829
06/24/2022 11:01:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.50 on epoch=834
06/24/2022 11:01:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.63 on epoch=839
06/24/2022 11:01:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.61 on epoch=844
06/24/2022 11:01:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.52 on epoch=849
06/24/2022 11:01:57 - INFO - __main__ - Global step 1700 Train loss 0.55 Classification-F1 0.4817813765182186 on epoch=849
06/24/2022 11:01:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.58 on epoch=854
06/24/2022 11:02:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.54 on epoch=859
06/24/2022 11:02:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.55 on epoch=864
06/24/2022 11:02:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.56 on epoch=869
06/24/2022 11:02:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.60 on epoch=874
06/24/2022 11:02:04 - INFO - __main__ - Global step 1750 Train loss 0.57 Classification-F1 0.4554554554554554 on epoch=874
06/24/2022 11:02:05 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.59 on epoch=879
06/24/2022 11:02:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.45 on epoch=884
06/24/2022 11:02:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.43 on epoch=889
06/24/2022 11:02:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.60 on epoch=894
06/24/2022 11:02:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.51 on epoch=899
06/24/2022 11:02:10 - INFO - __main__ - Global step 1800 Train loss 0.52 Classification-F1 0.4682306940371457 on epoch=899
06/24/2022 11:02:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.52 on epoch=904
06/24/2022 11:02:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.57 on epoch=909
06/24/2022 11:02:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.48 on epoch=914
06/24/2022 11:02:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.51 on epoch=919
06/24/2022 11:02:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.52 on epoch=924
06/24/2022 11:02:17 - INFO - __main__ - Global step 1850 Train loss 0.52 Classification-F1 0.4458874458874459 on epoch=924
06/24/2022 11:02:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.47 on epoch=929
06/24/2022 11:02:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.58 on epoch=934
06/24/2022 11:02:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.49 on epoch=939
06/24/2022 11:02:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.41 on epoch=944
06/24/2022 11:02:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.52 on epoch=949
06/24/2022 11:02:24 - INFO - __main__ - Global step 1900 Train loss 0.50 Classification-F1 0.3073593073593074 on epoch=949
06/24/2022 11:02:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.52 on epoch=954
06/24/2022 11:02:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=959
06/24/2022 11:02:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.51 on epoch=964
06/24/2022 11:02:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.46 on epoch=969
06/24/2022 11:02:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.54 on epoch=974
06/24/2022 11:02:30 - INFO - __main__ - Global step 1950 Train loss 0.50 Classification-F1 0.39756367663344405 on epoch=974
06/24/2022 11:02:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.53 on epoch=979
06/24/2022 11:02:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=984
06/24/2022 11:02:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.49 on epoch=989
06/24/2022 11:02:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.56 on epoch=994
06/24/2022 11:02:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.53 on epoch=999
06/24/2022 11:02:37 - INFO - __main__ - Global step 2000 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=999
06/24/2022 11:02:37 - INFO - __main__ - save last model!
06/24/2022 11:02:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 11:02:37 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 11:02:37 - INFO - __main__ - Printing 3 examples
06/24/2022 11:02:37 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 11:02:37 - INFO - __main__ - ['0']
06/24/2022 11:02:37 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 11:02:37 - INFO - __main__ - ['1']
06/24/2022 11:02:37 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 11:02:37 - INFO - __main__ - ['1']
06/24/2022 11:02:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:02:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:02:38 - INFO - __main__ - Printing 3 examples
06/24/2022 11:02:38 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 11:02:38 - INFO - __main__ - ['1']
06/24/2022 11:02:38 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 11:02:38 - INFO - __main__ - ['1']
06/24/2022 11:02:38 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 11:02:38 - INFO - __main__ - ['1']
06/24/2022 11:02:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 11:02:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:02:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:02:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:02:38 - INFO - __main__ - Printing 3 examples
06/24/2022 11:02:38 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 11:02:38 - INFO - __main__ - ['1']
06/24/2022 11:02:38 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 11:02:38 - INFO - __main__ - ['1']
06/24/2022 11:02:38 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 11:02:38 - INFO - __main__ - ['1']
06/24/2022 11:02:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:02:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:02:38 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:02:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:02:43 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:02:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:02:43 - INFO - __main__ - Starting training!
06/24/2022 11:02:49 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 11:04:18 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_13_0.5_8_predictions.txt
06/24/2022 11:04:18 - INFO - __main__ - Classification-F1 on test data: 0.4948
06/24/2022 11:04:18 - INFO - __main__ - prefix=paws_16_13, lr=0.5, bsz=8, dev_performance=0.6825396825396826, test_performance=0.49477599376143055
06/24/2022 11:04:18 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.4, bsz=8 ...
06/24/2022 11:04:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:04:19 - INFO - __main__ - Printing 3 examples
06/24/2022 11:04:19 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 11:04:19 - INFO - __main__ - ['1']
06/24/2022 11:04:19 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 11:04:19 - INFO - __main__ - ['1']
06/24/2022 11:04:19 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 11:04:19 - INFO - __main__ - ['1']
06/24/2022 11:04:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:04:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:04:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:04:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:04:19 - INFO - __main__ - Printing 3 examples
06/24/2022 11:04:19 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 11:04:19 - INFO - __main__ - ['1']
06/24/2022 11:04:19 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 11:04:19 - INFO - __main__ - ['1']
06/24/2022 11:04:19 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 11:04:19 - INFO - __main__ - ['1']
06/24/2022 11:04:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:04:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:04:19 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:04:25 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:04:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:04:25 - INFO - __main__ - Starting training!
06/24/2022 11:04:26 - INFO - __main__ - Step 10 Global step 10 Train loss 5.97 on epoch=4
06/24/2022 11:04:27 - INFO - __main__ - Step 20 Global step 20 Train loss 5.93 on epoch=9
06/24/2022 11:04:29 - INFO - __main__ - Step 30 Global step 30 Train loss 5.91 on epoch=14
06/24/2022 11:04:30 - INFO - __main__ - Step 40 Global step 40 Train loss 5.89 on epoch=19
06/24/2022 11:04:31 - INFO - __main__ - Step 50 Global step 50 Train loss 5.91 on epoch=24
06/24/2022 11:04:33 - INFO - __main__ - Global step 50 Train loss 5.92 Classification-F1 0.0 on epoch=24
06/24/2022 11:04:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 11:04:34 - INFO - __main__ - Step 60 Global step 60 Train loss 5.90 on epoch=29
06/24/2022 11:04:35 - INFO - __main__ - Step 70 Global step 70 Train loss 5.86 on epoch=34
06/24/2022 11:04:36 - INFO - __main__ - Step 80 Global step 80 Train loss 5.86 on epoch=39
06/24/2022 11:04:38 - INFO - __main__ - Step 90 Global step 90 Train loss 5.80 on epoch=44
06/24/2022 11:04:39 - INFO - __main__ - Step 100 Global step 100 Train loss 5.71 on epoch=49
06/24/2022 11:04:40 - INFO - __main__ - Global step 100 Train loss 5.82 Classification-F1 0.0 on epoch=49
06/24/2022 11:04:41 - INFO - __main__ - Step 110 Global step 110 Train loss 5.81 on epoch=54
06/24/2022 11:04:42 - INFO - __main__ - Step 120 Global step 120 Train loss 5.61 on epoch=59
06/24/2022 11:04:43 - INFO - __main__ - Step 130 Global step 130 Train loss 5.70 on epoch=64
06/24/2022 11:04:45 - INFO - __main__ - Step 140 Global step 140 Train loss 5.70 on epoch=69
06/24/2022 11:04:46 - INFO - __main__ - Step 150 Global step 150 Train loss 5.40 on epoch=74
06/24/2022 11:04:52 - INFO - __main__ - Global step 150 Train loss 5.64 Classification-F1 0.0 on epoch=74
06/24/2022 11:04:53 - INFO - __main__ - Step 160 Global step 160 Train loss 5.19 on epoch=79
06/24/2022 11:04:55 - INFO - __main__ - Step 170 Global step 170 Train loss 4.88 on epoch=84
06/24/2022 11:04:56 - INFO - __main__ - Step 180 Global step 180 Train loss 4.65 on epoch=89
06/24/2022 11:04:57 - INFO - __main__ - Step 190 Global step 190 Train loss 4.56 on epoch=94
06/24/2022 11:04:59 - INFO - __main__ - Step 200 Global step 200 Train loss 4.35 on epoch=99
06/24/2022 11:05:09 - INFO - __main__ - Global step 200 Train loss 4.72 Classification-F1 0.0 on epoch=99
06/24/2022 11:05:10 - INFO - __main__ - Step 210 Global step 210 Train loss 4.27 on epoch=104
06/24/2022 11:05:11 - INFO - __main__ - Step 220 Global step 220 Train loss 4.22 on epoch=109
06/24/2022 11:05:12 - INFO - __main__ - Step 230 Global step 230 Train loss 4.23 on epoch=114
06/24/2022 11:05:14 - INFO - __main__ - Step 240 Global step 240 Train loss 4.10 on epoch=119
06/24/2022 11:05:15 - INFO - __main__ - Step 250 Global step 250 Train loss 4.12 on epoch=124
06/24/2022 11:05:25 - INFO - __main__ - Global step 250 Train loss 4.18 Classification-F1 0.0 on epoch=124
06/24/2022 11:05:26 - INFO - __main__ - Step 260 Global step 260 Train loss 3.89 on epoch=129
06/24/2022 11:05:28 - INFO - __main__ - Step 270 Global step 270 Train loss 3.93 on epoch=134
06/24/2022 11:05:29 - INFO - __main__ - Step 280 Global step 280 Train loss 3.90 on epoch=139
06/24/2022 11:05:30 - INFO - __main__ - Step 290 Global step 290 Train loss 3.77 on epoch=144
06/24/2022 11:05:31 - INFO - __main__ - Step 300 Global step 300 Train loss 3.75 on epoch=149
06/24/2022 11:05:35 - INFO - __main__ - Global step 300 Train loss 3.85 Classification-F1 0.0 on epoch=149
06/24/2022 11:05:36 - INFO - __main__ - Step 310 Global step 310 Train loss 3.70 on epoch=154
06/24/2022 11:05:37 - INFO - __main__ - Step 320 Global step 320 Train loss 3.78 on epoch=159
06/24/2022 11:05:38 - INFO - __main__ - Step 330 Global step 330 Train loss 3.56 on epoch=164
06/24/2022 11:05:40 - INFO - __main__ - Step 340 Global step 340 Train loss 3.65 on epoch=169
06/24/2022 11:05:41 - INFO - __main__ - Step 350 Global step 350 Train loss 3.63 on epoch=174
06/24/2022 11:05:48 - INFO - __main__ - Global step 350 Train loss 3.66 Classification-F1 0.0 on epoch=174
06/24/2022 11:05:49 - INFO - __main__ - Step 360 Global step 360 Train loss 3.55 on epoch=179
06/24/2022 11:05:50 - INFO - __main__ - Step 370 Global step 370 Train loss 3.47 on epoch=184
06/24/2022 11:05:51 - INFO - __main__ - Step 380 Global step 380 Train loss 3.46 on epoch=189
06/24/2022 11:05:53 - INFO - __main__ - Step 390 Global step 390 Train loss 3.31 on epoch=194
06/24/2022 11:05:54 - INFO - __main__ - Step 400 Global step 400 Train loss 3.19 on epoch=199
06/24/2022 11:06:01 - INFO - __main__ - Global step 400 Train loss 3.40 Classification-F1 0.0 on epoch=199
06/24/2022 11:06:02 - INFO - __main__ - Step 410 Global step 410 Train loss 3.21 on epoch=204
06/24/2022 11:06:03 - INFO - __main__ - Step 420 Global step 420 Train loss 3.16 on epoch=209
06/24/2022 11:06:04 - INFO - __main__ - Step 430 Global step 430 Train loss 3.16 on epoch=214
06/24/2022 11:06:06 - INFO - __main__ - Step 440 Global step 440 Train loss 3.09 on epoch=219
06/24/2022 11:06:07 - INFO - __main__ - Step 450 Global step 450 Train loss 3.06 on epoch=224
06/24/2022 11:06:10 - INFO - __main__ - Global step 450 Train loss 3.14 Classification-F1 0.007407407407407407 on epoch=224
06/24/2022 11:06:10 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.007407407407407407 on epoch=224, global_step=450
06/24/2022 11:06:11 - INFO - __main__ - Step 460 Global step 460 Train loss 3.02 on epoch=229
06/24/2022 11:06:13 - INFO - __main__ - Step 470 Global step 470 Train loss 2.88 on epoch=234
06/24/2022 11:06:14 - INFO - __main__ - Step 480 Global step 480 Train loss 2.88 on epoch=239
06/24/2022 11:06:15 - INFO - __main__ - Step 490 Global step 490 Train loss 2.71 on epoch=244
06/24/2022 11:06:16 - INFO - __main__ - Step 500 Global step 500 Train loss 2.73 on epoch=249
06/24/2022 11:06:18 - INFO - __main__ - Global step 500 Train loss 2.84 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 11:06:18 - INFO - __main__ - Saving model with best Classification-F1: 0.007407407407407407 -> 0.3333333333333333 on epoch=249, global_step=500
06/24/2022 11:06:20 - INFO - __main__ - Step 510 Global step 510 Train loss 2.71 on epoch=254
06/24/2022 11:06:21 - INFO - __main__ - Step 520 Global step 520 Train loss 2.61 on epoch=259
06/24/2022 11:06:22 - INFO - __main__ - Step 530 Global step 530 Train loss 2.60 on epoch=264
06/24/2022 11:06:23 - INFO - __main__ - Step 540 Global step 540 Train loss 2.54 on epoch=269
06/24/2022 11:06:24 - INFO - __main__ - Step 550 Global step 550 Train loss 2.49 on epoch=274
06/24/2022 11:06:27 - INFO - __main__ - Global step 550 Train loss 2.59 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 11:06:28 - INFO - __main__ - Step 560 Global step 560 Train loss 2.29 on epoch=279
06/24/2022 11:06:30 - INFO - __main__ - Step 570 Global step 570 Train loss 2.38 on epoch=284
06/24/2022 11:06:31 - INFO - __main__ - Step 580 Global step 580 Train loss 2.32 on epoch=289
06/24/2022 11:06:32 - INFO - __main__ - Step 590 Global step 590 Train loss 2.37 on epoch=294
06/24/2022 11:06:33 - INFO - __main__ - Step 600 Global step 600 Train loss 2.13 on epoch=299
06/24/2022 11:06:34 - INFO - __main__ - Global step 600 Train loss 2.30 Classification-F1 0.3333333333333333 on epoch=299
06/24/2022 11:06:35 - INFO - __main__ - Step 610 Global step 610 Train loss 2.32 on epoch=304
06/24/2022 11:06:37 - INFO - __main__ - Step 620 Global step 620 Train loss 2.11 on epoch=309
06/24/2022 11:06:38 - INFO - __main__ - Step 630 Global step 630 Train loss 2.03 on epoch=314
06/24/2022 11:06:39 - INFO - __main__ - Step 640 Global step 640 Train loss 2.00 on epoch=319
06/24/2022 11:06:41 - INFO - __main__ - Step 650 Global step 650 Train loss 2.00 on epoch=324
06/24/2022 11:06:41 - INFO - __main__ - Global step 650 Train loss 2.09 Classification-F1 0.3992490613266583 on epoch=324
06/24/2022 11:06:41 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=324, global_step=650
06/24/2022 11:06:42 - INFO - __main__ - Step 660 Global step 660 Train loss 1.93 on epoch=329
06/24/2022 11:06:44 - INFO - __main__ - Step 670 Global step 670 Train loss 1.83 on epoch=334
06/24/2022 11:06:45 - INFO - __main__ - Step 680 Global step 680 Train loss 1.93 on epoch=339
06/24/2022 11:06:46 - INFO - __main__ - Step 690 Global step 690 Train loss 1.87 on epoch=344
06/24/2022 11:06:47 - INFO - __main__ - Step 700 Global step 700 Train loss 1.86 on epoch=349
06/24/2022 11:06:48 - INFO - __main__ - Global step 700 Train loss 1.88 Classification-F1 0.3333333333333333 on epoch=349
06/24/2022 11:06:49 - INFO - __main__ - Step 710 Global step 710 Train loss 1.75 on epoch=354
06/24/2022 11:06:50 - INFO - __main__ - Step 720 Global step 720 Train loss 1.76 on epoch=359
06/24/2022 11:06:51 - INFO - __main__ - Step 730 Global step 730 Train loss 1.82 on epoch=364
06/24/2022 11:06:53 - INFO - __main__ - Step 740 Global step 740 Train loss 1.68 on epoch=369
06/24/2022 11:06:54 - INFO - __main__ - Step 750 Global step 750 Train loss 1.66 on epoch=374
06/24/2022 11:06:54 - INFO - __main__ - Global step 750 Train loss 1.73 Classification-F1 0.5933528836754642 on epoch=374
06/24/2022 11:06:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5933528836754642 on epoch=374, global_step=750
06/24/2022 11:06:56 - INFO - __main__ - Step 760 Global step 760 Train loss 1.67 on epoch=379
06/24/2022 11:06:57 - INFO - __main__ - Step 770 Global step 770 Train loss 1.61 on epoch=384
06/24/2022 11:06:58 - INFO - __main__ - Step 780 Global step 780 Train loss 1.52 on epoch=389
06/24/2022 11:06:59 - INFO - __main__ - Step 790 Global step 790 Train loss 1.38 on epoch=394
06/24/2022 11:07:01 - INFO - __main__ - Step 800 Global step 800 Train loss 1.42 on epoch=399
06/24/2022 11:07:01 - INFO - __main__ - Global step 800 Train loss 1.52 Classification-F1 0.5555555555555556 on epoch=399
06/24/2022 11:07:02 - INFO - __main__ - Step 810 Global step 810 Train loss 1.54 on epoch=404
06/24/2022 11:07:03 - INFO - __main__ - Step 820 Global step 820 Train loss 1.50 on epoch=409
06/24/2022 11:07:05 - INFO - __main__ - Step 830 Global step 830 Train loss 1.31 on epoch=414
06/24/2022 11:07:06 - INFO - __main__ - Step 840 Global step 840 Train loss 1.42 on epoch=419
06/24/2022 11:07:07 - INFO - __main__ - Step 850 Global step 850 Train loss 1.29 on epoch=424
06/24/2022 11:07:07 - INFO - __main__ - Global step 850 Train loss 1.41 Classification-F1 0.5307917888563051 on epoch=424
06/24/2022 11:07:09 - INFO - __main__ - Step 860 Global step 860 Train loss 1.34 on epoch=429
06/24/2022 11:07:10 - INFO - __main__ - Step 870 Global step 870 Train loss 1.22 on epoch=434
06/24/2022 11:07:11 - INFO - __main__ - Step 880 Global step 880 Train loss 1.24 on epoch=439
06/24/2022 11:07:12 - INFO - __main__ - Step 890 Global step 890 Train loss 1.27 on epoch=444
06/24/2022 11:07:14 - INFO - __main__ - Step 900 Global step 900 Train loss 1.25 on epoch=449
06/24/2022 11:07:14 - INFO - __main__ - Global step 900 Train loss 1.27 Classification-F1 0.46843853820598 on epoch=449
06/24/2022 11:07:15 - INFO - __main__ - Step 910 Global step 910 Train loss 1.23 on epoch=454
06/24/2022 11:07:16 - INFO - __main__ - Step 920 Global step 920 Train loss 1.26 on epoch=459
06/24/2022 11:07:18 - INFO - __main__ - Step 930 Global step 930 Train loss 1.20 on epoch=464
06/24/2022 11:07:19 - INFO - __main__ - Step 940 Global step 940 Train loss 1.13 on epoch=469
06/24/2022 11:07:20 - INFO - __main__ - Step 950 Global step 950 Train loss 1.17 on epoch=474
06/24/2022 11:07:20 - INFO - __main__ - Global step 950 Train loss 1.20 Classification-F1 0.3073593073593074 on epoch=474
06/24/2022 11:07:22 - INFO - __main__ - Step 960 Global step 960 Train loss 1.08 on epoch=479
06/24/2022 11:07:23 - INFO - __main__ - Step 970 Global step 970 Train loss 1.08 on epoch=484
06/24/2022 11:07:24 - INFO - __main__ - Step 980 Global step 980 Train loss 1.11 on epoch=489
06/24/2022 11:07:25 - INFO - __main__ - Step 990 Global step 990 Train loss 1.01 on epoch=494
06/24/2022 11:07:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.10 on epoch=499
06/24/2022 11:07:27 - INFO - __main__ - Global step 1000 Train loss 1.08 Classification-F1 0.4554554554554554 on epoch=499
06/24/2022 11:07:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.05 on epoch=504
06/24/2022 11:07:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.98 on epoch=509
06/24/2022 11:07:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.96 on epoch=514
06/24/2022 11:07:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.99 on epoch=519
06/24/2022 11:07:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.08 on epoch=524
06/24/2022 11:07:34 - INFO - __main__ - Global step 1050 Train loss 1.01 Classification-F1 0.5151515151515151 on epoch=524
06/24/2022 11:07:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.02 on epoch=529
06/24/2022 11:07:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.99 on epoch=534
06/24/2022 11:07:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.93 on epoch=539
06/24/2022 11:07:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.80 on epoch=544
06/24/2022 11:07:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.89 on epoch=549
06/24/2022 11:07:40 - INFO - __main__ - Global step 1100 Train loss 0.93 Classification-F1 0.37662337662337664 on epoch=549
06/24/2022 11:07:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.95 on epoch=554
06/24/2022 11:07:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.01 on epoch=559
06/24/2022 11:07:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.84 on epoch=564
06/24/2022 11:07:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.86 on epoch=569
06/24/2022 11:07:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.99 on epoch=574
06/24/2022 11:07:47 - INFO - __main__ - Global step 1150 Train loss 0.93 Classification-F1 0.4920634920634921 on epoch=574
06/24/2022 11:07:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.82 on epoch=579
06/24/2022 11:07:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.96 on epoch=584
06/24/2022 11:07:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.95 on epoch=589
06/24/2022 11:07:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.91 on epoch=594
06/24/2022 11:07:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.84 on epoch=599
06/24/2022 11:07:53 - INFO - __main__ - Global step 1200 Train loss 0.90 Classification-F1 0.4231177094379639 on epoch=599
06/24/2022 11:07:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.75 on epoch=604
06/24/2022 11:07:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.86 on epoch=609
06/24/2022 11:07:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.82 on epoch=614
06/24/2022 11:07:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=619
06/24/2022 11:08:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.76 on epoch=624
06/24/2022 11:08:00 - INFO - __main__ - Global step 1250 Train loss 0.80 Classification-F1 0.5733333333333335 on epoch=624
06/24/2022 11:08:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.87 on epoch=629
06/24/2022 11:08:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.81 on epoch=634
06/24/2022 11:08:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.75 on epoch=639
06/24/2022 11:08:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.69 on epoch=644
06/24/2022 11:08:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.83 on epoch=649
06/24/2022 11:08:07 - INFO - __main__ - Global step 1300 Train loss 0.79 Classification-F1 0.3454545454545454 on epoch=649
06/24/2022 11:08:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.71 on epoch=654
06/24/2022 11:08:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.84 on epoch=659
06/24/2022 11:08:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.86 on epoch=664
06/24/2022 11:08:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.70 on epoch=669
06/24/2022 11:08:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.86 on epoch=674
06/24/2022 11:08:13 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.3816425120772947 on epoch=674
06/24/2022 11:08:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.67 on epoch=679
06/24/2022 11:08:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.65 on epoch=684
06/24/2022 11:08:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.80 on epoch=689
06/24/2022 11:08:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.78 on epoch=694
06/24/2022 11:08:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.72 on epoch=699
06/24/2022 11:08:20 - INFO - __main__ - Global step 1400 Train loss 0.73 Classification-F1 0.39999999999999997 on epoch=699
06/24/2022 11:08:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.65 on epoch=704
06/24/2022 11:08:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.72 on epoch=709
06/24/2022 11:08:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.64 on epoch=714
06/24/2022 11:08:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.65 on epoch=719
06/24/2022 11:08:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.69 on epoch=724
06/24/2022 11:08:26 - INFO - __main__ - Global step 1450 Train loss 0.67 Classification-F1 0.5076923076923077 on epoch=724
06/24/2022 11:08:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.64 on epoch=729
06/24/2022 11:08:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.66 on epoch=734
06/24/2022 11:08:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.75 on epoch=739
06/24/2022 11:08:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.72 on epoch=744
06/24/2022 11:08:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.59 on epoch=749
06/24/2022 11:08:33 - INFO - __main__ - Global step 1500 Train loss 0.67 Classification-F1 0.4285714285714286 on epoch=749
06/24/2022 11:08:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.79 on epoch=754
06/24/2022 11:08:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.56 on epoch=759
06/24/2022 11:08:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.69 on epoch=764
06/24/2022 11:08:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.60 on epoch=769
06/24/2022 11:08:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.72 on epoch=774
06/24/2022 11:08:40 - INFO - __main__ - Global step 1550 Train loss 0.67 Classification-F1 0.5835835835835835 on epoch=774
06/24/2022 11:08:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.74 on epoch=779
06/24/2022 11:08:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.70 on epoch=784
06/24/2022 11:08:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.69 on epoch=789
06/24/2022 11:08:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.67 on epoch=794
06/24/2022 11:08:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.65 on epoch=799
06/24/2022 11:08:46 - INFO - __main__ - Global step 1600 Train loss 0.69 Classification-F1 0.3191489361702127 on epoch=799
06/24/2022 11:08:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.80 on epoch=804
06/24/2022 11:08:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.64 on epoch=809
06/24/2022 11:08:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.64 on epoch=814
06/24/2022 11:08:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.60 on epoch=819
06/24/2022 11:08:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.72 on epoch=824
06/24/2022 11:08:53 - INFO - __main__ - Global step 1650 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=824
06/24/2022 11:08:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.57 on epoch=829
06/24/2022 11:08:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.71 on epoch=834
06/24/2022 11:08:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.67 on epoch=839
06/24/2022 11:08:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.62 on epoch=844
06/24/2022 11:08:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.60 on epoch=849
06/24/2022 11:08:59 - INFO - __main__ - Global step 1700 Train loss 0.64 Classification-F1 0.4554554554554554 on epoch=849
06/24/2022 11:09:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.59 on epoch=854
06/24/2022 11:09:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.59 on epoch=859
06/24/2022 11:09:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.65 on epoch=864
06/24/2022 11:09:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.69 on epoch=869
06/24/2022 11:09:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.65 on epoch=874
06/24/2022 11:09:06 - INFO - __main__ - Global step 1750 Train loss 0.63 Classification-F1 0.3816425120772947 on epoch=874
06/24/2022 11:09:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.60 on epoch=879
06/24/2022 11:09:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.62 on epoch=884
06/24/2022 11:09:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.53 on epoch=889
06/24/2022 11:09:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.56 on epoch=894
06/24/2022 11:09:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.61 on epoch=899
06/24/2022 11:09:12 - INFO - __main__ - Global step 1800 Train loss 0.58 Classification-F1 0.4181818181818182 on epoch=899
06/24/2022 11:09:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.58 on epoch=904
06/24/2022 11:09:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.71 on epoch=909
06/24/2022 11:09:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.58 on epoch=914
06/24/2022 11:09:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.52 on epoch=919
06/24/2022 11:09:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.64 on epoch=924
06/24/2022 11:09:19 - INFO - __main__ - Global step 1850 Train loss 0.61 Classification-F1 0.3191489361702127 on epoch=924
06/24/2022 11:09:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.51 on epoch=929
06/24/2022 11:09:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.59 on epoch=934
06/24/2022 11:09:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.59 on epoch=939
06/24/2022 11:09:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.68 on epoch=944
06/24/2022 11:09:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.56 on epoch=949
06/24/2022 11:09:26 - INFO - __main__ - Global step 1900 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=949
06/24/2022 11:09:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.65 on epoch=954
06/24/2022 11:09:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.61 on epoch=959
06/24/2022 11:09:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.59 on epoch=964
06/24/2022 11:09:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.60 on epoch=969
06/24/2022 11:09:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=974
06/24/2022 11:09:32 - INFO - __main__ - Global step 1950 Train loss 0.59 Classification-F1 0.36374269005847953 on epoch=974
06/24/2022 11:09:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.66 on epoch=979
06/24/2022 11:09:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.56 on epoch=984
06/24/2022 11:09:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.51 on epoch=989
06/24/2022 11:09:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.65 on epoch=994
06/24/2022 11:09:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.63 on epoch=999
06/24/2022 11:09:39 - INFO - __main__ - Global step 2000 Train loss 0.60 Classification-F1 0.36374269005847953 on epoch=999
06/24/2022 11:09:39 - INFO - __main__ - save last model!
06/24/2022 11:09:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 11:09:39 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 11:09:39 - INFO - __main__ - Printing 3 examples
06/24/2022 11:09:39 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 11:09:39 - INFO - __main__ - ['0']
06/24/2022 11:09:39 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 11:09:39 - INFO - __main__ - ['1']
06/24/2022 11:09:39 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 11:09:39 - INFO - __main__ - ['1']
06/24/2022 11:09:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:09:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:09:40 - INFO - __main__ - Printing 3 examples
06/24/2022 11:09:40 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 11:09:40 - INFO - __main__ - ['1']
06/24/2022 11:09:40 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 11:09:40 - INFO - __main__ - ['1']
06/24/2022 11:09:40 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 11:09:40 - INFO - __main__ - ['1']
06/24/2022 11:09:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 11:09:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:09:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:09:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:09:40 - INFO - __main__ - Printing 3 examples
06/24/2022 11:09:40 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 11:09:40 - INFO - __main__ - ['1']
06/24/2022 11:09:40 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 11:09:40 - INFO - __main__ - ['1']
06/24/2022 11:09:40 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 11:09:40 - INFO - __main__ - ['1']
06/24/2022 11:09:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:09:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:09:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:09:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:09:46 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:09:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:09:46 - INFO - __main__ - Starting training!
06/24/2022 11:09:51 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 11:11:15 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_13_0.4_8_predictions.txt
06/24/2022 11:11:15 - INFO - __main__ - Classification-F1 on test data: 0.3399
06/24/2022 11:11:15 - INFO - __main__ - prefix=paws_16_13, lr=0.4, bsz=8, dev_performance=0.5933528836754642, test_performance=0.33987408710814376
06/24/2022 11:11:15 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.3, bsz=8 ...
06/24/2022 11:11:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:11:16 - INFO - __main__ - Printing 3 examples
06/24/2022 11:11:16 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 11:11:16 - INFO - __main__ - ['1']
06/24/2022 11:11:16 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 11:11:16 - INFO - __main__ - ['1']
06/24/2022 11:11:16 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 11:11:16 - INFO - __main__ - ['1']
06/24/2022 11:11:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:11:16 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:11:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:11:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:11:16 - INFO - __main__ - Printing 3 examples
06/24/2022 11:11:16 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 11:11:16 - INFO - __main__ - ['1']
06/24/2022 11:11:16 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 11:11:16 - INFO - __main__ - ['1']
06/24/2022 11:11:16 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 11:11:16 - INFO - __main__ - ['1']
06/24/2022 11:11:16 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:11:16 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:11:16 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:11:22 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:11:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:11:23 - INFO - __main__ - Starting training!
06/24/2022 11:11:24 - INFO - __main__ - Step 10 Global step 10 Train loss 5.98 on epoch=4
06/24/2022 11:11:25 - INFO - __main__ - Step 20 Global step 20 Train loss 6.05 on epoch=9
06/24/2022 11:11:27 - INFO - __main__ - Step 30 Global step 30 Train loss 5.82 on epoch=14
06/24/2022 11:11:28 - INFO - __main__ - Step 40 Global step 40 Train loss 5.92 on epoch=19
06/24/2022 11:11:29 - INFO - __main__ - Step 50 Global step 50 Train loss 5.86 on epoch=24
06/24/2022 11:11:30 - INFO - __main__ - Global step 50 Train loss 5.93 Classification-F1 0.0 on epoch=24
06/24/2022 11:11:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 11:11:31 - INFO - __main__ - Step 60 Global step 60 Train loss 5.89 on epoch=29
06/24/2022 11:11:33 - INFO - __main__ - Step 70 Global step 70 Train loss 5.91 on epoch=34
06/24/2022 11:11:34 - INFO - __main__ - Step 80 Global step 80 Train loss 5.84 on epoch=39
06/24/2022 11:11:35 - INFO - __main__ - Step 90 Global step 90 Train loss 5.77 on epoch=44
06/24/2022 11:11:36 - INFO - __main__ - Step 100 Global step 100 Train loss 5.73 on epoch=49
06/24/2022 11:11:40 - INFO - __main__ - Global step 100 Train loss 5.83 Classification-F1 0.0 on epoch=49
06/24/2022 11:11:42 - INFO - __main__ - Step 110 Global step 110 Train loss 5.73 on epoch=54
06/24/2022 11:11:43 - INFO - __main__ - Step 120 Global step 120 Train loss 5.74 on epoch=59
06/24/2022 11:11:44 - INFO - __main__ - Step 130 Global step 130 Train loss 5.72 on epoch=64
06/24/2022 11:11:45 - INFO - __main__ - Step 140 Global step 140 Train loss 5.67 on epoch=69
06/24/2022 11:11:47 - INFO - __main__ - Step 150 Global step 150 Train loss 5.77 on epoch=74
06/24/2022 11:11:48 - INFO - __main__ - Global step 150 Train loss 5.73 Classification-F1 0.0 on epoch=74
06/24/2022 11:11:49 - INFO - __main__ - Step 160 Global step 160 Train loss 5.74 on epoch=79
06/24/2022 11:11:50 - INFO - __main__ - Step 170 Global step 170 Train loss 5.69 on epoch=84
06/24/2022 11:11:51 - INFO - __main__ - Step 180 Global step 180 Train loss 5.59 on epoch=89
06/24/2022 11:11:53 - INFO - __main__ - Step 190 Global step 190 Train loss 5.59 on epoch=94
06/24/2022 11:11:54 - INFO - __main__ - Step 200 Global step 200 Train loss 5.64 on epoch=99
06/24/2022 11:11:54 - INFO - __main__ - Global step 200 Train loss 5.65 Classification-F1 0.0 on epoch=99
06/24/2022 11:11:56 - INFO - __main__ - Step 210 Global step 210 Train loss 5.69 on epoch=104
06/24/2022 11:11:57 - INFO - __main__ - Step 220 Global step 220 Train loss 5.57 on epoch=109
06/24/2022 11:11:58 - INFO - __main__ - Step 230 Global step 230 Train loss 5.60 on epoch=114
06/24/2022 11:11:59 - INFO - __main__ - Step 240 Global step 240 Train loss 5.45 on epoch=119
06/24/2022 11:12:01 - INFO - __main__ - Step 250 Global step 250 Train loss 5.54 on epoch=124
06/24/2022 11:12:01 - INFO - __main__ - Global step 250 Train loss 5.57 Classification-F1 0.0 on epoch=124
06/24/2022 11:12:03 - INFO - __main__ - Step 260 Global step 260 Train loss 5.52 on epoch=129
06/24/2022 11:12:04 - INFO - __main__ - Step 270 Global step 270 Train loss 5.47 on epoch=134
06/24/2022 11:12:05 - INFO - __main__ - Step 280 Global step 280 Train loss 5.33 on epoch=139
06/24/2022 11:12:06 - INFO - __main__ - Step 290 Global step 290 Train loss 5.45 on epoch=144
06/24/2022 11:12:08 - INFO - __main__ - Step 300 Global step 300 Train loss 5.37 on epoch=149
06/24/2022 11:12:14 - INFO - __main__ - Global step 300 Train loss 5.43 Classification-F1 0.0 on epoch=149
06/24/2022 11:12:15 - INFO - __main__ - Step 310 Global step 310 Train loss 5.28 on epoch=154
06/24/2022 11:12:16 - INFO - __main__ - Step 320 Global step 320 Train loss 5.21 on epoch=159
06/24/2022 11:12:18 - INFO - __main__ - Step 330 Global step 330 Train loss 5.08 on epoch=164
06/24/2022 11:12:19 - INFO - __main__ - Step 340 Global step 340 Train loss 5.13 on epoch=169
06/24/2022 11:12:20 - INFO - __main__ - Step 350 Global step 350 Train loss 4.99 on epoch=174
06/24/2022 11:12:30 - INFO - __main__ - Global step 350 Train loss 5.14 Classification-F1 0.0 on epoch=174
06/24/2022 11:12:32 - INFO - __main__ - Step 360 Global step 360 Train loss 4.85 on epoch=179
06/24/2022 11:12:33 - INFO - __main__ - Step 370 Global step 370 Train loss 4.73 on epoch=184
06/24/2022 11:12:34 - INFO - __main__ - Step 380 Global step 380 Train loss 4.66 on epoch=189
06/24/2022 11:12:35 - INFO - __main__ - Step 390 Global step 390 Train loss 4.51 on epoch=194
06/24/2022 11:12:37 - INFO - __main__ - Step 400 Global step 400 Train loss 4.52 on epoch=199
06/24/2022 11:12:47 - INFO - __main__ - Global step 400 Train loss 4.65 Classification-F1 0.0 on epoch=199
06/24/2022 11:12:49 - INFO - __main__ - Step 410 Global step 410 Train loss 4.29 on epoch=204
06/24/2022 11:12:50 - INFO - __main__ - Step 420 Global step 420 Train loss 4.23 on epoch=209
06/24/2022 11:12:51 - INFO - __main__ - Step 430 Global step 430 Train loss 4.23 on epoch=214
06/24/2022 11:12:52 - INFO - __main__ - Step 440 Global step 440 Train loss 4.16 on epoch=219
06/24/2022 11:12:54 - INFO - __main__ - Step 450 Global step 450 Train loss 4.14 on epoch=224
06/24/2022 11:13:04 - INFO - __main__ - Global step 450 Train loss 4.21 Classification-F1 0.0 on epoch=224
06/24/2022 11:13:05 - INFO - __main__ - Step 460 Global step 460 Train loss 4.16 on epoch=229
06/24/2022 11:13:07 - INFO - __main__ - Step 470 Global step 470 Train loss 4.00 on epoch=234
06/24/2022 11:13:08 - INFO - __main__ - Step 480 Global step 480 Train loss 4.02 on epoch=239
06/24/2022 11:13:09 - INFO - __main__ - Step 490 Global step 490 Train loss 3.90 on epoch=244
06/24/2022 11:13:10 - INFO - __main__ - Step 500 Global step 500 Train loss 3.84 on epoch=249
06/24/2022 11:13:21 - INFO - __main__ - Global step 500 Train loss 3.98 Classification-F1 0.0 on epoch=249
06/24/2022 11:13:22 - INFO - __main__ - Step 510 Global step 510 Train loss 3.95 on epoch=254
06/24/2022 11:13:23 - INFO - __main__ - Step 520 Global step 520 Train loss 3.77 on epoch=259
06/24/2022 11:13:24 - INFO - __main__ - Step 530 Global step 530 Train loss 3.90 on epoch=264
06/24/2022 11:13:26 - INFO - __main__ - Step 540 Global step 540 Train loss 3.63 on epoch=269
06/24/2022 11:13:27 - INFO - __main__ - Step 550 Global step 550 Train loss 3.71 on epoch=274
06/24/2022 11:13:34 - INFO - __main__ - Global step 550 Train loss 3.79 Classification-F1 0.0 on epoch=274
06/24/2022 11:13:35 - INFO - __main__ - Step 560 Global step 560 Train loss 3.71 on epoch=279
06/24/2022 11:13:37 - INFO - __main__ - Step 570 Global step 570 Train loss 3.69 on epoch=284
06/24/2022 11:13:38 - INFO - __main__ - Step 580 Global step 580 Train loss 3.72 on epoch=289
06/24/2022 11:13:39 - INFO - __main__ - Step 590 Global step 590 Train loss 3.48 on epoch=294
06/24/2022 11:13:40 - INFO - __main__ - Step 600 Global step 600 Train loss 3.58 on epoch=299
06/24/2022 11:13:51 - INFO - __main__ - Global step 600 Train loss 3.64 Classification-F1 0.0 on epoch=299
06/24/2022 11:13:52 - INFO - __main__ - Step 610 Global step 610 Train loss 3.39 on epoch=304
06/24/2022 11:13:53 - INFO - __main__ - Step 620 Global step 620 Train loss 3.51 on epoch=309
06/24/2022 11:13:55 - INFO - __main__ - Step 630 Global step 630 Train loss 3.35 on epoch=314
06/24/2022 11:13:56 - INFO - __main__ - Step 640 Global step 640 Train loss 3.36 on epoch=319
06/24/2022 11:13:57 - INFO - __main__ - Step 650 Global step 650 Train loss 3.28 on epoch=324
06/24/2022 11:14:05 - INFO - __main__ - Global step 650 Train loss 3.38 Classification-F1 0.0 on epoch=324
06/24/2022 11:14:06 - INFO - __main__ - Step 660 Global step 660 Train loss 3.26 on epoch=329
06/24/2022 11:14:08 - INFO - __main__ - Step 670 Global step 670 Train loss 3.18 on epoch=334
06/24/2022 11:14:09 - INFO - __main__ - Step 680 Global step 680 Train loss 3.14 on epoch=339
06/24/2022 11:14:10 - INFO - __main__ - Step 690 Global step 690 Train loss 2.99 on epoch=344
06/24/2022 11:14:11 - INFO - __main__ - Step 700 Global step 700 Train loss 3.04 on epoch=349
06/24/2022 11:14:14 - INFO - __main__ - Global step 700 Train loss 3.12 Classification-F1 0.01764705882352941 on epoch=349
06/24/2022 11:14:14 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.01764705882352941 on epoch=349, global_step=700
06/24/2022 11:14:16 - INFO - __main__ - Step 710 Global step 710 Train loss 3.09 on epoch=354
06/24/2022 11:14:17 - INFO - __main__ - Step 720 Global step 720 Train loss 3.00 on epoch=359
06/24/2022 11:14:18 - INFO - __main__ - Step 730 Global step 730 Train loss 2.89 on epoch=364
06/24/2022 11:14:19 - INFO - __main__ - Step 740 Global step 740 Train loss 2.90 on epoch=369
06/24/2022 11:14:21 - INFO - __main__ - Step 750 Global step 750 Train loss 2.86 on epoch=374
06/24/2022 11:14:23 - INFO - __main__ - Global step 750 Train loss 2.95 Classification-F1 0.0617283950617284 on epoch=374
06/24/2022 11:14:23 - INFO - __main__ - Saving model with best Classification-F1: 0.01764705882352941 -> 0.0617283950617284 on epoch=374, global_step=750
06/24/2022 11:14:24 - INFO - __main__ - Step 760 Global step 760 Train loss 2.90 on epoch=379
06/24/2022 11:14:25 - INFO - __main__ - Step 770 Global step 770 Train loss 2.77 on epoch=384
06/24/2022 11:14:27 - INFO - __main__ - Step 780 Global step 780 Train loss 2.85 on epoch=389
06/24/2022 11:14:28 - INFO - __main__ - Step 790 Global step 790 Train loss 2.91 on epoch=394
06/24/2022 11:14:29 - INFO - __main__ - Step 800 Global step 800 Train loss 2.72 on epoch=399
06/24/2022 11:14:33 - INFO - __main__ - Global step 800 Train loss 2.83 Classification-F1 0.2566069906223359 on epoch=399
06/24/2022 11:14:33 - INFO - __main__ - Saving model with best Classification-F1: 0.0617283950617284 -> 0.2566069906223359 on epoch=399, global_step=800
06/24/2022 11:14:34 - INFO - __main__ - Step 810 Global step 810 Train loss 2.50 on epoch=404
06/24/2022 11:14:35 - INFO - __main__ - Step 820 Global step 820 Train loss 2.79 on epoch=409
06/24/2022 11:14:36 - INFO - __main__ - Step 830 Global step 830 Train loss 2.65 on epoch=414
06/24/2022 11:14:38 - INFO - __main__ - Step 840 Global step 840 Train loss 2.62 on epoch=419
06/24/2022 11:14:39 - INFO - __main__ - Step 850 Global step 850 Train loss 2.51 on epoch=424
06/24/2022 11:14:42 - INFO - __main__ - Global step 850 Train loss 2.61 Classification-F1 0.22695035460992907 on epoch=424
06/24/2022 11:14:44 - INFO - __main__ - Step 860 Global step 860 Train loss 2.65 on epoch=429
06/24/2022 11:14:45 - INFO - __main__ - Step 870 Global step 870 Train loss 2.45 on epoch=434
06/24/2022 11:14:46 - INFO - __main__ - Step 880 Global step 880 Train loss 2.52 on epoch=439
06/24/2022 11:14:47 - INFO - __main__ - Step 890 Global step 890 Train loss 2.45 on epoch=444
06/24/2022 11:14:49 - INFO - __main__ - Step 900 Global step 900 Train loss 2.48 on epoch=449
06/24/2022 11:14:56 - INFO - __main__ - Global step 900 Train loss 2.51 Classification-F1 0.21276595744680848 on epoch=449
06/24/2022 11:14:57 - INFO - __main__ - Step 910 Global step 910 Train loss 2.38 on epoch=454
06/24/2022 11:14:58 - INFO - __main__ - Step 920 Global step 920 Train loss 2.27 on epoch=459
06/24/2022 11:15:00 - INFO - __main__ - Step 930 Global step 930 Train loss 2.23 on epoch=464
06/24/2022 11:15:01 - INFO - __main__ - Step 940 Global step 940 Train loss 2.24 on epoch=469
06/24/2022 11:15:02 - INFO - __main__ - Step 950 Global step 950 Train loss 2.26 on epoch=474
06/24/2022 11:15:04 - INFO - __main__ - Global step 950 Train loss 2.28 Classification-F1 0.4458874458874459 on epoch=474
06/24/2022 11:15:04 - INFO - __main__ - Saving model with best Classification-F1: 0.2566069906223359 -> 0.4458874458874459 on epoch=474, global_step=950
06/24/2022 11:15:06 - INFO - __main__ - Step 960 Global step 960 Train loss 2.14 on epoch=479
06/24/2022 11:15:07 - INFO - __main__ - Step 970 Global step 970 Train loss 2.22 on epoch=484
06/24/2022 11:15:08 - INFO - __main__ - Step 980 Global step 980 Train loss 2.07 on epoch=489
06/24/2022 11:15:10 - INFO - __main__ - Step 990 Global step 990 Train loss 2.08 on epoch=494
06/24/2022 11:15:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.98 on epoch=499
06/24/2022 11:15:12 - INFO - __main__ - Global step 1000 Train loss 2.10 Classification-F1 0.3552492046659597 on epoch=499
06/24/2022 11:15:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.03 on epoch=504
06/24/2022 11:15:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.84 on epoch=509
06/24/2022 11:15:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.86 on epoch=514
06/24/2022 11:15:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.92 on epoch=519
06/24/2022 11:15:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.75 on epoch=524
06/24/2022 11:15:19 - INFO - __main__ - Global step 1050 Train loss 1.88 Classification-F1 0.4285714285714286 on epoch=524
06/24/2022 11:15:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.76 on epoch=529
06/24/2022 11:15:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.69 on epoch=534
06/24/2022 11:15:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.64 on epoch=539
06/24/2022 11:15:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.63 on epoch=544
06/24/2022 11:15:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.58 on epoch=549
06/24/2022 11:15:26 - INFO - __main__ - Global step 1100 Train loss 1.66 Classification-F1 0.625 on epoch=549
06/24/2022 11:15:26 - INFO - __main__ - Saving model with best Classification-F1: 0.4458874458874459 -> 0.625 on epoch=549, global_step=1100
06/24/2022 11:15:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.57 on epoch=554
06/24/2022 11:15:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.36 on epoch=559
06/24/2022 11:15:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.55 on epoch=564
06/24/2022 11:15:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.51 on epoch=569
06/24/2022 11:15:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.48 on epoch=574
06/24/2022 11:15:33 - INFO - __main__ - Global step 1150 Train loss 1.49 Classification-F1 0.46843853820598 on epoch=574
06/24/2022 11:15:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.49 on epoch=579
06/24/2022 11:15:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.41 on epoch=584
06/24/2022 11:15:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.43 on epoch=589
06/24/2022 11:15:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.35 on epoch=594
06/24/2022 11:15:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.44 on epoch=599
06/24/2022 11:15:39 - INFO - __main__ - Global step 1200 Train loss 1.42 Classification-F1 0.5076923076923077 on epoch=599
06/24/2022 11:15:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.43 on epoch=604
06/24/2022 11:15:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.34 on epoch=609
06/24/2022 11:15:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.39 on epoch=614
06/24/2022 11:15:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.37 on epoch=619
06/24/2022 11:15:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.35 on epoch=624
06/24/2022 11:15:46 - INFO - __main__ - Global step 1250 Train loss 1.38 Classification-F1 0.3333333333333333 on epoch=624
06/24/2022 11:15:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.30 on epoch=629
06/24/2022 11:15:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.28 on epoch=634
06/24/2022 11:15:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.25 on epoch=639
06/24/2022 11:15:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.22 on epoch=644
06/24/2022 11:15:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.03 on epoch=649
06/24/2022 11:15:53 - INFO - __main__ - Global step 1300 Train loss 1.22 Classification-F1 0.3992490613266583 on epoch=649
06/24/2022 11:15:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.10 on epoch=654
06/24/2022 11:15:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.19 on epoch=659
06/24/2022 11:15:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.14 on epoch=664
06/24/2022 11:15:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.08 on epoch=669
06/24/2022 11:15:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.01 on epoch=674
06/24/2022 11:15:59 - INFO - __main__ - Global step 1350 Train loss 1.10 Classification-F1 0.3552492046659597 on epoch=674
06/24/2022 11:16:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.14 on epoch=679
06/24/2022 11:16:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.99 on epoch=684
06/24/2022 11:16:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.07 on epoch=689
06/24/2022 11:16:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.15 on epoch=694
06/24/2022 11:16:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.00 on epoch=699
06/24/2022 11:16:06 - INFO - __main__ - Global step 1400 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=699
06/24/2022 11:16:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.02 on epoch=704
06/24/2022 11:16:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.96 on epoch=709
06/24/2022 11:16:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.96 on epoch=714
06/24/2022 11:16:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.01 on epoch=719
06/24/2022 11:16:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.99 on epoch=724
06/24/2022 11:16:13 - INFO - __main__ - Global step 1450 Train loss 0.99 Classification-F1 0.3992490613266583 on epoch=724
06/24/2022 11:16:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.95 on epoch=729
06/24/2022 11:16:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.89 on epoch=734
06/24/2022 11:16:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.95 on epoch=739
06/24/2022 11:16:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.91 on epoch=744
06/24/2022 11:16:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.92 on epoch=749
06/24/2022 11:16:19 - INFO - __main__ - Global step 1500 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=749
06/24/2022 11:16:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.04 on epoch=754
06/24/2022 11:16:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.98 on epoch=759
06/24/2022 11:16:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.89 on epoch=764
06/24/2022 11:16:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.85 on epoch=769
06/24/2022 11:16:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.89 on epoch=774
06/24/2022 11:16:26 - INFO - __main__ - Global step 1550 Train loss 0.93 Classification-F1 0.3816425120772947 on epoch=774
06/24/2022 11:16:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.88 on epoch=779
06/24/2022 11:16:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.96 on epoch=784
06/24/2022 11:16:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.83 on epoch=789
06/24/2022 11:16:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.89 on epoch=794
06/24/2022 11:16:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.84 on epoch=799
06/24/2022 11:16:33 - INFO - __main__ - Global step 1600 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=799
06/24/2022 11:16:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.88 on epoch=804
06/24/2022 11:16:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.90 on epoch=809
06/24/2022 11:16:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.84 on epoch=814
06/24/2022 11:16:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.76 on epoch=819
06/24/2022 11:16:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.81 on epoch=824
06/24/2022 11:16:39 - INFO - __main__ - Global step 1650 Train loss 0.84 Classification-F1 0.3333333333333333 on epoch=824
06/24/2022 11:16:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.86 on epoch=829
06/24/2022 11:16:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.87 on epoch=834
06/24/2022 11:16:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.79 on epoch=839
06/24/2022 11:16:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.85 on epoch=844
06/24/2022 11:16:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.78 on epoch=849
06/24/2022 11:16:46 - INFO - __main__ - Global step 1700 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=849
06/24/2022 11:16:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.82 on epoch=854
06/24/2022 11:16:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.72 on epoch=859
06/24/2022 11:16:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.78 on epoch=864
06/24/2022 11:16:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.86 on epoch=869
06/24/2022 11:16:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.79 on epoch=874
06/24/2022 11:16:53 - INFO - __main__ - Global step 1750 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=874
06/24/2022 11:16:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.76 on epoch=879
06/24/2022 11:16:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.73 on epoch=884
06/24/2022 11:16:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.69 on epoch=889
06/24/2022 11:16:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.75 on epoch=894
06/24/2022 11:16:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.70 on epoch=899
06/24/2022 11:16:59 - INFO - __main__ - Global step 1800 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=899
06/24/2022 11:17:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.73 on epoch=904
06/24/2022 11:17:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.78 on epoch=909
06/24/2022 11:17:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.66 on epoch=914
06/24/2022 11:17:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.83 on epoch=919
06/24/2022 11:17:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.58 on epoch=924
06/24/2022 11:17:06 - INFO - __main__ - Global step 1850 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=924
06/24/2022 11:17:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.66 on epoch=929
06/24/2022 11:17:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.77 on epoch=934
06/24/2022 11:17:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.76 on epoch=939
06/24/2022 11:17:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.69 on epoch=944
06/24/2022 11:17:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.64 on epoch=949
06/24/2022 11:17:12 - INFO - __main__ - Global step 1900 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=949
06/24/2022 11:17:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.79 on epoch=954
06/24/2022 11:17:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.71 on epoch=959
06/24/2022 11:17:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.72 on epoch=964
06/24/2022 11:17:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.73 on epoch=969
06/24/2022 11:17:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.75 on epoch=974
06/24/2022 11:17:19 - INFO - __main__ - Global step 1950 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=974
06/24/2022 11:17:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.73 on epoch=979
06/24/2022 11:17:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.65 on epoch=984
06/24/2022 11:17:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.66 on epoch=989
06/24/2022 11:17:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.69 on epoch=994
06/24/2022 11:17:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.83 on epoch=999
06/24/2022 11:17:26 - INFO - __main__ - Global step 2000 Train loss 0.71 Classification-F1 0.49090909090909085 on epoch=999
06/24/2022 11:17:26 - INFO - __main__ - save last model!
06/24/2022 11:17:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 11:17:26 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 11:17:26 - INFO - __main__ - Printing 3 examples
06/24/2022 11:17:26 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 11:17:26 - INFO - __main__ - ['0']
06/24/2022 11:17:26 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 11:17:26 - INFO - __main__ - ['1']
06/24/2022 11:17:26 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 11:17:26 - INFO - __main__ - ['1']
06/24/2022 11:17:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:17:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:17:27 - INFO - __main__ - Printing 3 examples
06/24/2022 11:17:27 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 11:17:27 - INFO - __main__ - ['1']
06/24/2022 11:17:27 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 11:17:27 - INFO - __main__ - ['1']
06/24/2022 11:17:27 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 11:17:27 - INFO - __main__ - ['1']
06/24/2022 11:17:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 11:17:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:17:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:17:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:17:27 - INFO - __main__ - Printing 3 examples
06/24/2022 11:17:27 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 11:17:27 - INFO - __main__ - ['1']
06/24/2022 11:17:27 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 11:17:27 - INFO - __main__ - ['1']
06/24/2022 11:17:27 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 11:17:27 - INFO - __main__ - ['1']
06/24/2022 11:17:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:17:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:17:27 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:17:30 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:17:33 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:17:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:17:33 - INFO - __main__ - Starting training!
06/24/2022 11:17:38 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 11:19:02 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_13_0.3_8_predictions.txt
06/24/2022 11:19:02 - INFO - __main__ - Classification-F1 on test data: 0.3651
06/24/2022 11:19:02 - INFO - __main__ - prefix=paws_16_13, lr=0.3, bsz=8, dev_performance=0.625, test_performance=0.36513973862092985
06/24/2022 11:19:02 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.2, bsz=8 ...
06/24/2022 11:19:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:19:03 - INFO - __main__ - Printing 3 examples
06/24/2022 11:19:03 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 11:19:03 - INFO - __main__ - ['1']
06/24/2022 11:19:03 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 11:19:03 - INFO - __main__ - ['1']
06/24/2022 11:19:03 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 11:19:03 - INFO - __main__ - ['1']
06/24/2022 11:19:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:19:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:19:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:19:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:19:03 - INFO - __main__ - Printing 3 examples
06/24/2022 11:19:03 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 11:19:03 - INFO - __main__ - ['1']
06/24/2022 11:19:03 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 11:19:03 - INFO - __main__ - ['1']
06/24/2022 11:19:03 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 11:19:03 - INFO - __main__ - ['1']
06/24/2022 11:19:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:19:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:19:03 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:19:09 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:19:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:19:09 - INFO - __main__ - Starting training!
06/24/2022 11:19:11 - INFO - __main__ - Step 10 Global step 10 Train loss 6.02 on epoch=4
06/24/2022 11:19:12 - INFO - __main__ - Step 20 Global step 20 Train loss 5.92 on epoch=9
06/24/2022 11:19:13 - INFO - __main__ - Step 30 Global step 30 Train loss 5.90 on epoch=14
06/24/2022 11:19:15 - INFO - __main__ - Step 40 Global step 40 Train loss 5.95 on epoch=19
06/24/2022 11:19:16 - INFO - __main__ - Step 50 Global step 50 Train loss 5.89 on epoch=24
06/24/2022 11:19:21 - INFO - __main__ - Global step 50 Train loss 5.93 Classification-F1 0.0 on epoch=24
06/24/2022 11:19:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 11:19:22 - INFO - __main__ - Step 60 Global step 60 Train loss 5.79 on epoch=29
06/24/2022 11:19:23 - INFO - __main__ - Step 70 Global step 70 Train loss 5.84 on epoch=34
06/24/2022 11:19:24 - INFO - __main__ - Step 80 Global step 80 Train loss 5.87 on epoch=39
06/24/2022 11:19:26 - INFO - __main__ - Step 90 Global step 90 Train loss 5.88 on epoch=44
06/24/2022 11:19:27 - INFO - __main__ - Step 100 Global step 100 Train loss 5.83 on epoch=49
06/24/2022 11:19:29 - INFO - __main__ - Global step 100 Train loss 5.84 Classification-F1 0.0 on epoch=49
06/24/2022 11:19:30 - INFO - __main__ - Step 110 Global step 110 Train loss 5.88 on epoch=54
06/24/2022 11:19:31 - INFO - __main__ - Step 120 Global step 120 Train loss 5.82 on epoch=59
06/24/2022 11:19:32 - INFO - __main__ - Step 130 Global step 130 Train loss 5.80 on epoch=64
06/24/2022 11:19:34 - INFO - __main__ - Step 140 Global step 140 Train loss 5.86 on epoch=69
06/24/2022 11:19:35 - INFO - __main__ - Step 150 Global step 150 Train loss 5.81 on epoch=74
06/24/2022 11:19:37 - INFO - __main__ - Global step 150 Train loss 5.83 Classification-F1 0.0 on epoch=74
06/24/2022 11:19:38 - INFO - __main__ - Step 160 Global step 160 Train loss 5.85 on epoch=79
06/24/2022 11:19:40 - INFO - __main__ - Step 170 Global step 170 Train loss 5.83 on epoch=84
06/24/2022 11:19:41 - INFO - __main__ - Step 180 Global step 180 Train loss 5.77 on epoch=89
06/24/2022 11:19:42 - INFO - __main__ - Step 190 Global step 190 Train loss 5.85 on epoch=94
06/24/2022 11:19:43 - INFO - __main__ - Step 200 Global step 200 Train loss 5.80 on epoch=99
06/24/2022 11:19:46 - INFO - __main__ - Global step 200 Train loss 5.82 Classification-F1 0.0 on epoch=99
06/24/2022 11:19:47 - INFO - __main__ - Step 210 Global step 210 Train loss 5.70 on epoch=104
06/24/2022 11:19:49 - INFO - __main__ - Step 220 Global step 220 Train loss 5.65 on epoch=109
06/24/2022 11:19:50 - INFO - __main__ - Step 230 Global step 230 Train loss 5.74 on epoch=114
06/24/2022 11:19:51 - INFO - __main__ - Step 240 Global step 240 Train loss 5.84 on epoch=119
06/24/2022 11:19:52 - INFO - __main__ - Step 250 Global step 250 Train loss 5.79 on epoch=124
06/24/2022 11:19:54 - INFO - __main__ - Global step 250 Train loss 5.75 Classification-F1 0.0 on epoch=124
06/24/2022 11:19:56 - INFO - __main__ - Step 260 Global step 260 Train loss 5.64 on epoch=129
06/24/2022 11:19:57 - INFO - __main__ - Step 270 Global step 270 Train loss 5.67 on epoch=134
06/24/2022 11:19:58 - INFO - __main__ - Step 280 Global step 280 Train loss 5.67 on epoch=139
06/24/2022 11:20:00 - INFO - __main__ - Step 290 Global step 290 Train loss 5.57 on epoch=144
06/24/2022 11:20:01 - INFO - __main__ - Step 300 Global step 300 Train loss 5.61 on epoch=149
06/24/2022 11:20:02 - INFO - __main__ - Global step 300 Train loss 5.63 Classification-F1 0.0 on epoch=149
06/24/2022 11:20:03 - INFO - __main__ - Step 310 Global step 310 Train loss 5.61 on epoch=154
06/24/2022 11:20:04 - INFO - __main__ - Step 320 Global step 320 Train loss 5.54 on epoch=159
06/24/2022 11:20:06 - INFO - __main__ - Step 330 Global step 330 Train loss 5.68 on epoch=164
06/24/2022 11:20:07 - INFO - __main__ - Step 340 Global step 340 Train loss 5.57 on epoch=169
06/24/2022 11:20:08 - INFO - __main__ - Step 350 Global step 350 Train loss 5.60 on epoch=174
06/24/2022 11:20:14 - INFO - __main__ - Global step 350 Train loss 5.60 Classification-F1 0.0 on epoch=174
06/24/2022 11:20:15 - INFO - __main__ - Step 360 Global step 360 Train loss 5.59 on epoch=179
06/24/2022 11:20:17 - INFO - __main__ - Step 370 Global step 370 Train loss 5.55 on epoch=184
06/24/2022 11:20:18 - INFO - __main__ - Step 380 Global step 380 Train loss 5.47 on epoch=189
06/24/2022 11:20:19 - INFO - __main__ - Step 390 Global step 390 Train loss 5.44 on epoch=194
06/24/2022 11:20:21 - INFO - __main__ - Step 400 Global step 400 Train loss 5.53 on epoch=199
06/24/2022 11:20:23 - INFO - __main__ - Global step 400 Train loss 5.51 Classification-F1 0.0 on epoch=199
06/24/2022 11:20:24 - INFO - __main__ - Step 410 Global step 410 Train loss 5.36 on epoch=204
06/24/2022 11:20:26 - INFO - __main__ - Step 420 Global step 420 Train loss 5.39 on epoch=209
06/24/2022 11:20:27 - INFO - __main__ - Step 430 Global step 430 Train loss 5.34 on epoch=214
06/24/2022 11:20:28 - INFO - __main__ - Step 440 Global step 440 Train loss 5.27 on epoch=219
06/24/2022 11:20:29 - INFO - __main__ - Step 450 Global step 450 Train loss 5.17 on epoch=224
06/24/2022 11:20:31 - INFO - __main__ - Global step 450 Train loss 5.30 Classification-F1 0.0 on epoch=224
06/24/2022 11:20:32 - INFO - __main__ - Step 460 Global step 460 Train loss 5.18 on epoch=229
06/24/2022 11:20:33 - INFO - __main__ - Step 470 Global step 470 Train loss 5.09 on epoch=234
06/24/2022 11:20:35 - INFO - __main__ - Step 480 Global step 480 Train loss 5.06 on epoch=239
06/24/2022 11:20:36 - INFO - __main__ - Step 490 Global step 490 Train loss 4.89 on epoch=244
06/24/2022 11:20:37 - INFO - __main__ - Step 500 Global step 500 Train loss 4.71 on epoch=249
06/24/2022 11:20:47 - INFO - __main__ - Global step 500 Train loss 4.98 Classification-F1 0.0 on epoch=249
06/24/2022 11:20:49 - INFO - __main__ - Step 510 Global step 510 Train loss 4.56 on epoch=254
06/24/2022 11:20:50 - INFO - __main__ - Step 520 Global step 520 Train loss 4.44 on epoch=259
06/24/2022 11:20:51 - INFO - __main__ - Step 530 Global step 530 Train loss 4.39 on epoch=264
06/24/2022 11:20:52 - INFO - __main__ - Step 540 Global step 540 Train loss 4.18 on epoch=269
06/24/2022 11:20:54 - INFO - __main__ - Step 550 Global step 550 Train loss 4.17 on epoch=274
06/24/2022 11:21:00 - INFO - __main__ - Global step 550 Train loss 4.35 Classification-F1 0.0 on epoch=274
06/24/2022 11:21:01 - INFO - __main__ - Step 560 Global step 560 Train loss 4.15 on epoch=279
06/24/2022 11:21:02 - INFO - __main__ - Step 570 Global step 570 Train loss 4.22 on epoch=284
06/24/2022 11:21:03 - INFO - __main__ - Step 580 Global step 580 Train loss 4.24 on epoch=289
06/24/2022 11:21:05 - INFO - __main__ - Step 590 Global step 590 Train loss 4.11 on epoch=294
06/24/2022 11:21:06 - INFO - __main__ - Step 600 Global step 600 Train loss 4.14 on epoch=299
06/24/2022 11:21:16 - INFO - __main__ - Global step 600 Train loss 4.17 Classification-F1 0.0 on epoch=299
06/24/2022 11:21:17 - INFO - __main__ - Step 610 Global step 610 Train loss 4.04 on epoch=304
06/24/2022 11:21:19 - INFO - __main__ - Step 620 Global step 620 Train loss 4.04 on epoch=309
06/24/2022 11:21:20 - INFO - __main__ - Step 630 Global step 630 Train loss 3.94 on epoch=314
06/24/2022 11:21:21 - INFO - __main__ - Step 640 Global step 640 Train loss 4.00 on epoch=319
06/24/2022 11:21:22 - INFO - __main__ - Step 650 Global step 650 Train loss 3.82 on epoch=324
06/24/2022 11:21:33 - INFO - __main__ - Global step 650 Train loss 3.97 Classification-F1 0.0 on epoch=324
06/24/2022 11:21:34 - INFO - __main__ - Step 660 Global step 660 Train loss 3.96 on epoch=329
06/24/2022 11:21:35 - INFO - __main__ - Step 670 Global step 670 Train loss 3.93 on epoch=334
06/24/2022 11:21:37 - INFO - __main__ - Step 680 Global step 680 Train loss 3.78 on epoch=339
06/24/2022 11:21:38 - INFO - __main__ - Step 690 Global step 690 Train loss 3.54 on epoch=344
06/24/2022 11:21:39 - INFO - __main__ - Step 700 Global step 700 Train loss 3.78 on epoch=349
06/24/2022 11:21:50 - INFO - __main__ - Global step 700 Train loss 3.80 Classification-F1 0.0 on epoch=349
06/24/2022 11:21:51 - INFO - __main__ - Step 710 Global step 710 Train loss 3.68 on epoch=354
06/24/2022 11:21:52 - INFO - __main__ - Step 720 Global step 720 Train loss 3.75 on epoch=359
06/24/2022 11:21:53 - INFO - __main__ - Step 730 Global step 730 Train loss 3.53 on epoch=364
06/24/2022 11:21:55 - INFO - __main__ - Step 740 Global step 740 Train loss 3.55 on epoch=369
06/24/2022 11:21:56 - INFO - __main__ - Step 750 Global step 750 Train loss 3.50 on epoch=374
06/24/2022 11:22:07 - INFO - __main__ - Global step 750 Train loss 3.60 Classification-F1 0.0 on epoch=374
06/24/2022 11:22:08 - INFO - __main__ - Step 760 Global step 760 Train loss 3.63 on epoch=379
06/24/2022 11:22:09 - INFO - __main__ - Step 770 Global step 770 Train loss 3.54 on epoch=384
06/24/2022 11:22:10 - INFO - __main__ - Step 780 Global step 780 Train loss 3.45 on epoch=389
06/24/2022 11:22:12 - INFO - __main__ - Step 790 Global step 790 Train loss 3.41 on epoch=394
06/24/2022 11:22:13 - INFO - __main__ - Step 800 Global step 800 Train loss 3.27 on epoch=399
06/24/2022 11:22:23 - INFO - __main__ - Global step 800 Train loss 3.46 Classification-F1 0.005263157894736842 on epoch=399
06/24/2022 11:22:24 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.005263157894736842 on epoch=399, global_step=800
06/24/2022 11:22:25 - INFO - __main__ - Step 810 Global step 810 Train loss 3.17 on epoch=404
06/24/2022 11:22:26 - INFO - __main__ - Step 820 Global step 820 Train loss 3.27 on epoch=409
06/24/2022 11:22:27 - INFO - __main__ - Step 830 Global step 830 Train loss 3.19 on epoch=414
06/24/2022 11:22:29 - INFO - __main__ - Step 840 Global step 840 Train loss 3.13 on epoch=419
06/24/2022 11:22:30 - INFO - __main__ - Step 850 Global step 850 Train loss 3.14 on epoch=424
06/24/2022 11:22:40 - INFO - __main__ - Global step 850 Train loss 3.18 Classification-F1 0.028409090909090908 on epoch=424
06/24/2022 11:22:40 - INFO - __main__ - Saving model with best Classification-F1: 0.005263157894736842 -> 0.028409090909090908 on epoch=424, global_step=850
06/24/2022 11:22:42 - INFO - __main__ - Step 860 Global step 860 Train loss 3.02 on epoch=429
06/24/2022 11:22:43 - INFO - __main__ - Step 870 Global step 870 Train loss 3.00 on epoch=434
06/24/2022 11:22:44 - INFO - __main__ - Step 880 Global step 880 Train loss 2.99 on epoch=439
06/24/2022 11:22:45 - INFO - __main__ - Step 890 Global step 890 Train loss 2.91 on epoch=444
06/24/2022 11:22:47 - INFO - __main__ - Step 900 Global step 900 Train loss 2.94 on epoch=449
06/24/2022 11:22:57 - INFO - __main__ - Global step 900 Train loss 2.97 Classification-F1 0.07563025210084033 on epoch=449
06/24/2022 11:22:57 - INFO - __main__ - Saving model with best Classification-F1: 0.028409090909090908 -> 0.07563025210084033 on epoch=449, global_step=900
06/24/2022 11:22:58 - INFO - __main__ - Step 910 Global step 910 Train loss 2.90 on epoch=454
06/24/2022 11:23:00 - INFO - __main__ - Step 920 Global step 920 Train loss 2.86 on epoch=459
06/24/2022 11:23:01 - INFO - __main__ - Step 930 Global step 930 Train loss 2.82 on epoch=464
06/24/2022 11:23:02 - INFO - __main__ - Step 940 Global step 940 Train loss 2.79 on epoch=469
06/24/2022 11:23:03 - INFO - __main__ - Step 950 Global step 950 Train loss 2.83 on epoch=474
06/24/2022 11:23:14 - INFO - __main__ - Global step 950 Train loss 2.84 Classification-F1 0.052083333333333336 on epoch=474
06/24/2022 11:23:16 - INFO - __main__ - Step 960 Global step 960 Train loss 2.74 on epoch=479
06/24/2022 11:23:17 - INFO - __main__ - Step 970 Global step 970 Train loss 2.60 on epoch=484
06/24/2022 11:23:18 - INFO - __main__ - Step 980 Global step 980 Train loss 2.69 on epoch=489
06/24/2022 11:23:19 - INFO - __main__ - Step 990 Global step 990 Train loss 2.58 on epoch=494
06/24/2022 11:23:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.62 on epoch=499
06/24/2022 11:23:32 - INFO - __main__ - Global step 1000 Train loss 2.65 Classification-F1 0.032051282051282055 on epoch=499
06/24/2022 11:23:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.58 on epoch=504
06/24/2022 11:23:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.44 on epoch=509
06/24/2022 11:23:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.59 on epoch=514
06/24/2022 11:23:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.51 on epoch=519
06/24/2022 11:23:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.50 on epoch=524
06/24/2022 11:23:48 - INFO - __main__ - Global step 1050 Train loss 2.52 Classification-F1 0.09059233449477352 on epoch=524
06/24/2022 11:23:48 - INFO - __main__ - Saving model with best Classification-F1: 0.07563025210084033 -> 0.09059233449477352 on epoch=524, global_step=1050
06/24/2022 11:23:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.46 on epoch=529
06/24/2022 11:23:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.46 on epoch=534
06/24/2022 11:23:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.40 on epoch=539
06/24/2022 11:23:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.48 on epoch=544
06/24/2022 11:23:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.28 on epoch=549
06/24/2022 11:24:01 - INFO - __main__ - Global step 1100 Train loss 2.42 Classification-F1 0.10204081632653061 on epoch=549
06/24/2022 11:24:01 - INFO - __main__ - Saving model with best Classification-F1: 0.09059233449477352 -> 0.10204081632653061 on epoch=549, global_step=1100
06/24/2022 11:24:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.42 on epoch=554
06/24/2022 11:24:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 2.22 on epoch=559
06/24/2022 11:24:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.27 on epoch=564
06/24/2022 11:24:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 2.20 on epoch=569
06/24/2022 11:24:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 2.12 on epoch=574
06/24/2022 11:24:18 - INFO - __main__ - Global step 1150 Train loss 2.25 Classification-F1 0.32222222222222224 on epoch=574
06/24/2022 11:24:18 - INFO - __main__ - Saving model with best Classification-F1: 0.10204081632653061 -> 0.32222222222222224 on epoch=574, global_step=1150
06/24/2022 11:24:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.18 on epoch=579
06/24/2022 11:24:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.02 on epoch=584
06/24/2022 11:24:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 2.08 on epoch=589
06/24/2022 11:24:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.18 on epoch=594
06/24/2022 11:24:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.12 on epoch=599
06/24/2022 11:24:28 - INFO - __main__ - Global step 1200 Train loss 2.12 Classification-F1 0.3950617283950617 on epoch=599
06/24/2022 11:24:28 - INFO - __main__ - Saving model with best Classification-F1: 0.32222222222222224 -> 0.3950617283950617 on epoch=599, global_step=1200
06/24/2022 11:24:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.14 on epoch=604
06/24/2022 11:24:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.18 on epoch=609
06/24/2022 11:24:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.08 on epoch=614
06/24/2022 11:24:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.02 on epoch=619
06/24/2022 11:24:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.91 on epoch=624
06/24/2022 11:24:41 - INFO - __main__ - Global step 1250 Train loss 2.06 Classification-F1 0.34885439763488546 on epoch=624
06/24/2022 11:24:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.14 on epoch=629
06/24/2022 11:24:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.10 on epoch=634
06/24/2022 11:24:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.93 on epoch=639
06/24/2022 11:24:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.93 on epoch=644
06/24/2022 11:24:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.89 on epoch=649
06/24/2022 11:24:49 - INFO - __main__ - Global step 1300 Train loss 2.00 Classification-F1 0.4009852216748768 on epoch=649
06/24/2022 11:24:49 - INFO - __main__ - Saving model with best Classification-F1: 0.3950617283950617 -> 0.4009852216748768 on epoch=649, global_step=1300
06/24/2022 11:24:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.90 on epoch=654
06/24/2022 11:24:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.96 on epoch=659
06/24/2022 11:24:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.95 on epoch=664
06/24/2022 11:24:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.74 on epoch=669
06/24/2022 11:24:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.71 on epoch=674
06/24/2022 11:25:02 - INFO - __main__ - Global step 1350 Train loss 1.85 Classification-F1 0.4225828262339419 on epoch=674
06/24/2022 11:25:02 - INFO - __main__ - Saving model with best Classification-F1: 0.4009852216748768 -> 0.4225828262339419 on epoch=674, global_step=1350
06/24/2022 11:25:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.72 on epoch=679
06/24/2022 11:25:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.78 on epoch=684
06/24/2022 11:25:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.74 on epoch=689
06/24/2022 11:25:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.53 on epoch=694
06/24/2022 11:25:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.51 on epoch=699
06/24/2022 11:25:09 - INFO - __main__ - Global step 1400 Train loss 1.66 Classification-F1 0.625 on epoch=699
06/24/2022 11:25:09 - INFO - __main__ - Saving model with best Classification-F1: 0.4225828262339419 -> 0.625 on epoch=699, global_step=1400
06/24/2022 11:25:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.78 on epoch=704
06/24/2022 11:25:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.55 on epoch=709
06/24/2022 11:25:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.56 on epoch=714
06/24/2022 11:25:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.51 on epoch=719
06/24/2022 11:25:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.59 on epoch=724
06/24/2022 11:25:16 - INFO - __main__ - Global step 1450 Train loss 1.60 Classification-F1 0.28888888888888886 on epoch=724
06/24/2022 11:25:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.55 on epoch=729
06/24/2022 11:25:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.67 on epoch=734
06/24/2022 11:25:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.55 on epoch=739
06/24/2022 11:25:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.46 on epoch=744
06/24/2022 11:25:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.66 on epoch=749
06/24/2022 11:25:24 - INFO - __main__ - Global step 1500 Train loss 1.58 Classification-F1 0.4666666666666667 on epoch=749
06/24/2022 11:25:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.47 on epoch=754
06/24/2022 11:25:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.52 on epoch=759
06/24/2022 11:25:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.42 on epoch=764
06/24/2022 11:25:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.67 on epoch=769
06/24/2022 11:25:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.46 on epoch=774
06/24/2022 11:25:31 - INFO - __main__ - Global step 1550 Train loss 1.51 Classification-F1 0.5076923076923077 on epoch=774
06/24/2022 11:25:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.46 on epoch=779
06/24/2022 11:25:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.44 on epoch=784
06/24/2022 11:25:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.43 on epoch=789
06/24/2022 11:25:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.33 on epoch=794
06/24/2022 11:25:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.32 on epoch=799
06/24/2022 11:25:37 - INFO - __main__ - Global step 1600 Train loss 1.39 Classification-F1 0.625 on epoch=799
06/24/2022 11:25:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.43 on epoch=804
06/24/2022 11:25:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.29 on epoch=809
06/24/2022 11:25:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.39 on epoch=814
06/24/2022 11:25:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.27 on epoch=819
06/24/2022 11:25:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.25 on epoch=824
06/24/2022 11:25:44 - INFO - __main__ - Global step 1650 Train loss 1.33 Classification-F1 0.3816425120772947 on epoch=824
06/24/2022 11:25:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.27 on epoch=829
06/24/2022 11:25:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.17 on epoch=834
06/24/2022 11:25:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.28 on epoch=839
06/24/2022 11:25:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.24 on epoch=844
06/24/2022 11:25:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.23 on epoch=849
06/24/2022 11:25:51 - INFO - __main__ - Global step 1700 Train loss 1.24 Classification-F1 0.4458874458874459 on epoch=849
06/24/2022 11:25:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.20 on epoch=854
06/24/2022 11:25:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.20 on epoch=859
06/24/2022 11:25:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.12 on epoch=864
06/24/2022 11:25:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.21 on epoch=869
06/24/2022 11:25:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.22 on epoch=874
06/24/2022 11:25:57 - INFO - __main__ - Global step 1750 Train loss 1.19 Classification-F1 0.3816425120772947 on epoch=874
06/24/2022 11:25:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.17 on epoch=879
06/24/2022 11:26:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.13 on epoch=884
06/24/2022 11:26:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.15 on epoch=889
06/24/2022 11:26:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.12 on epoch=894
06/24/2022 11:26:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.20 on epoch=899
06/24/2022 11:26:04 - INFO - __main__ - Global step 1800 Train loss 1.15 Classification-F1 0.2727272727272727 on epoch=899
06/24/2022 11:26:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.11 on epoch=904
06/24/2022 11:26:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.24 on epoch=909
06/24/2022 11:26:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.13 on epoch=914
06/24/2022 11:26:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.15 on epoch=919
06/24/2022 11:26:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.21 on epoch=924
06/24/2022 11:26:11 - INFO - __main__ - Global step 1850 Train loss 1.17 Classification-F1 0.5333333333333333 on epoch=924
06/24/2022 11:26:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.05 on epoch=929
06/24/2022 11:26:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.05 on epoch=934
06/24/2022 11:26:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.02 on epoch=939
06/24/2022 11:26:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.05 on epoch=944
06/24/2022 11:26:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.05 on epoch=949
06/24/2022 11:26:18 - INFO - __main__ - Global step 1900 Train loss 1.04 Classification-F1 0.5607843137254902 on epoch=949
06/24/2022 11:26:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.03 on epoch=954
06/24/2022 11:26:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.10 on epoch=959
06/24/2022 11:26:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.10 on epoch=964
06/24/2022 11:26:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.06 on epoch=969
06/24/2022 11:26:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.11 on epoch=974
06/24/2022 11:26:24 - INFO - __main__ - Global step 1950 Train loss 1.08 Classification-F1 0.36374269005847953 on epoch=974
06/24/2022 11:26:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.92 on epoch=979
06/24/2022 11:26:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.98 on epoch=984
06/24/2022 11:26:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.03 on epoch=989
06/24/2022 11:26:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.95 on epoch=994
06/24/2022 11:26:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.00 on epoch=999
06/24/2022 11:26:31 - INFO - __main__ - Global step 2000 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=999
06/24/2022 11:26:31 - INFO - __main__ - save last model!
06/24/2022 11:26:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 11:26:31 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 11:26:31 - INFO - __main__ - Printing 3 examples
06/24/2022 11:26:31 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 11:26:31 - INFO - __main__ - ['0']
06/24/2022 11:26:31 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 11:26:31 - INFO - __main__ - ['1']
06/24/2022 11:26:31 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 11:26:31 - INFO - __main__ - ['1']
06/24/2022 11:26:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:26:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:26:32 - INFO - __main__ - Printing 3 examples
06/24/2022 11:26:32 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 11:26:32 - INFO - __main__ - ['1']
06/24/2022 11:26:32 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 11:26:32 - INFO - __main__ - ['1']
06/24/2022 11:26:32 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 11:26:32 - INFO - __main__ - ['1']
06/24/2022 11:26:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 11:26:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:26:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:26:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:26:32 - INFO - __main__ - Printing 3 examples
06/24/2022 11:26:32 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 11:26:32 - INFO - __main__ - ['1']
06/24/2022 11:26:32 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 11:26:32 - INFO - __main__ - ['1']
06/24/2022 11:26:32 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 11:26:32 - INFO - __main__ - ['1']
06/24/2022 11:26:32 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:26:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:26:32 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:26:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:26:37 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:26:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:26:38 - INFO - __main__ - Starting training!
06/24/2022 11:26:43 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 11:28:07 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_13_0.2_8_predictions.txt
06/24/2022 11:28:07 - INFO - __main__ - Classification-F1 on test data: 0.3344
06/24/2022 11:28:07 - INFO - __main__ - prefix=paws_16_13, lr=0.2, bsz=8, dev_performance=0.625, test_performance=0.3343550907561005
06/24/2022 11:28:07 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.5, bsz=8 ...
06/24/2022 11:28:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:28:08 - INFO - __main__ - Printing 3 examples
06/24/2022 11:28:08 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 11:28:08 - INFO - __main__ - ['1']
06/24/2022 11:28:08 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 11:28:08 - INFO - __main__ - ['1']
06/24/2022 11:28:08 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 11:28:08 - INFO - __main__ - ['1']
06/24/2022 11:28:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:28:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:28:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:28:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:28:08 - INFO - __main__ - Printing 3 examples
06/24/2022 11:28:08 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 11:28:08 - INFO - __main__ - ['1']
06/24/2022 11:28:08 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 11:28:08 - INFO - __main__ - ['1']
06/24/2022 11:28:08 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 11:28:08 - INFO - __main__ - ['1']
06/24/2022 11:28:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:28:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:28:08 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:28:14 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:28:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:28:14 - INFO - __main__ - Starting training!
06/24/2022 11:28:16 - INFO - __main__ - Step 10 Global step 10 Train loss 5.96 on epoch=4
06/24/2022 11:28:17 - INFO - __main__ - Step 20 Global step 20 Train loss 5.92 on epoch=9
06/24/2022 11:28:18 - INFO - __main__ - Step 30 Global step 30 Train loss 5.89 on epoch=14
06/24/2022 11:28:19 - INFO - __main__ - Step 40 Global step 40 Train loss 5.86 on epoch=19
06/24/2022 11:28:21 - INFO - __main__ - Step 50 Global step 50 Train loss 5.87 on epoch=24
06/24/2022 11:28:31 - INFO - __main__ - Global step 50 Train loss 5.90 Classification-F1 0.0 on epoch=24
06/24/2022 11:28:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 11:28:32 - INFO - __main__ - Step 60 Global step 60 Train loss 5.74 on epoch=29
06/24/2022 11:28:33 - INFO - __main__ - Step 70 Global step 70 Train loss 5.64 on epoch=34
06/24/2022 11:28:35 - INFO - __main__ - Step 80 Global step 80 Train loss 5.76 on epoch=39
06/24/2022 11:28:36 - INFO - __main__ - Step 90 Global step 90 Train loss 5.69 on epoch=44
06/24/2022 11:28:37 - INFO - __main__ - Step 100 Global step 100 Train loss 5.63 on epoch=49
06/24/2022 11:28:43 - INFO - __main__ - Global step 100 Train loss 5.69 Classification-F1 0.0 on epoch=49
06/24/2022 11:28:44 - INFO - __main__ - Step 110 Global step 110 Train loss 5.60 on epoch=54
06/24/2022 11:28:46 - INFO - __main__ - Step 120 Global step 120 Train loss 5.65 on epoch=59
06/24/2022 11:28:47 - INFO - __main__ - Step 130 Global step 130 Train loss 5.53 on epoch=64
06/24/2022 11:28:48 - INFO - __main__ - Step 140 Global step 140 Train loss 5.50 on epoch=69
06/24/2022 11:28:50 - INFO - __main__ - Step 150 Global step 150 Train loss 5.33 on epoch=74
06/24/2022 11:28:56 - INFO - __main__ - Global step 150 Train loss 5.52 Classification-F1 0.0 on epoch=74
06/24/2022 11:28:57 - INFO - __main__ - Step 160 Global step 160 Train loss 5.43 on epoch=79
06/24/2022 11:28:58 - INFO - __main__ - Step 170 Global step 170 Train loss 5.33 on epoch=84
06/24/2022 11:28:59 - INFO - __main__ - Step 180 Global step 180 Train loss 5.30 on epoch=89
06/24/2022 11:29:01 - INFO - __main__ - Step 190 Global step 190 Train loss 5.07 on epoch=94
06/24/2022 11:29:02 - INFO - __main__ - Step 200 Global step 200 Train loss 4.72 on epoch=99
06/24/2022 11:29:12 - INFO - __main__ - Global step 200 Train loss 5.17 Classification-F1 0.0 on epoch=99
06/24/2022 11:29:13 - INFO - __main__ - Step 210 Global step 210 Train loss 4.55 on epoch=104
06/24/2022 11:29:15 - INFO - __main__ - Step 220 Global step 220 Train loss 4.47 on epoch=109
06/24/2022 11:29:16 - INFO - __main__ - Step 230 Global step 230 Train loss 4.23 on epoch=114
06/24/2022 11:29:17 - INFO - __main__ - Step 240 Global step 240 Train loss 4.12 on epoch=119
06/24/2022 11:29:19 - INFO - __main__ - Step 250 Global step 250 Train loss 4.04 on epoch=124
06/24/2022 11:29:29 - INFO - __main__ - Global step 250 Train loss 4.28 Classification-F1 0.0 on epoch=124
06/24/2022 11:29:31 - INFO - __main__ - Step 260 Global step 260 Train loss 3.88 on epoch=129
06/24/2022 11:29:32 - INFO - __main__ - Step 270 Global step 270 Train loss 3.87 on epoch=134
06/24/2022 11:29:33 - INFO - __main__ - Step 280 Global step 280 Train loss 3.78 on epoch=139
06/24/2022 11:29:34 - INFO - __main__ - Step 290 Global step 290 Train loss 3.64 on epoch=144
06/24/2022 11:29:36 - INFO - __main__ - Step 300 Global step 300 Train loss 3.35 on epoch=149
06/24/2022 11:29:46 - INFO - __main__ - Global step 300 Train loss 3.71 Classification-F1 0.0 on epoch=149
06/24/2022 11:29:48 - INFO - __main__ - Step 310 Global step 310 Train loss 3.36 on epoch=154
06/24/2022 11:29:49 - INFO - __main__ - Step 320 Global step 320 Train loss 3.22 on epoch=159
06/24/2022 11:29:50 - INFO - __main__ - Step 330 Global step 330 Train loss 3.10 on epoch=164
06/24/2022 11:29:51 - INFO - __main__ - Step 340 Global step 340 Train loss 3.21 on epoch=169
06/24/2022 11:29:53 - INFO - __main__ - Step 350 Global step 350 Train loss 2.95 on epoch=174
06/24/2022 11:29:54 - INFO - __main__ - Global step 350 Train loss 3.17 Classification-F1 0.0923076923076923 on epoch=174
06/24/2022 11:29:54 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0923076923076923 on epoch=174, global_step=350
06/24/2022 11:29:56 - INFO - __main__ - Step 360 Global step 360 Train loss 2.85 on epoch=179
06/24/2022 11:29:57 - INFO - __main__ - Step 370 Global step 370 Train loss 2.71 on epoch=184
06/24/2022 11:29:58 - INFO - __main__ - Step 380 Global step 380 Train loss 2.62 on epoch=189
06/24/2022 11:29:59 - INFO - __main__ - Step 390 Global step 390 Train loss 2.68 on epoch=194
06/24/2022 11:30:01 - INFO - __main__ - Step 400 Global step 400 Train loss 2.53 on epoch=199
06/24/2022 11:30:06 - INFO - __main__ - Global step 400 Train loss 2.68 Classification-F1 0.16304347826086957 on epoch=199
06/24/2022 11:30:06 - INFO - __main__ - Saving model with best Classification-F1: 0.0923076923076923 -> 0.16304347826086957 on epoch=199, global_step=400
06/24/2022 11:30:08 - INFO - __main__ - Step 410 Global step 410 Train loss 2.57 on epoch=204
06/24/2022 11:30:09 - INFO - __main__ - Step 420 Global step 420 Train loss 2.46 on epoch=209
06/24/2022 11:30:10 - INFO - __main__ - Step 430 Global step 430 Train loss 2.24 on epoch=214
06/24/2022 11:30:11 - INFO - __main__ - Step 440 Global step 440 Train loss 2.17 on epoch=219
06/24/2022 11:30:13 - INFO - __main__ - Step 450 Global step 450 Train loss 2.38 on epoch=224
06/24/2022 11:30:14 - INFO - __main__ - Global step 450 Train loss 2.36 Classification-F1 0.3992490613266583 on epoch=224
06/24/2022 11:30:14 - INFO - __main__ - Saving model with best Classification-F1: 0.16304347826086957 -> 0.3992490613266583 on epoch=224, global_step=450
06/24/2022 11:30:15 - INFO - __main__ - Step 460 Global step 460 Train loss 2.13 on epoch=229
06/24/2022 11:30:16 - INFO - __main__ - Step 470 Global step 470 Train loss 2.12 on epoch=234
06/24/2022 11:30:17 - INFO - __main__ - Step 480 Global step 480 Train loss 1.89 on epoch=239
06/24/2022 11:30:19 - INFO - __main__ - Step 490 Global step 490 Train loss 1.94 on epoch=244
06/24/2022 11:30:20 - INFO - __main__ - Step 500 Global step 500 Train loss 1.81 on epoch=249
06/24/2022 11:30:20 - INFO - __main__ - Global step 500 Train loss 1.98 Classification-F1 0.5844155844155844 on epoch=249
06/24/2022 11:30:20 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5844155844155844 on epoch=249, global_step=500
06/24/2022 11:30:22 - INFO - __main__ - Step 510 Global step 510 Train loss 1.71 on epoch=254
06/24/2022 11:30:23 - INFO - __main__ - Step 520 Global step 520 Train loss 1.56 on epoch=259
06/24/2022 11:30:24 - INFO - __main__ - Step 530 Global step 530 Train loss 1.59 on epoch=264
06/24/2022 11:30:26 - INFO - __main__ - Step 540 Global step 540 Train loss 1.49 on epoch=269
06/24/2022 11:30:27 - INFO - __main__ - Step 550 Global step 550 Train loss 1.46 on epoch=274
06/24/2022 11:30:27 - INFO - __main__ - Global step 550 Train loss 1.56 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 11:30:29 - INFO - __main__ - Step 560 Global step 560 Train loss 1.62 on epoch=279
06/24/2022 11:30:30 - INFO - __main__ - Step 570 Global step 570 Train loss 1.36 on epoch=284
06/24/2022 11:30:31 - INFO - __main__ - Step 580 Global step 580 Train loss 1.41 on epoch=289
06/24/2022 11:30:32 - INFO - __main__ - Step 590 Global step 590 Train loss 1.54 on epoch=294
06/24/2022 11:30:34 - INFO - __main__ - Step 600 Global step 600 Train loss 1.47 on epoch=299
06/24/2022 11:30:34 - INFO - __main__ - Global step 600 Train loss 1.48 Classification-F1 0.3816425120772947 on epoch=299
06/24/2022 11:30:35 - INFO - __main__ - Step 610 Global step 610 Train loss 1.25 on epoch=304
06/24/2022 11:30:37 - INFO - __main__ - Step 620 Global step 620 Train loss 1.27 on epoch=309
06/24/2022 11:30:38 - INFO - __main__ - Step 630 Global step 630 Train loss 1.35 on epoch=314
06/24/2022 11:30:39 - INFO - __main__ - Step 640 Global step 640 Train loss 1.14 on epoch=319
06/24/2022 11:30:41 - INFO - __main__ - Step 650 Global step 650 Train loss 1.23 on epoch=324
06/24/2022 11:30:41 - INFO - __main__ - Global step 650 Train loss 1.25 Classification-F1 0.3816425120772947 on epoch=324
06/24/2022 11:30:42 - INFO - __main__ - Step 660 Global step 660 Train loss 1.14 on epoch=329
06/24/2022 11:30:44 - INFO - __main__ - Step 670 Global step 670 Train loss 1.19 on epoch=334
06/24/2022 11:30:45 - INFO - __main__ - Step 680 Global step 680 Train loss 1.19 on epoch=339
06/24/2022 11:30:46 - INFO - __main__ - Step 690 Global step 690 Train loss 1.14 on epoch=344
06/24/2022 11:30:47 - INFO - __main__ - Step 700 Global step 700 Train loss 1.10 on epoch=349
06/24/2022 11:30:48 - INFO - __main__ - Global step 700 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=349
06/24/2022 11:30:49 - INFO - __main__ - Step 710 Global step 710 Train loss 1.01 on epoch=354
06/24/2022 11:30:50 - INFO - __main__ - Step 720 Global step 720 Train loss 1.08 on epoch=359
06/24/2022 11:30:52 - INFO - __main__ - Step 730 Global step 730 Train loss 1.04 on epoch=364
06/24/2022 11:30:53 - INFO - __main__ - Step 740 Global step 740 Train loss 1.01 on epoch=369
06/24/2022 11:30:54 - INFO - __main__ - Step 750 Global step 750 Train loss 1.03 on epoch=374
06/24/2022 11:30:55 - INFO - __main__ - Global step 750 Train loss 1.04 Classification-F1 0.40566959921798634 on epoch=374
06/24/2022 11:30:56 - INFO - __main__ - Step 760 Global step 760 Train loss 1.02 on epoch=379
06/24/2022 11:30:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.88 on epoch=384
06/24/2022 11:30:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=389
06/24/2022 11:31:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.98 on epoch=394
06/24/2022 11:31:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.92 on epoch=399
06/24/2022 11:31:01 - INFO - __main__ - Global step 800 Train loss 0.93 Classification-F1 0.5465587044534412 on epoch=399
06/24/2022 11:31:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.81 on epoch=404
06/24/2022 11:31:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.97 on epoch=409
06/24/2022 11:31:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.89 on epoch=414
06/24/2022 11:31:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.82 on epoch=419
06/24/2022 11:31:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.82 on epoch=424
06/24/2022 11:31:08 - INFO - __main__ - Global step 850 Train loss 0.86 Classification-F1 0.3816425120772947 on epoch=424
06/24/2022 11:31:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.76 on epoch=429
06/24/2022 11:31:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.78 on epoch=434
06/24/2022 11:31:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.90 on epoch=439
06/24/2022 11:31:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.82 on epoch=444
06/24/2022 11:31:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.81 on epoch=449
06/24/2022 11:31:15 - INFO - __main__ - Global step 900 Train loss 0.81 Classification-F1 0.40566959921798634 on epoch=449
06/24/2022 11:31:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.83 on epoch=454
06/24/2022 11:31:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.73 on epoch=459
06/24/2022 11:31:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.67 on epoch=464
06/24/2022 11:31:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.71 on epoch=469
06/24/2022 11:31:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.70 on epoch=474
06/24/2022 11:31:21 - INFO - __main__ - Global step 950 Train loss 0.73 Classification-F1 0.4385964912280702 on epoch=474
06/24/2022 11:31:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.77 on epoch=479
06/24/2022 11:31:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.60 on epoch=484
06/24/2022 11:31:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.74 on epoch=489
06/24/2022 11:31:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.63 on epoch=494
06/24/2022 11:31:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.66 on epoch=499
06/24/2022 11:31:28 - INFO - __main__ - Global step 1000 Train loss 0.68 Classification-F1 0.5270935960591133 on epoch=499
06/24/2022 11:31:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.71 on epoch=504
06/24/2022 11:31:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.62 on epoch=509
06/24/2022 11:31:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.69 on epoch=514
06/24/2022 11:31:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.71 on epoch=519
06/24/2022 11:31:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.69 on epoch=524
06/24/2022 11:31:35 - INFO - __main__ - Global step 1050 Train loss 0.68 Classification-F1 0.46843853820598 on epoch=524
06/24/2022 11:31:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.64 on epoch=529
06/24/2022 11:31:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.69 on epoch=534
06/24/2022 11:31:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.65 on epoch=539
06/24/2022 11:31:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.71 on epoch=544
06/24/2022 11:31:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.63 on epoch=549
06/24/2022 11:31:42 - INFO - __main__ - Global step 1100 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=549
06/24/2022 11:31:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.69 on epoch=554
06/24/2022 11:31:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.70 on epoch=559
06/24/2022 11:31:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.67 on epoch=564
06/24/2022 11:31:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.65 on epoch=569
06/24/2022 11:31:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.65 on epoch=574
06/24/2022 11:31:48 - INFO - __main__ - Global step 1150 Train loss 0.67 Classification-F1 0.3191489361702127 on epoch=574
06/24/2022 11:31:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.66 on epoch=579
06/24/2022 11:31:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.53 on epoch=584
06/24/2022 11:31:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.63 on epoch=589
06/24/2022 11:31:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.63 on epoch=594
06/24/2022 11:31:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.67 on epoch=599
06/24/2022 11:31:55 - INFO - __main__ - Global step 1200 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=599
06/24/2022 11:31:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.65 on epoch=604
06/24/2022 11:31:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.63 on epoch=609
06/24/2022 11:31:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.55 on epoch=614
06/24/2022 11:32:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.65 on epoch=619
06/24/2022 11:32:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.57 on epoch=624
06/24/2022 11:32:02 - INFO - __main__ - Global step 1250 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=624
06/24/2022 11:32:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.66 on epoch=629
06/24/2022 11:32:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.60 on epoch=634
06/24/2022 11:32:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.54 on epoch=639
06/24/2022 11:32:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.64 on epoch=644
06/24/2022 11:32:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.60 on epoch=649
06/24/2022 11:32:08 - INFO - __main__ - Global step 1300 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=649
06/24/2022 11:32:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.60 on epoch=654
06/24/2022 11:32:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.53 on epoch=659
06/24/2022 11:32:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.56 on epoch=664
06/24/2022 11:32:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.72 on epoch=669
06/24/2022 11:32:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.60 on epoch=674
06/24/2022 11:32:15 - INFO - __main__ - Global step 1350 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=674
06/24/2022 11:32:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.54 on epoch=679
06/24/2022 11:32:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.54 on epoch=684
06/24/2022 11:32:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.54 on epoch=689
06/24/2022 11:32:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.56 on epoch=694
06/24/2022 11:32:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.61 on epoch=699
06/24/2022 11:32:22 - INFO - __main__ - Global step 1400 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=699
06/24/2022 11:32:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.61 on epoch=704
06/24/2022 11:32:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.59 on epoch=709
06/24/2022 11:32:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.58 on epoch=714
06/24/2022 11:32:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.55 on epoch=719
06/24/2022 11:32:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.58 on epoch=724
06/24/2022 11:32:28 - INFO - __main__ - Global step 1450 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=724
06/24/2022 11:32:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.56 on epoch=729
06/24/2022 11:32:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.56 on epoch=734
06/24/2022 11:32:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.60 on epoch=739
06/24/2022 11:32:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.49 on epoch=744
06/24/2022 11:32:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.67 on epoch=749
06/24/2022 11:32:35 - INFO - __main__ - Global step 1500 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=749
06/24/2022 11:32:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.50 on epoch=754
06/24/2022 11:32:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.54 on epoch=759
06/24/2022 11:32:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.50 on epoch=764
06/24/2022 11:32:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.53 on epoch=769
06/24/2022 11:32:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.53 on epoch=774
06/24/2022 11:32:42 - INFO - __main__ - Global step 1550 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=774
06/24/2022 11:32:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.56 on epoch=779
06/24/2022 11:32:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.57 on epoch=784
06/24/2022 11:32:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=789
06/24/2022 11:32:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.56 on epoch=794
06/24/2022 11:32:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.52 on epoch=799
06/24/2022 11:32:48 - INFO - __main__ - Global step 1600 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=799
06/24/2022 11:32:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=804
06/24/2022 11:32:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.55 on epoch=809
06/24/2022 11:32:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.58 on epoch=814
06/24/2022 11:32:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.51 on epoch=819
06/24/2022 11:32:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.51 on epoch=824
06/24/2022 11:32:55 - INFO - __main__ - Global step 1650 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=824
06/24/2022 11:32:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.48 on epoch=829
06/24/2022 11:32:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.53 on epoch=834
06/24/2022 11:32:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.48 on epoch=839
06/24/2022 11:33:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.53 on epoch=844
06/24/2022 11:33:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.52 on epoch=849
06/24/2022 11:33:02 - INFO - __main__ - Global step 1700 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=849
06/24/2022 11:33:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.51 on epoch=854
06/24/2022 11:33:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=859
06/24/2022 11:33:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.55 on epoch=864
06/24/2022 11:33:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.52 on epoch=869
06/24/2022 11:33:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.47 on epoch=874
06/24/2022 11:33:09 - INFO - __main__ - Global step 1750 Train loss 0.50 Classification-F1 0.3191489361702127 on epoch=874
06/24/2022 11:33:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.54 on epoch=879
06/24/2022 11:33:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.48 on epoch=884
06/24/2022 11:33:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.56 on epoch=889
06/24/2022 11:33:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=894
06/24/2022 11:33:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.50 on epoch=899
06/24/2022 11:33:15 - INFO - __main__ - Global step 1800 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=899
06/24/2022 11:33:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.61 on epoch=904
06/24/2022 11:33:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.50 on epoch=909
06/24/2022 11:33:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.52 on epoch=914
06/24/2022 11:33:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.50 on epoch=919
06/24/2022 11:33:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=924
06/24/2022 11:33:22 - INFO - __main__ - Global step 1850 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=924
06/24/2022 11:33:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.47 on epoch=929
06/24/2022 11:33:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.48 on epoch=934
06/24/2022 11:33:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.53 on epoch=939
06/24/2022 11:33:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.47 on epoch=944
06/24/2022 11:33:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.46 on epoch=949
06/24/2022 11:33:29 - INFO - __main__ - Global step 1900 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=949
06/24/2022 11:33:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.52 on epoch=954
06/24/2022 11:33:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=959
06/24/2022 11:33:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.51 on epoch=964
06/24/2022 11:33:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.49 on epoch=969
06/24/2022 11:33:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=974
06/24/2022 11:33:36 - INFO - __main__ - Global step 1950 Train loss 0.47 Classification-F1 0.3992490613266583 on epoch=974
06/24/2022 11:33:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.52 on epoch=979
06/24/2022 11:33:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=984
06/24/2022 11:33:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.51 on epoch=989
06/24/2022 11:33:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.54 on epoch=994
06/24/2022 11:33:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.57 on epoch=999
06/24/2022 11:33:42 - INFO - __main__ - Global step 2000 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=999
06/24/2022 11:33:42 - INFO - __main__ - save last model!
06/24/2022 11:33:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 11:33:42 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 11:33:42 - INFO - __main__ - Printing 3 examples
06/24/2022 11:33:42 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 11:33:42 - INFO - __main__ - ['0']
06/24/2022 11:33:42 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 11:33:42 - INFO - __main__ - ['1']
06/24/2022 11:33:42 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 11:33:42 - INFO - __main__ - ['1']
06/24/2022 11:33:42 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:33:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:33:43 - INFO - __main__ - Printing 3 examples
06/24/2022 11:33:43 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 11:33:43 - INFO - __main__ - ['1']
06/24/2022 11:33:43 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 11:33:43 - INFO - __main__ - ['1']
06/24/2022 11:33:43 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 11:33:43 - INFO - __main__ - ['1']
06/24/2022 11:33:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 11:33:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:33:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:33:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:33:43 - INFO - __main__ - Printing 3 examples
06/24/2022 11:33:43 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 11:33:43 - INFO - __main__ - ['1']
06/24/2022 11:33:43 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 11:33:43 - INFO - __main__ - ['1']
06/24/2022 11:33:43 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 11:33:43 - INFO - __main__ - ['1']
06/24/2022 11:33:43 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:33:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:33:43 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:33:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:33:49 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:33:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:33:50 - INFO - __main__ - Starting training!
06/24/2022 11:33:54 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 11:35:18 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_21_0.5_8_predictions.txt
06/24/2022 11:35:18 - INFO - __main__ - Classification-F1 on test data: 0.3073
06/24/2022 11:35:18 - INFO - __main__ - prefix=paws_16_21, lr=0.5, bsz=8, dev_performance=0.5844155844155844, test_performance=0.3073303166999166
06/24/2022 11:35:18 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.4, bsz=8 ...
06/24/2022 11:35:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:35:19 - INFO - __main__ - Printing 3 examples
06/24/2022 11:35:19 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 11:35:19 - INFO - __main__ - ['1']
06/24/2022 11:35:19 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 11:35:19 - INFO - __main__ - ['1']
06/24/2022 11:35:19 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 11:35:19 - INFO - __main__ - ['1']
06/24/2022 11:35:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:35:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:35:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:35:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:35:19 - INFO - __main__ - Printing 3 examples
06/24/2022 11:35:19 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 11:35:19 - INFO - __main__ - ['1']
06/24/2022 11:35:19 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 11:35:19 - INFO - __main__ - ['1']
06/24/2022 11:35:19 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 11:35:19 - INFO - __main__ - ['1']
06/24/2022 11:35:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:35:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:35:19 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:35:25 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:35:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:35:26 - INFO - __main__ - Starting training!
06/24/2022 11:35:27 - INFO - __main__ - Step 10 Global step 10 Train loss 5.98 on epoch=4
06/24/2022 11:35:28 - INFO - __main__ - Step 20 Global step 20 Train loss 5.99 on epoch=9
06/24/2022 11:35:30 - INFO - __main__ - Step 30 Global step 30 Train loss 5.96 on epoch=14
06/24/2022 11:35:31 - INFO - __main__ - Step 40 Global step 40 Train loss 5.93 on epoch=19
06/24/2022 11:35:32 - INFO - __main__ - Step 50 Global step 50 Train loss 5.92 on epoch=24
06/24/2022 11:35:34 - INFO - __main__ - Global step 50 Train loss 5.96 Classification-F1 0.0 on epoch=24
06/24/2022 11:35:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 11:35:35 - INFO - __main__ - Step 60 Global step 60 Train loss 5.94 on epoch=29
06/24/2022 11:35:36 - INFO - __main__ - Step 70 Global step 70 Train loss 5.88 on epoch=34
06/24/2022 11:35:37 - INFO - __main__ - Step 80 Global step 80 Train loss 5.87 on epoch=39
06/24/2022 11:35:39 - INFO - __main__ - Step 90 Global step 90 Train loss 5.82 on epoch=44
06/24/2022 11:35:40 - INFO - __main__ - Step 100 Global step 100 Train loss 5.77 on epoch=49
06/24/2022 11:35:42 - INFO - __main__ - Global step 100 Train loss 5.86 Classification-F1 0.0 on epoch=49
06/24/2022 11:35:43 - INFO - __main__ - Step 110 Global step 110 Train loss 5.73 on epoch=54
06/24/2022 11:35:44 - INFO - __main__ - Step 120 Global step 120 Train loss 5.83 on epoch=59
06/24/2022 11:35:45 - INFO - __main__ - Step 130 Global step 130 Train loss 5.78 on epoch=64
06/24/2022 11:35:47 - INFO - __main__ - Step 140 Global step 140 Train loss 5.73 on epoch=69
06/24/2022 11:35:48 - INFO - __main__ - Step 150 Global step 150 Train loss 5.79 on epoch=74
06/24/2022 11:35:58 - INFO - __main__ - Global step 150 Train loss 5.77 Classification-F1 0.0 on epoch=74
06/24/2022 11:35:59 - INFO - __main__ - Step 160 Global step 160 Train loss 5.82 on epoch=79
06/24/2022 11:36:01 - INFO - __main__ - Step 170 Global step 170 Train loss 5.72 on epoch=84
06/24/2022 11:36:02 - INFO - __main__ - Step 180 Global step 180 Train loss 5.66 on epoch=89
06/24/2022 11:36:03 - INFO - __main__ - Step 190 Global step 190 Train loss 5.61 on epoch=94
06/24/2022 11:36:05 - INFO - __main__ - Step 200 Global step 200 Train loss 5.64 on epoch=99
06/24/2022 11:36:15 - INFO - __main__ - Global step 200 Train loss 5.69 Classification-F1 0.0 on epoch=99
06/24/2022 11:36:16 - INFO - __main__ - Step 210 Global step 210 Train loss 5.40 on epoch=104
06/24/2022 11:36:17 - INFO - __main__ - Step 220 Global step 220 Train loss 5.48 on epoch=109
06/24/2022 11:36:18 - INFO - __main__ - Step 230 Global step 230 Train loss 5.37 on epoch=114
06/24/2022 11:36:20 - INFO - __main__ - Step 240 Global step 240 Train loss 4.99 on epoch=119
06/24/2022 11:36:21 - INFO - __main__ - Step 250 Global step 250 Train loss 4.70 on epoch=124
06/24/2022 11:36:29 - INFO - __main__ - Global step 250 Train loss 5.19 Classification-F1 0.0 on epoch=124
06/24/2022 11:36:30 - INFO - __main__ - Step 260 Global step 260 Train loss 4.72 on epoch=129
06/24/2022 11:36:32 - INFO - __main__ - Step 270 Global step 270 Train loss 4.48 on epoch=134
06/24/2022 11:36:33 - INFO - __main__ - Step 280 Global step 280 Train loss 4.32 on epoch=139
06/24/2022 11:36:34 - INFO - __main__ - Step 290 Global step 290 Train loss 4.20 on epoch=144
06/24/2022 11:36:35 - INFO - __main__ - Step 300 Global step 300 Train loss 4.28 on epoch=149
06/24/2022 11:36:41 - INFO - __main__ - Global step 300 Train loss 4.40 Classification-F1 0.0 on epoch=149
06/24/2022 11:36:43 - INFO - __main__ - Step 310 Global step 310 Train loss 3.99 on epoch=154
06/24/2022 11:36:44 - INFO - __main__ - Step 320 Global step 320 Train loss 3.96 on epoch=159
06/24/2022 11:36:45 - INFO - __main__ - Step 330 Global step 330 Train loss 4.08 on epoch=164
06/24/2022 11:36:47 - INFO - __main__ - Step 340 Global step 340 Train loss 3.79 on epoch=169
06/24/2022 11:36:48 - INFO - __main__ - Step 350 Global step 350 Train loss 3.78 on epoch=174
06/24/2022 11:36:58 - INFO - __main__ - Global step 350 Train loss 3.92 Classification-F1 0.0 on epoch=174
06/24/2022 11:36:59 - INFO - __main__ - Step 360 Global step 360 Train loss 3.68 on epoch=179
06/24/2022 11:37:01 - INFO - __main__ - Step 370 Global step 370 Train loss 3.61 on epoch=184
06/24/2022 11:37:02 - INFO - __main__ - Step 380 Global step 380 Train loss 3.56 on epoch=189
06/24/2022 11:37:03 - INFO - __main__ - Step 390 Global step 390 Train loss 3.60 on epoch=194
06/24/2022 11:37:05 - INFO - __main__ - Step 400 Global step 400 Train loss 3.35 on epoch=199
06/24/2022 11:37:15 - INFO - __main__ - Global step 400 Train loss 3.56 Classification-F1 0.0 on epoch=199
06/24/2022 11:37:16 - INFO - __main__ - Step 410 Global step 410 Train loss 3.41 on epoch=204
06/24/2022 11:37:18 - INFO - __main__ - Step 420 Global step 420 Train loss 3.28 on epoch=209
06/24/2022 11:37:19 - INFO - __main__ - Step 430 Global step 430 Train loss 3.18 on epoch=214
06/24/2022 11:37:20 - INFO - __main__ - Step 440 Global step 440 Train loss 3.26 on epoch=219
06/24/2022 11:37:22 - INFO - __main__ - Step 450 Global step 450 Train loss 3.02 on epoch=224
06/24/2022 11:37:24 - INFO - __main__ - Global step 450 Train loss 3.23 Classification-F1 0.017543859649122806 on epoch=224
06/24/2022 11:37:24 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.017543859649122806 on epoch=224, global_step=450
06/24/2022 11:37:25 - INFO - __main__ - Step 460 Global step 460 Train loss 3.13 on epoch=229
06/24/2022 11:37:26 - INFO - __main__ - Step 470 Global step 470 Train loss 3.05 on epoch=234
06/24/2022 11:37:27 - INFO - __main__ - Step 480 Global step 480 Train loss 2.90 on epoch=239
06/24/2022 11:37:29 - INFO - __main__ - Step 490 Global step 490 Train loss 2.97 on epoch=244
06/24/2022 11:37:30 - INFO - __main__ - Step 500 Global step 500 Train loss 2.87 on epoch=249
06/24/2022 11:37:32 - INFO - __main__ - Global step 500 Train loss 2.98 Classification-F1 0.09677419354838711 on epoch=249
06/24/2022 11:37:32 - INFO - __main__ - Saving model with best Classification-F1: 0.017543859649122806 -> 0.09677419354838711 on epoch=249, global_step=500
06/24/2022 11:37:33 - INFO - __main__ - Step 510 Global step 510 Train loss 2.81 on epoch=254
06/24/2022 11:37:35 - INFO - __main__ - Step 520 Global step 520 Train loss 2.77 on epoch=259
06/24/2022 11:37:36 - INFO - __main__ - Step 530 Global step 530 Train loss 2.76 on epoch=264
06/24/2022 11:37:37 - INFO - __main__ - Step 540 Global step 540 Train loss 2.71 on epoch=269
06/24/2022 11:37:38 - INFO - __main__ - Step 550 Global step 550 Train loss 2.68 on epoch=274
06/24/2022 11:37:40 - INFO - __main__ - Global step 550 Train loss 2.74 Classification-F1 0.23188405797101452 on epoch=274
06/24/2022 11:37:40 - INFO - __main__ - Saving model with best Classification-F1: 0.09677419354838711 -> 0.23188405797101452 on epoch=274, global_step=550
06/24/2022 11:37:42 - INFO - __main__ - Step 560 Global step 560 Train loss 2.74 on epoch=279
06/24/2022 11:37:43 - INFO - __main__ - Step 570 Global step 570 Train loss 2.53 on epoch=284
06/24/2022 11:37:44 - INFO - __main__ - Step 580 Global step 580 Train loss 2.48 on epoch=289
06/24/2022 11:37:46 - INFO - __main__ - Step 590 Global step 590 Train loss 2.39 on epoch=294
06/24/2022 11:37:47 - INFO - __main__ - Step 600 Global step 600 Train loss 2.42 on epoch=299
06/24/2022 11:37:48 - INFO - __main__ - Global step 600 Train loss 2.51 Classification-F1 0.3333333333333333 on epoch=299
06/24/2022 11:37:48 - INFO - __main__ - Saving model with best Classification-F1: 0.23188405797101452 -> 0.3333333333333333 on epoch=299, global_step=600
06/24/2022 11:37:50 - INFO - __main__ - Step 610 Global step 610 Train loss 2.36 on epoch=304
06/24/2022 11:37:51 - INFO - __main__ - Step 620 Global step 620 Train loss 2.32 on epoch=309
06/24/2022 11:37:52 - INFO - __main__ - Step 630 Global step 630 Train loss 2.25 on epoch=314
06/24/2022 11:37:54 - INFO - __main__ - Step 640 Global step 640 Train loss 2.16 on epoch=319
06/24/2022 11:37:55 - INFO - __main__ - Step 650 Global step 650 Train loss 2.14 on epoch=324
06/24/2022 11:37:57 - INFO - __main__ - Global step 650 Train loss 2.25 Classification-F1 0.3333333333333333 on epoch=324
06/24/2022 11:37:58 - INFO - __main__ - Step 660 Global step 660 Train loss 2.07 on epoch=329
06/24/2022 11:37:59 - INFO - __main__ - Step 670 Global step 670 Train loss 2.07 on epoch=334
06/24/2022 11:38:01 - INFO - __main__ - Step 680 Global step 680 Train loss 2.07 on epoch=339
06/24/2022 11:38:02 - INFO - __main__ - Step 690 Global step 690 Train loss 2.05 on epoch=344
06/24/2022 11:38:03 - INFO - __main__ - Step 700 Global step 700 Train loss 1.84 on epoch=349
06/24/2022 11:38:04 - INFO - __main__ - Global step 700 Train loss 2.02 Classification-F1 0.3333333333333333 on epoch=349
06/24/2022 11:38:05 - INFO - __main__ - Step 710 Global step 710 Train loss 2.02 on epoch=354
06/24/2022 11:38:06 - INFO - __main__ - Step 720 Global step 720 Train loss 2.03 on epoch=359
06/24/2022 11:38:08 - INFO - __main__ - Step 730 Global step 730 Train loss 2.03 on epoch=364
06/24/2022 11:38:09 - INFO - __main__ - Step 740 Global step 740 Train loss 1.94 on epoch=369
06/24/2022 11:38:10 - INFO - __main__ - Step 750 Global step 750 Train loss 1.91 on epoch=374
06/24/2022 11:38:11 - INFO - __main__ - Global step 750 Train loss 1.98 Classification-F1 0.3992490613266583 on epoch=374
06/24/2022 11:38:11 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=374, global_step=750
06/24/2022 11:38:12 - INFO - __main__ - Step 760 Global step 760 Train loss 1.81 on epoch=379
06/24/2022 11:38:14 - INFO - __main__ - Step 770 Global step 770 Train loss 1.78 on epoch=384
06/24/2022 11:38:15 - INFO - __main__ - Step 780 Global step 780 Train loss 1.65 on epoch=389
06/24/2022 11:38:16 - INFO - __main__ - Step 790 Global step 790 Train loss 1.70 on epoch=394
06/24/2022 11:38:17 - INFO - __main__ - Step 800 Global step 800 Train loss 1.68 on epoch=399
06/24/2022 11:38:19 - INFO - __main__ - Global step 800 Train loss 1.72 Classification-F1 0.3333333333333333 on epoch=399
06/24/2022 11:38:20 - INFO - __main__ - Step 810 Global step 810 Train loss 1.55 on epoch=404
06/24/2022 11:38:21 - INFO - __main__ - Step 820 Global step 820 Train loss 1.61 on epoch=409
06/24/2022 11:38:23 - INFO - __main__ - Step 830 Global step 830 Train loss 1.58 on epoch=414
06/24/2022 11:38:24 - INFO - __main__ - Step 840 Global step 840 Train loss 1.69 on epoch=419
06/24/2022 11:38:25 - INFO - __main__ - Step 850 Global step 850 Train loss 1.50 on epoch=424
06/24/2022 11:38:26 - INFO - __main__ - Global step 850 Train loss 1.59 Classification-F1 0.4385964912280702 on epoch=424
06/24/2022 11:38:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4385964912280702 on epoch=424, global_step=850
06/24/2022 11:38:27 - INFO - __main__ - Step 860 Global step 860 Train loss 1.50 on epoch=429
06/24/2022 11:38:28 - INFO - __main__ - Step 870 Global step 870 Train loss 1.54 on epoch=434
06/24/2022 11:38:30 - INFO - __main__ - Step 880 Global step 880 Train loss 1.34 on epoch=439
06/24/2022 11:38:31 - INFO - __main__ - Step 890 Global step 890 Train loss 1.44 on epoch=444
06/24/2022 11:38:32 - INFO - __main__ - Step 900 Global step 900 Train loss 1.40 on epoch=449
06/24/2022 11:38:33 - INFO - __main__ - Global step 900 Train loss 1.44 Classification-F1 0.5555555555555556 on epoch=449
06/24/2022 11:38:33 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.5555555555555556 on epoch=449, global_step=900
06/24/2022 11:38:34 - INFO - __main__ - Step 910 Global step 910 Train loss 1.47 on epoch=454
06/24/2022 11:38:35 - INFO - __main__ - Step 920 Global step 920 Train loss 1.45 on epoch=459
06/24/2022 11:38:36 - INFO - __main__ - Step 930 Global step 930 Train loss 1.48 on epoch=464
06/24/2022 11:38:38 - INFO - __main__ - Step 940 Global step 940 Train loss 1.41 on epoch=469
06/24/2022 11:38:39 - INFO - __main__ - Step 950 Global step 950 Train loss 1.29 on epoch=474
06/24/2022 11:38:39 - INFO - __main__ - Global step 950 Train loss 1.42 Classification-F1 0.5 on epoch=474
06/24/2022 11:38:41 - INFO - __main__ - Step 960 Global step 960 Train loss 1.32 on epoch=479
06/24/2022 11:38:42 - INFO - __main__ - Step 970 Global step 970 Train loss 1.28 on epoch=484
06/24/2022 11:38:43 - INFO - __main__ - Step 980 Global step 980 Train loss 1.27 on epoch=489
06/24/2022 11:38:44 - INFO - __main__ - Step 990 Global step 990 Train loss 1.24 on epoch=494
06/24/2022 11:38:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.15 on epoch=499
06/24/2022 11:38:46 - INFO - __main__ - Global step 1000 Train loss 1.25 Classification-F1 0.39139139139139134 on epoch=499
06/24/2022 11:38:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.16 on epoch=504
06/24/2022 11:38:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.13 on epoch=509
06/24/2022 11:38:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.02 on epoch=514
06/24/2022 11:38:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.07 on epoch=519
06/24/2022 11:38:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.09 on epoch=524
06/24/2022 11:38:53 - INFO - __main__ - Global step 1050 Train loss 1.09 Classification-F1 0.3073593073593074 on epoch=524
06/24/2022 11:38:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.09 on epoch=529
06/24/2022 11:38:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.07 on epoch=534
06/24/2022 11:38:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.04 on epoch=539
06/24/2022 11:38:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.09 on epoch=544
06/24/2022 11:38:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.97 on epoch=549
06/24/2022 11:38:59 - INFO - __main__ - Global step 1100 Train loss 1.05 Classification-F1 0.4589371980676329 on epoch=549
06/24/2022 11:39:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.02 on epoch=554
06/24/2022 11:39:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.89 on epoch=559
06/24/2022 11:39:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.00 on epoch=564
06/24/2022 11:39:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.88 on epoch=569
06/24/2022 11:39:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.03 on epoch=574
06/24/2022 11:39:06 - INFO - __main__ - Global step 1150 Train loss 0.97 Classification-F1 0.4909862142099682 on epoch=574
06/24/2022 11:39:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.91 on epoch=579
06/24/2022 11:39:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.90 on epoch=584
06/24/2022 11:39:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.85 on epoch=589
06/24/2022 11:39:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.85 on epoch=594
06/24/2022 11:39:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.98 on epoch=599
06/24/2022 11:39:13 - INFO - __main__ - Global step 1200 Train loss 0.90 Classification-F1 0.3992490613266583 on epoch=599
06/24/2022 11:39:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.89 on epoch=604
06/24/2022 11:39:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=609
06/24/2022 11:39:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.81 on epoch=614
06/24/2022 11:39:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=619
06/24/2022 11:39:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.77 on epoch=624
06/24/2022 11:39:20 - INFO - __main__ - Global step 1250 Train loss 0.81 Classification-F1 0.3992490613266583 on epoch=624
06/24/2022 11:39:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.84 on epoch=629
06/24/2022 11:39:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.85 on epoch=634
06/24/2022 11:39:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.70 on epoch=639
06/24/2022 11:39:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.88 on epoch=644
06/24/2022 11:39:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.63 on epoch=649
06/24/2022 11:39:27 - INFO - __main__ - Global step 1300 Train loss 0.78 Classification-F1 0.46843853820598 on epoch=649
06/24/2022 11:39:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.80 on epoch=654
06/24/2022 11:39:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.82 on epoch=659
06/24/2022 11:39:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.79 on epoch=664
06/24/2022 11:39:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.78 on epoch=669
06/24/2022 11:39:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.70 on epoch=674
06/24/2022 11:39:33 - INFO - __main__ - Global step 1350 Train loss 0.78 Classification-F1 0.3992490613266583 on epoch=674
06/24/2022 11:39:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.76 on epoch=679
06/24/2022 11:39:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.65 on epoch=684
06/24/2022 11:39:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.81 on epoch=689
06/24/2022 11:39:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.68 on epoch=694
06/24/2022 11:39:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.86 on epoch=699
06/24/2022 11:39:40 - INFO - __main__ - Global step 1400 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=699
06/24/2022 11:39:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.78 on epoch=704
06/24/2022 11:39:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.73 on epoch=709
06/24/2022 11:39:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.81 on epoch=714
06/24/2022 11:39:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.70 on epoch=719
06/24/2022 11:39:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.74 on epoch=724
06/24/2022 11:39:47 - INFO - __main__ - Global step 1450 Train loss 0.75 Classification-F1 0.4385964912280702 on epoch=724
06/24/2022 11:39:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.68 on epoch=729
06/24/2022 11:39:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.70 on epoch=734
06/24/2022 11:39:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.71 on epoch=739
06/24/2022 11:39:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.64 on epoch=744
06/24/2022 11:39:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.70 on epoch=749
06/24/2022 11:39:53 - INFO - __main__ - Global step 1500 Train loss 0.69 Classification-F1 0.3191489361702127 on epoch=749
06/24/2022 11:39:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.68 on epoch=754
06/24/2022 11:39:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.77 on epoch=759
06/24/2022 11:39:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.69 on epoch=764
06/24/2022 11:39:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.84 on epoch=769
06/24/2022 11:40:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.75 on epoch=774
06/24/2022 11:40:00 - INFO - __main__ - Global step 1550 Train loss 0.75 Classification-F1 0.3191489361702127 on epoch=774
06/24/2022 11:40:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.71 on epoch=779
06/24/2022 11:40:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.71 on epoch=784
06/24/2022 11:40:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.72 on epoch=789
06/24/2022 11:40:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.65 on epoch=794
06/24/2022 11:40:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.66 on epoch=799
06/24/2022 11:40:07 - INFO - __main__ - Global step 1600 Train loss 0.69 Classification-F1 0.3454545454545454 on epoch=799
06/24/2022 11:40:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.77 on epoch=804
06/24/2022 11:40:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.62 on epoch=809
06/24/2022 11:40:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.71 on epoch=814
06/24/2022 11:40:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.57 on epoch=819
06/24/2022 11:40:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.64 on epoch=824
06/24/2022 11:40:14 - INFO - __main__ - Global step 1650 Train loss 0.66 Classification-F1 0.4385964912280702 on epoch=824
06/24/2022 11:40:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.69 on epoch=829
06/24/2022 11:40:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.65 on epoch=834
06/24/2022 11:40:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.58 on epoch=839
06/24/2022 11:40:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.61 on epoch=844
06/24/2022 11:40:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.60 on epoch=849
06/24/2022 11:40:20 - INFO - __main__ - Global step 1700 Train loss 0.63 Classification-F1 0.4181818181818182 on epoch=849
06/24/2022 11:40:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.75 on epoch=854
06/24/2022 11:40:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.55 on epoch=859
06/24/2022 11:40:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.59 on epoch=864
06/24/2022 11:40:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.62 on epoch=869
06/24/2022 11:40:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.63 on epoch=874
06/24/2022 11:40:27 - INFO - __main__ - Global step 1750 Train loss 0.63 Classification-F1 0.36374269005847953 on epoch=874
06/24/2022 11:40:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.65 on epoch=879
06/24/2022 11:40:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.63 on epoch=884
06/24/2022 11:40:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.60 on epoch=889
06/24/2022 11:40:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.63 on epoch=894
06/24/2022 11:40:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.60 on epoch=899
06/24/2022 11:40:34 - INFO - __main__ - Global step 1800 Train loss 0.62 Classification-F1 0.36374269005847953 on epoch=899
06/24/2022 11:40:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.51 on epoch=904
06/24/2022 11:40:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.69 on epoch=909
06/24/2022 11:40:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.63 on epoch=914
06/24/2022 11:40:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.61 on epoch=919
06/24/2022 11:40:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.61 on epoch=924
06/24/2022 11:40:40 - INFO - __main__ - Global step 1850 Train loss 0.61 Classification-F1 0.3992490613266583 on epoch=924
06/24/2022 11:40:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.55 on epoch=929
06/24/2022 11:40:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.54 on epoch=934
06/24/2022 11:40:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.63 on epoch=939
06/24/2022 11:40:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.59 on epoch=944
06/24/2022 11:40:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.58 on epoch=949
06/24/2022 11:40:47 - INFO - __main__ - Global step 1900 Train loss 0.58 Classification-F1 0.4181818181818182 on epoch=949
06/24/2022 11:40:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.57 on epoch=954
06/24/2022 11:40:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.56 on epoch=959
06/24/2022 11:40:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.68 on epoch=964
06/24/2022 11:40:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.61 on epoch=969
06/24/2022 11:40:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.54 on epoch=974
06/24/2022 11:40:54 - INFO - __main__ - Global step 1950 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=974
06/24/2022 11:40:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.54 on epoch=979
06/24/2022 11:40:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.59 on epoch=984
06/24/2022 11:40:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.61 on epoch=989
06/24/2022 11:40:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.53 on epoch=994
06/24/2022 11:41:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.53 on epoch=999
06/24/2022 11:41:01 - INFO - __main__ - Global step 2000 Train loss 0.56 Classification-F1 0.49090909090909085 on epoch=999
06/24/2022 11:41:01 - INFO - __main__ - save last model!
06/24/2022 11:41:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 11:41:01 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 11:41:01 - INFO - __main__ - Printing 3 examples
06/24/2022 11:41:01 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 11:41:01 - INFO - __main__ - ['0']
06/24/2022 11:41:01 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 11:41:01 - INFO - __main__ - ['1']
06/24/2022 11:41:01 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 11:41:01 - INFO - __main__ - ['1']
06/24/2022 11:41:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:41:01 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:41:01 - INFO - __main__ - Printing 3 examples
06/24/2022 11:41:01 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 11:41:01 - INFO - __main__ - ['1']
06/24/2022 11:41:01 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 11:41:01 - INFO - __main__ - ['1']
06/24/2022 11:41:01 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 11:41:01 - INFO - __main__ - ['1']
06/24/2022 11:41:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 11:41:01 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:41:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:41:01 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:41:01 - INFO - __main__ - Printing 3 examples
06/24/2022 11:41:01 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 11:41:01 - INFO - __main__ - ['1']
06/24/2022 11:41:01 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 11:41:01 - INFO - __main__ - ['1']
06/24/2022 11:41:01 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 11:41:01 - INFO - __main__ - ['1']
06/24/2022 11:41:01 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:41:01 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:41:01 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:41:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:41:07 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:41:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:41:08 - INFO - __main__ - Starting training!
06/24/2022 11:41:13 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 11:42:38 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_21_0.4_8_predictions.txt
06/24/2022 11:42:38 - INFO - __main__ - Classification-F1 on test data: 0.4010
06/24/2022 11:42:38 - INFO - __main__ - prefix=paws_16_21, lr=0.4, bsz=8, dev_performance=0.5555555555555556, test_performance=0.4010175567509586
06/24/2022 11:42:38 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.3, bsz=8 ...
06/24/2022 11:42:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:42:39 - INFO - __main__ - Printing 3 examples
06/24/2022 11:42:39 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 11:42:39 - INFO - __main__ - ['1']
06/24/2022 11:42:39 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 11:42:39 - INFO - __main__ - ['1']
06/24/2022 11:42:39 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 11:42:39 - INFO - __main__ - ['1']
06/24/2022 11:42:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:42:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:42:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:42:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:42:39 - INFO - __main__ - Printing 3 examples
06/24/2022 11:42:39 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 11:42:39 - INFO - __main__ - ['1']
06/24/2022 11:42:39 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 11:42:39 - INFO - __main__ - ['1']
06/24/2022 11:42:39 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 11:42:39 - INFO - __main__ - ['1']
06/24/2022 11:42:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:42:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:42:39 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:42:45 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:42:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:42:45 - INFO - __main__ - Starting training!
06/24/2022 11:42:46 - INFO - __main__ - Step 10 Global step 10 Train loss 5.92 on epoch=4
06/24/2022 11:42:48 - INFO - __main__ - Step 20 Global step 20 Train loss 5.86 on epoch=9
06/24/2022 11:42:49 - INFO - __main__ - Step 30 Global step 30 Train loss 5.94 on epoch=14
06/24/2022 11:42:50 - INFO - __main__ - Step 40 Global step 40 Train loss 5.93 on epoch=19
06/24/2022 11:42:51 - INFO - __main__ - Step 50 Global step 50 Train loss 6.00 on epoch=24
06/24/2022 11:42:53 - INFO - __main__ - Global step 50 Train loss 5.93 Classification-F1 0.0 on epoch=24
06/24/2022 11:42:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 11:42:55 - INFO - __main__ - Step 60 Global step 60 Train loss 5.89 on epoch=29
06/24/2022 11:42:56 - INFO - __main__ - Step 70 Global step 70 Train loss 5.86 on epoch=34
06/24/2022 11:42:57 - INFO - __main__ - Step 80 Global step 80 Train loss 5.82 on epoch=39
06/24/2022 11:42:58 - INFO - __main__ - Step 90 Global step 90 Train loss 5.83 on epoch=44
06/24/2022 11:43:00 - INFO - __main__ - Step 100 Global step 100 Train loss 5.72 on epoch=49
06/24/2022 11:43:01 - INFO - __main__ - Global step 100 Train loss 5.82 Classification-F1 0.0 on epoch=49
06/24/2022 11:43:03 - INFO - __main__ - Step 110 Global step 110 Train loss 5.75 on epoch=54
06/24/2022 11:43:04 - INFO - __main__ - Step 120 Global step 120 Train loss 5.69 on epoch=59
06/24/2022 11:43:05 - INFO - __main__ - Step 130 Global step 130 Train loss 5.62 on epoch=64
06/24/2022 11:43:06 - INFO - __main__ - Step 140 Global step 140 Train loss 5.70 on epoch=69
06/24/2022 11:43:07 - INFO - __main__ - Step 150 Global step 150 Train loss 5.66 on epoch=74
06/24/2022 11:43:08 - INFO - __main__ - Global step 150 Train loss 5.69 Classification-F1 0.0 on epoch=74
06/24/2022 11:43:10 - INFO - __main__ - Step 160 Global step 160 Train loss 5.67 on epoch=79
06/24/2022 11:43:11 - INFO - __main__ - Step 170 Global step 170 Train loss 5.59 on epoch=84
06/24/2022 11:43:12 - INFO - __main__ - Step 180 Global step 180 Train loss 5.51 on epoch=89
06/24/2022 11:43:13 - INFO - __main__ - Step 190 Global step 190 Train loss 5.50 on epoch=94
06/24/2022 11:43:15 - INFO - __main__ - Step 200 Global step 200 Train loss 5.48 on epoch=99
06/24/2022 11:43:20 - INFO - __main__ - Global step 200 Train loss 5.55 Classification-F1 0.0 on epoch=99
06/24/2022 11:43:22 - INFO - __main__ - Step 210 Global step 210 Train loss 5.48 on epoch=104
06/24/2022 11:43:23 - INFO - __main__ - Step 220 Global step 220 Train loss 5.42 on epoch=109
06/24/2022 11:43:24 - INFO - __main__ - Step 230 Global step 230 Train loss 5.41 on epoch=114
06/24/2022 11:43:25 - INFO - __main__ - Step 240 Global step 240 Train loss 5.39 on epoch=119
06/24/2022 11:43:27 - INFO - __main__ - Step 250 Global step 250 Train loss 5.36 on epoch=124
06/24/2022 11:43:32 - INFO - __main__ - Global step 250 Train loss 5.41 Classification-F1 0.0 on epoch=124
06/24/2022 11:43:34 - INFO - __main__ - Step 260 Global step 260 Train loss 5.39 on epoch=129
06/24/2022 11:43:35 - INFO - __main__ - Step 270 Global step 270 Train loss 5.29 on epoch=134
06/24/2022 11:43:36 - INFO - __main__ - Step 280 Global step 280 Train loss 5.19 on epoch=139
06/24/2022 11:43:37 - INFO - __main__ - Step 290 Global step 290 Train loss 5.09 on epoch=144
06/24/2022 11:43:39 - INFO - __main__ - Step 300 Global step 300 Train loss 5.00 on epoch=149
06/24/2022 11:43:50 - INFO - __main__ - Global step 300 Train loss 5.19 Classification-F1 0.0 on epoch=149
06/24/2022 11:43:51 - INFO - __main__ - Step 310 Global step 310 Train loss 5.16 on epoch=154
06/24/2022 11:43:52 - INFO - __main__ - Step 320 Global step 320 Train loss 4.98 on epoch=159
06/24/2022 11:43:53 - INFO - __main__ - Step 330 Global step 330 Train loss 4.92 on epoch=164
06/24/2022 11:43:55 - INFO - __main__ - Step 340 Global step 340 Train loss 4.86 on epoch=169
06/24/2022 11:43:56 - INFO - __main__ - Step 350 Global step 350 Train loss 4.81 on epoch=174
06/24/2022 11:44:08 - INFO - __main__ - Global step 350 Train loss 4.94 Classification-F1 0.0 on epoch=174
06/24/2022 11:44:09 - INFO - __main__ - Step 360 Global step 360 Train loss 4.69 on epoch=179
06/24/2022 11:44:10 - INFO - __main__ - Step 370 Global step 370 Train loss 4.65 on epoch=184
06/24/2022 11:44:11 - INFO - __main__ - Step 380 Global step 380 Train loss 4.55 on epoch=189
06/24/2022 11:44:13 - INFO - __main__ - Step 390 Global step 390 Train loss 4.48 on epoch=194
06/24/2022 11:44:14 - INFO - __main__ - Step 400 Global step 400 Train loss 4.36 on epoch=199
06/24/2022 11:44:26 - INFO - __main__ - Global step 400 Train loss 4.54 Classification-F1 0.0 on epoch=199
06/24/2022 11:44:27 - INFO - __main__ - Step 410 Global step 410 Train loss 4.31 on epoch=204
06/24/2022 11:44:28 - INFO - __main__ - Step 420 Global step 420 Train loss 4.27 on epoch=209
06/24/2022 11:44:29 - INFO - __main__ - Step 430 Global step 430 Train loss 4.20 on epoch=214
06/24/2022 11:44:31 - INFO - __main__ - Step 440 Global step 440 Train loss 4.04 on epoch=219
06/24/2022 11:44:32 - INFO - __main__ - Step 450 Global step 450 Train loss 4.17 on epoch=224
06/24/2022 11:44:44 - INFO - __main__ - Global step 450 Train loss 4.20 Classification-F1 0.0 on epoch=224
06/24/2022 11:44:45 - INFO - __main__ - Step 460 Global step 460 Train loss 4.14 on epoch=229
06/24/2022 11:44:47 - INFO - __main__ - Step 470 Global step 470 Train loss 4.03 on epoch=234
06/24/2022 11:44:48 - INFO - __main__ - Step 480 Global step 480 Train loss 4.03 on epoch=239
06/24/2022 11:44:49 - INFO - __main__ - Step 490 Global step 490 Train loss 3.91 on epoch=244
06/24/2022 11:44:50 - INFO - __main__ - Step 500 Global step 500 Train loss 3.98 on epoch=249
06/24/2022 11:45:01 - INFO - __main__ - Global step 500 Train loss 4.02 Classification-F1 0.0 on epoch=249
06/24/2022 11:45:02 - INFO - __main__ - Step 510 Global step 510 Train loss 3.76 on epoch=254
06/24/2022 11:45:04 - INFO - __main__ - Step 520 Global step 520 Train loss 3.75 on epoch=259
06/24/2022 11:45:05 - INFO - __main__ - Step 530 Global step 530 Train loss 3.72 on epoch=264
06/24/2022 11:45:06 - INFO - __main__ - Step 540 Global step 540 Train loss 3.71 on epoch=269
06/24/2022 11:45:07 - INFO - __main__ - Step 550 Global step 550 Train loss 3.70 on epoch=274
06/24/2022 11:45:15 - INFO - __main__ - Global step 550 Train loss 3.73 Classification-F1 0.0 on epoch=274
06/24/2022 11:45:17 - INFO - __main__ - Step 560 Global step 560 Train loss 3.46 on epoch=279
06/24/2022 11:45:18 - INFO - __main__ - Step 570 Global step 570 Train loss 3.71 on epoch=284
06/24/2022 11:45:19 - INFO - __main__ - Step 580 Global step 580 Train loss 3.50 on epoch=289
06/24/2022 11:45:20 - INFO - __main__ - Step 590 Global step 590 Train loss 3.36 on epoch=294
06/24/2022 11:45:22 - INFO - __main__ - Step 600 Global step 600 Train loss 3.50 on epoch=299
06/24/2022 11:45:30 - INFO - __main__ - Global step 600 Train loss 3.51 Classification-F1 0.0 on epoch=299
06/24/2022 11:45:31 - INFO - __main__ - Step 610 Global step 610 Train loss 3.36 on epoch=304
06/24/2022 11:45:32 - INFO - __main__ - Step 620 Global step 620 Train loss 3.34 on epoch=309
06/24/2022 11:45:33 - INFO - __main__ - Step 630 Global step 630 Train loss 3.20 on epoch=314
06/24/2022 11:45:35 - INFO - __main__ - Step 640 Global step 640 Train loss 3.20 on epoch=319
06/24/2022 11:45:36 - INFO - __main__ - Step 650 Global step 650 Train loss 3.19 on epoch=324
06/24/2022 11:45:37 - INFO - __main__ - Global step 650 Train loss 3.26 Classification-F1 0.014285714285714287 on epoch=324
06/24/2022 11:45:38 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.014285714285714287 on epoch=324, global_step=650
06/24/2022 11:45:39 - INFO - __main__ - Step 660 Global step 660 Train loss 3.14 on epoch=329
06/24/2022 11:45:40 - INFO - __main__ - Step 670 Global step 670 Train loss 3.01 on epoch=334
06/24/2022 11:45:41 - INFO - __main__ - Step 680 Global step 680 Train loss 2.99 on epoch=339
06/24/2022 11:45:42 - INFO - __main__ - Step 690 Global step 690 Train loss 2.89 on epoch=344
06/24/2022 11:45:44 - INFO - __main__ - Step 700 Global step 700 Train loss 2.88 on epoch=349
06/24/2022 11:45:45 - INFO - __main__ - Global step 700 Train loss 2.98 Classification-F1 0.2074074074074074 on epoch=349
06/24/2022 11:45:45 - INFO - __main__ - Saving model with best Classification-F1: 0.014285714285714287 -> 0.2074074074074074 on epoch=349, global_step=700
06/24/2022 11:45:46 - INFO - __main__ - Step 710 Global step 710 Train loss 2.95 on epoch=354
06/24/2022 11:45:47 - INFO - __main__ - Step 720 Global step 720 Train loss 2.84 on epoch=359
06/24/2022 11:45:48 - INFO - __main__ - Step 730 Global step 730 Train loss 2.72 on epoch=364
06/24/2022 11:45:50 - INFO - __main__ - Step 740 Global step 740 Train loss 2.65 on epoch=369
06/24/2022 11:45:51 - INFO - __main__ - Step 750 Global step 750 Train loss 2.76 on epoch=374
06/24/2022 11:45:53 - INFO - __main__ - Global step 750 Train loss 2.78 Classification-F1 0.3191489361702127 on epoch=374
06/24/2022 11:45:53 - INFO - __main__ - Saving model with best Classification-F1: 0.2074074074074074 -> 0.3191489361702127 on epoch=374, global_step=750
06/24/2022 11:45:54 - INFO - __main__ - Step 760 Global step 760 Train loss 2.73 on epoch=379
06/24/2022 11:45:55 - INFO - __main__ - Step 770 Global step 770 Train loss 2.47 on epoch=384
06/24/2022 11:45:57 - INFO - __main__ - Step 780 Global step 780 Train loss 2.33 on epoch=389
06/24/2022 11:45:58 - INFO - __main__ - Step 790 Global step 790 Train loss 2.53 on epoch=394
06/24/2022 11:45:59 - INFO - __main__ - Step 800 Global step 800 Train loss 2.54 on epoch=399
06/24/2022 11:46:01 - INFO - __main__ - Global step 800 Train loss 2.52 Classification-F1 0.3333333333333333 on epoch=399
06/24/2022 11:46:01 - INFO - __main__ - Saving model with best Classification-F1: 0.3191489361702127 -> 0.3333333333333333 on epoch=399, global_step=800
06/24/2022 11:46:03 - INFO - __main__ - Step 810 Global step 810 Train loss 2.44 on epoch=404
06/24/2022 11:46:04 - INFO - __main__ - Step 820 Global step 820 Train loss 2.36 on epoch=409
06/24/2022 11:46:05 - INFO - __main__ - Step 830 Global step 830 Train loss 2.42 on epoch=414
06/24/2022 11:46:06 - INFO - __main__ - Step 840 Global step 840 Train loss 2.23 on epoch=419
06/24/2022 11:46:08 - INFO - __main__ - Step 850 Global step 850 Train loss 2.36 on epoch=424
06/24/2022 11:46:09 - INFO - __main__ - Global step 850 Train loss 2.36 Classification-F1 0.46843853820598 on epoch=424
06/24/2022 11:46:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.46843853820598 on epoch=424, global_step=850
06/24/2022 11:46:10 - INFO - __main__ - Step 860 Global step 860 Train loss 2.13 on epoch=429
06/24/2022 11:46:11 - INFO - __main__ - Step 870 Global step 870 Train loss 2.33 on epoch=434
06/24/2022 11:46:13 - INFO - __main__ - Step 880 Global step 880 Train loss 2.18 on epoch=439
06/24/2022 11:46:14 - INFO - __main__ - Step 890 Global step 890 Train loss 2.16 on epoch=444
06/24/2022 11:46:15 - INFO - __main__ - Step 900 Global step 900 Train loss 2.15 on epoch=449
06/24/2022 11:46:16 - INFO - __main__ - Global step 900 Train loss 2.19 Classification-F1 0.5134502923976608 on epoch=449
06/24/2022 11:46:16 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5134502923976608 on epoch=449, global_step=900
06/24/2022 11:46:17 - INFO - __main__ - Step 910 Global step 910 Train loss 2.03 on epoch=454
06/24/2022 11:46:19 - INFO - __main__ - Step 920 Global step 920 Train loss 1.99 on epoch=459
06/24/2022 11:46:20 - INFO - __main__ - Step 930 Global step 930 Train loss 2.01 on epoch=464
06/24/2022 11:46:21 - INFO - __main__ - Step 940 Global step 940 Train loss 1.91 on epoch=469
06/24/2022 11:46:22 - INFO - __main__ - Step 950 Global step 950 Train loss 2.09 on epoch=474
06/24/2022 11:46:23 - INFO - __main__ - Global step 950 Train loss 2.00 Classification-F1 0.3333333333333333 on epoch=474
06/24/2022 11:46:24 - INFO - __main__ - Step 960 Global step 960 Train loss 1.97 on epoch=479
06/24/2022 11:46:25 - INFO - __main__ - Step 970 Global step 970 Train loss 1.79 on epoch=484
06/24/2022 11:46:26 - INFO - __main__ - Step 980 Global step 980 Train loss 2.19 on epoch=489
06/24/2022 11:46:28 - INFO - __main__ - Step 990 Global step 990 Train loss 2.33 on epoch=494
06/24/2022 11:46:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.30 on epoch=499
06/24/2022 11:46:30 - INFO - __main__ - Global step 1000 Train loss 2.12 Classification-F1 0.3333333333333333 on epoch=499
06/24/2022 11:46:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.98 on epoch=504
06/24/2022 11:46:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.72 on epoch=509
06/24/2022 11:46:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.80 on epoch=514
06/24/2022 11:46:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.79 on epoch=519
06/24/2022 11:46:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.70 on epoch=524
06/24/2022 11:46:37 - INFO - __main__ - Global step 1050 Train loss 1.80 Classification-F1 0.36374269005847953 on epoch=524
06/24/2022 11:46:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.69 on epoch=529
06/24/2022 11:46:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.64 on epoch=534
06/24/2022 11:46:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.60 on epoch=539
06/24/2022 11:46:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.63 on epoch=544
06/24/2022 11:46:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.45 on epoch=549
06/24/2022 11:46:43 - INFO - __main__ - Global step 1100 Train loss 1.60 Classification-F1 0.36374269005847953 on epoch=549
06/24/2022 11:46:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.50 on epoch=554
06/24/2022 11:46:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.52 on epoch=559
06/24/2022 11:46:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.54 on epoch=564
06/24/2022 11:46:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.43 on epoch=569
06/24/2022 11:46:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.46 on epoch=574
06/24/2022 11:46:50 - INFO - __main__ - Global step 1150 Train loss 1.49 Classification-F1 0.3043478260869565 on epoch=574
06/24/2022 11:46:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.45 on epoch=579
06/24/2022 11:46:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.51 on epoch=584
06/24/2022 11:46:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.34 on epoch=589
06/24/2022 11:46:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.33 on epoch=594
06/24/2022 11:46:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.53 on epoch=599
06/24/2022 11:46:57 - INFO - __main__ - Global step 1200 Train loss 1.43 Classification-F1 0.3333333333333333 on epoch=599
06/24/2022 11:46:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.39 on epoch=604
06/24/2022 11:46:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.38 on epoch=609
06/24/2022 11:47:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.32 on epoch=614
06/24/2022 11:47:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.32 on epoch=619
06/24/2022 11:47:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.33 on epoch=624
06/24/2022 11:47:03 - INFO - __main__ - Global step 1250 Train loss 1.35 Classification-F1 0.3816425120772947 on epoch=624
06/24/2022 11:47:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.24 on epoch=629
06/24/2022 11:47:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.32 on epoch=634
06/24/2022 11:47:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.23 on epoch=639
06/24/2022 11:47:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.19 on epoch=644
06/24/2022 11:47:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.15 on epoch=649
06/24/2022 11:47:10 - INFO - __main__ - Global step 1300 Train loss 1.22 Classification-F1 0.3191489361702127 on epoch=649
06/24/2022 11:47:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.19 on epoch=654
06/24/2022 11:47:12 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.09 on epoch=659
06/24/2022 11:47:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.25 on epoch=664
06/24/2022 11:47:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.12 on epoch=669
06/24/2022 11:47:16 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.18 on epoch=674
06/24/2022 11:47:17 - INFO - __main__ - Global step 1350 Train loss 1.17 Classification-F1 0.3816425120772947 on epoch=674
06/24/2022 11:47:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.11 on epoch=679
06/24/2022 11:47:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.24 on epoch=684
06/24/2022 11:47:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.11 on epoch=689
06/24/2022 11:47:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.15 on epoch=694
06/24/2022 11:47:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.02 on epoch=699
06/24/2022 11:47:23 - INFO - __main__ - Global step 1400 Train loss 1.13 Classification-F1 0.3992490613266583 on epoch=699
06/24/2022 11:47:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.10 on epoch=704
06/24/2022 11:47:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.04 on epoch=709
06/24/2022 11:47:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.13 on epoch=714
06/24/2022 11:47:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.07 on epoch=719
06/24/2022 11:47:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.08 on epoch=724
06/24/2022 11:47:30 - INFO - __main__ - Global step 1450 Train loss 1.08 Classification-F1 0.4181818181818182 on epoch=724
06/24/2022 11:47:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.08 on epoch=729
06/24/2022 11:47:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.94 on epoch=734
06/24/2022 11:47:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.00 on epoch=739
06/24/2022 11:47:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.09 on epoch=744
06/24/2022 11:47:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.01 on epoch=749
06/24/2022 11:47:36 - INFO - __main__ - Global step 1500 Train loss 1.02 Classification-F1 0.3333333333333333 on epoch=749
06/24/2022 11:47:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.03 on epoch=754
06/24/2022 11:47:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.01 on epoch=759
06/24/2022 11:47:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.98 on epoch=764
06/24/2022 11:47:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.01 on epoch=769
06/24/2022 11:47:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.06 on epoch=774
06/24/2022 11:47:43 - INFO - __main__ - Global step 1550 Train loss 1.02 Classification-F1 0.3333333333333333 on epoch=774
06/24/2022 11:47:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.01 on epoch=779
06/24/2022 11:47:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.88 on epoch=784
06/24/2022 11:47:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.01 on epoch=789
06/24/2022 11:47:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.98 on epoch=794
06/24/2022 11:47:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.85 on epoch=799
06/24/2022 11:47:49 - INFO - __main__ - Global step 1600 Train loss 0.94 Classification-F1 0.3454545454545454 on epoch=799
06/24/2022 11:47:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.96 on epoch=804
06/24/2022 11:47:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.04 on epoch=809
06/24/2022 11:47:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.97 on epoch=814
06/24/2022 11:47:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.02 on epoch=819
06/24/2022 11:47:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.89 on epoch=824
06/24/2022 11:47:56 - INFO - __main__ - Global step 1650 Train loss 0.98 Classification-F1 0.4231177094379639 on epoch=824
06/24/2022 11:47:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.88 on epoch=829
06/24/2022 11:47:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.03 on epoch=834
06/24/2022 11:48:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.98 on epoch=839
06/24/2022 11:48:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.88 on epoch=844
06/24/2022 11:48:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.04 on epoch=849
06/24/2022 11:48:03 - INFO - __main__ - Global step 1700 Train loss 0.96 Classification-F1 0.3191489361702127 on epoch=849
06/24/2022 11:48:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.93 on epoch=854
06/24/2022 11:48:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.94 on epoch=859
06/24/2022 11:48:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.91 on epoch=864
06/24/2022 11:48:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.94 on epoch=869
06/24/2022 11:48:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.87 on epoch=874
06/24/2022 11:48:09 - INFO - __main__ - Global step 1750 Train loss 0.92 Classification-F1 0.3992490613266583 on epoch=874
06/24/2022 11:48:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.88 on epoch=879
06/24/2022 11:48:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.86 on epoch=884
06/24/2022 11:48:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.86 on epoch=889
06/24/2022 11:48:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.91 on epoch=894
06/24/2022 11:48:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.95 on epoch=899
06/24/2022 11:48:16 - INFO - __main__ - Global step 1800 Train loss 0.89 Classification-F1 0.3191489361702127 on epoch=899
06/24/2022 11:48:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.90 on epoch=904
06/24/2022 11:48:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.86 on epoch=909
06/24/2022 11:48:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.81 on epoch=914
06/24/2022 11:48:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.86 on epoch=919
06/24/2022 11:48:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.83 on epoch=924
06/24/2022 11:48:22 - INFO - __main__ - Global step 1850 Train loss 0.85 Classification-F1 0.4231177094379639 on epoch=924
06/24/2022 11:48:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.83 on epoch=929
06/24/2022 11:48:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.85 on epoch=934
06/24/2022 11:48:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.81 on epoch=939
06/24/2022 11:48:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.84 on epoch=944
06/24/2022 11:48:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.83 on epoch=949
06/24/2022 11:48:29 - INFO - __main__ - Global step 1900 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=949
06/24/2022 11:48:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.84 on epoch=954
06/24/2022 11:48:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.81 on epoch=959
06/24/2022 11:48:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.77 on epoch=964
06/24/2022 11:48:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.71 on epoch=969
06/24/2022 11:48:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.85 on epoch=974
06/24/2022 11:48:36 - INFO - __main__ - Global step 1950 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=974
06/24/2022 11:48:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.83 on epoch=979
06/24/2022 11:48:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.88 on epoch=984
06/24/2022 11:48:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.78 on epoch=989
06/24/2022 11:48:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.83 on epoch=994
06/24/2022 11:48:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.77 on epoch=999
06/24/2022 11:48:42 - INFO - __main__ - Global step 2000 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=999
06/24/2022 11:48:42 - INFO - __main__ - save last model!
06/24/2022 11:48:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 11:48:42 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 11:48:42 - INFO - __main__ - Printing 3 examples
06/24/2022 11:48:42 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 11:48:42 - INFO - __main__ - ['0']
06/24/2022 11:48:42 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 11:48:42 - INFO - __main__ - ['1']
06/24/2022 11:48:42 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 11:48:42 - INFO - __main__ - ['1']
06/24/2022 11:48:42 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:48:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:48:43 - INFO - __main__ - Printing 3 examples
06/24/2022 11:48:43 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 11:48:43 - INFO - __main__ - ['1']
06/24/2022 11:48:43 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 11:48:43 - INFO - __main__ - ['1']
06/24/2022 11:48:43 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 11:48:43 - INFO - __main__ - ['1']
06/24/2022 11:48:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 11:48:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:48:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:48:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:48:43 - INFO - __main__ - Printing 3 examples
06/24/2022 11:48:43 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 11:48:43 - INFO - __main__ - ['1']
06/24/2022 11:48:43 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 11:48:43 - INFO - __main__ - ['1']
06/24/2022 11:48:43 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 11:48:43 - INFO - __main__ - ['1']
06/24/2022 11:48:43 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:48:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:48:43 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:48:46 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:48:49 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:48:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:48:49 - INFO - __main__ - Starting training!
06/24/2022 11:48:54 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 11:50:18 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_21_0.3_8_predictions.txt
06/24/2022 11:50:18 - INFO - __main__ - Classification-F1 on test data: 0.3291
06/24/2022 11:50:18 - INFO - __main__ - prefix=paws_16_21, lr=0.3, bsz=8, dev_performance=0.5134502923976608, test_performance=0.3290897136768685
06/24/2022 11:50:18 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.2, bsz=8 ...
06/24/2022 11:50:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:50:20 - INFO - __main__ - Printing 3 examples
06/24/2022 11:50:20 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 11:50:20 - INFO - __main__ - ['1']
06/24/2022 11:50:20 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 11:50:20 - INFO - __main__ - ['1']
06/24/2022 11:50:20 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 11:50:20 - INFO - __main__ - ['1']
06/24/2022 11:50:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:50:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:50:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:50:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:50:20 - INFO - __main__ - Printing 3 examples
06/24/2022 11:50:20 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 11:50:20 - INFO - __main__ - ['1']
06/24/2022 11:50:20 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 11:50:20 - INFO - __main__ - ['1']
06/24/2022 11:50:20 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 11:50:20 - INFO - __main__ - ['1']
06/24/2022 11:50:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:50:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:50:20 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:50:25 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:50:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:50:25 - INFO - __main__ - Starting training!
06/24/2022 11:50:27 - INFO - __main__ - Step 10 Global step 10 Train loss 5.97 on epoch=4
06/24/2022 11:50:28 - INFO - __main__ - Step 20 Global step 20 Train loss 6.09 on epoch=9
06/24/2022 11:50:29 - INFO - __main__ - Step 30 Global step 30 Train loss 5.94 on epoch=14
06/24/2022 11:50:30 - INFO - __main__ - Step 40 Global step 40 Train loss 5.98 on epoch=19
06/24/2022 11:50:32 - INFO - __main__ - Step 50 Global step 50 Train loss 5.91 on epoch=24
06/24/2022 11:50:34 - INFO - __main__ - Global step 50 Train loss 5.98 Classification-F1 0.0 on epoch=24
06/24/2022 11:50:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 11:50:35 - INFO - __main__ - Step 60 Global step 60 Train loss 5.92 on epoch=29
06/24/2022 11:50:36 - INFO - __main__ - Step 70 Global step 70 Train loss 5.82 on epoch=34
06/24/2022 11:50:38 - INFO - __main__ - Step 80 Global step 80 Train loss 5.90 on epoch=39
06/24/2022 11:50:39 - INFO - __main__ - Step 90 Global step 90 Train loss 5.93 on epoch=44
06/24/2022 11:50:40 - INFO - __main__ - Step 100 Global step 100 Train loss 5.89 on epoch=49
06/24/2022 11:50:41 - INFO - __main__ - Global step 100 Train loss 5.89 Classification-F1 0.0 on epoch=49
06/24/2022 11:50:42 - INFO - __main__ - Step 110 Global step 110 Train loss 5.76 on epoch=54
06/24/2022 11:50:43 - INFO - __main__ - Step 120 Global step 120 Train loss 5.86 on epoch=59
06/24/2022 11:50:44 - INFO - __main__ - Step 130 Global step 130 Train loss 5.84 on epoch=64
06/24/2022 11:50:46 - INFO - __main__ - Step 140 Global step 140 Train loss 5.94 on epoch=69
06/24/2022 11:50:47 - INFO - __main__ - Step 150 Global step 150 Train loss 5.75 on epoch=74
06/24/2022 11:50:48 - INFO - __main__ - Global step 150 Train loss 5.83 Classification-F1 0.0 on epoch=74
06/24/2022 11:50:49 - INFO - __main__ - Step 160 Global step 160 Train loss 5.72 on epoch=79
06/24/2022 11:50:50 - INFO - __main__ - Step 170 Global step 170 Train loss 5.82 on epoch=84
06/24/2022 11:50:51 - INFO - __main__ - Step 180 Global step 180 Train loss 5.71 on epoch=89
06/24/2022 11:50:53 - INFO - __main__ - Step 190 Global step 190 Train loss 5.74 on epoch=94
06/24/2022 11:50:54 - INFO - __main__ - Step 200 Global step 200 Train loss 5.67 on epoch=99
06/24/2022 11:50:55 - INFO - __main__ - Global step 200 Train loss 5.73 Classification-F1 0.0 on epoch=99
06/24/2022 11:50:56 - INFO - __main__ - Step 210 Global step 210 Train loss 5.68 on epoch=104
06/24/2022 11:50:57 - INFO - __main__ - Step 220 Global step 220 Train loss 5.77 on epoch=109
06/24/2022 11:50:59 - INFO - __main__ - Step 230 Global step 230 Train loss 5.71 on epoch=114
06/24/2022 11:51:00 - INFO - __main__ - Step 240 Global step 240 Train loss 5.73 on epoch=119
06/24/2022 11:51:01 - INFO - __main__ - Step 250 Global step 250 Train loss 5.64 on epoch=124
06/24/2022 11:51:02 - INFO - __main__ - Global step 250 Train loss 5.71 Classification-F1 0.0 on epoch=124
06/24/2022 11:51:03 - INFO - __main__ - Step 260 Global step 260 Train loss 5.66 on epoch=129
06/24/2022 11:51:05 - INFO - __main__ - Step 270 Global step 270 Train loss 5.74 on epoch=134
06/24/2022 11:51:06 - INFO - __main__ - Step 280 Global step 280 Train loss 5.56 on epoch=139
06/24/2022 11:51:07 - INFO - __main__ - Step 290 Global step 290 Train loss 5.69 on epoch=144
06/24/2022 11:51:08 - INFO - __main__ - Step 300 Global step 300 Train loss 5.64 on epoch=149
06/24/2022 11:51:10 - INFO - __main__ - Global step 300 Train loss 5.66 Classification-F1 0.0 on epoch=149
06/24/2022 11:51:12 - INFO - __main__ - Step 310 Global step 310 Train loss 5.62 on epoch=154
06/24/2022 11:51:13 - INFO - __main__ - Step 320 Global step 320 Train loss 5.59 on epoch=159
06/24/2022 11:51:14 - INFO - __main__ - Step 330 Global step 330 Train loss 5.59 on epoch=164
06/24/2022 11:51:15 - INFO - __main__ - Step 340 Global step 340 Train loss 5.50 on epoch=169
06/24/2022 11:51:16 - INFO - __main__ - Step 350 Global step 350 Train loss 5.54 on epoch=174
06/24/2022 11:51:17 - INFO - __main__ - Global step 350 Train loss 5.57 Classification-F1 0.0 on epoch=174
06/24/2022 11:51:18 - INFO - __main__ - Step 360 Global step 360 Train loss 5.60 on epoch=179
06/24/2022 11:51:20 - INFO - __main__ - Step 370 Global step 370 Train loss 5.47 on epoch=184
06/24/2022 11:51:21 - INFO - __main__ - Step 380 Global step 380 Train loss 5.55 on epoch=189
06/24/2022 11:51:22 - INFO - __main__ - Step 390 Global step 390 Train loss 5.57 on epoch=194
06/24/2022 11:51:23 - INFO - __main__ - Step 400 Global step 400 Train loss 5.57 on epoch=199
06/24/2022 11:51:29 - INFO - __main__ - Global step 400 Train loss 5.55 Classification-F1 0.0 on epoch=199
06/24/2022 11:51:31 - INFO - __main__ - Step 410 Global step 410 Train loss 5.36 on epoch=204
06/24/2022 11:51:32 - INFO - __main__ - Step 420 Global step 420 Train loss 5.48 on epoch=209
06/24/2022 11:51:33 - INFO - __main__ - Step 430 Global step 430 Train loss 5.45 on epoch=214
06/24/2022 11:51:34 - INFO - __main__ - Step 440 Global step 440 Train loss 5.55 on epoch=219
06/24/2022 11:51:36 - INFO - __main__ - Step 450 Global step 450 Train loss 5.42 on epoch=224
06/24/2022 11:51:38 - INFO - __main__ - Global step 450 Train loss 5.45 Classification-F1 0.0 on epoch=224
06/24/2022 11:51:39 - INFO - __main__ - Step 460 Global step 460 Train loss 5.45 on epoch=229
06/24/2022 11:51:40 - INFO - __main__ - Step 470 Global step 470 Train loss 5.41 on epoch=234
06/24/2022 11:51:41 - INFO - __main__ - Step 480 Global step 480 Train loss 5.28 on epoch=239
06/24/2022 11:51:43 - INFO - __main__ - Step 490 Global step 490 Train loss 5.41 on epoch=244
06/24/2022 11:51:44 - INFO - __main__ - Step 500 Global step 500 Train loss 5.36 on epoch=249
06/24/2022 11:51:45 - INFO - __main__ - Global step 500 Train loss 5.38 Classification-F1 0.0 on epoch=249
06/24/2022 11:51:46 - INFO - __main__ - Step 510 Global step 510 Train loss 5.31 on epoch=254
06/24/2022 11:51:48 - INFO - __main__ - Step 520 Global step 520 Train loss 5.40 on epoch=259
06/24/2022 11:51:49 - INFO - __main__ - Step 530 Global step 530 Train loss 5.28 on epoch=264
06/24/2022 11:51:50 - INFO - __main__ - Step 540 Global step 540 Train loss 5.30 on epoch=269
06/24/2022 11:51:51 - INFO - __main__ - Step 550 Global step 550 Train loss 5.26 on epoch=274
06/24/2022 11:51:53 - INFO - __main__ - Global step 550 Train loss 5.31 Classification-F1 0.0 on epoch=274
06/24/2022 11:51:54 - INFO - __main__ - Step 560 Global step 560 Train loss 5.17 on epoch=279
06/24/2022 11:51:55 - INFO - __main__ - Step 570 Global step 570 Train loss 5.22 on epoch=284
06/24/2022 11:51:56 - INFO - __main__ - Step 580 Global step 580 Train loss 5.20 on epoch=289
06/24/2022 11:51:58 - INFO - __main__ - Step 590 Global step 590 Train loss 5.16 on epoch=294
06/24/2022 11:51:59 - INFO - __main__ - Step 600 Global step 600 Train loss 5.20 on epoch=299
06/24/2022 11:52:01 - INFO - __main__ - Global step 600 Train loss 5.19 Classification-F1 0.0 on epoch=299
06/24/2022 11:52:02 - INFO - __main__ - Step 610 Global step 610 Train loss 5.09 on epoch=304
06/24/2022 11:52:03 - INFO - __main__ - Step 620 Global step 620 Train loss 4.99 on epoch=309
06/24/2022 11:52:04 - INFO - __main__ - Step 630 Global step 630 Train loss 5.03 on epoch=314
06/24/2022 11:52:06 - INFO - __main__ - Step 640 Global step 640 Train loss 4.97 on epoch=319
06/24/2022 11:52:07 - INFO - __main__ - Step 650 Global step 650 Train loss 5.02 on epoch=324
06/24/2022 11:52:18 - INFO - __main__ - Global step 650 Train loss 5.02 Classification-F1 0.0 on epoch=324
06/24/2022 11:52:19 - INFO - __main__ - Step 660 Global step 660 Train loss 4.93 on epoch=329
06/24/2022 11:52:20 - INFO - __main__ - Step 670 Global step 670 Train loss 4.81 on epoch=334
06/24/2022 11:52:21 - INFO - __main__ - Step 680 Global step 680 Train loss 4.84 on epoch=339
06/24/2022 11:52:23 - INFO - __main__ - Step 690 Global step 690 Train loss 4.70 on epoch=344
06/24/2022 11:52:24 - INFO - __main__ - Step 700 Global step 700 Train loss 4.72 on epoch=349
06/24/2022 11:52:35 - INFO - __main__ - Global step 700 Train loss 4.80 Classification-F1 0.0 on epoch=349
06/24/2022 11:52:36 - INFO - __main__ - Step 710 Global step 710 Train loss 4.72 on epoch=354
06/24/2022 11:52:37 - INFO - __main__ - Step 720 Global step 720 Train loss 4.62 on epoch=359
06/24/2022 11:52:38 - INFO - __main__ - Step 730 Global step 730 Train loss 4.57 on epoch=364
06/24/2022 11:52:40 - INFO - __main__ - Step 740 Global step 740 Train loss 4.47 on epoch=369
06/24/2022 11:52:41 - INFO - __main__ - Step 750 Global step 750 Train loss 4.53 on epoch=374
06/24/2022 11:52:52 - INFO - __main__ - Global step 750 Train loss 4.58 Classification-F1 0.0 on epoch=374
06/24/2022 11:52:54 - INFO - __main__ - Step 760 Global step 760 Train loss 4.43 on epoch=379
06/24/2022 11:52:55 - INFO - __main__ - Step 770 Global step 770 Train loss 4.42 on epoch=384
06/24/2022 11:52:56 - INFO - __main__ - Step 780 Global step 780 Train loss 4.45 on epoch=389
06/24/2022 11:52:57 - INFO - __main__ - Step 790 Global step 790 Train loss 4.46 on epoch=394
06/24/2022 11:52:59 - INFO - __main__ - Step 800 Global step 800 Train loss 4.31 on epoch=399
06/24/2022 11:53:10 - INFO - __main__ - Global step 800 Train loss 4.41 Classification-F1 0.0 on epoch=399
06/24/2022 11:53:12 - INFO - __main__ - Step 810 Global step 810 Train loss 4.24 on epoch=404
06/24/2022 11:53:13 - INFO - __main__ - Step 820 Global step 820 Train loss 4.32 on epoch=409
06/24/2022 11:53:14 - INFO - __main__ - Step 830 Global step 830 Train loss 4.37 on epoch=414
06/24/2022 11:53:15 - INFO - __main__ - Step 840 Global step 840 Train loss 4.28 on epoch=419
06/24/2022 11:53:17 - INFO - __main__ - Step 850 Global step 850 Train loss 4.22 on epoch=424
06/24/2022 11:53:28 - INFO - __main__ - Global step 850 Train loss 4.28 Classification-F1 0.0 on epoch=424
06/24/2022 11:53:29 - INFO - __main__ - Step 860 Global step 860 Train loss 4.37 on epoch=429
06/24/2022 11:53:31 - INFO - __main__ - Step 870 Global step 870 Train loss 4.23 on epoch=434
06/24/2022 11:53:32 - INFO - __main__ - Step 880 Global step 880 Train loss 4.29 on epoch=439
06/24/2022 11:53:33 - INFO - __main__ - Step 890 Global step 890 Train loss 4.12 on epoch=444
06/24/2022 11:53:34 - INFO - __main__ - Step 900 Global step 900 Train loss 4.08 on epoch=449
06/24/2022 11:53:46 - INFO - __main__ - Global step 900 Train loss 4.22 Classification-F1 0.0 on epoch=449
06/24/2022 11:53:47 - INFO - __main__ - Step 910 Global step 910 Train loss 4.14 on epoch=454
06/24/2022 11:53:48 - INFO - __main__ - Step 920 Global step 920 Train loss 4.17 on epoch=459
06/24/2022 11:53:50 - INFO - __main__ - Step 930 Global step 930 Train loss 4.18 on epoch=464
06/24/2022 11:53:51 - INFO - __main__ - Step 940 Global step 940 Train loss 4.22 on epoch=469
06/24/2022 11:53:52 - INFO - __main__ - Step 950 Global step 950 Train loss 4.07 on epoch=474
06/24/2022 11:54:04 - INFO - __main__ - Global step 950 Train loss 4.16 Classification-F1 0.0 on epoch=474
06/24/2022 11:54:06 - INFO - __main__ - Step 960 Global step 960 Train loss 4.09 on epoch=479
06/24/2022 11:54:07 - INFO - __main__ - Step 970 Global step 970 Train loss 4.09 on epoch=484
06/24/2022 11:54:08 - INFO - __main__ - Step 980 Global step 980 Train loss 4.09 on epoch=489
06/24/2022 11:54:09 - INFO - __main__ - Step 990 Global step 990 Train loss 4.04 on epoch=494
06/24/2022 11:54:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 4.20 on epoch=499
06/24/2022 11:54:23 - INFO - __main__ - Global step 1000 Train loss 4.10 Classification-F1 0.0 on epoch=499
06/24/2022 11:54:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 4.04 on epoch=504
06/24/2022 11:54:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 4.04 on epoch=509
06/24/2022 11:54:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 4.00 on epoch=514
06/24/2022 11:54:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.96 on epoch=519
06/24/2022 11:54:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.83 on epoch=524
06/24/2022 11:54:41 - INFO - __main__ - Global step 1050 Train loss 3.98 Classification-F1 0.0 on epoch=524
06/24/2022 11:54:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.92 on epoch=529
06/24/2022 11:54:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.89 on epoch=534
06/24/2022 11:54:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.78 on epoch=539
06/24/2022 11:54:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.96 on epoch=544
06/24/2022 11:54:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.86 on epoch=549
06/24/2022 11:54:59 - INFO - __main__ - Global step 1100 Train loss 3.88 Classification-F1 0.0 on epoch=549
06/24/2022 11:55:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.87 on epoch=554
06/24/2022 11:55:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.86 on epoch=559
06/24/2022 11:55:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.83 on epoch=564
06/24/2022 11:55:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.75 on epoch=569
06/24/2022 11:55:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.74 on epoch=574
06/24/2022 11:55:17 - INFO - __main__ - Global step 1150 Train loss 3.81 Classification-F1 0.0 on epoch=574
06/24/2022 11:55:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.81 on epoch=579
06/24/2022 11:55:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.75 on epoch=584
06/24/2022 11:55:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.67 on epoch=589
06/24/2022 11:55:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.69 on epoch=594
06/24/2022 11:55:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.65 on epoch=599
06/24/2022 11:55:35 - INFO - __main__ - Global step 1200 Train loss 3.71 Classification-F1 0.0 on epoch=599
06/24/2022 11:55:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.68 on epoch=604
06/24/2022 11:55:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.60 on epoch=609
06/24/2022 11:55:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.45 on epoch=614
06/24/2022 11:55:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.51 on epoch=619
06/24/2022 11:55:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.45 on epoch=624
06/24/2022 11:55:53 - INFO - __main__ - Global step 1250 Train loss 3.54 Classification-F1 0.004830917874396135 on epoch=624
06/24/2022 11:55:53 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.004830917874396135 on epoch=624, global_step=1250
06/24/2022 11:55:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.38 on epoch=629
06/24/2022 11:55:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.48 on epoch=634
06/24/2022 11:55:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.27 on epoch=639
06/24/2022 11:55:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.17 on epoch=644
06/24/2022 11:55:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.27 on epoch=649
06/24/2022 11:56:10 - INFO - __main__ - Global step 1300 Train loss 3.31 Classification-F1 0.03909465020576132 on epoch=649
06/24/2022 11:56:10 - INFO - __main__ - Saving model with best Classification-F1: 0.004830917874396135 -> 0.03909465020576132 on epoch=649, global_step=1300
06/24/2022 11:56:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.27 on epoch=654
06/24/2022 11:56:12 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.21 on epoch=659
06/24/2022 11:56:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 3.23 on epoch=664
06/24/2022 11:56:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 3.07 on epoch=669
06/24/2022 11:56:16 - INFO - __main__ - Step 1350 Global step 1350 Train loss 3.12 on epoch=674
06/24/2022 11:56:27 - INFO - __main__ - Global step 1350 Train loss 3.18 Classification-F1 0.0619010619010619 on epoch=674
06/24/2022 11:56:27 - INFO - __main__ - Saving model with best Classification-F1: 0.03909465020576132 -> 0.0619010619010619 on epoch=674, global_step=1350
06/24/2022 11:56:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.92 on epoch=679
06/24/2022 11:56:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 3.03 on epoch=684
06/24/2022 11:56:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.98 on epoch=689
06/24/2022 11:56:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.85 on epoch=694
06/24/2022 11:56:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.99 on epoch=699
06/24/2022 11:56:44 - INFO - __main__ - Global step 1400 Train loss 2.96 Classification-F1 0.0727536231884058 on epoch=699
06/24/2022 11:56:44 - INFO - __main__ - Saving model with best Classification-F1: 0.0619010619010619 -> 0.0727536231884058 on epoch=699, global_step=1400
06/24/2022 11:56:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.78 on epoch=704
06/24/2022 11:56:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.73 on epoch=709
06/24/2022 11:56:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.80 on epoch=714
06/24/2022 11:56:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.58 on epoch=719
06/24/2022 11:56:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.70 on epoch=724
06/24/2022 11:57:01 - INFO - __main__ - Global step 1450 Train loss 2.72 Classification-F1 0.18316498316498317 on epoch=724
06/24/2022 11:57:01 - INFO - __main__ - Saving model with best Classification-F1: 0.0727536231884058 -> 0.18316498316498317 on epoch=724, global_step=1450
06/24/2022 11:57:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.81 on epoch=729
06/24/2022 11:57:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.66 on epoch=734
06/24/2022 11:57:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.76 on epoch=739
06/24/2022 11:57:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.58 on epoch=744
06/24/2022 11:57:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.47 on epoch=749
06/24/2022 11:57:15 - INFO - __main__ - Global step 1500 Train loss 2.65 Classification-F1 0.2748655913978495 on epoch=749
06/24/2022 11:57:15 - INFO - __main__ - Saving model with best Classification-F1: 0.18316498316498317 -> 0.2748655913978495 on epoch=749, global_step=1500
06/24/2022 11:57:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.46 on epoch=754
06/24/2022 11:57:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.36 on epoch=759
06/24/2022 11:57:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 2.34 on epoch=764
06/24/2022 11:57:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 2.32 on epoch=769
06/24/2022 11:57:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 2.28 on epoch=774
06/24/2022 11:57:24 - INFO - __main__ - Global step 1550 Train loss 2.35 Classification-F1 0.4589371980676329 on epoch=774
06/24/2022 11:57:24 - INFO - __main__ - Saving model with best Classification-F1: 0.2748655913978495 -> 0.4589371980676329 on epoch=774, global_step=1550
06/24/2022 11:57:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 2.24 on epoch=779
06/24/2022 11:57:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 2.19 on epoch=784
06/24/2022 11:57:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 2.07 on epoch=789
06/24/2022 11:57:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 2.08 on epoch=794
06/24/2022 11:57:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 2.06 on epoch=799
06/24/2022 11:57:32 - INFO - __main__ - Global step 1600 Train loss 2.13 Classification-F1 0.539313399778516 on epoch=799
06/24/2022 11:57:32 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.539313399778516 on epoch=799, global_step=1600
06/24/2022 11:57:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 2.00 on epoch=804
06/24/2022 11:57:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.94 on epoch=809
06/24/2022 11:57:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 2.17 on epoch=814
06/24/2022 11:57:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.99 on epoch=819
06/24/2022 11:57:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.85 on epoch=824
06/24/2022 11:57:40 - INFO - __main__ - Global step 1650 Train loss 1.99 Classification-F1 0.4554554554554554 on epoch=824
06/24/2022 11:57:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.88 on epoch=829
06/24/2022 11:57:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.88 on epoch=834
06/24/2022 11:57:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.71 on epoch=839
06/24/2022 11:57:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.76 on epoch=844
06/24/2022 11:57:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.74 on epoch=849
06/24/2022 11:57:47 - INFO - __main__ - Global step 1700 Train loss 1.79 Classification-F1 0.4285714285714286 on epoch=849
06/24/2022 11:57:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.80 on epoch=854
06/24/2022 11:57:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.55 on epoch=859
06/24/2022 11:57:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.73 on epoch=864
06/24/2022 11:57:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.60 on epoch=869
06/24/2022 11:57:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.49 on epoch=874
06/24/2022 11:57:54 - INFO - __main__ - Global step 1750 Train loss 1.63 Classification-F1 0.4682306940371457 on epoch=874
06/24/2022 11:57:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.58 on epoch=879
06/24/2022 11:57:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.54 on epoch=884
06/24/2022 11:57:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.52 on epoch=889
06/24/2022 11:57:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.39 on epoch=894
06/24/2022 11:58:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.48 on epoch=899
06/24/2022 11:58:01 - INFO - __main__ - Global step 1800 Train loss 1.50 Classification-F1 0.37254901960784315 on epoch=899
06/24/2022 11:58:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.41 on epoch=904
06/24/2022 11:58:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.33 on epoch=909
06/24/2022 11:58:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.33 on epoch=914
06/24/2022 11:58:06 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.39 on epoch=919
06/24/2022 11:58:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.47 on epoch=924
06/24/2022 11:58:07 - INFO - __main__ - Global step 1850 Train loss 1.38 Classification-F1 0.49090909090909085 on epoch=924
06/24/2022 11:58:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.26 on epoch=929
06/24/2022 11:58:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.35 on epoch=934
06/24/2022 11:58:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.31 on epoch=939
06/24/2022 11:58:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.18 on epoch=944
06/24/2022 11:58:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.24 on epoch=949
06/24/2022 11:58:14 - INFO - __main__ - Global step 1900 Train loss 1.27 Classification-F1 0.4589371980676329 on epoch=949
06/24/2022 11:58:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.32 on epoch=954
06/24/2022 11:58:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.26 on epoch=959
06/24/2022 11:58:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.10 on epoch=964
06/24/2022 11:58:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.21 on epoch=969
06/24/2022 11:58:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.11 on epoch=974
06/24/2022 11:58:21 - INFO - __main__ - Global step 1950 Train loss 1.20 Classification-F1 0.4181818181818182 on epoch=974
06/24/2022 11:58:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.15 on epoch=979
06/24/2022 11:58:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.02 on epoch=984
06/24/2022 11:58:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.11 on epoch=989
06/24/2022 11:58:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.07 on epoch=994
06/24/2022 11:58:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.08 on epoch=999
06/24/2022 11:58:27 - INFO - __main__ - Global step 2000 Train loss 1.08 Classification-F1 0.3992490613266583 on epoch=999
06/24/2022 11:58:27 - INFO - __main__ - save last model!
06/24/2022 11:58:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 11:58:27 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 11:58:27 - INFO - __main__ - Printing 3 examples
06/24/2022 11:58:27 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 11:58:27 - INFO - __main__ - ['0']
06/24/2022 11:58:27 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 11:58:27 - INFO - __main__ - ['1']
06/24/2022 11:58:27 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 11:58:27 - INFO - __main__ - ['1']
06/24/2022 11:58:27 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 11:58:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:58:28 - INFO - __main__ - Printing 3 examples
06/24/2022 11:58:28 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 11:58:28 - INFO - __main__ - ['1']
06/24/2022 11:58:28 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 11:58:28 - INFO - __main__ - ['1']
06/24/2022 11:58:28 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 11:58:28 - INFO - __main__ - ['1']
06/24/2022 11:58:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 11:58:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:58:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 11:58:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 11:58:28 - INFO - __main__ - Printing 3 examples
06/24/2022 11:58:28 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 11:58:28 - INFO - __main__ - ['1']
06/24/2022 11:58:28 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 11:58:28 - INFO - __main__ - ['1']
06/24/2022 11:58:28 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 11:58:28 - INFO - __main__ - ['1']
06/24/2022 11:58:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 11:58:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:58:28 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 11:58:31 - INFO - __main__ - Tokenizing Output ...
06/24/2022 11:58:33 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 11:58:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 11:58:34 - INFO - __main__ - Starting training!
06/24/2022 11:58:39 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 12:00:04 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_21_0.2_8_predictions.txt
06/24/2022 12:00:04 - INFO - __main__ - Classification-F1 on test data: 0.3495
06/24/2022 12:00:05 - INFO - __main__ - prefix=paws_16_21, lr=0.2, bsz=8, dev_performance=0.539313399778516, test_performance=0.3495046425203463
06/24/2022 12:00:05 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.5, bsz=8 ...
06/24/2022 12:00:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:00:05 - INFO - __main__ - Printing 3 examples
06/24/2022 12:00:05 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 12:00:05 - INFO - __main__ - ['1']
06/24/2022 12:00:05 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 12:00:05 - INFO - __main__ - ['1']
06/24/2022 12:00:05 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 12:00:05 - INFO - __main__ - ['1']
06/24/2022 12:00:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:00:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:00:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:00:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:00:06 - INFO - __main__ - Printing 3 examples
06/24/2022 12:00:06 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 12:00:06 - INFO - __main__ - ['1']
06/24/2022 12:00:06 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 12:00:06 - INFO - __main__ - ['1']
06/24/2022 12:00:06 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 12:00:06 - INFO - __main__ - ['1']
06/24/2022 12:00:06 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:00:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:00:06 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:00:11 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:00:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:00:11 - INFO - __main__ - Starting training!
06/24/2022 12:00:13 - INFO - __main__ - Step 10 Global step 10 Train loss 5.97 on epoch=4
06/24/2022 12:00:14 - INFO - __main__ - Step 20 Global step 20 Train loss 6.00 on epoch=9
06/24/2022 12:00:15 - INFO - __main__ - Step 30 Global step 30 Train loss 5.92 on epoch=14
06/24/2022 12:00:16 - INFO - __main__ - Step 40 Global step 40 Train loss 5.92 on epoch=19
06/24/2022 12:00:18 - INFO - __main__ - Step 50 Global step 50 Train loss 5.82 on epoch=24
06/24/2022 12:00:21 - INFO - __main__ - Global step 50 Train loss 5.93 Classification-F1 0.0 on epoch=24
06/24/2022 12:00:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 12:00:23 - INFO - __main__ - Step 60 Global step 60 Train loss 5.82 on epoch=29
06/24/2022 12:00:24 - INFO - __main__ - Step 70 Global step 70 Train loss 5.88 on epoch=34
06/24/2022 12:00:25 - INFO - __main__ - Step 80 Global step 80 Train loss 5.79 on epoch=39
06/24/2022 12:00:26 - INFO - __main__ - Step 90 Global step 90 Train loss 5.79 on epoch=44
06/24/2022 12:00:28 - INFO - __main__ - Step 100 Global step 100 Train loss 5.88 on epoch=49
06/24/2022 12:00:30 - INFO - __main__ - Global step 100 Train loss 5.83 Classification-F1 0.0 on epoch=49
06/24/2022 12:00:31 - INFO - __main__ - Step 110 Global step 110 Train loss 5.83 on epoch=54
06/24/2022 12:00:32 - INFO - __main__ - Step 120 Global step 120 Train loss 5.71 on epoch=59
06/24/2022 12:00:33 - INFO - __main__ - Step 130 Global step 130 Train loss 5.68 on epoch=64
06/24/2022 12:00:35 - INFO - __main__ - Step 140 Global step 140 Train loss 5.66 on epoch=69
06/24/2022 12:00:36 - INFO - __main__ - Step 150 Global step 150 Train loss 5.65 on epoch=74
06/24/2022 12:00:38 - INFO - __main__ - Global step 150 Train loss 5.71 Classification-F1 0.0 on epoch=74
06/24/2022 12:00:39 - INFO - __main__ - Step 160 Global step 160 Train loss 5.58 on epoch=79
06/24/2022 12:00:40 - INFO - __main__ - Step 170 Global step 170 Train loss 5.50 on epoch=84
06/24/2022 12:00:42 - INFO - __main__ - Step 180 Global step 180 Train loss 5.51 on epoch=89
06/24/2022 12:00:43 - INFO - __main__ - Step 190 Global step 190 Train loss 5.47 on epoch=94
06/24/2022 12:00:44 - INFO - __main__ - Step 200 Global step 200 Train loss 5.35 on epoch=99
06/24/2022 12:00:48 - INFO - __main__ - Global step 200 Train loss 5.48 Classification-F1 0.0 on epoch=99
06/24/2022 12:00:49 - INFO - __main__ - Step 210 Global step 210 Train loss 5.30 on epoch=104
06/24/2022 12:00:50 - INFO - __main__ - Step 220 Global step 220 Train loss 5.21 on epoch=109
06/24/2022 12:00:52 - INFO - __main__ - Step 230 Global step 230 Train loss 5.14 on epoch=114
06/24/2022 12:00:53 - INFO - __main__ - Step 240 Global step 240 Train loss 5.16 on epoch=119
06/24/2022 12:00:54 - INFO - __main__ - Step 250 Global step 250 Train loss 5.02 on epoch=124
06/24/2022 12:01:02 - INFO - __main__ - Global step 250 Train loss 5.17 Classification-F1 0.0 on epoch=124
06/24/2022 12:01:03 - INFO - __main__ - Step 260 Global step 260 Train loss 4.89 on epoch=129
06/24/2022 12:01:05 - INFO - __main__ - Step 270 Global step 270 Train loss 4.83 on epoch=134
06/24/2022 12:01:06 - INFO - __main__ - Step 280 Global step 280 Train loss 4.76 on epoch=139
06/24/2022 12:01:07 - INFO - __main__ - Step 290 Global step 290 Train loss 4.75 on epoch=144
06/24/2022 12:01:08 - INFO - __main__ - Step 300 Global step 300 Train loss 4.56 on epoch=149
06/24/2022 12:01:20 - INFO - __main__ - Global step 300 Train loss 4.76 Classification-F1 0.0 on epoch=149
06/24/2022 12:01:21 - INFO - __main__ - Step 310 Global step 310 Train loss 4.50 on epoch=154
06/24/2022 12:01:22 - INFO - __main__ - Step 320 Global step 320 Train loss 4.46 on epoch=159
06/24/2022 12:01:23 - INFO - __main__ - Step 330 Global step 330 Train loss 4.39 on epoch=164
06/24/2022 12:01:25 - INFO - __main__ - Step 340 Global step 340 Train loss 4.29 on epoch=169
06/24/2022 12:01:26 - INFO - __main__ - Step 350 Global step 350 Train loss 4.13 on epoch=174
06/24/2022 12:01:37 - INFO - __main__ - Global step 350 Train loss 4.35 Classification-F1 0.0 on epoch=174
06/24/2022 12:01:39 - INFO - __main__ - Step 360 Global step 360 Train loss 4.18 on epoch=179
06/24/2022 12:01:40 - INFO - __main__ - Step 370 Global step 370 Train loss 3.99 on epoch=184
06/24/2022 12:01:41 - INFO - __main__ - Step 380 Global step 380 Train loss 3.96 on epoch=189
06/24/2022 12:01:42 - INFO - __main__ - Step 390 Global step 390 Train loss 4.05 on epoch=194
06/24/2022 12:01:44 - INFO - __main__ - Step 400 Global step 400 Train loss 3.93 on epoch=199
06/24/2022 12:01:54 - INFO - __main__ - Global step 400 Train loss 4.02 Classification-F1 0.0 on epoch=199
06/24/2022 12:01:56 - INFO - __main__ - Step 410 Global step 410 Train loss 3.77 on epoch=204
06/24/2022 12:01:57 - INFO - __main__ - Step 420 Global step 420 Train loss 3.76 on epoch=209
06/24/2022 12:01:58 - INFO - __main__ - Step 430 Global step 430 Train loss 3.74 on epoch=214
06/24/2022 12:01:59 - INFO - __main__ - Step 440 Global step 440 Train loss 3.74 on epoch=219
06/24/2022 12:02:01 - INFO - __main__ - Step 450 Global step 450 Train loss 3.77 on epoch=224
06/24/2022 12:02:06 - INFO - __main__ - Global step 450 Train loss 3.75 Classification-F1 0.0 on epoch=224
06/24/2022 12:02:07 - INFO - __main__ - Step 460 Global step 460 Train loss 3.73 on epoch=229
06/24/2022 12:02:08 - INFO - __main__ - Step 470 Global step 470 Train loss 3.58 on epoch=234
06/24/2022 12:02:09 - INFO - __main__ - Step 480 Global step 480 Train loss 3.66 on epoch=239
06/24/2022 12:02:11 - INFO - __main__ - Step 490 Global step 490 Train loss 3.49 on epoch=244
06/24/2022 12:02:12 - INFO - __main__ - Step 500 Global step 500 Train loss 3.58 on epoch=249
06/24/2022 12:02:17 - INFO - __main__ - Global step 500 Train loss 3.61 Classification-F1 0.0 on epoch=249
06/24/2022 12:02:18 - INFO - __main__ - Step 510 Global step 510 Train loss 3.32 on epoch=254
06/24/2022 12:02:19 - INFO - __main__ - Step 520 Global step 520 Train loss 3.33 on epoch=259
06/24/2022 12:02:20 - INFO - __main__ - Step 530 Global step 530 Train loss 3.28 on epoch=264
06/24/2022 12:02:22 - INFO - __main__ - Step 540 Global step 540 Train loss 3.22 on epoch=269
06/24/2022 12:02:23 - INFO - __main__ - Step 550 Global step 550 Train loss 3.11 on epoch=274
06/24/2022 12:02:27 - INFO - __main__ - Global step 550 Train loss 3.25 Classification-F1 0.009049773755656108 on epoch=274
06/24/2022 12:02:27 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.009049773755656108 on epoch=274, global_step=550
06/24/2022 12:02:28 - INFO - __main__ - Step 560 Global step 560 Train loss 3.08 on epoch=279
06/24/2022 12:02:29 - INFO - __main__ - Step 570 Global step 570 Train loss 2.86 on epoch=284
06/24/2022 12:02:31 - INFO - __main__ - Step 580 Global step 580 Train loss 2.83 on epoch=289
06/24/2022 12:02:32 - INFO - __main__ - Step 590 Global step 590 Train loss 2.73 on epoch=294
06/24/2022 12:02:33 - INFO - __main__ - Step 600 Global step 600 Train loss 2.75 on epoch=299
06/24/2022 12:02:39 - INFO - __main__ - Global step 600 Train loss 2.85 Classification-F1 0.06944444444444445 on epoch=299
06/24/2022 12:02:39 - INFO - __main__ - Saving model with best Classification-F1: 0.009049773755656108 -> 0.06944444444444445 on epoch=299, global_step=600
06/24/2022 12:02:41 - INFO - __main__ - Step 610 Global step 610 Train loss 2.79 on epoch=304
06/24/2022 12:02:42 - INFO - __main__ - Step 620 Global step 620 Train loss 2.51 on epoch=309
06/24/2022 12:02:43 - INFO - __main__ - Step 630 Global step 630 Train loss 2.52 on epoch=314
06/24/2022 12:02:44 - INFO - __main__ - Step 640 Global step 640 Train loss 2.44 on epoch=319
06/24/2022 12:02:46 - INFO - __main__ - Step 650 Global step 650 Train loss 2.58 on epoch=324
06/24/2022 12:02:56 - INFO - __main__ - Global step 650 Train loss 2.57 Classification-F1 0.14718614718614717 on epoch=324
06/24/2022 12:02:56 - INFO - __main__ - Saving model with best Classification-F1: 0.06944444444444445 -> 0.14718614718614717 on epoch=324, global_step=650
06/24/2022 12:02:57 - INFO - __main__ - Step 660 Global step 660 Train loss 2.52 on epoch=329
06/24/2022 12:02:58 - INFO - __main__ - Step 670 Global step 670 Train loss 2.23 on epoch=334
06/24/2022 12:03:00 - INFO - __main__ - Step 680 Global step 680 Train loss 2.21 on epoch=339
06/24/2022 12:03:01 - INFO - __main__ - Step 690 Global step 690 Train loss 2.17 on epoch=344
06/24/2022 12:03:02 - INFO - __main__ - Step 700 Global step 700 Train loss 2.17 on epoch=349
06/24/2022 12:03:04 - INFO - __main__ - Global step 700 Train loss 2.26 Classification-F1 0.5901477832512315 on epoch=349
06/24/2022 12:03:04 - INFO - __main__ - Saving model with best Classification-F1: 0.14718614718614717 -> 0.5901477832512315 on epoch=349, global_step=700
06/24/2022 12:03:05 - INFO - __main__ - Step 710 Global step 710 Train loss 2.12 on epoch=354
06/24/2022 12:03:06 - INFO - __main__ - Step 720 Global step 720 Train loss 1.91 on epoch=359
06/24/2022 12:03:07 - INFO - __main__ - Step 730 Global step 730 Train loss 1.84 on epoch=364
06/24/2022 12:03:09 - INFO - __main__ - Step 740 Global step 740 Train loss 1.90 on epoch=369
06/24/2022 12:03:10 - INFO - __main__ - Step 750 Global step 750 Train loss 1.80 on epoch=374
06/24/2022 12:03:12 - INFO - __main__ - Global step 750 Train loss 1.92 Classification-F1 0.5465587044534412 on epoch=374
06/24/2022 12:03:13 - INFO - __main__ - Step 760 Global step 760 Train loss 1.88 on epoch=379
06/24/2022 12:03:14 - INFO - __main__ - Step 770 Global step 770 Train loss 1.74 on epoch=384
06/24/2022 12:03:15 - INFO - __main__ - Step 780 Global step 780 Train loss 1.81 on epoch=389
06/24/2022 12:03:17 - INFO - __main__ - Step 790 Global step 790 Train loss 1.80 on epoch=394
06/24/2022 12:03:18 - INFO - __main__ - Step 800 Global step 800 Train loss 1.61 on epoch=399
06/24/2022 12:03:18 - INFO - __main__ - Global step 800 Train loss 1.77 Classification-F1 0.49090909090909085 on epoch=399
06/24/2022 12:03:20 - INFO - __main__ - Step 810 Global step 810 Train loss 1.56 on epoch=404
06/24/2022 12:03:21 - INFO - __main__ - Step 820 Global step 820 Train loss 1.54 on epoch=409
06/24/2022 12:03:22 - INFO - __main__ - Step 830 Global step 830 Train loss 1.66 on epoch=414
06/24/2022 12:03:23 - INFO - __main__ - Step 840 Global step 840 Train loss 1.43 on epoch=419
06/24/2022 12:03:24 - INFO - __main__ - Step 850 Global step 850 Train loss 1.38 on epoch=424
06/24/2022 12:03:25 - INFO - __main__ - Global step 850 Train loss 1.51 Classification-F1 0.39756367663344405 on epoch=424
06/24/2022 12:03:26 - INFO - __main__ - Step 860 Global step 860 Train loss 1.43 on epoch=429
06/24/2022 12:03:27 - INFO - __main__ - Step 870 Global step 870 Train loss 1.48 on epoch=434
06/24/2022 12:03:29 - INFO - __main__ - Step 880 Global step 880 Train loss 1.33 on epoch=439
06/24/2022 12:03:30 - INFO - __main__ - Step 890 Global step 890 Train loss 1.24 on epoch=444
06/24/2022 12:03:31 - INFO - __main__ - Step 900 Global step 900 Train loss 1.24 on epoch=449
06/24/2022 12:03:32 - INFO - __main__ - Global step 900 Train loss 1.35 Classification-F1 0.3816425120772947 on epoch=449
06/24/2022 12:03:33 - INFO - __main__ - Step 910 Global step 910 Train loss 1.30 on epoch=454
06/24/2022 12:03:34 - INFO - __main__ - Step 920 Global step 920 Train loss 1.33 on epoch=459
06/24/2022 12:03:35 - INFO - __main__ - Step 930 Global step 930 Train loss 1.25 on epoch=464
06/24/2022 12:03:36 - INFO - __main__ - Step 940 Global step 940 Train loss 1.31 on epoch=469
06/24/2022 12:03:38 - INFO - __main__ - Step 950 Global step 950 Train loss 1.18 on epoch=474
06/24/2022 12:03:38 - INFO - __main__ - Global step 950 Train loss 1.28 Classification-F1 0.49090909090909085 on epoch=474
06/24/2022 12:03:39 - INFO - __main__ - Step 960 Global step 960 Train loss 1.06 on epoch=479
06/24/2022 12:03:41 - INFO - __main__ - Step 970 Global step 970 Train loss 1.17 on epoch=484
06/24/2022 12:03:42 - INFO - __main__ - Step 980 Global step 980 Train loss 1.12 on epoch=489
06/24/2022 12:03:43 - INFO - __main__ - Step 990 Global step 990 Train loss 1.11 on epoch=494
06/24/2022 12:03:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.09 on epoch=499
06/24/2022 12:03:45 - INFO - __main__ - Global step 1000 Train loss 1.11 Classification-F1 0.4909862142099682 on epoch=499
06/24/2022 12:03:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.19 on epoch=504
06/24/2022 12:03:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.20 on epoch=509
06/24/2022 12:03:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.92 on epoch=514
06/24/2022 12:03:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.00 on epoch=519
06/24/2022 12:03:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.95 on epoch=524
06/24/2022 12:03:51 - INFO - __main__ - Global step 1050 Train loss 1.05 Classification-F1 0.3333333333333333 on epoch=524
06/24/2022 12:03:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.95 on epoch=529
06/24/2022 12:03:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.93 on epoch=534
06/24/2022 12:03:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.11 on epoch=539
06/24/2022 12:03:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.95 on epoch=544
06/24/2022 12:03:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.91 on epoch=549
06/24/2022 12:03:58 - INFO - __main__ - Global step 1100 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=549
06/24/2022 12:03:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.88 on epoch=554
06/24/2022 12:04:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.95 on epoch=559
06/24/2022 12:04:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.01 on epoch=564
06/24/2022 12:04:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.87 on epoch=569
06/24/2022 12:04:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.93 on epoch=574
06/24/2022 12:04:04 - INFO - __main__ - Global step 1150 Train loss 0.93 Classification-F1 0.3333333333333333 on epoch=574
06/24/2022 12:04:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.85 on epoch=579
06/24/2022 12:04:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.94 on epoch=584
06/24/2022 12:04:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.87 on epoch=589
06/24/2022 12:04:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.92 on epoch=594
06/24/2022 12:04:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.80 on epoch=599
06/24/2022 12:04:11 - INFO - __main__ - Global step 1200 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=599
06/24/2022 12:04:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.87 on epoch=604
06/24/2022 12:04:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=609
06/24/2022 12:04:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.86 on epoch=614
06/24/2022 12:04:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.77 on epoch=619
06/24/2022 12:04:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.79 on epoch=624
06/24/2022 12:04:17 - INFO - __main__ - Global step 1250 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=624
06/24/2022 12:04:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.86 on epoch=629
06/24/2022 12:04:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.83 on epoch=634
06/24/2022 12:04:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.70 on epoch=639
06/24/2022 12:04:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.81 on epoch=644
06/24/2022 12:04:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.77 on epoch=649
06/24/2022 12:04:24 - INFO - __main__ - Global step 1300 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=649
06/24/2022 12:04:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.75 on epoch=654
06/24/2022 12:04:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.77 on epoch=659
06/24/2022 12:04:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.79 on epoch=664
06/24/2022 12:04:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.77 on epoch=669
06/24/2022 12:04:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.77 on epoch=674
06/24/2022 12:04:30 - INFO - __main__ - Global step 1350 Train loss 0.77 Classification-F1 0.3191489361702127 on epoch=674
06/24/2022 12:04:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.74 on epoch=679
06/24/2022 12:04:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.69 on epoch=684
06/24/2022 12:04:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.79 on epoch=689
06/24/2022 12:04:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.81 on epoch=694
06/24/2022 12:04:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.69 on epoch=699
06/24/2022 12:04:37 - INFO - __main__ - Global step 1400 Train loss 0.74 Classification-F1 0.3992490613266583 on epoch=699
06/24/2022 12:04:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.72 on epoch=704
06/24/2022 12:04:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.69 on epoch=709
06/24/2022 12:04:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.73 on epoch=714
06/24/2022 12:04:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.71 on epoch=719
06/24/2022 12:04:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.72 on epoch=724
06/24/2022 12:04:43 - INFO - __main__ - Global step 1450 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=724
06/24/2022 12:04:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.74 on epoch=729
06/24/2022 12:04:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.70 on epoch=734
06/24/2022 12:04:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.69 on epoch=739
06/24/2022 12:04:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.63 on epoch=744
06/24/2022 12:04:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.62 on epoch=749
06/24/2022 12:04:50 - INFO - __main__ - Global step 1500 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=749
06/24/2022 12:04:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.70 on epoch=754
06/24/2022 12:04:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.74 on epoch=759
06/24/2022 12:04:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.63 on epoch=764
06/24/2022 12:04:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.70 on epoch=769
06/24/2022 12:04:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.68 on epoch=774
06/24/2022 12:04:56 - INFO - __main__ - Global step 1550 Train loss 0.69 Classification-F1 0.3992490613266583 on epoch=774
06/24/2022 12:04:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.71 on epoch=779
06/24/2022 12:04:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.63 on epoch=784
06/24/2022 12:05:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.56 on epoch=789
06/24/2022 12:05:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.61 on epoch=794
06/24/2022 12:05:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.66 on epoch=799
06/24/2022 12:05:03 - INFO - __main__ - Global step 1600 Train loss 0.63 Classification-F1 0.3191489361702127 on epoch=799
06/24/2022 12:05:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.58 on epoch=804
06/24/2022 12:05:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.57 on epoch=809
06/24/2022 12:05:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.60 on epoch=814
06/24/2022 12:05:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.66 on epoch=819
06/24/2022 12:05:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.66 on epoch=824
06/24/2022 12:05:09 - INFO - __main__ - Global step 1650 Train loss 0.61 Classification-F1 0.3992490613266583 on epoch=824
06/24/2022 12:05:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.57 on epoch=829
06/24/2022 12:05:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.59 on epoch=834
06/24/2022 12:05:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.57 on epoch=839
06/24/2022 12:05:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.66 on epoch=844
06/24/2022 12:05:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.60 on epoch=849
06/24/2022 12:05:16 - INFO - __main__ - Global step 1700 Train loss 0.60 Classification-F1 0.4589371980676329 on epoch=849
06/24/2022 12:05:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.54 on epoch=854
06/24/2022 12:05:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.55 on epoch=859
06/24/2022 12:05:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.61 on epoch=864
06/24/2022 12:05:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.63 on epoch=869
06/24/2022 12:05:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.56 on epoch=874
06/24/2022 12:05:23 - INFO - __main__ - Global step 1750 Train loss 0.58 Classification-F1 0.39756367663344405 on epoch=874
06/24/2022 12:05:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.62 on epoch=879
06/24/2022 12:05:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.58 on epoch=884
06/24/2022 12:05:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.53 on epoch=889
06/24/2022 12:05:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.58 on epoch=894
06/24/2022 12:05:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.56 on epoch=899
06/24/2022 12:05:29 - INFO - __main__ - Global step 1800 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=899
06/24/2022 12:05:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.53 on epoch=904
06/24/2022 12:05:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.48 on epoch=909
06/24/2022 12:05:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.63 on epoch=914
06/24/2022 12:05:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.57 on epoch=919
06/24/2022 12:05:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.55 on epoch=924
06/24/2022 12:05:36 - INFO - __main__ - Global step 1850 Train loss 0.55 Classification-F1 0.36374269005847953 on epoch=924
06/24/2022 12:05:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.54 on epoch=929
06/24/2022 12:05:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.53 on epoch=934
06/24/2022 12:05:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.51 on epoch=939
06/24/2022 12:05:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.51 on epoch=944
06/24/2022 12:05:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.53 on epoch=949
06/24/2022 12:05:42 - INFO - __main__ - Global step 1900 Train loss 0.52 Classification-F1 0.46843853820598 on epoch=949
06/24/2022 12:05:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.57 on epoch=954
06/24/2022 12:05:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.56 on epoch=959
06/24/2022 12:05:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.52 on epoch=964
06/24/2022 12:05:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.49 on epoch=969
06/24/2022 12:05:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.67 on epoch=974
06/24/2022 12:05:49 - INFO - __main__ - Global step 1950 Train loss 0.56 Classification-F1 0.3816425120772947 on epoch=974
06/24/2022 12:05:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.51 on epoch=979
06/24/2022 12:05:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.51 on epoch=984
06/24/2022 12:05:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.49 on epoch=989
06/24/2022 12:05:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.49 on epoch=994
06/24/2022 12:05:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.58 on epoch=999
06/24/2022 12:05:55 - INFO - __main__ - Global step 2000 Train loss 0.52 Classification-F1 0.3992490613266583 on epoch=999
06/24/2022 12:05:55 - INFO - __main__ - save last model!
06/24/2022 12:05:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 12:05:55 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 12:05:55 - INFO - __main__ - Printing 3 examples
06/24/2022 12:05:55 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 12:05:55 - INFO - __main__ - ['0']
06/24/2022 12:05:55 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 12:05:55 - INFO - __main__ - ['1']
06/24/2022 12:05:55 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 12:05:55 - INFO - __main__ - ['1']
06/24/2022 12:05:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:05:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:05:56 - INFO - __main__ - Printing 3 examples
06/24/2022 12:05:56 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 12:05:56 - INFO - __main__ - ['1']
06/24/2022 12:05:56 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 12:05:56 - INFO - __main__ - ['1']
06/24/2022 12:05:56 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 12:05:56 - INFO - __main__ - ['1']
06/24/2022 12:05:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 12:05:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:05:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:05:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:05:56 - INFO - __main__ - Printing 3 examples
06/24/2022 12:05:56 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 12:05:56 - INFO - __main__ - ['1']
06/24/2022 12:05:56 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 12:05:56 - INFO - __main__ - ['1']
06/24/2022 12:05:56 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 12:05:56 - INFO - __main__ - ['1']
06/24/2022 12:05:56 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:05:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:05:56 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:05:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:06:02 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:06:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:06:02 - INFO - __main__ - Starting training!
06/24/2022 12:06:07 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 12:07:31 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_42_0.5_8_predictions.txt
06/24/2022 12:07:31 - INFO - __main__ - Classification-F1 on test data: 0.3364
06/24/2022 12:07:31 - INFO - __main__ - prefix=paws_16_42, lr=0.5, bsz=8, dev_performance=0.5901477832512315, test_performance=0.3364154775105269
06/24/2022 12:07:31 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.4, bsz=8 ...
06/24/2022 12:07:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:07:34 - INFO - __main__ - Printing 3 examples
06/24/2022 12:07:34 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 12:07:34 - INFO - __main__ - ['1']
06/24/2022 12:07:34 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 12:07:34 - INFO - __main__ - ['1']
06/24/2022 12:07:34 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 12:07:34 - INFO - __main__ - ['1']
06/24/2022 12:07:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:07:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:07:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:07:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:07:34 - INFO - __main__ - Printing 3 examples
06/24/2022 12:07:34 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 12:07:34 - INFO - __main__ - ['1']
06/24/2022 12:07:34 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 12:07:34 - INFO - __main__ - ['1']
06/24/2022 12:07:34 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 12:07:34 - INFO - __main__ - ['1']
06/24/2022 12:07:34 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:07:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:07:34 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:07:41 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:07:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:07:41 - INFO - __main__ - Starting training!
06/24/2022 12:07:42 - INFO - __main__ - Step 10 Global step 10 Train loss 5.99 on epoch=4
06/24/2022 12:07:44 - INFO - __main__ - Step 20 Global step 20 Train loss 5.91 on epoch=9
06/24/2022 12:07:45 - INFO - __main__ - Step 30 Global step 30 Train loss 5.96 on epoch=14
06/24/2022 12:07:46 - INFO - __main__ - Step 40 Global step 40 Train loss 5.88 on epoch=19
06/24/2022 12:07:47 - INFO - __main__ - Step 50 Global step 50 Train loss 5.89 on epoch=24
06/24/2022 12:07:48 - INFO - __main__ - Global step 50 Train loss 5.93 Classification-F1 0.0 on epoch=24
06/24/2022 12:07:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 12:07:50 - INFO - __main__ - Step 60 Global step 60 Train loss 5.82 on epoch=29
06/24/2022 12:07:51 - INFO - __main__ - Step 70 Global step 70 Train loss 5.94 on epoch=34
06/24/2022 12:07:52 - INFO - __main__ - Step 80 Global step 80 Train loss 5.88 on epoch=39
06/24/2022 12:07:53 - INFO - __main__ - Step 90 Global step 90 Train loss 5.76 on epoch=44
06/24/2022 12:07:54 - INFO - __main__ - Step 100 Global step 100 Train loss 5.80 on epoch=49
06/24/2022 12:07:55 - INFO - __main__ - Global step 100 Train loss 5.84 Classification-F1 0.0 on epoch=49
06/24/2022 12:07:56 - INFO - __main__ - Step 110 Global step 110 Train loss 5.70 on epoch=54
06/24/2022 12:07:58 - INFO - __main__ - Step 120 Global step 120 Train loss 5.81 on epoch=59
06/24/2022 12:07:59 - INFO - __main__ - Step 130 Global step 130 Train loss 5.62 on epoch=64
06/24/2022 12:08:00 - INFO - __main__ - Step 140 Global step 140 Train loss 5.72 on epoch=69
06/24/2022 12:08:01 - INFO - __main__ - Step 150 Global step 150 Train loss 5.61 on epoch=74
06/24/2022 12:08:08 - INFO - __main__ - Global step 150 Train loss 5.69 Classification-F1 0.0 on epoch=74
06/24/2022 12:08:09 - INFO - __main__ - Step 160 Global step 160 Train loss 5.68 on epoch=79
06/24/2022 12:08:10 - INFO - __main__ - Step 170 Global step 170 Train loss 5.53 on epoch=84
06/24/2022 12:08:12 - INFO - __main__ - Step 180 Global step 180 Train loss 5.68 on epoch=89
06/24/2022 12:08:13 - INFO - __main__ - Step 190 Global step 190 Train loss 5.46 on epoch=94
06/24/2022 12:08:14 - INFO - __main__ - Step 200 Global step 200 Train loss 5.48 on epoch=99
06/24/2022 12:08:21 - INFO - __main__ - Global step 200 Train loss 5.57 Classification-F1 0.0 on epoch=99
06/24/2022 12:08:22 - INFO - __main__ - Step 210 Global step 210 Train loss 5.35 on epoch=104
06/24/2022 12:08:24 - INFO - __main__ - Step 220 Global step 220 Train loss 5.31 on epoch=109
06/24/2022 12:08:25 - INFO - __main__ - Step 230 Global step 230 Train loss 5.19 on epoch=114
06/24/2022 12:08:26 - INFO - __main__ - Step 240 Global step 240 Train loss 4.97 on epoch=119
06/24/2022 12:08:27 - INFO - __main__ - Step 250 Global step 250 Train loss 4.75 on epoch=124
06/24/2022 12:08:38 - INFO - __main__ - Global step 250 Train loss 5.11 Classification-F1 0.0 on epoch=124
06/24/2022 12:08:39 - INFO - __main__ - Step 260 Global step 260 Train loss 4.56 on epoch=129
06/24/2022 12:08:40 - INFO - __main__ - Step 270 Global step 270 Train loss 4.55 on epoch=134
06/24/2022 12:08:41 - INFO - __main__ - Step 280 Global step 280 Train loss 4.40 on epoch=139
06/24/2022 12:08:43 - INFO - __main__ - Step 290 Global step 290 Train loss 4.24 on epoch=144
06/24/2022 12:08:44 - INFO - __main__ - Step 300 Global step 300 Train loss 4.17 on epoch=149
06/24/2022 12:08:54 - INFO - __main__ - Global step 300 Train loss 4.38 Classification-F1 0.0 on epoch=149
06/24/2022 12:08:55 - INFO - __main__ - Step 310 Global step 310 Train loss 4.09 on epoch=154
06/24/2022 12:08:56 - INFO - __main__ - Step 320 Global step 320 Train loss 4.07 on epoch=159
06/24/2022 12:08:58 - INFO - __main__ - Step 330 Global step 330 Train loss 4.06 on epoch=164
06/24/2022 12:08:59 - INFO - __main__ - Step 340 Global step 340 Train loss 3.96 on epoch=169
06/24/2022 12:09:00 - INFO - __main__ - Step 350 Global step 350 Train loss 4.01 on epoch=174
06/24/2022 12:09:11 - INFO - __main__ - Global step 350 Train loss 4.04 Classification-F1 0.0 on epoch=174
06/24/2022 12:09:12 - INFO - __main__ - Step 360 Global step 360 Train loss 3.79 on epoch=179
06/24/2022 12:09:14 - INFO - __main__ - Step 370 Global step 370 Train loss 3.72 on epoch=184
06/24/2022 12:09:15 - INFO - __main__ - Step 380 Global step 380 Train loss 3.77 on epoch=189
06/24/2022 12:09:16 - INFO - __main__ - Step 390 Global step 390 Train loss 3.64 on epoch=194
06/24/2022 12:09:17 - INFO - __main__ - Step 400 Global step 400 Train loss 3.61 on epoch=199
06/24/2022 12:09:28 - INFO - __main__ - Global step 400 Train loss 3.71 Classification-F1 0.0 on epoch=199
06/24/2022 12:09:29 - INFO - __main__ - Step 410 Global step 410 Train loss 3.58 on epoch=204
06/24/2022 12:09:30 - INFO - __main__ - Step 420 Global step 420 Train loss 3.46 on epoch=209
06/24/2022 12:09:31 - INFO - __main__ - Step 430 Global step 430 Train loss 3.59 on epoch=214
06/24/2022 12:09:33 - INFO - __main__ - Step 440 Global step 440 Train loss 3.42 on epoch=219
06/24/2022 12:09:34 - INFO - __main__ - Step 450 Global step 450 Train loss 3.52 on epoch=224
06/24/2022 12:09:41 - INFO - __main__ - Global step 450 Train loss 3.51 Classification-F1 0.007352941176470588 on epoch=224
06/24/2022 12:09:41 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.007352941176470588 on epoch=224, global_step=450
06/24/2022 12:09:42 - INFO - __main__ - Step 460 Global step 460 Train loss 3.37 on epoch=229
06/24/2022 12:09:43 - INFO - __main__ - Step 470 Global step 470 Train loss 3.39 on epoch=234
06/24/2022 12:09:45 - INFO - __main__ - Step 480 Global step 480 Train loss 3.32 on epoch=239
06/24/2022 12:09:46 - INFO - __main__ - Step 490 Global step 490 Train loss 3.12 on epoch=244
06/24/2022 12:09:47 - INFO - __main__ - Step 500 Global step 500 Train loss 3.22 on epoch=249
06/24/2022 12:09:50 - INFO - __main__ - Global step 500 Train loss 3.29 Classification-F1 0.009049773755656108 on epoch=249
06/24/2022 12:09:50 - INFO - __main__ - Saving model with best Classification-F1: 0.007352941176470588 -> 0.009049773755656108 on epoch=249, global_step=500
06/24/2022 12:09:51 - INFO - __main__ - Step 510 Global step 510 Train loss 2.99 on epoch=254
06/24/2022 12:09:52 - INFO - __main__ - Step 520 Global step 520 Train loss 2.97 on epoch=259
06/24/2022 12:09:53 - INFO - __main__ - Step 530 Global step 530 Train loss 2.82 on epoch=264
06/24/2022 12:09:54 - INFO - __main__ - Step 540 Global step 540 Train loss 2.87 on epoch=269
06/24/2022 12:09:56 - INFO - __main__ - Step 550 Global step 550 Train loss 2.80 on epoch=274
06/24/2022 12:09:58 - INFO - __main__ - Global step 550 Train loss 2.89 Classification-F1 0.16666666666666669 on epoch=274
06/24/2022 12:09:58 - INFO - __main__ - Saving model with best Classification-F1: 0.009049773755656108 -> 0.16666666666666669 on epoch=274, global_step=550
06/24/2022 12:09:59 - INFO - __main__ - Step 560 Global step 560 Train loss 2.81 on epoch=279
06/24/2022 12:10:00 - INFO - __main__ - Step 570 Global step 570 Train loss 2.64 on epoch=284
06/24/2022 12:10:02 - INFO - __main__ - Step 580 Global step 580 Train loss 2.53 on epoch=289
06/24/2022 12:10:03 - INFO - __main__ - Step 590 Global step 590 Train loss 2.57 on epoch=294
06/24/2022 12:10:04 - INFO - __main__ - Step 600 Global step 600 Train loss 2.57 on epoch=299
06/24/2022 12:10:10 - INFO - __main__ - Global step 600 Train loss 2.62 Classification-F1 0.3992490613266583 on epoch=299
06/24/2022 12:10:10 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666669 -> 0.3992490613266583 on epoch=299, global_step=600
06/24/2022 12:10:11 - INFO - __main__ - Step 610 Global step 610 Train loss 2.40 on epoch=304
06/24/2022 12:10:12 - INFO - __main__ - Step 620 Global step 620 Train loss 2.34 on epoch=309
06/24/2022 12:10:14 - INFO - __main__ - Step 630 Global step 630 Train loss 2.32 on epoch=314
06/24/2022 12:10:15 - INFO - __main__ - Step 640 Global step 640 Train loss 2.21 on epoch=319
06/24/2022 12:10:16 - INFO - __main__ - Step 650 Global step 650 Train loss 2.28 on epoch=324
06/24/2022 12:10:18 - INFO - __main__ - Global step 650 Train loss 2.31 Classification-F1 0.4589371980676329 on epoch=324
06/24/2022 12:10:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4589371980676329 on epoch=324, global_step=650
06/24/2022 12:10:19 - INFO - __main__ - Step 660 Global step 660 Train loss 2.36 on epoch=329
06/24/2022 12:10:20 - INFO - __main__ - Step 670 Global step 670 Train loss 2.28 on epoch=334
06/24/2022 12:10:21 - INFO - __main__ - Step 680 Global step 680 Train loss 2.07 on epoch=339
06/24/2022 12:10:23 - INFO - __main__ - Step 690 Global step 690 Train loss 2.02 on epoch=344
06/24/2022 12:10:24 - INFO - __main__ - Step 700 Global step 700 Train loss 1.97 on epoch=349
06/24/2022 12:10:26 - INFO - __main__ - Global step 700 Train loss 2.14 Classification-F1 0.3333333333333333 on epoch=349
06/24/2022 12:10:27 - INFO - __main__ - Step 710 Global step 710 Train loss 1.96 on epoch=354
06/24/2022 12:10:28 - INFO - __main__ - Step 720 Global step 720 Train loss 1.92 on epoch=359
06/24/2022 12:10:29 - INFO - __main__ - Step 730 Global step 730 Train loss 1.94 on epoch=364
06/24/2022 12:10:30 - INFO - __main__ - Step 740 Global step 740 Train loss 1.94 on epoch=369
06/24/2022 12:10:32 - INFO - __main__ - Step 750 Global step 750 Train loss 1.81 on epoch=374
06/24/2022 12:10:32 - INFO - __main__ - Global step 750 Train loss 1.91 Classification-F1 0.41700404858299595 on epoch=374
06/24/2022 12:10:33 - INFO - __main__ - Step 760 Global step 760 Train loss 1.77 on epoch=379
06/24/2022 12:10:34 - INFO - __main__ - Step 770 Global step 770 Train loss 1.74 on epoch=384
06/24/2022 12:10:36 - INFO - __main__ - Step 780 Global step 780 Train loss 1.85 on epoch=389
06/24/2022 12:10:37 - INFO - __main__ - Step 790 Global step 790 Train loss 1.67 on epoch=394
06/24/2022 12:10:38 - INFO - __main__ - Step 800 Global step 800 Train loss 1.64 on epoch=399
06/24/2022 12:10:39 - INFO - __main__ - Global step 800 Train loss 1.73 Classification-F1 0.3992490613266583 on epoch=399
06/24/2022 12:10:40 - INFO - __main__ - Step 810 Global step 810 Train loss 1.68 on epoch=404
06/24/2022 12:10:42 - INFO - __main__ - Step 820 Global step 820 Train loss 1.76 on epoch=409
06/24/2022 12:10:43 - INFO - __main__ - Step 830 Global step 830 Train loss 1.67 on epoch=414
06/24/2022 12:10:44 - INFO - __main__ - Step 840 Global step 840 Train loss 1.59 on epoch=419
06/24/2022 12:10:45 - INFO - __main__ - Step 850 Global step 850 Train loss 1.42 on epoch=424
06/24/2022 12:10:46 - INFO - __main__ - Global step 850 Train loss 1.62 Classification-F1 0.3333333333333333 on epoch=424
06/24/2022 12:10:47 - INFO - __main__ - Step 860 Global step 860 Train loss 1.49 on epoch=429
06/24/2022 12:10:48 - INFO - __main__ - Step 870 Global step 870 Train loss 1.47 on epoch=434
06/24/2022 12:10:49 - INFO - __main__ - Step 880 Global step 880 Train loss 1.50 on epoch=439
06/24/2022 12:10:50 - INFO - __main__ - Step 890 Global step 890 Train loss 1.31 on epoch=444
06/24/2022 12:10:52 - INFO - __main__ - Step 900 Global step 900 Train loss 1.31 on epoch=449
06/24/2022 12:10:52 - INFO - __main__ - Global step 900 Train loss 1.42 Classification-F1 0.5307917888563051 on epoch=449
06/24/2022 12:10:52 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5307917888563051 on epoch=449, global_step=900
06/24/2022 12:10:53 - INFO - __main__ - Step 910 Global step 910 Train loss 1.11 on epoch=454
06/24/2022 12:10:54 - INFO - __main__ - Step 920 Global step 920 Train loss 1.30 on epoch=459
06/24/2022 12:10:56 - INFO - __main__ - Step 930 Global step 930 Train loss 1.31 on epoch=464
06/24/2022 12:10:57 - INFO - __main__ - Step 940 Global step 940 Train loss 1.28 on epoch=469
06/24/2022 12:10:58 - INFO - __main__ - Step 950 Global step 950 Train loss 1.28 on epoch=474
06/24/2022 12:10:58 - INFO - __main__ - Global step 950 Train loss 1.26 Classification-F1 0.3043478260869565 on epoch=474
06/24/2022 12:11:00 - INFO - __main__ - Step 960 Global step 960 Train loss 1.24 on epoch=479
06/24/2022 12:11:01 - INFO - __main__ - Step 970 Global step 970 Train loss 1.28 on epoch=484
06/24/2022 12:11:02 - INFO - __main__ - Step 980 Global step 980 Train loss 1.21 on epoch=489
06/24/2022 12:11:03 - INFO - __main__ - Step 990 Global step 990 Train loss 1.20 on epoch=494
06/24/2022 12:11:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.23 on epoch=499
06/24/2022 12:11:05 - INFO - __main__ - Global step 1000 Train loss 1.23 Classification-F1 0.464039408866995 on epoch=499
06/24/2022 12:11:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.10 on epoch=504
06/24/2022 12:11:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.20 on epoch=509
06/24/2022 12:11:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.13 on epoch=514
06/24/2022 12:11:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.05 on epoch=519
06/24/2022 12:11:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.06 on epoch=524
06/24/2022 12:11:11 - INFO - __main__ - Global step 1050 Train loss 1.11 Classification-F1 0.49090909090909085 on epoch=524
06/24/2022 12:11:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.95 on epoch=529
06/24/2022 12:11:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.07 on epoch=534
06/24/2022 12:11:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.02 on epoch=539
06/24/2022 12:11:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.05 on epoch=544
06/24/2022 12:11:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.93 on epoch=549
06/24/2022 12:11:18 - INFO - __main__ - Global step 1100 Train loss 1.00 Classification-F1 0.3333333333333333 on epoch=549
06/24/2022 12:11:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.92 on epoch=554
06/24/2022 12:11:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.93 on epoch=559
06/24/2022 12:11:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.99 on epoch=564
06/24/2022 12:11:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.90 on epoch=569
06/24/2022 12:11:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.88 on epoch=574
06/24/2022 12:11:24 - INFO - __main__ - Global step 1150 Train loss 0.92 Classification-F1 0.46843853820598 on epoch=574
06/24/2022 12:11:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.91 on epoch=579
06/24/2022 12:11:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.95 on epoch=584
06/24/2022 12:11:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.98 on epoch=589
06/24/2022 12:11:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.79 on epoch=594
06/24/2022 12:11:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.93 on epoch=599
06/24/2022 12:11:30 - INFO - __main__ - Global step 1200 Train loss 0.91 Classification-F1 0.5151515151515151 on epoch=599
06/24/2022 12:11:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.85 on epoch=604
06/24/2022 12:11:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.88 on epoch=609
06/24/2022 12:11:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.76 on epoch=614
06/24/2022 12:11:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.80 on epoch=619
06/24/2022 12:11:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.87 on epoch=624
06/24/2022 12:11:37 - INFO - __main__ - Global step 1250 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=624
06/24/2022 12:11:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.78 on epoch=629
06/24/2022 12:11:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.86 on epoch=634
06/24/2022 12:11:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.68 on epoch=639
06/24/2022 12:11:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.82 on epoch=644
06/24/2022 12:11:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.75 on epoch=649
06/24/2022 12:11:43 - INFO - __main__ - Global step 1300 Train loss 0.78 Classification-F1 0.3043478260869565 on epoch=649
06/24/2022 12:11:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.70 on epoch=654
06/24/2022 12:11:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.79 on epoch=659
06/24/2022 12:11:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.72 on epoch=664
06/24/2022 12:11:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.76 on epoch=669
06/24/2022 12:11:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.73 on epoch=674
06/24/2022 12:11:49 - INFO - __main__ - Global step 1350 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=674
06/24/2022 12:11:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.67 on epoch=679
06/24/2022 12:11:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.81 on epoch=684
06/24/2022 12:11:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.59 on epoch=689
06/24/2022 12:11:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.78 on epoch=694
06/24/2022 12:11:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.76 on epoch=699
06/24/2022 12:11:56 - INFO - __main__ - Global step 1400 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=699
06/24/2022 12:11:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.68 on epoch=704
06/24/2022 12:11:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.65 on epoch=709
06/24/2022 12:11:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.74 on epoch=714
06/24/2022 12:12:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.71 on epoch=719
06/24/2022 12:12:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.62 on epoch=724
06/24/2022 12:12:02 - INFO - __main__ - Global step 1450 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=724
06/24/2022 12:12:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.66 on epoch=729
06/24/2022 12:12:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.69 on epoch=734
06/24/2022 12:12:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.73 on epoch=739
06/24/2022 12:12:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.66 on epoch=744
06/24/2022 12:12:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.65 on epoch=749
06/24/2022 12:12:09 - INFO - __main__ - Global step 1500 Train loss 0.68 Classification-F1 0.3992490613266583 on epoch=749
06/24/2022 12:12:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.59 on epoch=754
06/24/2022 12:12:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.74 on epoch=759
06/24/2022 12:12:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.63 on epoch=764
06/24/2022 12:12:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.59 on epoch=769
06/24/2022 12:12:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.67 on epoch=774
06/24/2022 12:12:15 - INFO - __main__ - Global step 1550 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=774
06/24/2022 12:12:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.56 on epoch=779
06/24/2022 12:12:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.68 on epoch=784
06/24/2022 12:12:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.61 on epoch=789
06/24/2022 12:12:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.56 on epoch=794
06/24/2022 12:12:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.66 on epoch=799
06/24/2022 12:12:21 - INFO - __main__ - Global step 1600 Train loss 0.61 Classification-F1 0.3191489361702127 on epoch=799
06/24/2022 12:12:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.67 on epoch=804
06/24/2022 12:12:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.51 on epoch=809
06/24/2022 12:12:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.63 on epoch=814
06/24/2022 12:12:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.73 on epoch=819
06/24/2022 12:12:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.56 on epoch=824
06/24/2022 12:12:28 - INFO - __main__ - Global step 1650 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=824
06/24/2022 12:12:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.55 on epoch=829
06/24/2022 12:12:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.73 on epoch=834
06/24/2022 12:12:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.64 on epoch=839
06/24/2022 12:12:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.57 on epoch=844
06/24/2022 12:12:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.67 on epoch=849
06/24/2022 12:12:34 - INFO - __main__ - Global step 1700 Train loss 0.63 Classification-F1 0.3992490613266583 on epoch=849
06/24/2022 12:12:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.68 on epoch=854
06/24/2022 12:12:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.66 on epoch=859
06/24/2022 12:12:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.58 on epoch=864
06/24/2022 12:12:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.55 on epoch=869
06/24/2022 12:12:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.64 on epoch=874
06/24/2022 12:12:40 - INFO - __main__ - Global step 1750 Train loss 0.62 Classification-F1 0.3816425120772947 on epoch=874
06/24/2022 12:12:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.65 on epoch=879
06/24/2022 12:12:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.56 on epoch=884
06/24/2022 12:12:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.57 on epoch=889
06/24/2022 12:12:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.63 on epoch=894
06/24/2022 12:12:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.59 on epoch=899
06/24/2022 12:12:47 - INFO - __main__ - Global step 1800 Train loss 0.60 Classification-F1 0.5636363636363637 on epoch=899
06/24/2022 12:12:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5307917888563051 -> 0.5636363636363637 on epoch=899, global_step=1800
06/24/2022 12:12:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.61 on epoch=904
06/24/2022 12:12:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.69 on epoch=909
06/24/2022 12:12:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.57 on epoch=914
06/24/2022 12:12:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.57 on epoch=919
06/24/2022 12:12:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.58 on epoch=924
06/24/2022 12:12:53 - INFO - __main__ - Global step 1850 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=924
06/24/2022 12:12:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.56 on epoch=929
06/24/2022 12:12:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.65 on epoch=934
06/24/2022 12:12:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.61 on epoch=939
06/24/2022 12:12:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.61 on epoch=944
06/24/2022 12:12:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.63 on epoch=949
06/24/2022 12:13:00 - INFO - __main__ - Global step 1900 Train loss 0.61 Classification-F1 0.3043478260869565 on epoch=949
06/24/2022 12:13:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.64 on epoch=954
06/24/2022 12:13:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.58 on epoch=959
06/24/2022 12:13:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.63 on epoch=964
06/24/2022 12:13:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.59 on epoch=969
06/24/2022 12:13:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.60 on epoch=974
06/24/2022 12:13:06 - INFO - __main__ - Global step 1950 Train loss 0.61 Classification-F1 0.3191489361702127 on epoch=974
06/24/2022 12:13:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.60 on epoch=979
06/24/2022 12:13:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.53 on epoch=984
06/24/2022 12:13:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.50 on epoch=989
06/24/2022 12:13:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.57 on epoch=994
06/24/2022 12:13:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.60 on epoch=999
06/24/2022 12:13:12 - INFO - __main__ - Global step 2000 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=999
06/24/2022 12:13:12 - INFO - __main__ - save last model!
06/24/2022 12:13:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 12:13:13 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 12:13:13 - INFO - __main__ - Printing 3 examples
06/24/2022 12:13:13 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 12:13:13 - INFO - __main__ - ['0']
06/24/2022 12:13:13 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 12:13:13 - INFO - __main__ - ['1']
06/24/2022 12:13:13 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 12:13:13 - INFO - __main__ - ['1']
06/24/2022 12:13:13 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:13:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:13:13 - INFO - __main__ - Printing 3 examples
06/24/2022 12:13:13 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 12:13:13 - INFO - __main__ - ['1']
06/24/2022 12:13:13 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 12:13:13 - INFO - __main__ - ['1']
06/24/2022 12:13:13 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 12:13:13 - INFO - __main__ - ['1']
06/24/2022 12:13:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 12:13:13 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:13:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:13:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:13:13 - INFO - __main__ - Printing 3 examples
06/24/2022 12:13:13 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 12:13:13 - INFO - __main__ - ['1']
06/24/2022 12:13:13 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 12:13:13 - INFO - __main__ - ['1']
06/24/2022 12:13:13 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 12:13:13 - INFO - __main__ - ['1']
06/24/2022 12:13:13 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:13:13 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:13:13 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:13:17 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:13:24 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:13:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:13:24 - INFO - __main__ - Starting training!
06/24/2022 12:13:24 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 12:14:47 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_42_0.4_8_predictions.txt
06/24/2022 12:14:48 - INFO - __main__ - Classification-F1 on test data: 0.3102
06/24/2022 12:14:48 - INFO - __main__ - prefix=paws_16_42, lr=0.4, bsz=8, dev_performance=0.5636363636363637, test_performance=0.31021538354369876
06/24/2022 12:14:48 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.3, bsz=8 ...
06/24/2022 12:14:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:14:49 - INFO - __main__ - Printing 3 examples
06/24/2022 12:14:49 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 12:14:49 - INFO - __main__ - ['1']
06/24/2022 12:14:49 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 12:14:49 - INFO - __main__ - ['1']
06/24/2022 12:14:49 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 12:14:49 - INFO - __main__ - ['1']
06/24/2022 12:14:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:14:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:14:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:14:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:14:49 - INFO - __main__ - Printing 3 examples
06/24/2022 12:14:49 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 12:14:49 - INFO - __main__ - ['1']
06/24/2022 12:14:49 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 12:14:49 - INFO - __main__ - ['1']
06/24/2022 12:14:49 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 12:14:49 - INFO - __main__ - ['1']
06/24/2022 12:14:49 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:14:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:14:49 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:14:55 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:14:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:14:55 - INFO - __main__ - Starting training!
06/24/2022 12:14:57 - INFO - __main__ - Step 10 Global step 10 Train loss 6.00 on epoch=4
06/24/2022 12:14:58 - INFO - __main__ - Step 20 Global step 20 Train loss 5.93 on epoch=9
06/24/2022 12:14:59 - INFO - __main__ - Step 30 Global step 30 Train loss 5.96 on epoch=14
06/24/2022 12:15:00 - INFO - __main__ - Step 40 Global step 40 Train loss 5.95 on epoch=19
06/24/2022 12:15:01 - INFO - __main__ - Step 50 Global step 50 Train loss 5.86 on epoch=24
06/24/2022 12:15:02 - INFO - __main__ - Global step 50 Train loss 5.94 Classification-F1 0.0 on epoch=24
06/24/2022 12:15:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 12:15:03 - INFO - __main__ - Step 60 Global step 60 Train loss 5.86 on epoch=29
06/24/2022 12:15:05 - INFO - __main__ - Step 70 Global step 70 Train loss 5.92 on epoch=34
06/24/2022 12:15:06 - INFO - __main__ - Step 80 Global step 80 Train loss 5.95 on epoch=39
06/24/2022 12:15:07 - INFO - __main__ - Step 90 Global step 90 Train loss 5.86 on epoch=44
06/24/2022 12:15:08 - INFO - __main__ - Step 100 Global step 100 Train loss 5.84 on epoch=49
06/24/2022 12:15:14 - INFO - __main__ - Global step 100 Train loss 5.89 Classification-F1 0.0 on epoch=49
06/24/2022 12:15:15 - INFO - __main__ - Step 110 Global step 110 Train loss 5.92 on epoch=54
06/24/2022 12:15:16 - INFO - __main__ - Step 120 Global step 120 Train loss 5.79 on epoch=59
06/24/2022 12:15:17 - INFO - __main__ - Step 130 Global step 130 Train loss 5.77 on epoch=64
06/24/2022 12:15:19 - INFO - __main__ - Step 140 Global step 140 Train loss 5.76 on epoch=69
06/24/2022 12:15:20 - INFO - __main__ - Step 150 Global step 150 Train loss 5.69 on epoch=74
06/24/2022 12:15:23 - INFO - __main__ - Global step 150 Train loss 5.79 Classification-F1 0.0 on epoch=74
06/24/2022 12:15:24 - INFO - __main__ - Step 160 Global step 160 Train loss 5.70 on epoch=79
06/24/2022 12:15:26 - INFO - __main__ - Step 170 Global step 170 Train loss 5.74 on epoch=84
06/24/2022 12:15:27 - INFO - __main__ - Step 180 Global step 180 Train loss 5.77 on epoch=89
06/24/2022 12:15:28 - INFO - __main__ - Step 190 Global step 190 Train loss 5.74 on epoch=94
06/24/2022 12:15:29 - INFO - __main__ - Step 200 Global step 200 Train loss 5.69 on epoch=99
06/24/2022 12:15:30 - INFO - __main__ - Global step 200 Train loss 5.73 Classification-F1 0.0 on epoch=99
06/24/2022 12:15:32 - INFO - __main__ - Step 210 Global step 210 Train loss 5.70 on epoch=104
06/24/2022 12:15:33 - INFO - __main__ - Step 220 Global step 220 Train loss 5.60 on epoch=109
06/24/2022 12:15:34 - INFO - __main__ - Step 230 Global step 230 Train loss 5.67 on epoch=114
06/24/2022 12:15:35 - INFO - __main__ - Step 240 Global step 240 Train loss 5.66 on epoch=119
06/24/2022 12:15:36 - INFO - __main__ - Step 250 Global step 250 Train loss 5.61 on epoch=124
06/24/2022 12:15:39 - INFO - __main__ - Global step 250 Train loss 5.65 Classification-F1 0.0 on epoch=124
06/24/2022 12:15:40 - INFO - __main__ - Step 260 Global step 260 Train loss 5.66 on epoch=129
06/24/2022 12:15:41 - INFO - __main__ - Step 270 Global step 270 Train loss 5.53 on epoch=134
06/24/2022 12:15:42 - INFO - __main__ - Step 280 Global step 280 Train loss 5.53 on epoch=139
06/24/2022 12:15:43 - INFO - __main__ - Step 290 Global step 290 Train loss 5.56 on epoch=144
06/24/2022 12:15:45 - INFO - __main__ - Step 300 Global step 300 Train loss 5.47 on epoch=149
06/24/2022 12:15:51 - INFO - __main__ - Global step 300 Train loss 5.55 Classification-F1 0.0 on epoch=149
06/24/2022 12:15:52 - INFO - __main__ - Step 310 Global step 310 Train loss 5.34 on epoch=154
06/24/2022 12:15:53 - INFO - __main__ - Step 320 Global step 320 Train loss 5.32 on epoch=159
06/24/2022 12:15:55 - INFO - __main__ - Step 330 Global step 330 Train loss 5.13 on epoch=164
06/24/2022 12:15:56 - INFO - __main__ - Step 340 Global step 340 Train loss 5.06 on epoch=169
06/24/2022 12:15:57 - INFO - __main__ - Step 350 Global step 350 Train loss 4.79 on epoch=174
06/24/2022 12:16:03 - INFO - __main__ - Global step 350 Train loss 5.13 Classification-F1 0.0 on epoch=174
06/24/2022 12:16:04 - INFO - __main__ - Step 360 Global step 360 Train loss 4.76 on epoch=179
06/24/2022 12:16:05 - INFO - __main__ - Step 370 Global step 370 Train loss 4.49 on epoch=184
06/24/2022 12:16:06 - INFO - __main__ - Step 380 Global step 380 Train loss 4.33 on epoch=189
06/24/2022 12:16:08 - INFO - __main__ - Step 390 Global step 390 Train loss 4.33 on epoch=194
06/24/2022 12:16:09 - INFO - __main__ - Step 400 Global step 400 Train loss 4.14 on epoch=199
06/24/2022 12:16:19 - INFO - __main__ - Global step 400 Train loss 4.41 Classification-F1 0.0 on epoch=199
06/24/2022 12:16:20 - INFO - __main__ - Step 410 Global step 410 Train loss 4.23 on epoch=204
06/24/2022 12:16:21 - INFO - __main__ - Step 420 Global step 420 Train loss 4.01 on epoch=209
06/24/2022 12:16:22 - INFO - __main__ - Step 430 Global step 430 Train loss 3.95 on epoch=214
06/24/2022 12:16:24 - INFO - __main__ - Step 440 Global step 440 Train loss 3.98 on epoch=219
06/24/2022 12:16:25 - INFO - __main__ - Step 450 Global step 450 Train loss 3.87 on epoch=224
06/24/2022 12:16:35 - INFO - __main__ - Global step 450 Train loss 4.01 Classification-F1 0.0 on epoch=224
06/24/2022 12:16:36 - INFO - __main__ - Step 460 Global step 460 Train loss 3.74 on epoch=229
06/24/2022 12:16:37 - INFO - __main__ - Step 470 Global step 470 Train loss 3.84 on epoch=234
06/24/2022 12:16:39 - INFO - __main__ - Step 480 Global step 480 Train loss 3.76 on epoch=239
06/24/2022 12:16:40 - INFO - __main__ - Step 490 Global step 490 Train loss 3.82 on epoch=244
06/24/2022 12:16:41 - INFO - __main__ - Step 500 Global step 500 Train loss 4.13 on epoch=249
06/24/2022 12:16:46 - INFO - __main__ - Global step 500 Train loss 3.86 Classification-F1 0.0 on epoch=249
06/24/2022 12:16:47 - INFO - __main__ - Step 510 Global step 510 Train loss 3.88 on epoch=254
06/24/2022 12:16:49 - INFO - __main__ - Step 520 Global step 520 Train loss 3.86 on epoch=259
06/24/2022 12:16:50 - INFO - __main__ - Step 530 Global step 530 Train loss 3.75 on epoch=264
06/24/2022 12:16:51 - INFO - __main__ - Step 540 Global step 540 Train loss 3.77 on epoch=269
06/24/2022 12:16:52 - INFO - __main__ - Step 550 Global step 550 Train loss 3.72 on epoch=274
06/24/2022 12:16:59 - INFO - __main__ - Global step 550 Train loss 3.80 Classification-F1 0.0 on epoch=274
06/24/2022 12:17:00 - INFO - __main__ - Step 560 Global step 560 Train loss 3.74 on epoch=279
06/24/2022 12:17:02 - INFO - __main__ - Step 570 Global step 570 Train loss 3.54 on epoch=284
06/24/2022 12:17:03 - INFO - __main__ - Step 580 Global step 580 Train loss 3.48 on epoch=289
06/24/2022 12:17:04 - INFO - __main__ - Step 590 Global step 590 Train loss 3.45 on epoch=294
06/24/2022 12:17:05 - INFO - __main__ - Step 600 Global step 600 Train loss 3.56 on epoch=299
06/24/2022 12:17:09 - INFO - __main__ - Global step 600 Train loss 3.55 Classification-F1 0.0 on epoch=299
06/24/2022 12:17:10 - INFO - __main__ - Step 610 Global step 610 Train loss 3.41 on epoch=304
06/24/2022 12:17:11 - INFO - __main__ - Step 620 Global step 620 Train loss 3.32 on epoch=309
06/24/2022 12:17:12 - INFO - __main__ - Step 630 Global step 630 Train loss 3.33 on epoch=314
06/24/2022 12:17:14 - INFO - __main__ - Step 640 Global step 640 Train loss 3.32 on epoch=319
06/24/2022 12:17:15 - INFO - __main__ - Step 650 Global step 650 Train loss 3.32 on epoch=324
06/24/2022 12:17:17 - INFO - __main__ - Global step 650 Train loss 3.34 Classification-F1 0.0 on epoch=324
06/24/2022 12:17:18 - INFO - __main__ - Step 660 Global step 660 Train loss 3.27 on epoch=329
06/24/2022 12:17:20 - INFO - __main__ - Step 670 Global step 670 Train loss 3.15 on epoch=334
06/24/2022 12:17:21 - INFO - __main__ - Step 680 Global step 680 Train loss 3.09 on epoch=339
06/24/2022 12:17:22 - INFO - __main__ - Step 690 Global step 690 Train loss 3.06 on epoch=344
06/24/2022 12:17:23 - INFO - __main__ - Step 700 Global step 700 Train loss 3.12 on epoch=349
06/24/2022 12:17:26 - INFO - __main__ - Global step 700 Train loss 3.14 Classification-F1 0.013986013986013986 on epoch=349
06/24/2022 12:17:26 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.013986013986013986 on epoch=349, global_step=700
06/24/2022 12:17:27 - INFO - __main__ - Step 710 Global step 710 Train loss 3.08 on epoch=354
06/24/2022 12:17:28 - INFO - __main__ - Step 720 Global step 720 Train loss 3.00 on epoch=359
06/24/2022 12:17:29 - INFO - __main__ - Step 730 Global step 730 Train loss 2.96 on epoch=364
06/24/2022 12:17:31 - INFO - __main__ - Step 740 Global step 740 Train loss 2.91 on epoch=369
06/24/2022 12:17:32 - INFO - __main__ - Step 750 Global step 750 Train loss 2.94 on epoch=374
06/24/2022 12:17:38 - INFO - __main__ - Global step 750 Train loss 2.98 Classification-F1 0.03726708074534161 on epoch=374
06/24/2022 12:17:38 - INFO - __main__ - Saving model with best Classification-F1: 0.013986013986013986 -> 0.03726708074534161 on epoch=374, global_step=750
06/24/2022 12:17:39 - INFO - __main__ - Step 760 Global step 760 Train loss 3.02 on epoch=379
06/24/2022 12:17:40 - INFO - __main__ - Step 770 Global step 770 Train loss 2.87 on epoch=384
06/24/2022 12:17:41 - INFO - __main__ - Step 780 Global step 780 Train loss 2.90 on epoch=389
06/24/2022 12:17:43 - INFO - __main__ - Step 790 Global step 790 Train loss 2.75 on epoch=394
06/24/2022 12:17:44 - INFO - __main__ - Step 800 Global step 800 Train loss 2.89 on epoch=399
06/24/2022 12:17:46 - INFO - __main__ - Global step 800 Train loss 2.89 Classification-F1 0.1388888888888889 on epoch=399
06/24/2022 12:17:46 - INFO - __main__ - Saving model with best Classification-F1: 0.03726708074534161 -> 0.1388888888888889 on epoch=399, global_step=800
06/24/2022 12:17:47 - INFO - __main__ - Step 810 Global step 810 Train loss 2.77 on epoch=404
06/24/2022 12:17:48 - INFO - __main__ - Step 820 Global step 820 Train loss 2.63 on epoch=409
06/24/2022 12:17:50 - INFO - __main__ - Step 830 Global step 830 Train loss 2.73 on epoch=414
06/24/2022 12:17:51 - INFO - __main__ - Step 840 Global step 840 Train loss 2.70 on epoch=419
06/24/2022 12:17:52 - INFO - __main__ - Step 850 Global step 850 Train loss 2.58 on epoch=424
06/24/2022 12:17:54 - INFO - __main__ - Global step 850 Train loss 2.68 Classification-F1 0.3816425120772947 on epoch=424
06/24/2022 12:17:54 - INFO - __main__ - Saving model with best Classification-F1: 0.1388888888888889 -> 0.3816425120772947 on epoch=424, global_step=850
06/24/2022 12:17:55 - INFO - __main__ - Step 860 Global step 860 Train loss 2.70 on epoch=429
06/24/2022 12:17:56 - INFO - __main__ - Step 870 Global step 870 Train loss 2.55 on epoch=434
06/24/2022 12:17:58 - INFO - __main__ - Step 880 Global step 880 Train loss 2.56 on epoch=439
06/24/2022 12:17:59 - INFO - __main__ - Step 890 Global step 890 Train loss 2.51 on epoch=444
06/24/2022 12:18:00 - INFO - __main__ - Step 900 Global step 900 Train loss 2.46 on epoch=449
06/24/2022 12:18:02 - INFO - __main__ - Global step 900 Train loss 2.55 Classification-F1 0.3191489361702127 on epoch=449
06/24/2022 12:18:03 - INFO - __main__ - Step 910 Global step 910 Train loss 2.44 on epoch=454
06/24/2022 12:18:05 - INFO - __main__ - Step 920 Global step 920 Train loss 2.41 on epoch=459
06/24/2022 12:18:06 - INFO - __main__ - Step 930 Global step 930 Train loss 2.47 on epoch=464
06/24/2022 12:18:07 - INFO - __main__ - Step 940 Global step 940 Train loss 2.58 on epoch=469
06/24/2022 12:18:08 - INFO - __main__ - Step 950 Global step 950 Train loss 2.29 on epoch=474
06/24/2022 12:18:10 - INFO - __main__ - Global step 950 Train loss 2.44 Classification-F1 0.3764102564102564 on epoch=474
06/24/2022 12:18:12 - INFO - __main__ - Step 960 Global step 960 Train loss 2.35 on epoch=479
06/24/2022 12:18:13 - INFO - __main__ - Step 970 Global step 970 Train loss 2.37 on epoch=484
06/24/2022 12:18:14 - INFO - __main__ - Step 980 Global step 980 Train loss 2.23 on epoch=489
06/24/2022 12:18:15 - INFO - __main__ - Step 990 Global step 990 Train loss 2.25 on epoch=494
06/24/2022 12:18:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.26 on epoch=499
06/24/2022 12:18:18 - INFO - __main__ - Global step 1000 Train loss 2.29 Classification-F1 0.5307917888563051 on epoch=499
06/24/2022 12:18:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.5307917888563051 on epoch=499, global_step=1000
06/24/2022 12:18:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.32 on epoch=504
06/24/2022 12:18:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.23 on epoch=509
06/24/2022 12:18:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.12 on epoch=514
06/24/2022 12:18:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.04 on epoch=519
06/24/2022 12:18:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.97 on epoch=524
06/24/2022 12:18:25 - INFO - __main__ - Global step 1050 Train loss 2.14 Classification-F1 0.3333333333333333 on epoch=524
06/24/2022 12:18:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.06 on epoch=529
06/24/2022 12:18:28 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.06 on epoch=534
06/24/2022 12:18:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.98 on epoch=539
06/24/2022 12:18:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.98 on epoch=544
06/24/2022 12:18:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.10 on epoch=549
06/24/2022 12:18:32 - INFO - __main__ - Global step 1100 Train loss 2.03 Classification-F1 0.4589371980676329 on epoch=549
06/24/2022 12:18:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.94 on epoch=554
06/24/2022 12:18:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.86 on epoch=559
06/24/2022 12:18:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.79 on epoch=564
06/24/2022 12:18:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.88 on epoch=569
06/24/2022 12:18:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.75 on epoch=574
06/24/2022 12:18:39 - INFO - __main__ - Global step 1150 Train loss 1.85 Classification-F1 0.49090909090909085 on epoch=574
06/24/2022 12:18:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.98 on epoch=579
06/24/2022 12:18:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.80 on epoch=584
06/24/2022 12:18:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.77 on epoch=589
06/24/2022 12:18:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.74 on epoch=594
06/24/2022 12:18:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.60 on epoch=599
06/24/2022 12:18:45 - INFO - __main__ - Global step 1200 Train loss 1.78 Classification-F1 0.3191489361702127 on epoch=599
06/24/2022 12:18:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.77 on epoch=604
06/24/2022 12:18:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.64 on epoch=609
06/24/2022 12:18:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.69 on epoch=614
06/24/2022 12:18:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.65 on epoch=619
06/24/2022 12:18:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.60 on epoch=624
06/24/2022 12:18:52 - INFO - __main__ - Global step 1250 Train loss 1.67 Classification-F1 0.3333333333333333 on epoch=624
06/24/2022 12:18:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.58 on epoch=629
06/24/2022 12:18:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.57 on epoch=634
06/24/2022 12:18:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.43 on epoch=639
06/24/2022 12:18:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.54 on epoch=644
06/24/2022 12:18:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.58 on epoch=649
06/24/2022 12:18:58 - INFO - __main__ - Global step 1300 Train loss 1.54 Classification-F1 0.3191489361702127 on epoch=649
06/24/2022 12:18:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.57 on epoch=654
06/24/2022 12:19:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.51 on epoch=659
06/24/2022 12:19:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.49 on epoch=664
06/24/2022 12:19:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.51 on epoch=669
06/24/2022 12:19:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.42 on epoch=674
06/24/2022 12:19:05 - INFO - __main__ - Global step 1350 Train loss 1.50 Classification-F1 0.3333333333333333 on epoch=674
06/24/2022 12:19:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.38 on epoch=679
06/24/2022 12:19:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.38 on epoch=684
06/24/2022 12:19:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.32 on epoch=689
06/24/2022 12:19:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.61 on epoch=694
06/24/2022 12:19:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.35 on epoch=699
06/24/2022 12:19:11 - INFO - __main__ - Global step 1400 Train loss 1.41 Classification-F1 0.3333333333333333 on epoch=699
06/24/2022 12:19:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.22 on epoch=704
06/24/2022 12:19:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.35 on epoch=709
06/24/2022 12:19:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.44 on epoch=714
06/24/2022 12:19:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.33 on epoch=719
06/24/2022 12:19:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.32 on epoch=724
06/24/2022 12:19:17 - INFO - __main__ - Global step 1450 Train loss 1.33 Classification-F1 0.3333333333333333 on epoch=724
06/24/2022 12:19:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.12 on epoch=729
06/24/2022 12:19:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.29 on epoch=734
06/24/2022 12:19:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.21 on epoch=739
06/24/2022 12:19:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.18 on epoch=744
06/24/2022 12:19:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.28 on epoch=749
06/24/2022 12:19:24 - INFO - __main__ - Global step 1500 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=749
06/24/2022 12:19:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.29 on epoch=754
06/24/2022 12:19:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.20 on epoch=759
06/24/2022 12:19:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.14 on epoch=764
06/24/2022 12:19:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.07 on epoch=769
06/24/2022 12:19:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.14 on epoch=774
06/24/2022 12:19:30 - INFO - __main__ - Global step 1550 Train loss 1.17 Classification-F1 0.3333333333333333 on epoch=774
06/24/2022 12:19:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.25 on epoch=779
06/24/2022 12:19:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.10 on epoch=784
06/24/2022 12:19:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.18 on epoch=789
06/24/2022 12:19:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.10 on epoch=794
06/24/2022 12:19:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.20 on epoch=799
06/24/2022 12:19:37 - INFO - __main__ - Global step 1600 Train loss 1.16 Classification-F1 0.3333333333333333 on epoch=799
06/24/2022 12:19:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.04 on epoch=804
06/24/2022 12:19:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.09 on epoch=809
06/24/2022 12:19:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.03 on epoch=814
06/24/2022 12:19:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.31 on epoch=819
06/24/2022 12:19:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.23 on epoch=824
06/24/2022 12:19:43 - INFO - __main__ - Global step 1650 Train loss 1.14 Classification-F1 0.3333333333333333 on epoch=824
06/24/2022 12:19:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.26 on epoch=829
06/24/2022 12:19:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.12 on epoch=834
06/24/2022 12:19:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.11 on epoch=839
06/24/2022 12:19:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.11 on epoch=844
06/24/2022 12:19:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.09 on epoch=849
06/24/2022 12:19:49 - INFO - __main__ - Global step 1700 Train loss 1.14 Classification-F1 0.3333333333333333 on epoch=849
06/24/2022 12:19:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.04 on epoch=854
06/24/2022 12:19:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.23 on epoch=859
06/24/2022 12:19:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.08 on epoch=864
06/24/2022 12:19:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.07 on epoch=869
06/24/2022 12:19:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.18 on epoch=874
06/24/2022 12:19:56 - INFO - __main__ - Global step 1750 Train loss 1.12 Classification-F1 0.3992490613266583 on epoch=874
06/24/2022 12:19:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.01 on epoch=879
06/24/2022 12:19:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.12 on epoch=884
06/24/2022 12:20:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.11 on epoch=889
06/24/2022 12:20:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.07 on epoch=894
06/24/2022 12:20:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.99 on epoch=899
06/24/2022 12:20:02 - INFO - __main__ - Global step 1800 Train loss 1.06 Classification-F1 0.3191489361702127 on epoch=899
06/24/2022 12:20:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.14 on epoch=904
06/24/2022 12:20:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.95 on epoch=909
06/24/2022 12:20:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.99 on epoch=914
06/24/2022 12:20:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.08 on epoch=919
06/24/2022 12:20:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.97 on epoch=924
06/24/2022 12:20:09 - INFO - __main__ - Global step 1850 Train loss 1.03 Classification-F1 0.3333333333333333 on epoch=924
06/24/2022 12:20:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.04 on epoch=929
06/24/2022 12:20:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.05 on epoch=934
06/24/2022 12:20:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.05 on epoch=939
06/24/2022 12:20:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.10 on epoch=944
06/24/2022 12:20:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.04 on epoch=949
06/24/2022 12:20:15 - INFO - __main__ - Global step 1900 Train loss 1.05 Classification-F1 0.3333333333333333 on epoch=949
06/24/2022 12:20:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.96 on epoch=954
06/24/2022 12:20:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.93 on epoch=959
06/24/2022 12:20:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.08 on epoch=964
06/24/2022 12:20:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.01 on epoch=969
06/24/2022 12:20:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.02 on epoch=974
06/24/2022 12:20:22 - INFO - __main__ - Global step 1950 Train loss 1.00 Classification-F1 0.3333333333333333 on epoch=974
06/24/2022 12:20:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.97 on epoch=979
06/24/2022 12:20:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.07 on epoch=984
06/24/2022 12:20:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.90 on epoch=989
06/24/2022 12:20:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.97 on epoch=994
06/24/2022 12:20:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.97 on epoch=999
06/24/2022 12:20:28 - INFO - __main__ - Global step 2000 Train loss 0.98 Classification-F1 0.3333333333333333 on epoch=999
06/24/2022 12:20:28 - INFO - __main__ - save last model!
06/24/2022 12:20:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 12:20:28 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 12:20:28 - INFO - __main__ - Printing 3 examples
06/24/2022 12:20:28 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 12:20:28 - INFO - __main__ - ['0']
06/24/2022 12:20:28 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 12:20:28 - INFO - __main__ - ['1']
06/24/2022 12:20:28 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 12:20:28 - INFO - __main__ - ['1']
06/24/2022 12:20:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:20:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:20:29 - INFO - __main__ - Printing 3 examples
06/24/2022 12:20:29 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 12:20:29 - INFO - __main__ - ['1']
06/24/2022 12:20:29 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 12:20:29 - INFO - __main__ - ['1']
06/24/2022 12:20:29 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 12:20:29 - INFO - __main__ - ['1']
06/24/2022 12:20:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 12:20:29 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:20:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:20:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:20:29 - INFO - __main__ - Printing 3 examples
06/24/2022 12:20:29 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 12:20:29 - INFO - __main__ - ['1']
06/24/2022 12:20:29 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 12:20:29 - INFO - __main__ - ['1']
06/24/2022 12:20:29 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 12:20:29 - INFO - __main__ - ['1']
06/24/2022 12:20:29 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:20:29 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:20:29 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:20:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:20:35 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:20:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:20:35 - INFO - __main__ - Starting training!
06/24/2022 12:20:40 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 12:22:05 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_42_0.3_8_predictions.txt
06/24/2022 12:22:05 - INFO - __main__ - Classification-F1 on test data: 0.3698
06/24/2022 12:22:05 - INFO - __main__ - prefix=paws_16_42, lr=0.3, bsz=8, dev_performance=0.5307917888563051, test_performance=0.36982989450851067
06/24/2022 12:22:05 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.2, bsz=8 ...
06/24/2022 12:22:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:22:06 - INFO - __main__ - Printing 3 examples
06/24/2022 12:22:06 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 12:22:06 - INFO - __main__ - ['1']
06/24/2022 12:22:06 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 12:22:06 - INFO - __main__ - ['1']
06/24/2022 12:22:06 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 12:22:06 - INFO - __main__ - ['1']
06/24/2022 12:22:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:22:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:22:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:22:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:22:06 - INFO - __main__ - Printing 3 examples
06/24/2022 12:22:06 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 12:22:06 - INFO - __main__ - ['1']
06/24/2022 12:22:06 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 12:22:06 - INFO - __main__ - ['1']
06/24/2022 12:22:06 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 12:22:06 - INFO - __main__ - ['1']
06/24/2022 12:22:06 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:22:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:22:06 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:22:12 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:22:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:22:13 - INFO - __main__ - Starting training!
06/24/2022 12:22:14 - INFO - __main__ - Step 10 Global step 10 Train loss 6.01 on epoch=4
06/24/2022 12:22:15 - INFO - __main__ - Step 20 Global step 20 Train loss 6.01 on epoch=9
06/24/2022 12:22:17 - INFO - __main__ - Step 30 Global step 30 Train loss 5.92 on epoch=14
06/24/2022 12:22:18 - INFO - __main__ - Step 40 Global step 40 Train loss 5.98 on epoch=19
06/24/2022 12:22:19 - INFO - __main__ - Step 50 Global step 50 Train loss 5.99 on epoch=24
06/24/2022 12:22:21 - INFO - __main__ - Global step 50 Train loss 5.98 Classification-F1 0.0 on epoch=24
06/24/2022 12:22:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 12:22:22 - INFO - __main__ - Step 60 Global step 60 Train loss 5.87 on epoch=29
06/24/2022 12:22:23 - INFO - __main__ - Step 70 Global step 70 Train loss 5.77 on epoch=34
06/24/2022 12:22:24 - INFO - __main__ - Step 80 Global step 80 Train loss 5.90 on epoch=39
06/24/2022 12:22:26 - INFO - __main__ - Step 90 Global step 90 Train loss 5.85 on epoch=44
06/24/2022 12:22:27 - INFO - __main__ - Step 100 Global step 100 Train loss 5.84 on epoch=49
06/24/2022 12:22:28 - INFO - __main__ - Global step 100 Train loss 5.85 Classification-F1 0.0 on epoch=49
06/24/2022 12:22:30 - INFO - __main__ - Step 110 Global step 110 Train loss 5.72 on epoch=54
06/24/2022 12:22:31 - INFO - __main__ - Step 120 Global step 120 Train loss 5.88 on epoch=59
06/24/2022 12:22:32 - INFO - __main__ - Step 130 Global step 130 Train loss 5.86 on epoch=64
06/24/2022 12:22:33 - INFO - __main__ - Step 140 Global step 140 Train loss 5.78 on epoch=69
06/24/2022 12:22:34 - INFO - __main__ - Step 150 Global step 150 Train loss 5.76 on epoch=74
06/24/2022 12:22:35 - INFO - __main__ - Global step 150 Train loss 5.80 Classification-F1 0.0 on epoch=74
06/24/2022 12:22:36 - INFO - __main__ - Step 160 Global step 160 Train loss 5.73 on epoch=79
06/24/2022 12:22:38 - INFO - __main__ - Step 170 Global step 170 Train loss 5.81 on epoch=84
06/24/2022 12:22:39 - INFO - __main__ - Step 180 Global step 180 Train loss 5.69 on epoch=89
06/24/2022 12:22:40 - INFO - __main__ - Step 190 Global step 190 Train loss 5.72 on epoch=94
06/24/2022 12:22:41 - INFO - __main__ - Step 200 Global step 200 Train loss 5.72 on epoch=99
06/24/2022 12:22:43 - INFO - __main__ - Global step 200 Train loss 5.73 Classification-F1 0.0 on epoch=99
06/24/2022 12:22:44 - INFO - __main__ - Step 210 Global step 210 Train loss 5.76 on epoch=104
06/24/2022 12:22:45 - INFO - __main__ - Step 220 Global step 220 Train loss 5.63 on epoch=109
06/24/2022 12:22:46 - INFO - __main__ - Step 230 Global step 230 Train loss 5.79 on epoch=114
06/24/2022 12:22:47 - INFO - __main__ - Step 240 Global step 240 Train loss 5.76 on epoch=119
06/24/2022 12:22:49 - INFO - __main__ - Step 250 Global step 250 Train loss 5.67 on epoch=124
06/24/2022 12:22:50 - INFO - __main__ - Global step 250 Train loss 5.72 Classification-F1 0.0 on epoch=124
06/24/2022 12:22:51 - INFO - __main__ - Step 260 Global step 260 Train loss 5.70 on epoch=129
06/24/2022 12:22:52 - INFO - __main__ - Step 270 Global step 270 Train loss 5.72 on epoch=134
06/24/2022 12:22:53 - INFO - __main__ - Step 280 Global step 280 Train loss 5.65 on epoch=139
06/24/2022 12:22:55 - INFO - __main__ - Step 290 Global step 290 Train loss 5.59 on epoch=144
06/24/2022 12:22:56 - INFO - __main__ - Step 300 Global step 300 Train loss 5.63 on epoch=149
06/24/2022 12:22:58 - INFO - __main__ - Global step 300 Train loss 5.66 Classification-F1 0.0 on epoch=149
06/24/2022 12:22:59 - INFO - __main__ - Step 310 Global step 310 Train loss 5.68 on epoch=154
06/24/2022 12:23:00 - INFO - __main__ - Step 320 Global step 320 Train loss 5.53 on epoch=159
06/24/2022 12:23:01 - INFO - __main__ - Step 330 Global step 330 Train loss 5.58 on epoch=164
06/24/2022 12:23:03 - INFO - __main__ - Step 340 Global step 340 Train loss 5.56 on epoch=169
06/24/2022 12:23:04 - INFO - __main__ - Step 350 Global step 350 Train loss 5.66 on epoch=174
06/24/2022 12:23:06 - INFO - __main__ - Global step 350 Train loss 5.60 Classification-F1 0.0 on epoch=174
06/24/2022 12:23:07 - INFO - __main__ - Step 360 Global step 360 Train loss 5.58 on epoch=179
06/24/2022 12:23:08 - INFO - __main__ - Step 370 Global step 370 Train loss 5.60 on epoch=184
06/24/2022 12:23:10 - INFO - __main__ - Step 380 Global step 380 Train loss 5.57 on epoch=189
06/24/2022 12:23:11 - INFO - __main__ - Step 390 Global step 390 Train loss 5.59 on epoch=194
06/24/2022 12:23:12 - INFO - __main__ - Step 400 Global step 400 Train loss 5.53 on epoch=199
06/24/2022 12:23:13 - INFO - __main__ - Global step 400 Train loss 5.58 Classification-F1 0.0 on epoch=199
06/24/2022 12:23:15 - INFO - __main__ - Step 410 Global step 410 Train loss 5.53 on epoch=204
06/24/2022 12:23:16 - INFO - __main__ - Step 420 Global step 420 Train loss 5.58 on epoch=209
06/24/2022 12:23:17 - INFO - __main__ - Step 430 Global step 430 Train loss 5.48 on epoch=214
06/24/2022 12:23:18 - INFO - __main__ - Step 440 Global step 440 Train loss 5.54 on epoch=219
06/24/2022 12:23:20 - INFO - __main__ - Step 450 Global step 450 Train loss 5.38 on epoch=224
06/24/2022 12:23:21 - INFO - __main__ - Global step 450 Train loss 5.50 Classification-F1 0.0 on epoch=224
06/24/2022 12:23:22 - INFO - __main__ - Step 460 Global step 460 Train loss 5.56 on epoch=229
06/24/2022 12:23:23 - INFO - __main__ - Step 470 Global step 470 Train loss 5.37 on epoch=234
06/24/2022 12:23:24 - INFO - __main__ - Step 480 Global step 480 Train loss 5.43 on epoch=239
06/24/2022 12:23:25 - INFO - __main__ - Step 490 Global step 490 Train loss 5.36 on epoch=244
06/24/2022 12:23:27 - INFO - __main__ - Step 500 Global step 500 Train loss 5.33 on epoch=249
06/24/2022 12:23:32 - INFO - __main__ - Global step 500 Train loss 5.41 Classification-F1 0.0 on epoch=249
06/24/2022 12:23:33 - INFO - __main__ - Step 510 Global step 510 Train loss 5.37 on epoch=254
06/24/2022 12:23:34 - INFO - __main__ - Step 520 Global step 520 Train loss 5.37 on epoch=259
06/24/2022 12:23:35 - INFO - __main__ - Step 530 Global step 530 Train loss 5.37 on epoch=264
06/24/2022 12:23:37 - INFO - __main__ - Step 540 Global step 540 Train loss 5.32 on epoch=269
06/24/2022 12:23:38 - INFO - __main__ - Step 550 Global step 550 Train loss 5.31 on epoch=274
06/24/2022 12:23:44 - INFO - __main__ - Global step 550 Train loss 5.35 Classification-F1 0.0 on epoch=274
06/24/2022 12:23:45 - INFO - __main__ - Step 560 Global step 560 Train loss 5.29 on epoch=279
06/24/2022 12:23:46 - INFO - __main__ - Step 570 Global step 570 Train loss 5.30 on epoch=284
06/24/2022 12:23:47 - INFO - __main__ - Step 580 Global step 580 Train loss 5.20 on epoch=289
06/24/2022 12:23:48 - INFO - __main__ - Step 590 Global step 590 Train loss 5.24 on epoch=294
06/24/2022 12:23:50 - INFO - __main__ - Step 600 Global step 600 Train loss 5.10 on epoch=299
06/24/2022 12:23:56 - INFO - __main__ - Global step 600 Train loss 5.23 Classification-F1 0.0 on epoch=299
06/24/2022 12:23:57 - INFO - __main__ - Step 610 Global step 610 Train loss 5.17 on epoch=304
06/24/2022 12:23:58 - INFO - __main__ - Step 620 Global step 620 Train loss 5.21 on epoch=309
06/24/2022 12:23:59 - INFO - __main__ - Step 630 Global step 630 Train loss 5.09 on epoch=314
06/24/2022 12:24:01 - INFO - __main__ - Step 640 Global step 640 Train loss 5.06 on epoch=319
06/24/2022 12:24:02 - INFO - __main__ - Step 650 Global step 650 Train loss 5.04 on epoch=324
06/24/2022 12:24:12 - INFO - __main__ - Global step 650 Train loss 5.11 Classification-F1 0.0 on epoch=324
06/24/2022 12:24:13 - INFO - __main__ - Step 660 Global step 660 Train loss 5.10 on epoch=329
06/24/2022 12:24:14 - INFO - __main__ - Step 670 Global step 670 Train loss 5.15 on epoch=334
06/24/2022 12:24:16 - INFO - __main__ - Step 680 Global step 680 Train loss 5.04 on epoch=339
06/24/2022 12:24:17 - INFO - __main__ - Step 690 Global step 690 Train loss 4.94 on epoch=344
06/24/2022 12:24:18 - INFO - __main__ - Step 700 Global step 700 Train loss 4.98 on epoch=349
06/24/2022 12:24:28 - INFO - __main__ - Global step 700 Train loss 5.04 Classification-F1 0.0 on epoch=349
06/24/2022 12:24:30 - INFO - __main__ - Step 710 Global step 710 Train loss 4.93 on epoch=354
06/24/2022 12:24:31 - INFO - __main__ - Step 720 Global step 720 Train loss 4.81 on epoch=359
06/24/2022 12:24:32 - INFO - __main__ - Step 730 Global step 730 Train loss 4.85 on epoch=364
06/24/2022 12:24:33 - INFO - __main__ - Step 740 Global step 740 Train loss 4.84 on epoch=369
06/24/2022 12:24:34 - INFO - __main__ - Step 750 Global step 750 Train loss 4.77 on epoch=374
06/24/2022 12:24:45 - INFO - __main__ - Global step 750 Train loss 4.84 Classification-F1 0.0 on epoch=374
06/24/2022 12:24:47 - INFO - __main__ - Step 760 Global step 760 Train loss 4.72 on epoch=379
06/24/2022 12:24:48 - INFO - __main__ - Step 770 Global step 770 Train loss 4.68 on epoch=384
06/24/2022 12:24:49 - INFO - __main__ - Step 780 Global step 780 Train loss 4.58 on epoch=389
06/24/2022 12:24:50 - INFO - __main__ - Step 790 Global step 790 Train loss 4.72 on epoch=394
06/24/2022 12:24:51 - INFO - __main__ - Step 800 Global step 800 Train loss 4.48 on epoch=399
06/24/2022 12:25:02 - INFO - __main__ - Global step 800 Train loss 4.64 Classification-F1 0.0 on epoch=399
06/24/2022 12:25:03 - INFO - __main__ - Step 810 Global step 810 Train loss 4.60 on epoch=404
06/24/2022 12:25:04 - INFO - __main__ - Step 820 Global step 820 Train loss 4.46 on epoch=409
06/24/2022 12:25:05 - INFO - __main__ - Step 830 Global step 830 Train loss 4.57 on epoch=414
06/24/2022 12:25:07 - INFO - __main__ - Step 840 Global step 840 Train loss 4.41 on epoch=419
06/24/2022 12:25:08 - INFO - __main__ - Step 850 Global step 850 Train loss 4.39 on epoch=424
06/24/2022 12:25:19 - INFO - __main__ - Global step 850 Train loss 4.49 Classification-F1 0.0 on epoch=424
06/24/2022 12:25:20 - INFO - __main__ - Step 860 Global step 860 Train loss 4.42 on epoch=429
06/24/2022 12:25:21 - INFO - __main__ - Step 870 Global step 870 Train loss 4.30 on epoch=434
06/24/2022 12:25:23 - INFO - __main__ - Step 880 Global step 880 Train loss 4.26 on epoch=439
06/24/2022 12:25:24 - INFO - __main__ - Step 890 Global step 890 Train loss 4.18 on epoch=444
06/24/2022 12:25:25 - INFO - __main__ - Step 900 Global step 900 Train loss 4.16 on epoch=449
06/24/2022 12:25:36 - INFO - __main__ - Global step 900 Train loss 4.27 Classification-F1 0.0 on epoch=449
06/24/2022 12:25:38 - INFO - __main__ - Step 910 Global step 910 Train loss 4.22 on epoch=454
06/24/2022 12:25:39 - INFO - __main__ - Step 920 Global step 920 Train loss 4.03 on epoch=459
06/24/2022 12:25:40 - INFO - __main__ - Step 930 Global step 930 Train loss 4.10 on epoch=464
06/24/2022 12:25:41 - INFO - __main__ - Step 940 Global step 940 Train loss 4.14 on epoch=469
06/24/2022 12:25:42 - INFO - __main__ - Step 950 Global step 950 Train loss 4.11 on epoch=474
06/24/2022 12:25:54 - INFO - __main__ - Global step 950 Train loss 4.12 Classification-F1 0.0 on epoch=474
06/24/2022 12:25:55 - INFO - __main__ - Step 960 Global step 960 Train loss 3.98 on epoch=479
06/24/2022 12:25:56 - INFO - __main__ - Step 970 Global step 970 Train loss 3.90 on epoch=484
06/24/2022 12:25:58 - INFO - __main__ - Step 980 Global step 980 Train loss 3.94 on epoch=489
06/24/2022 12:25:59 - INFO - __main__ - Step 990 Global step 990 Train loss 3.83 on epoch=494
06/24/2022 12:26:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.89 on epoch=499
06/24/2022 12:26:11 - INFO - __main__ - Global step 1000 Train loss 3.91 Classification-F1 0.0 on epoch=499
06/24/2022 12:26:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.85 on epoch=504
06/24/2022 12:26:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.93 on epoch=509
06/24/2022 12:26:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.71 on epoch=514
06/24/2022 12:26:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.72 on epoch=519
06/24/2022 12:26:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.84 on epoch=524
06/24/2022 12:26:29 - INFO - __main__ - Global step 1050 Train loss 3.81 Classification-F1 0.0 on epoch=524
06/24/2022 12:26:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.61 on epoch=529
06/24/2022 12:26:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.52 on epoch=534
06/24/2022 12:26:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.60 on epoch=539
06/24/2022 12:26:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.65 on epoch=544
06/24/2022 12:26:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.57 on epoch=549
06/24/2022 12:26:38 - INFO - __main__ - Global step 1100 Train loss 3.59 Classification-F1 0.0 on epoch=549
06/24/2022 12:26:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.58 on epoch=554
06/24/2022 12:26:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.49 on epoch=559
06/24/2022 12:26:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.40 on epoch=564
06/24/2022 12:26:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.32 on epoch=569
06/24/2022 12:26:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.36 on epoch=574
06/24/2022 12:26:47 - INFO - __main__ - Global step 1150 Train loss 3.43 Classification-F1 0.0 on epoch=574
06/24/2022 12:26:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.17 on epoch=579
06/24/2022 12:26:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.23 on epoch=584
06/24/2022 12:26:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.37 on epoch=589
06/24/2022 12:26:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.13 on epoch=594
06/24/2022 12:26:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.19 on epoch=599
06/24/2022 12:26:59 - INFO - __main__ - Global step 1200 Train loss 3.22 Classification-F1 0.0065359477124183 on epoch=599
06/24/2022 12:26:59 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0065359477124183 on epoch=599, global_step=1200
06/24/2022 12:27:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.00 on epoch=604
06/24/2022 12:27:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.12 on epoch=609
06/24/2022 12:27:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.05 on epoch=614
06/24/2022 12:27:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.99 on epoch=619
06/24/2022 12:27:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.92 on epoch=624
06/24/2022 12:27:16 - INFO - __main__ - Global step 1250 Train loss 3.01 Classification-F1 0.05454545454545455 on epoch=624
06/24/2022 12:27:16 - INFO - __main__ - Saving model with best Classification-F1: 0.0065359477124183 -> 0.05454545454545455 on epoch=624, global_step=1250
06/24/2022 12:27:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.88 on epoch=629
06/24/2022 12:27:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.96 on epoch=634
06/24/2022 12:27:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.93 on epoch=639
06/24/2022 12:27:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.88 on epoch=644
06/24/2022 12:27:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.80 on epoch=649
06/24/2022 12:27:32 - INFO - __main__ - Global step 1300 Train loss 2.89 Classification-F1 0.03535353535353535 on epoch=649
06/24/2022 12:27:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.76 on epoch=654
06/24/2022 12:27:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.72 on epoch=659
06/24/2022 12:27:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.74 on epoch=664
06/24/2022 12:27:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.71 on epoch=669
06/24/2022 12:27:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.72 on epoch=674
06/24/2022 12:27:49 - INFO - __main__ - Global step 1350 Train loss 2.73 Classification-F1 0.02666666666666667 on epoch=674
06/24/2022 12:27:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.60 on epoch=679
06/24/2022 12:27:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.59 on epoch=684
06/24/2022 12:27:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.40 on epoch=689
06/24/2022 12:27:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.48 on epoch=694
06/24/2022 12:27:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.39 on epoch=699
06/24/2022 12:28:01 - INFO - __main__ - Global step 1400 Train loss 2.49 Classification-F1 0.065 on epoch=699
06/24/2022 12:28:01 - INFO - __main__ - Saving model with best Classification-F1: 0.05454545454545455 -> 0.065 on epoch=699, global_step=1400
06/24/2022 12:28:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.52 on epoch=704
06/24/2022 12:28:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.33 on epoch=709
06/24/2022 12:28:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.30 on epoch=714
06/24/2022 12:28:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.22 on epoch=719
06/24/2022 12:28:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.34 on epoch=724
06/24/2022 12:28:09 - INFO - __main__ - Global step 1450 Train loss 2.34 Classification-F1 0.3325358851674641 on epoch=724
06/24/2022 12:28:09 - INFO - __main__ - Saving model with best Classification-F1: 0.065 -> 0.3325358851674641 on epoch=724, global_step=1450
06/24/2022 12:28:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.27 on epoch=729
06/24/2022 12:28:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.06 on epoch=734
06/24/2022 12:28:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.12 on epoch=739
06/24/2022 12:28:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.10 on epoch=744
06/24/2022 12:28:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.02 on epoch=749
06/24/2022 12:28:17 - INFO - __main__ - Global step 1500 Train loss 2.11 Classification-F1 0.46843853820598 on epoch=749
06/24/2022 12:28:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3325358851674641 -> 0.46843853820598 on epoch=749, global_step=1500
06/24/2022 12:28:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.00 on epoch=754
06/24/2022 12:28:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.09 on epoch=759
06/24/2022 12:28:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 2.02 on epoch=764
06/24/2022 12:28:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.91 on epoch=769
06/24/2022 12:28:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 2.11 on epoch=774
06/24/2022 12:28:25 - INFO - __main__ - Global step 1550 Train loss 2.03 Classification-F1 0.2727272727272727 on epoch=774
06/24/2022 12:28:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.95 on epoch=779
06/24/2022 12:28:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.96 on epoch=784
06/24/2022 12:28:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.91 on epoch=789
06/24/2022 12:28:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.93 on epoch=794
06/24/2022 12:28:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.80 on epoch=799
06/24/2022 12:28:32 - INFO - __main__ - Global step 1600 Train loss 1.91 Classification-F1 0.4554554554554554 on epoch=799
06/24/2022 12:28:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.81 on epoch=804
06/24/2022 12:28:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.92 on epoch=809
06/24/2022 12:28:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.87 on epoch=814
06/24/2022 12:28:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.82 on epoch=819
06/24/2022 12:28:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.71 on epoch=824
06/24/2022 12:28:38 - INFO - __main__ - Global step 1650 Train loss 1.83 Classification-F1 0.3191489361702127 on epoch=824
06/24/2022 12:28:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.74 on epoch=829
06/24/2022 12:28:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.62 on epoch=834
06/24/2022 12:28:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.73 on epoch=839
06/24/2022 12:28:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.58 on epoch=844
06/24/2022 12:28:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.52 on epoch=849
06/24/2022 12:28:45 - INFO - __main__ - Global step 1700 Train loss 1.64 Classification-F1 0.3333333333333333 on epoch=849
06/24/2022 12:28:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.58 on epoch=854
06/24/2022 12:28:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.50 on epoch=859
06/24/2022 12:28:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.52 on epoch=864
06/24/2022 12:28:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.49 on epoch=869
06/24/2022 12:28:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.60 on epoch=874
06/24/2022 12:28:51 - INFO - __main__ - Global step 1750 Train loss 1.54 Classification-F1 0.3333333333333333 on epoch=874
06/24/2022 12:28:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.49 on epoch=879
06/24/2022 12:28:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.34 on epoch=884
06/24/2022 12:28:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.38 on epoch=889
06/24/2022 12:28:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.32 on epoch=894
06/24/2022 12:28:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.37 on epoch=899
06/24/2022 12:28:58 - INFO - __main__ - Global step 1800 Train loss 1.38 Classification-F1 0.3333333333333333 on epoch=899
06/24/2022 12:28:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.47 on epoch=904
06/24/2022 12:29:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.50 on epoch=909
06/24/2022 12:29:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.43 on epoch=914
06/24/2022 12:29:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.37 on epoch=919
06/24/2022 12:29:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.34 on epoch=924
06/24/2022 12:29:04 - INFO - __main__ - Global step 1850 Train loss 1.42 Classification-F1 0.3333333333333333 on epoch=924
06/24/2022 12:29:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.16 on epoch=929
06/24/2022 12:29:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.24 on epoch=934
06/24/2022 12:29:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.29 on epoch=939
06/24/2022 12:29:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.22 on epoch=944
06/24/2022 12:29:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.19 on epoch=949
06/24/2022 12:29:11 - INFO - __main__ - Global step 1900 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=949
06/24/2022 12:29:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.17 on epoch=954
06/24/2022 12:29:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.16 on epoch=959
06/24/2022 12:29:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.08 on epoch=964
06/24/2022 12:29:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.20 on epoch=969
06/24/2022 12:29:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.12 on epoch=974
06/24/2022 12:29:17 - INFO - __main__ - Global step 1950 Train loss 1.14 Classification-F1 0.3333333333333333 on epoch=974
06/24/2022 12:29:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.13 on epoch=979
06/24/2022 12:29:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.13 on epoch=984
06/24/2022 12:29:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.07 on epoch=989
06/24/2022 12:29:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.15 on epoch=994
06/24/2022 12:29:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.05 on epoch=999
06/24/2022 12:29:24 - INFO - __main__ - Global step 2000 Train loss 1.10 Classification-F1 0.3333333333333333 on epoch=999
06/24/2022 12:29:24 - INFO - __main__ - save last model!
06/24/2022 12:29:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 12:29:24 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 12:29:24 - INFO - __main__ - Printing 3 examples
06/24/2022 12:29:24 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 12:29:24 - INFO - __main__ - ['0']
06/24/2022 12:29:24 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 12:29:24 - INFO - __main__ - ['1']
06/24/2022 12:29:24 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 12:29:24 - INFO - __main__ - ['1']
06/24/2022 12:29:24 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:29:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:29:24 - INFO - __main__ - Printing 3 examples
06/24/2022 12:29:24 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 12:29:24 - INFO - __main__ - ['0']
06/24/2022 12:29:24 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 12:29:24 - INFO - __main__ - ['0']
06/24/2022 12:29:24 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 12:29:24 - INFO - __main__ - ['0']
06/24/2022 12:29:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 12:29:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:29:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:29:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:29:24 - INFO - __main__ - Printing 3 examples
06/24/2022 12:29:24 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 12:29:24 - INFO - __main__ - ['0']
06/24/2022 12:29:24 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 12:29:24 - INFO - __main__ - ['0']
06/24/2022 12:29:24 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 12:29:24 - INFO - __main__ - ['0']
06/24/2022 12:29:24 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:29:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:29:25 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:29:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:29:31 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:29:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:29:31 - INFO - __main__ - Starting training!
06/24/2022 12:29:36 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 12:31:08 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_42_0.2_8_predictions.txt
06/24/2022 12:31:08 - INFO - __main__ - Classification-F1 on test data: 0.3085
06/24/2022 12:31:08 - INFO - __main__ - prefix=paws_16_42, lr=0.2, bsz=8, dev_performance=0.46843853820598, test_performance=0.30845980231562403
06/24/2022 12:31:08 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.5, bsz=8 ...
06/24/2022 12:31:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:31:09 - INFO - __main__ - Printing 3 examples
06/24/2022 12:31:09 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 12:31:09 - INFO - __main__ - ['0']
06/24/2022 12:31:09 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 12:31:09 - INFO - __main__ - ['0']
06/24/2022 12:31:09 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 12:31:09 - INFO - __main__ - ['0']
06/24/2022 12:31:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:31:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:31:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:31:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:31:09 - INFO - __main__ - Printing 3 examples
06/24/2022 12:31:09 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 12:31:09 - INFO - __main__ - ['0']
06/24/2022 12:31:10 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 12:31:10 - INFO - __main__ - ['0']
06/24/2022 12:31:10 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 12:31:10 - INFO - __main__ - ['0']
06/24/2022 12:31:10 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:31:10 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:31:10 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:31:15 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:31:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:31:16 - INFO - __main__ - Starting training!
06/24/2022 12:31:17 - INFO - __main__ - Step 10 Global step 10 Train loss 5.87 on epoch=4
06/24/2022 12:31:19 - INFO - __main__ - Step 20 Global step 20 Train loss 5.75 on epoch=9
06/24/2022 12:31:20 - INFO - __main__ - Step 30 Global step 30 Train loss 5.73 on epoch=14
06/24/2022 12:31:21 - INFO - __main__ - Step 40 Global step 40 Train loss 5.64 on epoch=19
06/24/2022 12:31:22 - INFO - __main__ - Step 50 Global step 50 Train loss 5.62 on epoch=24
06/24/2022 12:31:23 - INFO - __main__ - Global step 50 Train loss 5.72 Classification-F1 0.0 on epoch=24
06/24/2022 12:31:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 12:31:24 - INFO - __main__ - Step 60 Global step 60 Train loss 5.58 on epoch=29
06/24/2022 12:31:25 - INFO - __main__ - Step 70 Global step 70 Train loss 5.56 on epoch=34
06/24/2022 12:31:27 - INFO - __main__ - Step 80 Global step 80 Train loss 5.52 on epoch=39
06/24/2022 12:31:28 - INFO - __main__ - Step 90 Global step 90 Train loss 5.49 on epoch=44
06/24/2022 12:31:29 - INFO - __main__ - Step 100 Global step 100 Train loss 5.50 on epoch=49
06/24/2022 12:31:35 - INFO - __main__ - Global step 100 Train loss 5.53 Classification-F1 0.0 on epoch=49
06/24/2022 12:31:36 - INFO - __main__ - Step 110 Global step 110 Train loss 5.40 on epoch=54
06/24/2022 12:31:37 - INFO - __main__ - Step 120 Global step 120 Train loss 5.36 on epoch=59
06/24/2022 12:31:39 - INFO - __main__ - Step 130 Global step 130 Train loss 5.36 on epoch=64
06/24/2022 12:31:40 - INFO - __main__ - Step 140 Global step 140 Train loss 5.22 on epoch=69
06/24/2022 12:31:41 - INFO - __main__ - Step 150 Global step 150 Train loss 5.15 on epoch=74
06/24/2022 12:31:45 - INFO - __main__ - Global step 150 Train loss 5.30 Classification-F1 0.0 on epoch=74
06/24/2022 12:31:46 - INFO - __main__ - Step 160 Global step 160 Train loss 5.10 on epoch=79
06/24/2022 12:31:48 - INFO - __main__ - Step 170 Global step 170 Train loss 5.03 on epoch=84
06/24/2022 12:31:49 - INFO - __main__ - Step 180 Global step 180 Train loss 4.81 on epoch=89
06/24/2022 12:31:50 - INFO - __main__ - Step 190 Global step 190 Train loss 4.85 on epoch=94
06/24/2022 12:31:51 - INFO - __main__ - Step 200 Global step 200 Train loss 4.64 on epoch=99
06/24/2022 12:31:57 - INFO - __main__ - Global step 200 Train loss 4.89 Classification-F1 0.0 on epoch=99
06/24/2022 12:31:59 - INFO - __main__ - Step 210 Global step 210 Train loss 4.57 on epoch=104
06/24/2022 12:32:00 - INFO - __main__ - Step 220 Global step 220 Train loss 4.36 on epoch=109
06/24/2022 12:32:01 - INFO - __main__ - Step 230 Global step 230 Train loss 4.13 on epoch=114
06/24/2022 12:32:02 - INFO - __main__ - Step 240 Global step 240 Train loss 4.29 on epoch=119
06/24/2022 12:32:03 - INFO - __main__ - Step 250 Global step 250 Train loss 4.31 on epoch=124
06/24/2022 12:32:05 - INFO - __main__ - Global step 250 Train loss 4.33 Classification-F1 0.0 on epoch=124
06/24/2022 12:32:06 - INFO - __main__ - Step 260 Global step 260 Train loss 4.10 on epoch=129
06/24/2022 12:32:08 - INFO - __main__ - Step 270 Global step 270 Train loss 3.93 on epoch=134
06/24/2022 12:32:09 - INFO - __main__ - Step 280 Global step 280 Train loss 4.05 on epoch=139
06/24/2022 12:32:10 - INFO - __main__ - Step 290 Global step 290 Train loss 4.03 on epoch=144
06/24/2022 12:32:11 - INFO - __main__ - Step 300 Global step 300 Train loss 3.92 on epoch=149
06/24/2022 12:32:13 - INFO - __main__ - Global step 300 Train loss 4.01 Classification-F1 0.0 on epoch=149
06/24/2022 12:32:14 - INFO - __main__ - Step 310 Global step 310 Train loss 3.97 on epoch=154
06/24/2022 12:32:16 - INFO - __main__ - Step 320 Global step 320 Train loss 3.74 on epoch=159
06/24/2022 12:32:17 - INFO - __main__ - Step 330 Global step 330 Train loss 3.68 on epoch=164
06/24/2022 12:32:18 - INFO - __main__ - Step 340 Global step 340 Train loss 3.76 on epoch=169
06/24/2022 12:32:19 - INFO - __main__ - Step 350 Global step 350 Train loss 3.69 on epoch=174
06/24/2022 12:32:30 - INFO - __main__ - Global step 350 Train loss 3.77 Classification-F1 0.0 on epoch=174
06/24/2022 12:32:31 - INFO - __main__ - Step 360 Global step 360 Train loss 3.57 on epoch=179
06/24/2022 12:32:33 - INFO - __main__ - Step 370 Global step 370 Train loss 3.45 on epoch=184
06/24/2022 12:32:34 - INFO - __main__ - Step 380 Global step 380 Train loss 3.44 on epoch=189
06/24/2022 12:32:35 - INFO - __main__ - Step 390 Global step 390 Train loss 3.32 on epoch=194
06/24/2022 12:32:36 - INFO - __main__ - Step 400 Global step 400 Train loss 3.38 on epoch=199
06/24/2022 12:32:47 - INFO - __main__ - Global step 400 Train loss 3.43 Classification-F1 0.0 on epoch=199
06/24/2022 12:32:49 - INFO - __main__ - Step 410 Global step 410 Train loss 3.05 on epoch=204
06/24/2022 12:32:50 - INFO - __main__ - Step 420 Global step 420 Train loss 3.12 on epoch=209
06/24/2022 12:32:51 - INFO - __main__ - Step 430 Global step 430 Train loss 3.04 on epoch=214
06/24/2022 12:32:52 - INFO - __main__ - Step 440 Global step 440 Train loss 2.93 on epoch=219
06/24/2022 12:32:54 - INFO - __main__ - Step 450 Global step 450 Train loss 2.76 on epoch=224
06/24/2022 12:33:02 - INFO - __main__ - Global step 450 Train loss 2.98 Classification-F1 0.08637873754152824 on epoch=224
06/24/2022 12:33:02 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.08637873754152824 on epoch=224, global_step=450
06/24/2022 12:33:03 - INFO - __main__ - Step 460 Global step 460 Train loss 2.73 on epoch=229
06/24/2022 12:33:04 - INFO - __main__ - Step 470 Global step 470 Train loss 2.73 on epoch=234
06/24/2022 12:33:05 - INFO - __main__ - Step 480 Global step 480 Train loss 2.86 on epoch=239
06/24/2022 12:33:07 - INFO - __main__ - Step 490 Global step 490 Train loss 2.62 on epoch=244
06/24/2022 12:33:08 - INFO - __main__ - Step 500 Global step 500 Train loss 2.65 on epoch=249
06/24/2022 12:33:11 - INFO - __main__ - Global step 500 Train loss 2.72 Classification-F1 0.1776223776223776 on epoch=249
06/24/2022 12:33:11 - INFO - __main__ - Saving model with best Classification-F1: 0.08637873754152824 -> 0.1776223776223776 on epoch=249, global_step=500
06/24/2022 12:33:12 - INFO - __main__ - Step 510 Global step 510 Train loss 2.47 on epoch=254
06/24/2022 12:33:13 - INFO - __main__ - Step 520 Global step 520 Train loss 2.36 on epoch=259
06/24/2022 12:33:15 - INFO - __main__ - Step 530 Global step 530 Train loss 2.23 on epoch=264
06/24/2022 12:33:16 - INFO - __main__ - Step 540 Global step 540 Train loss 2.22 on epoch=269
06/24/2022 12:33:17 - INFO - __main__ - Step 550 Global step 550 Train loss 2.31 on epoch=274
06/24/2022 12:33:19 - INFO - __main__ - Global step 550 Train loss 2.32 Classification-F1 0.33793103448275863 on epoch=274
06/24/2022 12:33:19 - INFO - __main__ - Saving model with best Classification-F1: 0.1776223776223776 -> 0.33793103448275863 on epoch=274, global_step=550
06/24/2022 12:33:20 - INFO - __main__ - Step 560 Global step 560 Train loss 2.09 on epoch=279
06/24/2022 12:33:21 - INFO - __main__ - Step 570 Global step 570 Train loss 1.96 on epoch=284
06/24/2022 12:33:23 - INFO - __main__ - Step 580 Global step 580 Train loss 1.72 on epoch=289
06/24/2022 12:33:24 - INFO - __main__ - Step 590 Global step 590 Train loss 1.95 on epoch=294
06/24/2022 12:33:25 - INFO - __main__ - Step 600 Global step 600 Train loss 1.55 on epoch=299
06/24/2022 12:33:25 - INFO - __main__ - Global step 600 Train loss 1.86 Classification-F1 0.3191489361702127 on epoch=299
06/24/2022 12:33:27 - INFO - __main__ - Step 610 Global step 610 Train loss 1.62 on epoch=304
06/24/2022 12:33:28 - INFO - __main__ - Step 620 Global step 620 Train loss 1.47 on epoch=309
06/24/2022 12:33:29 - INFO - __main__ - Step 630 Global step 630 Train loss 1.52 on epoch=314
06/24/2022 12:33:30 - INFO - __main__ - Step 640 Global step 640 Train loss 1.42 on epoch=319
06/24/2022 12:33:32 - INFO - __main__ - Step 650 Global step 650 Train loss 1.36 on epoch=324
06/24/2022 12:33:32 - INFO - __main__ - Global step 650 Train loss 1.48 Classification-F1 0.3191489361702127 on epoch=324
06/24/2022 12:33:33 - INFO - __main__ - Step 660 Global step 660 Train loss 1.41 on epoch=329
06/24/2022 12:33:34 - INFO - __main__ - Step 670 Global step 670 Train loss 1.35 on epoch=334
06/24/2022 12:33:36 - INFO - __main__ - Step 680 Global step 680 Train loss 1.15 on epoch=339
06/24/2022 12:33:37 - INFO - __main__ - Step 690 Global step 690 Train loss 1.19 on epoch=344
06/24/2022 12:33:38 - INFO - __main__ - Step 700 Global step 700 Train loss 1.13 on epoch=349
06/24/2022 12:33:39 - INFO - __main__ - Global step 700 Train loss 1.25 Classification-F1 0.3816425120772947 on epoch=349
06/24/2022 12:33:39 - INFO - __main__ - Saving model with best Classification-F1: 0.33793103448275863 -> 0.3816425120772947 on epoch=349, global_step=700
06/24/2022 12:33:40 - INFO - __main__ - Step 710 Global step 710 Train loss 1.03 on epoch=354
06/24/2022 12:33:41 - INFO - __main__ - Step 720 Global step 720 Train loss 1.15 on epoch=359
06/24/2022 12:33:42 - INFO - __main__ - Step 730 Global step 730 Train loss 1.05 on epoch=364
06/24/2022 12:33:44 - INFO - __main__ - Step 740 Global step 740 Train loss 1.04 on epoch=369
06/24/2022 12:33:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.97 on epoch=374
06/24/2022 12:33:45 - INFO - __main__ - Global step 750 Train loss 1.05 Classification-F1 0.3333333333333333 on epoch=374
06/24/2022 12:33:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.98 on epoch=379
06/24/2022 12:33:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.96 on epoch=384
06/24/2022 12:33:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.98 on epoch=389
06/24/2022 12:33:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.94 on epoch=394
06/24/2022 12:33:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.84 on epoch=399
06/24/2022 12:33:52 - INFO - __main__ - Global step 800 Train loss 0.94 Classification-F1 0.3043478260869565 on epoch=399
06/24/2022 12:33:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.87 on epoch=404
06/24/2022 12:33:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.84 on epoch=409
06/24/2022 12:33:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.91 on epoch=414
06/24/2022 12:33:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.86 on epoch=419
06/24/2022 12:33:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.80 on epoch=424
06/24/2022 12:33:58 - INFO - __main__ - Global step 850 Train loss 0.86 Classification-F1 0.39756367663344405 on epoch=424
06/24/2022 12:33:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.39756367663344405 on epoch=424, global_step=850
06/24/2022 12:34:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.71 on epoch=429
06/24/2022 12:34:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.87 on epoch=434
06/24/2022 12:34:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.82 on epoch=439
06/24/2022 12:34:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.74 on epoch=444
06/24/2022 12:34:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.73 on epoch=449
06/24/2022 12:34:05 - INFO - __main__ - Global step 900 Train loss 0.77 Classification-F1 0.3191489361702127 on epoch=449
06/24/2022 12:34:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.76 on epoch=454
06/24/2022 12:34:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.75 on epoch=459
06/24/2022 12:34:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.72 on epoch=464
06/24/2022 12:34:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.73 on epoch=469
06/24/2022 12:34:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.70 on epoch=474
06/24/2022 12:34:11 - INFO - __main__ - Global step 950 Train loss 0.73 Classification-F1 0.4682306940371457 on epoch=474
06/24/2022 12:34:11 - INFO - __main__ - Saving model with best Classification-F1: 0.39756367663344405 -> 0.4682306940371457 on epoch=474, global_step=950
06/24/2022 12:34:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.73 on epoch=479
06/24/2022 12:34:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.72 on epoch=484
06/24/2022 12:34:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.61 on epoch=489
06/24/2022 12:34:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.75 on epoch=494
06/24/2022 12:34:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.68 on epoch=499
06/24/2022 12:34:18 - INFO - __main__ - Global step 1000 Train loss 0.70 Classification-F1 0.3043478260869565 on epoch=499
06/24/2022 12:34:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.77 on epoch=504
06/24/2022 12:34:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.74 on epoch=509
06/24/2022 12:34:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.68 on epoch=514
06/24/2022 12:34:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.68 on epoch=519
06/24/2022 12:34:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.69 on epoch=524
06/24/2022 12:34:24 - INFO - __main__ - Global step 1050 Train loss 0.71 Classification-F1 0.4181818181818182 on epoch=524
06/24/2022 12:34:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.71 on epoch=529
06/24/2022 12:34:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.62 on epoch=534
06/24/2022 12:34:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.57 on epoch=539
06/24/2022 12:34:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.57 on epoch=544
06/24/2022 12:34:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.68 on epoch=549
06/24/2022 12:34:31 - INFO - __main__ - Global step 1100 Train loss 0.63 Classification-F1 0.3043478260869565 on epoch=549
06/24/2022 12:34:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.65 on epoch=554
06/24/2022 12:34:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.64 on epoch=559
06/24/2022 12:34:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.61 on epoch=564
06/24/2022 12:34:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.54 on epoch=569
06/24/2022 12:34:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.60 on epoch=574
06/24/2022 12:34:37 - INFO - __main__ - Global step 1150 Train loss 0.61 Classification-F1 0.43529411764705883 on epoch=574
06/24/2022 12:34:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.61 on epoch=579
06/24/2022 12:34:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.59 on epoch=584
06/24/2022 12:34:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.66 on epoch=589
06/24/2022 12:34:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.57 on epoch=594
06/24/2022 12:34:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.56 on epoch=599
06/24/2022 12:34:44 - INFO - __main__ - Global step 1200 Train loss 0.60 Classification-F1 0.3043478260869565 on epoch=599
06/24/2022 12:34:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.52 on epoch=604
06/24/2022 12:34:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.64 on epoch=609
06/24/2022 12:34:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.56 on epoch=614
06/24/2022 12:34:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.69 on epoch=619
06/24/2022 12:34:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.70 on epoch=624
06/24/2022 12:34:50 - INFO - __main__ - Global step 1250 Train loss 0.62 Classification-F1 0.36374269005847953 on epoch=624
06/24/2022 12:34:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.53 on epoch=629
06/24/2022 12:34:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.54 on epoch=634
06/24/2022 12:34:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.61 on epoch=639
06/24/2022 12:34:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.50 on epoch=644
06/24/2022 12:34:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.66 on epoch=649
06/24/2022 12:34:57 - INFO - __main__ - Global step 1300 Train loss 0.57 Classification-F1 0.5901477832512315 on epoch=649
06/24/2022 12:34:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4682306940371457 -> 0.5901477832512315 on epoch=649, global_step=1300
06/24/2022 12:34:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.62 on epoch=654
06/24/2022 12:34:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.54 on epoch=659
06/24/2022 12:35:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.58 on epoch=664
06/24/2022 12:35:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.58 on epoch=669
06/24/2022 12:35:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.51 on epoch=674
06/24/2022 12:35:03 - INFO - __main__ - Global step 1350 Train loss 0.56 Classification-F1 0.4385964912280702 on epoch=674
06/24/2022 12:35:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.57 on epoch=679
06/24/2022 12:35:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.67 on epoch=684
06/24/2022 12:35:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
06/24/2022 12:35:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.58 on epoch=694
06/24/2022 12:35:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.56 on epoch=699
06/24/2022 12:35:10 - INFO - __main__ - Global step 1400 Train loss 0.58 Classification-F1 0.4554554554554554 on epoch=699
06/24/2022 12:35:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.54 on epoch=704
06/24/2022 12:35:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.52 on epoch=709
06/24/2022 12:35:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.51 on epoch=714
06/24/2022 12:35:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.63 on epoch=719
06/24/2022 12:35:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.54 on epoch=724
06/24/2022 12:35:16 - INFO - __main__ - Global step 1450 Train loss 0.55 Classification-F1 0.5636363636363637 on epoch=724
06/24/2022 12:35:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.61 on epoch=729
06/24/2022 12:35:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.61 on epoch=734
06/24/2022 12:35:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.58 on epoch=739
06/24/2022 12:35:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=744
06/24/2022 12:35:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.57 on epoch=749
06/24/2022 12:35:23 - INFO - __main__ - Global step 1500 Train loss 0.56 Classification-F1 0.49090909090909085 on epoch=749
06/24/2022 12:35:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.56 on epoch=754
06/24/2022 12:35:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.57 on epoch=759
06/24/2022 12:35:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.60 on epoch=764
06/24/2022 12:35:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=769
06/24/2022 12:35:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.59 on epoch=774
06/24/2022 12:35:29 - INFO - __main__ - Global step 1550 Train loss 0.55 Classification-F1 0.3191489361702127 on epoch=774
06/24/2022 12:35:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.46 on epoch=779
06/24/2022 12:35:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.56 on epoch=784
06/24/2022 12:35:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.52 on epoch=789
06/24/2022 12:35:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.49 on epoch=794
06/24/2022 12:35:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.53 on epoch=799
06/24/2022 12:35:36 - INFO - __main__ - Global step 1600 Train loss 0.51 Classification-F1 0.4909862142099682 on epoch=799
06/24/2022 12:35:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=804
06/24/2022 12:35:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.55 on epoch=809
06/24/2022 12:35:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.57 on epoch=814
06/24/2022 12:35:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.57 on epoch=819
06/24/2022 12:35:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.52 on epoch=824
06/24/2022 12:35:43 - INFO - __main__ - Global step 1650 Train loss 0.54 Classification-F1 0.6476476476476476 on epoch=824
06/24/2022 12:35:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5901477832512315 -> 0.6476476476476476 on epoch=824, global_step=1650
06/24/2022 12:35:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.53 on epoch=829
06/24/2022 12:35:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.60 on epoch=834
06/24/2022 12:35:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.48 on epoch=839
06/24/2022 12:35:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.53 on epoch=844
06/24/2022 12:35:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.46 on epoch=849
06/24/2022 12:35:49 - INFO - __main__ - Global step 1700 Train loss 0.52 Classification-F1 0.3191489361702127 on epoch=849
06/24/2022 12:35:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.52 on epoch=854
06/24/2022 12:35:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.49 on epoch=859
06/24/2022 12:35:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.48 on epoch=864
06/24/2022 12:35:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.66 on epoch=869
06/24/2022 12:35:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.49 on epoch=874
06/24/2022 12:35:56 - INFO - __main__ - Global step 1750 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=874
06/24/2022 12:35:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.58 on epoch=879
06/24/2022 12:35:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.53 on epoch=884
06/24/2022 12:35:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.52 on epoch=889
06/24/2022 12:36:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.53 on epoch=894
06/24/2022 12:36:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.50 on epoch=899
06/24/2022 12:36:02 - INFO - __main__ - Global step 1800 Train loss 0.53 Classification-F1 0.6267232237539766 on epoch=899
06/24/2022 12:36:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=904
06/24/2022 12:36:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.45 on epoch=909
06/24/2022 12:36:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.50 on epoch=914
06/24/2022 12:36:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.51 on epoch=919
06/24/2022 12:36:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.49 on epoch=924
06/24/2022 12:36:09 - INFO - __main__ - Global step 1850 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=924
06/24/2022 12:36:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.63 on epoch=929
06/24/2022 12:36:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.53 on epoch=934
06/24/2022 12:36:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
06/24/2022 12:36:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.46 on epoch=944
06/24/2022 12:36:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=949
06/24/2022 12:36:16 - INFO - __main__ - Global step 1900 Train loss 0.50 Classification-F1 0.5195195195195195 on epoch=949
06/24/2022 12:36:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.47 on epoch=954
06/24/2022 12:36:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.50 on epoch=959
06/24/2022 12:36:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=964
06/24/2022 12:36:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.50 on epoch=969
06/24/2022 12:36:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.45 on epoch=974
06/24/2022 12:36:22 - INFO - __main__ - Global step 1950 Train loss 0.48 Classification-F1 0.4385964912280702 on epoch=974
06/24/2022 12:36:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.47 on epoch=979
06/24/2022 12:36:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.54 on epoch=984
06/24/2022 12:36:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.49 on epoch=989
06/24/2022 12:36:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.51 on epoch=994
06/24/2022 12:36:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=999
06/24/2022 12:36:29 - INFO - __main__ - Global step 2000 Train loss 0.50 Classification-F1 0.4385964912280702 on epoch=999
06/24/2022 12:36:29 - INFO - __main__ - save last model!
06/24/2022 12:36:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 12:36:29 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 12:36:29 - INFO - __main__ - Printing 3 examples
06/24/2022 12:36:29 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 12:36:29 - INFO - __main__ - ['0']
06/24/2022 12:36:29 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 12:36:29 - INFO - __main__ - ['1']
06/24/2022 12:36:29 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 12:36:29 - INFO - __main__ - ['1']
06/24/2022 12:36:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:36:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:36:29 - INFO - __main__ - Printing 3 examples
06/24/2022 12:36:29 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 12:36:29 - INFO - __main__ - ['0']
06/24/2022 12:36:29 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 12:36:29 - INFO - __main__ - ['0']
06/24/2022 12:36:29 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 12:36:29 - INFO - __main__ - ['0']
06/24/2022 12:36:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 12:36:29 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:36:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:36:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:36:29 - INFO - __main__ - Printing 3 examples
06/24/2022 12:36:29 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 12:36:29 - INFO - __main__ - ['0']
06/24/2022 12:36:29 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 12:36:29 - INFO - __main__ - ['0']
06/24/2022 12:36:29 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 12:36:29 - INFO - __main__ - ['0']
06/24/2022 12:36:29 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:36:29 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:36:29 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:36:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:36:35 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:36:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:36:36 - INFO - __main__ - Starting training!
06/24/2022 12:36:41 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 12:38:06 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_87_0.5_8_predictions.txt
06/24/2022 12:38:06 - INFO - __main__ - Classification-F1 on test data: 0.3789
06/24/2022 12:38:06 - INFO - __main__ - prefix=paws_16_87, lr=0.5, bsz=8, dev_performance=0.6476476476476476, test_performance=0.3788506545024802
06/24/2022 12:38:06 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.4, bsz=8 ...
06/24/2022 12:38:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:38:07 - INFO - __main__ - Printing 3 examples
06/24/2022 12:38:07 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 12:38:07 - INFO - __main__ - ['0']
06/24/2022 12:38:07 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 12:38:07 - INFO - __main__ - ['0']
06/24/2022 12:38:07 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 12:38:07 - INFO - __main__ - ['0']
06/24/2022 12:38:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:38:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:38:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:38:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:38:08 - INFO - __main__ - Printing 3 examples
06/24/2022 12:38:08 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 12:38:08 - INFO - __main__ - ['0']
06/24/2022 12:38:08 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 12:38:08 - INFO - __main__ - ['0']
06/24/2022 12:38:08 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 12:38:08 - INFO - __main__ - ['0']
06/24/2022 12:38:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:38:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:38:08 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:38:13 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:38:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:38:14 - INFO - __main__ - Starting training!
06/24/2022 12:38:15 - INFO - __main__ - Step 10 Global step 10 Train loss 5.75 on epoch=4
06/24/2022 12:38:17 - INFO - __main__ - Step 20 Global step 20 Train loss 5.84 on epoch=9
06/24/2022 12:38:18 - INFO - __main__ - Step 30 Global step 30 Train loss 5.75 on epoch=14
06/24/2022 12:38:19 - INFO - __main__ - Step 40 Global step 40 Train loss 5.74 on epoch=19
06/24/2022 12:38:20 - INFO - __main__ - Step 50 Global step 50 Train loss 5.67 on epoch=24
06/24/2022 12:38:24 - INFO - __main__ - Global step 50 Train loss 5.75 Classification-F1 0.0 on epoch=24
06/24/2022 12:38:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 12:38:25 - INFO - __main__ - Step 60 Global step 60 Train loss 5.70 on epoch=29
06/24/2022 12:38:26 - INFO - __main__ - Step 70 Global step 70 Train loss 5.64 on epoch=34
06/24/2022 12:38:27 - INFO - __main__ - Step 80 Global step 80 Train loss 5.68 on epoch=39
06/24/2022 12:38:29 - INFO - __main__ - Step 90 Global step 90 Train loss 5.71 on epoch=44
06/24/2022 12:38:30 - INFO - __main__ - Step 100 Global step 100 Train loss 5.75 on epoch=49
06/24/2022 12:38:37 - INFO - __main__ - Global step 100 Train loss 5.70 Classification-F1 0.0 on epoch=49
06/24/2022 12:38:38 - INFO - __main__ - Step 110 Global step 110 Train loss 5.67 on epoch=54
06/24/2022 12:38:39 - INFO - __main__ - Step 120 Global step 120 Train loss 5.58 on epoch=59
06/24/2022 12:38:41 - INFO - __main__ - Step 130 Global step 130 Train loss 5.63 on epoch=64
06/24/2022 12:38:42 - INFO - __main__ - Step 140 Global step 140 Train loss 5.59 on epoch=69
06/24/2022 12:38:43 - INFO - __main__ - Step 150 Global step 150 Train loss 5.58 on epoch=74
06/24/2022 12:38:48 - INFO - __main__ - Global step 150 Train loss 5.61 Classification-F1 0.0 on epoch=74
06/24/2022 12:38:49 - INFO - __main__ - Step 160 Global step 160 Train loss 5.47 on epoch=79
06/24/2022 12:38:50 - INFO - __main__ - Step 170 Global step 170 Train loss 5.49 on epoch=84
06/24/2022 12:38:51 - INFO - __main__ - Step 180 Global step 180 Train loss 5.53 on epoch=89
06/24/2022 12:38:52 - INFO - __main__ - Step 190 Global step 190 Train loss 5.58 on epoch=94
06/24/2022 12:38:54 - INFO - __main__ - Step 200 Global step 200 Train loss 5.53 on epoch=99
06/24/2022 12:39:04 - INFO - __main__ - Global step 200 Train loss 5.52 Classification-F1 0.0 on epoch=99
06/24/2022 12:39:05 - INFO - __main__ - Step 210 Global step 210 Train loss 5.49 on epoch=104
06/24/2022 12:39:06 - INFO - __main__ - Step 220 Global step 220 Train loss 5.46 on epoch=109
06/24/2022 12:39:07 - INFO - __main__ - Step 230 Global step 230 Train loss 5.38 on epoch=114
06/24/2022 12:39:09 - INFO - __main__ - Step 240 Global step 240 Train loss 5.45 on epoch=119
06/24/2022 12:39:10 - INFO - __main__ - Step 250 Global step 250 Train loss 5.44 on epoch=124
06/24/2022 12:39:12 - INFO - __main__ - Global step 250 Train loss 5.44 Classification-F1 0.0 on epoch=124
06/24/2022 12:39:13 - INFO - __main__ - Step 260 Global step 260 Train loss 5.29 on epoch=129
06/24/2022 12:39:14 - INFO - __main__ - Step 270 Global step 270 Train loss 5.34 on epoch=134
06/24/2022 12:39:16 - INFO - __main__ - Step 280 Global step 280 Train loss 5.32 on epoch=139
06/24/2022 12:39:17 - INFO - __main__ - Step 290 Global step 290 Train loss 5.37 on epoch=144
06/24/2022 12:39:18 - INFO - __main__ - Step 300 Global step 300 Train loss 5.24 on epoch=149
06/24/2022 12:39:21 - INFO - __main__ - Global step 300 Train loss 5.31 Classification-F1 0.0 on epoch=149
06/24/2022 12:39:22 - INFO - __main__ - Step 310 Global step 310 Train loss 5.11 on epoch=154
06/24/2022 12:39:23 - INFO - __main__ - Step 320 Global step 320 Train loss 5.13 on epoch=159
06/24/2022 12:39:25 - INFO - __main__ - Step 330 Global step 330 Train loss 4.93 on epoch=164
06/24/2022 12:39:26 - INFO - __main__ - Step 340 Global step 340 Train loss 4.64 on epoch=169
06/24/2022 12:39:27 - INFO - __main__ - Step 350 Global step 350 Train loss 4.62 on epoch=174
06/24/2022 12:39:38 - INFO - __main__ - Global step 350 Train loss 4.89 Classification-F1 0.0 on epoch=174
06/24/2022 12:39:39 - INFO - __main__ - Step 360 Global step 360 Train loss 4.37 on epoch=179
06/24/2022 12:39:40 - INFO - __main__ - Step 370 Global step 370 Train loss 4.38 on epoch=184
06/24/2022 12:39:41 - INFO - __main__ - Step 380 Global step 380 Train loss 4.36 on epoch=189
06/24/2022 12:39:43 - INFO - __main__ - Step 390 Global step 390 Train loss 4.20 on epoch=194
06/24/2022 12:39:44 - INFO - __main__ - Step 400 Global step 400 Train loss 3.98 on epoch=199
06/24/2022 12:39:55 - INFO - __main__ - Global step 400 Train loss 4.26 Classification-F1 0.0 on epoch=199
06/24/2022 12:39:56 - INFO - __main__ - Step 410 Global step 410 Train loss 4.10 on epoch=204
06/24/2022 12:39:57 - INFO - __main__ - Step 420 Global step 420 Train loss 3.96 on epoch=209
06/24/2022 12:39:58 - INFO - __main__ - Step 430 Global step 430 Train loss 3.94 on epoch=214
06/24/2022 12:40:00 - INFO - __main__ - Step 440 Global step 440 Train loss 3.78 on epoch=219
06/24/2022 12:40:01 - INFO - __main__ - Step 450 Global step 450 Train loss 3.79 on epoch=224
06/24/2022 12:40:09 - INFO - __main__ - Global step 450 Train loss 3.91 Classification-F1 0.0 on epoch=224
06/24/2022 12:40:10 - INFO - __main__ - Step 460 Global step 460 Train loss 3.73 on epoch=229
06/24/2022 12:40:11 - INFO - __main__ - Step 470 Global step 470 Train loss 3.69 on epoch=234
06/24/2022 12:40:12 - INFO - __main__ - Step 480 Global step 480 Train loss 3.66 on epoch=239
06/24/2022 12:40:13 - INFO - __main__ - Step 490 Global step 490 Train loss 3.47 on epoch=244
06/24/2022 12:40:15 - INFO - __main__ - Step 500 Global step 500 Train loss 3.39 on epoch=249
06/24/2022 12:40:22 - INFO - __main__ - Global step 500 Train loss 3.59 Classification-F1 0.0 on epoch=249
06/24/2022 12:40:23 - INFO - __main__ - Step 510 Global step 510 Train loss 3.46 on epoch=254
06/24/2022 12:40:25 - INFO - __main__ - Step 520 Global step 520 Train loss 3.38 on epoch=259
06/24/2022 12:40:26 - INFO - __main__ - Step 530 Global step 530 Train loss 3.28 on epoch=264
06/24/2022 12:40:27 - INFO - __main__ - Step 540 Global step 540 Train loss 3.31 on epoch=269
06/24/2022 12:40:28 - INFO - __main__ - Step 550 Global step 550 Train loss 3.19 on epoch=274
06/24/2022 12:40:39 - INFO - __main__ - Global step 550 Train loss 3.32 Classification-F1 0.0 on epoch=274
06/24/2022 12:40:40 - INFO - __main__ - Step 560 Global step 560 Train loss 3.12 on epoch=279
06/24/2022 12:40:41 - INFO - __main__ - Step 570 Global step 570 Train loss 2.98 on epoch=284
06/24/2022 12:40:42 - INFO - __main__ - Step 580 Global step 580 Train loss 2.90 on epoch=289
06/24/2022 12:40:44 - INFO - __main__ - Step 590 Global step 590 Train loss 2.93 on epoch=294
06/24/2022 12:40:45 - INFO - __main__ - Step 600 Global step 600 Train loss 2.92 on epoch=299
06/24/2022 12:40:55 - INFO - __main__ - Global step 600 Train loss 2.97 Classification-F1 0.06666666666666668 on epoch=299
06/24/2022 12:40:55 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.06666666666666668 on epoch=299, global_step=600
06/24/2022 12:40:57 - INFO - __main__ - Step 610 Global step 610 Train loss 2.87 on epoch=304
06/24/2022 12:40:58 - INFO - __main__ - Step 620 Global step 620 Train loss 2.66 on epoch=309
06/24/2022 12:40:59 - INFO - __main__ - Step 630 Global step 630 Train loss 2.72 on epoch=314
06/24/2022 12:41:00 - INFO - __main__ - Step 640 Global step 640 Train loss 2.67 on epoch=319
06/24/2022 12:41:01 - INFO - __main__ - Step 650 Global step 650 Train loss 2.50 on epoch=324
06/24/2022 12:41:11 - INFO - __main__ - Global step 650 Train loss 2.68 Classification-F1 0.10526315789473684 on epoch=324
06/24/2022 12:41:11 - INFO - __main__ - Saving model with best Classification-F1: 0.06666666666666668 -> 0.10526315789473684 on epoch=324, global_step=650
06/24/2022 12:41:13 - INFO - __main__ - Step 660 Global step 660 Train loss 2.51 on epoch=329
06/24/2022 12:41:14 - INFO - __main__ - Step 670 Global step 670 Train loss 2.38 on epoch=334
06/24/2022 12:41:15 - INFO - __main__ - Step 680 Global step 680 Train loss 2.47 on epoch=339
06/24/2022 12:41:16 - INFO - __main__ - Step 690 Global step 690 Train loss 2.48 on epoch=344
06/24/2022 12:41:17 - INFO - __main__ - Step 700 Global step 700 Train loss 2.33 on epoch=349
06/24/2022 12:41:24 - INFO - __main__ - Global step 700 Train loss 2.43 Classification-F1 0.1816216216216216 on epoch=349
06/24/2022 12:41:25 - INFO - __main__ - Saving model with best Classification-F1: 0.10526315789473684 -> 0.1816216216216216 on epoch=349, global_step=700
06/24/2022 12:41:26 - INFO - __main__ - Step 710 Global step 710 Train loss 2.19 on epoch=354
06/24/2022 12:41:27 - INFO - __main__ - Step 720 Global step 720 Train loss 2.17 on epoch=359
06/24/2022 12:41:28 - INFO - __main__ - Step 730 Global step 730 Train loss 2.23 on epoch=364
06/24/2022 12:41:29 - INFO - __main__ - Step 740 Global step 740 Train loss 2.09 on epoch=369
06/24/2022 12:41:31 - INFO - __main__ - Step 750 Global step 750 Train loss 2.13 on epoch=374
06/24/2022 12:41:37 - INFO - __main__ - Global step 750 Train loss 2.16 Classification-F1 0.21739130434782608 on epoch=374
06/24/2022 12:41:37 - INFO - __main__ - Saving model with best Classification-F1: 0.1816216216216216 -> 0.21739130434782608 on epoch=374, global_step=750
06/24/2022 12:41:38 - INFO - __main__ - Step 760 Global step 760 Train loss 1.98 on epoch=379
06/24/2022 12:41:39 - INFO - __main__ - Step 770 Global step 770 Train loss 1.89 on epoch=384
06/24/2022 12:41:40 - INFO - __main__ - Step 780 Global step 780 Train loss 1.76 on epoch=389
06/24/2022 12:41:42 - INFO - __main__ - Step 790 Global step 790 Train loss 1.78 on epoch=394
06/24/2022 12:41:43 - INFO - __main__ - Step 800 Global step 800 Train loss 1.84 on epoch=399
06/24/2022 12:41:45 - INFO - __main__ - Global step 800 Train loss 1.85 Classification-F1 0.3992490613266583 on epoch=399
06/24/2022 12:41:45 - INFO - __main__ - Saving model with best Classification-F1: 0.21739130434782608 -> 0.3992490613266583 on epoch=399, global_step=800
06/24/2022 12:41:46 - INFO - __main__ - Step 810 Global step 810 Train loss 1.65 on epoch=404
06/24/2022 12:41:48 - INFO - __main__ - Step 820 Global step 820 Train loss 1.73 on epoch=409
06/24/2022 12:41:49 - INFO - __main__ - Step 830 Global step 830 Train loss 1.62 on epoch=414
06/24/2022 12:41:50 - INFO - __main__ - Step 840 Global step 840 Train loss 1.55 on epoch=419
06/24/2022 12:41:51 - INFO - __main__ - Step 850 Global step 850 Train loss 1.61 on epoch=424
06/24/2022 12:41:52 - INFO - __main__ - Global step 850 Train loss 1.63 Classification-F1 0.3333333333333333 on epoch=424
06/24/2022 12:41:53 - INFO - __main__ - Step 860 Global step 860 Train loss 1.49 on epoch=429
06/24/2022 12:41:54 - INFO - __main__ - Step 870 Global step 870 Train loss 1.49 on epoch=434
06/24/2022 12:41:56 - INFO - __main__ - Step 880 Global step 880 Train loss 1.44 on epoch=439
06/24/2022 12:41:57 - INFO - __main__ - Step 890 Global step 890 Train loss 1.44 on epoch=444
06/24/2022 12:41:58 - INFO - __main__ - Step 900 Global step 900 Train loss 1.30 on epoch=449
06/24/2022 12:41:58 - INFO - __main__ - Global step 900 Train loss 1.43 Classification-F1 0.3333333333333333 on epoch=449
06/24/2022 12:42:00 - INFO - __main__ - Step 910 Global step 910 Train loss 1.31 on epoch=454
06/24/2022 12:42:01 - INFO - __main__ - Step 920 Global step 920 Train loss 1.27 on epoch=459
06/24/2022 12:42:02 - INFO - __main__ - Step 930 Global step 930 Train loss 1.34 on epoch=464
06/24/2022 12:42:03 - INFO - __main__ - Step 940 Global step 940 Train loss 1.32 on epoch=469
06/24/2022 12:42:04 - INFO - __main__ - Step 950 Global step 950 Train loss 1.33 on epoch=474
06/24/2022 12:42:05 - INFO - __main__ - Global step 950 Train loss 1.31 Classification-F1 0.3333333333333333 on epoch=474
06/24/2022 12:42:06 - INFO - __main__ - Step 960 Global step 960 Train loss 1.12 on epoch=479
06/24/2022 12:42:07 - INFO - __main__ - Step 970 Global step 970 Train loss 1.16 on epoch=484
06/24/2022 12:42:08 - INFO - __main__ - Step 980 Global step 980 Train loss 1.15 on epoch=489
06/24/2022 12:42:10 - INFO - __main__ - Step 990 Global step 990 Train loss 1.08 on epoch=494
06/24/2022 12:42:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.06 on epoch=499
06/24/2022 12:42:11 - INFO - __main__ - Global step 1000 Train loss 1.12 Classification-F1 0.3333333333333333 on epoch=499
06/24/2022 12:42:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.08 on epoch=504
06/24/2022 12:42:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.07 on epoch=509
06/24/2022 12:42:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.05 on epoch=514
06/24/2022 12:42:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.09 on epoch=519
06/24/2022 12:42:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.03 on epoch=524
06/24/2022 12:42:18 - INFO - __main__ - Global step 1050 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=524
06/24/2022 12:42:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.00 on epoch=529
06/24/2022 12:42:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.04 on epoch=534
06/24/2022 12:42:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.03 on epoch=539
06/24/2022 12:42:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.01 on epoch=544
06/24/2022 12:42:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.99 on epoch=549
06/24/2022 12:42:24 - INFO - __main__ - Global step 1100 Train loss 1.01 Classification-F1 0.3992490613266583 on epoch=549
06/24/2022 12:42:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.96 on epoch=554
06/24/2022 12:42:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.87 on epoch=559
06/24/2022 12:42:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.82 on epoch=564
06/24/2022 12:42:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.86 on epoch=569
06/24/2022 12:42:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.92 on epoch=574
06/24/2022 12:42:30 - INFO - __main__ - Global step 1150 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=574
06/24/2022 12:42:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.87 on epoch=579
06/24/2022 12:42:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.82 on epoch=584
06/24/2022 12:42:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.83 on epoch=589
06/24/2022 12:42:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.77 on epoch=594
06/24/2022 12:42:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.83 on epoch=599
06/24/2022 12:42:37 - INFO - __main__ - Global step 1200 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=599
06/24/2022 12:42:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.89 on epoch=604
06/24/2022 12:42:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.77 on epoch=609
06/24/2022 12:42:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.77 on epoch=614
06/24/2022 12:42:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.79 on epoch=619
06/24/2022 12:42:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.75 on epoch=624
06/24/2022 12:42:43 - INFO - __main__ - Global step 1250 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=624
06/24/2022 12:42:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.80 on epoch=629
06/24/2022 12:42:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.73 on epoch=634
06/24/2022 12:42:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.75 on epoch=639
06/24/2022 12:42:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.68 on epoch=644
06/24/2022 12:42:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.71 on epoch=649
06/24/2022 12:42:50 - INFO - __main__ - Global step 1300 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=649
06/24/2022 12:42:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.75 on epoch=654
06/24/2022 12:42:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.75 on epoch=659
06/24/2022 12:42:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.71 on epoch=664
06/24/2022 12:42:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.79 on epoch=669
06/24/2022 12:42:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.71 on epoch=674
06/24/2022 12:42:56 - INFO - __main__ - Global step 1350 Train loss 0.74 Classification-F1 0.3191489361702127 on epoch=674
06/24/2022 12:42:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.76 on epoch=679
06/24/2022 12:42:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.68 on epoch=684
06/24/2022 12:43:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.74 on epoch=689
06/24/2022 12:43:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.65 on epoch=694
06/24/2022 12:43:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.63 on epoch=699
06/24/2022 12:43:02 - INFO - __main__ - Global step 1400 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=699
06/24/2022 12:43:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.74 on epoch=704
06/24/2022 12:43:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.75 on epoch=709
06/24/2022 12:43:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.71 on epoch=714
06/24/2022 12:43:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.72 on epoch=719
06/24/2022 12:43:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.68 on epoch=724
06/24/2022 12:43:09 - INFO - __main__ - Global step 1450 Train loss 0.72 Classification-F1 0.3191489361702127 on epoch=724
06/24/2022 12:43:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.76 on epoch=729
06/24/2022 12:43:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.64 on epoch=734
06/24/2022 12:43:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.67 on epoch=739
06/24/2022 12:43:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.62 on epoch=744
06/24/2022 12:43:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.66 on epoch=749
06/24/2022 12:43:15 - INFO - __main__ - Global step 1500 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=749
06/24/2022 12:43:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.70 on epoch=754
06/24/2022 12:43:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.59 on epoch=759
06/24/2022 12:43:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.59 on epoch=764
06/24/2022 12:43:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.58 on epoch=769
06/24/2022 12:43:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.58 on epoch=774
06/24/2022 12:43:22 - INFO - __main__ - Global step 1550 Train loss 0.61 Classification-F1 0.3191489361702127 on epoch=774
06/24/2022 12:43:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.56 on epoch=779
06/24/2022 12:43:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.63 on epoch=784
06/24/2022 12:43:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.57 on epoch=789
06/24/2022 12:43:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.67 on epoch=794
06/24/2022 12:43:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.61 on epoch=799
06/24/2022 12:43:28 - INFO - __main__ - Global step 1600 Train loss 0.61 Classification-F1 0.4589371980676329 on epoch=799
06/24/2022 12:43:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4589371980676329 on epoch=799, global_step=1600
06/24/2022 12:43:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.60 on epoch=804
06/24/2022 12:43:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.71 on epoch=809
06/24/2022 12:43:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.64 on epoch=814
06/24/2022 12:43:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.68 on epoch=819
06/24/2022 12:43:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.62 on epoch=824
06/24/2022 12:43:34 - INFO - __main__ - Global step 1650 Train loss 0.65 Classification-F1 0.3992490613266583 on epoch=824
06/24/2022 12:43:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.59 on epoch=829
06/24/2022 12:43:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.51 on epoch=834
06/24/2022 12:43:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.62 on epoch=839
06/24/2022 12:43:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.66 on epoch=844
06/24/2022 12:43:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.68 on epoch=849
06/24/2022 12:43:41 - INFO - __main__ - Global step 1700 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=849
06/24/2022 12:43:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.56 on epoch=854
06/24/2022 12:43:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.57 on epoch=859
06/24/2022 12:43:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.60 on epoch=864
06/24/2022 12:43:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=869
06/24/2022 12:43:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.51 on epoch=874
06/24/2022 12:43:47 - INFO - __main__ - Global step 1750 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=874
06/24/2022 12:43:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.55 on epoch=879
06/24/2022 12:43:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.58 on epoch=884
06/24/2022 12:43:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.65 on epoch=889
06/24/2022 12:43:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.55 on epoch=894
06/24/2022 12:43:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.56 on epoch=899
06/24/2022 12:43:54 - INFO - __main__ - Global step 1800 Train loss 0.58 Classification-F1 0.3816425120772947 on epoch=899
06/24/2022 12:43:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.55 on epoch=904
06/24/2022 12:43:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.57 on epoch=909
06/24/2022 12:43:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.62 on epoch=914
06/24/2022 12:43:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.58 on epoch=919
06/24/2022 12:44:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.58 on epoch=924
06/24/2022 12:44:00 - INFO - __main__ - Global step 1850 Train loss 0.58 Classification-F1 0.3454545454545454 on epoch=924
06/24/2022 12:44:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.51 on epoch=929
06/24/2022 12:44:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.56 on epoch=934
06/24/2022 12:44:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.53 on epoch=939
06/24/2022 12:44:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.60 on epoch=944
06/24/2022 12:44:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.54 on epoch=949
06/24/2022 12:44:07 - INFO - __main__ - Global step 1900 Train loss 0.55 Classification-F1 0.28888888888888886 on epoch=949
06/24/2022 12:44:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.55 on epoch=954
06/24/2022 12:44:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.52 on epoch=959
06/24/2022 12:44:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.57 on epoch=964
06/24/2022 12:44:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.53 on epoch=969
06/24/2022 12:44:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.68 on epoch=974
06/24/2022 12:44:13 - INFO - __main__ - Global step 1950 Train loss 0.57 Classification-F1 0.3992490613266583 on epoch=974
06/24/2022 12:44:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.56 on epoch=979
06/24/2022 12:44:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.53 on epoch=984
06/24/2022 12:44:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.45 on epoch=989
06/24/2022 12:44:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.53 on epoch=994
06/24/2022 12:44:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.55 on epoch=999
06/24/2022 12:44:19 - INFO - __main__ - Global step 2000 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=999
06/24/2022 12:44:19 - INFO - __main__ - save last model!
06/24/2022 12:44:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 12:44:19 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 12:44:19 - INFO - __main__ - Printing 3 examples
06/24/2022 12:44:19 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 12:44:19 - INFO - __main__ - ['0']
06/24/2022 12:44:19 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 12:44:19 - INFO - __main__ - ['1']
06/24/2022 12:44:19 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 12:44:19 - INFO - __main__ - ['1']
06/24/2022 12:44:19 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:44:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:44:20 - INFO - __main__ - Printing 3 examples
06/24/2022 12:44:20 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 12:44:20 - INFO - __main__ - ['0']
06/24/2022 12:44:20 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 12:44:20 - INFO - __main__ - ['0']
06/24/2022 12:44:20 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 12:44:20 - INFO - __main__ - ['0']
06/24/2022 12:44:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 12:44:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:44:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:44:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:44:20 - INFO - __main__ - Printing 3 examples
06/24/2022 12:44:20 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 12:44:20 - INFO - __main__ - ['0']
06/24/2022 12:44:20 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 12:44:20 - INFO - __main__ - ['0']
06/24/2022 12:44:20 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 12:44:20 - INFO - __main__ - ['0']
06/24/2022 12:44:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:44:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:44:20 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:44:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:44:26 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:44:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:44:27 - INFO - __main__ - Starting training!
06/24/2022 12:44:31 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 12:45:55 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_87_0.4_8_predictions.txt
06/24/2022 12:45:55 - INFO - __main__ - Classification-F1 on test data: 0.3198
06/24/2022 12:45:56 - INFO - __main__ - prefix=paws_16_87, lr=0.4, bsz=8, dev_performance=0.4589371980676329, test_performance=0.3197962952438908
06/24/2022 12:45:56 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.3, bsz=8 ...
06/24/2022 12:45:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:45:56 - INFO - __main__ - Printing 3 examples
06/24/2022 12:45:56 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 12:45:56 - INFO - __main__ - ['0']
06/24/2022 12:45:56 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 12:45:56 - INFO - __main__ - ['0']
06/24/2022 12:45:56 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 12:45:56 - INFO - __main__ - ['0']
06/24/2022 12:45:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:45:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:45:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:45:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:45:57 - INFO - __main__ - Printing 3 examples
06/24/2022 12:45:57 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 12:45:57 - INFO - __main__ - ['0']
06/24/2022 12:45:57 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 12:45:57 - INFO - __main__ - ['0']
06/24/2022 12:45:57 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 12:45:57 - INFO - __main__ - ['0']
06/24/2022 12:45:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:45:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:45:57 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:46:03 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:46:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:46:03 - INFO - __main__ - Starting training!
06/24/2022 12:46:04 - INFO - __main__ - Step 10 Global step 10 Train loss 5.71 on epoch=4
06/24/2022 12:46:06 - INFO - __main__ - Step 20 Global step 20 Train loss 5.74 on epoch=9
06/24/2022 12:46:07 - INFO - __main__ - Step 30 Global step 30 Train loss 5.70 on epoch=14
06/24/2022 12:46:08 - INFO - __main__ - Step 40 Global step 40 Train loss 5.80 on epoch=19
06/24/2022 12:46:09 - INFO - __main__ - Step 50 Global step 50 Train loss 5.63 on epoch=24
06/24/2022 12:46:10 - INFO - __main__ - Global step 50 Train loss 5.71 Classification-F1 0.0 on epoch=24
06/24/2022 12:46:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 12:46:11 - INFO - __main__ - Step 60 Global step 60 Train loss 5.73 on epoch=29
06/24/2022 12:46:13 - INFO - __main__ - Step 70 Global step 70 Train loss 5.68 on epoch=34
06/24/2022 12:46:14 - INFO - __main__ - Step 80 Global step 80 Train loss 5.56 on epoch=39
06/24/2022 12:46:15 - INFO - __main__ - Step 90 Global step 90 Train loss 5.64 on epoch=44
06/24/2022 12:46:17 - INFO - __main__ - Step 100 Global step 100 Train loss 5.57 on epoch=49
06/24/2022 12:46:18 - INFO - __main__ - Global step 100 Train loss 5.64 Classification-F1 0.0 on epoch=49
06/24/2022 12:46:19 - INFO - __main__ - Step 110 Global step 110 Train loss 5.56 on epoch=54
06/24/2022 12:46:21 - INFO - __main__ - Step 120 Global step 120 Train loss 5.59 on epoch=59
06/24/2022 12:46:22 - INFO - __main__ - Step 130 Global step 130 Train loss 5.58 on epoch=64
06/24/2022 12:46:23 - INFO - __main__ - Step 140 Global step 140 Train loss 5.64 on epoch=69
06/24/2022 12:46:24 - INFO - __main__ - Step 150 Global step 150 Train loss 5.59 on epoch=74
06/24/2022 12:46:26 - INFO - __main__ - Global step 150 Train loss 5.59 Classification-F1 0.0 on epoch=74
06/24/2022 12:46:27 - INFO - __main__ - Step 160 Global step 160 Train loss 5.57 on epoch=79
06/24/2022 12:46:29 - INFO - __main__ - Step 170 Global step 170 Train loss 5.60 on epoch=84
06/24/2022 12:46:30 - INFO - __main__ - Step 180 Global step 180 Train loss 5.47 on epoch=89
06/24/2022 12:46:31 - INFO - __main__ - Step 190 Global step 190 Train loss 5.56 on epoch=94
06/24/2022 12:46:32 - INFO - __main__ - Step 200 Global step 200 Train loss 5.52 on epoch=99
06/24/2022 12:46:34 - INFO - __main__ - Global step 200 Train loss 5.54 Classification-F1 0.0 on epoch=99
06/24/2022 12:46:36 - INFO - __main__ - Step 210 Global step 210 Train loss 5.41 on epoch=104
06/24/2022 12:46:37 - INFO - __main__ - Step 220 Global step 220 Train loss 5.37 on epoch=109
06/24/2022 12:46:38 - INFO - __main__ - Step 230 Global step 230 Train loss 5.32 on epoch=114
06/24/2022 12:46:39 - INFO - __main__ - Step 240 Global step 240 Train loss 5.40 on epoch=119
06/24/2022 12:46:41 - INFO - __main__ - Step 250 Global step 250 Train loss 5.37 on epoch=124
06/24/2022 12:46:41 - INFO - __main__ - Global step 250 Train loss 5.37 Classification-F1 0.0 on epoch=124
06/24/2022 12:46:43 - INFO - __main__ - Step 260 Global step 260 Train loss 5.32 on epoch=129
06/24/2022 12:46:44 - INFO - __main__ - Step 270 Global step 270 Train loss 5.37 on epoch=134
06/24/2022 12:46:45 - INFO - __main__ - Step 280 Global step 280 Train loss 5.25 on epoch=139
06/24/2022 12:46:46 - INFO - __main__ - Step 290 Global step 290 Train loss 5.21 on epoch=144
06/24/2022 12:46:47 - INFO - __main__ - Step 300 Global step 300 Train loss 5.30 on epoch=149
06/24/2022 12:46:49 - INFO - __main__ - Global step 300 Train loss 5.29 Classification-F1 0.0 on epoch=149
06/24/2022 12:46:50 - INFO - __main__ - Step 310 Global step 310 Train loss 5.25 on epoch=154
06/24/2022 12:46:51 - INFO - __main__ - Step 320 Global step 320 Train loss 5.19 on epoch=159
06/24/2022 12:46:52 - INFO - __main__ - Step 330 Global step 330 Train loss 5.20 on epoch=164
06/24/2022 12:46:54 - INFO - __main__ - Step 340 Global step 340 Train loss 5.09 on epoch=169
06/24/2022 12:46:55 - INFO - __main__ - Step 350 Global step 350 Train loss 5.14 on epoch=174
06/24/2022 12:46:58 - INFO - __main__ - Global step 350 Train loss 5.17 Classification-F1 0.0 on epoch=174
06/24/2022 12:46:59 - INFO - __main__ - Step 360 Global step 360 Train loss 4.99 on epoch=179
06/24/2022 12:47:00 - INFO - __main__ - Step 370 Global step 370 Train loss 5.02 on epoch=184
06/24/2022 12:47:01 - INFO - __main__ - Step 380 Global step 380 Train loss 4.96 on epoch=189
06/24/2022 12:47:03 - INFO - __main__ - Step 390 Global step 390 Train loss 4.79 on epoch=194
06/24/2022 12:47:04 - INFO - __main__ - Step 400 Global step 400 Train loss 4.84 on epoch=199
06/24/2022 12:47:14 - INFO - __main__ - Global step 400 Train loss 4.92 Classification-F1 0.0 on epoch=199
06/24/2022 12:47:15 - INFO - __main__ - Step 410 Global step 410 Train loss 4.80 on epoch=204
06/24/2022 12:47:17 - INFO - __main__ - Step 420 Global step 420 Train loss 4.71 on epoch=209
06/24/2022 12:47:18 - INFO - __main__ - Step 430 Global step 430 Train loss 4.57 on epoch=214
06/24/2022 12:47:19 - INFO - __main__ - Step 440 Global step 440 Train loss 4.47 on epoch=219
06/24/2022 12:47:20 - INFO - __main__ - Step 450 Global step 450 Train loss 4.36 on epoch=224
06/24/2022 12:47:31 - INFO - __main__ - Global step 450 Train loss 4.58 Classification-F1 0.0 on epoch=224
06/24/2022 12:47:32 - INFO - __main__ - Step 460 Global step 460 Train loss 4.25 on epoch=229
06/24/2022 12:47:33 - INFO - __main__ - Step 470 Global step 470 Train loss 4.20 on epoch=234
06/24/2022 12:47:34 - INFO - __main__ - Step 480 Global step 480 Train loss 4.25 on epoch=239
06/24/2022 12:47:36 - INFO - __main__ - Step 490 Global step 490 Train loss 4.08 on epoch=244
06/24/2022 12:47:37 - INFO - __main__ - Step 500 Global step 500 Train loss 3.92 on epoch=249
06/24/2022 12:47:43 - INFO - __main__ - Global step 500 Train loss 4.14 Classification-F1 0.0 on epoch=249
06/24/2022 12:47:44 - INFO - __main__ - Step 510 Global step 510 Train loss 3.83 on epoch=254
06/24/2022 12:47:45 - INFO - __main__ - Step 520 Global step 520 Train loss 3.77 on epoch=259
06/24/2022 12:47:46 - INFO - __main__ - Step 530 Global step 530 Train loss 3.68 on epoch=264
06/24/2022 12:47:48 - INFO - __main__ - Step 540 Global step 540 Train loss 3.58 on epoch=269
06/24/2022 12:47:49 - INFO - __main__ - Step 550 Global step 550 Train loss 3.57 on epoch=274
06/24/2022 12:47:59 - INFO - __main__ - Global step 550 Train loss 3.69 Classification-F1 0.0 on epoch=274
06/24/2022 12:48:00 - INFO - __main__ - Step 560 Global step 560 Train loss 3.70 on epoch=279
06/24/2022 12:48:02 - INFO - __main__ - Step 570 Global step 570 Train loss 3.63 on epoch=284
06/24/2022 12:48:03 - INFO - __main__ - Step 580 Global step 580 Train loss 3.65 on epoch=289
06/24/2022 12:48:04 - INFO - __main__ - Step 590 Global step 590 Train loss 3.57 on epoch=294
06/24/2022 12:48:05 - INFO - __main__ - Step 600 Global step 600 Train loss 3.47 on epoch=299
06/24/2022 12:48:16 - INFO - __main__ - Global step 600 Train loss 3.60 Classification-F1 0.0 on epoch=299
06/24/2022 12:48:17 - INFO - __main__ - Step 610 Global step 610 Train loss 3.36 on epoch=304
06/24/2022 12:48:18 - INFO - __main__ - Step 620 Global step 620 Train loss 3.27 on epoch=309
06/24/2022 12:48:20 - INFO - __main__ - Step 630 Global step 630 Train loss 3.26 on epoch=314
06/24/2022 12:48:21 - INFO - __main__ - Step 640 Global step 640 Train loss 3.07 on epoch=319
06/24/2022 12:48:22 - INFO - __main__ - Step 650 Global step 650 Train loss 3.02 on epoch=324
06/24/2022 12:48:32 - INFO - __main__ - Global step 650 Train loss 3.20 Classification-F1 0.04347826086956521 on epoch=324
06/24/2022 12:48:32 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.04347826086956521 on epoch=324, global_step=650
06/24/2022 12:48:33 - INFO - __main__ - Step 660 Global step 660 Train loss 2.89 on epoch=329
06/24/2022 12:48:35 - INFO - __main__ - Step 670 Global step 670 Train loss 2.94 on epoch=334
06/24/2022 12:48:36 - INFO - __main__ - Step 680 Global step 680 Train loss 2.78 on epoch=339
06/24/2022 12:48:37 - INFO - __main__ - Step 690 Global step 690 Train loss 2.84 on epoch=344
06/24/2022 12:48:38 - INFO - __main__ - Step 700 Global step 700 Train loss 2.71 on epoch=349
06/24/2022 12:48:49 - INFO - __main__ - Global step 700 Train loss 2.83 Classification-F1 0.06245461147421932 on epoch=349
06/24/2022 12:48:49 - INFO - __main__ - Saving model with best Classification-F1: 0.04347826086956521 -> 0.06245461147421932 on epoch=349, global_step=700
06/24/2022 12:48:50 - INFO - __main__ - Step 710 Global step 710 Train loss 2.58 on epoch=354
06/24/2022 12:48:51 - INFO - __main__ - Step 720 Global step 720 Train loss 2.53 on epoch=359
06/24/2022 12:48:53 - INFO - __main__ - Step 730 Global step 730 Train loss 2.44 on epoch=364
06/24/2022 12:48:54 - INFO - __main__ - Step 740 Global step 740 Train loss 2.55 on epoch=369
06/24/2022 12:48:55 - INFO - __main__ - Step 750 Global step 750 Train loss 2.68 on epoch=374
06/24/2022 12:49:06 - INFO - __main__ - Global step 750 Train loss 2.55 Classification-F1 0.048935813641696 on epoch=374
06/24/2022 12:49:07 - INFO - __main__ - Step 760 Global step 760 Train loss 2.34 on epoch=379
06/24/2022 12:49:08 - INFO - __main__ - Step 770 Global step 770 Train loss 2.36 on epoch=384
06/24/2022 12:49:10 - INFO - __main__ - Step 780 Global step 780 Train loss 2.31 on epoch=389
06/24/2022 12:49:11 - INFO - __main__ - Step 790 Global step 790 Train loss 2.33 on epoch=394
06/24/2022 12:49:12 - INFO - __main__ - Step 800 Global step 800 Train loss 2.28 on epoch=399
06/24/2022 12:49:18 - INFO - __main__ - Global step 800 Train loss 2.33 Classification-F1 0.17747858017135862 on epoch=399
06/24/2022 12:49:18 - INFO - __main__ - Saving model with best Classification-F1: 0.06245461147421932 -> 0.17747858017135862 on epoch=399, global_step=800
06/24/2022 12:49:20 - INFO - __main__ - Step 810 Global step 810 Train loss 2.13 on epoch=404
06/24/2022 12:49:21 - INFO - __main__ - Step 820 Global step 820 Train loss 2.04 on epoch=409
06/24/2022 12:49:22 - INFO - __main__ - Step 830 Global step 830 Train loss 2.16 on epoch=414
06/24/2022 12:49:23 - INFO - __main__ - Step 840 Global step 840 Train loss 2.14 on epoch=419
06/24/2022 12:49:25 - INFO - __main__ - Step 850 Global step 850 Train loss 2.03 on epoch=424
06/24/2022 12:49:26 - INFO - __main__ - Global step 850 Train loss 2.10 Classification-F1 0.3333333333333333 on epoch=424
06/24/2022 12:49:26 - INFO - __main__ - Saving model with best Classification-F1: 0.17747858017135862 -> 0.3333333333333333 on epoch=424, global_step=850
06/24/2022 12:49:28 - INFO - __main__ - Step 860 Global step 860 Train loss 1.83 on epoch=429
06/24/2022 12:49:29 - INFO - __main__ - Step 870 Global step 870 Train loss 1.85 on epoch=434
06/24/2022 12:49:30 - INFO - __main__ - Step 880 Global step 880 Train loss 1.91 on epoch=439
06/24/2022 12:49:31 - INFO - __main__ - Step 890 Global step 890 Train loss 1.83 on epoch=444
06/24/2022 12:49:33 - INFO - __main__ - Step 900 Global step 900 Train loss 1.82 on epoch=449
06/24/2022 12:49:35 - INFO - __main__ - Global step 900 Train loss 1.85 Classification-F1 0.3333333333333333 on epoch=449
06/24/2022 12:49:36 - INFO - __main__ - Step 910 Global step 910 Train loss 1.79 on epoch=454
06/24/2022 12:49:37 - INFO - __main__ - Step 920 Global step 920 Train loss 1.58 on epoch=459
06/24/2022 12:49:38 - INFO - __main__ - Step 930 Global step 930 Train loss 1.75 on epoch=464
06/24/2022 12:49:40 - INFO - __main__ - Step 940 Global step 940 Train loss 1.75 on epoch=469
06/24/2022 12:49:41 - INFO - __main__ - Step 950 Global step 950 Train loss 1.55 on epoch=474
06/24/2022 12:49:41 - INFO - __main__ - Global step 950 Train loss 1.68 Classification-F1 0.3333333333333333 on epoch=474
06/24/2022 12:49:43 - INFO - __main__ - Step 960 Global step 960 Train loss 1.65 on epoch=479
06/24/2022 12:49:44 - INFO - __main__ - Step 970 Global step 970 Train loss 1.45 on epoch=484
06/24/2022 12:49:45 - INFO - __main__ - Step 980 Global step 980 Train loss 1.61 on epoch=489
06/24/2022 12:49:46 - INFO - __main__ - Step 990 Global step 990 Train loss 1.45 on epoch=494
06/24/2022 12:49:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.51 on epoch=499
06/24/2022 12:49:48 - INFO - __main__ - Global step 1000 Train loss 1.54 Classification-F1 0.3333333333333333 on epoch=499
06/24/2022 12:49:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.48 on epoch=504
06/24/2022 12:49:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.53 on epoch=509
06/24/2022 12:49:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.36 on epoch=514
06/24/2022 12:49:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.38 on epoch=519
06/24/2022 12:49:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.37 on epoch=524
06/24/2022 12:49:55 - INFO - __main__ - Global step 1050 Train loss 1.43 Classification-F1 0.3333333333333333 on epoch=524
06/24/2022 12:49:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.29 on epoch=529
06/24/2022 12:49:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.23 on epoch=534
06/24/2022 12:49:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.26 on epoch=539
06/24/2022 12:49:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.21 on epoch=544
06/24/2022 12:50:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.34 on epoch=549
06/24/2022 12:50:01 - INFO - __main__ - Global step 1100 Train loss 1.27 Classification-F1 0.3333333333333333 on epoch=549
06/24/2022 12:50:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.29 on epoch=554
06/24/2022 12:50:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.11 on epoch=559
06/24/2022 12:50:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.16 on epoch=564
06/24/2022 12:50:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.26 on epoch=569
06/24/2022 12:50:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.20 on epoch=574
06/24/2022 12:50:08 - INFO - __main__ - Global step 1150 Train loss 1.20 Classification-F1 0.3333333333333333 on epoch=574
06/24/2022 12:50:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.17 on epoch=579
06/24/2022 12:50:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.06 on epoch=584
06/24/2022 12:50:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.96 on epoch=589
06/24/2022 12:50:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.08 on epoch=594
06/24/2022 12:50:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.07 on epoch=599
06/24/2022 12:50:14 - INFO - __main__ - Global step 1200 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=599
06/24/2022 12:50:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.02 on epoch=604
06/24/2022 12:50:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.94 on epoch=609
06/24/2022 12:50:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.01 on epoch=614
06/24/2022 12:50:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.92 on epoch=619
06/24/2022 12:50:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.96 on epoch=624
06/24/2022 12:50:20 - INFO - __main__ - Global step 1250 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=624
06/24/2022 12:50:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.91 on epoch=629
06/24/2022 12:50:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.00 on epoch=634
06/24/2022 12:50:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.89 on epoch=639
06/24/2022 12:50:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.95 on epoch=644
06/24/2022 12:50:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.90 on epoch=649
06/24/2022 12:50:27 - INFO - __main__ - Global step 1300 Train loss 0.93 Classification-F1 0.3191489361702127 on epoch=649
06/24/2022 12:50:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.97 on epoch=654
06/24/2022 12:50:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.96 on epoch=659
06/24/2022 12:50:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.92 on epoch=664
06/24/2022 12:50:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.93 on epoch=669
06/24/2022 12:50:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.82 on epoch=674
06/24/2022 12:50:33 - INFO - __main__ - Global step 1350 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=674
06/24/2022 12:50:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.90 on epoch=679
06/24/2022 12:50:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.93 on epoch=684
06/24/2022 12:50:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.92 on epoch=689
06/24/2022 12:50:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.85 on epoch=694
06/24/2022 12:50:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.87 on epoch=699
06/24/2022 12:50:40 - INFO - __main__ - Global step 1400 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=699
06/24/2022 12:50:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.84 on epoch=704
06/24/2022 12:50:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.79 on epoch=709
06/24/2022 12:50:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.79 on epoch=714
06/24/2022 12:50:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.82 on epoch=719
06/24/2022 12:50:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.89 on epoch=724
06/24/2022 12:50:46 - INFO - __main__ - Global step 1450 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=724
06/24/2022 12:50:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.75 on epoch=729
06/24/2022 12:50:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.85 on epoch=734
06/24/2022 12:50:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.78 on epoch=739
06/24/2022 12:50:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.86 on epoch=744
06/24/2022 12:50:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.83 on epoch=749
06/24/2022 12:50:53 - INFO - __main__ - Global step 1500 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=749
06/24/2022 12:50:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.79 on epoch=754
06/24/2022 12:50:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.68 on epoch=759
06/24/2022 12:50:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.85 on epoch=764
06/24/2022 12:50:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.86 on epoch=769
06/24/2022 12:50:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.75 on epoch=774
06/24/2022 12:50:59 - INFO - __main__ - Global step 1550 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=774
06/24/2022 12:51:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.81 on epoch=779
06/24/2022 12:51:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.78 on epoch=784
06/24/2022 12:51:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.75 on epoch=789
06/24/2022 12:51:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.79 on epoch=794
06/24/2022 12:51:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.77 on epoch=799
06/24/2022 12:51:06 - INFO - __main__ - Global step 1600 Train loss 0.78 Classification-F1 0.3992490613266583 on epoch=799
06/24/2022 12:51:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=799, global_step=1600
06/24/2022 12:51:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.72 on epoch=804
06/24/2022 12:51:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.80 on epoch=809
06/24/2022 12:51:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.65 on epoch=814
06/24/2022 12:51:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.67 on epoch=819
06/24/2022 12:51:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.68 on epoch=824
06/24/2022 12:51:12 - INFO - __main__ - Global step 1650 Train loss 0.70 Classification-F1 0.3191489361702127 on epoch=824
06/24/2022 12:51:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.77 on epoch=829
06/24/2022 12:51:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.77 on epoch=834
06/24/2022 12:51:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.70 on epoch=839
06/24/2022 12:51:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.68 on epoch=844
06/24/2022 12:51:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.63 on epoch=849
06/24/2022 12:51:19 - INFO - __main__ - Global step 1700 Train loss 0.71 Classification-F1 0.3992490613266583 on epoch=849
06/24/2022 12:51:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.71 on epoch=854
06/24/2022 12:51:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.74 on epoch=859
06/24/2022 12:51:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.65 on epoch=864
06/24/2022 12:51:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.67 on epoch=869
06/24/2022 12:51:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.60 on epoch=874
06/24/2022 12:51:25 - INFO - __main__ - Global step 1750 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=874
06/24/2022 12:51:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.67 on epoch=879
06/24/2022 12:51:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.59 on epoch=884
06/24/2022 12:51:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.60 on epoch=889
06/24/2022 12:51:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.65 on epoch=894
06/24/2022 12:51:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.59 on epoch=899
06/24/2022 12:51:32 - INFO - __main__ - Global step 1800 Train loss 0.62 Classification-F1 0.3191489361702127 on epoch=899
06/24/2022 12:51:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.65 on epoch=904
06/24/2022 12:51:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.70 on epoch=909
06/24/2022 12:51:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.62 on epoch=914
06/24/2022 12:51:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.69 on epoch=919
06/24/2022 12:51:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.61 on epoch=924
06/24/2022 12:51:38 - INFO - __main__ - Global step 1850 Train loss 0.66 Classification-F1 0.3191489361702127 on epoch=924
06/24/2022 12:51:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.63 on epoch=929
06/24/2022 12:51:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.55 on epoch=934
06/24/2022 12:51:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.56 on epoch=939
06/24/2022 12:51:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.57 on epoch=944
06/24/2022 12:51:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.62 on epoch=949
06/24/2022 12:51:45 - INFO - __main__ - Global step 1900 Train loss 0.59 Classification-F1 0.3191489361702127 on epoch=949
06/24/2022 12:51:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.58 on epoch=954
06/24/2022 12:51:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.58 on epoch=959
06/24/2022 12:51:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.54 on epoch=964
06/24/2022 12:51:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.63 on epoch=969
06/24/2022 12:51:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.62 on epoch=974
06/24/2022 12:51:51 - INFO - __main__ - Global step 1950 Train loss 0.59 Classification-F1 0.3816425120772947 on epoch=974
06/24/2022 12:51:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.49 on epoch=979
06/24/2022 12:51:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.56 on epoch=984
06/24/2022 12:51:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.57 on epoch=989
06/24/2022 12:51:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.57 on epoch=994
06/24/2022 12:51:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.58 on epoch=999
06/24/2022 12:51:58 - INFO - __main__ - Global step 2000 Train loss 0.55 Classification-F1 0.5588547189819725 on epoch=999
06/24/2022 12:51:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5588547189819725 on epoch=999, global_step=2000
06/24/2022 12:51:58 - INFO - __main__ - save last model!
06/24/2022 12:51:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 12:51:58 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 12:51:58 - INFO - __main__ - Printing 3 examples
06/24/2022 12:51:58 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 12:51:58 - INFO - __main__ - ['0']
06/24/2022 12:51:58 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 12:51:58 - INFO - __main__ - ['1']
06/24/2022 12:51:58 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 12:51:58 - INFO - __main__ - ['1']
06/24/2022 12:51:58 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:51:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:51:58 - INFO - __main__ - Printing 3 examples
06/24/2022 12:51:58 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 12:51:58 - INFO - __main__ - ['0']
06/24/2022 12:51:58 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 12:51:58 - INFO - __main__ - ['0']
06/24/2022 12:51:58 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 12:51:58 - INFO - __main__ - ['0']
06/24/2022 12:51:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 12:51:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:51:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:51:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:51:58 - INFO - __main__ - Printing 3 examples
06/24/2022 12:51:58 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 12:51:58 - INFO - __main__ - ['0']
06/24/2022 12:51:58 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 12:51:58 - INFO - __main__ - ['0']
06/24/2022 12:51:58 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 12:51:58 - INFO - __main__ - ['0']
06/24/2022 12:51:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:51:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:51:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:52:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:52:04 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:52:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:52:04 - INFO - __main__ - Starting training!
06/24/2022 12:52:10 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 12:53:36 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_87_0.3_8_predictions.txt
06/24/2022 12:53:36 - INFO - __main__ - Classification-F1 on test data: 0.4423
06/24/2022 12:53:36 - INFO - __main__ - prefix=paws_16_87, lr=0.3, bsz=8, dev_performance=0.5588547189819725, test_performance=0.4423322017661965
06/24/2022 12:53:36 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.2, bsz=8 ...
06/24/2022 12:53:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:53:37 - INFO - __main__ - Printing 3 examples
06/24/2022 12:53:37 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 12:53:37 - INFO - __main__ - ['0']
06/24/2022 12:53:37 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 12:53:37 - INFO - __main__ - ['0']
06/24/2022 12:53:37 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 12:53:37 - INFO - __main__ - ['0']
06/24/2022 12:53:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:53:37 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:53:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 12:53:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 12:53:37 - INFO - __main__ - Printing 3 examples
06/24/2022 12:53:37 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 12:53:37 - INFO - __main__ - ['0']
06/24/2022 12:53:37 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 12:53:37 - INFO - __main__ - ['0']
06/24/2022 12:53:37 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 12:53:37 - INFO - __main__ - ['0']
06/24/2022 12:53:37 - INFO - __main__ - Tokenizing Input ...
06/24/2022 12:53:37 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:53:37 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 12:53:43 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 12:53:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 12:53:43 - INFO - __main__ - Starting training!
06/24/2022 12:53:45 - INFO - __main__ - Step 10 Global step 10 Train loss 5.85 on epoch=4
06/24/2022 12:53:46 - INFO - __main__ - Step 20 Global step 20 Train loss 5.70 on epoch=9
06/24/2022 12:53:47 - INFO - __main__ - Step 30 Global step 30 Train loss 5.74 on epoch=14
06/24/2022 12:53:49 - INFO - __main__ - Step 40 Global step 40 Train loss 5.76 on epoch=19
06/24/2022 12:53:50 - INFO - __main__ - Step 50 Global step 50 Train loss 5.70 on epoch=24
06/24/2022 12:53:51 - INFO - __main__ - Global step 50 Train loss 5.75 Classification-F1 0.0 on epoch=24
06/24/2022 12:53:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
06/24/2022 12:53:52 - INFO - __main__ - Step 60 Global step 60 Train loss 5.69 on epoch=29
06/24/2022 12:53:53 - INFO - __main__ - Step 70 Global step 70 Train loss 5.67 on epoch=34
06/24/2022 12:53:54 - INFO - __main__ - Step 80 Global step 80 Train loss 5.55 on epoch=39
06/24/2022 12:53:56 - INFO - __main__ - Step 90 Global step 90 Train loss 5.61 on epoch=44
06/24/2022 12:53:57 - INFO - __main__ - Step 100 Global step 100 Train loss 5.67 on epoch=49
06/24/2022 12:53:58 - INFO - __main__ - Global step 100 Train loss 5.64 Classification-F1 0.0 on epoch=49
06/24/2022 12:53:59 - INFO - __main__ - Step 110 Global step 110 Train loss 5.74 on epoch=54
06/24/2022 12:54:01 - INFO - __main__ - Step 120 Global step 120 Train loss 5.63 on epoch=59
06/24/2022 12:54:02 - INFO - __main__ - Step 130 Global step 130 Train loss 5.61 on epoch=64
06/24/2022 12:54:03 - INFO - __main__ - Step 140 Global step 140 Train loss 5.58 on epoch=69
06/24/2022 12:54:04 - INFO - __main__ - Step 150 Global step 150 Train loss 5.62 on epoch=74
06/24/2022 12:54:06 - INFO - __main__ - Global step 150 Train loss 5.63 Classification-F1 0.0 on epoch=74
06/24/2022 12:54:07 - INFO - __main__ - Step 160 Global step 160 Train loss 5.56 on epoch=79
06/24/2022 12:54:08 - INFO - __main__ - Step 170 Global step 170 Train loss 5.54 on epoch=84
06/24/2022 12:54:09 - INFO - __main__ - Step 180 Global step 180 Train loss 5.50 on epoch=89
06/24/2022 12:54:11 - INFO - __main__ - Step 190 Global step 190 Train loss 5.51 on epoch=94
06/24/2022 12:54:12 - INFO - __main__ - Step 200 Global step 200 Train loss 5.55 on epoch=99
06/24/2022 12:54:13 - INFO - __main__ - Global step 200 Train loss 5.53 Classification-F1 0.0 on epoch=99
06/24/2022 12:54:14 - INFO - __main__ - Step 210 Global step 210 Train loss 5.48 on epoch=104
06/24/2022 12:54:15 - INFO - __main__ - Step 220 Global step 220 Train loss 5.55 on epoch=109
06/24/2022 12:54:17 - INFO - __main__ - Step 230 Global step 230 Train loss 5.45 on epoch=114
06/24/2022 12:54:18 - INFO - __main__ - Step 240 Global step 240 Train loss 5.52 on epoch=119
06/24/2022 12:54:19 - INFO - __main__ - Step 250 Global step 250 Train loss 5.43 on epoch=124
06/24/2022 12:54:20 - INFO - __main__ - Global step 250 Train loss 5.49 Classification-F1 0.0 on epoch=124
06/24/2022 12:54:21 - INFO - __main__ - Step 260 Global step 260 Train loss 5.49 on epoch=129
06/24/2022 12:54:22 - INFO - __main__ - Step 270 Global step 270 Train loss 5.47 on epoch=134
06/24/2022 12:54:23 - INFO - __main__ - Step 280 Global step 280 Train loss 5.51 on epoch=139
06/24/2022 12:54:25 - INFO - __main__ - Step 290 Global step 290 Train loss 5.40 on epoch=144
06/24/2022 12:54:26 - INFO - __main__ - Step 300 Global step 300 Train loss 5.37 on epoch=149
06/24/2022 12:54:26 - INFO - __main__ - Global step 300 Train loss 5.45 Classification-F1 0.0 on epoch=149
06/24/2022 12:54:28 - INFO - __main__ - Step 310 Global step 310 Train loss 5.44 on epoch=154
06/24/2022 12:54:29 - INFO - __main__ - Step 320 Global step 320 Train loss 5.36 on epoch=159
06/24/2022 12:54:30 - INFO - __main__ - Step 330 Global step 330 Train loss 5.40 on epoch=164
06/24/2022 12:54:31 - INFO - __main__ - Step 340 Global step 340 Train loss 5.46 on epoch=169
06/24/2022 12:54:33 - INFO - __main__ - Step 350 Global step 350 Train loss 5.32 on epoch=174
06/24/2022 12:54:33 - INFO - __main__ - Global step 350 Train loss 5.40 Classification-F1 0.0 on epoch=174
06/24/2022 12:54:34 - INFO - __main__ - Step 360 Global step 360 Train loss 5.28 on epoch=179
06/24/2022 12:54:36 - INFO - __main__ - Step 370 Global step 370 Train loss 5.44 on epoch=184
06/24/2022 12:54:37 - INFO - __main__ - Step 380 Global step 380 Train loss 5.32 on epoch=189
06/24/2022 12:54:38 - INFO - __main__ - Step 390 Global step 390 Train loss 5.36 on epoch=194
06/24/2022 12:54:39 - INFO - __main__ - Step 400 Global step 400 Train loss 5.30 on epoch=199
06/24/2022 12:54:40 - INFO - __main__ - Global step 400 Train loss 5.34 Classification-F1 0.0 on epoch=199
06/24/2022 12:54:41 - INFO - __main__ - Step 410 Global step 410 Train loss 5.34 on epoch=204
06/24/2022 12:54:42 - INFO - __main__ - Step 420 Global step 420 Train loss 5.29 on epoch=209
06/24/2022 12:54:43 - INFO - __main__ - Step 430 Global step 430 Train loss 5.30 on epoch=214
06/24/2022 12:54:45 - INFO - __main__ - Step 440 Global step 440 Train loss 5.32 on epoch=219
06/24/2022 12:54:46 - INFO - __main__ - Step 450 Global step 450 Train loss 5.29 on epoch=224
06/24/2022 12:54:46 - INFO - __main__ - Global step 450 Train loss 5.31 Classification-F1 0.0 on epoch=224
06/24/2022 12:54:47 - INFO - __main__ - Step 460 Global step 460 Train loss 5.32 on epoch=229
06/24/2022 12:54:49 - INFO - __main__ - Step 470 Global step 470 Train loss 5.19 on epoch=234
06/24/2022 12:54:50 - INFO - __main__ - Step 480 Global step 480 Train loss 5.16 on epoch=239
06/24/2022 12:54:51 - INFO - __main__ - Step 490 Global step 490 Train loss 5.17 on epoch=244
06/24/2022 12:54:52 - INFO - __main__ - Step 500 Global step 500 Train loss 5.19 on epoch=249
06/24/2022 12:54:54 - INFO - __main__ - Global step 500 Train loss 5.20 Classification-F1 0.0 on epoch=249
06/24/2022 12:54:55 - INFO - __main__ - Step 510 Global step 510 Train loss 5.13 on epoch=254
06/24/2022 12:54:56 - INFO - __main__ - Step 520 Global step 520 Train loss 5.22 on epoch=259
06/24/2022 12:54:57 - INFO - __main__ - Step 530 Global step 530 Train loss 5.20 on epoch=264
06/24/2022 12:54:59 - INFO - __main__ - Step 540 Global step 540 Train loss 5.14 on epoch=269
06/24/2022 12:55:00 - INFO - __main__ - Step 550 Global step 550 Train loss 5.14 on epoch=274
06/24/2022 12:55:00 - INFO - __main__ - Global step 550 Train loss 5.17 Classification-F1 0.0 on epoch=274
06/24/2022 12:55:02 - INFO - __main__ - Step 560 Global step 560 Train loss 5.05 on epoch=279
06/24/2022 12:55:03 - INFO - __main__ - Step 570 Global step 570 Train loss 5.04 on epoch=284
06/24/2022 12:55:04 - INFO - __main__ - Step 580 Global step 580 Train loss 5.02 on epoch=289
06/24/2022 12:55:05 - INFO - __main__ - Step 590 Global step 590 Train loss 5.02 on epoch=294
06/24/2022 12:55:06 - INFO - __main__ - Step 600 Global step 600 Train loss 4.93 on epoch=299
06/24/2022 12:55:13 - INFO - __main__ - Global step 600 Train loss 5.01 Classification-F1 0.0 on epoch=299
06/24/2022 12:55:14 - INFO - __main__ - Step 610 Global step 610 Train loss 4.84 on epoch=304
06/24/2022 12:55:16 - INFO - __main__ - Step 620 Global step 620 Train loss 4.87 on epoch=309
06/24/2022 12:55:17 - INFO - __main__ - Step 630 Global step 630 Train loss 4.74 on epoch=314
06/24/2022 12:55:18 - INFO - __main__ - Step 640 Global step 640 Train loss 4.66 on epoch=319
06/24/2022 12:55:19 - INFO - __main__ - Step 650 Global step 650 Train loss 4.56 on epoch=324
06/24/2022 12:55:30 - INFO - __main__ - Global step 650 Train loss 4.73 Classification-F1 0.0 on epoch=324
06/24/2022 12:55:31 - INFO - __main__ - Step 660 Global step 660 Train loss 4.44 on epoch=329
06/24/2022 12:55:32 - INFO - __main__ - Step 670 Global step 670 Train loss 4.44 on epoch=334
06/24/2022 12:55:33 - INFO - __main__ - Step 680 Global step 680 Train loss 4.29 on epoch=339
06/24/2022 12:55:35 - INFO - __main__ - Step 690 Global step 690 Train loss 4.18 on epoch=344
06/24/2022 12:55:36 - INFO - __main__ - Step 700 Global step 700 Train loss 4.26 on epoch=349
06/24/2022 12:55:42 - INFO - __main__ - Global step 700 Train loss 4.32 Classification-F1 0.0 on epoch=349
06/24/2022 12:55:43 - INFO - __main__ - Step 710 Global step 710 Train loss 4.13 on epoch=354
06/24/2022 12:55:44 - INFO - __main__ - Step 720 Global step 720 Train loss 4.06 on epoch=359
06/24/2022 12:55:45 - INFO - __main__ - Step 730 Global step 730 Train loss 4.13 on epoch=364
06/24/2022 12:55:47 - INFO - __main__ - Step 740 Global step 740 Train loss 4.00 on epoch=369
06/24/2022 12:55:48 - INFO - __main__ - Step 750 Global step 750 Train loss 3.93 on epoch=374
06/24/2022 12:55:53 - INFO - __main__ - Global step 750 Train loss 4.05 Classification-F1 0.0 on epoch=374
06/24/2022 12:55:55 - INFO - __main__ - Step 760 Global step 760 Train loss 4.02 on epoch=379
06/24/2022 12:55:56 - INFO - __main__ - Step 770 Global step 770 Train loss 4.03 on epoch=384
06/24/2022 12:55:57 - INFO - __main__ - Step 780 Global step 780 Train loss 3.84 on epoch=389
06/24/2022 12:55:58 - INFO - __main__ - Step 790 Global step 790 Train loss 3.84 on epoch=394
06/24/2022 12:55:59 - INFO - __main__ - Step 800 Global step 800 Train loss 3.82 on epoch=399
06/24/2022 12:56:06 - INFO - __main__ - Global step 800 Train loss 3.91 Classification-F1 0.0 on epoch=399
06/24/2022 12:56:07 - INFO - __main__ - Step 810 Global step 810 Train loss 3.90 on epoch=404
06/24/2022 12:56:08 - INFO - __main__ - Step 820 Global step 820 Train loss 3.73 on epoch=409
06/24/2022 12:56:10 - INFO - __main__ - Step 830 Global step 830 Train loss 3.88 on epoch=414
06/24/2022 12:56:11 - INFO - __main__ - Step 840 Global step 840 Train loss 3.81 on epoch=419
06/24/2022 12:56:12 - INFO - __main__ - Step 850 Global step 850 Train loss 3.75 on epoch=424
06/24/2022 12:56:13 - INFO - __main__ - Global step 850 Train loss 3.81 Classification-F1 0.0 on epoch=424
06/24/2022 12:56:14 - INFO - __main__ - Step 860 Global step 860 Train loss 3.71 on epoch=429
06/24/2022 12:56:15 - INFO - __main__ - Step 870 Global step 870 Train loss 3.70 on epoch=434
06/24/2022 12:56:17 - INFO - __main__ - Step 880 Global step 880 Train loss 3.58 on epoch=439
06/24/2022 12:56:18 - INFO - __main__ - Step 890 Global step 890 Train loss 3.63 on epoch=444
06/24/2022 12:56:19 - INFO - __main__ - Step 900 Global step 900 Train loss 3.53 on epoch=449
06/24/2022 12:56:22 - INFO - __main__ - Global step 900 Train loss 3.63 Classification-F1 0.0 on epoch=449
06/24/2022 12:56:24 - INFO - __main__ - Step 910 Global step 910 Train loss 3.60 on epoch=454
06/24/2022 12:56:25 - INFO - __main__ - Step 920 Global step 920 Train loss 3.55 on epoch=459
06/24/2022 12:56:26 - INFO - __main__ - Step 930 Global step 930 Train loss 3.49 on epoch=464
06/24/2022 12:56:27 - INFO - __main__ - Step 940 Global step 940 Train loss 3.56 on epoch=469
06/24/2022 12:56:28 - INFO - __main__ - Step 950 Global step 950 Train loss 3.53 on epoch=474
06/24/2022 12:56:32 - INFO - __main__ - Global step 950 Train loss 3.54 Classification-F1 0.0 on epoch=474
06/24/2022 12:56:33 - INFO - __main__ - Step 960 Global step 960 Train loss 3.46 on epoch=479
06/24/2022 12:56:34 - INFO - __main__ - Step 970 Global step 970 Train loss 3.53 on epoch=484
06/24/2022 12:56:35 - INFO - __main__ - Step 980 Global step 980 Train loss 3.42 on epoch=489
06/24/2022 12:56:37 - INFO - __main__ - Step 990 Global step 990 Train loss 3.52 on epoch=494
06/24/2022 12:56:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.35 on epoch=499
06/24/2022 12:56:44 - INFO - __main__ - Global step 1000 Train loss 3.46 Classification-F1 0.0 on epoch=499
06/24/2022 12:56:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.26 on epoch=504
06/24/2022 12:56:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.30 on epoch=509
06/24/2022 12:56:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.33 on epoch=514
06/24/2022 12:56:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.33 on epoch=519
06/24/2022 12:56:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.20 on epoch=524
06/24/2022 12:56:54 - INFO - __main__ - Global step 1050 Train loss 3.28 Classification-F1 0.0 on epoch=524
06/24/2022 12:56:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.26 on epoch=529
06/24/2022 12:56:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.29 on epoch=534
06/24/2022 12:56:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.15 on epoch=539
06/24/2022 12:56:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.01 on epoch=544
06/24/2022 12:57:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.12 on epoch=549
06/24/2022 12:57:02 - INFO - __main__ - Global step 1100 Train loss 3.17 Classification-F1 0.019047619047619046 on epoch=549
06/24/2022 12:57:02 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.019047619047619046 on epoch=549, global_step=1100
06/24/2022 12:57:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.14 on epoch=554
06/24/2022 12:57:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.13 on epoch=559
06/24/2022 12:57:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.12 on epoch=564
06/24/2022 12:57:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.09 on epoch=569
06/24/2022 12:57:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 2.93 on epoch=574
06/24/2022 12:57:15 - INFO - __main__ - Global step 1150 Train loss 3.08 Classification-F1 0.0 on epoch=574
06/24/2022 12:57:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.08 on epoch=579
06/24/2022 12:57:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.98 on epoch=584
06/24/2022 12:57:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.01 on epoch=589
06/24/2022 12:57:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.85 on epoch=594
06/24/2022 12:57:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.00 on epoch=599
06/24/2022 12:57:23 - INFO - __main__ - Global step 1200 Train loss 2.98 Classification-F1 0.06451612903225808 on epoch=599
06/24/2022 12:57:23 - INFO - __main__ - Saving model with best Classification-F1: 0.019047619047619046 -> 0.06451612903225808 on epoch=599, global_step=1200
06/24/2022 12:57:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.90 on epoch=604
06/24/2022 12:57:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.79 on epoch=609
06/24/2022 12:57:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.81 on epoch=614
06/24/2022 12:57:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.69 on epoch=619
06/24/2022 12:57:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.86 on epoch=624
06/24/2022 12:57:32 - INFO - __main__ - Global step 1250 Train loss 2.81 Classification-F1 0.12727272727272726 on epoch=624
06/24/2022 12:57:32 - INFO - __main__ - Saving model with best Classification-F1: 0.06451612903225808 -> 0.12727272727272726 on epoch=624, global_step=1250
06/24/2022 12:57:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.93 on epoch=629
06/24/2022 12:57:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.60 on epoch=634
06/24/2022 12:57:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.57 on epoch=639
06/24/2022 12:57:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.63 on epoch=644
06/24/2022 12:57:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.62 on epoch=649
06/24/2022 12:57:41 - INFO - __main__ - Global step 1300 Train loss 2.67 Classification-F1 0.2566069906223359 on epoch=649
06/24/2022 12:57:41 - INFO - __main__ - Saving model with best Classification-F1: 0.12727272727272726 -> 0.2566069906223359 on epoch=649, global_step=1300
06/24/2022 12:57:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.55 on epoch=654
06/24/2022 12:57:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.64 on epoch=659
06/24/2022 12:57:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.63 on epoch=664
06/24/2022 12:57:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.48 on epoch=669
06/24/2022 12:57:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.59 on epoch=674
06/24/2022 12:57:51 - INFO - __main__ - Global step 1350 Train loss 2.58 Classification-F1 0.21739130434782608 on epoch=674
06/24/2022 12:57:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.45 on epoch=679
06/24/2022 12:57:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.48 on epoch=684
06/24/2022 12:57:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.42 on epoch=689
06/24/2022 12:57:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.46 on epoch=694
06/24/2022 12:57:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.49 on epoch=699
06/24/2022 12:57:59 - INFO - __main__ - Global step 1400 Train loss 2.46 Classification-F1 0.3333333333333333 on epoch=699
06/24/2022 12:57:59 - INFO - __main__ - Saving model with best Classification-F1: 0.2566069906223359 -> 0.3333333333333333 on epoch=699, global_step=1400
06/24/2022 12:58:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.43 on epoch=704
06/24/2022 12:58:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.32 on epoch=709
06/24/2022 12:58:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.35 on epoch=714
06/24/2022 12:58:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.36 on epoch=719
06/24/2022 12:58:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.29 on epoch=724
06/24/2022 12:58:09 - INFO - __main__ - Global step 1450 Train loss 2.35 Classification-F1 0.3191489361702127 on epoch=724
06/24/2022 12:58:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.35 on epoch=729
06/24/2022 12:58:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.40 on epoch=734
06/24/2022 12:58:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.25 on epoch=739
06/24/2022 12:58:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.95 on epoch=744
06/24/2022 12:58:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.32 on epoch=749
06/24/2022 12:58:17 - INFO - __main__ - Global step 1500 Train loss 2.26 Classification-F1 0.3333333333333333 on epoch=749
06/24/2022 12:58:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.22 on epoch=754
06/24/2022 12:58:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.09 on epoch=759
06/24/2022 12:58:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 2.15 on epoch=764
06/24/2022 12:58:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 2.04 on epoch=769
06/24/2022 12:58:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 2.09 on epoch=774
06/24/2022 12:58:26 - INFO - __main__ - Global step 1550 Train loss 2.12 Classification-F1 0.3992490613266583 on epoch=774
06/24/2022 12:58:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=774, global_step=1550
06/24/2022 12:58:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 2.00 on epoch=779
06/24/2022 12:58:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.97 on epoch=784
06/24/2022 12:58:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 2.10 on epoch=789
06/24/2022 12:58:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 2.11 on epoch=794
06/24/2022 12:58:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 2.00 on epoch=799
06/24/2022 12:58:36 - INFO - __main__ - Global step 1600 Train loss 2.04 Classification-F1 0.3992490613266583 on epoch=799
06/24/2022 12:58:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.87 on epoch=804
06/24/2022 12:58:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.87 on epoch=809
06/24/2022 12:58:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.73 on epoch=814
06/24/2022 12:58:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.91 on epoch=819
06/24/2022 12:58:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.88 on epoch=824
06/24/2022 12:58:44 - INFO - __main__ - Global step 1650 Train loss 1.85 Classification-F1 0.3266888150609081 on epoch=824
06/24/2022 12:58:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.74 on epoch=829
06/24/2022 12:58:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.70 on epoch=834
06/24/2022 12:58:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.85 on epoch=839
06/24/2022 12:58:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.92 on epoch=844
06/24/2022 12:58:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.75 on epoch=849
06/24/2022 12:58:52 - INFO - __main__ - Global step 1700 Train loss 1.79 Classification-F1 0.5607843137254902 on epoch=849
06/24/2022 12:58:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5607843137254902 on epoch=849, global_step=1700
06/24/2022 12:58:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.83 on epoch=854
06/24/2022 12:58:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.76 on epoch=859
06/24/2022 12:58:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.77 on epoch=864
06/24/2022 12:58:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.67 on epoch=869
06/24/2022 12:58:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.65 on epoch=874
06/24/2022 12:58:59 - INFO - __main__ - Global step 1750 Train loss 1.73 Classification-F1 0.5076923076923077 on epoch=874
06/24/2022 12:59:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.61 on epoch=879
06/24/2022 12:59:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.69 on epoch=884
06/24/2022 12:59:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.51 on epoch=889
06/24/2022 12:59:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.58 on epoch=894
06/24/2022 12:59:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.63 on epoch=899
06/24/2022 12:59:07 - INFO - __main__ - Global step 1800 Train loss 1.60 Classification-F1 0.39139139139139134 on epoch=899
06/24/2022 12:59:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.61 on epoch=904
06/24/2022 12:59:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.46 on epoch=909
06/24/2022 12:59:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.55 on epoch=914
06/24/2022 12:59:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.35 on epoch=919
06/24/2022 12:59:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.48 on epoch=924
06/24/2022 12:59:14 - INFO - __main__ - Global step 1850 Train loss 1.49 Classification-F1 0.3992490613266583 on epoch=924
06/24/2022 12:59:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.43 on epoch=929
06/24/2022 12:59:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.43 on epoch=934
06/24/2022 12:59:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.39 on epoch=939
06/24/2022 12:59:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.28 on epoch=944
06/24/2022 12:59:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.28 on epoch=949
06/24/2022 12:59:20 - INFO - __main__ - Global step 1900 Train loss 1.36 Classification-F1 0.3333333333333333 on epoch=949
06/24/2022 12:59:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.49 on epoch=954
06/24/2022 12:59:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.33 on epoch=959
06/24/2022 12:59:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.33 on epoch=964
06/24/2022 12:59:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.36 on epoch=969
06/24/2022 12:59:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.26 on epoch=974
06/24/2022 12:59:27 - INFO - __main__ - Global step 1950 Train loss 1.35 Classification-F1 0.5333333333333333 on epoch=974
06/24/2022 12:59:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.23 on epoch=979
06/24/2022 12:59:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.20 on epoch=984
06/24/2022 12:59:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.27 on epoch=989
06/24/2022 12:59:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.18 on epoch=994
06/24/2022 12:59:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.17 on epoch=999
06/24/2022 12:59:34 - INFO - __main__ - Global step 2000 Train loss 1.21 Classification-F1 0.36374269005847953 on epoch=999
06/24/2022 12:59:34 - INFO - __main__ - save last model!
06/24/2022 12:59:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 12:59:34 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 12:59:34 - INFO - __main__ - Printing 3 examples
06/24/2022 12:59:34 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 12:59:34 - INFO - __main__ - ['0']
06/24/2022 12:59:34 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 12:59:34 - INFO - __main__ - ['1']
06/24/2022 12:59:34 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 12:59:34 - INFO - __main__ - ['1']
06/24/2022 12:59:34 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 12:59:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 12:59:46 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 13:01:11 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/paws_16_87_0.2_8_predictions.txt
06/24/2022 13:01:12 - INFO - __main__ - Classification-F1 on test data: 0.3991
06/24/2022 13:01:12 - INFO - __main__ - prefix=paws_16_87, lr=0.2, bsz=8, dev_performance=0.5607843137254902, test_performance=0.3991281554063541
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (91108): No such process
t5base para ft downstream
Task: glue-mrpc, Checkpoint: None, Identifier: T5-base-ft-nopara2para
06/24/2022 13:01:17 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/24/2022 13:01:17 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-glue-mrpc
Output directory () already exists and is not empty.
06/24/2022 13:01:17 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/24/2022 13:01:17 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-glue-mrpc
06/24/2022 13:01:19 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/24/2022 13:01:19 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/24/2022 13:01:19 - INFO - __main__ - args.device: cuda:1
06/24/2022 13:01:19 - INFO - __main__ - args.device: cuda:0
06/24/2022 13:01:19 - INFO - __main__ - Using 2 gpus
06/24/2022 13:01:19 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/24/2022 13:01:19 - INFO - __main__ - Using 2 gpus
06/24/2022 13:01:19 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/24/2022 13:01:24 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.0005, bsz=8 ...
06/24/2022 13:01:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:01:25 - INFO - __main__ - Printing 3 examples
06/24/2022 13:01:25 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 13:01:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:01:25 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 13:01:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:01:25 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 13:01:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:01:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:01:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:01:25 - INFO - __main__ - Printing 3 examples
06/24/2022 13:01:25 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 13:01:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:01:25 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 13:01:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:01:25 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 13:01:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:01:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:01:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:01:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:01:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:01:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:01:25 - INFO - __main__ - Printing 3 examples
06/24/2022 13:01:25 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 13:01:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:01:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:01:25 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 13:01:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:01:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:01:25 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 13:01:25 - INFO - __main__ - Printing 3 examples
06/24/2022 13:01:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:01:25 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:01:25 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 13:01:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:01:25 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 13:01:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:01:25 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 13:01:25 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:01:25 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:01:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:01:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:01:25 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:01:25 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:01:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:01:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:01:29 - INFO - __main__ - Starting training!
06/24/2022 13:01:29 - INFO - __main__ - Starting training!
06/24/2022 13:01:32 - INFO - __main__ - Step 10 Global step 10 Train loss 15.758972 on epoch=4
06/24/2022 13:01:34 - INFO - __main__ - Step 20 Global step 20 Train loss 11.300932 on epoch=9
06/24/2022 13:01:36 - INFO - __main__ - Step 30 Global step 30 Train loss 5.852479 on epoch=14
06/24/2022 13:01:39 - INFO - __main__ - Step 40 Global step 40 Train loss 3.657104 on epoch=19
06/24/2022 13:01:41 - INFO - __main__ - Step 50 Global step 50 Train loss 2.539168 on epoch=24
06/24/2022 13:01:42 - INFO - __main__ - Global step 50 Train loss 7.821732 ACC 0.5 on epoch=24
06/24/2022 13:01:45 - INFO - __main__ - Step 60 Global step 60 Train loss 1.757793 on epoch=29
06/24/2022 13:01:47 - INFO - __main__ - Step 70 Global step 70 Train loss 1.304571 on epoch=34
06/24/2022 13:01:49 - INFO - __main__ - Step 80 Global step 80 Train loss 1.263317 on epoch=39
06/24/2022 13:01:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.985309 on epoch=44
06/24/2022 13:01:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.507924 on epoch=49
06/24/2022 13:01:55 - INFO - __main__ - Global step 100 Train loss 1.163783 ACC 0.5 on epoch=49
06/24/2022 13:01:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.623975 on epoch=54
06/24/2022 13:02:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.566541 on epoch=59
06/24/2022 13:02:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.521851 on epoch=64
06/24/2022 13:02:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.663815 on epoch=69
06/24/2022 13:02:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.330235 on epoch=74
06/24/2022 13:02:07 - INFO - __main__ - Global step 150 Train loss 0.541283 ACC 0.5 on epoch=74
06/24/2022 13:02:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.445822 on epoch=79
06/24/2022 13:02:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.467130 on epoch=84
06/24/2022 13:02:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.421894 on epoch=89
06/24/2022 13:02:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.598824 on epoch=94
06/24/2022 13:02:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.428862 on epoch=99
06/24/2022 13:02:20 - INFO - __main__ - Global step 200 Train loss 0.472506 ACC 0.5 on epoch=99
06/24/2022 13:02:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.335485 on epoch=104
06/24/2022 13:02:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.418117 on epoch=109
06/24/2022 13:02:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.396459 on epoch=114
06/24/2022 13:02:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.390026 on epoch=119
06/24/2022 13:02:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.502809 on epoch=124
06/24/2022 13:02:33 - INFO - __main__ - Global step 250 Train loss 0.408579 ACC 0.53125 on epoch=124
06/24/2022 13:02:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.361520 on epoch=129
06/24/2022 13:02:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.376828 on epoch=134
06/24/2022 13:02:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.370527 on epoch=139
06/24/2022 13:02:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.338626 on epoch=144
06/24/2022 13:02:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.329265 on epoch=149
06/24/2022 13:02:46 - INFO - __main__ - Global step 300 Train loss 0.355353 ACC 0.59375 on epoch=149
06/24/2022 13:02:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.395881 on epoch=154
06/24/2022 13:02:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.324081 on epoch=159
06/24/2022 13:02:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.212415 on epoch=164
06/24/2022 13:02:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.251254 on epoch=169
06/24/2022 13:02:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.313249 on epoch=174
06/24/2022 13:03:00 - INFO - __main__ - Global step 350 Train loss 0.299376 ACC 0.5 on epoch=174
06/24/2022 13:03:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.276431 on epoch=179
06/24/2022 13:03:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.250467 on epoch=184
06/24/2022 13:03:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.243612 on epoch=189
06/24/2022 13:03:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.245029 on epoch=194
06/24/2022 13:03:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.141459 on epoch=199
06/24/2022 13:03:12 - INFO - __main__ - Global step 400 Train loss 0.231400 ACC 0.59375 on epoch=199
06/24/2022 13:03:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.102107 on epoch=204
06/24/2022 13:03:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.124273 on epoch=209
06/24/2022 13:03:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.119167 on epoch=214
06/24/2022 13:03:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.087444 on epoch=219
06/24/2022 13:03:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.124322 on epoch=224
06/24/2022 13:03:25 - INFO - __main__ - Global step 450 Train loss 0.111462 ACC 0.75 on epoch=224
06/24/2022 13:03:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.055999 on epoch=229
06/24/2022 13:03:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.093461 on epoch=234
06/24/2022 13:03:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.072203 on epoch=239
06/24/2022 13:03:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.085943 on epoch=244
06/24/2022 13:03:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.032119 on epoch=249
06/24/2022 13:03:38 - INFO - __main__ - Global step 500 Train loss 0.067945 ACC 0.625 on epoch=249
06/24/2022 13:03:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.037056 on epoch=254
06/24/2022 13:03:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.043355 on epoch=259
06/24/2022 13:03:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.029380 on epoch=264
06/24/2022 13:03:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.024328 on epoch=269
06/24/2022 13:03:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.024529 on epoch=274
06/24/2022 13:03:51 - INFO - __main__ - Global step 550 Train loss 0.031729 ACC 0.625 on epoch=274
06/24/2022 13:03:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.018113 on epoch=279
06/24/2022 13:03:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.022986 on epoch=284
06/24/2022 13:03:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.010400 on epoch=289
06/24/2022 13:04:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.012364 on epoch=294
06/24/2022 13:04:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.004013 on epoch=299
06/24/2022 13:04:03 - INFO - __main__ - Global step 600 Train loss 0.013575 ACC 0.59375 on epoch=299
06/24/2022 13:04:03 - INFO - __main__ - save last model!
06/24/2022 13:04:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:04:04 - INFO - __main__ - Printing 3 examples
06/24/2022 13:04:04 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 13:04:04 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:04 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 13:04:04 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:04 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 13:04:04 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:04:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:04:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:04:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:04:04 - INFO - __main__ - Printing 3 examples
06/24/2022 13:04:04 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 13:04:04 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:04 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 13:04:04 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:04 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 13:04:04 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:04 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:04:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:04:04 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:04:06 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:04:06 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:04:07 - INFO - __main__ - Printing 3 examples
06/24/2022 13:04:07 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:04:07 - INFO - __main__ - ['equivalent']
06/24/2022 13:04:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:04:07 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:04:07 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:04:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:04:07 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:04:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:04:08 - INFO - __main__ - Starting training!
06/24/2022 13:04:12 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_100_0.0005_8_predictions.txt
06/24/2022 13:04:12 - INFO - __main__ - ACC on test data: 0.5515
06/24/2022 13:04:12 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.0005, bsz=8, dev_performance=0.75, test_performance=0.5514705882352942
06/24/2022 13:04:12 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.0003, bsz=8 ...
06/24/2022 13:04:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:04:13 - INFO - __main__ - Printing 3 examples
06/24/2022 13:04:13 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 13:04:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 13:04:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:13 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 13:04:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:04:13 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:04:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:04:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:04:13 - INFO - __main__ - Printing 3 examples
06/24/2022 13:04:13 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 13:04:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:13 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 13:04:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 13:04:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:04:13 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:04:14 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:04:14 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:04:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:04:17 - INFO - __main__ - Starting training!
06/24/2022 13:04:19 - INFO - __main__ - Step 10 Global step 10 Train loss 15.452395 on epoch=4
06/24/2022 13:04:21 - INFO - __main__ - Step 20 Global step 20 Train loss 10.847142 on epoch=9
06/24/2022 13:04:24 - INFO - __main__ - Step 30 Global step 30 Train loss 7.301808 on epoch=14
06/24/2022 13:04:26 - INFO - __main__ - Step 40 Global step 40 Train loss 5.995261 on epoch=19
06/24/2022 13:04:29 - INFO - __main__ - Step 50 Global step 50 Train loss 4.796272 on epoch=24
06/24/2022 13:04:29 - INFO - __main__ - Global step 50 Train loss 8.878575 ACC 0.28125 on epoch=24
06/24/2022 13:04:32 - INFO - __main__ - Step 60 Global step 60 Train loss 4.069953 on epoch=29
06/24/2022 13:04:34 - INFO - __main__ - Step 70 Global step 70 Train loss 2.971880 on epoch=34
06/24/2022 13:04:37 - INFO - __main__ - Step 80 Global step 80 Train loss 1.381011 on epoch=39
06/24/2022 13:04:39 - INFO - __main__ - Step 90 Global step 90 Train loss 1.580632 on epoch=44
06/24/2022 13:04:42 - INFO - __main__ - Step 100 Global step 100 Train loss 1.419468 on epoch=49
06/24/2022 13:04:42 - INFO - __main__ - Global step 100 Train loss 2.284589 ACC 0.4375 on epoch=49
06/24/2022 13:04:45 - INFO - __main__ - Step 110 Global step 110 Train loss 1.381645 on epoch=54
06/24/2022 13:04:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.886517 on epoch=59
06/24/2022 13:04:50 - INFO - __main__ - Step 130 Global step 130 Train loss 1.127266 on epoch=64
06/24/2022 13:04:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.782483 on epoch=69
06/24/2022 13:04:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.788453 on epoch=74
06/24/2022 13:04:55 - INFO - __main__ - Global step 150 Train loss 0.993273 ACC 0.53125 on epoch=74
06/24/2022 13:04:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.645452 on epoch=79
06/24/2022 13:05:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.807365 on epoch=84
06/24/2022 13:05:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.528547 on epoch=89
06/24/2022 13:05:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.578807 on epoch=94
06/24/2022 13:05:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.444811 on epoch=99
06/24/2022 13:05:09 - INFO - __main__ - Global step 200 Train loss 0.600997 ACC 0.5 on epoch=99
06/24/2022 13:05:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.372433 on epoch=104
06/24/2022 13:05:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.585447 on epoch=109
06/24/2022 13:05:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.465815 on epoch=114
06/24/2022 13:05:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.555582 on epoch=119
06/24/2022 13:05:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.474929 on epoch=124
06/24/2022 13:05:21 - INFO - __main__ - Global step 250 Train loss 0.490841 ACC 0.59375 on epoch=124
06/24/2022 13:05:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.542072 on epoch=129
06/24/2022 13:05:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.523403 on epoch=134
06/24/2022 13:05:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.292778 on epoch=139
06/24/2022 13:05:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.377296 on epoch=144
06/24/2022 13:05:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.407546 on epoch=149
06/24/2022 13:05:34 - INFO - __main__ - Global step 300 Train loss 0.428619 ACC 0.53125 on epoch=149
06/24/2022 13:05:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.323308 on epoch=154
06/24/2022 13:05:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.378435 on epoch=159
06/24/2022 13:05:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.373702 on epoch=164
06/24/2022 13:05:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.430914 on epoch=169
06/24/2022 13:05:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.369559 on epoch=174
06/24/2022 13:05:47 - INFO - __main__ - Global step 350 Train loss 0.375184 ACC 0.53125 on epoch=174
06/24/2022 13:05:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.264899 on epoch=179
06/24/2022 13:05:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.511994 on epoch=184
06/24/2022 13:05:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.446737 on epoch=189
06/24/2022 13:05:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.391319 on epoch=194
06/24/2022 13:05:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.255931 on epoch=199
06/24/2022 13:06:00 - INFO - __main__ - Global step 400 Train loss 0.374176 ACC 0.59375 on epoch=199
06/24/2022 13:06:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.405479 on epoch=204
06/24/2022 13:06:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.460231 on epoch=209
06/24/2022 13:06:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.310228 on epoch=214
06/24/2022 13:06:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.308213 on epoch=219
06/24/2022 13:06:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.336993 on epoch=224
06/24/2022 13:06:12 - INFO - __main__ - Global step 450 Train loss 0.364229 ACC 0.53125 on epoch=224
06/24/2022 13:06:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.236630 on epoch=229
06/24/2022 13:06:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.250639 on epoch=234
06/24/2022 13:06:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.334758 on epoch=239
06/24/2022 13:06:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.333430 on epoch=244
06/24/2022 13:06:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.231467 on epoch=249
06/24/2022 13:06:25 - INFO - __main__ - Global step 500 Train loss 0.277385 ACC 0.5625 on epoch=249
06/24/2022 13:06:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.205643 on epoch=254
06/24/2022 13:06:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.242037 on epoch=259
06/24/2022 13:06:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.229085 on epoch=264
06/24/2022 13:06:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.170801 on epoch=269
06/24/2022 13:06:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.234483 on epoch=274
06/24/2022 13:06:38 - INFO - __main__ - Global step 550 Train loss 0.216410 ACC 0.5 on epoch=274
06/24/2022 13:06:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.263630 on epoch=279
06/24/2022 13:06:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.195696 on epoch=284
06/24/2022 13:06:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.159166 on epoch=289
06/24/2022 13:06:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.206626 on epoch=294
06/24/2022 13:06:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.158136 on epoch=299
06/24/2022 13:06:50 - INFO - __main__ - Global step 600 Train loss 0.196650 ACC 0.5625 on epoch=299
06/24/2022 13:06:50 - INFO - __main__ - save last model!
06/24/2022 13:06:51 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:06:51 - INFO - __main__ - Printing 3 examples
06/24/2022 13:06:51 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 13:06:51 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:06:51 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 13:06:51 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:06:51 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 13:06:51 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:06:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:06:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:06:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:06:51 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:06:51 - INFO - __main__ - Printing 3 examples
06/24/2022 13:06:51 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 13:06:51 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:06:51 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 13:06:51 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:06:51 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 13:06:51 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:06:51 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:06:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:06:51 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:06:53 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:06:54 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:06:54 - INFO - __main__ - Printing 3 examples
06/24/2022 13:06:54 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:06:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:06:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:06:54 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:06:54 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:06:54 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:06:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:06:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:06:54 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:06:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:06:56 - INFO - __main__ - Starting training!
06/24/2022 13:06:59 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_100_0.0003_8_predictions.txt
06/24/2022 13:06:59 - INFO - __main__ - ACC on test data: 0.6520
06/24/2022 13:06:59 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.0003, bsz=8, dev_performance=0.59375, test_performance=0.6519607843137255
06/24/2022 13:06:59 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.0002, bsz=8 ...
06/24/2022 13:07:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:07:00 - INFO - __main__ - Printing 3 examples
06/24/2022 13:07:00 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 13:07:00 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:07:00 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 13:07:00 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:07:00 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 13:07:00 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:07:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:07:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:07:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:07:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:07:00 - INFO - __main__ - Printing 3 examples
06/24/2022 13:07:00 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 13:07:00 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:07:00 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 13:07:00 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:07:00 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 13:07:00 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:07:00 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:07:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:07:00 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:07:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:07:04 - INFO - __main__ - Starting training!
06/24/2022 13:07:06 - INFO - __main__ - Step 10 Global step 10 Train loss 15.947950 on epoch=4
06/24/2022 13:07:08 - INFO - __main__ - Step 20 Global step 20 Train loss 13.376551 on epoch=9
06/24/2022 13:07:11 - INFO - __main__ - Step 30 Global step 30 Train loss 7.978387 on epoch=14
06/24/2022 13:07:13 - INFO - __main__ - Step 40 Global step 40 Train loss 6.754299 on epoch=19
06/24/2022 13:07:16 - INFO - __main__ - Step 50 Global step 50 Train loss 5.323020 on epoch=24
06/24/2022 13:07:17 - INFO - __main__ - Global step 50 Train loss 9.876041 ACC 0.0 on epoch=24
06/24/2022 13:07:20 - INFO - __main__ - Step 60 Global step 60 Train loss 4.365619 on epoch=29
06/24/2022 13:07:22 - INFO - __main__ - Step 70 Global step 70 Train loss 3.853290 on epoch=34
06/24/2022 13:07:24 - INFO - __main__ - Step 80 Global step 80 Train loss 3.630177 on epoch=39
06/24/2022 13:07:27 - INFO - __main__ - Step 90 Global step 90 Train loss 2.222097 on epoch=44
06/24/2022 13:07:30 - INFO - __main__ - Step 100 Global step 100 Train loss 1.805123 on epoch=49
06/24/2022 13:07:30 - INFO - __main__ - Global step 100 Train loss 3.175261 ACC 0.5 on epoch=49
06/24/2022 13:07:33 - INFO - __main__ - Step 110 Global step 110 Train loss 1.559830 on epoch=54
06/24/2022 13:07:35 - INFO - __main__ - Step 120 Global step 120 Train loss 1.233554 on epoch=59
06/24/2022 13:07:38 - INFO - __main__ - Step 130 Global step 130 Train loss 1.104978 on epoch=64
06/24/2022 13:07:40 - INFO - __main__ - Step 140 Global step 140 Train loss 1.239435 on epoch=69
06/24/2022 13:07:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.863762 on epoch=74
06/24/2022 13:07:43 - INFO - __main__ - Global step 150 Train loss 1.200312 ACC 0.46875 on epoch=74
06/24/2022 13:07:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.903449 on epoch=79
06/24/2022 13:07:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.632460 on epoch=84
06/24/2022 13:07:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.656910 on epoch=89
06/24/2022 13:07:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.528917 on epoch=94
06/24/2022 13:07:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.729220 on epoch=99
06/24/2022 13:07:56 - INFO - __main__ - Global step 200 Train loss 0.690192 ACC 0.5 on epoch=99
06/24/2022 13:07:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.474743 on epoch=104
06/24/2022 13:08:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.375693 on epoch=109
06/24/2022 13:08:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.608474 on epoch=114
06/24/2022 13:08:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.480587 on epoch=119
06/24/2022 13:08:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.334627 on epoch=124
06/24/2022 13:08:09 - INFO - __main__ - Global step 250 Train loss 0.454825 ACC 0.6875 on epoch=124
06/24/2022 13:08:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.359038 on epoch=129
06/24/2022 13:08:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.315003 on epoch=134
06/24/2022 13:08:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.321146 on epoch=139
06/24/2022 13:08:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.346055 on epoch=144
06/24/2022 13:08:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.282338 on epoch=149
06/24/2022 13:08:23 - INFO - __main__ - Global step 300 Train loss 0.324716 ACC 0.5625 on epoch=149
06/24/2022 13:08:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.264686 on epoch=154
06/24/2022 13:08:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.363930 on epoch=159
06/24/2022 13:08:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.357096 on epoch=164
06/24/2022 13:08:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.365043 on epoch=169
06/24/2022 13:08:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.209361 on epoch=174
06/24/2022 13:08:36 - INFO - __main__ - Global step 350 Train loss 0.312023 ACC 0.53125 on epoch=174
06/24/2022 13:08:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.172649 on epoch=179
06/24/2022 13:08:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.176148 on epoch=184
06/24/2022 13:08:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.108987 on epoch=189
06/24/2022 13:08:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.117052 on epoch=194
06/24/2022 13:08:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.194501 on epoch=199
06/24/2022 13:08:49 - INFO - __main__ - Global step 400 Train loss 0.153867 ACC 0.59375 on epoch=199
06/24/2022 13:08:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.097054 on epoch=204
06/24/2022 13:08:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.075728 on epoch=209
06/24/2022 13:08:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.043926 on epoch=214
06/24/2022 13:08:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.036826 on epoch=219
06/24/2022 13:09:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.061828 on epoch=224
06/24/2022 13:09:02 - INFO - __main__ - Global step 450 Train loss 0.063072 ACC 0.59375 on epoch=224
06/24/2022 13:09:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.020010 on epoch=229
06/24/2022 13:09:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.048801 on epoch=234
06/24/2022 13:09:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.071162 on epoch=239
06/24/2022 13:09:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.010295 on epoch=244
06/24/2022 13:09:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.062454 on epoch=249
06/24/2022 13:09:15 - INFO - __main__ - Global step 500 Train loss 0.042544 ACC 0.5625 on epoch=249
06/24/2022 13:09:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.034448 on epoch=254
06/24/2022 13:09:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.066116 on epoch=259
06/24/2022 13:09:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.135928 on epoch=264
06/24/2022 13:09:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.023895 on epoch=269
06/24/2022 13:09:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.011429 on epoch=274
06/24/2022 13:09:28 - INFO - __main__ - Global step 550 Train loss 0.054363 ACC 0.625 on epoch=274
06/24/2022 13:09:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.059420 on epoch=279
06/24/2022 13:09:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.031220 on epoch=284
06/24/2022 13:09:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.045169 on epoch=289
06/24/2022 13:09:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.033160 on epoch=294
06/24/2022 13:09:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.018512 on epoch=299
06/24/2022 13:09:41 - INFO - __main__ - Global step 600 Train loss 0.037496 ACC 0.65625 on epoch=299
06/24/2022 13:09:41 - INFO - __main__ - save last model!
06/24/2022 13:09:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:09:41 - INFO - __main__ - Printing 3 examples
06/24/2022 13:09:41 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 13:09:41 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 13:09:41 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:41 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 13:09:41 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:09:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:09:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:09:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:09:41 - INFO - __main__ - Printing 3 examples
06/24/2022 13:09:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 13:09:41 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:41 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 13:09:41 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 13:09:41 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:09:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:09:41 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:09:43 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:09:44 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:09:44 - INFO - __main__ - Printing 3 examples
06/24/2022 13:09:44 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:09:44 - INFO - __main__ - ['equivalent']
06/24/2022 13:09:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:09:44 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:44 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:09:44 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:44 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:09:44 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:09:44 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:09:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:09:45 - INFO - __main__ - Starting training!
06/24/2022 13:09:49 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_100_0.0002_8_predictions.txt
06/24/2022 13:09:49 - INFO - __main__ - ACC on test data: 0.4534
06/24/2022 13:09:49 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.0002, bsz=8, dev_performance=0.6875, test_performance=0.4534313725490196
06/24/2022 13:09:49 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.0001, bsz=8 ...
06/24/2022 13:09:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:09:50 - INFO - __main__ - Printing 3 examples
06/24/2022 13:09:50 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/24/2022 13:09:50 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/24/2022 13:09:50 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:50 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/24/2022 13:09:50 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:09:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:09:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:09:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:09:50 - INFO - __main__ - Printing 3 examples
06/24/2022 13:09:50 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/24/2022 13:09:50 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:51 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/24/2022 13:09:51 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:51 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/24/2022 13:09:51 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:09:51 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:09:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:09:51 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:09:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:09:54 - INFO - __main__ - Starting training!
06/24/2022 13:09:56 - INFO - __main__ - Step 10 Global step 10 Train loss 15.700514 on epoch=4
06/24/2022 13:09:59 - INFO - __main__ - Step 20 Global step 20 Train loss 13.980951 on epoch=9
06/24/2022 13:10:01 - INFO - __main__ - Step 30 Global step 30 Train loss 12.288737 on epoch=14
06/24/2022 13:10:03 - INFO - __main__ - Step 40 Global step 40 Train loss 8.914417 on epoch=19
06/24/2022 13:10:06 - INFO - __main__ - Step 50 Global step 50 Train loss 7.622079 on epoch=24
06/24/2022 13:10:08 - INFO - __main__ - Global step 50 Train loss 11.701339 ACC 0.0 on epoch=24
06/24/2022 13:10:11 - INFO - __main__ - Step 60 Global step 60 Train loss 7.070422 on epoch=29
06/24/2022 13:10:14 - INFO - __main__ - Step 70 Global step 70 Train loss 6.704043 on epoch=34
06/24/2022 13:10:16 - INFO - __main__ - Step 80 Global step 80 Train loss 6.262274 on epoch=39
06/24/2022 13:10:18 - INFO - __main__ - Step 90 Global step 90 Train loss 5.863819 on epoch=44
06/24/2022 13:10:21 - INFO - __main__ - Step 100 Global step 100 Train loss 5.197189 on epoch=49
06/24/2022 13:10:21 - INFO - __main__ - Global step 100 Train loss 6.219550 ACC 0.15625 on epoch=49
06/24/2022 13:10:24 - INFO - __main__ - Step 110 Global step 110 Train loss 4.770746 on epoch=54
06/24/2022 13:10:27 - INFO - __main__ - Step 120 Global step 120 Train loss 4.276522 on epoch=59
06/24/2022 13:10:29 - INFO - __main__ - Step 130 Global step 130 Train loss 4.130580 on epoch=64
06/24/2022 13:10:32 - INFO - __main__ - Step 140 Global step 140 Train loss 3.852044 on epoch=69
06/24/2022 13:10:34 - INFO - __main__ - Step 150 Global step 150 Train loss 3.560636 on epoch=74
06/24/2022 13:10:35 - INFO - __main__ - Global step 150 Train loss 4.118106 ACC 0.21875 on epoch=74
06/24/2022 13:10:37 - INFO - __main__ - Step 160 Global step 160 Train loss 3.589387 on epoch=79
06/24/2022 13:10:40 - INFO - __main__ - Step 170 Global step 170 Train loss 3.184564 on epoch=84
06/24/2022 13:10:42 - INFO - __main__ - Step 180 Global step 180 Train loss 2.632162 on epoch=89
06/24/2022 13:10:45 - INFO - __main__ - Step 190 Global step 190 Train loss 2.493997 on epoch=94
06/24/2022 13:10:47 - INFO - __main__ - Step 200 Global step 200 Train loss 2.274417 on epoch=99
06/24/2022 13:10:47 - INFO - __main__ - Global step 200 Train loss 2.834906 ACC 0.5 on epoch=99
06/24/2022 13:10:50 - INFO - __main__ - Step 210 Global step 210 Train loss 1.862114 on epoch=104
06/24/2022 13:10:53 - INFO - __main__ - Step 220 Global step 220 Train loss 2.152361 on epoch=109
06/24/2022 13:10:55 - INFO - __main__ - Step 230 Global step 230 Train loss 1.920807 on epoch=114
06/24/2022 13:10:58 - INFO - __main__ - Step 240 Global step 240 Train loss 1.736175 on epoch=119
06/24/2022 13:11:00 - INFO - __main__ - Step 250 Global step 250 Train loss 1.551162 on epoch=124
06/24/2022 13:11:01 - INFO - __main__ - Global step 250 Train loss 1.844524 ACC 0.5 on epoch=124
06/24/2022 13:11:03 - INFO - __main__ - Step 260 Global step 260 Train loss 1.542275 on epoch=129
06/24/2022 13:11:06 - INFO - __main__ - Step 270 Global step 270 Train loss 1.803636 on epoch=134
06/24/2022 13:11:08 - INFO - __main__ - Step 280 Global step 280 Train loss 1.419750 on epoch=139
06/24/2022 13:11:11 - INFO - __main__ - Step 290 Global step 290 Train loss 1.409351 on epoch=144
06/24/2022 13:11:13 - INFO - __main__ - Step 300 Global step 300 Train loss 1.146216 on epoch=149
06/24/2022 13:11:13 - INFO - __main__ - Global step 300 Train loss 1.464246 ACC 0.5 on epoch=149
06/24/2022 13:11:16 - INFO - __main__ - Step 310 Global step 310 Train loss 1.332970 on epoch=154
06/24/2022 13:11:18 - INFO - __main__ - Step 320 Global step 320 Train loss 1.217557 on epoch=159
06/24/2022 13:11:21 - INFO - __main__ - Step 330 Global step 330 Train loss 1.111411 on epoch=164
06/24/2022 13:11:23 - INFO - __main__ - Step 340 Global step 340 Train loss 1.125092 on epoch=169
06/24/2022 13:11:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.803813 on epoch=174
06/24/2022 13:11:26 - INFO - __main__ - Global step 350 Train loss 1.118169 ACC 0.5 on epoch=174
06/24/2022 13:11:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.847276 on epoch=179
06/24/2022 13:11:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.810549 on epoch=184
06/24/2022 13:11:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.673797 on epoch=189
06/24/2022 13:11:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.538370 on epoch=194
06/24/2022 13:11:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.776575 on epoch=199
06/24/2022 13:11:39 - INFO - __main__ - Global step 400 Train loss 0.729313 ACC 0.625 on epoch=199
06/24/2022 13:11:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.820401 on epoch=204
06/24/2022 13:11:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.699943 on epoch=209
06/24/2022 13:11:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.792334 on epoch=214
06/24/2022 13:11:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.597704 on epoch=219
06/24/2022 13:11:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.588973 on epoch=224
06/24/2022 13:11:52 - INFO - __main__ - Global step 450 Train loss 0.699871 ACC 0.53125 on epoch=224
06/24/2022 13:11:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.427345 on epoch=229
06/24/2022 13:11:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.470591 on epoch=234
06/24/2022 13:11:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.448708 on epoch=239
06/24/2022 13:12:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.391758 on epoch=244
06/24/2022 13:12:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.665138 on epoch=249
06/24/2022 13:12:04 - INFO - __main__ - Global step 500 Train loss 0.480708 ACC 0.53125 on epoch=249
06/24/2022 13:12:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.649415 on epoch=254
06/24/2022 13:12:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.487022 on epoch=259
06/24/2022 13:12:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.447560 on epoch=264
06/24/2022 13:12:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.309363 on epoch=269
06/24/2022 13:12:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.359781 on epoch=274
06/24/2022 13:12:17 - INFO - __main__ - Global step 550 Train loss 0.450628 ACC 0.5625 on epoch=274
06/24/2022 13:12:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.487611 on epoch=279
06/24/2022 13:12:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.597439 on epoch=284
06/24/2022 13:12:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.414053 on epoch=289
06/24/2022 13:12:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.478989 on epoch=294
06/24/2022 13:12:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.351843 on epoch=299
06/24/2022 13:12:30 - INFO - __main__ - Global step 600 Train loss 0.465987 ACC 0.5625 on epoch=299
06/24/2022 13:12:30 - INFO - __main__ - save last model!
06/24/2022 13:12:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:12:30 - INFO - __main__ - Printing 3 examples
06/24/2022 13:12:30 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 13:12:30 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 13:12:30 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:30 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 13:12:30 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:12:31 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:12:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:12:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:12:31 - INFO - __main__ - Printing 3 examples
06/24/2022 13:12:31 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 13:12:31 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:31 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 13:12:31 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:31 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 13:12:31 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:31 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:12:31 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:12:31 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:12:32 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:12:33 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:12:33 - INFO - __main__ - Printing 3 examples
06/24/2022 13:12:33 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:12:33 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:33 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:12:33 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:12:33 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:12:33 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:12:33 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:12:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:12:33 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:12:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:12:34 - INFO - __main__ - Starting training!
06/24/2022 13:12:38 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_100_0.0001_8_predictions.txt
06/24/2022 13:12:38 - INFO - __main__ - ACC on test data: 0.4828
06/24/2022 13:12:39 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.0001, bsz=8, dev_performance=0.625, test_performance=0.48284313725490197
06/24/2022 13:12:39 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.0005, bsz=8 ...
06/24/2022 13:12:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:12:40 - INFO - __main__ - Printing 3 examples
06/24/2022 13:12:40 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 13:12:40 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:40 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 13:12:40 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:40 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 13:12:40 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:12:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:12:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:12:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:12:40 - INFO - __main__ - Printing 3 examples
06/24/2022 13:12:40 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 13:12:40 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:40 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 13:12:40 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:40 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 13:12:40 - INFO - __main__ - ['equivalent']
06/24/2022 13:12:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:12:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:12:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:12:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:12:43 - INFO - __main__ - Starting training!
06/24/2022 13:12:45 - INFO - __main__ - Step 10 Global step 10 Train loss 16.250891 on epoch=4
06/24/2022 13:12:48 - INFO - __main__ - Step 20 Global step 20 Train loss 12.012247 on epoch=9
06/24/2022 13:12:50 - INFO - __main__ - Step 30 Global step 30 Train loss 6.830044 on epoch=14
06/24/2022 13:12:53 - INFO - __main__ - Step 40 Global step 40 Train loss 4.600336 on epoch=19
06/24/2022 13:12:55 - INFO - __main__ - Step 50 Global step 50 Train loss 3.668097 on epoch=24
06/24/2022 13:12:56 - INFO - __main__ - Global step 50 Train loss 8.672323 ACC 0.3125 on epoch=24
06/24/2022 13:12:59 - INFO - __main__ - Step 60 Global step 60 Train loss 2.788155 on epoch=29
06/24/2022 13:13:01 - INFO - __main__ - Step 70 Global step 70 Train loss 1.727443 on epoch=34
06/24/2022 13:13:04 - INFO - __main__ - Step 80 Global step 80 Train loss 1.929770 on epoch=39
06/24/2022 13:13:06 - INFO - __main__ - Step 90 Global step 90 Train loss 1.407163 on epoch=44
06/24/2022 13:13:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.916625 on epoch=49
06/24/2022 13:13:09 - INFO - __main__ - Global step 100 Train loss 1.753832 ACC 0.5 on epoch=49
06/24/2022 13:13:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.722805 on epoch=54
06/24/2022 13:13:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.607715 on epoch=59
06/24/2022 13:13:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.697580 on epoch=64
06/24/2022 13:13:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.509464 on epoch=69
06/24/2022 13:13:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.698256 on epoch=74
06/24/2022 13:13:22 - INFO - __main__ - Global step 150 Train loss 0.647164 ACC 0.59375 on epoch=74
06/24/2022 13:13:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.768704 on epoch=79
06/24/2022 13:13:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.643544 on epoch=84
06/24/2022 13:13:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.631276 on epoch=89
06/24/2022 13:13:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.508205 on epoch=94
06/24/2022 13:13:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.467354 on epoch=99
06/24/2022 13:13:35 - INFO - __main__ - Global step 200 Train loss 0.603817 ACC 0.375 on epoch=99
06/24/2022 13:13:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.519869 on epoch=104
06/24/2022 13:13:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.464909 on epoch=109
06/24/2022 13:13:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.433371 on epoch=114
06/24/2022 13:13:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.539843 on epoch=119
06/24/2022 13:13:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.557467 on epoch=124
06/24/2022 13:13:48 - INFO - __main__ - Global step 250 Train loss 0.503092 ACC 0.53125 on epoch=124
06/24/2022 13:13:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.489164 on epoch=129
06/24/2022 13:13:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.374028 on epoch=134
06/24/2022 13:13:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.475245 on epoch=139
06/24/2022 13:13:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.415912 on epoch=144
06/24/2022 13:14:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.438812 on epoch=149
06/24/2022 13:14:00 - INFO - __main__ - Global step 300 Train loss 0.438632 ACC 0.5625 on epoch=149
06/24/2022 13:14:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.506268 on epoch=154
06/24/2022 13:14:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.388625 on epoch=159
06/24/2022 13:14:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.438347 on epoch=164
06/24/2022 13:14:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.532155 on epoch=169
06/24/2022 13:14:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.281507 on epoch=174
06/24/2022 13:14:13 - INFO - __main__ - Global step 350 Train loss 0.429380 ACC 0.5 on epoch=174
06/24/2022 13:14:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.301832 on epoch=179
06/24/2022 13:14:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.255444 on epoch=184
06/24/2022 13:14:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.309653 on epoch=189
06/24/2022 13:14:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.400428 on epoch=194
06/24/2022 13:14:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.264118 on epoch=199
06/24/2022 13:14:26 - INFO - __main__ - Global step 400 Train loss 0.306295 ACC 0.46875 on epoch=199
06/24/2022 13:14:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.273768 on epoch=204
06/24/2022 13:14:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.266306 on epoch=209
06/24/2022 13:14:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.316668 on epoch=214
06/24/2022 13:14:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.294442 on epoch=219
06/24/2022 13:14:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.249058 on epoch=224
06/24/2022 13:14:38 - INFO - __main__ - Global step 450 Train loss 0.280049 ACC 0.53125 on epoch=224
06/24/2022 13:14:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.254714 on epoch=229
06/24/2022 13:14:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.336856 on epoch=234
06/24/2022 13:14:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.248281 on epoch=239
06/24/2022 13:14:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.262826 on epoch=244
06/24/2022 13:14:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.249474 on epoch=249
06/24/2022 13:14:51 - INFO - __main__ - Global step 500 Train loss 0.270430 ACC 0.53125 on epoch=249
06/24/2022 13:14:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.263767 on epoch=254
06/24/2022 13:14:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.249875 on epoch=259
06/24/2022 13:14:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.267393 on epoch=264
06/24/2022 13:15:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.233044 on epoch=269
06/24/2022 13:15:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.146698 on epoch=274
06/24/2022 13:15:04 - INFO - __main__ - Global step 550 Train loss 0.232156 ACC 0.4375 on epoch=274
06/24/2022 13:15:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.210126 on epoch=279
06/24/2022 13:15:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.171009 on epoch=284
06/24/2022 13:15:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.177165 on epoch=289
06/24/2022 13:15:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.144729 on epoch=294
06/24/2022 13:15:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.125816 on epoch=299
06/24/2022 13:15:16 - INFO - __main__ - Global step 600 Train loss 0.165769 ACC 0.4375 on epoch=299
06/24/2022 13:15:16 - INFO - __main__ - save last model!
06/24/2022 13:15:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:15:17 - INFO - __main__ - Printing 3 examples
06/24/2022 13:15:17 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 13:15:17 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:17 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 13:15:17 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:17 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 13:15:17 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:15:17 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:15:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:15:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:15:17 - INFO - __main__ - Printing 3 examples
06/24/2022 13:15:17 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 13:15:17 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:17 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 13:15:17 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:17 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 13:15:17 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:17 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:15:17 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:15:17 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:15:19 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:15:19 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:15:19 - INFO - __main__ - Printing 3 examples
06/24/2022 13:15:19 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:15:19 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:19 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:15:19 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:15:19 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:15:19 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:15:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:15:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:15:20 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:15:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:15:22 - INFO - __main__ - Starting training!
06/24/2022 13:15:26 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_13_0.0005_8_predictions.txt
06/24/2022 13:15:26 - INFO - __main__ - ACC on test data: 0.4632
06/24/2022 13:15:26 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.0005, bsz=8, dev_performance=0.59375, test_performance=0.4632352941176471
06/24/2022 13:15:26 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.0003, bsz=8 ...
06/24/2022 13:15:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:15:27 - INFO - __main__ - Printing 3 examples
06/24/2022 13:15:27 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 13:15:27 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 13:15:27 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:27 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 13:15:27 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:15:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:15:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:15:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:15:27 - INFO - __main__ - Printing 3 examples
06/24/2022 13:15:27 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 13:15:27 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:27 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 13:15:27 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:27 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 13:15:27 - INFO - __main__ - ['equivalent']
06/24/2022 13:15:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:15:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:15:27 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:15:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:15:31 - INFO - __main__ - Starting training!
06/24/2022 13:15:33 - INFO - __main__ - Step 10 Global step 10 Train loss 16.649475 on epoch=4
06/24/2022 13:15:35 - INFO - __main__ - Step 20 Global step 20 Train loss 12.060805 on epoch=9
06/24/2022 13:15:37 - INFO - __main__ - Step 30 Global step 30 Train loss 8.614456 on epoch=14
06/24/2022 13:15:40 - INFO - __main__ - Step 40 Global step 40 Train loss 6.940703 on epoch=19
06/24/2022 13:15:42 - INFO - __main__ - Step 50 Global step 50 Train loss 4.883762 on epoch=24
06/24/2022 13:15:43 - INFO - __main__ - Global step 50 Train loss 9.829841 ACC 0.0625 on epoch=24
06/24/2022 13:15:45 - INFO - __main__ - Step 60 Global step 60 Train loss 3.811897 on epoch=29
06/24/2022 13:15:48 - INFO - __main__ - Step 70 Global step 70 Train loss 3.446158 on epoch=34
06/24/2022 13:15:50 - INFO - __main__ - Step 80 Global step 80 Train loss 2.458899 on epoch=39
06/24/2022 13:15:53 - INFO - __main__ - Step 90 Global step 90 Train loss 2.746387 on epoch=44
06/24/2022 13:15:55 - INFO - __main__ - Step 100 Global step 100 Train loss 1.495736 on epoch=49
06/24/2022 13:15:56 - INFO - __main__ - Global step 100 Train loss 2.791815 ACC 0.5 on epoch=49
06/24/2022 13:15:59 - INFO - __main__ - Step 110 Global step 110 Train loss 1.302874 on epoch=54
06/24/2022 13:16:01 - INFO - __main__ - Step 120 Global step 120 Train loss 1.038483 on epoch=59
06/24/2022 13:16:04 - INFO - __main__ - Step 130 Global step 130 Train loss 1.098457 on epoch=64
06/24/2022 13:16:06 - INFO - __main__ - Step 140 Global step 140 Train loss 1.078774 on epoch=69
06/24/2022 13:16:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.867334 on epoch=74
06/24/2022 13:16:09 - INFO - __main__ - Global step 150 Train loss 1.077185 ACC 0.5 on epoch=74
06/24/2022 13:16:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.840120 on epoch=79
06/24/2022 13:16:14 - INFO - __main__ - Step 170 Global step 170 Train loss 1.014433 on epoch=84
06/24/2022 13:16:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.694789 on epoch=89
06/24/2022 13:16:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.742606 on epoch=94
06/24/2022 13:16:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.739459 on epoch=99
06/24/2022 13:16:22 - INFO - __main__ - Global step 200 Train loss 0.806281 ACC 0.53125 on epoch=99
06/24/2022 13:16:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.436763 on epoch=104
06/24/2022 13:16:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.795800 on epoch=109
06/24/2022 13:16:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.657723 on epoch=114
06/24/2022 13:16:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.506854 on epoch=119
06/24/2022 13:16:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.590037 on epoch=124
06/24/2022 13:16:35 - INFO - __main__ - Global step 250 Train loss 0.597435 ACC 0.53125 on epoch=124
06/24/2022 13:16:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.567879 on epoch=129
06/24/2022 13:16:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.421712 on epoch=134
06/24/2022 13:16:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.736990 on epoch=139
06/24/2022 13:16:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.563351 on epoch=144
06/24/2022 13:16:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.564739 on epoch=149
06/24/2022 13:16:48 - INFO - __main__ - Global step 300 Train loss 0.570934 ACC 0.5 on epoch=149
06/24/2022 13:16:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.582952 on epoch=154
06/24/2022 13:16:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.479042 on epoch=159
06/24/2022 13:16:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.416961 on epoch=164
06/24/2022 13:16:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.627742 on epoch=169
06/24/2022 13:17:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.468273 on epoch=174
06/24/2022 13:17:01 - INFO - __main__ - Global step 350 Train loss 0.514994 ACC 0.46875 on epoch=174
06/24/2022 13:17:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.550319 on epoch=179
06/24/2022 13:17:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.522378 on epoch=184
06/24/2022 13:17:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.551243 on epoch=189
06/24/2022 13:17:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.384563 on epoch=194
06/24/2022 13:17:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.522462 on epoch=199
06/24/2022 13:17:14 - INFO - __main__ - Global step 400 Train loss 0.506193 ACC 0.5 on epoch=199
06/24/2022 13:17:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.584971 on epoch=204
06/24/2022 13:17:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.475546 on epoch=209
06/24/2022 13:17:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.504078 on epoch=214
06/24/2022 13:17:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.448386 on epoch=219
06/24/2022 13:17:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.435023 on epoch=224
06/24/2022 13:17:27 - INFO - __main__ - Global step 450 Train loss 0.489601 ACC 0.5 on epoch=224
06/24/2022 13:17:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.449661 on epoch=229
06/24/2022 13:17:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.392744 on epoch=234
06/24/2022 13:17:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.407468 on epoch=239
06/24/2022 13:17:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.353532 on epoch=244
06/24/2022 13:17:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.550309 on epoch=249
06/24/2022 13:17:40 - INFO - __main__ - Global step 500 Train loss 0.430743 ACC 0.46875 on epoch=249
06/24/2022 13:17:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.437259 on epoch=254
06/24/2022 13:17:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.422469 on epoch=259
06/24/2022 13:17:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.382512 on epoch=264
06/24/2022 13:17:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.485974 on epoch=269
06/24/2022 13:17:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.435183 on epoch=274
06/24/2022 13:17:53 - INFO - __main__ - Global step 550 Train loss 0.432680 ACC 0.5 on epoch=274
06/24/2022 13:17:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.321884 on epoch=279
06/24/2022 13:17:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.532016 on epoch=284
06/24/2022 13:18:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.340597 on epoch=289
06/24/2022 13:18:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.389981 on epoch=294
06/24/2022 13:18:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.387072 on epoch=299
06/24/2022 13:18:06 - INFO - __main__ - Global step 600 Train loss 0.394310 ACC 0.5625 on epoch=299
06/24/2022 13:18:06 - INFO - __main__ - save last model!
06/24/2022 13:18:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:18:06 - INFO - __main__ - Printing 3 examples
06/24/2022 13:18:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 13:18:06 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 13:18:06 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 13:18:06 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:18:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:18:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:18:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:18:07 - INFO - __main__ - Printing 3 examples
06/24/2022 13:18:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 13:18:07 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 13:18:07 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:07 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 13:18:07 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:18:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:18:07 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:18:09 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:18:09 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:18:09 - INFO - __main__ - Printing 3 examples
06/24/2022 13:18:09 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:18:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:09 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:18:09 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:18:09 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:18:09 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:18:09 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:18:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:18:10 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:18:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:18:11 - INFO - __main__ - Starting training!
06/24/2022 13:18:15 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_13_0.0003_8_predictions.txt
06/24/2022 13:18:15 - INFO - __main__ - ACC on test data: 0.4485
06/24/2022 13:18:15 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.0003, bsz=8, dev_performance=0.5625, test_performance=0.4485294117647059
06/24/2022 13:18:15 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.0002, bsz=8 ...
06/24/2022 13:18:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:18:16 - INFO - __main__ - Printing 3 examples
06/24/2022 13:18:16 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 13:18:16 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:16 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 13:18:16 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:16 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 13:18:16 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:18:16 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:18:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:18:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:18:16 - INFO - __main__ - Printing 3 examples
06/24/2022 13:18:16 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 13:18:16 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:16 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 13:18:16 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:16 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 13:18:16 - INFO - __main__ - ['equivalent']
06/24/2022 13:18:16 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:18:16 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:18:16 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:18:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:18:20 - INFO - __main__ - Starting training!
06/24/2022 13:18:22 - INFO - __main__ - Step 10 Global step 10 Train loss 15.972918 on epoch=4
06/24/2022 13:18:24 - INFO - __main__ - Step 20 Global step 20 Train loss 13.872093 on epoch=9
06/24/2022 13:18:27 - INFO - __main__ - Step 30 Global step 30 Train loss 9.081735 on epoch=14
06/24/2022 13:18:29 - INFO - __main__ - Step 40 Global step 40 Train loss 7.423144 on epoch=19
06/24/2022 13:18:32 - INFO - __main__ - Step 50 Global step 50 Train loss 6.060874 on epoch=24
06/24/2022 13:18:33 - INFO - __main__ - Global step 50 Train loss 10.482153 ACC 0.09375 on epoch=24
06/24/2022 13:18:36 - INFO - __main__ - Step 60 Global step 60 Train loss 5.740721 on epoch=29
06/24/2022 13:18:38 - INFO - __main__ - Step 70 Global step 70 Train loss 4.539741 on epoch=34
06/24/2022 13:18:41 - INFO - __main__ - Step 80 Global step 80 Train loss 3.886717 on epoch=39
06/24/2022 13:18:43 - INFO - __main__ - Step 90 Global step 90 Train loss 2.995070 on epoch=44
06/24/2022 13:18:46 - INFO - __main__ - Step 100 Global step 100 Train loss 2.187322 on epoch=49
06/24/2022 13:18:46 - INFO - __main__ - Global step 100 Train loss 3.869914 ACC 0.5 on epoch=49
06/24/2022 13:18:49 - INFO - __main__ - Step 110 Global step 110 Train loss 2.055948 on epoch=54
06/24/2022 13:18:52 - INFO - __main__ - Step 120 Global step 120 Train loss 1.939883 on epoch=59
06/24/2022 13:18:54 - INFO - __main__ - Step 130 Global step 130 Train loss 1.513589 on epoch=64
06/24/2022 13:18:56 - INFO - __main__ - Step 140 Global step 140 Train loss 1.688634 on epoch=69
06/24/2022 13:18:59 - INFO - __main__ - Step 150 Global step 150 Train loss 1.309568 on epoch=74
06/24/2022 13:18:59 - INFO - __main__ - Global step 150 Train loss 1.701524 ACC 0.5 on epoch=74
06/24/2022 13:19:02 - INFO - __main__ - Step 160 Global step 160 Train loss 1.759523 on epoch=79
06/24/2022 13:19:04 - INFO - __main__ - Step 170 Global step 170 Train loss 1.277818 on epoch=84
06/24/2022 13:19:07 - INFO - __main__ - Step 180 Global step 180 Train loss 1.191850 on epoch=89
06/24/2022 13:19:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.893156 on epoch=94
06/24/2022 13:19:12 - INFO - __main__ - Step 200 Global step 200 Train loss 1.076413 on epoch=99
06/24/2022 13:19:12 - INFO - __main__ - Global step 200 Train loss 1.239752 ACC 0.5 on epoch=99
06/24/2022 13:19:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.916139 on epoch=104
06/24/2022 13:19:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.896207 on epoch=109
06/24/2022 13:19:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.571176 on epoch=114
06/24/2022 13:19:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.695814 on epoch=119
06/24/2022 13:19:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.607266 on epoch=124
06/24/2022 13:19:25 - INFO - __main__ - Global step 250 Train loss 0.737320 ACC 0.5 on epoch=124
06/24/2022 13:19:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.609997 on epoch=129
06/24/2022 13:19:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.564872 on epoch=134
06/24/2022 13:19:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.355903 on epoch=139
06/24/2022 13:19:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.606620 on epoch=144
06/24/2022 13:19:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.465142 on epoch=149
06/24/2022 13:19:38 - INFO - __main__ - Global step 300 Train loss 0.520507 ACC 0.5 on epoch=149
06/24/2022 13:19:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.474429 on epoch=154
06/24/2022 13:19:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.474934 on epoch=159
06/24/2022 13:19:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.389654 on epoch=164
06/24/2022 13:19:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.513131 on epoch=169
06/24/2022 13:19:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.406778 on epoch=174
06/24/2022 13:19:51 - INFO - __main__ - Global step 350 Train loss 0.451785 ACC 0.5 on epoch=174
06/24/2022 13:19:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.470969 on epoch=179
06/24/2022 13:19:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.383301 on epoch=184
06/24/2022 13:19:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.330787 on epoch=189
06/24/2022 13:20:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.349108 on epoch=194
06/24/2022 13:20:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.212886 on epoch=199
06/24/2022 13:20:04 - INFO - __main__ - Global step 400 Train loss 0.349410 ACC 0.53125 on epoch=199
06/24/2022 13:20:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.309599 on epoch=204
06/24/2022 13:20:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.363313 on epoch=209
06/24/2022 13:20:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.233756 on epoch=214
06/24/2022 13:20:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.257287 on epoch=219
06/24/2022 13:20:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.231830 on epoch=224
06/24/2022 13:20:17 - INFO - __main__ - Global step 450 Train loss 0.279157 ACC 0.5625 on epoch=224
06/24/2022 13:20:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.220015 on epoch=229
06/24/2022 13:20:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.175791 on epoch=234
06/24/2022 13:20:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.248759 on epoch=239
06/24/2022 13:20:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.161878 on epoch=244
06/24/2022 13:20:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.168833 on epoch=249
06/24/2022 13:20:30 - INFO - __main__ - Global step 500 Train loss 0.195055 ACC 0.53125 on epoch=249
06/24/2022 13:20:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.160236 on epoch=254
06/24/2022 13:20:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.147328 on epoch=259
06/24/2022 13:20:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.143753 on epoch=264
06/24/2022 13:20:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.104383 on epoch=269
06/24/2022 13:20:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.136429 on epoch=274
06/24/2022 13:20:43 - INFO - __main__ - Global step 550 Train loss 0.138426 ACC 0.53125 on epoch=274
06/24/2022 13:20:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.113761 on epoch=279
06/24/2022 13:20:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.128451 on epoch=284
06/24/2022 13:20:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.026913 on epoch=289
06/24/2022 13:20:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.038289 on epoch=294
06/24/2022 13:20:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.064677 on epoch=299
06/24/2022 13:20:56 - INFO - __main__ - Global step 600 Train loss 0.074418 ACC 0.5625 on epoch=299
06/24/2022 13:20:56 - INFO - __main__ - save last model!
06/24/2022 13:20:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:20:57 - INFO - __main__ - Printing 3 examples
06/24/2022 13:20:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 13:20:57 - INFO - __main__ - ['equivalent']
06/24/2022 13:20:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 13:20:57 - INFO - __main__ - ['equivalent']
06/24/2022 13:20:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 13:20:57 - INFO - __main__ - ['equivalent']
06/24/2022 13:20:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:20:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:20:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:20:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:20:57 - INFO - __main__ - Printing 3 examples
06/24/2022 13:20:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 13:20:57 - INFO - __main__ - ['equivalent']
06/24/2022 13:20:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 13:20:57 - INFO - __main__ - ['equivalent']
06/24/2022 13:20:57 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 13:20:57 - INFO - __main__ - ['equivalent']
06/24/2022 13:20:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:20:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:20:57 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:20:59 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:20:59 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:20:59 - INFO - __main__ - Printing 3 examples
06/24/2022 13:20:59 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:20:59 - INFO - __main__ - ['equivalent']
06/24/2022 13:20:59 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:20:59 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:20:59 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:20:59 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:20:59 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:20:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:21:00 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:21:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:21:01 - INFO - __main__ - Starting training!
06/24/2022 13:21:05 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_13_0.0002_8_predictions.txt
06/24/2022 13:21:05 - INFO - __main__ - ACC on test data: 0.6593
06/24/2022 13:21:05 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.0002, bsz=8, dev_performance=0.5625, test_performance=0.6593137254901961
06/24/2022 13:21:05 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.0001, bsz=8 ...
06/24/2022 13:21:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:21:06 - INFO - __main__ - Printing 3 examples
06/24/2022 13:21:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/24/2022 13:21:06 - INFO - __main__ - ['equivalent']
06/24/2022 13:21:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/24/2022 13:21:06 - INFO - __main__ - ['equivalent']
06/24/2022 13:21:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/24/2022 13:21:06 - INFO - __main__ - ['equivalent']
06/24/2022 13:21:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:21:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:21:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:21:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:21:06 - INFO - __main__ - Printing 3 examples
06/24/2022 13:21:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/24/2022 13:21:06 - INFO - __main__ - ['equivalent']
06/24/2022 13:21:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/24/2022 13:21:06 - INFO - __main__ - ['equivalent']
06/24/2022 13:21:06 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/24/2022 13:21:06 - INFO - __main__ - ['equivalent']
06/24/2022 13:21:06 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:21:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:21:06 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:21:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:21:10 - INFO - __main__ - Starting training!
06/24/2022 13:21:12 - INFO - __main__ - Step 10 Global step 10 Train loss 16.569094 on epoch=4
06/24/2022 13:21:14 - INFO - __main__ - Step 20 Global step 20 Train loss 14.411240 on epoch=9
06/24/2022 13:21:17 - INFO - __main__ - Step 30 Global step 30 Train loss 10.153823 on epoch=14
06/24/2022 13:21:19 - INFO - __main__ - Step 40 Global step 40 Train loss 8.524346 on epoch=19
06/24/2022 13:21:21 - INFO - __main__ - Step 50 Global step 50 Train loss 7.598200 on epoch=24
06/24/2022 13:21:25 - INFO - __main__ - Global step 50 Train loss 11.451341 ACC 0.0 on epoch=24
06/24/2022 13:21:27 - INFO - __main__ - Step 60 Global step 60 Train loss 7.008420 on epoch=29
06/24/2022 13:21:30 - INFO - __main__ - Step 70 Global step 70 Train loss 6.250839 on epoch=34
06/24/2022 13:21:32 - INFO - __main__ - Step 80 Global step 80 Train loss 5.965562 on epoch=39
06/24/2022 13:21:35 - INFO - __main__ - Step 90 Global step 90 Train loss 5.431690 on epoch=44
06/24/2022 13:21:37 - INFO - __main__ - Step 100 Global step 100 Train loss 5.073416 on epoch=49
06/24/2022 13:21:38 - INFO - __main__ - Global step 100 Train loss 5.945986 ACC 0.21875 on epoch=49
06/24/2022 13:21:42 - INFO - __main__ - Step 110 Global step 110 Train loss 4.313345 on epoch=54
06/24/2022 13:21:44 - INFO - __main__ - Step 120 Global step 120 Train loss 4.593355 on epoch=59
06/24/2022 13:21:47 - INFO - __main__ - Step 130 Global step 130 Train loss 4.113497 on epoch=64
06/24/2022 13:21:49 - INFO - __main__ - Step 140 Global step 140 Train loss 3.992549 on epoch=69
06/24/2022 13:21:51 - INFO - __main__ - Step 150 Global step 150 Train loss 3.345209 on epoch=74
06/24/2022 13:21:52 - INFO - __main__ - Global step 150 Train loss 4.071591 ACC 0.34375 on epoch=74
06/24/2022 13:21:55 - INFO - __main__ - Step 160 Global step 160 Train loss 2.484226 on epoch=79
06/24/2022 13:21:57 - INFO - __main__ - Step 170 Global step 170 Train loss 3.633284 on epoch=84
06/24/2022 13:22:00 - INFO - __main__ - Step 180 Global step 180 Train loss 2.351732 on epoch=89
06/24/2022 13:22:02 - INFO - __main__ - Step 190 Global step 190 Train loss 3.048569 on epoch=94
06/24/2022 13:22:05 - INFO - __main__ - Step 200 Global step 200 Train loss 2.206678 on epoch=99
06/24/2022 13:22:05 - INFO - __main__ - Global step 200 Train loss 2.744898 ACC 0.5 on epoch=99
06/24/2022 13:22:08 - INFO - __main__ - Step 210 Global step 210 Train loss 1.397110 on epoch=104
06/24/2022 13:22:11 - INFO - __main__ - Step 220 Global step 220 Train loss 1.690982 on epoch=109
06/24/2022 13:22:13 - INFO - __main__ - Step 230 Global step 230 Train loss 1.646285 on epoch=114
06/24/2022 13:22:16 - INFO - __main__ - Step 240 Global step 240 Train loss 1.169910 on epoch=119
06/24/2022 13:22:18 - INFO - __main__ - Step 250 Global step 250 Train loss 1.224917 on epoch=124
06/24/2022 13:22:18 - INFO - __main__ - Global step 250 Train loss 1.425841 ACC 0.5 on epoch=124
06/24/2022 13:22:21 - INFO - __main__ - Step 260 Global step 260 Train loss 1.466514 on epoch=129
06/24/2022 13:22:23 - INFO - __main__ - Step 270 Global step 270 Train loss 1.823963 on epoch=134
06/24/2022 13:22:26 - INFO - __main__ - Step 280 Global step 280 Train loss 1.639396 on epoch=139
06/24/2022 13:22:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.869367 on epoch=144
06/24/2022 13:22:31 - INFO - __main__ - Step 300 Global step 300 Train loss 1.226807 on epoch=149
06/24/2022 13:22:31 - INFO - __main__ - Global step 300 Train loss 1.405209 ACC 0.5 on epoch=149
06/24/2022 13:22:33 - INFO - __main__ - Step 310 Global step 310 Train loss 1.444451 on epoch=154
06/24/2022 13:22:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.500834 on epoch=159
06/24/2022 13:22:38 - INFO - __main__ - Step 330 Global step 330 Train loss 1.104483 on epoch=164
06/24/2022 13:22:41 - INFO - __main__ - Step 340 Global step 340 Train loss 1.118659 on epoch=169
06/24/2022 13:22:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.874939 on epoch=174
06/24/2022 13:22:44 - INFO - __main__ - Global step 350 Train loss 1.208673 ACC 0.5 on epoch=174
06/24/2022 13:22:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.879857 on epoch=179
06/24/2022 13:22:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.810293 on epoch=184
06/24/2022 13:22:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.682563 on epoch=189
06/24/2022 13:22:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.925585 on epoch=194
06/24/2022 13:22:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.595928 on epoch=199
06/24/2022 13:22:56 - INFO - __main__ - Global step 400 Train loss 0.778845 ACC 0.5 on epoch=199
06/24/2022 13:22:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.731870 on epoch=204
06/24/2022 13:23:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.716127 on epoch=209
06/24/2022 13:23:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.641199 on epoch=214
06/24/2022 13:23:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.723332 on epoch=219
06/24/2022 13:23:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.734564 on epoch=224
06/24/2022 13:23:09 - INFO - __main__ - Global step 450 Train loss 0.709418 ACC 0.46875 on epoch=224
06/24/2022 13:23:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.811117 on epoch=229
06/24/2022 13:23:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.728975 on epoch=234
06/24/2022 13:23:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.632268 on epoch=239
06/24/2022 13:23:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.522981 on epoch=244
06/24/2022 13:23:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.573231 on epoch=249
06/24/2022 13:23:22 - INFO - __main__ - Global step 500 Train loss 0.653715 ACC 0.53125 on epoch=249
06/24/2022 13:23:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.586331 on epoch=254
06/24/2022 13:23:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.407296 on epoch=259
06/24/2022 13:23:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.605652 on epoch=264
06/24/2022 13:23:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.453097 on epoch=269
06/24/2022 13:23:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.645851 on epoch=274
06/24/2022 13:23:35 - INFO - __main__ - Global step 550 Train loss 0.539645 ACC 0.53125 on epoch=274
06/24/2022 13:23:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.423748 on epoch=279
06/24/2022 13:23:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.491587 on epoch=284
06/24/2022 13:23:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.477374 on epoch=289
06/24/2022 13:23:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.504891 on epoch=294
06/24/2022 13:23:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.497537 on epoch=299
06/24/2022 13:23:48 - INFO - __main__ - Global step 600 Train loss 0.479027 ACC 0.53125 on epoch=299
06/24/2022 13:23:48 - INFO - __main__ - save last model!
06/24/2022 13:23:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:23:49 - INFO - __main__ - Printing 3 examples
06/24/2022 13:23:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 13:23:49 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:49 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 13:23:49 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 13:23:49 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:23:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:23:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:23:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:23:49 - INFO - __main__ - Printing 3 examples
06/24/2022 13:23:49 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 13:23:49 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:49 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 13:23:49 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 13:23:49 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:49 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:23:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:23:49 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:23:51 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:23:51 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:23:51 - INFO - __main__ - Printing 3 examples
06/24/2022 13:23:51 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:23:51 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:51 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:23:51 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:23:51 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:23:51 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:23:51 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:23:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:23:52 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:23:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:23:53 - INFO - __main__ - Starting training!
06/24/2022 13:23:57 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_13_0.0001_8_predictions.txt
06/24/2022 13:23:57 - INFO - __main__ - ACC on test data: 0.5245
06/24/2022 13:23:57 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.0001, bsz=8, dev_performance=0.53125, test_performance=0.5245098039215687
06/24/2022 13:23:57 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.0005, bsz=8 ...
06/24/2022 13:23:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:23:58 - INFO - __main__ - Printing 3 examples
06/24/2022 13:23:58 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 13:23:58 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:58 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 13:23:58 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 13:23:58 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:23:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:23:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:23:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:23:58 - INFO - __main__ - Printing 3 examples
06/24/2022 13:23:58 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 13:23:58 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:58 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 13:23:58 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 13:23:58 - INFO - __main__ - ['equivalent']
06/24/2022 13:23:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:23:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:23:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:24:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:24:03 - INFO - __main__ - Starting training!
06/24/2022 13:24:05 - INFO - __main__ - Step 10 Global step 10 Train loss 16.269009 on epoch=4
06/24/2022 13:24:07 - INFO - __main__ - Step 20 Global step 20 Train loss 13.600550 on epoch=9
06/24/2022 13:24:10 - INFO - __main__ - Step 30 Global step 30 Train loss 7.213983 on epoch=14
06/24/2022 13:24:12 - INFO - __main__ - Step 40 Global step 40 Train loss 4.816021 on epoch=19
06/24/2022 13:24:14 - INFO - __main__ - Step 50 Global step 50 Train loss 2.624086 on epoch=24
06/24/2022 13:24:15 - INFO - __main__ - Global step 50 Train loss 8.904730 ACC 0.5 on epoch=24
06/24/2022 13:24:17 - INFO - __main__ - Step 60 Global step 60 Train loss 3.559724 on epoch=29
06/24/2022 13:24:20 - INFO - __main__ - Step 70 Global step 70 Train loss 1.646412 on epoch=34
06/24/2022 13:24:22 - INFO - __main__ - Step 80 Global step 80 Train loss 2.022491 on epoch=39
06/24/2022 13:24:25 - INFO - __main__ - Step 90 Global step 90 Train loss 1.683522 on epoch=44
06/24/2022 13:24:27 - INFO - __main__ - Step 100 Global step 100 Train loss 1.113569 on epoch=49
06/24/2022 13:24:28 - INFO - __main__ - Global step 100 Train loss 2.005144 ACC 0.46875 on epoch=49
06/24/2022 13:24:30 - INFO - __main__ - Step 110 Global step 110 Train loss 1.150459 on epoch=54
06/24/2022 13:24:33 - INFO - __main__ - Step 120 Global step 120 Train loss 1.277932 on epoch=59
06/24/2022 13:24:35 - INFO - __main__ - Step 130 Global step 130 Train loss 1.089730 on epoch=64
06/24/2022 13:24:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.756667 on epoch=69
06/24/2022 13:24:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.807453 on epoch=74
06/24/2022 13:24:41 - INFO - __main__ - Global step 150 Train loss 1.016448 ACC 0.625 on epoch=74
06/24/2022 13:24:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.611895 on epoch=79
06/24/2022 13:24:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.579008 on epoch=84
06/24/2022 13:24:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.503556 on epoch=89
06/24/2022 13:24:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.548112 on epoch=94
06/24/2022 13:24:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.700304 on epoch=99
06/24/2022 13:24:54 - INFO - __main__ - Global step 200 Train loss 0.588575 ACC 0.59375 on epoch=99
06/24/2022 13:24:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.385986 on epoch=104
06/24/2022 13:24:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.474990 on epoch=109
06/24/2022 13:25:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.448761 on epoch=114
06/24/2022 13:25:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.449645 on epoch=119
06/24/2022 13:25:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.340770 on epoch=124
06/24/2022 13:25:07 - INFO - __main__ - Global step 250 Train loss 0.420030 ACC 0.59375 on epoch=124
06/24/2022 13:25:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.260515 on epoch=129
06/24/2022 13:25:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.456586 on epoch=134
06/24/2022 13:25:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.359980 on epoch=139
06/24/2022 13:25:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.250250 on epoch=144
06/24/2022 13:25:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.241698 on epoch=149
06/24/2022 13:25:20 - INFO - __main__ - Global step 300 Train loss 0.313806 ACC 0.59375 on epoch=149
06/24/2022 13:25:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.189774 on epoch=154
06/24/2022 13:25:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.251005 on epoch=159
06/24/2022 13:25:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.246890 on epoch=164
06/24/2022 13:25:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.194808 on epoch=169
06/24/2022 13:25:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.169705 on epoch=174
06/24/2022 13:25:32 - INFO - __main__ - Global step 350 Train loss 0.210436 ACC 0.625 on epoch=174
06/24/2022 13:25:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.252082 on epoch=179
06/24/2022 13:25:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.226344 on epoch=184
06/24/2022 13:25:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.076267 on epoch=189
06/24/2022 13:25:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.143078 on epoch=194
06/24/2022 13:25:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.018444 on epoch=199
06/24/2022 13:25:45 - INFO - __main__ - Global step 400 Train loss 0.143243 ACC 0.65625 on epoch=199
06/24/2022 13:25:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.026261 on epoch=204
06/24/2022 13:25:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.033909 on epoch=209
06/24/2022 13:25:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.058827 on epoch=214
06/24/2022 13:25:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.017641 on epoch=219
06/24/2022 13:25:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.031388 on epoch=224
06/24/2022 13:25:58 - INFO - __main__ - Global step 450 Train loss 0.033605 ACC 0.59375 on epoch=224
06/24/2022 13:26:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.040782 on epoch=229
06/24/2022 13:26:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.013654 on epoch=234
06/24/2022 13:26:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.023157 on epoch=239
06/24/2022 13:26:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.009267 on epoch=244
06/24/2022 13:26:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.020085 on epoch=249
06/24/2022 13:26:11 - INFO - __main__ - Global step 500 Train loss 0.021389 ACC 0.65625 on epoch=249
06/24/2022 13:26:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.017609 on epoch=254
06/24/2022 13:26:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.016439 on epoch=259
06/24/2022 13:26:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.036158 on epoch=264
06/24/2022 13:26:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.009759 on epoch=269
06/24/2022 13:26:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.031095 on epoch=274
06/24/2022 13:26:24 - INFO - __main__ - Global step 550 Train loss 0.022212 ACC 0.65625 on epoch=274
06/24/2022 13:26:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.017492 on epoch=279
06/24/2022 13:26:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.006276 on epoch=284
06/24/2022 13:26:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000585 on epoch=289
06/24/2022 13:26:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.010125 on epoch=294
06/24/2022 13:26:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.016509 on epoch=299
06/24/2022 13:26:37 - INFO - __main__ - Global step 600 Train loss 0.010197 ACC 0.625 on epoch=299
06/24/2022 13:26:37 - INFO - __main__ - save last model!
06/24/2022 13:26:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:26:37 - INFO - __main__ - Printing 3 examples
06/24/2022 13:26:37 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 13:26:37 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:37 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 13:26:37 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:37 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 13:26:37 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:26:37 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:26:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:26:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:26:37 - INFO - __main__ - Printing 3 examples
06/24/2022 13:26:37 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 13:26:37 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:37 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 13:26:37 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:37 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 13:26:37 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:37 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:26:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:26:38 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:26:40 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:26:40 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:26:40 - INFO - __main__ - Printing 3 examples
06/24/2022 13:26:40 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:26:40 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:40 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:26:40 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:26:40 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:26:40 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:26:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:26:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:26:40 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:26:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:26:41 - INFO - __main__ - Starting training!
06/24/2022 13:26:45 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_21_0.0005_8_predictions.txt
06/24/2022 13:26:46 - INFO - __main__ - ACC on test data: 0.6078
06/24/2022 13:26:46 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.0005, bsz=8, dev_performance=0.65625, test_performance=0.6078431372549019
06/24/2022 13:26:46 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.0003, bsz=8 ...
06/24/2022 13:26:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:26:47 - INFO - __main__ - Printing 3 examples
06/24/2022 13:26:47 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 13:26:47 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:47 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 13:26:47 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:47 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 13:26:47 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:26:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:26:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:26:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:26:47 - INFO - __main__ - Printing 3 examples
06/24/2022 13:26:47 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 13:26:47 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:47 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 13:26:47 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:47 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 13:26:47 - INFO - __main__ - ['equivalent']
06/24/2022 13:26:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:26:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:26:47 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:26:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:26:51 - INFO - __main__ - Starting training!
06/24/2022 13:26:53 - INFO - __main__ - Step 10 Global step 10 Train loss 16.417582 on epoch=4
06/24/2022 13:26:55 - INFO - __main__ - Step 20 Global step 20 Train loss 13.570015 on epoch=9
06/24/2022 13:26:57 - INFO - __main__ - Step 30 Global step 30 Train loss 7.751677 on epoch=14
06/24/2022 13:27:00 - INFO - __main__ - Step 40 Global step 40 Train loss 6.114939 on epoch=19
06/24/2022 13:27:02 - INFO - __main__ - Step 50 Global step 50 Train loss 4.725837 on epoch=24
06/24/2022 13:27:03 - INFO - __main__ - Global step 50 Train loss 9.716010 ACC 0.0 on epoch=24
06/24/2022 13:27:05 - INFO - __main__ - Step 60 Global step 60 Train loss 3.686312 on epoch=29
06/24/2022 13:27:08 - INFO - __main__ - Step 70 Global step 70 Train loss 2.275886 on epoch=34
06/24/2022 13:27:10 - INFO - __main__ - Step 80 Global step 80 Train loss 1.827136 on epoch=39
06/24/2022 13:27:13 - INFO - __main__ - Step 90 Global step 90 Train loss 1.451385 on epoch=44
06/24/2022 13:27:15 - INFO - __main__ - Step 100 Global step 100 Train loss 1.442074 on epoch=49
06/24/2022 13:27:16 - INFO - __main__ - Global step 100 Train loss 2.136559 ACC 0.5 on epoch=49
06/24/2022 13:27:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.899167 on epoch=54
06/24/2022 13:27:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.917892 on epoch=59
06/24/2022 13:27:24 - INFO - __main__ - Step 130 Global step 130 Train loss 1.058774 on epoch=64
06/24/2022 13:27:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.701531 on epoch=69
06/24/2022 13:27:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.858973 on epoch=74
06/24/2022 13:27:29 - INFO - __main__ - Global step 150 Train loss 0.887267 ACC 0.5 on epoch=74
06/24/2022 13:27:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.724260 on epoch=79
06/24/2022 13:27:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.586017 on epoch=84
06/24/2022 13:27:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.451157 on epoch=89
06/24/2022 13:27:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.462196 on epoch=94
06/24/2022 13:27:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.453334 on epoch=99
06/24/2022 13:27:42 - INFO - __main__ - Global step 200 Train loss 0.535393 ACC 0.59375 on epoch=99
06/24/2022 13:27:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.577752 on epoch=104
06/24/2022 13:27:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.506072 on epoch=109
06/24/2022 13:27:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.647708 on epoch=114
06/24/2022 13:27:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.322354 on epoch=119
06/24/2022 13:27:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.488308 on epoch=124
06/24/2022 13:27:55 - INFO - __main__ - Global step 250 Train loss 0.508439 ACC 0.5625 on epoch=124
06/24/2022 13:27:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.289471 on epoch=129
06/24/2022 13:28:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.446529 on epoch=134
06/24/2022 13:28:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.348857 on epoch=139
06/24/2022 13:28:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.357278 on epoch=144
06/24/2022 13:28:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.510135 on epoch=149
06/24/2022 13:28:08 - INFO - __main__ - Global step 300 Train loss 0.390454 ACC 0.5625 on epoch=149
06/24/2022 13:28:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.417855 on epoch=154
06/24/2022 13:28:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.332886 on epoch=159
06/24/2022 13:28:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.303565 on epoch=164
06/24/2022 13:28:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.336778 on epoch=169
06/24/2022 13:28:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.253135 on epoch=174
06/24/2022 13:28:21 - INFO - __main__ - Global step 350 Train loss 0.328844 ACC 0.625 on epoch=174
06/24/2022 13:28:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.282159 on epoch=179
06/24/2022 13:28:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.343439 on epoch=184
06/24/2022 13:28:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.223640 on epoch=189
06/24/2022 13:28:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.165915 on epoch=194
06/24/2022 13:28:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.140494 on epoch=199
06/24/2022 13:28:34 - INFO - __main__ - Global step 400 Train loss 0.231130 ACC 0.625 on epoch=199
06/24/2022 13:28:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.135343 on epoch=204
06/24/2022 13:28:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.121904 on epoch=209
06/24/2022 13:28:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.073627 on epoch=214
06/24/2022 13:28:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.072017 on epoch=219
06/24/2022 13:28:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.107587 on epoch=224
06/24/2022 13:28:47 - INFO - __main__ - Global step 450 Train loss 0.102096 ACC 0.625 on epoch=224
06/24/2022 13:28:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.111678 on epoch=229
06/24/2022 13:28:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.085141 on epoch=234
06/24/2022 13:28:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.076306 on epoch=239
06/24/2022 13:28:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.044439 on epoch=244
06/24/2022 13:28:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.055396 on epoch=249
06/24/2022 13:29:00 - INFO - __main__ - Global step 500 Train loss 0.074592 ACC 0.625 on epoch=249
06/24/2022 13:29:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.051034 on epoch=254
06/24/2022 13:29:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.012018 on epoch=259
06/24/2022 13:29:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.016443 on epoch=264
06/24/2022 13:29:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.066758 on epoch=269
06/24/2022 13:29:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.024591 on epoch=274
06/24/2022 13:29:12 - INFO - __main__ - Global step 550 Train loss 0.034169 ACC 0.59375 on epoch=274
06/24/2022 13:29:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.014401 on epoch=279
06/24/2022 13:29:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.042449 on epoch=284
06/24/2022 13:29:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.017467 on epoch=289
06/24/2022 13:29:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.029623 on epoch=294
06/24/2022 13:29:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.006864 on epoch=299
06/24/2022 13:29:25 - INFO - __main__ - Global step 600 Train loss 0.022161 ACC 0.625 on epoch=299
06/24/2022 13:29:25 - INFO - __main__ - save last model!
06/24/2022 13:29:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:29:26 - INFO - __main__ - Printing 3 examples
06/24/2022 13:29:26 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 13:29:26 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:26 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 13:29:26 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:26 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 13:29:26 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:29:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:29:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:29:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:29:26 - INFO - __main__ - Printing 3 examples
06/24/2022 13:29:26 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 13:29:26 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:26 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 13:29:26 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:26 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 13:29:26 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:29:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:29:26 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:29:28 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:29:28 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:29:28 - INFO - __main__ - Printing 3 examples
06/24/2022 13:29:28 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:29:28 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:28 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:29:28 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:29:28 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:29:28 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:29:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:29:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:29:29 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:29:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:29:30 - INFO - __main__ - Starting training!
06/24/2022 13:29:34 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_21_0.0003_8_predictions.txt
06/24/2022 13:29:34 - INFO - __main__ - ACC on test data: 0.6569
06/24/2022 13:29:34 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.0003, bsz=8, dev_performance=0.625, test_performance=0.6568627450980392
06/24/2022 13:29:34 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.0002, bsz=8 ...
06/24/2022 13:29:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:29:35 - INFO - __main__ - Printing 3 examples
06/24/2022 13:29:35 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 13:29:35 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:35 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 13:29:35 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:35 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 13:29:35 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:29:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:29:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:29:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:29:35 - INFO - __main__ - Printing 3 examples
06/24/2022 13:29:35 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 13:29:35 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:35 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 13:29:35 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:35 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 13:29:35 - INFO - __main__ - ['equivalent']
06/24/2022 13:29:35 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:29:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:29:35 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:29:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:29:39 - INFO - __main__ - Starting training!
06/24/2022 13:29:41 - INFO - __main__ - Step 10 Global step 10 Train loss 15.946652 on epoch=4
06/24/2022 13:29:43 - INFO - __main__ - Step 20 Global step 20 Train loss 14.217108 on epoch=9
06/24/2022 13:29:45 - INFO - __main__ - Step 30 Global step 30 Train loss 10.312651 on epoch=14
06/24/2022 13:29:48 - INFO - __main__ - Step 40 Global step 40 Train loss 7.768369 on epoch=19
06/24/2022 13:29:51 - INFO - __main__ - Step 50 Global step 50 Train loss 6.367254 on epoch=24
06/24/2022 13:29:52 - INFO - __main__ - Global step 50 Train loss 10.922406 ACC 0.0 on epoch=24
06/24/2022 13:29:55 - INFO - __main__ - Step 60 Global step 60 Train loss 5.945690 on epoch=29
06/24/2022 13:29:57 - INFO - __main__ - Step 70 Global step 70 Train loss 5.340730 on epoch=34
06/24/2022 13:30:00 - INFO - __main__ - Step 80 Global step 80 Train loss 4.040781 on epoch=39
06/24/2022 13:30:02 - INFO - __main__ - Step 90 Global step 90 Train loss 3.610175 on epoch=44
06/24/2022 13:30:05 - INFO - __main__ - Step 100 Global step 100 Train loss 2.606601 on epoch=49
06/24/2022 13:30:05 - INFO - __main__ - Global step 100 Train loss 4.308795 ACC 0.46875 on epoch=49
06/24/2022 13:30:08 - INFO - __main__ - Step 110 Global step 110 Train loss 2.756520 on epoch=54
06/24/2022 13:30:11 - INFO - __main__ - Step 120 Global step 120 Train loss 1.681145 on epoch=59
06/24/2022 13:30:13 - INFO - __main__ - Step 130 Global step 130 Train loss 1.741554 on epoch=64
06/24/2022 13:30:16 - INFO - __main__ - Step 140 Global step 140 Train loss 2.332321 on epoch=69
06/24/2022 13:30:18 - INFO - __main__ - Step 150 Global step 150 Train loss 1.902946 on epoch=74
06/24/2022 13:30:19 - INFO - __main__ - Global step 150 Train loss 2.082897 ACC 0.5 on epoch=74
06/24/2022 13:30:21 - INFO - __main__ - Step 160 Global step 160 Train loss 2.287931 on epoch=79
06/24/2022 13:30:24 - INFO - __main__ - Step 170 Global step 170 Train loss 1.345162 on epoch=84
06/24/2022 13:30:27 - INFO - __main__ - Step 180 Global step 180 Train loss 1.040812 on epoch=89
06/24/2022 13:30:29 - INFO - __main__ - Step 190 Global step 190 Train loss 1.501074 on epoch=94
06/24/2022 13:30:32 - INFO - __main__ - Step 200 Global step 200 Train loss 1.108976 on epoch=99
06/24/2022 13:30:32 - INFO - __main__ - Global step 200 Train loss 1.456791 ACC 0.5 on epoch=99
06/24/2022 13:30:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.880691 on epoch=104
06/24/2022 13:30:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.770534 on epoch=109
06/24/2022 13:30:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.568568 on epoch=114
06/24/2022 13:30:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.926232 on epoch=119
06/24/2022 13:30:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.698112 on epoch=124
06/24/2022 13:30:45 - INFO - __main__ - Global step 250 Train loss 0.768827 ACC 0.5 on epoch=124
06/24/2022 13:30:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.600248 on epoch=129
06/24/2022 13:30:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.621124 on epoch=134
06/24/2022 13:30:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.415622 on epoch=139
06/24/2022 13:30:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.579641 on epoch=144
06/24/2022 13:30:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.633485 on epoch=149
06/24/2022 13:30:57 - INFO - __main__ - Global step 300 Train loss 0.570024 ACC 0.5 on epoch=149
06/24/2022 13:31:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.626800 on epoch=154
06/24/2022 13:31:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.510653 on epoch=159
06/24/2022 13:31:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.645536 on epoch=164
06/24/2022 13:31:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.547301 on epoch=169
06/24/2022 13:31:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.450714 on epoch=174
06/24/2022 13:31:10 - INFO - __main__ - Global step 350 Train loss 0.556201 ACC 0.625 on epoch=174
06/24/2022 13:31:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.515506 on epoch=179
06/24/2022 13:31:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.527322 on epoch=184
06/24/2022 13:31:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.351386 on epoch=189
06/24/2022 13:31:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.491950 on epoch=194
06/24/2022 13:31:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.373037 on epoch=199
06/24/2022 13:31:23 - INFO - __main__ - Global step 400 Train loss 0.451840 ACC 0.6875 on epoch=199
06/24/2022 13:31:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.306817 on epoch=204
06/24/2022 13:31:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.456990 on epoch=209
06/24/2022 13:31:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.494563 on epoch=214
06/24/2022 13:31:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.321185 on epoch=219
06/24/2022 13:31:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.330766 on epoch=224
06/24/2022 13:31:36 - INFO - __main__ - Global step 450 Train loss 0.382064 ACC 0.6875 on epoch=224
06/24/2022 13:31:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.354771 on epoch=229
06/24/2022 13:31:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.325210 on epoch=234
06/24/2022 13:31:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.370488 on epoch=239
06/24/2022 13:31:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.349464 on epoch=244
06/24/2022 13:31:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.457603 on epoch=249
06/24/2022 13:31:49 - INFO - __main__ - Global step 500 Train loss 0.371507 ACC 0.6875 on epoch=249
06/24/2022 13:31:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.241362 on epoch=254
06/24/2022 13:31:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.236380 on epoch=259
06/24/2022 13:31:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.268285 on epoch=264
06/24/2022 13:31:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.288117 on epoch=269
06/24/2022 13:32:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.289544 on epoch=274
06/24/2022 13:32:02 - INFO - __main__ - Global step 550 Train loss 0.264738 ACC 0.6875 on epoch=274
06/24/2022 13:32:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.387223 on epoch=279
06/24/2022 13:32:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.241139 on epoch=284
06/24/2022 13:32:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.201352 on epoch=289
06/24/2022 13:32:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.278025 on epoch=294
06/24/2022 13:32:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.208798 on epoch=299
06/24/2022 13:32:15 - INFO - __main__ - Global step 600 Train loss 0.263307 ACC 0.625 on epoch=299
06/24/2022 13:32:15 - INFO - __main__ - save last model!
06/24/2022 13:32:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:32:15 - INFO - __main__ - Printing 3 examples
06/24/2022 13:32:15 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 13:32:15 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:15 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 13:32:15 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 13:32:15 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:32:15 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:32:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:32:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:32:15 - INFO - __main__ - Printing 3 examples
06/24/2022 13:32:15 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 13:32:15 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:15 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 13:32:15 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 13:32:15 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:15 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:32:15 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:32:15 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:32:17 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:32:18 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:32:18 - INFO - __main__ - Printing 3 examples
06/24/2022 13:32:18 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:32:18 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:18 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:32:18 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:32:18 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:32:18 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:32:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:32:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:32:18 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:32:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:32:20 - INFO - __main__ - Starting training!
06/24/2022 13:32:23 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_21_0.0002_8_predictions.txt
06/24/2022 13:32:23 - INFO - __main__ - ACC on test data: 0.5441
06/24/2022 13:32:23 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.0002, bsz=8, dev_performance=0.6875, test_performance=0.5441176470588235
06/24/2022 13:32:23 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.0001, bsz=8 ...
06/24/2022 13:32:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:32:24 - INFO - __main__ - Printing 3 examples
06/24/2022 13:32:24 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/24/2022 13:32:24 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:24 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/24/2022 13:32:24 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:24 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/24/2022 13:32:24 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:32:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:32:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:32:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:32:24 - INFO - __main__ - Printing 3 examples
06/24/2022 13:32:24 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/24/2022 13:32:24 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:24 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/24/2022 13:32:24 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:24 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/24/2022 13:32:24 - INFO - __main__ - ['equivalent']
06/24/2022 13:32:24 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:32:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:32:25 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:32:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:32:29 - INFO - __main__ - Starting training!
06/24/2022 13:32:31 - INFO - __main__ - Step 10 Global step 10 Train loss 16.106421 on epoch=4
06/24/2022 13:32:33 - INFO - __main__ - Step 20 Global step 20 Train loss 14.239408 on epoch=9
06/24/2022 13:32:36 - INFO - __main__ - Step 30 Global step 30 Train loss 11.241466 on epoch=14
06/24/2022 13:32:38 - INFO - __main__ - Step 40 Global step 40 Train loss 9.309595 on epoch=19
06/24/2022 13:32:40 - INFO - __main__ - Step 50 Global step 50 Train loss 8.900849 on epoch=24
06/24/2022 13:32:43 - INFO - __main__ - Global step 50 Train loss 11.959547 ACC 0.0 on epoch=24
06/24/2022 13:32:46 - INFO - __main__ - Step 60 Global step 60 Train loss 7.647282 on epoch=29
06/24/2022 13:32:49 - INFO - __main__ - Step 70 Global step 70 Train loss 7.437879 on epoch=34
06/24/2022 13:32:51 - INFO - __main__ - Step 80 Global step 80 Train loss 6.066802 on epoch=39
06/24/2022 13:32:54 - INFO - __main__ - Step 90 Global step 90 Train loss 5.912650 on epoch=44
06/24/2022 13:32:56 - INFO - __main__ - Step 100 Global step 100 Train loss 5.698222 on epoch=49
06/24/2022 13:32:57 - INFO - __main__ - Global step 100 Train loss 6.552567 ACC 0.0 on epoch=49
06/24/2022 13:33:00 - INFO - __main__ - Step 110 Global step 110 Train loss 5.482762 on epoch=54
06/24/2022 13:33:02 - INFO - __main__ - Step 120 Global step 120 Train loss 4.707146 on epoch=59
06/24/2022 13:33:05 - INFO - __main__ - Step 130 Global step 130 Train loss 3.877069 on epoch=64
06/24/2022 13:33:07 - INFO - __main__ - Step 140 Global step 140 Train loss 4.357677 on epoch=69
06/24/2022 13:33:10 - INFO - __main__ - Step 150 Global step 150 Train loss 3.808189 on epoch=74
06/24/2022 13:33:10 - INFO - __main__ - Global step 150 Train loss 4.446569 ACC 0.03125 on epoch=74
06/24/2022 13:33:13 - INFO - __main__ - Step 160 Global step 160 Train loss 3.394341 on epoch=79
06/24/2022 13:33:16 - INFO - __main__ - Step 170 Global step 170 Train loss 3.816647 on epoch=84
06/24/2022 13:33:18 - INFO - __main__ - Step 180 Global step 180 Train loss 3.672714 on epoch=89
06/24/2022 13:33:21 - INFO - __main__ - Step 190 Global step 190 Train loss 2.907238 on epoch=94
06/24/2022 13:33:23 - INFO - __main__ - Step 200 Global step 200 Train loss 2.618507 on epoch=99
06/24/2022 13:33:24 - INFO - __main__ - Global step 200 Train loss 3.281889 ACC 0.5 on epoch=99
06/24/2022 13:33:27 - INFO - __main__ - Step 210 Global step 210 Train loss 2.410140 on epoch=104
06/24/2022 13:33:29 - INFO - __main__ - Step 220 Global step 220 Train loss 2.666181 on epoch=109
06/24/2022 13:33:32 - INFO - __main__ - Step 230 Global step 230 Train loss 2.136355 on epoch=114
06/24/2022 13:33:34 - INFO - __main__ - Step 240 Global step 240 Train loss 1.710012 on epoch=119
06/24/2022 13:33:37 - INFO - __main__ - Step 250 Global step 250 Train loss 2.042216 on epoch=124
06/24/2022 13:33:37 - INFO - __main__ - Global step 250 Train loss 2.192981 ACC 0.5 on epoch=124
06/24/2022 13:33:39 - INFO - __main__ - Step 260 Global step 260 Train loss 1.504922 on epoch=129
06/24/2022 13:33:42 - INFO - __main__ - Step 270 Global step 270 Train loss 2.203824 on epoch=134
06/24/2022 13:33:44 - INFO - __main__ - Step 280 Global step 280 Train loss 1.352420 on epoch=139
06/24/2022 13:33:47 - INFO - __main__ - Step 290 Global step 290 Train loss 1.682593 on epoch=144
06/24/2022 13:33:49 - INFO - __main__ - Step 300 Global step 300 Train loss 2.010936 on epoch=149
06/24/2022 13:33:50 - INFO - __main__ - Global step 300 Train loss 1.750939 ACC 0.5 on epoch=149
06/24/2022 13:33:52 - INFO - __main__ - Step 310 Global step 310 Train loss 1.744583 on epoch=154
06/24/2022 13:33:55 - INFO - __main__ - Step 320 Global step 320 Train loss 1.450496 on epoch=159
06/24/2022 13:33:57 - INFO - __main__ - Step 330 Global step 330 Train loss 1.061571 on epoch=164
06/24/2022 13:34:00 - INFO - __main__ - Step 340 Global step 340 Train loss 1.707029 on epoch=169
06/24/2022 13:34:02 - INFO - __main__ - Step 350 Global step 350 Train loss 1.627110 on epoch=174
06/24/2022 13:34:03 - INFO - __main__ - Global step 350 Train loss 1.518158 ACC 0.5 on epoch=174
06/24/2022 13:34:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.987504 on epoch=179
06/24/2022 13:34:08 - INFO - __main__ - Step 370 Global step 370 Train loss 1.203983 on epoch=184
06/24/2022 13:34:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.694109 on epoch=189
06/24/2022 13:34:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.879059 on epoch=194
06/24/2022 13:34:15 - INFO - __main__ - Step 400 Global step 400 Train loss 1.074278 on epoch=199
06/24/2022 13:34:16 - INFO - __main__ - Global step 400 Train loss 0.967786 ACC 0.625 on epoch=199
06/24/2022 13:34:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.631307 on epoch=204
06/24/2022 13:34:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.936733 on epoch=209
06/24/2022 13:34:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.873947 on epoch=214
06/24/2022 13:34:26 - INFO - __main__ - Step 440 Global step 440 Train loss 1.051990 on epoch=219
06/24/2022 13:34:29 - INFO - __main__ - Step 450 Global step 450 Train loss 1.144198 on epoch=224
06/24/2022 13:34:29 - INFO - __main__ - Global step 450 Train loss 0.927635 ACC 0.5 on epoch=224
06/24/2022 13:34:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.611449 on epoch=229
06/24/2022 13:34:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.844063 on epoch=234
06/24/2022 13:34:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.669733 on epoch=239
06/24/2022 13:34:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.699862 on epoch=244
06/24/2022 13:34:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.573456 on epoch=249
06/24/2022 13:34:42 - INFO - __main__ - Global step 500 Train loss 0.679713 ACC 0.65625 on epoch=249
06/24/2022 13:34:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.818210 on epoch=254
06/24/2022 13:34:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.652266 on epoch=259
06/24/2022 13:34:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.487865 on epoch=264
06/24/2022 13:34:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.532714 on epoch=269
06/24/2022 13:34:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.728492 on epoch=274
06/24/2022 13:34:55 - INFO - __main__ - Global step 550 Train loss 0.643910 ACC 0.53125 on epoch=274
06/24/2022 13:34:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.558461 on epoch=279
06/24/2022 13:35:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.501436 on epoch=284
06/24/2022 13:35:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.660338 on epoch=289
06/24/2022 13:35:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.629107 on epoch=294
06/24/2022 13:35:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.500841 on epoch=299
06/24/2022 13:35:08 - INFO - __main__ - Global step 600 Train loss 0.570037 ACC 0.53125 on epoch=299
06/24/2022 13:35:08 - INFO - __main__ - save last model!
06/24/2022 13:35:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:35:09 - INFO - __main__ - Printing 3 examples
06/24/2022 13:35:09 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 13:35:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:09 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 13:35:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:09 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 13:35:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:35:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:35:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:35:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:35:09 - INFO - __main__ - Printing 3 examples
06/24/2022 13:35:09 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 13:35:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:09 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 13:35:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:09 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 13:35:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:09 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:35:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:35:09 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:35:11 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:35:11 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:35:11 - INFO - __main__ - Printing 3 examples
06/24/2022 13:35:11 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:35:11 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:11 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:35:11 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:35:11 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:35:11 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:35:11 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:35:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:35:12 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:35:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:35:13 - INFO - __main__ - Starting training!
06/24/2022 13:35:18 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_21_0.0001_8_predictions.txt
06/24/2022 13:35:18 - INFO - __main__ - ACC on test data: 0.3750
06/24/2022 13:35:18 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.0001, bsz=8, dev_performance=0.65625, test_performance=0.375
06/24/2022 13:35:18 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.0005, bsz=8 ...
06/24/2022 13:35:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:35:19 - INFO - __main__ - Printing 3 examples
06/24/2022 13:35:19 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 13:35:19 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:19 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 13:35:19 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:19 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 13:35:19 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:35:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:35:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:35:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:35:19 - INFO - __main__ - Printing 3 examples
06/24/2022 13:35:19 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 13:35:19 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:19 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 13:35:19 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:19 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 13:35:19 - INFO - __main__ - ['equivalent']
06/24/2022 13:35:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:35:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:35:19 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:35:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:35:23 - INFO - __main__ - Starting training!
06/24/2022 13:35:25 - INFO - __main__ - Step 10 Global step 10 Train loss 16.234646 on epoch=4
06/24/2022 13:35:27 - INFO - __main__ - Step 20 Global step 20 Train loss 12.204785 on epoch=9
06/24/2022 13:35:29 - INFO - __main__ - Step 30 Global step 30 Train loss 8.561868 on epoch=14
06/24/2022 13:35:32 - INFO - __main__ - Step 40 Global step 40 Train loss 5.923984 on epoch=19
06/24/2022 13:35:34 - INFO - __main__ - Step 50 Global step 50 Train loss 4.564265 on epoch=24
06/24/2022 13:35:35 - INFO - __main__ - Global step 50 Train loss 9.497909 ACC 0.0625 on epoch=24
06/24/2022 13:35:37 - INFO - __main__ - Step 60 Global step 60 Train loss 3.316148 on epoch=29
06/24/2022 13:35:40 - INFO - __main__ - Step 70 Global step 70 Train loss 2.009087 on epoch=34
06/24/2022 13:35:42 - INFO - __main__ - Step 80 Global step 80 Train loss 1.695267 on epoch=39
06/24/2022 13:35:45 - INFO - __main__ - Step 90 Global step 90 Train loss 1.362106 on epoch=44
06/24/2022 13:35:47 - INFO - __main__ - Step 100 Global step 100 Train loss 1.525232 on epoch=49
06/24/2022 13:35:47 - INFO - __main__ - Global step 100 Train loss 1.981568 ACC 0.5 on epoch=49
06/24/2022 13:35:50 - INFO - __main__ - Step 110 Global step 110 Train loss 1.451583 on epoch=54
06/24/2022 13:35:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.931936 on epoch=59
06/24/2022 13:35:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.810828 on epoch=64
06/24/2022 13:35:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.585294 on epoch=69
06/24/2022 13:36:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.522471 on epoch=74
06/24/2022 13:36:00 - INFO - __main__ - Global step 150 Train loss 0.860422 ACC 0.5 on epoch=74
06/24/2022 13:36:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.631875 on epoch=79
06/24/2022 13:36:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.452117 on epoch=84
06/24/2022 13:36:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.548881 on epoch=89
06/24/2022 13:36:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.521384 on epoch=94
06/24/2022 13:36:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.459353 on epoch=99
06/24/2022 13:36:13 - INFO - __main__ - Global step 200 Train loss 0.522722 ACC 0.59375 on epoch=99
06/24/2022 13:36:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.471909 on epoch=104
06/24/2022 13:36:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.510411 on epoch=109
06/24/2022 13:36:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.568186 on epoch=114
06/24/2022 13:36:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.411903 on epoch=119
06/24/2022 13:36:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.423749 on epoch=124
06/24/2022 13:36:26 - INFO - __main__ - Global step 250 Train loss 0.477232 ACC 0.53125 on epoch=124
06/24/2022 13:36:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.402667 on epoch=129
06/24/2022 13:36:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.363932 on epoch=134
06/24/2022 13:36:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.473684 on epoch=139
06/24/2022 13:36:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.371763 on epoch=144
06/24/2022 13:36:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.472603 on epoch=149
06/24/2022 13:36:38 - INFO - __main__ - Global step 300 Train loss 0.416930 ACC 0.65625 on epoch=149
06/24/2022 13:36:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.260708 on epoch=154
06/24/2022 13:36:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.441304 on epoch=159
06/24/2022 13:36:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.360497 on epoch=164
06/24/2022 13:36:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.327656 on epoch=169
06/24/2022 13:36:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.247908 on epoch=174
06/24/2022 13:36:51 - INFO - __main__ - Global step 350 Train loss 0.327615 ACC 0.78125 on epoch=174
06/24/2022 13:36:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.218949 on epoch=179
06/24/2022 13:36:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.221599 on epoch=184
06/24/2022 13:36:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.233334 on epoch=189
06/24/2022 13:37:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.198285 on epoch=194
06/24/2022 13:37:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.102478 on epoch=199
06/24/2022 13:37:04 - INFO - __main__ - Global step 400 Train loss 0.194929 ACC 0.75 on epoch=199
06/24/2022 13:37:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.094985 on epoch=204
06/24/2022 13:37:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.068897 on epoch=209
06/24/2022 13:37:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.062553 on epoch=214
06/24/2022 13:37:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.055410 on epoch=219
06/24/2022 13:37:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.075229 on epoch=224
06/24/2022 13:37:16 - INFO - __main__ - Global step 450 Train loss 0.071415 ACC 0.6875 on epoch=224
06/24/2022 13:37:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.043315 on epoch=229
06/24/2022 13:37:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.098777 on epoch=234
06/24/2022 13:37:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.037874 on epoch=239
06/24/2022 13:37:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.056978 on epoch=244
06/24/2022 13:37:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.063091 on epoch=249
06/24/2022 13:37:28 - INFO - __main__ - Global step 500 Train loss 0.060007 ACC 0.5625 on epoch=249
06/24/2022 13:37:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.073302 on epoch=254
06/24/2022 13:37:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.141443 on epoch=259
06/24/2022 13:37:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.396551 on epoch=264
06/24/2022 13:37:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.185244 on epoch=269
06/24/2022 13:37:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.222947 on epoch=274
06/24/2022 13:37:41 - INFO - __main__ - Global step 550 Train loss 0.203897 ACC 0.71875 on epoch=274
06/24/2022 13:37:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.140913 on epoch=279
06/24/2022 13:37:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.140943 on epoch=284
06/24/2022 13:37:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.084142 on epoch=289
06/24/2022 13:37:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.068847 on epoch=294
06/24/2022 13:37:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.045450 on epoch=299
06/24/2022 13:37:53 - INFO - __main__ - Global step 600 Train loss 0.096059 ACC 0.6875 on epoch=299
06/24/2022 13:37:53 - INFO - __main__ - save last model!
06/24/2022 13:37:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:37:54 - INFO - __main__ - Printing 3 examples
06/24/2022 13:37:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 13:37:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:37:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 13:37:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:37:54 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 13:37:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:37:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:37:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:37:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:37:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:37:54 - INFO - __main__ - Printing 3 examples
06/24/2022 13:37:54 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 13:37:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:37:54 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 13:37:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:37:54 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 13:37:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:37:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:37:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:37:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:37:56 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:37:56 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:37:56 - INFO - __main__ - Printing 3 examples
06/24/2022 13:37:56 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:37:56 - INFO - __main__ - ['equivalent']
06/24/2022 13:37:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:37:56 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:37:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:37:56 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:37:56 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:37:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:37:57 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:37:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:37:58 - INFO - __main__ - Starting training!
06/24/2022 13:38:02 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_42_0.0005_8_predictions.txt
06/24/2022 13:38:02 - INFO - __main__ - ACC on test data: 0.6422
06/24/2022 13:38:02 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.0005, bsz=8, dev_performance=0.78125, test_performance=0.6421568627450981
06/24/2022 13:38:02 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.0003, bsz=8 ...
06/24/2022 13:38:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:38:03 - INFO - __main__ - Printing 3 examples
06/24/2022 13:38:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 13:38:03 - INFO - __main__ - ['equivalent']
06/24/2022 13:38:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 13:38:03 - INFO - __main__ - ['equivalent']
06/24/2022 13:38:03 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 13:38:03 - INFO - __main__ - ['equivalent']
06/24/2022 13:38:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:38:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:38:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:38:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:38:03 - INFO - __main__ - Printing 3 examples
06/24/2022 13:38:03 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 13:38:03 - INFO - __main__ - ['equivalent']
06/24/2022 13:38:03 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 13:38:03 - INFO - __main__ - ['equivalent']
06/24/2022 13:38:03 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 13:38:03 - INFO - __main__ - ['equivalent']
06/24/2022 13:38:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:38:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:38:03 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:38:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:38:07 - INFO - __main__ - Starting training!
06/24/2022 13:38:09 - INFO - __main__ - Step 10 Global step 10 Train loss 16.218563 on epoch=4
06/24/2022 13:38:11 - INFO - __main__ - Step 20 Global step 20 Train loss 13.267202 on epoch=9
06/24/2022 13:38:14 - INFO - __main__ - Step 30 Global step 30 Train loss 8.345491 on epoch=14
06/24/2022 13:38:16 - INFO - __main__ - Step 40 Global step 40 Train loss 6.409716 on epoch=19
06/24/2022 13:38:19 - INFO - __main__ - Step 50 Global step 50 Train loss 4.520921 on epoch=24
06/24/2022 13:38:19 - INFO - __main__ - Global step 50 Train loss 9.752378 ACC 0.28125 on epoch=24
06/24/2022 13:38:22 - INFO - __main__ - Step 60 Global step 60 Train loss 3.776409 on epoch=29
06/24/2022 13:38:24 - INFO - __main__ - Step 70 Global step 70 Train loss 2.509772 on epoch=34
06/24/2022 13:38:27 - INFO - __main__ - Step 80 Global step 80 Train loss 1.763500 on epoch=39
06/24/2022 13:38:29 - INFO - __main__ - Step 90 Global step 90 Train loss 2.116827 on epoch=44
06/24/2022 13:38:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.989752 on epoch=49
06/24/2022 13:38:32 - INFO - __main__ - Global step 100 Train loss 2.231252 ACC 0.46875 on epoch=49
06/24/2022 13:38:35 - INFO - __main__ - Step 110 Global step 110 Train loss 1.160567 on epoch=54
06/24/2022 13:38:38 - INFO - __main__ - Step 120 Global step 120 Train loss 1.139190 on epoch=59
06/24/2022 13:38:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.805499 on epoch=64
06/24/2022 13:38:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.728041 on epoch=69
06/24/2022 13:38:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.611842 on epoch=74
06/24/2022 13:38:46 - INFO - __main__ - Global step 150 Train loss 0.889028 ACC 0.5 on epoch=74
06/24/2022 13:38:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.479829 on epoch=79
06/24/2022 13:38:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.550220 on epoch=84
06/24/2022 13:38:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.609957 on epoch=89
06/24/2022 13:38:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.712518 on epoch=94
06/24/2022 13:38:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.541396 on epoch=99
06/24/2022 13:38:59 - INFO - __main__ - Global step 200 Train loss 0.578784 ACC 0.5 on epoch=99
06/24/2022 13:39:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.489155 on epoch=104
06/24/2022 13:39:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.754072 on epoch=109
06/24/2022 13:39:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.529747 on epoch=114
06/24/2022 13:39:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.464694 on epoch=119
06/24/2022 13:39:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.602012 on epoch=124
06/24/2022 13:39:12 - INFO - __main__ - Global step 250 Train loss 0.567936 ACC 0.6875 on epoch=124
06/24/2022 13:39:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.355923 on epoch=129
06/24/2022 13:39:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.465133 on epoch=134
06/24/2022 13:39:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.422556 on epoch=139
06/24/2022 13:39:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.303574 on epoch=144
06/24/2022 13:39:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.276864 on epoch=149
06/24/2022 13:39:25 - INFO - __main__ - Global step 300 Train loss 0.364810 ACC 0.6875 on epoch=149
06/24/2022 13:39:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.280093 on epoch=154
06/24/2022 13:39:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.460006 on epoch=159
06/24/2022 13:39:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.366592 on epoch=164
06/24/2022 13:39:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.214396 on epoch=169
06/24/2022 13:39:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.186181 on epoch=174
06/24/2022 13:39:38 - INFO - __main__ - Global step 350 Train loss 0.301453 ACC 0.78125 on epoch=174
06/24/2022 13:39:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.186970 on epoch=179
06/24/2022 13:39:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.125374 on epoch=184
06/24/2022 13:39:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.100010 on epoch=189
06/24/2022 13:39:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.078246 on epoch=194
06/24/2022 13:39:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.029988 on epoch=199
06/24/2022 13:39:52 - INFO - __main__ - Global step 400 Train loss 0.104118 ACC 0.59375 on epoch=199
06/24/2022 13:39:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.262017 on epoch=204
06/24/2022 13:39:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.187382 on epoch=209
06/24/2022 13:39:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.051036 on epoch=214
06/24/2022 13:40:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.073568 on epoch=219
06/24/2022 13:40:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.049137 on epoch=224
06/24/2022 13:40:05 - INFO - __main__ - Global step 450 Train loss 0.124628 ACC 0.75 on epoch=224
06/24/2022 13:40:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.126053 on epoch=229
06/24/2022 13:40:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.045949 on epoch=234
06/24/2022 13:40:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.031431 on epoch=239
06/24/2022 13:40:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.013905 on epoch=244
06/24/2022 13:40:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.035435 on epoch=249
06/24/2022 13:40:18 - INFO - __main__ - Global step 500 Train loss 0.050554 ACC 0.71875 on epoch=249
06/24/2022 13:40:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.032796 on epoch=254
06/24/2022 13:40:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.006448 on epoch=259
06/24/2022 13:40:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.045551 on epoch=264
06/24/2022 13:40:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.025470 on epoch=269
06/24/2022 13:40:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.035779 on epoch=274
06/24/2022 13:40:31 - INFO - __main__ - Global step 550 Train loss 0.029209 ACC 0.75 on epoch=274
06/24/2022 13:40:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.059188 on epoch=279
06/24/2022 13:40:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.012595 on epoch=284
06/24/2022 13:40:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.004981 on epoch=289
06/24/2022 13:40:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.008067 on epoch=294
06/24/2022 13:40:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.013451 on epoch=299
06/24/2022 13:40:44 - INFO - __main__ - Global step 600 Train loss 0.019657 ACC 0.75 on epoch=299
06/24/2022 13:40:44 - INFO - __main__ - save last model!
06/24/2022 13:40:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:40:44 - INFO - __main__ - Printing 3 examples
06/24/2022 13:40:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 13:40:44 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 13:40:44 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:44 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 13:40:44 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:40:44 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:40:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:40:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:40:45 - INFO - __main__ - Printing 3 examples
06/24/2022 13:40:45 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 13:40:45 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:45 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 13:40:45 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:45 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 13:40:45 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:45 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:40:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:40:45 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:40:46 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:40:47 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:40:47 - INFO - __main__ - Printing 3 examples
06/24/2022 13:40:47 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:40:47 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:47 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:40:47 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:40:47 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:40:47 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:40:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:40:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:40:47 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:40:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:40:48 - INFO - __main__ - Starting training!
06/24/2022 13:40:52 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_42_0.0003_8_predictions.txt
06/24/2022 13:40:52 - INFO - __main__ - ACC on test data: 0.6078
06/24/2022 13:40:53 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.0003, bsz=8, dev_performance=0.78125, test_performance=0.6078431372549019
06/24/2022 13:40:53 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.0002, bsz=8 ...
06/24/2022 13:40:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:40:54 - INFO - __main__ - Printing 3 examples
06/24/2022 13:40:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 13:40:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:54 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 13:40:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:54 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 13:40:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:40:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:40:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:40:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:40:54 - INFO - __main__ - Printing 3 examples
06/24/2022 13:40:54 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 13:40:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:54 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 13:40:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:54 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 13:40:54 - INFO - __main__ - ['equivalent']
06/24/2022 13:40:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:40:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:40:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:40:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:40:58 - INFO - __main__ - Starting training!
06/24/2022 13:41:00 - INFO - __main__ - Step 10 Global step 10 Train loss 16.042181 on epoch=4
06/24/2022 13:41:02 - INFO - __main__ - Step 20 Global step 20 Train loss 13.285515 on epoch=9
06/24/2022 13:41:04 - INFO - __main__ - Step 30 Global step 30 Train loss 10.708364 on epoch=14
06/24/2022 13:41:07 - INFO - __main__ - Step 40 Global step 40 Train loss 8.797954 on epoch=19
06/24/2022 13:41:09 - INFO - __main__ - Step 50 Global step 50 Train loss 6.728058 on epoch=24
06/24/2022 13:41:11 - INFO - __main__ - Global step 50 Train loss 11.112414 ACC 0.0 on epoch=24
06/24/2022 13:41:13 - INFO - __main__ - Step 60 Global step 60 Train loss 6.375790 on epoch=29
06/24/2022 13:41:16 - INFO - __main__ - Step 70 Global step 70 Train loss 5.311072 on epoch=34
06/24/2022 13:41:18 - INFO - __main__ - Step 80 Global step 80 Train loss 5.023088 on epoch=39
06/24/2022 13:41:21 - INFO - __main__ - Step 90 Global step 90 Train loss 4.306609 on epoch=44
06/24/2022 13:41:23 - INFO - __main__ - Step 100 Global step 100 Train loss 3.936186 on epoch=49
06/24/2022 13:41:24 - INFO - __main__ - Global step 100 Train loss 4.990549 ACC 0.15625 on epoch=49
06/24/2022 13:41:27 - INFO - __main__ - Step 110 Global step 110 Train loss 3.446099 on epoch=54
06/24/2022 13:41:29 - INFO - __main__ - Step 120 Global step 120 Train loss 3.120851 on epoch=59
06/24/2022 13:41:32 - INFO - __main__ - Step 130 Global step 130 Train loss 2.488034 on epoch=64
06/24/2022 13:41:34 - INFO - __main__ - Step 140 Global step 140 Train loss 2.063346 on epoch=69
06/24/2022 13:41:37 - INFO - __main__ - Step 150 Global step 150 Train loss 2.081718 on epoch=74
06/24/2022 13:41:37 - INFO - __main__ - Global step 150 Train loss 2.640010 ACC 0.5 on epoch=74
06/24/2022 13:41:40 - INFO - __main__ - Step 160 Global step 160 Train loss 1.298694 on epoch=79
06/24/2022 13:41:43 - INFO - __main__ - Step 170 Global step 170 Train loss 1.541119 on epoch=84
06/24/2022 13:41:45 - INFO - __main__ - Step 180 Global step 180 Train loss 1.343921 on epoch=89
06/24/2022 13:41:48 - INFO - __main__ - Step 190 Global step 190 Train loss 1.512270 on epoch=94
06/24/2022 13:41:50 - INFO - __main__ - Step 200 Global step 200 Train loss 1.306502 on epoch=99
06/24/2022 13:41:50 - INFO - __main__ - Global step 200 Train loss 1.400501 ACC 0.5 on epoch=99
06/24/2022 13:41:53 - INFO - __main__ - Step 210 Global step 210 Train loss 1.047872 on epoch=104
06/24/2022 13:41:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.868897 on epoch=109
06/24/2022 13:41:58 - INFO - __main__ - Step 230 Global step 230 Train loss 1.461754 on epoch=114
06/24/2022 13:42:00 - INFO - __main__ - Step 240 Global step 240 Train loss 1.012024 on epoch=119
06/24/2022 13:42:03 - INFO - __main__ - Step 250 Global step 250 Train loss 1.000284 on epoch=124
06/24/2022 13:42:03 - INFO - __main__ - Global step 250 Train loss 1.078166 ACC 0.5 on epoch=124
06/24/2022 13:42:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.787098 on epoch=129
06/24/2022 13:42:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.827283 on epoch=134
06/24/2022 13:42:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.742814 on epoch=139
06/24/2022 13:42:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.661855 on epoch=144
06/24/2022 13:42:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.578401 on epoch=149
06/24/2022 13:42:16 - INFO - __main__ - Global step 300 Train loss 0.719490 ACC 0.5 on epoch=149
06/24/2022 13:42:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.543317 on epoch=154
06/24/2022 13:42:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.661214 on epoch=159
06/24/2022 13:42:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.709982 on epoch=164
06/24/2022 13:42:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.571265 on epoch=169
06/24/2022 13:42:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.411617 on epoch=174
06/24/2022 13:42:30 - INFO - __main__ - Global step 350 Train loss 0.579479 ACC 0.65625 on epoch=174
06/24/2022 13:42:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.553015 on epoch=179
06/24/2022 13:42:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.424075 on epoch=184
06/24/2022 13:42:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.412140 on epoch=189
06/24/2022 13:42:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.468264 on epoch=194
06/24/2022 13:42:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.492457 on epoch=199
06/24/2022 13:42:43 - INFO - __main__ - Global step 400 Train loss 0.469990 ACC 0.5 on epoch=199
06/24/2022 13:42:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.510298 on epoch=204
06/24/2022 13:42:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.454581 on epoch=209
06/24/2022 13:42:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.478903 on epoch=214
06/24/2022 13:42:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.518880 on epoch=219
06/24/2022 13:42:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.528003 on epoch=224
06/24/2022 13:42:56 - INFO - __main__ - Global step 450 Train loss 0.498133 ACC 0.6875 on epoch=224
06/24/2022 13:42:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.365121 on epoch=229
06/24/2022 13:43:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.504984 on epoch=234
06/24/2022 13:43:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.412041 on epoch=239
06/24/2022 13:43:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.319648 on epoch=244
06/24/2022 13:43:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.428809 on epoch=249
06/24/2022 13:43:09 - INFO - __main__ - Global step 500 Train loss 0.406120 ACC 0.53125 on epoch=249
06/24/2022 13:43:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.551253 on epoch=254
06/24/2022 13:43:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.422643 on epoch=259
06/24/2022 13:43:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.295352 on epoch=264
06/24/2022 13:43:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.431387 on epoch=269
06/24/2022 13:43:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.355336 on epoch=274
06/24/2022 13:43:22 - INFO - __main__ - Global step 550 Train loss 0.411194 ACC 0.5 on epoch=274
06/24/2022 13:43:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.444188 on epoch=279
06/24/2022 13:43:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.337588 on epoch=284
06/24/2022 13:43:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.340361 on epoch=289
06/24/2022 13:43:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.320985 on epoch=294
06/24/2022 13:43:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.270162 on epoch=299
06/24/2022 13:43:34 - INFO - __main__ - Global step 600 Train loss 0.342657 ACC 0.5625 on epoch=299
06/24/2022 13:43:34 - INFO - __main__ - save last model!
06/24/2022 13:43:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:43:35 - INFO - __main__ - Printing 3 examples
06/24/2022 13:43:35 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 13:43:35 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:35 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 13:43:35 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:35 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 13:43:35 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:43:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:43:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:43:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:43:35 - INFO - __main__ - Printing 3 examples
06/24/2022 13:43:35 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 13:43:35 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:35 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 13:43:35 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:35 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 13:43:35 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:35 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:43:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:43:35 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:43:37 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:43:37 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:43:37 - INFO - __main__ - Printing 3 examples
06/24/2022 13:43:37 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:43:37 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:37 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:43:37 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:43:37 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:43:37 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:43:37 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:43:37 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:43:38 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:43:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:43:39 - INFO - __main__ - Starting training!
06/24/2022 13:43:43 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_42_0.0002_8_predictions.txt
06/24/2022 13:43:43 - INFO - __main__ - ACC on test data: 0.4975
06/24/2022 13:43:43 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.0002, bsz=8, dev_performance=0.6875, test_performance=0.49754901960784315
06/24/2022 13:43:43 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.0001, bsz=8 ...
06/24/2022 13:43:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:43:44 - INFO - __main__ - Printing 3 examples
06/24/2022 13:43:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/24/2022 13:43:44 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/24/2022 13:43:44 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:44 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/24/2022 13:43:44 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:43:44 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:43:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:43:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:43:44 - INFO - __main__ - Printing 3 examples
06/24/2022 13:43:44 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/24/2022 13:43:44 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:44 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/24/2022 13:43:44 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:44 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/24/2022 13:43:44 - INFO - __main__ - ['equivalent']
06/24/2022 13:43:44 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:43:44 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:43:44 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:43:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:43:49 - INFO - __main__ - Starting training!
06/24/2022 13:43:51 - INFO - __main__ - Step 10 Global step 10 Train loss 16.470011 on epoch=4
06/24/2022 13:43:53 - INFO - __main__ - Step 20 Global step 20 Train loss 15.533587 on epoch=9
06/24/2022 13:43:55 - INFO - __main__ - Step 30 Global step 30 Train loss 12.115275 on epoch=14
06/24/2022 13:43:58 - INFO - __main__ - Step 40 Global step 40 Train loss 9.297746 on epoch=19
06/24/2022 13:44:00 - INFO - __main__ - Step 50 Global step 50 Train loss 8.531717 on epoch=24
06/24/2022 13:44:03 - INFO - __main__ - Global step 50 Train loss 12.389666 ACC 0.0 on epoch=24
06/24/2022 13:44:06 - INFO - __main__ - Step 60 Global step 60 Train loss 7.428632 on epoch=29
06/24/2022 13:44:08 - INFO - __main__ - Step 70 Global step 70 Train loss 6.553570 on epoch=34
06/24/2022 13:44:10 - INFO - __main__ - Step 80 Global step 80 Train loss 5.788070 on epoch=39
06/24/2022 13:44:13 - INFO - __main__ - Step 90 Global step 90 Train loss 5.672934 on epoch=44
06/24/2022 13:44:15 - INFO - __main__ - Step 100 Global step 100 Train loss 5.020940 on epoch=49
06/24/2022 13:44:16 - INFO - __main__ - Global step 100 Train loss 6.092829 ACC 0.0 on epoch=49
06/24/2022 13:44:19 - INFO - __main__ - Step 110 Global step 110 Train loss 4.268747 on epoch=54
06/24/2022 13:44:21 - INFO - __main__ - Step 120 Global step 120 Train loss 4.493733 on epoch=59
06/24/2022 13:44:23 - INFO - __main__ - Step 130 Global step 130 Train loss 4.228281 on epoch=64
06/24/2022 13:44:26 - INFO - __main__ - Step 140 Global step 140 Train loss 4.400067 on epoch=69
06/24/2022 13:44:28 - INFO - __main__ - Step 150 Global step 150 Train loss 3.950871 on epoch=74
06/24/2022 13:44:29 - INFO - __main__ - Global step 150 Train loss 4.268340 ACC 0.03125 on epoch=74
06/24/2022 13:44:32 - INFO - __main__ - Step 160 Global step 160 Train loss 3.190266 on epoch=79
06/24/2022 13:44:34 - INFO - __main__ - Step 170 Global step 170 Train loss 2.838832 on epoch=84
06/24/2022 13:44:37 - INFO - __main__ - Step 180 Global step 180 Train loss 3.209516 on epoch=89
06/24/2022 13:44:39 - INFO - __main__ - Step 190 Global step 190 Train loss 2.564029 on epoch=94
06/24/2022 13:44:42 - INFO - __main__ - Step 200 Global step 200 Train loss 2.688386 on epoch=99
06/24/2022 13:44:42 - INFO - __main__ - Global step 200 Train loss 2.898206 ACC 0.5 on epoch=99
06/24/2022 13:44:45 - INFO - __main__ - Step 210 Global step 210 Train loss 2.404814 on epoch=104
06/24/2022 13:44:47 - INFO - __main__ - Step 220 Global step 220 Train loss 1.996541 on epoch=109
06/24/2022 13:44:50 - INFO - __main__ - Step 230 Global step 230 Train loss 1.840255 on epoch=114
06/24/2022 13:44:52 - INFO - __main__ - Step 240 Global step 240 Train loss 1.815708 on epoch=119
06/24/2022 13:44:55 - INFO - __main__ - Step 250 Global step 250 Train loss 1.217264 on epoch=124
06/24/2022 13:44:55 - INFO - __main__ - Global step 250 Train loss 1.854917 ACC 0.5 on epoch=124
06/24/2022 13:44:57 - INFO - __main__ - Step 260 Global step 260 Train loss 1.444480 on epoch=129
06/24/2022 13:45:00 - INFO - __main__ - Step 270 Global step 270 Train loss 1.734809 on epoch=134
06/24/2022 13:45:02 - INFO - __main__ - Step 280 Global step 280 Train loss 1.198184 on epoch=139
06/24/2022 13:45:05 - INFO - __main__ - Step 290 Global step 290 Train loss 1.321711 on epoch=144
06/24/2022 13:45:07 - INFO - __main__ - Step 300 Global step 300 Train loss 1.473882 on epoch=149
06/24/2022 13:45:08 - INFO - __main__ - Global step 300 Train loss 1.434613 ACC 0.5 on epoch=149
06/24/2022 13:45:10 - INFO - __main__ - Step 310 Global step 310 Train loss 1.814664 on epoch=154
06/24/2022 13:45:13 - INFO - __main__ - Step 320 Global step 320 Train loss 1.384532 on epoch=159
06/24/2022 13:45:15 - INFO - __main__ - Step 330 Global step 330 Train loss 1.479810 on epoch=164
06/24/2022 13:45:18 - INFO - __main__ - Step 340 Global step 340 Train loss 1.032611 on epoch=169
06/24/2022 13:45:20 - INFO - __main__ - Step 350 Global step 350 Train loss 1.288844 on epoch=174
06/24/2022 13:45:21 - INFO - __main__ - Global step 350 Train loss 1.400092 ACC 0.5 on epoch=174
06/24/2022 13:45:23 - INFO - __main__ - Step 360 Global step 360 Train loss 1.027698 on epoch=179
06/24/2022 13:45:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.896758 on epoch=184
06/24/2022 13:45:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.925735 on epoch=189
06/24/2022 13:45:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.987532 on epoch=194
06/24/2022 13:45:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.928158 on epoch=199
06/24/2022 13:45:33 - INFO - __main__ - Global step 400 Train loss 0.953176 ACC 0.5 on epoch=199
06/24/2022 13:45:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.621791 on epoch=204
06/24/2022 13:45:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.581636 on epoch=209
06/24/2022 13:45:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.604674 on epoch=214
06/24/2022 13:45:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.949407 on epoch=219
06/24/2022 13:45:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.717297 on epoch=224
06/24/2022 13:45:46 - INFO - __main__ - Global step 450 Train loss 0.694961 ACC 0.5 on epoch=224
06/24/2022 13:45:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.702748 on epoch=229
06/24/2022 13:45:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.537946 on epoch=234
06/24/2022 13:45:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.387157 on epoch=239
06/24/2022 13:45:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.714086 on epoch=244
06/24/2022 13:45:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.426995 on epoch=249
06/24/2022 13:45:59 - INFO - __main__ - Global step 500 Train loss 0.553786 ACC 0.5 on epoch=249
06/24/2022 13:46:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.700444 on epoch=254
06/24/2022 13:46:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.591306 on epoch=259
06/24/2022 13:46:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.547841 on epoch=264
06/24/2022 13:46:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.565975 on epoch=269
06/24/2022 13:46:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.607653 on epoch=274
06/24/2022 13:46:12 - INFO - __main__ - Global step 550 Train loss 0.602644 ACC 0.5 on epoch=274
06/24/2022 13:46:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.595469 on epoch=279
06/24/2022 13:46:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.579400 on epoch=284
06/24/2022 13:46:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.625218 on epoch=289
06/24/2022 13:46:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.634665 on epoch=294
06/24/2022 13:46:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.604689 on epoch=299
06/24/2022 13:46:25 - INFO - __main__ - Global step 600 Train loss 0.607888 ACC 0.5 on epoch=299
06/24/2022 13:46:25 - INFO - __main__ - save last model!
06/24/2022 13:46:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:46:25 - INFO - __main__ - Printing 3 examples
06/24/2022 13:46:25 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 13:46:25 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:25 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 13:46:25 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:25 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 13:46:25 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:46:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:46:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:46:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:46:26 - INFO - __main__ - Printing 3 examples
06/24/2022 13:46:26 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 13:46:26 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:26 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 13:46:26 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:26 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 13:46:26 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:46:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:46:26 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:46:28 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:46:28 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:46:28 - INFO - __main__ - Printing 3 examples
06/24/2022 13:46:28 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:46:28 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:28 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:46:28 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:46:28 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:46:28 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:46:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:46:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:46:28 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:46:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:46:29 - INFO - __main__ - Starting training!
06/24/2022 13:46:33 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_42_0.0001_8_predictions.txt
06/24/2022 13:46:33 - INFO - __main__ - ACC on test data: 0.6838
06/24/2022 13:46:33 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.0001, bsz=8, dev_performance=0.5, test_performance=0.6838235294117647
06/24/2022 13:46:33 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.0005, bsz=8 ...
06/24/2022 13:46:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:46:34 - INFO - __main__ - Printing 3 examples
06/24/2022 13:46:34 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 13:46:34 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:34 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 13:46:34 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:34 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 13:46:34 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:46:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:46:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:46:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:46:34 - INFO - __main__ - Printing 3 examples
06/24/2022 13:46:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 13:46:34 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:34 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 13:46:34 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 13:46:34 - INFO - __main__ - ['equivalent']
06/24/2022 13:46:34 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:46:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:46:34 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:46:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:46:38 - INFO - __main__ - Starting training!
06/24/2022 13:46:40 - INFO - __main__ - Step 10 Global step 10 Train loss 16.788933 on epoch=4
06/24/2022 13:46:42 - INFO - __main__ - Step 20 Global step 20 Train loss 10.971513 on epoch=9
06/24/2022 13:46:45 - INFO - __main__ - Step 30 Global step 30 Train loss 4.685573 on epoch=14
06/24/2022 13:46:47 - INFO - __main__ - Step 40 Global step 40 Train loss 3.618041 on epoch=19
06/24/2022 13:46:50 - INFO - __main__ - Step 50 Global step 50 Train loss 2.363402 on epoch=24
06/24/2022 13:46:50 - INFO - __main__ - Global step 50 Train loss 7.685493 ACC 0.5 on epoch=24
06/24/2022 13:46:53 - INFO - __main__ - Step 60 Global step 60 Train loss 1.357523 on epoch=29
06/24/2022 13:46:55 - INFO - __main__ - Step 70 Global step 70 Train loss 1.416727 on epoch=34
06/24/2022 13:46:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.641994 on epoch=39
06/24/2022 13:47:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.920386 on epoch=44
06/24/2022 13:47:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.601355 on epoch=49
06/24/2022 13:47:03 - INFO - __main__ - Global step 100 Train loss 0.987597 ACC 0.5 on epoch=49
06/24/2022 13:47:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.633341 on epoch=54
06/24/2022 13:47:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.452224 on epoch=59
06/24/2022 13:47:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.452887 on epoch=64
06/24/2022 13:47:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.385591 on epoch=69
06/24/2022 13:47:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.567432 on epoch=74
06/24/2022 13:47:16 - INFO - __main__ - Global step 150 Train loss 0.498295 ACC 0.5 on epoch=74
06/24/2022 13:47:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.409768 on epoch=79
06/24/2022 13:47:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.384104 on epoch=84
06/24/2022 13:47:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.479414 on epoch=89
06/24/2022 13:47:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.325372 on epoch=94
06/24/2022 13:47:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.379002 on epoch=99
06/24/2022 13:47:28 - INFO - __main__ - Global step 200 Train loss 0.395532 ACC 0.5 on epoch=99
06/24/2022 13:47:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.296413 on epoch=104
06/24/2022 13:47:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.246408 on epoch=109
06/24/2022 13:47:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.157363 on epoch=114
06/24/2022 13:47:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.128392 on epoch=119
06/24/2022 13:47:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.089064 on epoch=124
06/24/2022 13:47:41 - INFO - __main__ - Global step 250 Train loss 0.183528 ACC 0.5 on epoch=124
06/24/2022 13:47:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.037099 on epoch=129
06/24/2022 13:47:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.067976 on epoch=134
06/24/2022 13:47:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.050529 on epoch=139
06/24/2022 13:47:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.302912 on epoch=144
06/24/2022 13:47:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.064025 on epoch=149
06/24/2022 13:47:54 - INFO - __main__ - Global step 300 Train loss 0.104508 ACC 0.5 on epoch=149
06/24/2022 13:47:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.062853 on epoch=154
06/24/2022 13:47:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.006884 on epoch=159
06/24/2022 13:48:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.016017 on epoch=164
06/24/2022 13:48:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.012547 on epoch=169
06/24/2022 13:48:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.002648 on epoch=174
06/24/2022 13:48:06 - INFO - __main__ - Global step 350 Train loss 0.020190 ACC 0.53125 on epoch=174
06/24/2022 13:48:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.008996 on epoch=179
06/24/2022 13:48:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001992 on epoch=184
06/24/2022 13:48:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.004001 on epoch=189
06/24/2022 13:48:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.002025 on epoch=194
06/24/2022 13:48:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.007890 on epoch=199
06/24/2022 13:48:19 - INFO - __main__ - Global step 400 Train loss 0.004981 ACC 0.5 on epoch=199
06/24/2022 13:48:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.011319 on epoch=204
06/24/2022 13:48:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001299 on epoch=209
06/24/2022 13:48:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.011498 on epoch=214
06/24/2022 13:48:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.002584 on epoch=219
06/24/2022 13:48:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.003089 on epoch=224
06/24/2022 13:48:32 - INFO - __main__ - Global step 450 Train loss 0.005958 ACC 0.53125 on epoch=224
06/24/2022 13:48:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001555 on epoch=229
06/24/2022 13:48:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000169 on epoch=234
06/24/2022 13:48:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000319 on epoch=239
06/24/2022 13:48:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.005014 on epoch=244
06/24/2022 13:48:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000542 on epoch=249
06/24/2022 13:48:45 - INFO - __main__ - Global step 500 Train loss 0.001520 ACC 0.5 on epoch=249
06/24/2022 13:48:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001151 on epoch=254
06/24/2022 13:48:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000068 on epoch=259
06/24/2022 13:48:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001954 on epoch=264
06/24/2022 13:48:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.041078 on epoch=269
06/24/2022 13:48:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.023383 on epoch=274
06/24/2022 13:48:57 - INFO - __main__ - Global step 550 Train loss 0.013527 ACC 0.5 on epoch=274
06/24/2022 13:49:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.005739 on epoch=279
06/24/2022 13:49:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000022 on epoch=284
06/24/2022 13:49:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000378 on epoch=289
06/24/2022 13:49:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000015 on epoch=294
06/24/2022 13:49:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000007 on epoch=299
06/24/2022 13:49:10 - INFO - __main__ - Global step 600 Train loss 0.001232 ACC 0.46875 on epoch=299
06/24/2022 13:49:10 - INFO - __main__ - save last model!
06/24/2022 13:49:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:49:11 - INFO - __main__ - Printing 3 examples
06/24/2022 13:49:11 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 13:49:11 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:11 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 13:49:11 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:11 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 13:49:11 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:49:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:49:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:49:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:49:11 - INFO - __main__ - Printing 3 examples
06/24/2022 13:49:11 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 13:49:11 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:11 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 13:49:11 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:11 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 13:49:11 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:11 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:49:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:49:11 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:49:13 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:49:13 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:49:13 - INFO - __main__ - Printing 3 examples
06/24/2022 13:49:13 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:49:13 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:13 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:49:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:49:13 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:49:13 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:49:13 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:49:13 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:49:14 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:49:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:49:15 - INFO - __main__ - Starting training!
06/24/2022 13:49:19 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_87_0.0005_8_predictions.txt
06/24/2022 13:49:19 - INFO - __main__ - ACC on test data: 0.6740
06/24/2022 13:49:19 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.0005, bsz=8, dev_performance=0.53125, test_performance=0.6740196078431373
06/24/2022 13:49:19 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.0003, bsz=8 ...
06/24/2022 13:49:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:49:20 - INFO - __main__ - Printing 3 examples
06/24/2022 13:49:20 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 13:49:20 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:20 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 13:49:20 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:20 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 13:49:20 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:49:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:49:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:49:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:49:20 - INFO - __main__ - Printing 3 examples
06/24/2022 13:49:20 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 13:49:20 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:20 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 13:49:20 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:20 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 13:49:20 - INFO - __main__ - ['equivalent']
06/24/2022 13:49:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:49:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:49:20 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:49:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:49:24 - INFO - __main__ - Starting training!
06/24/2022 13:49:26 - INFO - __main__ - Step 10 Global step 10 Train loss 16.583843 on epoch=4
06/24/2022 13:49:28 - INFO - __main__ - Step 20 Global step 20 Train loss 11.551314 on epoch=9
06/24/2022 13:49:31 - INFO - __main__ - Step 30 Global step 30 Train loss 7.308434 on epoch=14
06/24/2022 13:49:33 - INFO - __main__ - Step 40 Global step 40 Train loss 6.259741 on epoch=19
06/24/2022 13:49:35 - INFO - __main__ - Step 50 Global step 50 Train loss 4.565417 on epoch=24
06/24/2022 13:49:36 - INFO - __main__ - Global step 50 Train loss 9.253751 ACC 0.40625 on epoch=24
06/24/2022 13:49:39 - INFO - __main__ - Step 60 Global step 60 Train loss 3.801456 on epoch=29
06/24/2022 13:49:41 - INFO - __main__ - Step 70 Global step 70 Train loss 2.925162 on epoch=34
06/24/2022 13:49:44 - INFO - __main__ - Step 80 Global step 80 Train loss 2.533160 on epoch=39
06/24/2022 13:49:46 - INFO - __main__ - Step 90 Global step 90 Train loss 1.907071 on epoch=44
06/24/2022 13:49:49 - INFO - __main__ - Step 100 Global step 100 Train loss 1.222392 on epoch=49
06/24/2022 13:49:49 - INFO - __main__ - Global step 100 Train loss 2.477848 ACC 0.5 on epoch=49
06/24/2022 13:49:52 - INFO - __main__ - Step 110 Global step 110 Train loss 1.428416 on epoch=54
06/24/2022 13:49:54 - INFO - __main__ - Step 120 Global step 120 Train loss 1.133335 on epoch=59
06/24/2022 13:49:57 - INFO - __main__ - Step 130 Global step 130 Train loss 1.048619 on epoch=64
06/24/2022 13:49:59 - INFO - __main__ - Step 140 Global step 140 Train loss 1.308988 on epoch=69
06/24/2022 13:50:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.749155 on epoch=74
06/24/2022 13:50:02 - INFO - __main__ - Global step 150 Train loss 1.133703 ACC 0.53125 on epoch=74
06/24/2022 13:50:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.930697 on epoch=79
06/24/2022 13:50:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.685063 on epoch=84
06/24/2022 13:50:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.590712 on epoch=89
06/24/2022 13:50:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.678829 on epoch=94
06/24/2022 13:50:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.527621 on epoch=99
06/24/2022 13:50:15 - INFO - __main__ - Global step 200 Train loss 0.682584 ACC 0.5 on epoch=99
06/24/2022 13:50:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.633319 on epoch=104
06/24/2022 13:50:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.718696 on epoch=109
06/24/2022 13:50:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.643660 on epoch=114
06/24/2022 13:50:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.532655 on epoch=119
06/24/2022 13:50:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.398206 on epoch=124
06/24/2022 13:50:28 - INFO - __main__ - Global step 250 Train loss 0.585307 ACC 0.5 on epoch=124
06/24/2022 13:50:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.694563 on epoch=129
06/24/2022 13:50:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.539255 on epoch=134
06/24/2022 13:50:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.566693 on epoch=139
06/24/2022 13:50:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.402699 on epoch=144
06/24/2022 13:50:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.524057 on epoch=149
06/24/2022 13:50:41 - INFO - __main__ - Global step 300 Train loss 0.545453 ACC 0.5 on epoch=149
06/24/2022 13:50:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.420732 on epoch=154
06/24/2022 13:50:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.458053 on epoch=159
06/24/2022 13:50:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.599411 on epoch=164
06/24/2022 13:50:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.537420 on epoch=169
06/24/2022 13:50:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.507246 on epoch=174
06/24/2022 13:50:54 - INFO - __main__ - Global step 350 Train loss 0.504573 ACC 0.5 on epoch=174
06/24/2022 13:50:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.486022 on epoch=179
06/24/2022 13:50:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.437559 on epoch=184
06/24/2022 13:51:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.348343 on epoch=189
06/24/2022 13:51:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.504150 on epoch=194
06/24/2022 13:51:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.403470 on epoch=199
06/24/2022 13:51:06 - INFO - __main__ - Global step 400 Train loss 0.435909 ACC 0.46875 on epoch=199
06/24/2022 13:51:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.344915 on epoch=204
06/24/2022 13:51:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.378900 on epoch=209
06/24/2022 13:51:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.454338 on epoch=214
06/24/2022 13:51:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.343903 on epoch=219
06/24/2022 13:51:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.394040 on epoch=224
06/24/2022 13:51:20 - INFO - __main__ - Global step 450 Train loss 0.383219 ACC 0.3125 on epoch=224
06/24/2022 13:51:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.345498 on epoch=229
06/24/2022 13:51:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.288551 on epoch=234
06/24/2022 13:51:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.294403 on epoch=239
06/24/2022 13:51:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.327655 on epoch=244
06/24/2022 13:51:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.437288 on epoch=249
06/24/2022 13:51:33 - INFO - __main__ - Global step 500 Train loss 0.338679 ACC 0.46875 on epoch=249
06/24/2022 13:51:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.255264 on epoch=254
06/24/2022 13:51:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.291323 on epoch=259
06/24/2022 13:51:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.221288 on epoch=264
06/24/2022 13:51:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.281183 on epoch=269
06/24/2022 13:51:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.193965 on epoch=274
06/24/2022 13:51:46 - INFO - __main__ - Global step 550 Train loss 0.248605 ACC 0.53125 on epoch=274
06/24/2022 13:51:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.191368 on epoch=279
06/24/2022 13:51:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.243910 on epoch=284
06/24/2022 13:51:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.232146 on epoch=289
06/24/2022 13:51:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.283167 on epoch=294
06/24/2022 13:51:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.180435 on epoch=299
06/24/2022 13:51:59 - INFO - __main__ - Global step 600 Train loss 0.226205 ACC 0.5625 on epoch=299
06/24/2022 13:51:59 - INFO - __main__ - save last model!
06/24/2022 13:51:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:51:59 - INFO - __main__ - Printing 3 examples
06/24/2022 13:51:59 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 13:51:59 - INFO - __main__ - ['equivalent']
06/24/2022 13:51:59 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 13:51:59 - INFO - __main__ - ['equivalent']
06/24/2022 13:51:59 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 13:51:59 - INFO - __main__ - ['equivalent']
06/24/2022 13:51:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:52:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:52:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:52:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:52:00 - INFO - __main__ - Printing 3 examples
06/24/2022 13:52:00 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 13:52:00 - INFO - __main__ - ['equivalent']
06/24/2022 13:52:00 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 13:52:00 - INFO - __main__ - ['equivalent']
06/24/2022 13:52:00 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 13:52:00 - INFO - __main__ - ['equivalent']
06/24/2022 13:52:00 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:52:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:52:00 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:52:02 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:52:02 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:52:02 - INFO - __main__ - Printing 3 examples
06/24/2022 13:52:02 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:52:02 - INFO - __main__ - ['equivalent']
06/24/2022 13:52:02 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:52:02 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:52:02 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:52:02 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:52:02 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:52:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:52:03 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:52:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:52:03 - INFO - __main__ - Starting training!
06/24/2022 13:52:08 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_87_0.0003_8_predictions.txt
06/24/2022 13:52:08 - INFO - __main__ - ACC on test data: 0.6176
06/24/2022 13:52:08 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.0003, bsz=8, dev_performance=0.5625, test_performance=0.6176470588235294
06/24/2022 13:52:08 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.0002, bsz=8 ...
06/24/2022 13:52:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:52:09 - INFO - __main__ - Printing 3 examples
06/24/2022 13:52:09 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 13:52:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:52:09 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 13:52:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:52:09 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 13:52:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:52:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:52:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:52:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:52:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:52:09 - INFO - __main__ - Printing 3 examples
06/24/2022 13:52:09 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 13:52:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:52:09 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 13:52:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:52:09 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 13:52:09 - INFO - __main__ - ['equivalent']
06/24/2022 13:52:09 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:52:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:52:09 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:52:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:52:13 - INFO - __main__ - Starting training!
06/24/2022 13:52:15 - INFO - __main__ - Step 10 Global step 10 Train loss 16.825024 on epoch=4
06/24/2022 13:52:18 - INFO - __main__ - Step 20 Global step 20 Train loss 13.526380 on epoch=9
06/24/2022 13:52:20 - INFO - __main__ - Step 30 Global step 30 Train loss 9.423085 on epoch=14
06/24/2022 13:52:23 - INFO - __main__ - Step 40 Global step 40 Train loss 7.526269 on epoch=19
06/24/2022 13:52:25 - INFO - __main__ - Step 50 Global step 50 Train loss 6.877139 on epoch=24
06/24/2022 13:52:26 - INFO - __main__ - Global step 50 Train loss 10.835580 ACC 0.125 on epoch=24
06/24/2022 13:52:28 - INFO - __main__ - Step 60 Global step 60 Train loss 5.268384 on epoch=29
06/24/2022 13:52:31 - INFO - __main__ - Step 70 Global step 70 Train loss 4.507007 on epoch=34
06/24/2022 13:52:33 - INFO - __main__ - Step 80 Global step 80 Train loss 3.761143 on epoch=39
06/24/2022 13:52:36 - INFO - __main__ - Step 90 Global step 90 Train loss 3.786615 on epoch=44
06/24/2022 13:52:38 - INFO - __main__ - Step 100 Global step 100 Train loss 2.185703 on epoch=49
06/24/2022 13:52:39 - INFO - __main__ - Global step 100 Train loss 3.901770 ACC 0.5 on epoch=49
06/24/2022 13:52:42 - INFO - __main__ - Step 110 Global step 110 Train loss 1.796842 on epoch=54
06/24/2022 13:52:44 - INFO - __main__ - Step 120 Global step 120 Train loss 2.400627 on epoch=59
06/24/2022 13:52:47 - INFO - __main__ - Step 130 Global step 130 Train loss 2.089113 on epoch=64
06/24/2022 13:52:49 - INFO - __main__ - Step 140 Global step 140 Train loss 1.689797 on epoch=69
06/24/2022 13:52:52 - INFO - __main__ - Step 150 Global step 150 Train loss 1.722287 on epoch=74
06/24/2022 13:52:52 - INFO - __main__ - Global step 150 Train loss 1.939733 ACC 0.5 on epoch=74
06/24/2022 13:52:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.744206 on epoch=79
06/24/2022 13:52:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.802680 on epoch=84
06/24/2022 13:53:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.841509 on epoch=89
06/24/2022 13:53:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.948815 on epoch=94
06/24/2022 13:53:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.996529 on epoch=99
06/24/2022 13:53:05 - INFO - __main__ - Global step 200 Train loss 0.866748 ACC 0.5 on epoch=99
06/24/2022 13:53:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.729235 on epoch=104
06/24/2022 13:53:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.883541 on epoch=109
06/24/2022 13:53:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.629174 on epoch=114
06/24/2022 13:53:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.541667 on epoch=119
06/24/2022 13:53:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.638093 on epoch=124
06/24/2022 13:53:18 - INFO - __main__ - Global step 250 Train loss 0.684342 ACC 0.5 on epoch=124
06/24/2022 13:53:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.548028 on epoch=129
06/24/2022 13:53:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.433263 on epoch=134
06/24/2022 13:53:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.605507 on epoch=139
06/24/2022 13:53:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.527501 on epoch=144
06/24/2022 13:53:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.491330 on epoch=149
06/24/2022 13:53:30 - INFO - __main__ - Global step 300 Train loss 0.521126 ACC 0.5 on epoch=149
06/24/2022 13:53:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.490766 on epoch=154
06/24/2022 13:53:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.476901 on epoch=159
06/24/2022 13:53:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.614568 on epoch=164
06/24/2022 13:53:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.433843 on epoch=169
06/24/2022 13:53:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.373619 on epoch=174
06/24/2022 13:53:43 - INFO - __main__ - Global step 350 Train loss 0.477939 ACC 0.5 on epoch=174
06/24/2022 13:53:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.401802 on epoch=179
06/24/2022 13:53:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.472204 on epoch=184
06/24/2022 13:53:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.406805 on epoch=189
06/24/2022 13:53:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.312730 on epoch=194
06/24/2022 13:53:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.378085 on epoch=199
06/24/2022 13:53:56 - INFO - __main__ - Global step 400 Train loss 0.394325 ACC 0.5 on epoch=199
06/24/2022 13:53:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.428581 on epoch=204
06/24/2022 13:54:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.310060 on epoch=209
06/24/2022 13:54:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.366985 on epoch=214
06/24/2022 13:54:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.429540 on epoch=219
06/24/2022 13:54:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.451687 on epoch=224
06/24/2022 13:54:09 - INFO - __main__ - Global step 450 Train loss 0.397371 ACC 0.4375 on epoch=224
06/24/2022 13:54:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.322895 on epoch=229
06/24/2022 13:54:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.430716 on epoch=234
06/24/2022 13:54:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.633810 on epoch=239
06/24/2022 13:54:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.351768 on epoch=244
06/24/2022 13:54:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.271250 on epoch=249
06/24/2022 13:54:22 - INFO - __main__ - Global step 500 Train loss 0.402088 ACC 0.40625 on epoch=249
06/24/2022 13:54:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.360285 on epoch=254
06/24/2022 13:54:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.300599 on epoch=259
06/24/2022 13:54:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.403202 on epoch=264
06/24/2022 13:54:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.267326 on epoch=269
06/24/2022 13:54:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.233891 on epoch=274
06/24/2022 13:54:35 - INFO - __main__ - Global step 550 Train loss 0.313061 ACC 0.46875 on epoch=274
06/24/2022 13:54:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.207040 on epoch=279
06/24/2022 13:54:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.298870 on epoch=284
06/24/2022 13:54:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.235629 on epoch=289
06/24/2022 13:54:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.212784 on epoch=294
06/24/2022 13:54:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.190526 on epoch=299
06/24/2022 13:54:47 - INFO - __main__ - Global step 600 Train loss 0.228970 ACC 0.46875 on epoch=299
06/24/2022 13:54:47 - INFO - __main__ - save last model!
06/24/2022 13:54:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:54:48 - INFO - __main__ - Printing 3 examples
06/24/2022 13:54:48 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 13:54:48 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:48 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 13:54:48 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:48 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 13:54:48 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 13:54:48 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:54:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:54:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:54:48 - INFO - __main__ - Printing 3 examples
06/24/2022 13:54:48 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 13:54:48 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:48 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 13:54:48 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:48 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 13:54:48 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:48 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:54:48 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:54:48 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:54:50 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:54:50 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:54:50 - INFO - __main__ - Printing 3 examples
06/24/2022 13:54:50 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:54:50 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:54:50 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:54:50 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:54:50 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:54:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:54:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:54:51 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:54:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:54:52 - INFO - __main__ - Starting training!
06/24/2022 13:54:55 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_87_0.0002_8_predictions.txt
06/24/2022 13:54:55 - INFO - __main__ - ACC on test data: 0.6838
06/24/2022 13:54:55 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.0002, bsz=8, dev_performance=0.5, test_performance=0.6838235294117647
06/24/2022 13:54:55 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.0001, bsz=8 ...
06/24/2022 13:54:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:54:56 - INFO - __main__ - Printing 3 examples
06/24/2022 13:54:56 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/24/2022 13:54:56 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/24/2022 13:54:56 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:56 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/24/2022 13:54:56 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:54:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:54:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:54:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:54:57 - INFO - __main__ - Printing 3 examples
06/24/2022 13:54:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/24/2022 13:54:57 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:57 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/24/2022 13:54:57 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/24/2022 13:54:57 - INFO - __main__ - ['equivalent']
06/24/2022 13:54:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:54:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:54:57 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:55:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:55:01 - INFO - __main__ - Starting training!
06/24/2022 13:55:03 - INFO - __main__ - Step 10 Global step 10 Train loss 16.747484 on epoch=4
06/24/2022 13:55:05 - INFO - __main__ - Step 20 Global step 20 Train loss 15.652324 on epoch=9
06/24/2022 13:55:08 - INFO - __main__ - Step 30 Global step 30 Train loss 12.530157 on epoch=14
06/24/2022 13:55:10 - INFO - __main__ - Step 40 Global step 40 Train loss 9.831985 on epoch=19
06/24/2022 13:55:13 - INFO - __main__ - Step 50 Global step 50 Train loss 8.264404 on epoch=24
06/24/2022 13:55:15 - INFO - __main__ - Global step 50 Train loss 12.605271 ACC 0.0 on epoch=24
06/24/2022 13:55:18 - INFO - __main__ - Step 60 Global step 60 Train loss 8.082253 on epoch=29
06/24/2022 13:55:20 - INFO - __main__ - Step 70 Global step 70 Train loss 6.776243 on epoch=34
06/24/2022 13:55:23 - INFO - __main__ - Step 80 Global step 80 Train loss 6.886922 on epoch=39
06/24/2022 13:55:25 - INFO - __main__ - Step 90 Global step 90 Train loss 6.061449 on epoch=44
06/24/2022 13:55:28 - INFO - __main__ - Step 100 Global step 100 Train loss 5.904142 on epoch=49
06/24/2022 13:55:29 - INFO - __main__ - Global step 100 Train loss 6.742202 ACC 0.1875 on epoch=49
06/24/2022 13:55:32 - INFO - __main__ - Step 110 Global step 110 Train loss 5.100109 on epoch=54
06/24/2022 13:55:34 - INFO - __main__ - Step 120 Global step 120 Train loss 4.990197 on epoch=59
06/24/2022 13:55:37 - INFO - __main__ - Step 130 Global step 130 Train loss 4.885708 on epoch=64
06/24/2022 13:55:39 - INFO - __main__ - Step 140 Global step 140 Train loss 4.700754 on epoch=69
06/24/2022 13:55:42 - INFO - __main__ - Step 150 Global step 150 Train loss 4.251595 on epoch=74
06/24/2022 13:55:42 - INFO - __main__ - Global step 150 Train loss 4.785672 ACC 0.03125 on epoch=74
06/24/2022 13:55:45 - INFO - __main__ - Step 160 Global step 160 Train loss 4.053708 on epoch=79
06/24/2022 13:55:47 - INFO - __main__ - Step 170 Global step 170 Train loss 3.625203 on epoch=84
06/24/2022 13:55:50 - INFO - __main__ - Step 180 Global step 180 Train loss 2.895851 on epoch=89
06/24/2022 13:55:52 - INFO - __main__ - Step 190 Global step 190 Train loss 2.719900 on epoch=94
06/24/2022 13:55:55 - INFO - __main__ - Step 200 Global step 200 Train loss 2.949079 on epoch=99
06/24/2022 13:55:55 - INFO - __main__ - Global step 200 Train loss 3.248748 ACC 0.46875 on epoch=99
06/24/2022 13:55:58 - INFO - __main__ - Step 210 Global step 210 Train loss 2.146370 on epoch=104
06/24/2022 13:56:01 - INFO - __main__ - Step 220 Global step 220 Train loss 2.146764 on epoch=109
06/24/2022 13:56:03 - INFO - __main__ - Step 230 Global step 230 Train loss 2.401010 on epoch=114
06/24/2022 13:56:05 - INFO - __main__ - Step 240 Global step 240 Train loss 1.919098 on epoch=119
06/24/2022 13:56:08 - INFO - __main__ - Step 250 Global step 250 Train loss 2.035860 on epoch=124
06/24/2022 13:56:08 - INFO - __main__ - Global step 250 Train loss 2.129821 ACC 0.5 on epoch=124
06/24/2022 13:56:11 - INFO - __main__ - Step 260 Global step 260 Train loss 1.558028 on epoch=129
06/24/2022 13:56:14 - INFO - __main__ - Step 270 Global step 270 Train loss 1.434635 on epoch=134
06/24/2022 13:56:16 - INFO - __main__ - Step 280 Global step 280 Train loss 1.514666 on epoch=139
06/24/2022 13:56:19 - INFO - __main__ - Step 290 Global step 290 Train loss 1.760831 on epoch=144
06/24/2022 13:56:21 - INFO - __main__ - Step 300 Global step 300 Train loss 1.604246 on epoch=149
06/24/2022 13:56:22 - INFO - __main__ - Global step 300 Train loss 1.574481 ACC 0.5 on epoch=149
06/24/2022 13:56:24 - INFO - __main__ - Step 310 Global step 310 Train loss 1.392370 on epoch=154
06/24/2022 13:56:26 - INFO - __main__ - Step 320 Global step 320 Train loss 1.422456 on epoch=159
06/24/2022 13:56:29 - INFO - __main__ - Step 330 Global step 330 Train loss 1.162135 on epoch=164
06/24/2022 13:56:31 - INFO - __main__ - Step 340 Global step 340 Train loss 1.377022 on epoch=169
06/24/2022 13:56:34 - INFO - __main__ - Step 350 Global step 350 Train loss 1.188496 on epoch=174
06/24/2022 13:56:34 - INFO - __main__ - Global step 350 Train loss 1.308496 ACC 0.5 on epoch=174
06/24/2022 13:56:37 - INFO - __main__ - Step 360 Global step 360 Train loss 1.255075 on epoch=179
06/24/2022 13:56:39 - INFO - __main__ - Step 370 Global step 370 Train loss 1.110776 on epoch=184
06/24/2022 13:56:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.899142 on epoch=189
06/24/2022 13:56:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.878883 on epoch=194
06/24/2022 13:56:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.810127 on epoch=199
06/24/2022 13:56:47 - INFO - __main__ - Global step 400 Train loss 0.990801 ACC 0.46875 on epoch=199
06/24/2022 13:56:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.878022 on epoch=204
06/24/2022 13:56:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.970891 on epoch=209
06/24/2022 13:56:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.764877 on epoch=214
06/24/2022 13:56:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.653750 on epoch=219
06/24/2022 13:56:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.682091 on epoch=224
06/24/2022 13:57:00 - INFO - __main__ - Global step 450 Train loss 0.789926 ACC 0.5 on epoch=224
06/24/2022 13:57:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.659567 on epoch=229
06/24/2022 13:57:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.555607 on epoch=234
06/24/2022 13:57:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.683476 on epoch=239
06/24/2022 13:57:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.672007 on epoch=244
06/24/2022 13:57:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.665155 on epoch=249
06/24/2022 13:57:13 - INFO - __main__ - Global step 500 Train loss 0.647163 ACC 0.5 on epoch=249
06/24/2022 13:57:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.432146 on epoch=254
06/24/2022 13:57:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.759478 on epoch=259
06/24/2022 13:57:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.749544 on epoch=264
06/24/2022 13:57:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.678233 on epoch=269
06/24/2022 13:57:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.638410 on epoch=274
06/24/2022 13:57:25 - INFO - __main__ - Global step 550 Train loss 0.651562 ACC 0.5 on epoch=274
06/24/2022 13:57:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.687389 on epoch=279
06/24/2022 13:57:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.502532 on epoch=284
06/24/2022 13:57:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.574616 on epoch=289
06/24/2022 13:57:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.366707 on epoch=294
06/24/2022 13:57:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.561205 on epoch=299
06/24/2022 13:57:38 - INFO - __main__ - Global step 600 Train loss 0.538490 ACC 0.5 on epoch=299
06/24/2022 13:57:38 - INFO - __main__ - save last model!
06/24/2022 13:57:41 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 13:57:41 - INFO - __main__ - Start tokenizing ... 408 instances
06/24/2022 13:57:41 - INFO - __main__ - Printing 3 examples
06/24/2022 13:57:41 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/24/2022 13:57:41 - INFO - __main__ - ['equivalent']
06/24/2022 13:57:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/24/2022 13:57:41 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:57:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/24/2022 13:57:41 - INFO - __main__ - ['not_equivalent']
06/24/2022 13:57:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:57:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:57:42 - INFO - __main__ - Loaded 408 examples from test data
06/24/2022 13:57:46 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-mrpc/glue-mrpc_16_87_0.0001_8_predictions.txt
06/24/2022 13:57:46 - INFO - __main__ - ACC on test data: 0.6838
06/24/2022 13:57:46 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.0001, bsz=8, dev_performance=0.5, test_performance=0.6838235294117647
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (91170): No such process
Task: glue-qqp, Checkpoint: None, Identifier: T5-base-ft-nopara2para
Output directory () already exists and is not empty.
06/24/2022 13:57:51 - INFO - __main__ - Namespace(task_dir='data/glue-qqp/', task_name='glue-qqp', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-glue-qqp', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/24/2022 13:57:51 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-glue-qqp
06/24/2022 13:57:51 - INFO - __main__ - Namespace(task_dir='data/glue-qqp/', task_name='glue-qqp', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-glue-qqp', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/24/2022 13:57:51 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-glue-qqp
06/24/2022 13:57:53 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/24/2022 13:57:53 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/24/2022 13:57:53 - INFO - __main__ - args.device: cuda:1
06/24/2022 13:57:53 - INFO - __main__ - args.device: cuda:0
06/24/2022 13:57:53 - INFO - __main__ - Using 2 gpus
06/24/2022 13:57:53 - INFO - __main__ - Using 2 gpus
06/24/2022 13:57:53 - INFO - __main__ - Fine-tuning the following samples: ['glue-qqp_16_100', 'glue-qqp_16_13', 'glue-qqp_16_21', 'glue-qqp_16_42', 'glue-qqp_16_87']
06/24/2022 13:57:53 - INFO - __main__ - Fine-tuning the following samples: ['glue-qqp_16_100', 'glue-qqp_16_13', 'glue-qqp_16_21', 'glue-qqp_16_42', 'glue-qqp_16_87']
06/24/2022 13:57:57 - INFO - __main__ - Running ... prefix=glue-qqp_16_100, lr=0.0005, bsz=8 ...
06/24/2022 13:57:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:57:58 - INFO - __main__ - Printing 3 examples
06/24/2022 13:57:58 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/24/2022 13:57:58 - INFO - __main__ - ['not_duplicate']
06/24/2022 13:57:58 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/24/2022 13:57:58 - INFO - __main__ - ['not_duplicate']
06/24/2022 13:57:58 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/24/2022 13:57:58 - INFO - __main__ - ['not_duplicate']
06/24/2022 13:57:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:57:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:57:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:57:58 - INFO - __main__ - Printing 3 examples
06/24/2022 13:57:58 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/24/2022 13:57:58 - INFO - __main__ - ['not_duplicate']
06/24/2022 13:57:58 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/24/2022 13:57:58 - INFO - __main__ - ['not_duplicate']
06/24/2022 13:57:58 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/24/2022 13:57:58 - INFO - __main__ - ['not_duplicate']
06/24/2022 13:57:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 13:57:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:57:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:57:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:57:58 - INFO - __main__ - Printing 3 examples
06/24/2022 13:57:58 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/24/2022 13:57:58 - INFO - __main__ - ['not_duplicate']
06/24/2022 13:57:58 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/24/2022 13:57:58 - INFO - __main__ - ['not_duplicate']
06/24/2022 13:57:58 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/24/2022 13:57:58 - INFO - __main__ - ['not_duplicate']
06/24/2022 13:57:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:57:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 13:57:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 13:57:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:57:58 - INFO - __main__ - Printing 3 examples
06/24/2022 13:57:58 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/24/2022 13:57:58 - INFO - __main__ - ['not_duplicate']
06/24/2022 13:57:58 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/24/2022 13:57:58 - INFO - __main__ - ['not_duplicate']
06/24/2022 13:57:58 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/24/2022 13:57:58 - INFO - __main__ - ['not_duplicate']
06/24/2022 13:57:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 13:57:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 13:57:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:57:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 13:58:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:58:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 13:58:03 - INFO - __main__ - Starting training!
06/24/2022 13:58:03 - INFO - __main__ - Starting training!
06/24/2022 13:58:06 - INFO - __main__ - Step 10 Global step 10 Train loss 17.316149 on epoch=4
06/24/2022 13:58:08 - INFO - __main__ - Step 20 Global step 20 Train loss 14.974356 on epoch=9
06/24/2022 13:58:10 - INFO - __main__ - Step 30 Global step 30 Train loss 11.476722 on epoch=14
06/24/2022 13:58:13 - INFO - __main__ - Step 40 Global step 40 Train loss 6.714932 on epoch=19
06/24/2022 13:58:15 - INFO - __main__ - Step 50 Global step 50 Train loss 4.428121 on epoch=24
06/24/2022 13:58:16 - INFO - __main__ - Global step 50 Train loss 10.982056 ACC 0.03125 on epoch=24
06/24/2022 13:58:19 - INFO - __main__ - Step 60 Global step 60 Train loss 3.173983 on epoch=29
06/24/2022 13:58:21 - INFO - __main__ - Step 70 Global step 70 Train loss 1.615213 on epoch=34
06/24/2022 13:58:24 - INFO - __main__ - Step 80 Global step 80 Train loss 1.687169 on epoch=39
06/24/2022 13:58:26 - INFO - __main__ - Step 90 Global step 90 Train loss 1.579575 on epoch=44
06/24/2022 13:58:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.963205 on epoch=49
06/24/2022 13:58:29 - INFO - __main__ - Global step 100 Train loss 1.803829 ACC 0.625 on epoch=49
06/24/2022 13:58:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.828173 on epoch=54
06/24/2022 13:58:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.844883 on epoch=59
06/24/2022 13:58:37 - INFO - __main__ - Step 130 Global step 130 Train loss 1.102239 on epoch=64
06/24/2022 13:58:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.840410 on epoch=69
06/24/2022 13:58:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.782179 on epoch=74
06/24/2022 13:58:43 - INFO - __main__ - Global step 150 Train loss 0.879577 ACC 0.5 on epoch=74
06/24/2022 13:58:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.500374 on epoch=79
06/24/2022 13:58:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.431401 on epoch=84
06/24/2022 13:58:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.469758 on epoch=89
06/24/2022 13:58:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.478351 on epoch=94
06/24/2022 13:58:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.380929 on epoch=99
06/24/2022 13:58:56 - INFO - __main__ - Global step 200 Train loss 0.452163 ACC 0.5 on epoch=99
06/24/2022 13:58:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.448932 on epoch=104
06/24/2022 13:59:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.431987 on epoch=109
06/24/2022 13:59:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.462273 on epoch=114
06/24/2022 13:59:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.434918 on epoch=119
06/24/2022 13:59:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.489977 on epoch=124
06/24/2022 13:59:08 - INFO - __main__ - Global step 250 Train loss 0.453617 ACC 0.5 on epoch=124
06/24/2022 13:59:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.516763 on epoch=129
06/24/2022 13:59:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.377787 on epoch=134
06/24/2022 13:59:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.362926 on epoch=139
06/24/2022 13:59:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.382536 on epoch=144
06/24/2022 13:59:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.424919 on epoch=149
06/24/2022 13:59:21 - INFO - __main__ - Global step 300 Train loss 0.412986 ACC 0.5 on epoch=149
06/24/2022 13:59:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.456522 on epoch=154
06/24/2022 13:59:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.397025 on epoch=159
06/24/2022 13:59:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.426976 on epoch=164
06/24/2022 13:59:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.360169 on epoch=169
06/24/2022 13:59:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.427380 on epoch=174
06/24/2022 13:59:34 - INFO - __main__ - Global step 350 Train loss 0.413614 ACC 0.625 on epoch=174
06/24/2022 13:59:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.302141 on epoch=179
06/24/2022 13:59:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.391868 on epoch=184
06/24/2022 13:59:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.406099 on epoch=189
06/24/2022 13:59:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.337581 on epoch=194
06/24/2022 13:59:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.303731 on epoch=199
06/24/2022 13:59:47 - INFO - __main__ - Global step 400 Train loss 0.348284 ACC 0.53125 on epoch=199
06/24/2022 13:59:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.366921 on epoch=204
06/24/2022 13:59:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.243128 on epoch=209
06/24/2022 13:59:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.299759 on epoch=214
06/24/2022 13:59:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.325665 on epoch=219
06/24/2022 13:59:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.273588 on epoch=224
06/24/2022 14:00:00 - INFO - __main__ - Global step 450 Train loss 0.301812 ACC 0.5625 on epoch=224
06/24/2022 14:00:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.328066 on epoch=229
06/24/2022 14:00:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.233309 on epoch=234
06/24/2022 14:00:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.302056 on epoch=239
06/24/2022 14:00:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.346647 on epoch=244
06/24/2022 14:00:12 - INFO - __main__ - Step 500 Global step 500 Train loss 1.614201 on epoch=249
06/24/2022 14:00:12 - INFO - __main__ - Global step 500 Train loss 0.564856 ACC 0.34375 on epoch=249
06/24/2022 14:00:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.434680 on epoch=254
06/24/2022 14:00:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.352076 on epoch=259
06/24/2022 14:00:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.362605 on epoch=264
06/24/2022 14:00:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.305475 on epoch=269
06/24/2022 14:00:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.247000 on epoch=274
06/24/2022 14:00:25 - INFO - __main__ - Global step 550 Train loss 0.340367 ACC 0.5 on epoch=274
06/24/2022 14:00:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.464678 on epoch=279
06/24/2022 14:00:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.228514 on epoch=284
06/24/2022 14:00:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.253269 on epoch=289
06/24/2022 14:00:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.300219 on epoch=294
06/24/2022 14:00:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.230227 on epoch=299
06/24/2022 14:00:38 - INFO - __main__ - Global step 600 Train loss 0.295381 ACC 0.53125 on epoch=299
06/24/2022 14:00:38 - INFO - __main__ - save last model!
06/24/2022 14:00:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:00:39 - INFO - __main__ - Printing 3 examples
06/24/2022 14:00:39 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/24/2022 14:00:39 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:00:39 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/24/2022 14:00:39 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:00:39 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/24/2022 14:00:39 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:00:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 14:00:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:00:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 14:00:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:00:39 - INFO - __main__ - Printing 3 examples
06/24/2022 14:00:39 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/24/2022 14:00:39 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:00:39 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/24/2022 14:00:39 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:00:39 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/24/2022 14:00:39 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:00:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:00:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:00:39 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:00:41 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 14:00:41 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 14:00:41 - INFO - __main__ - Printing 3 examples
06/24/2022 14:00:41 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 14:00:41 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:00:41 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 14:00:41 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:00:41 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 14:00:41 - INFO - __main__ - ['duplicate']
06/24/2022 14:00:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:00:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 14:00:43 - INFO - __main__ - Starting training!
06/24/2022 14:01:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:01:41 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 14:09:53 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_100_0.0005_8_predictions.txt
06/24/2022 14:09:53 - INFO - __main__ - ACC on test data: 0.4800
06/24/2022 14:09:53 - INFO - __main__ - prefix=glue-qqp_16_100, lr=0.0005, bsz=8, dev_performance=0.625, test_performance=0.47999010635666584
06/24/2022 14:09:54 - INFO - __main__ - Running ... prefix=glue-qqp_16_100, lr=0.0003, bsz=8 ...
06/24/2022 14:09:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:09:54 - INFO - __main__ - Printing 3 examples
06/24/2022 14:09:54 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/24/2022 14:09:54 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:09:54 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/24/2022 14:09:54 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:09:54 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/24/2022 14:09:54 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:09:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 14:09:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:09:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 14:09:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:09:55 - INFO - __main__ - Printing 3 examples
06/24/2022 14:09:55 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/24/2022 14:09:55 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:09:55 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/24/2022 14:09:55 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:09:55 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/24/2022 14:09:55 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:09:55 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:09:55 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:09:55 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:09:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 14:09:59 - INFO - __main__ - Starting training!
06/24/2022 14:10:01 - INFO - __main__ - Step 10 Global step 10 Train loss 16.887156 on epoch=4
06/24/2022 14:10:03 - INFO - __main__ - Step 20 Global step 20 Train loss 16.200184 on epoch=9
06/24/2022 14:10:06 - INFO - __main__ - Step 30 Global step 30 Train loss 11.123640 on epoch=14
06/24/2022 14:10:08 - INFO - __main__ - Step 40 Global step 40 Train loss 7.037106 on epoch=19
06/24/2022 14:10:11 - INFO - __main__ - Step 50 Global step 50 Train loss 5.316732 on epoch=24
06/24/2022 14:10:12 - INFO - __main__ - Global step 50 Train loss 11.312963 ACC 0.0 on epoch=24
06/24/2022 14:10:14 - INFO - __main__ - Step 60 Global step 60 Train loss 4.348641 on epoch=29
06/24/2022 14:10:17 - INFO - __main__ - Step 70 Global step 70 Train loss 2.915996 on epoch=34
06/24/2022 14:10:20 - INFO - __main__ - Step 80 Global step 80 Train loss 2.287783 on epoch=39
06/24/2022 14:10:22 - INFO - __main__ - Step 90 Global step 90 Train loss 1.519215 on epoch=44
06/24/2022 14:10:25 - INFO - __main__ - Step 100 Global step 100 Train loss 1.232300 on epoch=49
06/24/2022 14:10:25 - INFO - __main__ - Global step 100 Train loss 2.460787 ACC 0.5 on epoch=49
06/24/2022 14:10:28 - INFO - __main__ - Step 110 Global step 110 Train loss 1.122696 on epoch=54
06/24/2022 14:10:31 - INFO - __main__ - Step 120 Global step 120 Train loss 1.082341 on epoch=59
06/24/2022 14:10:33 - INFO - __main__ - Step 130 Global step 130 Train loss 1.277376 on epoch=64
06/24/2022 14:10:36 - INFO - __main__ - Step 140 Global step 140 Train loss 1.077315 on epoch=69
06/24/2022 14:10:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.756129 on epoch=74
06/24/2022 14:10:39 - INFO - __main__ - Global step 150 Train loss 1.063171 ACC 0.5 on epoch=74
06/24/2022 14:10:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.817350 on epoch=79
06/24/2022 14:10:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.483079 on epoch=84
06/24/2022 14:10:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.717925 on epoch=89
06/24/2022 14:10:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.741249 on epoch=94
06/24/2022 14:10:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.545810 on epoch=99
06/24/2022 14:10:52 - INFO - __main__ - Global step 200 Train loss 0.661083 ACC 0.5 on epoch=99
06/24/2022 14:10:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.545648 on epoch=104
06/24/2022 14:10:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.400171 on epoch=109
06/24/2022 14:10:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.494842 on epoch=114
06/24/2022 14:11:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.442466 on epoch=119
06/24/2022 14:11:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.451477 on epoch=124
06/24/2022 14:11:05 - INFO - __main__ - Global step 250 Train loss 0.466921 ACC 0.5625 on epoch=124
06/24/2022 14:11:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.426249 on epoch=129
06/24/2022 14:11:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.555248 on epoch=134
06/24/2022 14:11:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.328521 on epoch=139
06/24/2022 14:11:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.510031 on epoch=144
06/24/2022 14:11:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.426795 on epoch=149
06/24/2022 14:11:18 - INFO - __main__ - Global step 300 Train loss 0.449369 ACC 0.59375 on epoch=149
06/24/2022 14:11:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.342863 on epoch=154
06/24/2022 14:11:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.395894 on epoch=159
06/24/2022 14:11:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.367733 on epoch=164
06/24/2022 14:11:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.380286 on epoch=169
06/24/2022 14:11:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.353487 on epoch=174
06/24/2022 14:11:32 - INFO - __main__ - Global step 350 Train loss 0.368053 ACC 0.5 on epoch=174
06/24/2022 14:11:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.507814 on epoch=179
06/24/2022 14:11:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.264601 on epoch=184
06/24/2022 14:11:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.221931 on epoch=189
06/24/2022 14:11:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.314221 on epoch=194
06/24/2022 14:11:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.300437 on epoch=199
06/24/2022 14:11:45 - INFO - __main__ - Global step 400 Train loss 0.321801 ACC 0.53125 on epoch=199
06/24/2022 14:11:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.262601 on epoch=204
06/24/2022 14:11:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.263718 on epoch=209
06/24/2022 14:11:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.258211 on epoch=214
06/24/2022 14:11:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.244516 on epoch=219
06/24/2022 14:11:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.237818 on epoch=224
06/24/2022 14:11:57 - INFO - __main__ - Global step 450 Train loss 0.253373 ACC 0.5625 on epoch=224
06/24/2022 14:12:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.231482 on epoch=229
06/24/2022 14:12:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.158718 on epoch=234
06/24/2022 14:12:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.287678 on epoch=239
06/24/2022 14:12:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.226998 on epoch=244
06/24/2022 14:12:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.174359 on epoch=249
06/24/2022 14:12:10 - INFO - __main__ - Global step 500 Train loss 0.215847 ACC 0.53125 on epoch=249
06/24/2022 14:12:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.110253 on epoch=254
06/24/2022 14:12:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.140760 on epoch=259
06/24/2022 14:12:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.180117 on epoch=264
06/24/2022 14:12:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.112966 on epoch=269
06/24/2022 14:12:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.120820 on epoch=274
06/24/2022 14:12:23 - INFO - __main__ - Global step 550 Train loss 0.132983 ACC 0.59375 on epoch=274
06/24/2022 14:12:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.080307 on epoch=279
06/24/2022 14:12:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.091048 on epoch=284
06/24/2022 14:12:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.096744 on epoch=289
06/24/2022 14:12:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.077373 on epoch=294
06/24/2022 14:12:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.050335 on epoch=299
06/24/2022 14:12:36 - INFO - __main__ - Global step 600 Train loss 0.079161 ACC 0.5625 on epoch=299
06/24/2022 14:12:36 - INFO - __main__ - save last model!
06/24/2022 14:12:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:12:36 - INFO - __main__ - Printing 3 examples
06/24/2022 14:12:36 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/24/2022 14:12:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:12:36 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/24/2022 14:12:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:12:36 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/24/2022 14:12:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:12:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 14:12:36 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:12:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 14:12:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:12:37 - INFO - __main__ - Printing 3 examples
06/24/2022 14:12:37 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/24/2022 14:12:37 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:12:37 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/24/2022 14:12:37 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:12:37 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/24/2022 14:12:37 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:12:37 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:12:37 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:12:37 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:12:39 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 14:12:39 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 14:12:39 - INFO - __main__ - Printing 3 examples
06/24/2022 14:12:39 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 14:12:39 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:12:39 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 14:12:39 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:12:39 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 14:12:39 - INFO - __main__ - ['duplicate']
06/24/2022 14:12:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:12:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 14:12:41 - INFO - __main__ - Starting training!
06/24/2022 14:12:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:13:39 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 14:21:42 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_100_0.0003_8_predictions.txt
06/24/2022 14:21:42 - INFO - __main__ - ACC on test data: 0.4669
06/24/2022 14:21:42 - INFO - __main__ - prefix=glue-qqp_16_100, lr=0.0003, bsz=8, dev_performance=0.59375, test_performance=0.46688102893890676
06/24/2022 14:21:42 - INFO - __main__ - Running ... prefix=glue-qqp_16_100, lr=0.0002, bsz=8 ...
06/24/2022 14:21:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:21:43 - INFO - __main__ - Printing 3 examples
06/24/2022 14:21:43 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/24/2022 14:21:43 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:21:43 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/24/2022 14:21:43 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:21:43 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/24/2022 14:21:43 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:21:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 14:21:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:21:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 14:21:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:21:43 - INFO - __main__ - Printing 3 examples
06/24/2022 14:21:43 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/24/2022 14:21:43 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:21:43 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/24/2022 14:21:43 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:21:43 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/24/2022 14:21:43 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:21:43 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:21:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:21:43 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:21:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 14:21:48 - INFO - __main__ - Starting training!
06/24/2022 14:21:50 - INFO - __main__ - Step 10 Global step 10 Train loss 16.535854 on epoch=4
06/24/2022 14:21:52 - INFO - __main__ - Step 20 Global step 20 Train loss 15.164297 on epoch=9
06/24/2022 14:21:55 - INFO - __main__ - Step 30 Global step 30 Train loss 11.163489 on epoch=14
06/24/2022 14:21:57 - INFO - __main__ - Step 40 Global step 40 Train loss 8.799891 on epoch=19
06/24/2022 14:22:00 - INFO - __main__ - Step 50 Global step 50 Train loss 6.984374 on epoch=24
06/24/2022 14:22:01 - INFO - __main__ - Global step 50 Train loss 11.729580 ACC 0.0 on epoch=24
06/24/2022 14:22:03 - INFO - __main__ - Step 60 Global step 60 Train loss 6.349067 on epoch=29
06/24/2022 14:22:06 - INFO - __main__ - Step 70 Global step 70 Train loss 5.509421 on epoch=34
06/24/2022 14:22:08 - INFO - __main__ - Step 80 Global step 80 Train loss 4.525224 on epoch=39
06/24/2022 14:22:11 - INFO - __main__ - Step 90 Global step 90 Train loss 3.428813 on epoch=44
06/24/2022 14:22:13 - INFO - __main__ - Step 100 Global step 100 Train loss 3.152977 on epoch=49
06/24/2022 14:22:14 - INFO - __main__ - Global step 100 Train loss 4.593100 ACC 0.3125 on epoch=49
06/24/2022 14:22:17 - INFO - __main__ - Step 110 Global step 110 Train loss 3.126185 on epoch=54
06/24/2022 14:22:19 - INFO - __main__ - Step 120 Global step 120 Train loss 2.001891 on epoch=59
06/24/2022 14:22:22 - INFO - __main__ - Step 130 Global step 130 Train loss 1.751883 on epoch=64
06/24/2022 14:22:24 - INFO - __main__ - Step 140 Global step 140 Train loss 1.975666 on epoch=69
06/24/2022 14:22:27 - INFO - __main__ - Step 150 Global step 150 Train loss 1.469407 on epoch=74
06/24/2022 14:22:27 - INFO - __main__ - Global step 150 Train loss 2.065006 ACC 0.5625 on epoch=74
06/24/2022 14:22:30 - INFO - __main__ - Step 160 Global step 160 Train loss 1.541056 on epoch=79
06/24/2022 14:22:33 - INFO - __main__ - Step 170 Global step 170 Train loss 1.316692 on epoch=84
06/24/2022 14:22:35 - INFO - __main__ - Step 180 Global step 180 Train loss 1.284726 on epoch=89
06/24/2022 14:22:38 - INFO - __main__ - Step 190 Global step 190 Train loss 1.026913 on epoch=94
06/24/2022 14:22:40 - INFO - __main__ - Step 200 Global step 200 Train loss 1.103331 on epoch=99
06/24/2022 14:22:41 - INFO - __main__ - Global step 200 Train loss 1.254544 ACC 0.53125 on epoch=99
06/24/2022 14:22:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.996539 on epoch=104
06/24/2022 14:22:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.617984 on epoch=109
06/24/2022 14:22:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.691367 on epoch=114
06/24/2022 14:22:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.628448 on epoch=119
06/24/2022 14:22:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.860995 on epoch=124
06/24/2022 14:22:54 - INFO - __main__ - Global step 250 Train loss 0.759066 ACC 0.53125 on epoch=124
06/24/2022 14:22:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.530068 on epoch=129
06/24/2022 14:22:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.618163 on epoch=134
06/24/2022 14:23:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.659655 on epoch=139
06/24/2022 14:23:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.647549 on epoch=144
06/24/2022 14:23:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.599426 on epoch=149
06/24/2022 14:23:07 - INFO - __main__ - Global step 300 Train loss 0.610972 ACC 0.53125 on epoch=149
06/24/2022 14:23:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.493219 on epoch=154
06/24/2022 14:23:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.590546 on epoch=159
06/24/2022 14:23:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.564586 on epoch=164
06/24/2022 14:23:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.513277 on epoch=169
06/24/2022 14:23:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.487913 on epoch=174
06/24/2022 14:23:20 - INFO - __main__ - Global step 350 Train loss 0.529908 ACC 0.59375 on epoch=174
06/24/2022 14:23:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.603528 on epoch=179
06/24/2022 14:23:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.525012 on epoch=184
06/24/2022 14:23:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.313351 on epoch=189
06/24/2022 14:23:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.336508 on epoch=194
06/24/2022 14:23:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.425017 on epoch=199
06/24/2022 14:23:33 - INFO - __main__ - Global step 400 Train loss 0.440683 ACC 0.5 on epoch=199
06/24/2022 14:23:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.442318 on epoch=204
06/24/2022 14:23:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.440529 on epoch=209
06/24/2022 14:23:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.429648 on epoch=214
06/24/2022 14:23:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.444383 on epoch=219
06/24/2022 14:23:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.435581 on epoch=224
06/24/2022 14:23:46 - INFO - __main__ - Global step 450 Train loss 0.438492 ACC 0.59375 on epoch=224
06/24/2022 14:23:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.501684 on epoch=229
06/24/2022 14:23:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.437389 on epoch=234
06/24/2022 14:23:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.520823 on epoch=239
06/24/2022 14:23:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.374749 on epoch=244
06/24/2022 14:23:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.365063 on epoch=249
06/24/2022 14:23:59 - INFO - __main__ - Global step 500 Train loss 0.439941 ACC 0.59375 on epoch=249
06/24/2022 14:24:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.540764 on epoch=254
06/24/2022 14:24:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.463851 on epoch=259
06/24/2022 14:24:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.400576 on epoch=264
06/24/2022 14:24:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.501351 on epoch=269
06/24/2022 14:24:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.317598 on epoch=274
06/24/2022 14:24:12 - INFO - __main__ - Global step 550 Train loss 0.444828 ACC 0.53125 on epoch=274
06/24/2022 14:24:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.412351 on epoch=279
06/24/2022 14:24:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.285732 on epoch=284
06/24/2022 14:24:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.424682 on epoch=289
06/24/2022 14:24:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.371354 on epoch=294
06/24/2022 14:24:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.332782 on epoch=299
06/24/2022 14:24:25 - INFO - __main__ - Global step 600 Train loss 0.365380 ACC 0.5 on epoch=299
06/24/2022 14:24:25 - INFO - __main__ - save last model!
06/24/2022 14:24:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:24:26 - INFO - __main__ - Printing 3 examples
06/24/2022 14:24:26 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/24/2022 14:24:26 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:24:26 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/24/2022 14:24:26 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:24:26 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/24/2022 14:24:26 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:24:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 14:24:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:24:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 14:24:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:24:26 - INFO - __main__ - Printing 3 examples
06/24/2022 14:24:26 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/24/2022 14:24:26 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:24:26 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/24/2022 14:24:26 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:24:26 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/24/2022 14:24:26 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:24:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:24:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:24:26 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:24:28 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 14:24:28 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 14:24:28 - INFO - __main__ - Printing 3 examples
06/24/2022 14:24:28 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 14:24:28 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:24:28 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 14:24:28 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:24:28 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 14:24:28 - INFO - __main__ - ['duplicate']
06/24/2022 14:24:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:24:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 14:24:30 - INFO - __main__ - Starting training!
06/24/2022 14:24:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:25:28 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 14:34:06 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_100_0.0002_8_predictions.txt
06/24/2022 14:34:06 - INFO - __main__ - ACC on test data: 0.6100
06/24/2022 14:34:06 - INFO - __main__ - prefix=glue-qqp_16_100, lr=0.0002, bsz=8, dev_performance=0.59375, test_performance=0.609967845659164
06/24/2022 14:34:06 - INFO - __main__ - Running ... prefix=glue-qqp_16_100, lr=0.0001, bsz=8 ...
06/24/2022 14:34:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:34:07 - INFO - __main__ - Printing 3 examples
06/24/2022 14:34:07 - INFO - __main__ -  [glue-qqp] question 1: Calculus required for physics? [SEP] question 2: Can two algebraic structures of different cardinalities be homomorphic?
06/24/2022 14:34:07 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:34:07 - INFO - __main__ -  [glue-qqp] question 1: Why has Thailand never retained all their lost land taken by the French and the English? [SEP] question 2: Why is Thailand called the Land of Smiles?
06/24/2022 14:34:07 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:34:07 - INFO - __main__ -  [glue-qqp] question 1: How do I integrate the Stanford parser in a Java program? [SEP] question 2: Why isn't the Stanford Parser available in CPP?
06/24/2022 14:34:07 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:34:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 14:34:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:34:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 14:34:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:34:07 - INFO - __main__ - Printing 3 examples
06/24/2022 14:34:07 - INFO - __main__ -  [glue-qqp] question 1: Were I can find chicken roasted? [SEP] question 2: How do I roast a chicken?
06/24/2022 14:34:07 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:34:07 - INFO - __main__ -  [glue-qqp] question 1: What are some tips on making it through the job interview process at First Merchants? [SEP] question 2: What are some tips on making it through the job interview process at Lowe's?
06/24/2022 14:34:07 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:34:07 - INFO - __main__ -  [glue-qqp] question 1: If you could only read answers and interact with 10 people on Quora, who would they be? Why? [SEP] question 2: How do I an 18 year old women develop confidence to travel alone?
06/24/2022 14:34:07 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:34:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:34:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:34:07 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:34:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 14:34:11 - INFO - __main__ - Starting training!
06/24/2022 14:34:13 - INFO - __main__ - Step 10 Global step 10 Train loss 16.189739 on epoch=4
06/24/2022 14:34:15 - INFO - __main__ - Step 20 Global step 20 Train loss 15.560335 on epoch=9
06/24/2022 14:34:18 - INFO - __main__ - Step 30 Global step 30 Train loss 13.569592 on epoch=14
06/24/2022 14:34:20 - INFO - __main__ - Step 40 Global step 40 Train loss 11.544287 on epoch=19
06/24/2022 14:34:23 - INFO - __main__ - Step 50 Global step 50 Train loss 9.478109 on epoch=24
06/24/2022 14:34:26 - INFO - __main__ - Global step 50 Train loss 13.268411 ACC 0.0 on epoch=24
06/24/2022 14:34:29 - INFO - __main__ - Step 60 Global step 60 Train loss 8.124002 on epoch=29
06/24/2022 14:34:31 - INFO - __main__ - Step 70 Global step 70 Train loss 6.831346 on epoch=34
06/24/2022 14:34:34 - INFO - __main__ - Step 80 Global step 80 Train loss 7.012691 on epoch=39
06/24/2022 14:34:36 - INFO - __main__ - Step 90 Global step 90 Train loss 6.585606 on epoch=44
06/24/2022 14:34:39 - INFO - __main__ - Step 100 Global step 100 Train loss 6.071810 on epoch=49
06/24/2022 14:34:39 - INFO - __main__ - Global step 100 Train loss 6.925091 ACC 0.0 on epoch=49
06/24/2022 14:34:42 - INFO - __main__ - Step 110 Global step 110 Train loss 5.237969 on epoch=54
06/24/2022 14:34:44 - INFO - __main__ - Step 120 Global step 120 Train loss 5.071631 on epoch=59
06/24/2022 14:34:47 - INFO - __main__ - Step 130 Global step 130 Train loss 3.865374 on epoch=64
06/24/2022 14:34:49 - INFO - __main__ - Step 140 Global step 140 Train loss 4.505693 on epoch=69
06/24/2022 14:34:52 - INFO - __main__ - Step 150 Global step 150 Train loss 3.181048 on epoch=74
06/24/2022 14:34:52 - INFO - __main__ - Global step 150 Train loss 4.372344 ACC 0.28125 on epoch=74
06/24/2022 14:34:55 - INFO - __main__ - Step 160 Global step 160 Train loss 3.104552 on epoch=79
06/24/2022 14:34:58 - INFO - __main__ - Step 170 Global step 170 Train loss 2.616994 on epoch=84
06/24/2022 14:35:00 - INFO - __main__ - Step 180 Global step 180 Train loss 2.751410 on epoch=89
06/24/2022 14:35:03 - INFO - __main__ - Step 190 Global step 190 Train loss 2.189826 on epoch=94
06/24/2022 14:35:06 - INFO - __main__ - Step 200 Global step 200 Train loss 2.998683 on epoch=99
06/24/2022 14:35:06 - INFO - __main__ - Global step 200 Train loss 2.732293 ACC 0.5625 on epoch=99
06/24/2022 14:35:09 - INFO - __main__ - Step 210 Global step 210 Train loss 2.072867 on epoch=104
06/24/2022 14:35:11 - INFO - __main__ - Step 220 Global step 220 Train loss 1.746992 on epoch=109
06/24/2022 14:35:14 - INFO - __main__ - Step 230 Global step 230 Train loss 1.641248 on epoch=114
06/24/2022 14:35:16 - INFO - __main__ - Step 240 Global step 240 Train loss 1.767285 on epoch=119
06/24/2022 14:35:19 - INFO - __main__ - Step 250 Global step 250 Train loss 1.628012 on epoch=124
06/24/2022 14:35:19 - INFO - __main__ - Global step 250 Train loss 1.771281 ACC 0.5 on epoch=124
06/24/2022 14:35:22 - INFO - __main__ - Step 260 Global step 260 Train loss 1.538095 on epoch=129
06/24/2022 14:35:24 - INFO - __main__ - Step 270 Global step 270 Train loss 1.446774 on epoch=134
06/24/2022 14:35:27 - INFO - __main__ - Step 280 Global step 280 Train loss 2.008562 on epoch=139
06/24/2022 14:35:29 - INFO - __main__ - Step 290 Global step 290 Train loss 1.398911 on epoch=144
06/24/2022 14:35:32 - INFO - __main__ - Step 300 Global step 300 Train loss 2.137434 on epoch=149
06/24/2022 14:35:32 - INFO - __main__ - Global step 300 Train loss 1.705955 ACC 0.5 on epoch=149
06/24/2022 14:35:35 - INFO - __main__ - Step 310 Global step 310 Train loss 1.202246 on epoch=154
06/24/2022 14:35:37 - INFO - __main__ - Step 320 Global step 320 Train loss 1.112411 on epoch=159
06/24/2022 14:35:40 - INFO - __main__ - Step 330 Global step 330 Train loss 1.626698 on epoch=164
06/24/2022 14:35:42 - INFO - __main__ - Step 340 Global step 340 Train loss 1.319038 on epoch=169
06/24/2022 14:35:45 - INFO - __main__ - Step 350 Global step 350 Train loss 1.000682 on epoch=174
06/24/2022 14:35:45 - INFO - __main__ - Global step 350 Train loss 1.252215 ACC 0.5 on epoch=174
06/24/2022 14:35:47 - INFO - __main__ - Step 360 Global step 360 Train loss 1.086290 on epoch=179
06/24/2022 14:35:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.646766 on epoch=184
06/24/2022 14:35:52 - INFO - __main__ - Step 380 Global step 380 Train loss 1.337615 on epoch=189
06/24/2022 14:35:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.855681 on epoch=194
06/24/2022 14:35:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.749275 on epoch=199
06/24/2022 14:35:58 - INFO - __main__ - Global step 400 Train loss 0.935125 ACC 0.5 on epoch=199
06/24/2022 14:36:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.873579 on epoch=204
06/24/2022 14:36:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.641311 on epoch=209
06/24/2022 14:36:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.654893 on epoch=214
06/24/2022 14:36:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.657141 on epoch=219
06/24/2022 14:36:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.797447 on epoch=224
06/24/2022 14:36:11 - INFO - __main__ - Global step 450 Train loss 0.724874 ACC 0.53125 on epoch=224
06/24/2022 14:36:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.677585 on epoch=229
06/24/2022 14:36:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.636133 on epoch=234
06/24/2022 14:36:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.729271 on epoch=239
06/24/2022 14:36:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.620518 on epoch=244
06/24/2022 14:36:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.638800 on epoch=249
06/24/2022 14:36:24 - INFO - __main__ - Global step 500 Train loss 0.660461 ACC 0.5 on epoch=249
06/24/2022 14:36:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.537032 on epoch=254
06/24/2022 14:36:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.628672 on epoch=259
06/24/2022 14:36:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.636550 on epoch=264
06/24/2022 14:36:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.453704 on epoch=269
06/24/2022 14:36:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.603317 on epoch=274
06/24/2022 14:36:37 - INFO - __main__ - Global step 550 Train loss 0.571855 ACC 0.5 on epoch=274
06/24/2022 14:36:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.761014 on epoch=279
06/24/2022 14:36:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.907037 on epoch=284
06/24/2022 14:36:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.609023 on epoch=289
06/24/2022 14:36:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.570982 on epoch=294
06/24/2022 14:36:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.651458 on epoch=299
06/24/2022 14:36:50 - INFO - __main__ - Global step 600 Train loss 0.699903 ACC 0.46875 on epoch=299
06/24/2022 14:36:50 - INFO - __main__ - save last model!
06/24/2022 14:36:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:36:50 - INFO - __main__ - Printing 3 examples
06/24/2022 14:36:50 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/24/2022 14:36:50 - INFO - __main__ - ['duplicate']
06/24/2022 14:36:50 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/24/2022 14:36:50 - INFO - __main__ - ['duplicate']
06/24/2022 14:36:50 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/24/2022 14:36:50 - INFO - __main__ - ['duplicate']
06/24/2022 14:36:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 14:36:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:36:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 14:36:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:36:50 - INFO - __main__ - Printing 3 examples
06/24/2022 14:36:50 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/24/2022 14:36:50 - INFO - __main__ - ['duplicate']
06/24/2022 14:36:50 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/24/2022 14:36:50 - INFO - __main__ - ['duplicate']
06/24/2022 14:36:50 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/24/2022 14:36:50 - INFO - __main__ - ['duplicate']
06/24/2022 14:36:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:36:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:36:51 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:36:53 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 14:36:53 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 14:36:53 - INFO - __main__ - Printing 3 examples
06/24/2022 14:36:53 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 14:36:53 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:36:53 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 14:36:53 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:36:53 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 14:36:53 - INFO - __main__ - ['duplicate']
06/24/2022 14:36:53 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:36:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 14:36:55 - INFO - __main__ - Starting training!
06/24/2022 14:37:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:37:52 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 14:45:46 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_100_0.0001_8_predictions.txt
06/24/2022 14:45:46 - INFO - __main__ - ACC on test data: 0.4644
06/24/2022 14:45:46 - INFO - __main__ - prefix=glue-qqp_16_100, lr=0.0001, bsz=8, dev_performance=0.5625, test_performance=0.4643828839970319
06/24/2022 14:45:46 - INFO - __main__ - Running ... prefix=glue-qqp_16_13, lr=0.0005, bsz=8 ...
06/24/2022 14:45:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:45:47 - INFO - __main__ - Printing 3 examples
06/24/2022 14:45:47 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/24/2022 14:45:47 - INFO - __main__ - ['duplicate']
06/24/2022 14:45:47 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/24/2022 14:45:47 - INFO - __main__ - ['duplicate']
06/24/2022 14:45:47 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/24/2022 14:45:47 - INFO - __main__ - ['duplicate']
06/24/2022 14:45:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 14:45:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:45:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 14:45:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:45:47 - INFO - __main__ - Printing 3 examples
06/24/2022 14:45:47 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/24/2022 14:45:47 - INFO - __main__ - ['duplicate']
06/24/2022 14:45:47 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/24/2022 14:45:47 - INFO - __main__ - ['duplicate']
06/24/2022 14:45:47 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/24/2022 14:45:47 - INFO - __main__ - ['duplicate']
06/24/2022 14:45:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:45:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:45:47 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:45:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 14:45:52 - INFO - __main__ - Starting training!
06/24/2022 14:45:54 - INFO - __main__ - Step 10 Global step 10 Train loss 17.808029 on epoch=4
06/24/2022 14:45:56 - INFO - __main__ - Step 20 Global step 20 Train loss 14.009656 on epoch=9
06/24/2022 14:45:59 - INFO - __main__ - Step 30 Global step 30 Train loss 7.630668 on epoch=14
06/24/2022 14:46:01 - INFO - __main__ - Step 40 Global step 40 Train loss 5.110969 on epoch=19
06/24/2022 14:46:04 - INFO - __main__ - Step 50 Global step 50 Train loss 4.050478 on epoch=24
06/24/2022 14:46:04 - INFO - __main__ - Global step 50 Train loss 9.721959 ACC 0.5 on epoch=24
06/24/2022 14:46:07 - INFO - __main__ - Step 60 Global step 60 Train loss 1.933879 on epoch=29
06/24/2022 14:46:09 - INFO - __main__ - Step 70 Global step 70 Train loss 1.788329 on epoch=34
06/24/2022 14:46:12 - INFO - __main__ - Step 80 Global step 80 Train loss 1.837267 on epoch=39
06/24/2022 14:46:14 - INFO - __main__ - Step 90 Global step 90 Train loss 1.791830 on epoch=44
06/24/2022 14:46:17 - INFO - __main__ - Step 100 Global step 100 Train loss 1.153537 on epoch=49
06/24/2022 14:46:17 - INFO - __main__ - Global step 100 Train loss 1.700968 ACC 0.5 on epoch=49
06/24/2022 14:46:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.992342 on epoch=54
06/24/2022 14:46:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.583800 on epoch=59
06/24/2022 14:46:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.535506 on epoch=64
06/24/2022 14:46:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.604803 on epoch=69
06/24/2022 14:46:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.423446 on epoch=74
06/24/2022 14:46:30 - INFO - __main__ - Global step 150 Train loss 0.627979 ACC 0.53125 on epoch=74
06/24/2022 14:46:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.436436 on epoch=79
06/24/2022 14:46:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.624028 on epoch=84
06/24/2022 14:46:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.531421 on epoch=89
06/24/2022 14:46:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.475256 on epoch=94
06/24/2022 14:46:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.603926 on epoch=99
06/24/2022 14:46:43 - INFO - __main__ - Global step 200 Train loss 0.534214 ACC 0.5 on epoch=99
06/24/2022 14:46:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.330582 on epoch=104
06/24/2022 14:46:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.496893 on epoch=109
06/24/2022 14:46:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.468099 on epoch=114
06/24/2022 14:46:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.479015 on epoch=119
06/24/2022 14:46:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.399208 on epoch=124
06/24/2022 14:46:56 - INFO - __main__ - Global step 250 Train loss 0.434759 ACC 0.5 on epoch=124
06/24/2022 14:46:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.323945 on epoch=129
06/24/2022 14:47:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.344972 on epoch=134
06/24/2022 14:47:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.323353 on epoch=139
06/24/2022 14:47:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.270406 on epoch=144
06/24/2022 14:47:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.237658 on epoch=149
06/24/2022 14:47:09 - INFO - __main__ - Global step 300 Train loss 0.300067 ACC 0.6875 on epoch=149
06/24/2022 14:47:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.149765 on epoch=154
06/24/2022 14:47:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.251800 on epoch=159
06/24/2022 14:47:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.106128 on epoch=164
06/24/2022 14:47:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.107992 on epoch=169
06/24/2022 14:47:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.064312 on epoch=174
06/24/2022 14:47:22 - INFO - __main__ - Global step 350 Train loss 0.135999 ACC 0.65625 on epoch=174
06/24/2022 14:47:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.051129 on epoch=179
06/24/2022 14:47:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.033903 on epoch=184
06/24/2022 14:47:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.073977 on epoch=189
06/24/2022 14:47:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.059396 on epoch=194
06/24/2022 14:47:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.035224 on epoch=199
06/24/2022 14:47:35 - INFO - __main__ - Global step 400 Train loss 0.050726 ACC 0.65625 on epoch=199
06/24/2022 14:47:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.078107 on epoch=204
06/24/2022 14:47:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.014339 on epoch=209
06/24/2022 14:47:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.013953 on epoch=214
06/24/2022 14:47:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.024513 on epoch=219
06/24/2022 14:47:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.007126 on epoch=224
06/24/2022 14:47:48 - INFO - __main__ - Global step 450 Train loss 0.027608 ACC 0.6875 on epoch=224
06/24/2022 14:47:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.012634 on epoch=229
06/24/2022 14:47:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.026137 on epoch=234
06/24/2022 14:47:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.018156 on epoch=239
06/24/2022 14:47:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002134 on epoch=244
06/24/2022 14:48:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013432 on epoch=249
06/24/2022 14:48:01 - INFO - __main__ - Global step 500 Train loss 0.014499 ACC 0.65625 on epoch=249
06/24/2022 14:48:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001353 on epoch=254
06/24/2022 14:48:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.153684 on epoch=259
06/24/2022 14:48:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.024246 on epoch=264
06/24/2022 14:48:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004617 on epoch=269
06/24/2022 14:48:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.005261 on epoch=274
06/24/2022 14:48:14 - INFO - __main__ - Global step 550 Train loss 0.037832 ACC 0.6875 on epoch=274
06/24/2022 14:48:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.023931 on epoch=279
06/24/2022 14:48:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004441 on epoch=284
06/24/2022 14:48:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.053440 on epoch=289
06/24/2022 14:48:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.004964 on epoch=294
06/24/2022 14:48:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.066925 on epoch=299
06/24/2022 14:48:27 - INFO - __main__ - Global step 600 Train loss 0.030740 ACC 0.625 on epoch=299
06/24/2022 14:48:27 - INFO - __main__ - save last model!
06/24/2022 14:48:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:48:28 - INFO - __main__ - Printing 3 examples
06/24/2022 14:48:28 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/24/2022 14:48:28 - INFO - __main__ - ['duplicate']
06/24/2022 14:48:28 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/24/2022 14:48:28 - INFO - __main__ - ['duplicate']
06/24/2022 14:48:28 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/24/2022 14:48:28 - INFO - __main__ - ['duplicate']
06/24/2022 14:48:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 14:48:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:48:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 14:48:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:48:28 - INFO - __main__ - Printing 3 examples
06/24/2022 14:48:28 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/24/2022 14:48:28 - INFO - __main__ - ['duplicate']
06/24/2022 14:48:28 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/24/2022 14:48:28 - INFO - __main__ - ['duplicate']
06/24/2022 14:48:28 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/24/2022 14:48:28 - INFO - __main__ - ['duplicate']
06/24/2022 14:48:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:48:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:48:28 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:48:30 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 14:48:31 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 14:48:31 - INFO - __main__ - Printing 3 examples
06/24/2022 14:48:31 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 14:48:31 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:48:31 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 14:48:31 - INFO - __main__ - ['not_duplicate']
06/24/2022 14:48:31 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 14:48:31 - INFO - __main__ - ['duplicate']
06/24/2022 14:48:31 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:48:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 14:48:32 - INFO - __main__ - Starting training!
06/24/2022 14:48:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:49:30 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 14:57:38 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_13_0.0005_8_predictions.txt
06/24/2022 14:57:39 - INFO - __main__ - ACC on test data: 0.5745
06/24/2022 14:57:39 - INFO - __main__ - prefix=glue-qqp_16_13, lr=0.0005, bsz=8, dev_performance=0.6875, test_performance=0.5745238684145436
06/24/2022 14:57:39 - INFO - __main__ - Running ... prefix=glue-qqp_16_13, lr=0.0003, bsz=8 ...
06/24/2022 14:57:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:57:40 - INFO - __main__ - Printing 3 examples
06/24/2022 14:57:40 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/24/2022 14:57:40 - INFO - __main__ - ['duplicate']
06/24/2022 14:57:40 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/24/2022 14:57:40 - INFO - __main__ - ['duplicate']
06/24/2022 14:57:40 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/24/2022 14:57:40 - INFO - __main__ - ['duplicate']
06/24/2022 14:57:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 14:57:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:57:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 14:57:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:57:40 - INFO - __main__ - Printing 3 examples
06/24/2022 14:57:40 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/24/2022 14:57:40 - INFO - __main__ - ['duplicate']
06/24/2022 14:57:40 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/24/2022 14:57:40 - INFO - __main__ - ['duplicate']
06/24/2022 14:57:40 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/24/2022 14:57:40 - INFO - __main__ - ['duplicate']
06/24/2022 14:57:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:57:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:57:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:57:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 14:57:44 - INFO - __main__ - Starting training!
06/24/2022 14:57:46 - INFO - __main__ - Step 10 Global step 10 Train loss 18.347525 on epoch=4
06/24/2022 14:57:49 - INFO - __main__ - Step 20 Global step 20 Train loss 14.524882 on epoch=9
06/24/2022 14:57:51 - INFO - __main__ - Step 30 Global step 30 Train loss 8.980780 on epoch=14
06/24/2022 14:57:54 - INFO - __main__ - Step 40 Global step 40 Train loss 7.557801 on epoch=19
06/24/2022 14:57:56 - INFO - __main__ - Step 50 Global step 50 Train loss 5.668468 on epoch=24
06/24/2022 14:57:56 - INFO - __main__ - Global step 50 Train loss 11.015890 ACC 0.0 on epoch=24
06/24/2022 14:57:59 - INFO - __main__ - Step 60 Global step 60 Train loss 5.334682 on epoch=29
06/24/2022 14:58:02 - INFO - __main__ - Step 70 Global step 70 Train loss 4.293752 on epoch=34
06/24/2022 14:58:04 - INFO - __main__ - Step 80 Global step 80 Train loss 2.651937 on epoch=39
06/24/2022 14:58:07 - INFO - __main__ - Step 90 Global step 90 Train loss 2.604173 on epoch=44
06/24/2022 14:58:09 - INFO - __main__ - Step 100 Global step 100 Train loss 1.542440 on epoch=49
06/24/2022 14:58:10 - INFO - __main__ - Global step 100 Train loss 3.285397 ACC 0.5 on epoch=49
06/24/2022 14:58:12 - INFO - __main__ - Step 110 Global step 110 Train loss 1.340040 on epoch=54
06/24/2022 14:58:15 - INFO - __main__ - Step 120 Global step 120 Train loss 1.118102 on epoch=59
06/24/2022 14:58:17 - INFO - __main__ - Step 130 Global step 130 Train loss 1.551755 on epoch=64
06/24/2022 14:58:20 - INFO - __main__ - Step 140 Global step 140 Train loss 1.194562 on epoch=69
06/24/2022 14:58:22 - INFO - __main__ - Step 150 Global step 150 Train loss 1.234516 on epoch=74
06/24/2022 14:58:23 - INFO - __main__ - Global step 150 Train loss 1.287795 ACC 0.5 on epoch=74
06/24/2022 14:58:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.600011 on epoch=79
06/24/2022 14:58:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.530327 on epoch=84
06/24/2022 14:58:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.624161 on epoch=89
06/24/2022 14:58:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.578493 on epoch=94
06/24/2022 14:58:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.649533 on epoch=99
06/24/2022 14:58:36 - INFO - __main__ - Global step 200 Train loss 0.596505 ACC 0.5 on epoch=99
06/24/2022 14:58:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.461415 on epoch=104
06/24/2022 14:58:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.555869 on epoch=109
06/24/2022 14:58:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.567628 on epoch=114
06/24/2022 14:58:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.480084 on epoch=119
06/24/2022 14:58:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.474290 on epoch=124
06/24/2022 14:58:49 - INFO - __main__ - Global step 250 Train loss 0.507857 ACC 0.5 on epoch=124
06/24/2022 14:58:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.565856 on epoch=129
06/24/2022 14:58:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.381332 on epoch=134
06/24/2022 14:58:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.467095 on epoch=139
06/24/2022 14:58:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.468469 on epoch=144
06/24/2022 14:59:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.510498 on epoch=149
06/24/2022 14:59:02 - INFO - __main__ - Global step 300 Train loss 0.478650 ACC 0.53125 on epoch=149
06/24/2022 14:59:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.465676 on epoch=154
06/24/2022 14:59:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.491921 on epoch=159
06/24/2022 14:59:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.450859 on epoch=164
06/24/2022 14:59:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.435131 on epoch=169
06/24/2022 14:59:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.514848 on epoch=174
06/24/2022 14:59:15 - INFO - __main__ - Global step 350 Train loss 0.471687 ACC 0.53125 on epoch=174
06/24/2022 14:59:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.386891 on epoch=179
06/24/2022 14:59:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.349728 on epoch=184
06/24/2022 14:59:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.344792 on epoch=189
06/24/2022 14:59:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.319961 on epoch=194
06/24/2022 14:59:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.251980 on epoch=199
06/24/2022 14:59:28 - INFO - __main__ - Global step 400 Train loss 0.330670 ACC 0.59375 on epoch=199
06/24/2022 14:59:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.274590 on epoch=204
06/24/2022 14:59:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.197855 on epoch=209
06/24/2022 14:59:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.281091 on epoch=214
06/24/2022 14:59:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.158027 on epoch=219
06/24/2022 14:59:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.136458 on epoch=224
06/24/2022 14:59:41 - INFO - __main__ - Global step 450 Train loss 0.209604 ACC 0.6875 on epoch=224
06/24/2022 14:59:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.107011 on epoch=229
06/24/2022 14:59:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.062524 on epoch=234
06/24/2022 14:59:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.047398 on epoch=239
06/24/2022 14:59:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.037187 on epoch=244
06/24/2022 14:59:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.148701 on epoch=249
06/24/2022 14:59:54 - INFO - __main__ - Global step 500 Train loss 0.080564 ACC 0.6875 on epoch=249
06/24/2022 14:59:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.074257 on epoch=254
06/24/2022 14:59:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.059949 on epoch=259
06/24/2022 15:00:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.060329 on epoch=264
06/24/2022 15:00:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.029303 on epoch=269
06/24/2022 15:00:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.043259 on epoch=274
06/24/2022 15:00:07 - INFO - __main__ - Global step 550 Train loss 0.053419 ACC 0.6875 on epoch=274
06/24/2022 15:00:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.011855 on epoch=279
06/24/2022 15:00:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.115834 on epoch=284
06/24/2022 15:00:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.055238 on epoch=289
06/24/2022 15:00:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.032875 on epoch=294
06/24/2022 15:00:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.006354 on epoch=299
06/24/2022 15:00:20 - INFO - __main__ - Global step 600 Train loss 0.044431 ACC 0.65625 on epoch=299
06/24/2022 15:00:20 - INFO - __main__ - save last model!
06/24/2022 15:00:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:00:21 - INFO - __main__ - Printing 3 examples
06/24/2022 15:00:21 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/24/2022 15:00:21 - INFO - __main__ - ['duplicate']
06/24/2022 15:00:21 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/24/2022 15:00:21 - INFO - __main__ - ['duplicate']
06/24/2022 15:00:21 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/24/2022 15:00:21 - INFO - __main__ - ['duplicate']
06/24/2022 15:00:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 15:00:21 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:00:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 15:00:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:00:21 - INFO - __main__ - Printing 3 examples
06/24/2022 15:00:21 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/24/2022 15:00:21 - INFO - __main__ - ['duplicate']
06/24/2022 15:00:21 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/24/2022 15:00:21 - INFO - __main__ - ['duplicate']
06/24/2022 15:00:21 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/24/2022 15:00:21 - INFO - __main__ - ['duplicate']
06/24/2022 15:00:21 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:00:21 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:00:21 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:00:23 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 15:00:23 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 15:00:23 - INFO - __main__ - Printing 3 examples
06/24/2022 15:00:23 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 15:00:23 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:00:23 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 15:00:23 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:00:23 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 15:00:23 - INFO - __main__ - ['duplicate']
06/24/2022 15:00:23 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:00:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 15:00:25 - INFO - __main__ - Starting training!
06/24/2022 15:00:42 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:01:23 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 15:09:27 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_13_0.0003_8_predictions.txt
06/24/2022 15:09:28 - INFO - __main__ - ACC on test data: 0.5506
06/24/2022 15:09:28 - INFO - __main__ - prefix=glue-qqp_16_13, lr=0.0003, bsz=8, dev_performance=0.6875, test_performance=0.5505565174375464
06/24/2022 15:09:28 - INFO - __main__ - Running ... prefix=glue-qqp_16_13, lr=0.0002, bsz=8 ...
06/24/2022 15:09:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:09:29 - INFO - __main__ - Printing 3 examples
06/24/2022 15:09:29 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/24/2022 15:09:29 - INFO - __main__ - ['duplicate']
06/24/2022 15:09:29 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/24/2022 15:09:29 - INFO - __main__ - ['duplicate']
06/24/2022 15:09:29 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/24/2022 15:09:29 - INFO - __main__ - ['duplicate']
06/24/2022 15:09:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 15:09:29 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:09:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 15:09:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:09:29 - INFO - __main__ - Printing 3 examples
06/24/2022 15:09:29 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/24/2022 15:09:29 - INFO - __main__ - ['duplicate']
06/24/2022 15:09:29 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/24/2022 15:09:29 - INFO - __main__ - ['duplicate']
06/24/2022 15:09:29 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/24/2022 15:09:29 - INFO - __main__ - ['duplicate']
06/24/2022 15:09:29 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:09:29 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:09:29 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:09:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 15:09:33 - INFO - __main__ - Starting training!
06/24/2022 15:09:35 - INFO - __main__ - Step 10 Global step 10 Train loss 17.441948 on epoch=4
06/24/2022 15:09:38 - INFO - __main__ - Step 20 Global step 20 Train loss 15.625302 on epoch=9
06/24/2022 15:09:40 - INFO - __main__ - Step 30 Global step 30 Train loss 11.836142 on epoch=14
06/24/2022 15:09:43 - INFO - __main__ - Step 40 Global step 40 Train loss 8.973022 on epoch=19
06/24/2022 15:09:45 - INFO - __main__ - Step 50 Global step 50 Train loss 8.006848 on epoch=24
06/24/2022 15:09:46 - INFO - __main__ - Global step 50 Train loss 12.376652 ACC 0.0 on epoch=24
06/24/2022 15:09:49 - INFO - __main__ - Step 60 Global step 60 Train loss 7.178580 on epoch=29
06/24/2022 15:09:52 - INFO - __main__ - Step 70 Global step 70 Train loss 6.287191 on epoch=34
06/24/2022 15:09:54 - INFO - __main__ - Step 80 Global step 80 Train loss 5.743422 on epoch=39
06/24/2022 15:09:57 - INFO - __main__ - Step 90 Global step 90 Train loss 4.919099 on epoch=44
06/24/2022 15:09:59 - INFO - __main__ - Step 100 Global step 100 Train loss 3.643871 on epoch=49
06/24/2022 15:10:00 - INFO - __main__ - Global step 100 Train loss 5.554432 ACC 0.3125 on epoch=49
06/24/2022 15:10:03 - INFO - __main__ - Step 110 Global step 110 Train loss 2.786974 on epoch=54
06/24/2022 15:10:05 - INFO - __main__ - Step 120 Global step 120 Train loss 2.777256 on epoch=59
06/24/2022 15:10:08 - INFO - __main__ - Step 130 Global step 130 Train loss 2.250177 on epoch=64
06/24/2022 15:10:10 - INFO - __main__ - Step 140 Global step 140 Train loss 2.178177 on epoch=69
06/24/2022 15:10:13 - INFO - __main__ - Step 150 Global step 150 Train loss 2.132701 on epoch=74
06/24/2022 15:10:13 - INFO - __main__ - Global step 150 Train loss 2.425057 ACC 0.5 on epoch=74
06/24/2022 15:10:16 - INFO - __main__ - Step 160 Global step 160 Train loss 1.905972 on epoch=79
06/24/2022 15:10:19 - INFO - __main__ - Step 170 Global step 170 Train loss 2.072550 on epoch=84
06/24/2022 15:10:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.872406 on epoch=89
06/24/2022 15:10:24 - INFO - __main__ - Step 190 Global step 190 Train loss 1.409978 on epoch=94
06/24/2022 15:10:26 - INFO - __main__ - Step 200 Global step 200 Train loss 1.326227 on epoch=99
06/24/2022 15:10:27 - INFO - __main__ - Global step 200 Train loss 1.517427 ACC 0.5625 on epoch=99
06/24/2022 15:10:29 - INFO - __main__ - Step 210 Global step 210 Train loss 1.030820 on epoch=104
06/24/2022 15:10:32 - INFO - __main__ - Step 220 Global step 220 Train loss 1.173850 on epoch=109
06/24/2022 15:10:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.850318 on epoch=114
06/24/2022 15:10:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.830614 on epoch=119
06/24/2022 15:10:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.643746 on epoch=124
06/24/2022 15:10:40 - INFO - __main__ - Global step 250 Train loss 0.905870 ACC 0.5 on epoch=124
06/24/2022 15:10:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.579520 on epoch=129
06/24/2022 15:10:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.436175 on epoch=134
06/24/2022 15:10:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.661892 on epoch=139
06/24/2022 15:10:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.567019 on epoch=144
06/24/2022 15:10:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.542395 on epoch=149
06/24/2022 15:10:53 - INFO - __main__ - Global step 300 Train loss 0.557400 ACC 0.59375 on epoch=149
06/24/2022 15:10:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.639530 on epoch=154
06/24/2022 15:10:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.457241 on epoch=159
06/24/2022 15:11:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.495538 on epoch=164
06/24/2022 15:11:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.528267 on epoch=169
06/24/2022 15:11:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.483855 on epoch=174
06/24/2022 15:11:06 - INFO - __main__ - Global step 350 Train loss 0.520886 ACC 0.625 on epoch=174
06/24/2022 15:11:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.395787 on epoch=179
06/24/2022 15:11:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.432412 on epoch=184
06/24/2022 15:11:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.288309 on epoch=189
06/24/2022 15:11:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.478069 on epoch=194
06/24/2022 15:11:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.427233 on epoch=199
06/24/2022 15:11:19 - INFO - __main__ - Global step 400 Train loss 0.404362 ACC 0.53125 on epoch=199
06/24/2022 15:11:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.444603 on epoch=204
06/24/2022 15:11:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.483728 on epoch=209
06/24/2022 15:11:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.518825 on epoch=214
06/24/2022 15:11:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.486459 on epoch=219
06/24/2022 15:11:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.506053 on epoch=224
06/24/2022 15:11:32 - INFO - __main__ - Global step 450 Train loss 0.487933 ACC 0.5625 on epoch=224
06/24/2022 15:11:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.586284 on epoch=229
06/24/2022 15:11:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.392552 on epoch=234
06/24/2022 15:11:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.436816 on epoch=239
06/24/2022 15:11:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.548392 on epoch=244
06/24/2022 15:11:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.368601 on epoch=249
06/24/2022 15:11:45 - INFO - __main__ - Global step 500 Train loss 0.466529 ACC 0.5625 on epoch=249
06/24/2022 15:11:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.371558 on epoch=254
06/24/2022 15:11:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.445652 on epoch=259
06/24/2022 15:11:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.468528 on epoch=264
06/24/2022 15:11:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.403924 on epoch=269
06/24/2022 15:11:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.454758 on epoch=274
06/24/2022 15:11:58 - INFO - __main__ - Global step 550 Train loss 0.428884 ACC 0.625 on epoch=274
06/24/2022 15:12:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.387510 on epoch=279
06/24/2022 15:12:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.311899 on epoch=284
06/24/2022 15:12:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.321869 on epoch=289
06/24/2022 15:12:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.335812 on epoch=294
06/24/2022 15:12:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.326769 on epoch=299
06/24/2022 15:12:11 - INFO - __main__ - Global step 600 Train loss 0.336772 ACC 0.625 on epoch=299
06/24/2022 15:12:11 - INFO - __main__ - save last model!
06/24/2022 15:12:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:12:11 - INFO - __main__ - Printing 3 examples
06/24/2022 15:12:11 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/24/2022 15:12:11 - INFO - __main__ - ['duplicate']
06/24/2022 15:12:11 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/24/2022 15:12:11 - INFO - __main__ - ['duplicate']
06/24/2022 15:12:11 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/24/2022 15:12:11 - INFO - __main__ - ['duplicate']
06/24/2022 15:12:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 15:12:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:12:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 15:12:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:12:11 - INFO - __main__ - Printing 3 examples
06/24/2022 15:12:11 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/24/2022 15:12:11 - INFO - __main__ - ['duplicate']
06/24/2022 15:12:11 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/24/2022 15:12:11 - INFO - __main__ - ['duplicate']
06/24/2022 15:12:11 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/24/2022 15:12:11 - INFO - __main__ - ['duplicate']
06/24/2022 15:12:11 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:12:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:12:11 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:12:13 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 15:12:14 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 15:12:14 - INFO - __main__ - Printing 3 examples
06/24/2022 15:12:14 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 15:12:14 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:12:14 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 15:12:14 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:12:14 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 15:12:14 - INFO - __main__ - ['duplicate']
06/24/2022 15:12:14 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:12:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 15:12:16 - INFO - __main__ - Starting training!
06/24/2022 15:12:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:13:13 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 15:21:48 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_13_0.0002_8_predictions.txt
06/24/2022 15:21:48 - INFO - __main__ - ACC on test data: 0.6411
06/24/2022 15:21:49 - INFO - __main__ - prefix=glue-qqp_16_13, lr=0.0002, bsz=8, dev_performance=0.625, test_performance=0.641132822161761
06/24/2022 15:21:49 - INFO - __main__ - Running ... prefix=glue-qqp_16_13, lr=0.0001, bsz=8 ...
06/24/2022 15:21:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:21:49 - INFO - __main__ - Printing 3 examples
06/24/2022 15:21:49 - INFO - __main__ -  [glue-qqp] question 1: Can I develope mobile apps with c++? [SEP] question 2: How can I develop mobile apps with C++?
06/24/2022 15:21:49 - INFO - __main__ - ['duplicate']
06/24/2022 15:21:50 - INFO - __main__ -  [glue-qqp] question 1: What is the disadvantage of option subject anthropology? [SEP] question 2: What are disadvantages of anthropology?
06/24/2022 15:21:50 - INFO - __main__ - ['duplicate']
06/24/2022 15:21:50 - INFO - __main__ -  [glue-qqp] question 1: Why the banning of 500 and 1000 rupees notes? [SEP] question 2: Why did GOI demobilise 500 and 1000 rupee notes?
06/24/2022 15:21:50 - INFO - __main__ - ['duplicate']
06/24/2022 15:21:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 15:21:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:21:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 15:21:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:21:50 - INFO - __main__ - Printing 3 examples
06/24/2022 15:21:50 - INFO - __main__ -  [glue-qqp] question 1: What, according to you, is the best Disney film? [SEP] question 2: What is the best Disney movie?
06/24/2022 15:21:50 - INFO - __main__ - ['duplicate']
06/24/2022 15:21:50 - INFO - __main__ -  [glue-qqp] question 1: How do I stop masturbation and forget women? [SEP] question 2: How can I stop masturbating?
06/24/2022 15:21:50 - INFO - __main__ - ['duplicate']
06/24/2022 15:21:50 - INFO - __main__ -  [glue-qqp] question 1: How do I commit suicide with no pain? [SEP] question 2: What is the best way to commit suicide in India?
06/24/2022 15:21:50 - INFO - __main__ - ['duplicate']
06/24/2022 15:21:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:21:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:21:50 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:21:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 15:21:53 - INFO - __main__ - Starting training!
06/24/2022 15:21:55 - INFO - __main__ - Step 10 Global step 10 Train loss 17.048223 on epoch=4
06/24/2022 15:21:58 - INFO - __main__ - Step 20 Global step 20 Train loss 17.213093 on epoch=9
06/24/2022 15:22:00 - INFO - __main__ - Step 30 Global step 30 Train loss 14.703334 on epoch=14
06/24/2022 15:22:03 - INFO - __main__ - Step 40 Global step 40 Train loss 12.132921 on epoch=19
06/24/2022 15:22:05 - INFO - __main__ - Step 50 Global step 50 Train loss 9.983595 on epoch=24
06/24/2022 15:22:08 - INFO - __main__ - Global step 50 Train loss 14.216234 ACC 0.0 on epoch=24
06/24/2022 15:22:11 - INFO - __main__ - Step 60 Global step 60 Train loss 9.443384 on epoch=29
06/24/2022 15:22:13 - INFO - __main__ - Step 70 Global step 70 Train loss 8.362813 on epoch=34
06/24/2022 15:22:15 - INFO - __main__ - Step 80 Global step 80 Train loss 7.479672 on epoch=39
06/24/2022 15:22:18 - INFO - __main__ - Step 90 Global step 90 Train loss 7.335963 on epoch=44
06/24/2022 15:22:20 - INFO - __main__ - Step 100 Global step 100 Train loss 6.479703 on epoch=49
06/24/2022 15:22:21 - INFO - __main__ - Global step 100 Train loss 7.820308 ACC 0.03125 on epoch=49
06/24/2022 15:22:24 - INFO - __main__ - Step 110 Global step 110 Train loss 6.583454 on epoch=54
06/24/2022 15:22:26 - INFO - __main__ - Step 120 Global step 120 Train loss 6.517649 on epoch=59
06/24/2022 15:22:29 - INFO - __main__ - Step 130 Global step 130 Train loss 5.506632 on epoch=64
06/24/2022 15:22:31 - INFO - __main__ - Step 140 Global step 140 Train loss 5.497891 on epoch=69
06/24/2022 15:22:34 - INFO - __main__ - Step 150 Global step 150 Train loss 4.606159 on epoch=74
06/24/2022 15:22:34 - INFO - __main__ - Global step 150 Train loss 5.742357 ACC 0.1875 on epoch=74
06/24/2022 15:22:37 - INFO - __main__ - Step 160 Global step 160 Train loss 4.419684 on epoch=79
06/24/2022 15:22:39 - INFO - __main__ - Step 170 Global step 170 Train loss 4.291364 on epoch=84
06/24/2022 15:22:42 - INFO - __main__ - Step 180 Global step 180 Train loss 3.994485 on epoch=89
06/24/2022 15:22:44 - INFO - __main__ - Step 190 Global step 190 Train loss 3.685505 on epoch=94
06/24/2022 15:22:47 - INFO - __main__ - Step 200 Global step 200 Train loss 3.500052 on epoch=99
06/24/2022 15:22:47 - INFO - __main__ - Global step 200 Train loss 3.978218 ACC 0.03125 on epoch=99
06/24/2022 15:22:50 - INFO - __main__ - Step 210 Global step 210 Train loss 4.623126 on epoch=104
06/24/2022 15:22:52 - INFO - __main__ - Step 220 Global step 220 Train loss 2.477104 on epoch=109
06/24/2022 15:22:55 - INFO - __main__ - Step 230 Global step 230 Train loss 3.014527 on epoch=114
06/24/2022 15:22:57 - INFO - __main__ - Step 240 Global step 240 Train loss 2.490870 on epoch=119
06/24/2022 15:23:00 - INFO - __main__ - Step 250 Global step 250 Train loss 2.247365 on epoch=124
06/24/2022 15:23:00 - INFO - __main__ - Global step 250 Train loss 2.970598 ACC 0.5 on epoch=124
06/24/2022 15:23:03 - INFO - __main__ - Step 260 Global step 260 Train loss 2.099078 on epoch=129
06/24/2022 15:23:06 - INFO - __main__ - Step 270 Global step 270 Train loss 2.431807 on epoch=134
06/24/2022 15:23:08 - INFO - __main__ - Step 280 Global step 280 Train loss 1.958405 on epoch=139
06/24/2022 15:23:10 - INFO - __main__ - Step 290 Global step 290 Train loss 1.762265 on epoch=144
06/24/2022 15:23:13 - INFO - __main__ - Step 300 Global step 300 Train loss 1.455987 on epoch=149
06/24/2022 15:23:13 - INFO - __main__ - Global step 300 Train loss 1.941508 ACC 0.5 on epoch=149
06/24/2022 15:23:16 - INFO - __main__ - Step 310 Global step 310 Train loss 1.500669 on epoch=154
06/24/2022 15:23:18 - INFO - __main__ - Step 320 Global step 320 Train loss 1.897745 on epoch=159
06/24/2022 15:23:21 - INFO - __main__ - Step 330 Global step 330 Train loss 1.227721 on epoch=164
06/24/2022 15:23:23 - INFO - __main__ - Step 340 Global step 340 Train loss 1.588479 on epoch=169
06/24/2022 15:23:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.775666 on epoch=174
06/24/2022 15:23:26 - INFO - __main__ - Global step 350 Train loss 1.398056 ACC 0.5 on epoch=174
06/24/2022 15:23:28 - INFO - __main__ - Step 360 Global step 360 Train loss 1.421477 on epoch=179
06/24/2022 15:23:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.752504 on epoch=184
06/24/2022 15:23:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.964084 on epoch=189
06/24/2022 15:23:36 - INFO - __main__ - Step 390 Global step 390 Train loss 1.215762 on epoch=194
06/24/2022 15:23:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.841364 on epoch=199
06/24/2022 15:23:39 - INFO - __main__ - Global step 400 Train loss 1.039038 ACC 0.5 on epoch=199
06/24/2022 15:23:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.933409 on epoch=204
06/24/2022 15:23:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.757149 on epoch=209
06/24/2022 15:23:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.872556 on epoch=214
06/24/2022 15:23:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.888469 on epoch=219
06/24/2022 15:23:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.722731 on epoch=224
06/24/2022 15:23:51 - INFO - __main__ - Global step 450 Train loss 0.834863 ACC 0.5 on epoch=224
06/24/2022 15:23:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.626711 on epoch=229
06/24/2022 15:23:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.713060 on epoch=234
06/24/2022 15:23:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.877140 on epoch=239
06/24/2022 15:24:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.622784 on epoch=244
06/24/2022 15:24:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.487990 on epoch=249
06/24/2022 15:24:04 - INFO - __main__ - Global step 500 Train loss 0.665537 ACC 0.5625 on epoch=249
06/24/2022 15:24:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.539097 on epoch=254
06/24/2022 15:24:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.490374 on epoch=259
06/24/2022 15:24:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.494943 on epoch=264
06/24/2022 15:24:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.593674 on epoch=269
06/24/2022 15:24:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.539878 on epoch=274
06/24/2022 15:24:17 - INFO - __main__ - Global step 550 Train loss 0.531593 ACC 0.5625 on epoch=274
06/24/2022 15:24:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.489067 on epoch=279
06/24/2022 15:24:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.487636 on epoch=284
06/24/2022 15:24:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.541589 on epoch=289
06/24/2022 15:24:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.670768 on epoch=294
06/24/2022 15:24:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.642836 on epoch=299
06/24/2022 15:24:30 - INFO - __main__ - Global step 600 Train loss 0.566379 ACC 0.5625 on epoch=299
06/24/2022 15:24:30 - INFO - __main__ - save last model!
06/24/2022 15:24:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:24:31 - INFO - __main__ - Printing 3 examples
06/24/2022 15:24:31 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/24/2022 15:24:31 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:24:31 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/24/2022 15:24:31 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:24:31 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/24/2022 15:24:31 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:24:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 15:24:31 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:24:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 15:24:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:24:31 - INFO - __main__ - Printing 3 examples
06/24/2022 15:24:31 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/24/2022 15:24:31 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:24:31 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/24/2022 15:24:31 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:24:31 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/24/2022 15:24:31 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:24:31 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:24:31 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:24:31 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:24:32 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 15:24:33 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 15:24:33 - INFO - __main__ - Printing 3 examples
06/24/2022 15:24:33 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 15:24:33 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:24:33 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 15:24:33 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:24:33 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 15:24:33 - INFO - __main__ - ['duplicate']
06/24/2022 15:24:33 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:24:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 15:24:35 - INFO - __main__ - Starting training!
06/24/2022 15:24:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:25:32 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 15:34:18 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_13_0.0001_8_predictions.txt
06/24/2022 15:34:18 - INFO - __main__ - ACC on test data: 0.6261
06/24/2022 15:34:19 - INFO - __main__ - prefix=glue-qqp_16_13, lr=0.0001, bsz=8, dev_performance=0.5625, test_performance=0.6261192184021765
06/24/2022 15:34:19 - INFO - __main__ - Running ... prefix=glue-qqp_16_21, lr=0.0005, bsz=8 ...
06/24/2022 15:34:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:34:20 - INFO - __main__ - Printing 3 examples
06/24/2022 15:34:20 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/24/2022 15:34:20 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:34:20 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/24/2022 15:34:20 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:34:20 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/24/2022 15:34:20 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:34:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 15:34:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:34:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 15:34:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:34:20 - INFO - __main__ - Printing 3 examples
06/24/2022 15:34:20 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/24/2022 15:34:20 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:34:20 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/24/2022 15:34:20 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:34:20 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/24/2022 15:34:20 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:34:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:34:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:34:20 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:34:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 15:34:24 - INFO - __main__ - Starting training!
06/24/2022 15:34:26 - INFO - __main__ - Step 10 Global step 10 Train loss 16.906891 on epoch=4
06/24/2022 15:34:28 - INFO - __main__ - Step 20 Global step 20 Train loss 12.927127 on epoch=9
06/24/2022 15:34:31 - INFO - __main__ - Step 30 Global step 30 Train loss 6.940900 on epoch=14
06/24/2022 15:34:33 - INFO - __main__ - Step 40 Global step 40 Train loss 3.663696 on epoch=19
06/24/2022 15:34:36 - INFO - __main__ - Step 50 Global step 50 Train loss 2.484360 on epoch=24
06/24/2022 15:34:36 - INFO - __main__ - Global step 50 Train loss 8.584595 ACC 0.40625 on epoch=24
06/24/2022 15:34:39 - INFO - __main__ - Step 60 Global step 60 Train loss 1.951339 on epoch=29
06/24/2022 15:34:42 - INFO - __main__ - Step 70 Global step 70 Train loss 1.621473 on epoch=34
06/24/2022 15:34:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.919086 on epoch=39
06/24/2022 15:34:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.974251 on epoch=44
06/24/2022 15:34:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.734463 on epoch=49
06/24/2022 15:34:50 - INFO - __main__ - Global step 100 Train loss 1.240122 ACC 0.53125 on epoch=49
06/24/2022 15:34:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.831972 on epoch=54
06/24/2022 15:34:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.565556 on epoch=59
06/24/2022 15:34:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.371829 on epoch=64
06/24/2022 15:35:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.536869 on epoch=69
06/24/2022 15:35:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.480291 on epoch=74
06/24/2022 15:35:03 - INFO - __main__ - Global step 150 Train loss 0.557303 ACC 0.5 on epoch=74
06/24/2022 15:35:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.467886 on epoch=79
06/24/2022 15:35:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.412421 on epoch=84
06/24/2022 15:35:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.441765 on epoch=89
06/24/2022 15:35:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.463151 on epoch=94
06/24/2022 15:35:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.435901 on epoch=99
06/24/2022 15:35:17 - INFO - __main__ - Global step 200 Train loss 0.444225 ACC 0.5 on epoch=99
06/24/2022 15:35:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.433506 on epoch=104
06/24/2022 15:35:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.405891 on epoch=109
06/24/2022 15:35:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.340186 on epoch=114
06/24/2022 15:35:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.396681 on epoch=119
06/24/2022 15:35:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.379200 on epoch=124
06/24/2022 15:35:30 - INFO - __main__ - Global step 250 Train loss 0.391093 ACC 0.5 on epoch=124
06/24/2022 15:35:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.427638 on epoch=129
06/24/2022 15:35:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.431534 on epoch=134
06/24/2022 15:35:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.357760 on epoch=139
06/24/2022 15:35:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.329678 on epoch=144
06/24/2022 15:35:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.311908 on epoch=149
06/24/2022 15:35:43 - INFO - __main__ - Global step 300 Train loss 0.371704 ACC 0.40625 on epoch=149
06/24/2022 15:35:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.297029 on epoch=154
06/24/2022 15:35:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.277025 on epoch=159
06/24/2022 15:35:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.280014 on epoch=164
06/24/2022 15:35:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.325445 on epoch=169
06/24/2022 15:35:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.316989 on epoch=174
06/24/2022 15:35:56 - INFO - __main__ - Global step 350 Train loss 0.299301 ACC 0.53125 on epoch=174
06/24/2022 15:35:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.387178 on epoch=179
06/24/2022 15:36:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.268611 on epoch=184
06/24/2022 15:36:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.256341 on epoch=189
06/24/2022 15:36:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.291239 on epoch=194
06/24/2022 15:36:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.254571 on epoch=199
06/24/2022 15:36:09 - INFO - __main__ - Global step 400 Train loss 0.291588 ACC 0.5 on epoch=199
06/24/2022 15:36:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.290386 on epoch=204
06/24/2022 15:36:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.199999 on epoch=209
06/24/2022 15:36:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.277054 on epoch=214
06/24/2022 15:36:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.268922 on epoch=219
06/24/2022 15:36:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.241066 on epoch=224
06/24/2022 15:36:22 - INFO - __main__ - Global step 450 Train loss 0.255486 ACC 0.5 on epoch=224
06/24/2022 15:36:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.243529 on epoch=229
06/24/2022 15:36:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.243219 on epoch=234
06/24/2022 15:36:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.288805 on epoch=239
06/24/2022 15:36:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.230915 on epoch=244
06/24/2022 15:36:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.236916 on epoch=249
06/24/2022 15:36:35 - INFO - __main__ - Global step 500 Train loss 0.248677 ACC 0.5 on epoch=249
06/24/2022 15:36:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.301683 on epoch=254
06/24/2022 15:36:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.256300 on epoch=259
06/24/2022 15:36:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.300665 on epoch=264
06/24/2022 15:36:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.212631 on epoch=269
06/24/2022 15:36:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.232112 on epoch=274
06/24/2022 15:36:48 - INFO - __main__ - Global step 550 Train loss 0.260678 ACC 0.5 on epoch=274
06/24/2022 15:36:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.201241 on epoch=279
06/24/2022 15:36:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.192831 on epoch=284
06/24/2022 15:36:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.237933 on epoch=289
06/24/2022 15:36:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.185141 on epoch=294
06/24/2022 15:37:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.211137 on epoch=299
06/24/2022 15:37:02 - INFO - __main__ - Global step 600 Train loss 0.205657 ACC 0.5 on epoch=299
06/24/2022 15:37:02 - INFO - __main__ - save last model!
06/24/2022 15:37:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:37:02 - INFO - __main__ - Printing 3 examples
06/24/2022 15:37:02 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/24/2022 15:37:02 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:37:02 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/24/2022 15:37:02 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:37:02 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/24/2022 15:37:02 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:37:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 15:37:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:37:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 15:37:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:37:02 - INFO - __main__ - Printing 3 examples
06/24/2022 15:37:02 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/24/2022 15:37:02 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:37:02 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/24/2022 15:37:02 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:37:02 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/24/2022 15:37:02 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:37:02 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:37:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:37:02 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:37:04 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 15:37:05 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 15:37:05 - INFO - __main__ - Printing 3 examples
06/24/2022 15:37:05 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 15:37:05 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:37:05 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 15:37:05 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:37:05 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 15:37:05 - INFO - __main__ - ['duplicate']
06/24/2022 15:37:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:37:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 15:37:06 - INFO - __main__ - Starting training!
06/24/2022 15:37:23 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:38:04 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 15:46:45 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_21_0.0005_8_predictions.txt
06/24/2022 15:46:46 - INFO - __main__ - ACC on test data: 0.6305
06/24/2022 15:46:46 - INFO - __main__ - prefix=glue-qqp_16_21, lr=0.0005, bsz=8, dev_performance=0.53125, test_performance=0.6305218896858769
06/24/2022 15:46:46 - INFO - __main__ - Running ... prefix=glue-qqp_16_21, lr=0.0003, bsz=8 ...
06/24/2022 15:46:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:46:47 - INFO - __main__ - Printing 3 examples
06/24/2022 15:46:47 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/24/2022 15:46:47 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:46:47 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/24/2022 15:46:47 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:46:47 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/24/2022 15:46:47 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:46:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 15:46:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:46:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 15:46:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:46:47 - INFO - __main__ - Printing 3 examples
06/24/2022 15:46:47 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/24/2022 15:46:47 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:46:47 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/24/2022 15:46:47 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:46:47 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/24/2022 15:46:47 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:46:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:46:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:46:47 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:46:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 15:46:51 - INFO - __main__ - Starting training!
06/24/2022 15:46:53 - INFO - __main__ - Step 10 Global step 10 Train loss 16.711115 on epoch=4
06/24/2022 15:46:56 - INFO - __main__ - Step 20 Global step 20 Train loss 14.645308 on epoch=9
06/24/2022 15:46:58 - INFO - __main__ - Step 30 Global step 30 Train loss 9.874341 on epoch=14
06/24/2022 15:47:01 - INFO - __main__ - Step 40 Global step 40 Train loss 7.166681 on epoch=19
06/24/2022 15:47:03 - INFO - __main__ - Step 50 Global step 50 Train loss 5.965592 on epoch=24
06/24/2022 15:47:04 - INFO - __main__ - Global step 50 Train loss 10.872606 ACC 0.125 on epoch=24
06/24/2022 15:47:06 - INFO - __main__ - Step 60 Global step 60 Train loss 4.480172 on epoch=29
06/24/2022 15:47:09 - INFO - __main__ - Step 70 Global step 70 Train loss 3.668966 on epoch=34
06/24/2022 15:47:12 - INFO - __main__ - Step 80 Global step 80 Train loss 2.696353 on epoch=39
06/24/2022 15:47:14 - INFO - __main__ - Step 90 Global step 90 Train loss 1.933212 on epoch=44
06/24/2022 15:47:17 - INFO - __main__ - Step 100 Global step 100 Train loss 1.905906 on epoch=49
06/24/2022 15:47:17 - INFO - __main__ - Global step 100 Train loss 2.936921 ACC 0.53125 on epoch=49
06/24/2022 15:47:20 - INFO - __main__ - Step 110 Global step 110 Train loss 1.487445 on epoch=54
06/24/2022 15:47:22 - INFO - __main__ - Step 120 Global step 120 Train loss 1.577221 on epoch=59
06/24/2022 15:47:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.891569 on epoch=64
06/24/2022 15:47:28 - INFO - __main__ - Step 140 Global step 140 Train loss 1.019520 on epoch=69
06/24/2022 15:47:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.936505 on epoch=74
06/24/2022 15:47:30 - INFO - __main__ - Global step 150 Train loss 1.182452 ACC 0.5625 on epoch=74
06/24/2022 15:47:33 - INFO - __main__ - Step 160 Global step 160 Train loss 1.058253 on epoch=79
06/24/2022 15:47:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.745140 on epoch=84
06/24/2022 15:47:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.569837 on epoch=89
06/24/2022 15:47:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.564249 on epoch=94
06/24/2022 15:47:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.596372 on epoch=99
06/24/2022 15:47:44 - INFO - __main__ - Global step 200 Train loss 0.706770 ACC 0.5 on epoch=99
06/24/2022 15:47:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.522865 on epoch=104
06/24/2022 15:47:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.437099 on epoch=109
06/24/2022 15:47:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.598473 on epoch=114
06/24/2022 15:47:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.391773 on epoch=119
06/24/2022 15:47:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.379785 on epoch=124
06/24/2022 15:47:57 - INFO - __main__ - Global step 250 Train loss 0.465999 ACC 0.5 on epoch=124
06/24/2022 15:47:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.438323 on epoch=129
06/24/2022 15:48:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.459231 on epoch=134
06/24/2022 15:48:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.389406 on epoch=139
06/24/2022 15:48:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.314328 on epoch=144
06/24/2022 15:48:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.530123 on epoch=149
06/24/2022 15:48:10 - INFO - __main__ - Global step 300 Train loss 0.426282 ACC 0.5 on epoch=149
06/24/2022 15:48:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.297557 on epoch=154
06/24/2022 15:48:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.560957 on epoch=159
06/24/2022 15:48:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.451982 on epoch=164
06/24/2022 15:48:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.455846 on epoch=169
06/24/2022 15:48:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.536467 on epoch=174
06/24/2022 15:48:23 - INFO - __main__ - Global step 350 Train loss 0.460562 ACC 0.4375 on epoch=174
06/24/2022 15:48:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.326070 on epoch=179
06/24/2022 15:48:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.386740 on epoch=184
06/24/2022 15:48:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.430511 on epoch=189
06/24/2022 15:48:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.405389 on epoch=194
06/24/2022 15:48:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.324603 on epoch=199
06/24/2022 15:48:36 - INFO - __main__ - Global step 400 Train loss 0.374663 ACC 0.4375 on epoch=199
06/24/2022 15:48:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.319645 on epoch=204
06/24/2022 15:48:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.489072 on epoch=209
06/24/2022 15:48:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.285079 on epoch=214
06/24/2022 15:48:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.403812 on epoch=219
06/24/2022 15:48:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.370911 on epoch=224
06/24/2022 15:48:48 - INFO - __main__ - Global step 450 Train loss 0.373704 ACC 0.46875 on epoch=224
06/24/2022 15:48:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.324581 on epoch=229
06/24/2022 15:48:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.313697 on epoch=234
06/24/2022 15:48:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.342437 on epoch=239
06/24/2022 15:48:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.231967 on epoch=244
06/24/2022 15:49:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.233505 on epoch=249
06/24/2022 15:49:01 - INFO - __main__ - Global step 500 Train loss 0.289237 ACC 0.5 on epoch=249
06/24/2022 15:49:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.226418 on epoch=254
06/24/2022 15:49:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.281437 on epoch=259
06/24/2022 15:49:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.160327 on epoch=264
06/24/2022 15:49:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.183980 on epoch=269
06/24/2022 15:49:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.061290 on epoch=274
06/24/2022 15:49:14 - INFO - __main__ - Global step 550 Train loss 0.182690 ACC 0.46875 on epoch=274
06/24/2022 15:49:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.138167 on epoch=279
06/24/2022 15:49:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.065129 on epoch=284
06/24/2022 15:49:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.069278 on epoch=289
06/24/2022 15:49:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.048122 on epoch=294
06/24/2022 15:49:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.077526 on epoch=299
06/24/2022 15:49:27 - INFO - __main__ - Global step 600 Train loss 0.079645 ACC 0.4375 on epoch=299
06/24/2022 15:49:27 - INFO - __main__ - save last model!
06/24/2022 15:49:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:49:28 - INFO - __main__ - Printing 3 examples
06/24/2022 15:49:28 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/24/2022 15:49:28 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:49:28 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/24/2022 15:49:28 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:49:28 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/24/2022 15:49:28 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:49:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 15:49:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:49:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 15:49:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:49:28 - INFO - __main__ - Printing 3 examples
06/24/2022 15:49:28 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/24/2022 15:49:28 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:49:28 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/24/2022 15:49:28 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:49:28 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/24/2022 15:49:28 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:49:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:49:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:49:28 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:49:30 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 15:49:30 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 15:49:30 - INFO - __main__ - Printing 3 examples
06/24/2022 15:49:30 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 15:49:30 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:49:30 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 15:49:30 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:49:30 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 15:49:30 - INFO - __main__ - ['duplicate']
06/24/2022 15:49:30 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:49:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 15:49:32 - INFO - __main__ - Starting training!
06/24/2022 15:49:48 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:50:30 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 15:58:49 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_21_0.0003_8_predictions.txt
06/24/2022 15:58:49 - INFO - __main__ - ACC on test data: 0.5500
06/24/2022 15:58:49 - INFO - __main__ - prefix=glue-qqp_16_21, lr=0.0003, bsz=8, dev_performance=0.5625, test_performance=0.5499876329458323
06/24/2022 15:58:49 - INFO - __main__ - Running ... prefix=glue-qqp_16_21, lr=0.0002, bsz=8 ...
06/24/2022 15:58:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:58:50 - INFO - __main__ - Printing 3 examples
06/24/2022 15:58:50 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/24/2022 15:58:50 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:58:50 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/24/2022 15:58:50 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:58:50 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/24/2022 15:58:50 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:58:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 15:58:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:58:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 15:58:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:58:50 - INFO - __main__ - Printing 3 examples
06/24/2022 15:58:50 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/24/2022 15:58:50 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:58:50 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/24/2022 15:58:50 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:58:50 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/24/2022 15:58:50 - INFO - __main__ - ['not_duplicate']
06/24/2022 15:58:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:58:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:58:51 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:58:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 15:58:55 - INFO - __main__ - Starting training!
06/24/2022 15:58:57 - INFO - __main__ - Step 10 Global step 10 Train loss 16.904242 on epoch=4
06/24/2022 15:58:59 - INFO - __main__ - Step 20 Global step 20 Train loss 15.445036 on epoch=9
06/24/2022 15:59:02 - INFO - __main__ - Step 30 Global step 30 Train loss 11.643650 on epoch=14
06/24/2022 15:59:04 - INFO - __main__ - Step 40 Global step 40 Train loss 8.279547 on epoch=19
06/24/2022 15:59:07 - INFO - __main__ - Step 50 Global step 50 Train loss 7.389153 on epoch=24
06/24/2022 15:59:09 - INFO - __main__ - Global step 50 Train loss 11.932326 ACC 0.0625 on epoch=24
06/24/2022 15:59:11 - INFO - __main__ - Step 60 Global step 60 Train loss 5.897437 on epoch=29
06/24/2022 15:59:14 - INFO - __main__ - Step 70 Global step 70 Train loss 4.655948 on epoch=34
06/24/2022 15:59:16 - INFO - __main__ - Step 80 Global step 80 Train loss 4.311999 on epoch=39
06/24/2022 15:59:19 - INFO - __main__ - Step 90 Global step 90 Train loss 3.403813 on epoch=44
06/24/2022 15:59:21 - INFO - __main__ - Step 100 Global step 100 Train loss 3.925414 on epoch=49
06/24/2022 15:59:22 - INFO - __main__ - Global step 100 Train loss 4.438921 ACC 0.3125 on epoch=49
06/24/2022 15:59:25 - INFO - __main__ - Step 110 Global step 110 Train loss 2.319536 on epoch=54
06/24/2022 15:59:27 - INFO - __main__ - Step 120 Global step 120 Train loss 1.824198 on epoch=59
06/24/2022 15:59:30 - INFO - __main__ - Step 130 Global step 130 Train loss 1.543493 on epoch=64
06/24/2022 15:59:32 - INFO - __main__ - Step 140 Global step 140 Train loss 1.771474 on epoch=69
06/24/2022 15:59:35 - INFO - __main__ - Step 150 Global step 150 Train loss 1.456843 on epoch=74
06/24/2022 15:59:35 - INFO - __main__ - Global step 150 Train loss 1.783109 ACC 0.46875 on epoch=74
06/24/2022 15:59:38 - INFO - __main__ - Step 160 Global step 160 Train loss 1.306478 on epoch=79
06/24/2022 15:59:41 - INFO - __main__ - Step 170 Global step 170 Train loss 1.042063 on epoch=84
06/24/2022 15:59:43 - INFO - __main__ - Step 180 Global step 180 Train loss 1.458735 on epoch=89
06/24/2022 15:59:46 - INFO - __main__ - Step 190 Global step 190 Train loss 1.465225 on epoch=94
06/24/2022 15:59:48 - INFO - __main__ - Step 200 Global step 200 Train loss 1.045065 on epoch=99
06/24/2022 15:59:48 - INFO - __main__ - Global step 200 Train loss 1.263513 ACC 0.5 on epoch=99
06/24/2022 15:59:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.886789 on epoch=104
06/24/2022 15:59:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.772296 on epoch=109
06/24/2022 15:59:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.652482 on epoch=114
06/24/2022 15:59:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.547762 on epoch=119
06/24/2022 16:00:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.648001 on epoch=124
06/24/2022 16:00:02 - INFO - __main__ - Global step 250 Train loss 0.701466 ACC 0.53125 on epoch=124
06/24/2022 16:00:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.529595 on epoch=129
06/24/2022 16:00:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.448170 on epoch=134
06/24/2022 16:00:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.598836 on epoch=139
06/24/2022 16:00:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.559248 on epoch=144
06/24/2022 16:00:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.497799 on epoch=149
06/24/2022 16:00:15 - INFO - __main__ - Global step 300 Train loss 0.526730 ACC 0.4375 on epoch=149
06/24/2022 16:00:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.447489 on epoch=154
06/24/2022 16:00:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.400409 on epoch=159
06/24/2022 16:00:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.488800 on epoch=164
06/24/2022 16:00:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.577476 on epoch=169
06/24/2022 16:00:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.449514 on epoch=174
06/24/2022 16:00:28 - INFO - __main__ - Global step 350 Train loss 0.472738 ACC 0.59375 on epoch=174
06/24/2022 16:00:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.492994 on epoch=179
06/24/2022 16:00:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.570183 on epoch=184
06/24/2022 16:00:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.450479 on epoch=189
06/24/2022 16:00:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.538033 on epoch=194
06/24/2022 16:00:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.458981 on epoch=199
06/24/2022 16:00:41 - INFO - __main__ - Global step 400 Train loss 0.502134 ACC 0.53125 on epoch=199
06/24/2022 16:00:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.418802 on epoch=204
06/24/2022 16:00:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.423993 on epoch=209
06/24/2022 16:00:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.458460 on epoch=214
06/24/2022 16:00:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.442456 on epoch=219
06/24/2022 16:00:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.288373 on epoch=224
06/24/2022 16:00:54 - INFO - __main__ - Global step 450 Train loss 0.406417 ACC 0.59375 on epoch=224
06/24/2022 16:00:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.425674 on epoch=229
06/24/2022 16:00:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.431544 on epoch=234
06/24/2022 16:01:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.487096 on epoch=239
06/24/2022 16:01:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.344257 on epoch=244
06/24/2022 16:01:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.368399 on epoch=249
06/24/2022 16:01:07 - INFO - __main__ - Global step 500 Train loss 0.411394 ACC 0.65625 on epoch=249
06/24/2022 16:01:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.304823 on epoch=254
06/24/2022 16:01:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.375823 on epoch=259
06/24/2022 16:01:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.370913 on epoch=264
06/24/2022 16:01:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.388374 on epoch=269
06/24/2022 16:01:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.318850 on epoch=274
06/24/2022 16:01:20 - INFO - __main__ - Global step 550 Train loss 0.351757 ACC 0.53125 on epoch=274
06/24/2022 16:01:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.412083 on epoch=279
06/24/2022 16:01:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.364937 on epoch=284
06/24/2022 16:01:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.343307 on epoch=289
06/24/2022 16:01:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.395311 on epoch=294
06/24/2022 16:01:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.303805 on epoch=299
06/24/2022 16:01:33 - INFO - __main__ - Global step 600 Train loss 0.363889 ACC 0.5 on epoch=299
06/24/2022 16:01:33 - INFO - __main__ - save last model!
06/24/2022 16:01:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:01:33 - INFO - __main__ - Printing 3 examples
06/24/2022 16:01:33 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/24/2022 16:01:33 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:01:33 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/24/2022 16:01:33 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:01:33 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/24/2022 16:01:33 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:01:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 16:01:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:01:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 16:01:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:01:34 - INFO - __main__ - Printing 3 examples
06/24/2022 16:01:34 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/24/2022 16:01:34 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:01:34 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/24/2022 16:01:34 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:01:34 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/24/2022 16:01:34 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:01:34 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:01:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:01:34 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:01:35 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 16:01:36 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 16:01:36 - INFO - __main__ - Printing 3 examples
06/24/2022 16:01:36 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 16:01:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:01:36 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 16:01:36 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:01:36 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 16:01:36 - INFO - __main__ - ['duplicate']
06/24/2022 16:01:36 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:01:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 16:01:38 - INFO - __main__ - Starting training!
06/24/2022 16:01:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:02:35 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 16:11:02 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_21_0.0002_8_predictions.txt
06/24/2022 16:11:02 - INFO - __main__ - ACC on test data: 0.6352
06/24/2022 16:11:02 - INFO - __main__ - prefix=glue-qqp_16_21, lr=0.0002, bsz=8, dev_performance=0.65625, test_performance=0.6352213702696018
06/24/2022 16:11:02 - INFO - __main__ - Running ... prefix=glue-qqp_16_21, lr=0.0001, bsz=8 ...
06/24/2022 16:11:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:11:03 - INFO - __main__ - Printing 3 examples
06/24/2022 16:11:03 - INFO - __main__ -  [glue-qqp] question 1: How long before the space shuttle Challenger explosion/disaster would the crew on board have known they were going to die? [SEP] question 2: Did the Columbia crew in the shuttle know that they were going to die during the last fifteen minutes?
06/24/2022 16:11:03 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:11:03 - INFO - __main__ -  [glue-qqp] question 1: What's the best strategy for new products launch? [SEP] question 2: What should be the right strategy to launch a new product in the FMCG market in the lamp category?
06/24/2022 16:11:03 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:11:03 - INFO - __main__ -  [glue-qqp] question 1: Which one should I buy, a GoPro or DSLR camera? [SEP] question 2: Should I buy Nikon DSLR camera from Amazon?
06/24/2022 16:11:03 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:11:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 16:11:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:11:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 16:11:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:11:03 - INFO - __main__ - Printing 3 examples
06/24/2022 16:11:03 - INFO - __main__ -  [glue-qqp] question 1: What are some mind-blowing facts about Yahoo? [SEP] question 2: What are the mind-blowing facts about Yahoo?
06/24/2022 16:11:03 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:11:03 - INFO - __main__ -  [glue-qqp] question 1: Which is the best book for investment in mutual funds in India? [SEP] question 2: Who is the best mutual fund manager in India?
06/24/2022 16:11:03 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:11:03 - INFO - __main__ -  [glue-qqp] question 1: What are different types of yogurt and how are they different? [SEP] question 2: What are the differences between Greek yogurt and normal yogurt?
06/24/2022 16:11:03 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:11:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:11:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:11:03 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:11:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 16:11:08 - INFO - __main__ - Starting training!
06/24/2022 16:11:10 - INFO - __main__ - Step 10 Global step 10 Train loss 17.227852 on epoch=4
06/24/2022 16:11:12 - INFO - __main__ - Step 20 Global step 20 Train loss 15.653491 on epoch=9
06/24/2022 16:11:15 - INFO - __main__ - Step 30 Global step 30 Train loss 14.206694 on epoch=14
06/24/2022 16:11:17 - INFO - __main__ - Step 40 Global step 40 Train loss 12.231731 on epoch=19
06/24/2022 16:11:20 - INFO - __main__ - Step 50 Global step 50 Train loss 10.447841 on epoch=24
06/24/2022 16:11:23 - INFO - __main__ - Global step 50 Train loss 13.953522 ACC 0.0 on epoch=24
06/24/2022 16:11:25 - INFO - __main__ - Step 60 Global step 60 Train loss 8.330554 on epoch=29
06/24/2022 16:11:28 - INFO - __main__ - Step 70 Global step 70 Train loss 7.362733 on epoch=34
06/24/2022 16:11:30 - INFO - __main__ - Step 80 Global step 80 Train loss 7.155339 on epoch=39
06/24/2022 16:11:33 - INFO - __main__ - Step 90 Global step 90 Train loss 6.245553 on epoch=44
06/24/2022 16:11:35 - INFO - __main__ - Step 100 Global step 100 Train loss 5.515358 on epoch=49
06/24/2022 16:11:36 - INFO - __main__ - Global step 100 Train loss 6.921907 ACC 0.0 on epoch=49
06/24/2022 16:11:39 - INFO - __main__ - Step 110 Global step 110 Train loss 5.325627 on epoch=54
06/24/2022 16:11:41 - INFO - __main__ - Step 120 Global step 120 Train loss 4.819701 on epoch=59
06/24/2022 16:11:44 - INFO - __main__ - Step 130 Global step 130 Train loss 4.048008 on epoch=64
06/24/2022 16:11:46 - INFO - __main__ - Step 140 Global step 140 Train loss 3.952873 on epoch=69
06/24/2022 16:11:49 - INFO - __main__ - Step 150 Global step 150 Train loss 3.422978 on epoch=74
06/24/2022 16:11:50 - INFO - __main__ - Global step 150 Train loss 4.313838 ACC 0.1875 on epoch=74
06/24/2022 16:11:53 - INFO - __main__ - Step 160 Global step 160 Train loss 2.858120 on epoch=79
06/24/2022 16:11:55 - INFO - __main__ - Step 170 Global step 170 Train loss 3.321761 on epoch=84
06/24/2022 16:11:58 - INFO - __main__ - Step 180 Global step 180 Train loss 2.629620 on epoch=89
06/24/2022 16:12:00 - INFO - __main__ - Step 190 Global step 190 Train loss 3.033480 on epoch=94
06/24/2022 16:12:03 - INFO - __main__ - Step 200 Global step 200 Train loss 2.074773 on epoch=99
06/24/2022 16:12:03 - INFO - __main__ - Global step 200 Train loss 2.783551 ACC 0.5 on epoch=99
06/24/2022 16:12:06 - INFO - __main__ - Step 210 Global step 210 Train loss 2.414212 on epoch=104
06/24/2022 16:12:09 - INFO - __main__ - Step 220 Global step 220 Train loss 2.405377 on epoch=109
06/24/2022 16:12:11 - INFO - __main__ - Step 230 Global step 230 Train loss 1.945688 on epoch=114
06/24/2022 16:12:14 - INFO - __main__ - Step 240 Global step 240 Train loss 2.367150 on epoch=119
06/24/2022 16:12:16 - INFO - __main__ - Step 250 Global step 250 Train loss 1.750546 on epoch=124
06/24/2022 16:12:16 - INFO - __main__ - Global step 250 Train loss 2.176595 ACC 0.5 on epoch=124
06/24/2022 16:12:19 - INFO - __main__ - Step 260 Global step 260 Train loss 2.041129 on epoch=129
06/24/2022 16:12:22 - INFO - __main__ - Step 270 Global step 270 Train loss 1.744752 on epoch=134
06/24/2022 16:12:24 - INFO - __main__ - Step 280 Global step 280 Train loss 1.653888 on epoch=139
06/24/2022 16:12:27 - INFO - __main__ - Step 290 Global step 290 Train loss 1.177006 on epoch=144
06/24/2022 16:12:29 - INFO - __main__ - Step 300 Global step 300 Train loss 1.652546 on epoch=149
06/24/2022 16:12:29 - INFO - __main__ - Global step 300 Train loss 1.653864 ACC 0.5 on epoch=149
06/24/2022 16:12:32 - INFO - __main__ - Step 310 Global step 310 Train loss 1.048626 on epoch=154
06/24/2022 16:12:34 - INFO - __main__ - Step 320 Global step 320 Train loss 1.322046 on epoch=159
06/24/2022 16:12:37 - INFO - __main__ - Step 330 Global step 330 Train loss 1.317379 on epoch=164
06/24/2022 16:12:39 - INFO - __main__ - Step 340 Global step 340 Train loss 1.285466 on epoch=169
06/24/2022 16:12:42 - INFO - __main__ - Step 350 Global step 350 Train loss 1.383721 on epoch=174
06/24/2022 16:12:42 - INFO - __main__ - Global step 350 Train loss 1.271447 ACC 0.5 on epoch=174
06/24/2022 16:12:45 - INFO - __main__ - Step 360 Global step 360 Train loss 1.145703 on epoch=179
06/24/2022 16:12:47 - INFO - __main__ - Step 370 Global step 370 Train loss 1.018809 on epoch=184
06/24/2022 16:12:50 - INFO - __main__ - Step 380 Global step 380 Train loss 1.344161 on epoch=189
06/24/2022 16:12:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.788078 on epoch=194
06/24/2022 16:12:55 - INFO - __main__ - Step 400 Global step 400 Train loss 1.005222 on epoch=199
06/24/2022 16:12:55 - INFO - __main__ - Global step 400 Train loss 1.060395 ACC 0.5 on epoch=199
06/24/2022 16:12:57 - INFO - __main__ - Step 410 Global step 410 Train loss 1.356463 on epoch=204
06/24/2022 16:13:00 - INFO - __main__ - Step 420 Global step 420 Train loss 1.103884 on epoch=209
06/24/2022 16:13:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.987526 on epoch=214
06/24/2022 16:13:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.891916 on epoch=219
06/24/2022 16:13:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.733650 on epoch=224
06/24/2022 16:13:08 - INFO - __main__ - Global step 450 Train loss 1.014688 ACC 0.5 on epoch=224
06/24/2022 16:13:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.883192 on epoch=229
06/24/2022 16:13:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.706835 on epoch=234
06/24/2022 16:13:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.504932 on epoch=239
06/24/2022 16:13:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.585229 on epoch=244
06/24/2022 16:13:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.821025 on epoch=249
06/24/2022 16:13:20 - INFO - __main__ - Global step 500 Train loss 0.700243 ACC 0.53125 on epoch=249
06/24/2022 16:13:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.464662 on epoch=254
06/24/2022 16:13:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.581342 on epoch=259
06/24/2022 16:13:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.628949 on epoch=264
06/24/2022 16:13:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.643677 on epoch=269
06/24/2022 16:13:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.721966 on epoch=274
06/24/2022 16:13:33 - INFO - __main__ - Global step 550 Train loss 0.608119 ACC 0.46875 on epoch=274
06/24/2022 16:13:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.702242 on epoch=279
06/24/2022 16:13:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.644148 on epoch=284
06/24/2022 16:13:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.570220 on epoch=289
06/24/2022 16:13:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.657247 on epoch=294
06/24/2022 16:13:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.655185 on epoch=299
06/24/2022 16:13:46 - INFO - __main__ - Global step 600 Train loss 0.645808 ACC 0.5625 on epoch=299
06/24/2022 16:13:46 - INFO - __main__ - save last model!
06/24/2022 16:13:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:13:47 - INFO - __main__ - Printing 3 examples
06/24/2022 16:13:47 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 16:13:47 - INFO - __main__ - ['duplicate']
06/24/2022 16:13:47 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 16:13:47 - INFO - __main__ - ['duplicate']
06/24/2022 16:13:47 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 16:13:47 - INFO - __main__ - ['duplicate']
06/24/2022 16:13:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 16:13:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:13:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 16:13:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:13:47 - INFO - __main__ - Printing 3 examples
06/24/2022 16:13:47 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 16:13:47 - INFO - __main__ - ['duplicate']
06/24/2022 16:13:47 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 16:13:47 - INFO - __main__ - ['duplicate']
06/24/2022 16:13:47 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 16:13:47 - INFO - __main__ - ['duplicate']
06/24/2022 16:13:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:13:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:13:47 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:13:49 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 16:13:49 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 16:13:49 - INFO - __main__ - Printing 3 examples
06/24/2022 16:13:49 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 16:13:49 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:13:49 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 16:13:49 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:13:49 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 16:13:49 - INFO - __main__ - ['duplicate']
06/24/2022 16:13:49 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:13:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 16:13:51 - INFO - __main__ - Starting training!
06/24/2022 16:14:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:14:49 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 16:23:33 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_21_0.0001_8_predictions.txt
06/24/2022 16:23:33 - INFO - __main__ - ACC on test data: 0.6242
06/24/2022 16:23:33 - INFO - __main__ - prefix=glue-qqp_16_21, lr=0.0001, bsz=8, dev_performance=0.5625, test_performance=0.6241899579520158
06/24/2022 16:23:33 - INFO - __main__ - Running ... prefix=glue-qqp_16_42, lr=0.0005, bsz=8 ...
06/24/2022 16:23:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:23:34 - INFO - __main__ - Printing 3 examples
06/24/2022 16:23:34 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 16:23:34 - INFO - __main__ - ['duplicate']
06/24/2022 16:23:34 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 16:23:34 - INFO - __main__ - ['duplicate']
06/24/2022 16:23:34 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 16:23:34 - INFO - __main__ - ['duplicate']
06/24/2022 16:23:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 16:23:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:23:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 16:23:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:23:34 - INFO - __main__ - Printing 3 examples
06/24/2022 16:23:34 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 16:23:34 - INFO - __main__ - ['duplicate']
06/24/2022 16:23:34 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 16:23:34 - INFO - __main__ - ['duplicate']
06/24/2022 16:23:34 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 16:23:34 - INFO - __main__ - ['duplicate']
06/24/2022 16:23:34 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:23:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:23:34 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:23:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 16:23:39 - INFO - __main__ - Starting training!
06/24/2022 16:23:41 - INFO - __main__ - Step 10 Global step 10 Train loss 18.020855 on epoch=4
06/24/2022 16:23:43 - INFO - __main__ - Step 20 Global step 20 Train loss 14.658142 on epoch=9
06/24/2022 16:23:46 - INFO - __main__ - Step 30 Global step 30 Train loss 9.267756 on epoch=14
06/24/2022 16:23:48 - INFO - __main__ - Step 40 Global step 40 Train loss 6.481321 on epoch=19
06/24/2022 16:23:50 - INFO - __main__ - Step 50 Global step 50 Train loss 5.360723 on epoch=24
06/24/2022 16:23:51 - INFO - __main__ - Global step 50 Train loss 10.757760 ACC 0.09375 on epoch=24
06/24/2022 16:23:53 - INFO - __main__ - Step 60 Global step 60 Train loss 2.766757 on epoch=29
06/24/2022 16:23:56 - INFO - __main__ - Step 70 Global step 70 Train loss 2.074813 on epoch=34
06/24/2022 16:23:58 - INFO - __main__ - Step 80 Global step 80 Train loss 2.131797 on epoch=39
06/24/2022 16:24:01 - INFO - __main__ - Step 90 Global step 90 Train loss 1.825240 on epoch=44
06/24/2022 16:24:03 - INFO - __main__ - Step 100 Global step 100 Train loss 1.330760 on epoch=49
06/24/2022 16:24:03 - INFO - __main__ - Global step 100 Train loss 2.025874 ACC 0.46875 on epoch=49
06/24/2022 16:24:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.987675 on epoch=54
06/24/2022 16:24:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.895181 on epoch=59
06/24/2022 16:24:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.845053 on epoch=64
06/24/2022 16:24:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.626046 on epoch=69
06/24/2022 16:24:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.672229 on epoch=74
06/24/2022 16:24:16 - INFO - __main__ - Global step 150 Train loss 0.805237 ACC 0.5 on epoch=74
06/24/2022 16:24:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.612990 on epoch=79
06/24/2022 16:24:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.531591 on epoch=84
06/24/2022 16:24:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.468813 on epoch=89
06/24/2022 16:24:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.635500 on epoch=94
06/24/2022 16:24:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.669713 on epoch=99
06/24/2022 16:24:30 - INFO - __main__ - Global step 200 Train loss 0.583721 ACC 0.5 on epoch=99
06/24/2022 16:24:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.484343 on epoch=104
06/24/2022 16:24:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.511711 on epoch=109
06/24/2022 16:24:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.371984 on epoch=114
06/24/2022 16:24:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.552777 on epoch=119
06/24/2022 16:24:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.356466 on epoch=124
06/24/2022 16:24:42 - INFO - __main__ - Global step 250 Train loss 0.455456 ACC 0.5 on epoch=124
06/24/2022 16:24:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.384078 on epoch=129
06/24/2022 16:24:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.533024 on epoch=134
06/24/2022 16:24:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.380316 on epoch=139
06/24/2022 16:24:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.417174 on epoch=144
06/24/2022 16:24:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.369375 on epoch=149
06/24/2022 16:24:55 - INFO - __main__ - Global step 300 Train loss 0.416793 ACC 0.5 on epoch=149
06/24/2022 16:24:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.456299 on epoch=154
06/24/2022 16:25:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.385429 on epoch=159
06/24/2022 16:25:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.454512 on epoch=164
06/24/2022 16:25:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.389486 on epoch=169
06/24/2022 16:25:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.378413 on epoch=174
06/24/2022 16:25:07 - INFO - __main__ - Global step 350 Train loss 0.412828 ACC 0.40625 on epoch=174
06/24/2022 16:25:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.402449 on epoch=179
06/24/2022 16:25:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.392117 on epoch=184
06/24/2022 16:25:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.368375 on epoch=189
06/24/2022 16:25:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.441888 on epoch=194
06/24/2022 16:25:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.472944 on epoch=199
06/24/2022 16:25:20 - INFO - __main__ - Global step 400 Train loss 0.415555 ACC 0.53125 on epoch=199
06/24/2022 16:25:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.377468 on epoch=204
06/24/2022 16:25:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.410665 on epoch=209
06/24/2022 16:25:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.367471 on epoch=214
06/24/2022 16:25:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.397747 on epoch=219
06/24/2022 16:25:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.340184 on epoch=224
06/24/2022 16:25:33 - INFO - __main__ - Global step 450 Train loss 0.378707 ACC 0.53125 on epoch=224
06/24/2022 16:25:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.355721 on epoch=229
06/24/2022 16:25:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.345291 on epoch=234
06/24/2022 16:25:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.373708 on epoch=239
06/24/2022 16:25:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.345968 on epoch=244
06/24/2022 16:25:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.315513 on epoch=249
06/24/2022 16:25:46 - INFO - __main__ - Global step 500 Train loss 0.347240 ACC 0.59375 on epoch=249
06/24/2022 16:25:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.305959 on epoch=254
06/24/2022 16:25:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.405111 on epoch=259
06/24/2022 16:25:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.341601 on epoch=264
06/24/2022 16:25:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.292001 on epoch=269
06/24/2022 16:25:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.383066 on epoch=274
06/24/2022 16:25:59 - INFO - __main__ - Global step 550 Train loss 0.345547 ACC 0.53125 on epoch=274
06/24/2022 16:26:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.305307 on epoch=279
06/24/2022 16:26:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.303087 on epoch=284
06/24/2022 16:26:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.272882 on epoch=289
06/24/2022 16:26:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.335190 on epoch=294
06/24/2022 16:26:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.300936 on epoch=299
06/24/2022 16:26:11 - INFO - __main__ - Global step 600 Train loss 0.303481 ACC 0.5 on epoch=299
06/24/2022 16:26:11 - INFO - __main__ - save last model!
06/24/2022 16:26:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:26:12 - INFO - __main__ - Printing 3 examples
06/24/2022 16:26:12 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 16:26:12 - INFO - __main__ - ['duplicate']
06/24/2022 16:26:12 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 16:26:12 - INFO - __main__ - ['duplicate']
06/24/2022 16:26:12 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 16:26:12 - INFO - __main__ - ['duplicate']
06/24/2022 16:26:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 16:26:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:26:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 16:26:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:26:12 - INFO - __main__ - Printing 3 examples
06/24/2022 16:26:12 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 16:26:12 - INFO - __main__ - ['duplicate']
06/24/2022 16:26:12 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 16:26:12 - INFO - __main__ - ['duplicate']
06/24/2022 16:26:12 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 16:26:12 - INFO - __main__ - ['duplicate']
06/24/2022 16:26:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:26:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:26:12 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:26:14 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 16:26:14 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 16:26:14 - INFO - __main__ - Printing 3 examples
06/24/2022 16:26:14 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 16:26:14 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:26:14 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 16:26:14 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:26:14 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 16:26:14 - INFO - __main__ - ['duplicate']
06/24/2022 16:26:14 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:26:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 16:26:16 - INFO - __main__ - Starting training!
06/24/2022 16:26:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:27:13 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 16:35:14 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_42_0.0005_8_predictions.txt
06/24/2022 16:35:14 - INFO - __main__ - ACC on test data: 0.4882
06/24/2022 16:35:14 - INFO - __main__ - prefix=glue-qqp_16_42, lr=0.0005, bsz=8, dev_performance=0.59375, test_performance=0.488152362107346
06/24/2022 16:35:14 - INFO - __main__ - Running ... prefix=glue-qqp_16_42, lr=0.0003, bsz=8 ...
06/24/2022 16:35:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:35:15 - INFO - __main__ - Printing 3 examples
06/24/2022 16:35:15 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 16:35:15 - INFO - __main__ - ['duplicate']
06/24/2022 16:35:15 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 16:35:15 - INFO - __main__ - ['duplicate']
06/24/2022 16:35:15 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 16:35:15 - INFO - __main__ - ['duplicate']
06/24/2022 16:35:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 16:35:15 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:35:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 16:35:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:35:15 - INFO - __main__ - Printing 3 examples
06/24/2022 16:35:15 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 16:35:15 - INFO - __main__ - ['duplicate']
06/24/2022 16:35:15 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 16:35:15 - INFO - __main__ - ['duplicate']
06/24/2022 16:35:15 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 16:35:15 - INFO - __main__ - ['duplicate']
06/24/2022 16:35:15 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:35:15 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:35:15 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:35:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 16:35:19 - INFO - __main__ - Starting training!
06/24/2022 16:35:21 - INFO - __main__ - Step 10 Global step 10 Train loss 17.704569 on epoch=4
06/24/2022 16:35:24 - INFO - __main__ - Step 20 Global step 20 Train loss 15.193762 on epoch=9
06/24/2022 16:35:26 - INFO - __main__ - Step 30 Global step 30 Train loss 9.623875 on epoch=14
06/24/2022 16:35:29 - INFO - __main__ - Step 40 Global step 40 Train loss 7.354653 on epoch=19
06/24/2022 16:35:31 - INFO - __main__ - Step 50 Global step 50 Train loss 5.474201 on epoch=24
06/24/2022 16:35:31 - INFO - __main__ - Global step 50 Train loss 11.070210 ACC 0.375 on epoch=24
06/24/2022 16:35:34 - INFO - __main__ - Step 60 Global step 60 Train loss 3.978957 on epoch=29
06/24/2022 16:35:37 - INFO - __main__ - Step 70 Global step 70 Train loss 3.296670 on epoch=34
06/24/2022 16:35:39 - INFO - __main__ - Step 80 Global step 80 Train loss 2.725291 on epoch=39
06/24/2022 16:35:41 - INFO - __main__ - Step 90 Global step 90 Train loss 2.190627 on epoch=44
06/24/2022 16:35:44 - INFO - __main__ - Step 100 Global step 100 Train loss 1.805352 on epoch=49
06/24/2022 16:35:44 - INFO - __main__ - Global step 100 Train loss 2.799380 ACC 0.5 on epoch=49
06/24/2022 16:35:47 - INFO - __main__ - Step 110 Global step 110 Train loss 1.307440 on epoch=54
06/24/2022 16:35:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.946895 on epoch=59
06/24/2022 16:35:52 - INFO - __main__ - Step 130 Global step 130 Train loss 1.191574 on epoch=64
06/24/2022 16:35:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.684213 on epoch=69
06/24/2022 16:35:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.706901 on epoch=74
06/24/2022 16:35:57 - INFO - __main__ - Global step 150 Train loss 0.967404 ACC 0.53125 on epoch=74
06/24/2022 16:36:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.623983 on epoch=79
06/24/2022 16:36:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.503923 on epoch=84
06/24/2022 16:36:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.604226 on epoch=89
06/24/2022 16:36:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.672826 on epoch=94
06/24/2022 16:36:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.570814 on epoch=99
06/24/2022 16:36:10 - INFO - __main__ - Global step 200 Train loss 0.595154 ACC 0.46875 on epoch=99
06/24/2022 16:36:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.543535 on epoch=104
06/24/2022 16:36:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.558266 on epoch=109
06/24/2022 16:36:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.457733 on epoch=114
06/24/2022 16:36:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.522919 on epoch=119
06/24/2022 16:36:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.513012 on epoch=124
06/24/2022 16:36:23 - INFO - __main__ - Global step 250 Train loss 0.519093 ACC 0.53125 on epoch=124
06/24/2022 16:36:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.558657 on epoch=129
06/24/2022 16:36:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.427309 on epoch=134
06/24/2022 16:36:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.526805 on epoch=139
06/24/2022 16:36:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.375917 on epoch=144
06/24/2022 16:36:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.510963 on epoch=149
06/24/2022 16:36:36 - INFO - __main__ - Global step 300 Train loss 0.479930 ACC 0.46875 on epoch=149
06/24/2022 16:36:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.354946 on epoch=154
06/24/2022 16:36:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.303429 on epoch=159
06/24/2022 16:36:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.419913 on epoch=164
06/24/2022 16:36:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.382723 on epoch=169
06/24/2022 16:36:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.368230 on epoch=174
06/24/2022 16:36:48 - INFO - __main__ - Global step 350 Train loss 0.365848 ACC 0.46875 on epoch=174
06/24/2022 16:36:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.371779 on epoch=179
06/24/2022 16:36:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.352222 on epoch=184
06/24/2022 16:36:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.361339 on epoch=189
06/24/2022 16:36:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.299955 on epoch=194
06/24/2022 16:37:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.196449 on epoch=199
06/24/2022 16:37:01 - INFO - __main__ - Global step 400 Train loss 0.316349 ACC 0.46875 on epoch=199
06/24/2022 16:37:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.226479 on epoch=204
06/24/2022 16:37:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.221038 on epoch=209
06/24/2022 16:37:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.204870 on epoch=214
06/24/2022 16:37:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.150516 on epoch=219
06/24/2022 16:37:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.174123 on epoch=224
06/24/2022 16:37:13 - INFO - __main__ - Global step 450 Train loss 0.195405 ACC 0.46875 on epoch=224
06/24/2022 16:37:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.120009 on epoch=229
06/24/2022 16:37:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.085060 on epoch=234
06/24/2022 16:37:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.142200 on epoch=239
06/24/2022 16:37:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.058463 on epoch=244
06/24/2022 16:37:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.037204 on epoch=249
06/24/2022 16:37:26 - INFO - __main__ - Global step 500 Train loss 0.088587 ACC 0.46875 on epoch=249
06/24/2022 16:37:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.064147 on epoch=254
06/24/2022 16:37:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.061702 on epoch=259
06/24/2022 16:37:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.110192 on epoch=264
06/24/2022 16:37:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.084501 on epoch=269
06/24/2022 16:37:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.112994 on epoch=274
06/24/2022 16:37:38 - INFO - __main__ - Global step 550 Train loss 0.086707 ACC 0.5625 on epoch=274
06/24/2022 16:37:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.086416 on epoch=279
06/24/2022 16:37:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.038403 on epoch=284
06/24/2022 16:37:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.079591 on epoch=289
06/24/2022 16:37:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.186250 on epoch=294
06/24/2022 16:37:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.087908 on epoch=299
06/24/2022 16:37:51 - INFO - __main__ - Global step 600 Train loss 0.095714 ACC 0.59375 on epoch=299
06/24/2022 16:37:52 - INFO - __main__ - save last model!
06/24/2022 16:37:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:37:52 - INFO - __main__ - Printing 3 examples
06/24/2022 16:37:52 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 16:37:52 - INFO - __main__ - ['duplicate']
06/24/2022 16:37:52 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 16:37:52 - INFO - __main__ - ['duplicate']
06/24/2022 16:37:52 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 16:37:52 - INFO - __main__ - ['duplicate']
06/24/2022 16:37:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 16:37:52 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:37:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 16:37:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:37:52 - INFO - __main__ - Printing 3 examples
06/24/2022 16:37:52 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 16:37:52 - INFO - __main__ - ['duplicate']
06/24/2022 16:37:52 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 16:37:52 - INFO - __main__ - ['duplicate']
06/24/2022 16:37:52 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 16:37:52 - INFO - __main__ - ['duplicate']
06/24/2022 16:37:52 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:37:52 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:37:52 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:37:54 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 16:37:55 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 16:37:55 - INFO - __main__ - Printing 3 examples
06/24/2022 16:37:55 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 16:37:55 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:37:55 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 16:37:55 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:37:55 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 16:37:55 - INFO - __main__ - ['duplicate']
06/24/2022 16:37:55 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:37:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 16:37:56 - INFO - __main__ - Starting training!
06/24/2022 16:38:13 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:38:54 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 16:47:00 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_42_0.0003_8_predictions.txt
06/24/2022 16:47:01 - INFO - __main__ - ACC on test data: 0.4591
06/24/2022 16:47:01 - INFO - __main__ - prefix=glue-qqp_16_42, lr=0.0003, bsz=8, dev_performance=0.59375, test_performance=0.4591145189215929
06/24/2022 16:47:01 - INFO - __main__ - Running ... prefix=glue-qqp_16_42, lr=0.0002, bsz=8 ...
06/24/2022 16:47:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:47:02 - INFO - __main__ - Printing 3 examples
06/24/2022 16:47:02 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 16:47:02 - INFO - __main__ - ['duplicate']
06/24/2022 16:47:02 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 16:47:02 - INFO - __main__ - ['duplicate']
06/24/2022 16:47:02 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 16:47:02 - INFO - __main__ - ['duplicate']
06/24/2022 16:47:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 16:47:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:47:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 16:47:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:47:02 - INFO - __main__ - Printing 3 examples
06/24/2022 16:47:02 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 16:47:02 - INFO - __main__ - ['duplicate']
06/24/2022 16:47:02 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 16:47:02 - INFO - __main__ - ['duplicate']
06/24/2022 16:47:02 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 16:47:02 - INFO - __main__ - ['duplicate']
06/24/2022 16:47:02 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:47:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:47:02 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:47:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 16:47:06 - INFO - __main__ - Starting training!
06/24/2022 16:47:08 - INFO - __main__ - Step 10 Global step 10 Train loss 17.643604 on epoch=4
06/24/2022 16:47:11 - INFO - __main__ - Step 20 Global step 20 Train loss 18.918026 on epoch=9
06/24/2022 16:47:13 - INFO - __main__ - Step 30 Global step 30 Train loss 12.473397 on epoch=14
06/24/2022 16:47:16 - INFO - __main__ - Step 40 Global step 40 Train loss 10.248420 on epoch=19
06/24/2022 16:47:18 - INFO - __main__ - Step 50 Global step 50 Train loss 7.912160 on epoch=24
06/24/2022 16:47:19 - INFO - __main__ - Global step 50 Train loss 13.439121 ACC 0.0 on epoch=24
06/24/2022 16:47:22 - INFO - __main__ - Step 60 Global step 60 Train loss 6.982455 on epoch=29
06/24/2022 16:47:25 - INFO - __main__ - Step 70 Global step 70 Train loss 6.541156 on epoch=34
06/24/2022 16:47:27 - INFO - __main__ - Step 80 Global step 80 Train loss 5.828416 on epoch=39
06/24/2022 16:47:29 - INFO - __main__ - Step 90 Global step 90 Train loss 5.663475 on epoch=44
06/24/2022 16:47:32 - INFO - __main__ - Step 100 Global step 100 Train loss 5.397527 on epoch=49
06/24/2022 16:47:32 - INFO - __main__ - Global step 100 Train loss 6.082605 ACC 0.0 on epoch=49
06/24/2022 16:47:35 - INFO - __main__ - Step 110 Global step 110 Train loss 3.532768 on epoch=54
06/24/2022 16:47:37 - INFO - __main__ - Step 120 Global step 120 Train loss 3.801371 on epoch=59
06/24/2022 16:47:40 - INFO - __main__ - Step 130 Global step 130 Train loss 3.029297 on epoch=64
06/24/2022 16:47:43 - INFO - __main__ - Step 140 Global step 140 Train loss 2.716915 on epoch=69
06/24/2022 16:47:45 - INFO - __main__ - Step 150 Global step 150 Train loss 2.340209 on epoch=74
06/24/2022 16:47:46 - INFO - __main__ - Global step 150 Train loss 3.084112 ACC 0.4375 on epoch=74
06/24/2022 16:47:49 - INFO - __main__ - Step 160 Global step 160 Train loss 2.768471 on epoch=79
06/24/2022 16:47:51 - INFO - __main__ - Step 170 Global step 170 Train loss 1.578679 on epoch=84
06/24/2022 16:47:54 - INFO - __main__ - Step 180 Global step 180 Train loss 1.671910 on epoch=89
06/24/2022 16:47:56 - INFO - __main__ - Step 190 Global step 190 Train loss 1.330884 on epoch=94
06/24/2022 16:47:59 - INFO - __main__ - Step 200 Global step 200 Train loss 1.093279 on epoch=99
06/24/2022 16:47:59 - INFO - __main__ - Global step 200 Train loss 1.688645 ACC 0.46875 on epoch=99
06/24/2022 16:48:02 - INFO - __main__ - Step 210 Global step 210 Train loss 1.818769 on epoch=104
06/24/2022 16:48:04 - INFO - __main__ - Step 220 Global step 220 Train loss 1.413500 on epoch=109
06/24/2022 16:48:07 - INFO - __main__ - Step 230 Global step 230 Train loss 1.381386 on epoch=114
06/24/2022 16:48:09 - INFO - __main__ - Step 240 Global step 240 Train loss 1.597061 on epoch=119
06/24/2022 16:48:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.961679 on epoch=124
06/24/2022 16:48:12 - INFO - __main__ - Global step 250 Train loss 1.434479 ACC 0.5625 on epoch=124
06/24/2022 16:48:15 - INFO - __main__ - Step 260 Global step 260 Train loss 1.367478 on epoch=129
06/24/2022 16:48:18 - INFO - __main__ - Step 270 Global step 270 Train loss 1.054685 on epoch=134
06/24/2022 16:48:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.855900 on epoch=139
06/24/2022 16:48:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.985772 on epoch=144
06/24/2022 16:48:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.725072 on epoch=149
06/24/2022 16:48:26 - INFO - __main__ - Global step 300 Train loss 0.997781 ACC 0.59375 on epoch=149
06/24/2022 16:48:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.672797 on epoch=154
06/24/2022 16:48:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.723486 on epoch=159
06/24/2022 16:48:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.486068 on epoch=164
06/24/2022 16:48:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.592597 on epoch=169
06/24/2022 16:48:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.679943 on epoch=174
06/24/2022 16:48:39 - INFO - __main__ - Global step 350 Train loss 0.630978 ACC 0.5 on epoch=174
06/24/2022 16:48:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.595262 on epoch=179
06/24/2022 16:48:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.672168 on epoch=184
06/24/2022 16:48:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.698889 on epoch=189
06/24/2022 16:48:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.521522 on epoch=194
06/24/2022 16:48:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.706295 on epoch=199
06/24/2022 16:48:52 - INFO - __main__ - Global step 400 Train loss 0.638827 ACC 0.5 on epoch=199
06/24/2022 16:48:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.555083 on epoch=204
06/24/2022 16:48:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.463509 on epoch=209
06/24/2022 16:48:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.519363 on epoch=214
06/24/2022 16:49:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.463491 on epoch=219
06/24/2022 16:49:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.496211 on epoch=224
06/24/2022 16:49:05 - INFO - __main__ - Global step 450 Train loss 0.499532 ACC 0.46875 on epoch=224
06/24/2022 16:49:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.523301 on epoch=229
06/24/2022 16:49:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.509021 on epoch=234
06/24/2022 16:49:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.491456 on epoch=239
06/24/2022 16:49:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.504915 on epoch=244
06/24/2022 16:49:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.506040 on epoch=249
06/24/2022 16:49:18 - INFO - __main__ - Global step 500 Train loss 0.506947 ACC 0.46875 on epoch=249
06/24/2022 16:49:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.408424 on epoch=254
06/24/2022 16:49:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.609448 on epoch=259
06/24/2022 16:49:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.347234 on epoch=264
06/24/2022 16:49:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.579319 on epoch=269
06/24/2022 16:49:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.541747 on epoch=274
06/24/2022 16:49:31 - INFO - __main__ - Global step 550 Train loss 0.497234 ACC 0.5 on epoch=274
06/24/2022 16:49:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.473466 on epoch=279
06/24/2022 16:49:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.493824 on epoch=284
06/24/2022 16:49:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.357551 on epoch=289
06/24/2022 16:49:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.475918 on epoch=294
06/24/2022 16:49:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.472703 on epoch=299
06/24/2022 16:49:44 - INFO - __main__ - Global step 600 Train loss 0.454692 ACC 0.59375 on epoch=299
06/24/2022 16:49:44 - INFO - __main__ - save last model!
06/24/2022 16:49:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:49:45 - INFO - __main__ - Printing 3 examples
06/24/2022 16:49:45 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 16:49:45 - INFO - __main__ - ['duplicate']
06/24/2022 16:49:45 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 16:49:45 - INFO - __main__ - ['duplicate']
06/24/2022 16:49:45 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 16:49:45 - INFO - __main__ - ['duplicate']
06/24/2022 16:49:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 16:49:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:49:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 16:49:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:49:45 - INFO - __main__ - Printing 3 examples
06/24/2022 16:49:45 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 16:49:45 - INFO - __main__ - ['duplicate']
06/24/2022 16:49:45 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 16:49:45 - INFO - __main__ - ['duplicate']
06/24/2022 16:49:45 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 16:49:45 - INFO - __main__ - ['duplicate']
06/24/2022 16:49:45 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:49:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:49:45 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:49:47 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 16:49:47 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 16:49:47 - INFO - __main__ - Printing 3 examples
06/24/2022 16:49:47 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 16:49:47 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:49:47 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 16:49:47 - INFO - __main__ - ['not_duplicate']
06/24/2022 16:49:47 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 16:49:47 - INFO - __main__ - ['duplicate']
06/24/2022 16:49:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:49:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 16:49:49 - INFO - __main__ - Starting training!
06/24/2022 16:50:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:50:48 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 16:59:12 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_42_0.0002_8_predictions.txt
06/24/2022 16:59:12 - INFO - __main__ - ACC on test data: 0.4555
06/24/2022 16:59:13 - INFO - __main__ - prefix=glue-qqp_16_42, lr=0.0002, bsz=8, dev_performance=0.59375, test_performance=0.45545387088795447
06/24/2022 16:59:13 - INFO - __main__ - Running ... prefix=glue-qqp_16_42, lr=0.0001, bsz=8 ...
06/24/2022 16:59:13 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:59:13 - INFO - __main__ - Printing 3 examples
06/24/2022 16:59:13 - INFO - __main__ -  [glue-qqp] question 1: Why do some people think that having a baby is a blessing? [SEP] question 2: Why is having a baby a blessing?
06/24/2022 16:59:13 - INFO - __main__ - ['duplicate']
06/24/2022 16:59:13 - INFO - __main__ -  [glue-qqp] question 1: Why don't I get answers for some of my questions on Quora? [SEP] question 2: Why do some questions get more answers here in Quora?
06/24/2022 16:59:14 - INFO - __main__ - ['duplicate']
06/24/2022 16:59:14 - INFO - __main__ -  [glue-qqp] question 1: Which is the best and free small business accounting software for my business? [SEP] question 2: Which is the best free accounting software for a small firm?
06/24/2022 16:59:14 - INFO - __main__ - ['duplicate']
06/24/2022 16:59:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 16:59:14 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:59:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 16:59:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:59:14 - INFO - __main__ - Printing 3 examples
06/24/2022 16:59:14 - INFO - __main__ -  [glue-qqp] question 1: Have you heard about the Delta Charting Group out of Tucson, Arizona? [SEP] question 2: What are the good things about Delta Charting Group out of Tucson, Arizona?
06/24/2022 16:59:14 - INFO - __main__ - ['duplicate']
06/24/2022 16:59:14 - INFO - __main__ -  [glue-qqp] question 1: How many mark should a student obtain in JEE to get a seat in IIST? [SEP] question 2: How many marks are required in JEE to get in IIST?
06/24/2022 16:59:14 - INFO - __main__ - ['duplicate']
06/24/2022 16:59:14 - INFO - __main__ -  [glue-qqp] question 1: Why do people ask questions whose answer can be easily found on the internet? [SEP] question 2: Why do people ask basic questions instead of searching them?
06/24/2022 16:59:14 - INFO - __main__ - ['duplicate']
06/24/2022 16:59:14 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:59:14 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:59:14 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:59:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 16:59:18 - INFO - __main__ - Starting training!
06/24/2022 16:59:20 - INFO - __main__ - Step 10 Global step 10 Train loss 18.272678 on epoch=4
06/24/2022 16:59:22 - INFO - __main__ - Step 20 Global step 20 Train loss 16.803623 on epoch=9
06/24/2022 16:59:25 - INFO - __main__ - Step 30 Global step 30 Train loss 15.533298 on epoch=14
06/24/2022 16:59:27 - INFO - __main__ - Step 40 Global step 40 Train loss 12.166923 on epoch=19
06/24/2022 16:59:29 - INFO - __main__ - Step 50 Global step 50 Train loss 10.333843 on epoch=24
06/24/2022 16:59:33 - INFO - __main__ - Global step 50 Train loss 14.622074 ACC 0.0 on epoch=24
06/24/2022 16:59:35 - INFO - __main__ - Step 60 Global step 60 Train loss 9.179930 on epoch=29
06/24/2022 16:59:38 - INFO - __main__ - Step 70 Global step 70 Train loss 7.758360 on epoch=34
06/24/2022 16:59:40 - INFO - __main__ - Step 80 Global step 80 Train loss 7.793984 on epoch=39
06/24/2022 16:59:43 - INFO - __main__ - Step 90 Global step 90 Train loss 6.882613 on epoch=44
06/24/2022 16:59:46 - INFO - __main__ - Step 100 Global step 100 Train loss 6.456802 on epoch=49
06/24/2022 16:59:47 - INFO - __main__ - Global step 100 Train loss 7.614338 ACC 0.0 on epoch=49
06/24/2022 16:59:49 - INFO - __main__ - Step 110 Global step 110 Train loss 5.552238 on epoch=54
06/24/2022 16:59:52 - INFO - __main__ - Step 120 Global step 120 Train loss 5.519566 on epoch=59
06/24/2022 16:59:54 - INFO - __main__ - Step 130 Global step 130 Train loss 4.749779 on epoch=64
06/24/2022 16:59:57 - INFO - __main__ - Step 140 Global step 140 Train loss 4.238266 on epoch=69
06/24/2022 16:59:59 - INFO - __main__ - Step 150 Global step 150 Train loss 4.094152 on epoch=74
06/24/2022 17:00:00 - INFO - __main__ - Global step 150 Train loss 4.830800 ACC 0.09375 on epoch=74
06/24/2022 17:00:03 - INFO - __main__ - Step 160 Global step 160 Train loss 3.527770 on epoch=79
06/24/2022 17:00:05 - INFO - __main__ - Step 170 Global step 170 Train loss 3.727376 on epoch=84
06/24/2022 17:00:08 - INFO - __main__ - Step 180 Global step 180 Train loss 3.280461 on epoch=89
06/24/2022 17:00:10 - INFO - __main__ - Step 190 Global step 190 Train loss 3.207080 on epoch=94
06/24/2022 17:00:13 - INFO - __main__ - Step 200 Global step 200 Train loss 2.673671 on epoch=99
06/24/2022 17:00:13 - INFO - __main__ - Global step 200 Train loss 3.283272 ACC 0.4375 on epoch=99
06/24/2022 17:00:16 - INFO - __main__ - Step 210 Global step 210 Train loss 2.450956 on epoch=104
06/24/2022 17:00:19 - INFO - __main__ - Step 220 Global step 220 Train loss 2.309174 on epoch=109
06/24/2022 17:00:21 - INFO - __main__ - Step 230 Global step 230 Train loss 2.324091 on epoch=114
06/24/2022 17:00:24 - INFO - __main__ - Step 240 Global step 240 Train loss 2.155677 on epoch=119
06/24/2022 17:00:26 - INFO - __main__ - Step 250 Global step 250 Train loss 2.416513 on epoch=124
06/24/2022 17:00:26 - INFO - __main__ - Global step 250 Train loss 2.331282 ACC 0.5 on epoch=124
06/24/2022 17:00:29 - INFO - __main__ - Step 260 Global step 260 Train loss 2.013172 on epoch=129
06/24/2022 17:00:32 - INFO - __main__ - Step 270 Global step 270 Train loss 1.912812 on epoch=134
06/24/2022 17:00:34 - INFO - __main__ - Step 280 Global step 280 Train loss 1.726138 on epoch=139
06/24/2022 17:00:36 - INFO - __main__ - Step 290 Global step 290 Train loss 1.392307 on epoch=144
06/24/2022 17:00:39 - INFO - __main__ - Step 300 Global step 300 Train loss 1.573202 on epoch=149
06/24/2022 17:00:39 - INFO - __main__ - Global step 300 Train loss 1.723526 ACC 0.46875 on epoch=149
06/24/2022 17:00:42 - INFO - __main__ - Step 310 Global step 310 Train loss 1.834672 on epoch=154
06/24/2022 17:00:44 - INFO - __main__ - Step 320 Global step 320 Train loss 1.183606 on epoch=159
06/24/2022 17:00:47 - INFO - __main__ - Step 330 Global step 330 Train loss 1.763074 on epoch=164
06/24/2022 17:00:49 - INFO - __main__ - Step 340 Global step 340 Train loss 1.083744 on epoch=169
06/24/2022 17:00:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.027296 on epoch=174
06/24/2022 17:00:52 - INFO - __main__ - Global step 350 Train loss 1.378478 ACC 0.53125 on epoch=174
06/24/2022 17:00:55 - INFO - __main__ - Step 360 Global step 360 Train loss 1.117910 on epoch=179
06/24/2022 17:00:57 - INFO - __main__ - Step 370 Global step 370 Train loss 1.275832 on epoch=184
06/24/2022 17:01:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.992357 on epoch=189
06/24/2022 17:01:02 - INFO - __main__ - Step 390 Global step 390 Train loss 1.126953 on epoch=194
06/24/2022 17:01:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.795650 on epoch=199
06/24/2022 17:01:05 - INFO - __main__ - Global step 400 Train loss 1.061741 ACC 0.5625 on epoch=199
06/24/2022 17:01:08 - INFO - __main__ - Step 410 Global step 410 Train loss 1.558481 on epoch=204
06/24/2022 17:01:10 - INFO - __main__ - Step 420 Global step 420 Train loss 1.044547 on epoch=209
06/24/2022 17:01:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.968354 on epoch=214
06/24/2022 17:01:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.876924 on epoch=219
06/24/2022 17:01:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.837815 on epoch=224
06/24/2022 17:01:18 - INFO - __main__ - Global step 450 Train loss 1.057224 ACC 0.375 on epoch=224
06/24/2022 17:01:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.900295 on epoch=229
06/24/2022 17:01:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.682744 on epoch=234
06/24/2022 17:01:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.986483 on epoch=239
06/24/2022 17:01:27 - INFO - __main__ - Step 490 Global step 490 Train loss 1.426414 on epoch=244
06/24/2022 17:01:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.561926 on epoch=249
06/24/2022 17:01:30 - INFO - __main__ - Global step 500 Train loss 0.911573 ACC 0.375 on epoch=249
06/24/2022 17:01:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.575754 on epoch=254
06/24/2022 17:01:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.663891 on epoch=259
06/24/2022 17:01:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.641582 on epoch=264
06/24/2022 17:01:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.554088 on epoch=269
06/24/2022 17:01:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.458958 on epoch=274
06/24/2022 17:01:43 - INFO - __main__ - Global step 550 Train loss 0.578855 ACC 0.4375 on epoch=274
06/24/2022 17:01:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.599968 on epoch=279
06/24/2022 17:01:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.529346 on epoch=284
06/24/2022 17:01:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.745683 on epoch=289
06/24/2022 17:01:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.506267 on epoch=294
06/24/2022 17:01:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.726446 on epoch=299
06/24/2022 17:01:56 - INFO - __main__ - Global step 600 Train loss 0.621542 ACC 0.5625 on epoch=299
06/24/2022 17:01:56 - INFO - __main__ - save last model!
06/24/2022 17:01:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:01:56 - INFO - __main__ - Printing 3 examples
06/24/2022 17:01:56 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:01:56 - INFO - __main__ - ['duplicate']
06/24/2022 17:01:56 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 17:01:56 - INFO - __main__ - ['duplicate']
06/24/2022 17:01:56 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:01:56 - INFO - __main__ - ['duplicate']
06/24/2022 17:01:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 17:01:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:01:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 17:01:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:01:56 - INFO - __main__ - Printing 3 examples
06/24/2022 17:01:56 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 17:01:56 - INFO - __main__ - ['duplicate']
06/24/2022 17:01:56 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 17:01:56 - INFO - __main__ - ['duplicate']
06/24/2022 17:01:56 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 17:01:56 - INFO - __main__ - ['duplicate']
06/24/2022 17:01:56 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:01:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:01:56 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 17:01:58 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 17:01:59 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 17:01:59 - INFO - __main__ - Printing 3 examples
06/24/2022 17:01:59 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 17:01:59 - INFO - __main__ - ['not_duplicate']
06/24/2022 17:01:59 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 17:01:59 - INFO - __main__ - ['not_duplicate']
06/24/2022 17:01:59 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 17:01:59 - INFO - __main__ - ['duplicate']
06/24/2022 17:01:59 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:02:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 17:02:00 - INFO - __main__ - Starting training!
06/24/2022 17:02:17 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:02:58 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 17:10:59 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_42_0.0001_8_predictions.txt
06/24/2022 17:10:59 - INFO - __main__ - ACC on test data: 0.4520
06/24/2022 17:10:59 - INFO - __main__ - prefix=glue-qqp_16_42, lr=0.0001, bsz=8, dev_performance=0.5625, test_performance=0.45199109572099927
06/24/2022 17:10:59 - INFO - __main__ - Running ... prefix=glue-qqp_16_87, lr=0.0005, bsz=8 ...
06/24/2022 17:11:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:11:00 - INFO - __main__ - Printing 3 examples
06/24/2022 17:11:00 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:11:00 - INFO - __main__ - ['duplicate']
06/24/2022 17:11:00 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 17:11:00 - INFO - __main__ - ['duplicate']
06/24/2022 17:11:00 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:11:00 - INFO - __main__ - ['duplicate']
06/24/2022 17:11:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 17:11:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:11:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 17:11:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:11:00 - INFO - __main__ - Printing 3 examples
06/24/2022 17:11:00 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 17:11:00 - INFO - __main__ - ['duplicate']
06/24/2022 17:11:00 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 17:11:00 - INFO - __main__ - ['duplicate']
06/24/2022 17:11:00 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 17:11:00 - INFO - __main__ - ['duplicate']
06/24/2022 17:11:00 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:11:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:11:00 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 17:11:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 17:11:04 - INFO - __main__ - Starting training!
06/24/2022 17:11:06 - INFO - __main__ - Step 10 Global step 10 Train loss 17.898724 on epoch=4
06/24/2022 17:11:09 - INFO - __main__ - Step 20 Global step 20 Train loss 13.827563 on epoch=9
06/24/2022 17:11:11 - INFO - __main__ - Step 30 Global step 30 Train loss 6.380111 on epoch=14
06/24/2022 17:11:13 - INFO - __main__ - Step 40 Global step 40 Train loss 4.638848 on epoch=19
06/24/2022 17:11:16 - INFO - __main__ - Step 50 Global step 50 Train loss 3.628490 on epoch=24
06/24/2022 17:11:16 - INFO - __main__ - Global step 50 Train loss 9.274747 ACC 0.5 on epoch=24
06/24/2022 17:11:19 - INFO - __main__ - Step 60 Global step 60 Train loss 1.567149 on epoch=29
06/24/2022 17:11:22 - INFO - __main__ - Step 70 Global step 70 Train loss 2.039936 on epoch=34
06/24/2022 17:11:24 - INFO - __main__ - Step 80 Global step 80 Train loss 2.086025 on epoch=39
06/24/2022 17:11:27 - INFO - __main__ - Step 90 Global step 90 Train loss 1.201907 on epoch=44
06/24/2022 17:11:29 - INFO - __main__ - Step 100 Global step 100 Train loss 1.497404 on epoch=49
06/24/2022 17:11:29 - INFO - __main__ - Global step 100 Train loss 1.678484 ACC 0.5 on epoch=49
06/24/2022 17:11:32 - INFO - __main__ - Step 110 Global step 110 Train loss 4.228194 on epoch=54
06/24/2022 17:11:34 - INFO - __main__ - Step 120 Global step 120 Train loss 2.041856 on epoch=59
06/24/2022 17:11:37 - INFO - __main__ - Step 130 Global step 130 Train loss 1.239296 on epoch=64
06/24/2022 17:11:39 - INFO - __main__ - Step 140 Global step 140 Train loss 1.036641 on epoch=69
06/24/2022 17:11:42 - INFO - __main__ - Step 150 Global step 150 Train loss 1.005621 on epoch=74
06/24/2022 17:11:42 - INFO - __main__ - Global step 150 Train loss 1.910321 ACC 0.5 on epoch=74
06/24/2022 17:11:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.853135 on epoch=79
06/24/2022 17:11:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.492879 on epoch=84
06/24/2022 17:11:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.453423 on epoch=89
06/24/2022 17:11:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.520961 on epoch=94
06/24/2022 17:11:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.538607 on epoch=99
06/24/2022 17:11:55 - INFO - __main__ - Global step 200 Train loss 0.571801 ACC 0.5 on epoch=99
06/24/2022 17:11:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.545669 on epoch=104
06/24/2022 17:12:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.574404 on epoch=109
06/24/2022 17:12:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.494790 on epoch=114
06/24/2022 17:12:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.437557 on epoch=119
06/24/2022 17:12:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.352380 on epoch=124
06/24/2022 17:12:08 - INFO - __main__ - Global step 250 Train loss 0.480960 ACC 0.5 on epoch=124
06/24/2022 17:12:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.412729 on epoch=129
06/24/2022 17:12:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.379219 on epoch=134
06/24/2022 17:12:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.462133 on epoch=139
06/24/2022 17:12:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.453475 on epoch=144
06/24/2022 17:12:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.453394 on epoch=149
06/24/2022 17:12:21 - INFO - __main__ - Global step 300 Train loss 0.432190 ACC 0.5625 on epoch=149
06/24/2022 17:12:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.409714 on epoch=154
06/24/2022 17:12:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.395248 on epoch=159
06/24/2022 17:12:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.384632 on epoch=164
06/24/2022 17:12:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.323621 on epoch=169
06/24/2022 17:12:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.388903 on epoch=174
06/24/2022 17:12:34 - INFO - __main__ - Global step 350 Train loss 0.380424 ACC 0.5 on epoch=174
06/24/2022 17:12:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.347362 on epoch=179
06/24/2022 17:12:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.293549 on epoch=184
06/24/2022 17:12:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.576202 on epoch=189
06/24/2022 17:12:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.384932 on epoch=194
06/24/2022 17:12:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.280188 on epoch=199
06/24/2022 17:12:47 - INFO - __main__ - Global step 400 Train loss 0.376447 ACC 0.5 on epoch=199
06/24/2022 17:12:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.364283 on epoch=204
06/24/2022 17:12:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.389506 on epoch=209
06/24/2022 17:12:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.366814 on epoch=214
06/24/2022 17:12:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.264745 on epoch=219
06/24/2022 17:13:00 - INFO - __main__ - Step 450 Global step 450 Train loss 1.045789 on epoch=224
06/24/2022 17:13:00 - INFO - __main__ - Global step 450 Train loss 0.486227 ACC 0.625 on epoch=224
06/24/2022 17:13:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.334358 on epoch=229
06/24/2022 17:13:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.232755 on epoch=234
06/24/2022 17:13:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.338017 on epoch=239
06/24/2022 17:13:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.339587 on epoch=244
06/24/2022 17:13:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.292999 on epoch=249
06/24/2022 17:13:13 - INFO - __main__ - Global step 500 Train loss 0.307543 ACC 0.6875 on epoch=249
06/24/2022 17:13:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.327139 on epoch=254
06/24/2022 17:13:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.338464 on epoch=259
06/24/2022 17:13:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.235518 on epoch=264
06/24/2022 17:13:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.289339 on epoch=269
06/24/2022 17:13:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.229816 on epoch=274
06/24/2022 17:13:26 - INFO - __main__ - Global step 550 Train loss 0.284055 ACC 0.65625 on epoch=274
06/24/2022 17:13:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.268925 on epoch=279
06/24/2022 17:13:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.250204 on epoch=284
06/24/2022 17:13:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.267404 on epoch=289
06/24/2022 17:13:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.225353 on epoch=294
06/24/2022 17:13:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.277680 on epoch=299
06/24/2022 17:13:39 - INFO - __main__ - Global step 600 Train loss 0.257913 ACC 0.59375 on epoch=299
06/24/2022 17:13:39 - INFO - __main__ - save last model!
06/24/2022 17:13:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:13:39 - INFO - __main__ - Printing 3 examples
06/24/2022 17:13:39 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:13:39 - INFO - __main__ - ['duplicate']
06/24/2022 17:13:39 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 17:13:39 - INFO - __main__ - ['duplicate']
06/24/2022 17:13:39 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:13:39 - INFO - __main__ - ['duplicate']
06/24/2022 17:13:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 17:13:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:13:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 17:13:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:13:39 - INFO - __main__ - Printing 3 examples
06/24/2022 17:13:39 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 17:13:39 - INFO - __main__ - ['duplicate']
06/24/2022 17:13:39 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 17:13:40 - INFO - __main__ - ['duplicate']
06/24/2022 17:13:40 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 17:13:40 - INFO - __main__ - ['duplicate']
06/24/2022 17:13:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:13:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:13:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 17:13:41 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 17:13:42 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 17:13:42 - INFO - __main__ - Printing 3 examples
06/24/2022 17:13:42 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 17:13:42 - INFO - __main__ - ['not_duplicate']
06/24/2022 17:13:42 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 17:13:42 - INFO - __main__ - ['not_duplicate']
06/24/2022 17:13:42 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 17:13:42 - INFO - __main__ - ['duplicate']
06/24/2022 17:13:42 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:13:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 17:13:44 - INFO - __main__ - Starting training!
06/24/2022 17:14:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:14:41 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 17:22:42 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_87_0.0005_8_predictions.txt
06/24/2022 17:22:42 - INFO - __main__ - ACC on test data: 0.4084
06/24/2022 17:22:42 - INFO - __main__ - prefix=glue-qqp_16_87, lr=0.0005, bsz=8, dev_performance=0.6875, test_performance=0.4084095968340341
06/24/2022 17:22:42 - INFO - __main__ - Running ... prefix=glue-qqp_16_87, lr=0.0003, bsz=8 ...
06/24/2022 17:22:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:22:43 - INFO - __main__ - Printing 3 examples
06/24/2022 17:22:43 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:22:43 - INFO - __main__ - ['duplicate']
06/24/2022 17:22:43 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 17:22:43 - INFO - __main__ - ['duplicate']
06/24/2022 17:22:43 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:22:43 - INFO - __main__ - ['duplicate']
06/24/2022 17:22:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 17:22:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:22:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 17:22:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:22:43 - INFO - __main__ - Printing 3 examples
06/24/2022 17:22:43 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 17:22:43 - INFO - __main__ - ['duplicate']
06/24/2022 17:22:43 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 17:22:43 - INFO - __main__ - ['duplicate']
06/24/2022 17:22:43 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 17:22:43 - INFO - __main__ - ['duplicate']
06/24/2022 17:22:43 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:22:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:22:43 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 17:22:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 17:22:47 - INFO - __main__ - Starting training!
06/24/2022 17:22:50 - INFO - __main__ - Step 10 Global step 10 Train loss 16.904821 on epoch=4
06/24/2022 17:22:52 - INFO - __main__ - Step 20 Global step 20 Train loss 14.854736 on epoch=9
06/24/2022 17:22:54 - INFO - __main__ - Step 30 Global step 30 Train loss 12.472570 on epoch=14
06/24/2022 17:22:57 - INFO - __main__ - Step 40 Global step 40 Train loss 8.312599 on epoch=19
06/24/2022 17:22:59 - INFO - __main__ - Step 50 Global step 50 Train loss 5.755750 on epoch=24
06/24/2022 17:23:00 - INFO - __main__ - Global step 50 Train loss 11.660095 ACC 0.21875 on epoch=24
06/24/2022 17:23:03 - INFO - __main__ - Step 60 Global step 60 Train loss 4.718925 on epoch=29
06/24/2022 17:23:05 - INFO - __main__ - Step 70 Global step 70 Train loss 3.489136 on epoch=34
06/24/2022 17:23:08 - INFO - __main__ - Step 80 Global step 80 Train loss 2.849905 on epoch=39
06/24/2022 17:23:10 - INFO - __main__ - Step 90 Global step 90 Train loss 2.691889 on epoch=44
06/24/2022 17:23:13 - INFO - __main__ - Step 100 Global step 100 Train loss 1.870522 on epoch=49
06/24/2022 17:23:13 - INFO - __main__ - Global step 100 Train loss 3.124076 ACC 0.5625 on epoch=49
06/24/2022 17:23:16 - INFO - __main__ - Step 110 Global step 110 Train loss 1.795882 on epoch=54
06/24/2022 17:23:19 - INFO - __main__ - Step 120 Global step 120 Train loss 1.182941 on epoch=59
06/24/2022 17:23:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.755395 on epoch=64
06/24/2022 17:23:24 - INFO - __main__ - Step 140 Global step 140 Train loss 1.450541 on epoch=69
06/24/2022 17:23:27 - INFO - __main__ - Step 150 Global step 150 Train loss 1.170897 on epoch=74
06/24/2022 17:23:27 - INFO - __main__ - Global step 150 Train loss 1.271131 ACC 0.46875 on epoch=74
06/24/2022 17:23:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.745564 on epoch=79
06/24/2022 17:23:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.928393 on epoch=84
06/24/2022 17:23:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.847211 on epoch=89
06/24/2022 17:23:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.696232 on epoch=94
06/24/2022 17:23:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.719196 on epoch=99
06/24/2022 17:23:40 - INFO - __main__ - Global step 200 Train loss 0.787319 ACC 0.5 on epoch=99
06/24/2022 17:23:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.600503 on epoch=104
06/24/2022 17:23:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.493540 on epoch=109
06/24/2022 17:23:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.450646 on epoch=114
06/24/2022 17:23:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.693342 on epoch=119
06/24/2022 17:23:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.456717 on epoch=124
06/24/2022 17:23:53 - INFO - __main__ - Global step 250 Train loss 0.538950 ACC 0.46875 on epoch=124
06/24/2022 17:23:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.623411 on epoch=129
06/24/2022 17:23:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.479667 on epoch=134
06/24/2022 17:24:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.505723 on epoch=139
06/24/2022 17:24:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.463594 on epoch=144
06/24/2022 17:24:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.556600 on epoch=149
06/24/2022 17:24:06 - INFO - __main__ - Global step 300 Train loss 0.525799 ACC 0.5 on epoch=149
06/24/2022 17:24:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.396494 on epoch=154
06/24/2022 17:24:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.379672 on epoch=159
06/24/2022 17:24:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.553081 on epoch=164
06/24/2022 17:24:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.452642 on epoch=169
06/24/2022 17:24:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.547584 on epoch=174
06/24/2022 17:24:19 - INFO - __main__ - Global step 350 Train loss 0.465895 ACC 0.5625 on epoch=174
06/24/2022 17:24:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.549580 on epoch=179
06/24/2022 17:24:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.375210 on epoch=184
06/24/2022 17:24:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.484726 on epoch=189
06/24/2022 17:24:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.408198 on epoch=194
06/24/2022 17:24:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.411596 on epoch=199
06/24/2022 17:24:32 - INFO - __main__ - Global step 400 Train loss 0.445862 ACC 0.53125 on epoch=199
06/24/2022 17:24:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.394080 on epoch=204
06/24/2022 17:24:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.397853 on epoch=209
06/24/2022 17:24:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.425602 on epoch=214
06/24/2022 17:24:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.442642 on epoch=219
06/24/2022 17:24:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.306492 on epoch=224
06/24/2022 17:24:45 - INFO - __main__ - Global step 450 Train loss 0.393334 ACC 0.71875 on epoch=224
06/24/2022 17:24:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.293327 on epoch=229
06/24/2022 17:24:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.486932 on epoch=234
06/24/2022 17:24:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.323717 on epoch=239
06/24/2022 17:24:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.352835 on epoch=244
06/24/2022 17:24:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.355469 on epoch=249
06/24/2022 17:24:58 - INFO - __main__ - Global step 500 Train loss 0.362456 ACC 0.59375 on epoch=249
06/24/2022 17:25:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.426734 on epoch=254
06/24/2022 17:25:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.292143 on epoch=259
06/24/2022 17:25:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.288206 on epoch=264
06/24/2022 17:25:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.288445 on epoch=269
06/24/2022 17:25:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.277983 on epoch=274
06/24/2022 17:25:11 - INFO - __main__ - Global step 550 Train loss 0.314702 ACC 0.71875 on epoch=274
06/24/2022 17:25:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.287466 on epoch=279
06/24/2022 17:25:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.228440 on epoch=284
06/24/2022 17:25:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.185864 on epoch=289
06/24/2022 17:25:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.224698 on epoch=294
06/24/2022 17:25:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.155793 on epoch=299
06/24/2022 17:25:24 - INFO - __main__ - Global step 600 Train loss 0.216452 ACC 0.59375 on epoch=299
06/24/2022 17:25:24 - INFO - __main__ - save last model!
06/24/2022 17:25:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:25:25 - INFO - __main__ - Printing 3 examples
06/24/2022 17:25:25 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:25:25 - INFO - __main__ - ['duplicate']
06/24/2022 17:25:25 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 17:25:25 - INFO - __main__ - ['duplicate']
06/24/2022 17:25:25 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:25:25 - INFO - __main__ - ['duplicate']
06/24/2022 17:25:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 17:25:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:25:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 17:25:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:25:25 - INFO - __main__ - Printing 3 examples
06/24/2022 17:25:25 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 17:25:25 - INFO - __main__ - ['duplicate']
06/24/2022 17:25:25 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 17:25:25 - INFO - __main__ - ['duplicate']
06/24/2022 17:25:25 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 17:25:25 - INFO - __main__ - ['duplicate']
06/24/2022 17:25:25 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:25:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:25:25 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 17:25:27 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 17:25:27 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 17:25:27 - INFO - __main__ - Printing 3 examples
06/24/2022 17:25:27 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 17:25:27 - INFO - __main__ - ['not_duplicate']
06/24/2022 17:25:27 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 17:25:27 - INFO - __main__ - ['not_duplicate']
06/24/2022 17:25:27 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 17:25:27 - INFO - __main__ - ['duplicate']
06/24/2022 17:25:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:25:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 17:25:29 - INFO - __main__ - Starting training!
06/24/2022 17:25:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:26:27 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 17:34:54 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_87_0.0003_8_predictions.txt
06/24/2022 17:34:54 - INFO - __main__ - ACC on test data: 0.6157
06/24/2022 17:34:54 - INFO - __main__ - prefix=glue-qqp_16_87, lr=0.0003, bsz=8, dev_performance=0.71875, test_performance=0.6156814246846402
06/24/2022 17:34:54 - INFO - __main__ - Running ... prefix=glue-qqp_16_87, lr=0.0002, bsz=8 ...
06/24/2022 17:34:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:34:55 - INFO - __main__ - Printing 3 examples
06/24/2022 17:34:55 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:34:55 - INFO - __main__ - ['duplicate']
06/24/2022 17:34:55 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 17:34:55 - INFO - __main__ - ['duplicate']
06/24/2022 17:34:55 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:34:55 - INFO - __main__ - ['duplicate']
06/24/2022 17:34:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 17:34:55 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:34:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 17:34:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:34:55 - INFO - __main__ - Printing 3 examples
06/24/2022 17:34:55 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 17:34:55 - INFO - __main__ - ['duplicate']
06/24/2022 17:34:55 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 17:34:55 - INFO - __main__ - ['duplicate']
06/24/2022 17:34:55 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 17:34:55 - INFO - __main__ - ['duplicate']
06/24/2022 17:34:55 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:34:55 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:34:55 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 17:34:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 17:34:59 - INFO - __main__ - Starting training!
06/24/2022 17:35:02 - INFO - __main__ - Step 10 Global step 10 Train loss 17.901842 on epoch=4
06/24/2022 17:35:04 - INFO - __main__ - Step 20 Global step 20 Train loss 15.836893 on epoch=9
06/24/2022 17:35:07 - INFO - __main__ - Step 30 Global step 30 Train loss 11.365480 on epoch=14
06/24/2022 17:35:09 - INFO - __main__ - Step 40 Global step 40 Train loss 8.210039 on epoch=19
06/24/2022 17:35:12 - INFO - __main__ - Step 50 Global step 50 Train loss 7.090971 on epoch=24
06/24/2022 17:35:12 - INFO - __main__ - Global step 50 Train loss 12.081046 ACC 0.03125 on epoch=24
06/24/2022 17:35:15 - INFO - __main__ - Step 60 Global step 60 Train loss 6.209791 on epoch=29
06/24/2022 17:35:18 - INFO - __main__ - Step 70 Global step 70 Train loss 5.642301 on epoch=34
06/24/2022 17:35:20 - INFO - __main__ - Step 80 Global step 80 Train loss 4.719738 on epoch=39
06/24/2022 17:35:23 - INFO - __main__ - Step 90 Global step 90 Train loss 4.339285 on epoch=44
06/24/2022 17:35:25 - INFO - __main__ - Step 100 Global step 100 Train loss 3.394735 on epoch=49
06/24/2022 17:35:26 - INFO - __main__ - Global step 100 Train loss 4.861170 ACC 0.15625 on epoch=49
06/24/2022 17:35:29 - INFO - __main__ - Step 110 Global step 110 Train loss 2.904494 on epoch=54
06/24/2022 17:35:31 - INFO - __main__ - Step 120 Global step 120 Train loss 2.357697 on epoch=59
06/24/2022 17:35:34 - INFO - __main__ - Step 130 Global step 130 Train loss 1.721435 on epoch=64
06/24/2022 17:35:37 - INFO - __main__ - Step 140 Global step 140 Train loss 1.921498 on epoch=69
06/24/2022 17:35:39 - INFO - __main__ - Step 150 Global step 150 Train loss 1.693634 on epoch=74
06/24/2022 17:35:40 - INFO - __main__ - Global step 150 Train loss 2.119752 ACC 0.53125 on epoch=74
06/24/2022 17:35:42 - INFO - __main__ - Step 160 Global step 160 Train loss 1.610271 on epoch=79
06/24/2022 17:35:45 - INFO - __main__ - Step 170 Global step 170 Train loss 1.721738 on epoch=84
06/24/2022 17:35:47 - INFO - __main__ - Step 180 Global step 180 Train loss 1.600217 on epoch=89
06/24/2022 17:35:50 - INFO - __main__ - Step 190 Global step 190 Train loss 1.066348 on epoch=94
06/24/2022 17:35:53 - INFO - __main__ - Step 200 Global step 200 Train loss 1.270921 on epoch=99
06/24/2022 17:35:53 - INFO - __main__ - Global step 200 Train loss 1.453899 ACC 0.375 on epoch=99
06/24/2022 17:35:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.850573 on epoch=104
06/24/2022 17:35:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.922826 on epoch=109
06/24/2022 17:36:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.976922 on epoch=114
06/24/2022 17:36:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.542273 on epoch=119
06/24/2022 17:36:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.684016 on epoch=124
06/24/2022 17:36:06 - INFO - __main__ - Global step 250 Train loss 0.795322 ACC 0.5 on epoch=124
06/24/2022 17:36:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.460839 on epoch=129
06/24/2022 17:36:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.619751 on epoch=134
06/24/2022 17:36:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.636176 on epoch=139
06/24/2022 17:36:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.494281 on epoch=144
06/24/2022 17:36:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.525636 on epoch=149
06/24/2022 17:36:19 - INFO - __main__ - Global step 300 Train loss 0.547337 ACC 0.5 on epoch=149
06/24/2022 17:36:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.651000 on epoch=154
06/24/2022 17:36:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.508166 on epoch=159
06/24/2022 17:36:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.490302 on epoch=164
06/24/2022 17:36:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.602087 on epoch=169
06/24/2022 17:36:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.742872 on epoch=174
06/24/2022 17:36:32 - INFO - __main__ - Global step 350 Train loss 0.598885 ACC 0.5 on epoch=174
06/24/2022 17:36:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.487144 on epoch=179
06/24/2022 17:36:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.401701 on epoch=184
06/24/2022 17:36:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.418950 on epoch=189
06/24/2022 17:36:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.357088 on epoch=194
06/24/2022 17:36:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.523456 on epoch=199
06/24/2022 17:36:46 - INFO - __main__ - Global step 400 Train loss 0.437668 ACC 0.5 on epoch=199
06/24/2022 17:36:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.442552 on epoch=204
06/24/2022 17:36:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.526808 on epoch=209
06/24/2022 17:36:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.414491 on epoch=214
06/24/2022 17:36:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.493103 on epoch=219
06/24/2022 17:36:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.373794 on epoch=224
06/24/2022 17:36:59 - INFO - __main__ - Global step 450 Train loss 0.450150 ACC 0.5 on epoch=224
06/24/2022 17:37:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.356917 on epoch=229
06/24/2022 17:37:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.527429 on epoch=234
06/24/2022 17:37:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.434401 on epoch=239
06/24/2022 17:37:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.427031 on epoch=244
06/24/2022 17:37:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.384010 on epoch=249
06/24/2022 17:37:12 - INFO - __main__ - Global step 500 Train loss 0.425957 ACC 0.5625 on epoch=249
06/24/2022 17:37:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.392596 on epoch=254
06/24/2022 17:37:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.460598 on epoch=259
06/24/2022 17:37:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.366564 on epoch=264
06/24/2022 17:37:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.433844 on epoch=269
06/24/2022 17:37:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.548940 on epoch=274
06/24/2022 17:37:25 - INFO - __main__ - Global step 550 Train loss 0.440509 ACC 0.53125 on epoch=274
06/24/2022 17:37:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.506271 on epoch=279
06/24/2022 17:37:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.373396 on epoch=284
06/24/2022 17:37:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.480887 on epoch=289
06/24/2022 17:37:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.452081 on epoch=294
06/24/2022 17:37:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.346407 on epoch=299
06/24/2022 17:37:39 - INFO - __main__ - Global step 600 Train loss 0.431808 ACC 0.53125 on epoch=299
06/24/2022 17:37:39 - INFO - __main__ - save last model!
06/24/2022 17:37:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:37:39 - INFO - __main__ - Printing 3 examples
06/24/2022 17:37:39 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:37:39 - INFO - __main__ - ['duplicate']
06/24/2022 17:37:39 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 17:37:39 - INFO - __main__ - ['duplicate']
06/24/2022 17:37:39 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:37:39 - INFO - __main__ - ['duplicate']
06/24/2022 17:37:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 17:37:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:37:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 17:37:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:37:39 - INFO - __main__ - Printing 3 examples
06/24/2022 17:37:39 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 17:37:39 - INFO - __main__ - ['duplicate']
06/24/2022 17:37:39 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 17:37:39 - INFO - __main__ - ['duplicate']
06/24/2022 17:37:39 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 17:37:39 - INFO - __main__ - ['duplicate']
06/24/2022 17:37:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:37:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:37:39 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 17:37:41 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 17:37:42 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 17:37:42 - INFO - __main__ - Printing 3 examples
06/24/2022 17:37:42 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 17:37:42 - INFO - __main__ - ['not_duplicate']
06/24/2022 17:37:42 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 17:37:42 - INFO - __main__ - ['not_duplicate']
06/24/2022 17:37:42 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 17:37:42 - INFO - __main__ - ['duplicate']
06/24/2022 17:37:42 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:37:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 17:37:43 - INFO - __main__ - Starting training!
06/24/2022 17:38:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:38:41 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 17:47:37 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_87_0.0002_8_predictions.txt
06/24/2022 17:47:37 - INFO - __main__ - ACC on test data: 0.5761
06/24/2022 17:47:37 - INFO - __main__ - prefix=glue-qqp_16_87, lr=0.0002, bsz=8, dev_performance=0.5625, test_performance=0.5760821172396735
06/24/2022 17:47:37 - INFO - __main__ - Running ... prefix=glue-qqp_16_87, lr=0.0001, bsz=8 ...
06/24/2022 17:47:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:47:38 - INFO - __main__ - Printing 3 examples
06/24/2022 17:47:38 - INFO - __main__ -  [glue-qqp] question 1: What do you think about Modi government banning 500 & 1000 currency note from 9th November? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:47:38 - INFO - __main__ - ['duplicate']
06/24/2022 17:47:38 - INFO - __main__ -  [glue-qqp] question 1: What are the techniques of ASO in 2016? [SEP] question 2: Which are the techniques that helps to do ASO?
06/24/2022 17:47:38 - INFO - __main__ - ['duplicate']
06/24/2022 17:47:38 - INFO - __main__ -  [glue-qqp] question 1: Why does 500 and 1000 Rs notes banned by GOI and new notes of 500 and 2000 are issued? [SEP] question 2: What do you think about banning 500 and 1000 rupee notes in India?
06/24/2022 17:47:38 - INFO - __main__ - ['duplicate']
06/24/2022 17:47:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 17:47:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:47:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 17:47:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:47:38 - INFO - __main__ - Printing 3 examples
06/24/2022 17:47:38 - INFO - __main__ -  [glue-qqp] question 1: Why do so may people ask questions on Quora that can easily be found by a simple Google searh? [SEP] question 2: Why do people bother to ask questions on Quora they could just google to get the answer?
06/24/2022 17:47:38 - INFO - __main__ - ['duplicate']
06/24/2022 17:47:38 - INFO - __main__ -  [glue-qqp] question 1: What is the importance of conserving natural resources? [SEP] question 2: What is the necessity of conservation of natural resources?
06/24/2022 17:47:38 - INFO - __main__ - ['duplicate']
06/24/2022 17:47:38 - INFO - __main__ -  [glue-qqp] question 1: What was the best day of your life so far? [SEP] question 2: Can you share best day of your life?
06/24/2022 17:47:38 - INFO - __main__ - ['duplicate']
06/24/2022 17:47:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:47:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:47:38 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 17:47:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 17:47:42 - INFO - __main__ - Starting training!
06/24/2022 17:47:44 - INFO - __main__ - Step 10 Global step 10 Train loss 17.932980 on epoch=4
06/24/2022 17:47:47 - INFO - __main__ - Step 20 Global step 20 Train loss 16.691219 on epoch=9
06/24/2022 17:47:49 - INFO - __main__ - Step 30 Global step 30 Train loss 14.565748 on epoch=14
06/24/2022 17:47:52 - INFO - __main__ - Step 40 Global step 40 Train loss 13.169930 on epoch=19
06/24/2022 17:47:54 - INFO - __main__ - Step 50 Global step 50 Train loss 10.376322 on epoch=24
06/24/2022 17:47:56 - INFO - __main__ - Global step 50 Train loss 14.547238 ACC 0.0 on epoch=24
06/24/2022 17:47:59 - INFO - __main__ - Step 60 Global step 60 Train loss 9.223043 on epoch=29
06/24/2022 17:48:02 - INFO - __main__ - Step 70 Global step 70 Train loss 8.514039 on epoch=34
06/24/2022 17:48:04 - INFO - __main__ - Step 80 Global step 80 Train loss 6.817890 on epoch=39
06/24/2022 17:48:07 - INFO - __main__ - Step 90 Global step 90 Train loss 7.784029 on epoch=44
06/24/2022 17:48:09 - INFO - __main__ - Step 100 Global step 100 Train loss 6.475699 on epoch=49
06/24/2022 17:48:10 - INFO - __main__ - Global step 100 Train loss 7.762939 ACC 0.0 on epoch=49
06/24/2022 17:48:12 - INFO - __main__ - Step 110 Global step 110 Train loss 5.570372 on epoch=54
06/24/2022 17:48:15 - INFO - __main__ - Step 120 Global step 120 Train loss 5.061135 on epoch=59
06/24/2022 17:48:17 - INFO - __main__ - Step 130 Global step 130 Train loss 4.972918 on epoch=64
06/24/2022 17:48:20 - INFO - __main__ - Step 140 Global step 140 Train loss 4.206675 on epoch=69
06/24/2022 17:48:22 - INFO - __main__ - Step 150 Global step 150 Train loss 4.086426 on epoch=74
06/24/2022 17:48:23 - INFO - __main__ - Global step 150 Train loss 4.779505 ACC 0.4375 on epoch=74
06/24/2022 17:48:26 - INFO - __main__ - Step 160 Global step 160 Train loss 3.922182 on epoch=79
06/24/2022 17:48:28 - INFO - __main__ - Step 170 Global step 170 Train loss 4.158179 on epoch=84
06/24/2022 17:48:31 - INFO - __main__ - Step 180 Global step 180 Train loss 3.816730 on epoch=89
06/24/2022 17:48:33 - INFO - __main__ - Step 190 Global step 190 Train loss 3.425583 on epoch=94
06/24/2022 17:48:36 - INFO - __main__ - Step 200 Global step 200 Train loss 3.034215 on epoch=99
06/24/2022 17:48:36 - INFO - __main__ - Global step 200 Train loss 3.671378 ACC 0.53125 on epoch=99
06/24/2022 17:48:39 - INFO - __main__ - Step 210 Global step 210 Train loss 2.802343 on epoch=104
06/24/2022 17:48:41 - INFO - __main__ - Step 220 Global step 220 Train loss 2.939749 on epoch=109
06/24/2022 17:48:44 - INFO - __main__ - Step 230 Global step 230 Train loss 2.368034 on epoch=114
06/24/2022 17:48:46 - INFO - __main__ - Step 240 Global step 240 Train loss 2.249464 on epoch=119
06/24/2022 17:48:49 - INFO - __main__ - Step 250 Global step 250 Train loss 1.850100 on epoch=124
06/24/2022 17:48:49 - INFO - __main__ - Global step 250 Train loss 2.441938 ACC 0.5 on epoch=124
06/24/2022 17:48:52 - INFO - __main__ - Step 260 Global step 260 Train loss 2.038545 on epoch=129
06/24/2022 17:48:54 - INFO - __main__ - Step 270 Global step 270 Train loss 1.475860 on epoch=134
06/24/2022 17:48:57 - INFO - __main__ - Step 280 Global step 280 Train loss 1.504545 on epoch=139
06/24/2022 17:48:59 - INFO - __main__ - Step 290 Global step 290 Train loss 1.431822 on epoch=144
06/24/2022 17:49:02 - INFO - __main__ - Step 300 Global step 300 Train loss 1.309986 on epoch=149
06/24/2022 17:49:02 - INFO - __main__ - Global step 300 Train loss 1.552152 ACC 0.46875 on epoch=149
06/24/2022 17:49:04 - INFO - __main__ - Step 310 Global step 310 Train loss 1.629966 on epoch=154
06/24/2022 17:49:07 - INFO - __main__ - Step 320 Global step 320 Train loss 1.359257 on epoch=159
06/24/2022 17:49:09 - INFO - __main__ - Step 330 Global step 330 Train loss 1.017827 on epoch=164
06/24/2022 17:49:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.877015 on epoch=169
06/24/2022 17:49:14 - INFO - __main__ - Step 350 Global step 350 Train loss 1.502156 on epoch=174
06/24/2022 17:49:15 - INFO - __main__ - Global step 350 Train loss 1.277244 ACC 0.46875 on epoch=174
06/24/2022 17:49:17 - INFO - __main__ - Step 360 Global step 360 Train loss 1.475553 on epoch=179
06/24/2022 17:49:20 - INFO - __main__ - Step 370 Global step 370 Train loss 1.044616 on epoch=184
06/24/2022 17:49:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.893513 on epoch=189
06/24/2022 17:49:25 - INFO - __main__ - Step 390 Global step 390 Train loss 1.183740 on epoch=194
06/24/2022 17:49:27 - INFO - __main__ - Step 400 Global step 400 Train loss 1.055929 on epoch=199
06/24/2022 17:49:28 - INFO - __main__ - Global step 400 Train loss 1.130670 ACC 0.46875 on epoch=199
06/24/2022 17:49:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.828749 on epoch=204
06/24/2022 17:49:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.832622 on epoch=209
06/24/2022 17:49:35 - INFO - __main__ - Step 430 Global step 430 Train loss 1.320096 on epoch=214
06/24/2022 17:49:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.842145 on epoch=219
06/24/2022 17:49:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.795566 on epoch=224
06/24/2022 17:49:41 - INFO - __main__ - Global step 450 Train loss 0.923835 ACC 0.53125 on epoch=224
06/24/2022 17:49:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.770166 on epoch=229
06/24/2022 17:49:46 - INFO - __main__ - Step 470 Global step 470 Train loss 1.022161 on epoch=234
06/24/2022 17:49:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.808014 on epoch=239
06/24/2022 17:49:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.716608 on epoch=244
06/24/2022 17:49:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.720801 on epoch=249
06/24/2022 17:49:54 - INFO - __main__ - Global step 500 Train loss 0.807550 ACC 0.53125 on epoch=249
06/24/2022 17:49:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.732171 on epoch=254
06/24/2022 17:49:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.812526 on epoch=259
06/24/2022 17:50:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.669994 on epoch=264
06/24/2022 17:50:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.584040 on epoch=269
06/24/2022 17:50:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.524249 on epoch=274
06/24/2022 17:50:07 - INFO - __main__ - Global step 550 Train loss 0.664596 ACC 0.5 on epoch=274
06/24/2022 17:50:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.563589 on epoch=279
06/24/2022 17:50:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.547468 on epoch=284
06/24/2022 17:50:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.468214 on epoch=289
06/24/2022 17:50:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.673701 on epoch=294
06/24/2022 17:50:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.565675 on epoch=299
06/24/2022 17:50:20 - INFO - __main__ - Global step 600 Train loss 0.563730 ACC 0.53125 on epoch=299
06/24/2022 17:50:20 - INFO - __main__ - save last model!
06/24/2022 17:50:22 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 17:50:23 - INFO - __main__ - Start tokenizing ... 40430 instances
06/24/2022 17:50:23 - INFO - __main__ - Printing 3 examples
06/24/2022 17:50:23 - INFO - __main__ -  [glue-qqp] question 1: Why are African-Americans so beautiful? [SEP] question 2: Why are hispanics so beautiful?
06/24/2022 17:50:23 - INFO - __main__ - ['not_duplicate']
06/24/2022 17:50:23 - INFO - __main__ -  [glue-qqp] question 1: I want to pursue PhD in Computer Science about social network,what is the open problem in social networks? [SEP] question 2: I handle social media for a non-profit. Should I start going to social media networking events? Are there any good ones in the bay area?
06/24/2022 17:50:23 - INFO - __main__ - ['not_duplicate']
06/24/2022 17:50:23 - INFO - __main__ -  [glue-qqp] question 1: Is there a reason why we should travel alone? [SEP] question 2: What are some reasons to travel alone?
06/24/2022 17:50:23 - INFO - __main__ - ['duplicate']
06/24/2022 17:50:23 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:50:42 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:51:25 - INFO - __main__ - Loaded 40430 examples from test data
06/24/2022 17:59:34 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-glue-qqp/glue-qqp_16_87_0.0001_8_predictions.txt
06/24/2022 17:59:35 - INFO - __main__ - ACC on test data: 0.4044
06/24/2022 17:59:35 - INFO - __main__ - prefix=glue-qqp_16_87, lr=0.0001, bsz=8, dev_performance=0.53125, test_performance=0.40442740539203564
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (91288): No such process
Task: medical_questions_pairs, Checkpoint: None, Identifier: T5-base-ft-nopara2para
Output directory () already exists and is not empty.
06/24/2022 17:59:40 - INFO - __main__ - Namespace(task_dir='data/medical_questions_pairs/', task_name='medical_questions_pairs', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-medical_questions_pairs', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/24/2022 17:59:40 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-medical_questions_pairs
06/24/2022 17:59:40 - INFO - __main__ - Namespace(task_dir='data/medical_questions_pairs/', task_name='medical_questions_pairs', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-medical_questions_pairs', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/24/2022 17:59:40 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-medical_questions_pairs
06/24/2022 17:59:42 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/24/2022 17:59:42 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/24/2022 17:59:42 - INFO - __main__ - args.device: cuda:0
06/24/2022 17:59:42 - INFO - __main__ - args.device: cuda:1
06/24/2022 17:59:42 - INFO - __main__ - Using 2 gpus
06/24/2022 17:59:42 - INFO - __main__ - Using 2 gpus
06/24/2022 17:59:42 - INFO - __main__ - Fine-tuning the following samples: ['medical_questions_pairs_16_100', 'medical_questions_pairs_16_13', 'medical_questions_pairs_16_21', 'medical_questions_pairs_16_42', 'medical_questions_pairs_16_87']
06/24/2022 17:59:42 - INFO - __main__ - Fine-tuning the following samples: ['medical_questions_pairs_16_100', 'medical_questions_pairs_16_13', 'medical_questions_pairs_16_21', 'medical_questions_pairs_16_42', 'medical_questions_pairs_16_87']
06/24/2022 17:59:46 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_100, lr=0.0005, bsz=8 ...
06/24/2022 17:59:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:59:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:59:47 - INFO - __main__ - Printing 3 examples
06/24/2022 17:59:47 - INFO - __main__ - Printing 3 examples
06/24/2022 17:59:47 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 17:59:47 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 17:59:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 17:59:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 17:59:47 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 17:59:47 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 17:59:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 17:59:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 17:59:47 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 17:59:47 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 17:59:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 17:59:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 17:59:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 17:59:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 17:59:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:59:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:59:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 17:59:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 17:59:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:59:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 17:59:47 - INFO - __main__ - Printing 3 examples
06/24/2022 17:59:47 - INFO - __main__ - Printing 3 examples
06/24/2022 17:59:47 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 17:59:47 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 17:59:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 17:59:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 17:59:47 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 17:59:47 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 17:59:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 17:59:47 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 17:59:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 17:59:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 17:59:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:59:47 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 17:59:47 - INFO - __main__ - ['Dissimilar']
06/24/2022 17:59:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 17:59:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:59:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 17:59:47 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 17:59:47 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 17:59:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 17:59:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 17:59:52 - INFO - __main__ - Starting training!
06/24/2022 17:59:52 - INFO - __main__ - Starting training!
06/24/2022 17:59:54 - INFO - __main__ - Step 10 Global step 10 Train loss 18.292217 on epoch=4
06/24/2022 17:59:57 - INFO - __main__ - Step 20 Global step 20 Train loss 11.763894 on epoch=9
06/24/2022 17:59:59 - INFO - __main__ - Step 30 Global step 30 Train loss 7.198427 on epoch=14
06/24/2022 18:00:02 - INFO - __main__ - Step 40 Global step 40 Train loss 4.383221 on epoch=19
06/24/2022 18:00:04 - INFO - __main__ - Step 50 Global step 50 Train loss 3.636153 on epoch=24
06/24/2022 18:00:04 - INFO - __main__ - Global step 50 Train loss 9.054782 ACC 0.5 on epoch=24
06/24/2022 18:00:07 - INFO - __main__ - Step 60 Global step 60 Train loss 2.264397 on epoch=29
06/24/2022 18:00:10 - INFO - __main__ - Step 70 Global step 70 Train loss 1.799293 on epoch=34
06/24/2022 18:00:12 - INFO - __main__ - Step 80 Global step 80 Train loss 1.042050 on epoch=39
06/24/2022 18:00:15 - INFO - __main__ - Step 90 Global step 90 Train loss 1.208293 on epoch=44
06/24/2022 18:00:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.856700 on epoch=49
06/24/2022 18:00:18 - INFO - __main__ - Global step 100 Train loss 1.434146 ACC 0.375 on epoch=49
06/24/2022 18:00:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.689891 on epoch=54
06/24/2022 18:00:23 - INFO - __main__ - Step 120 Global step 120 Train loss 0.635929 on epoch=59
06/24/2022 18:00:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.683454 on epoch=64
06/24/2022 18:00:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.554282 on epoch=69
06/24/2022 18:00:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.593168 on epoch=74
06/24/2022 18:00:30 - INFO - __main__ - Global step 150 Train loss 0.631345 ACC 0.46875 on epoch=74
06/24/2022 18:00:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.436952 on epoch=79
06/24/2022 18:00:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.497202 on epoch=84
06/24/2022 18:00:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.627608 on epoch=89
06/24/2022 18:00:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.599124 on epoch=94
06/24/2022 18:00:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.510811 on epoch=99
06/24/2022 18:00:43 - INFO - __main__ - Global step 200 Train loss 0.534340 ACC 0.46875 on epoch=99
06/24/2022 18:00:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.445314 on epoch=104
06/24/2022 18:00:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.367169 on epoch=109
06/24/2022 18:00:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.296885 on epoch=114
06/24/2022 18:00:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.319538 on epoch=119
06/24/2022 18:00:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.272624 on epoch=124
06/24/2022 18:00:55 - INFO - __main__ - Global step 250 Train loss 0.340306 ACC 0.5 on epoch=124
06/24/2022 18:00:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.372177 on epoch=129
06/24/2022 18:01:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.292544 on epoch=134
06/24/2022 18:01:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.207753 on epoch=139
06/24/2022 18:01:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.304217 on epoch=144
06/24/2022 18:01:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.213044 on epoch=149
06/24/2022 18:01:08 - INFO - __main__ - Global step 300 Train loss 0.277947 ACC 0.5 on epoch=149
06/24/2022 18:01:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.195807 on epoch=154
06/24/2022 18:01:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.132290 on epoch=159
06/24/2022 18:01:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.062740 on epoch=164
06/24/2022 18:01:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.070258 on epoch=169
06/24/2022 18:01:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.036428 on epoch=174
06/24/2022 18:01:20 - INFO - __main__ - Global step 350 Train loss 0.099504 ACC 0.53125 on epoch=174
06/24/2022 18:01:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.083139 on epoch=179
06/24/2022 18:01:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.011822 on epoch=184
06/24/2022 18:01:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.013819 on epoch=189
06/24/2022 18:01:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.054732 on epoch=194
06/24/2022 18:01:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.036381 on epoch=199
06/24/2022 18:01:34 - INFO - __main__ - Global step 400 Train loss 0.039979 ACC 0.5 on epoch=199
06/24/2022 18:01:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.015582 on epoch=204
06/24/2022 18:01:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.006432 on epoch=209
06/24/2022 18:01:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.016510 on epoch=214
06/24/2022 18:01:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.064939 on epoch=219
06/24/2022 18:01:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.013026 on epoch=224
06/24/2022 18:01:47 - INFO - __main__ - Global step 450 Train loss 0.023298 ACC 0.5 on epoch=224
06/24/2022 18:01:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.022012 on epoch=229
06/24/2022 18:01:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.011207 on epoch=234
06/24/2022 18:01:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.005794 on epoch=239
06/24/2022 18:01:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002037 on epoch=244
06/24/2022 18:01:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.117404 on epoch=249
06/24/2022 18:01:59 - INFO - __main__ - Global step 500 Train loss 0.031691 ACC 0.5625 on epoch=249
06/24/2022 18:02:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.221278 on epoch=254
06/24/2022 18:02:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.089254 on epoch=259
06/24/2022 18:02:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.267143 on epoch=264
06/24/2022 18:02:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.550492 on epoch=269
06/24/2022 18:02:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.245829 on epoch=274
06/24/2022 18:02:12 - INFO - __main__ - Global step 550 Train loss 0.274799 ACC 0.46875 on epoch=274
06/24/2022 18:02:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.122098 on epoch=279
06/24/2022 18:02:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.200334 on epoch=284
06/24/2022 18:02:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.304807 on epoch=289
06/24/2022 18:02:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.480651 on epoch=294
06/24/2022 18:02:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.253897 on epoch=299
06/24/2022 18:02:25 - INFO - __main__ - Global step 600 Train loss 0.272357 ACC 0.5 on epoch=299
06/24/2022 18:02:25 - INFO - __main__ - save last model!
06/24/2022 18:02:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:02:26 - INFO - __main__ - Printing 3 examples
06/24/2022 18:02:26 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 18:02:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:26 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 18:02:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:26 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 18:02:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:02:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:02:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:02:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:02:26 - INFO - __main__ - Printing 3 examples
06/24/2022 18:02:26 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 18:02:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:26 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 18:02:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:26 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 18:02:26 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:02:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:02:26 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:02:28 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:02:28 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:02:28 - INFO - __main__ - Printing 3 examples
06/24/2022 18:02:28 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:02:28 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:28 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:02:28 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:28 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:02:28 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:02:28 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:02:29 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:02:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:02:30 - INFO - __main__ - Starting training!
06/24/2022 18:02:34 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_100_0.0005_8_predictions.txt
06/24/2022 18:02:34 - INFO - __main__ - ACC on test data: 0.5475
06/24/2022 18:02:34 - INFO - __main__ - prefix=medical_questions_pairs_16_100, lr=0.0005, bsz=8, dev_performance=0.5625, test_performance=0.5475409836065573
06/24/2022 18:02:34 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_100, lr=0.0003, bsz=8 ...
06/24/2022 18:02:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:02:35 - INFO - __main__ - Printing 3 examples
06/24/2022 18:02:35 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 18:02:35 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:35 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 18:02:35 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:35 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 18:02:35 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:02:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:02:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:02:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:02:35 - INFO - __main__ - Printing 3 examples
06/24/2022 18:02:35 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 18:02:35 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:35 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 18:02:35 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:35 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 18:02:35 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:02:35 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:02:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:02:35 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:02:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:02:39 - INFO - __main__ - Starting training!
06/24/2022 18:02:41 - INFO - __main__ - Step 10 Global step 10 Train loss 18.512966 on epoch=4
06/24/2022 18:02:43 - INFO - __main__ - Step 20 Global step 20 Train loss 13.394961 on epoch=9
06/24/2022 18:02:46 - INFO - __main__ - Step 30 Global step 30 Train loss 8.946405 on epoch=14
06/24/2022 18:02:48 - INFO - __main__ - Step 40 Global step 40 Train loss 6.171593 on epoch=19
06/24/2022 18:02:51 - INFO - __main__ - Step 50 Global step 50 Train loss 4.848401 on epoch=24
06/24/2022 18:02:51 - INFO - __main__ - Global step 50 Train loss 10.374866 ACC 0.5 on epoch=24
06/24/2022 18:02:54 - INFO - __main__ - Step 60 Global step 60 Train loss 3.417814 on epoch=29
06/24/2022 18:02:56 - INFO - __main__ - Step 70 Global step 70 Train loss 2.316872 on epoch=34
06/24/2022 18:02:59 - INFO - __main__ - Step 80 Global step 80 Train loss 1.597111 on epoch=39
06/24/2022 18:03:01 - INFO - __main__ - Step 90 Global step 90 Train loss 1.315001 on epoch=44
06/24/2022 18:03:04 - INFO - __main__ - Step 100 Global step 100 Train loss 0.912171 on epoch=49
06/24/2022 18:03:04 - INFO - __main__ - Global step 100 Train loss 1.911794 ACC 0.53125 on epoch=49
06/24/2022 18:03:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.900126 on epoch=54
06/24/2022 18:03:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.863416 on epoch=59
06/24/2022 18:03:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.659087 on epoch=64
06/24/2022 18:03:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.622054 on epoch=69
06/24/2022 18:03:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.660814 on epoch=74
06/24/2022 18:03:18 - INFO - __main__ - Global step 150 Train loss 0.741099 ACC 0.5 on epoch=74
06/24/2022 18:03:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.702142 on epoch=79
06/24/2022 18:03:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.509159 on epoch=84
06/24/2022 18:03:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.529595 on epoch=89
06/24/2022 18:03:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.474778 on epoch=94
06/24/2022 18:03:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.418135 on epoch=99
06/24/2022 18:03:31 - INFO - __main__ - Global step 200 Train loss 0.526762 ACC 0.46875 on epoch=99
06/24/2022 18:03:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.516649 on epoch=104
06/24/2022 18:03:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.478963 on epoch=109
06/24/2022 18:03:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.330470 on epoch=114
06/24/2022 18:03:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.548578 on epoch=119
06/24/2022 18:03:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.464475 on epoch=124
06/24/2022 18:03:43 - INFO - __main__ - Global step 250 Train loss 0.467827 ACC 0.5 on epoch=124
06/24/2022 18:03:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.478038 on epoch=129
06/24/2022 18:03:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.419422 on epoch=134
06/24/2022 18:03:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.451495 on epoch=139
06/24/2022 18:03:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.491247 on epoch=144
06/24/2022 18:03:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.351802 on epoch=149
06/24/2022 18:03:56 - INFO - __main__ - Global step 300 Train loss 0.438401 ACC 0.5 on epoch=149
06/24/2022 18:03:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.366699 on epoch=154
06/24/2022 18:04:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.356871 on epoch=159
06/24/2022 18:04:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.510481 on epoch=164
06/24/2022 18:04:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.279363 on epoch=169
06/24/2022 18:04:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.421021 on epoch=174
06/24/2022 18:04:09 - INFO - __main__ - Global step 350 Train loss 0.386887 ACC 0.59375 on epoch=174
06/24/2022 18:04:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.371612 on epoch=179
06/24/2022 18:04:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.219842 on epoch=184
06/24/2022 18:04:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.277974 on epoch=189
06/24/2022 18:04:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.299365 on epoch=194
06/24/2022 18:04:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.301898 on epoch=199
06/24/2022 18:04:23 - INFO - __main__ - Global step 400 Train loss 0.294138 ACC 0.5 on epoch=199
06/24/2022 18:04:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.214362 on epoch=204
06/24/2022 18:04:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.212177 on epoch=209
06/24/2022 18:04:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.175158 on epoch=214
06/24/2022 18:04:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.216231 on epoch=219
06/24/2022 18:04:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.189886 on epoch=224
06/24/2022 18:04:35 - INFO - __main__ - Global step 450 Train loss 0.201563 ACC 0.5 on epoch=224
06/24/2022 18:04:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.220386 on epoch=229
06/24/2022 18:04:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.150696 on epoch=234
06/24/2022 18:04:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.115919 on epoch=239
06/24/2022 18:04:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.121634 on epoch=244
06/24/2022 18:04:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.268814 on epoch=249
06/24/2022 18:04:48 - INFO - __main__ - Global step 500 Train loss 0.175490 ACC 0.4375 on epoch=249
06/24/2022 18:04:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.223831 on epoch=254
06/24/2022 18:04:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.277812 on epoch=259
06/24/2022 18:04:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.277348 on epoch=264
06/24/2022 18:04:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.275595 on epoch=269
06/24/2022 18:05:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.243429 on epoch=274
06/24/2022 18:05:01 - INFO - __main__ - Global step 550 Train loss 0.259603 ACC 0.4375 on epoch=274
06/24/2022 18:05:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.287457 on epoch=279
06/24/2022 18:05:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.163306 on epoch=284
06/24/2022 18:05:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.211434 on epoch=289
06/24/2022 18:05:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.256914 on epoch=294
06/24/2022 18:05:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.139433 on epoch=299
06/24/2022 18:05:14 - INFO - __main__ - Global step 600 Train loss 0.211709 ACC 0.375 on epoch=299
06/24/2022 18:05:14 - INFO - __main__ - save last model!
06/24/2022 18:05:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:05:15 - INFO - __main__ - Printing 3 examples
06/24/2022 18:05:15 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 18:05:15 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:15 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 18:05:15 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:15 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 18:05:15 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:05:15 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:05:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:05:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:05:15 - INFO - __main__ - Printing 3 examples
06/24/2022 18:05:15 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 18:05:15 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:15 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 18:05:15 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:15 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 18:05:15 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:15 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:05:15 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:05:15 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:05:17 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:05:17 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:05:17 - INFO - __main__ - Printing 3 examples
06/24/2022 18:05:17 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:05:17 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:17 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:05:17 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:17 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:05:17 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:17 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:05:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:05:18 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:05:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:05:19 - INFO - __main__ - Starting training!
06/24/2022 18:05:23 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_100_0.0003_8_predictions.txt
06/24/2022 18:05:23 - INFO - __main__ - ACC on test data: 0.5295
06/24/2022 18:05:23 - INFO - __main__ - prefix=medical_questions_pairs_16_100, lr=0.0003, bsz=8, dev_performance=0.59375, test_performance=0.5295081967213114
06/24/2022 18:05:23 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_100, lr=0.0002, bsz=8 ...
06/24/2022 18:05:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:05:24 - INFO - __main__ - Printing 3 examples
06/24/2022 18:05:24 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 18:05:24 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:24 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 18:05:24 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:24 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 18:05:24 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:05:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:05:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:05:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:05:24 - INFO - __main__ - Printing 3 examples
06/24/2022 18:05:24 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 18:05:24 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:24 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 18:05:24 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:24 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 18:05:24 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:05:24 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:05:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:05:24 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:05:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:05:28 - INFO - __main__ - Starting training!
06/24/2022 18:05:30 - INFO - __main__ - Step 10 Global step 10 Train loss 18.237942 on epoch=4
06/24/2022 18:05:32 - INFO - __main__ - Step 20 Global step 20 Train loss 14.513977 on epoch=9
06/24/2022 18:05:35 - INFO - __main__ - Step 30 Global step 30 Train loss 9.943667 on epoch=14
06/24/2022 18:05:37 - INFO - __main__ - Step 40 Global step 40 Train loss 8.041578 on epoch=19
06/24/2022 18:05:40 - INFO - __main__ - Step 50 Global step 50 Train loss 6.561862 on epoch=24
06/24/2022 18:05:41 - INFO - __main__ - Global step 50 Train loss 11.459805 ACC 0.0 on epoch=24
06/24/2022 18:05:44 - INFO - __main__ - Step 60 Global step 60 Train loss 5.236145 on epoch=29
06/24/2022 18:05:46 - INFO - __main__ - Step 70 Global step 70 Train loss 4.424365 on epoch=34
06/24/2022 18:05:49 - INFO - __main__ - Step 80 Global step 80 Train loss 3.707605 on epoch=39
06/24/2022 18:05:51 - INFO - __main__ - Step 90 Global step 90 Train loss 3.142318 on epoch=44
06/24/2022 18:05:54 - INFO - __main__ - Step 100 Global step 100 Train loss 2.134815 on epoch=49
06/24/2022 18:05:54 - INFO - __main__ - Global step 100 Train loss 3.729049 ACC 0.53125 on epoch=49
06/24/2022 18:05:57 - INFO - __main__ - Step 110 Global step 110 Train loss 2.039449 on epoch=54
06/24/2022 18:06:00 - INFO - __main__ - Step 120 Global step 120 Train loss 1.799205 on epoch=59
06/24/2022 18:06:02 - INFO - __main__ - Step 130 Global step 130 Train loss 1.343674 on epoch=64
06/24/2022 18:06:04 - INFO - __main__ - Step 140 Global step 140 Train loss 1.187591 on epoch=69
06/24/2022 18:06:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.938548 on epoch=74
06/24/2022 18:06:07 - INFO - __main__ - Global step 150 Train loss 1.461693 ACC 0.46875 on epoch=74
06/24/2022 18:06:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.844516 on epoch=79
06/24/2022 18:06:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.684753 on epoch=84
06/24/2022 18:06:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.563166 on epoch=89
06/24/2022 18:06:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.769018 on epoch=94
06/24/2022 18:06:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.548014 on epoch=99
06/24/2022 18:06:20 - INFO - __main__ - Global step 200 Train loss 0.681893 ACC 0.5 on epoch=99
06/24/2022 18:06:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.600550 on epoch=104
06/24/2022 18:06:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.624826 on epoch=109
06/24/2022 18:06:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.628997 on epoch=114
06/24/2022 18:06:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.495099 on epoch=119
06/24/2022 18:06:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.496335 on epoch=124
06/24/2022 18:06:33 - INFO - __main__ - Global step 250 Train loss 0.569161 ACC 0.5 on epoch=124
06/24/2022 18:06:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.327226 on epoch=129
06/24/2022 18:06:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.439164 on epoch=134
06/24/2022 18:06:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.423400 on epoch=139
06/24/2022 18:06:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.445518 on epoch=144
06/24/2022 18:06:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.373967 on epoch=149
06/24/2022 18:06:45 - INFO - __main__ - Global step 300 Train loss 0.401855 ACC 0.59375 on epoch=149
06/24/2022 18:06:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.417346 on epoch=154
06/24/2022 18:06:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.375232 on epoch=159
06/24/2022 18:06:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.395395 on epoch=164
06/24/2022 18:06:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.394499 on epoch=169
06/24/2022 18:06:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.286188 on epoch=174
06/24/2022 18:06:58 - INFO - __main__ - Global step 350 Train loss 0.373732 ACC 0.59375 on epoch=174
06/24/2022 18:07:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.357652 on epoch=179
06/24/2022 18:07:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.214704 on epoch=184
06/24/2022 18:07:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.244549 on epoch=189
06/24/2022 18:07:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.322835 on epoch=194
06/24/2022 18:07:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.224391 on epoch=199
06/24/2022 18:07:11 - INFO - __main__ - Global step 400 Train loss 0.272826 ACC 0.5 on epoch=199
06/24/2022 18:07:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.226598 on epoch=204
06/24/2022 18:07:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.362122 on epoch=209
06/24/2022 18:07:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.154417 on epoch=214
06/24/2022 18:07:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.284636 on epoch=219
06/24/2022 18:07:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.207119 on epoch=224
06/24/2022 18:07:24 - INFO - __main__ - Global step 450 Train loss 0.246979 ACC 0.5625 on epoch=224
06/24/2022 18:07:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.156022 on epoch=229
06/24/2022 18:07:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.186044 on epoch=234
06/24/2022 18:07:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.155692 on epoch=239
06/24/2022 18:07:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.189334 on epoch=244
06/24/2022 18:07:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.153519 on epoch=249
06/24/2022 18:07:36 - INFO - __main__ - Global step 500 Train loss 0.168122 ACC 0.53125 on epoch=249
06/24/2022 18:07:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.161081 on epoch=254
06/24/2022 18:07:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.173157 on epoch=259
06/24/2022 18:07:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.168884 on epoch=264
06/24/2022 18:07:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.107929 on epoch=269
06/24/2022 18:07:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.135921 on epoch=274
06/24/2022 18:07:49 - INFO - __main__ - Global step 550 Train loss 0.149394 ACC 0.5625 on epoch=274
06/24/2022 18:07:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.118829 on epoch=279
06/24/2022 18:07:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.049706 on epoch=284
06/24/2022 18:07:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.066961 on epoch=289
06/24/2022 18:07:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.073278 on epoch=294
06/24/2022 18:08:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.111684 on epoch=299
06/24/2022 18:08:01 - INFO - __main__ - Global step 600 Train loss 0.084092 ACC 0.59375 on epoch=299
06/24/2022 18:08:01 - INFO - __main__ - save last model!
06/24/2022 18:08:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:08:02 - INFO - __main__ - Printing 3 examples
06/24/2022 18:08:02 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 18:08:02 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:02 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 18:08:02 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:02 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 18:08:02 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:08:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:08:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:08:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:08:02 - INFO - __main__ - Printing 3 examples
06/24/2022 18:08:02 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 18:08:02 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:02 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 18:08:02 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:02 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 18:08:02 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:02 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:08:02 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:08:02 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:08:04 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:08:04 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:08:04 - INFO - __main__ - Printing 3 examples
06/24/2022 18:08:04 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:08:04 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:04 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:08:04 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:04 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:08:04 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:04 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:08:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:08:05 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:08:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:08:06 - INFO - __main__ - Starting training!
06/24/2022 18:08:10 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_100_0.0002_8_predictions.txt
06/24/2022 18:08:10 - INFO - __main__ - ACC on test data: 0.5557
06/24/2022 18:08:10 - INFO - __main__ - prefix=medical_questions_pairs_16_100, lr=0.0002, bsz=8, dev_performance=0.59375, test_performance=0.5557377049180328
06/24/2022 18:08:10 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_100, lr=0.0001, bsz=8 ...
06/24/2022 18:08:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:08:11 - INFO - __main__ - Printing 3 examples
06/24/2022 18:08:11 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/24/2022 18:08:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:11 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/24/2022 18:08:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:11 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/24/2022 18:08:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:08:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:08:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:08:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:08:11 - INFO - __main__ - Printing 3 examples
06/24/2022 18:08:11 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/24/2022 18:08:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:11 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/24/2022 18:08:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:11 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/24/2022 18:08:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:08:11 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:08:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:08:11 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:08:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:08:15 - INFO - __main__ - Starting training!
06/24/2022 18:08:17 - INFO - __main__ - Step 10 Global step 10 Train loss 18.328205 on epoch=4
06/24/2022 18:08:20 - INFO - __main__ - Step 20 Global step 20 Train loss 16.588856 on epoch=9
06/24/2022 18:08:22 - INFO - __main__ - Step 30 Global step 30 Train loss 10.834525 on epoch=14
06/24/2022 18:08:24 - INFO - __main__ - Step 40 Global step 40 Train loss 9.646507 on epoch=19
06/24/2022 18:08:27 - INFO - __main__ - Step 50 Global step 50 Train loss 8.626382 on epoch=24
06/24/2022 18:08:30 - INFO - __main__ - Global step 50 Train loss 12.804895 ACC 0.0 on epoch=24
06/24/2022 18:08:32 - INFO - __main__ - Step 60 Global step 60 Train loss 8.102094 on epoch=29
06/24/2022 18:08:35 - INFO - __main__ - Step 70 Global step 70 Train loss 6.743029 on epoch=34
06/24/2022 18:08:37 - INFO - __main__ - Step 80 Global step 80 Train loss 6.342576 on epoch=39
06/24/2022 18:08:40 - INFO - __main__ - Step 90 Global step 90 Train loss 6.298541 on epoch=44
06/24/2022 18:08:42 - INFO - __main__ - Step 100 Global step 100 Train loss 4.967235 on epoch=49
06/24/2022 18:08:44 - INFO - __main__ - Global step 100 Train loss 6.490695 ACC 0.0 on epoch=49
06/24/2022 18:08:47 - INFO - __main__ - Step 110 Global step 110 Train loss 5.106722 on epoch=54
06/24/2022 18:08:49 - INFO - __main__ - Step 120 Global step 120 Train loss 4.678239 on epoch=59
06/24/2022 18:08:52 - INFO - __main__ - Step 130 Global step 130 Train loss 3.414955 on epoch=64
06/24/2022 18:08:54 - INFO - __main__ - Step 140 Global step 140 Train loss 3.441778 on epoch=69
06/24/2022 18:08:57 - INFO - __main__ - Step 150 Global step 150 Train loss 2.879587 on epoch=74
06/24/2022 18:08:57 - INFO - __main__ - Global step 150 Train loss 3.904256 ACC 0.5 on epoch=74
06/24/2022 18:09:00 - INFO - __main__ - Step 160 Global step 160 Train loss 2.753782 on epoch=79
06/24/2022 18:09:02 - INFO - __main__ - Step 170 Global step 170 Train loss 1.447110 on epoch=84
06/24/2022 18:09:05 - INFO - __main__ - Step 180 Global step 180 Train loss 1.525288 on epoch=89
06/24/2022 18:09:07 - INFO - __main__ - Step 190 Global step 190 Train loss 1.329550 on epoch=94
06/24/2022 18:09:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.967410 on epoch=99
06/24/2022 18:09:10 - INFO - __main__ - Global step 200 Train loss 1.604628 ACC 0.53125 on epoch=99
06/24/2022 18:09:13 - INFO - __main__ - Step 210 Global step 210 Train loss 1.066308 on epoch=104
06/24/2022 18:09:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.929689 on epoch=109
06/24/2022 18:09:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.873893 on epoch=114
06/24/2022 18:09:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.767223 on epoch=119
06/24/2022 18:09:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.774645 on epoch=124
06/24/2022 18:09:23 - INFO - __main__ - Global step 250 Train loss 0.882352 ACC 0.46875 on epoch=124
06/24/2022 18:09:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.746282 on epoch=129
06/24/2022 18:09:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.567542 on epoch=134
06/24/2022 18:09:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.674801 on epoch=139
06/24/2022 18:09:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.763925 on epoch=144
06/24/2022 18:09:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.649881 on epoch=149
06/24/2022 18:09:36 - INFO - __main__ - Global step 300 Train loss 0.680487 ACC 0.5 on epoch=149
06/24/2022 18:09:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.376681 on epoch=154
06/24/2022 18:09:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.344922 on epoch=159
06/24/2022 18:09:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.421222 on epoch=164
06/24/2022 18:09:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.541286 on epoch=169
06/24/2022 18:09:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.601493 on epoch=174
06/24/2022 18:09:49 - INFO - __main__ - Global step 350 Train loss 0.457121 ACC 0.5 on epoch=174
06/24/2022 18:09:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.543256 on epoch=179
06/24/2022 18:09:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.503356 on epoch=184
06/24/2022 18:09:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.577731 on epoch=189
06/24/2022 18:09:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.434373 on epoch=194
06/24/2022 18:10:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.458723 on epoch=199
06/24/2022 18:10:01 - INFO - __main__ - Global step 400 Train loss 0.503488 ACC 0.5625 on epoch=199
06/24/2022 18:10:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.713529 on epoch=204
06/24/2022 18:10:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.526357 on epoch=209
06/24/2022 18:10:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.387896 on epoch=214
06/24/2022 18:10:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.456520 on epoch=219
06/24/2022 18:10:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.457191 on epoch=224
06/24/2022 18:10:15 - INFO - __main__ - Global step 450 Train loss 0.508299 ACC 0.46875 on epoch=224
06/24/2022 18:10:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.403555 on epoch=229
06/24/2022 18:10:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.494326 on epoch=234
06/24/2022 18:10:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.332156 on epoch=239
06/24/2022 18:10:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.392230 on epoch=244
06/24/2022 18:10:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.581065 on epoch=249
06/24/2022 18:10:28 - INFO - __main__ - Global step 500 Train loss 0.440666 ACC 0.5 on epoch=249
06/24/2022 18:10:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.372220 on epoch=254
06/24/2022 18:10:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.369672 on epoch=259
06/24/2022 18:10:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.473292 on epoch=264
06/24/2022 18:10:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.512437 on epoch=269
06/24/2022 18:10:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.395049 on epoch=274
06/24/2022 18:10:40 - INFO - __main__ - Global step 550 Train loss 0.424534 ACC 0.46875 on epoch=274
06/24/2022 18:10:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.343129 on epoch=279
06/24/2022 18:10:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.376333 on epoch=284
06/24/2022 18:10:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.369782 on epoch=289
06/24/2022 18:10:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.293529 on epoch=294
06/24/2022 18:10:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.306716 on epoch=299
06/24/2022 18:10:53 - INFO - __main__ - Global step 600 Train loss 0.337898 ACC 0.59375 on epoch=299
06/24/2022 18:10:54 - INFO - __main__ - save last model!
06/24/2022 18:10:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:10:54 - INFO - __main__ - Printing 3 examples
06/24/2022 18:10:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 18:10:54 - INFO - __main__ - ['Similar']
06/24/2022 18:10:54 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 18:10:54 - INFO - __main__ - ['Similar']
06/24/2022 18:10:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 18:10:54 - INFO - __main__ - ['Similar']
06/24/2022 18:10:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:10:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:10:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:10:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:10:54 - INFO - __main__ - Printing 3 examples
06/24/2022 18:10:54 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 18:10:54 - INFO - __main__ - ['Similar']
06/24/2022 18:10:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 18:10:54 - INFO - __main__ - ['Similar']
06/24/2022 18:10:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 18:10:54 - INFO - __main__ - ['Similar']
06/24/2022 18:10:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:10:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:10:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:10:56 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:10:57 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:10:57 - INFO - __main__ - Printing 3 examples
06/24/2022 18:10:57 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:10:57 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:10:57 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:10:57 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:10:57 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:10:57 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:10:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:10:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:10:57 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:10:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:10:58 - INFO - __main__ - Starting training!
06/24/2022 18:11:02 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_100_0.0001_8_predictions.txt
06/24/2022 18:11:02 - INFO - __main__ - ACC on test data: 0.5443
06/24/2022 18:11:02 - INFO - __main__ - prefix=medical_questions_pairs_16_100, lr=0.0001, bsz=8, dev_performance=0.59375, test_performance=0.5442622950819672
06/24/2022 18:11:02 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_13, lr=0.0005, bsz=8 ...
06/24/2022 18:11:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:11:03 - INFO - __main__ - Printing 3 examples
06/24/2022 18:11:03 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 18:11:03 - INFO - __main__ - ['Similar']
06/24/2022 18:11:03 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 18:11:03 - INFO - __main__ - ['Similar']
06/24/2022 18:11:03 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 18:11:03 - INFO - __main__ - ['Similar']
06/24/2022 18:11:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:11:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:11:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:11:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:11:03 - INFO - __main__ - Printing 3 examples
06/24/2022 18:11:03 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 18:11:03 - INFO - __main__ - ['Similar']
06/24/2022 18:11:03 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 18:11:03 - INFO - __main__ - ['Similar']
06/24/2022 18:11:03 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 18:11:03 - INFO - __main__ - ['Similar']
06/24/2022 18:11:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:11:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:11:03 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:11:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:11:07 - INFO - __main__ - Starting training!
06/24/2022 18:11:09 - INFO - __main__ - Step 10 Global step 10 Train loss 18.027409 on epoch=4
06/24/2022 18:11:12 - INFO - __main__ - Step 20 Global step 20 Train loss 11.006708 on epoch=9
06/24/2022 18:11:14 - INFO - __main__ - Step 30 Global step 30 Train loss 7.006364 on epoch=14
06/24/2022 18:11:17 - INFO - __main__ - Step 40 Global step 40 Train loss 5.298869 on epoch=19
06/24/2022 18:11:19 - INFO - __main__ - Step 50 Global step 50 Train loss 2.732635 on epoch=24
06/24/2022 18:11:19 - INFO - __main__ - Global step 50 Train loss 8.814397 ACC 0.53125 on epoch=24
06/24/2022 18:11:22 - INFO - __main__ - Step 60 Global step 60 Train loss 2.166094 on epoch=29
06/24/2022 18:11:25 - INFO - __main__ - Step 70 Global step 70 Train loss 1.258247 on epoch=34
06/24/2022 18:11:27 - INFO - __main__ - Step 80 Global step 80 Train loss 1.186198 on epoch=39
06/24/2022 18:11:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.795285 on epoch=44
06/24/2022 18:11:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.738268 on epoch=49
06/24/2022 18:11:32 - INFO - __main__ - Global step 100 Train loss 1.228819 ACC 0.4375 on epoch=49
06/24/2022 18:11:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.808724 on epoch=54
06/24/2022 18:11:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.571278 on epoch=59
06/24/2022 18:11:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.566275 on epoch=64
06/24/2022 18:11:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.493760 on epoch=69
06/24/2022 18:11:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.409211 on epoch=74
06/24/2022 18:11:45 - INFO - __main__ - Global step 150 Train loss 0.569850 ACC 0.5 on epoch=74
06/24/2022 18:11:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.566741 on epoch=79
06/24/2022 18:11:50 - INFO - __main__ - Step 170 Global step 170 Train loss 0.559090 on epoch=84
06/24/2022 18:11:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.480430 on epoch=89
06/24/2022 18:11:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.410914 on epoch=94
06/24/2022 18:11:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.278462 on epoch=99
06/24/2022 18:11:58 - INFO - __main__ - Global step 200 Train loss 0.459127 ACC 0.5 on epoch=99
06/24/2022 18:12:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.323705 on epoch=104
06/24/2022 18:12:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.345549 on epoch=109
06/24/2022 18:12:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.204587 on epoch=114
06/24/2022 18:12:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.468489 on epoch=119
06/24/2022 18:12:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.209646 on epoch=124
06/24/2022 18:12:10 - INFO - __main__ - Global step 250 Train loss 0.310395 ACC 0.46875 on epoch=124
06/24/2022 18:12:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.169084 on epoch=129
06/24/2022 18:12:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.209017 on epoch=134
06/24/2022 18:12:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.171909 on epoch=139
06/24/2022 18:12:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.142410 on epoch=144
06/24/2022 18:12:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.097104 on epoch=149
06/24/2022 18:12:23 - INFO - __main__ - Global step 300 Train loss 0.157905 ACC 0.5 on epoch=149
06/24/2022 18:12:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.044965 on epoch=154
06/24/2022 18:12:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.062610 on epoch=159
06/24/2022 18:12:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.052645 on epoch=164
06/24/2022 18:12:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.161819 on epoch=169
06/24/2022 18:12:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.053467 on epoch=174
06/24/2022 18:12:36 - INFO - __main__ - Global step 350 Train loss 0.075101 ACC 0.5625 on epoch=174
06/24/2022 18:12:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.626814 on epoch=179
06/24/2022 18:12:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.100811 on epoch=184
06/24/2022 18:12:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.085802 on epoch=189
06/24/2022 18:12:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.024225 on epoch=194
06/24/2022 18:12:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.070366 on epoch=199
06/24/2022 18:12:49 - INFO - __main__ - Global step 400 Train loss 0.181604 ACC 0.5 on epoch=199
06/24/2022 18:12:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.100018 on epoch=204
06/24/2022 18:12:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.223529 on epoch=209
06/24/2022 18:12:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.110511 on epoch=214
06/24/2022 18:12:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.074925 on epoch=219
06/24/2022 18:13:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.146650 on epoch=224
06/24/2022 18:13:02 - INFO - __main__ - Global step 450 Train loss 0.131127 ACC 0.53125 on epoch=224
06/24/2022 18:13:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.035425 on epoch=229
06/24/2022 18:13:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.034022 on epoch=234
06/24/2022 18:13:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.093702 on epoch=239
06/24/2022 18:13:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.151162 on epoch=244
06/24/2022 18:13:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.035184 on epoch=249
06/24/2022 18:13:15 - INFO - __main__ - Global step 500 Train loss 0.069899 ACC 0.5625 on epoch=249
06/24/2022 18:13:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.067140 on epoch=254
06/24/2022 18:13:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.029003 on epoch=259
06/24/2022 18:13:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.042477 on epoch=264
06/24/2022 18:13:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.011313 on epoch=269
06/24/2022 18:13:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.015043 on epoch=274
06/24/2022 18:13:28 - INFO - __main__ - Global step 550 Train loss 0.032995 ACC 0.53125 on epoch=274
06/24/2022 18:13:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.046789 on epoch=279
06/24/2022 18:13:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.015395 on epoch=284
06/24/2022 18:13:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.024087 on epoch=289
06/24/2022 18:13:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.042629 on epoch=294
06/24/2022 18:13:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.041104 on epoch=299
06/24/2022 18:13:40 - INFO - __main__ - Global step 600 Train loss 0.034001 ACC 0.5625 on epoch=299
06/24/2022 18:13:40 - INFO - __main__ - save last model!
06/24/2022 18:13:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:13:41 - INFO - __main__ - Printing 3 examples
06/24/2022 18:13:41 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 18:13:41 - INFO - __main__ - ['Similar']
06/24/2022 18:13:41 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 18:13:41 - INFO - __main__ - ['Similar']
06/24/2022 18:13:41 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 18:13:41 - INFO - __main__ - ['Similar']
06/24/2022 18:13:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:13:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:13:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:13:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:13:41 - INFO - __main__ - Printing 3 examples
06/24/2022 18:13:41 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 18:13:41 - INFO - __main__ - ['Similar']
06/24/2022 18:13:41 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 18:13:41 - INFO - __main__ - ['Similar']
06/24/2022 18:13:41 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 18:13:41 - INFO - __main__ - ['Similar']
06/24/2022 18:13:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:13:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:13:41 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:13:43 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:13:43 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:13:43 - INFO - __main__ - Printing 3 examples
06/24/2022 18:13:43 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:13:43 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:13:43 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:13:43 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:13:43 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:13:43 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:13:43 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:13:44 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:13:44 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:13:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:13:45 - INFO - __main__ - Starting training!
06/24/2022 18:13:49 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_13_0.0005_8_predictions.txt
06/24/2022 18:13:49 - INFO - __main__ - ACC on test data: 0.5607
06/24/2022 18:13:49 - INFO - __main__ - prefix=medical_questions_pairs_16_13, lr=0.0005, bsz=8, dev_performance=0.5625, test_performance=0.5606557377049181
06/24/2022 18:13:49 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_13, lr=0.0003, bsz=8 ...
06/24/2022 18:13:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:13:50 - INFO - __main__ - Printing 3 examples
06/24/2022 18:13:50 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 18:13:50 - INFO - __main__ - ['Similar']
06/24/2022 18:13:50 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 18:13:50 - INFO - __main__ - ['Similar']
06/24/2022 18:13:50 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 18:13:50 - INFO - __main__ - ['Similar']
06/24/2022 18:13:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:13:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:13:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:13:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:13:50 - INFO - __main__ - Printing 3 examples
06/24/2022 18:13:50 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 18:13:50 - INFO - __main__ - ['Similar']
06/24/2022 18:13:50 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 18:13:50 - INFO - __main__ - ['Similar']
06/24/2022 18:13:50 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 18:13:50 - INFO - __main__ - ['Similar']
06/24/2022 18:13:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:13:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:13:50 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:13:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:13:54 - INFO - __main__ - Starting training!
06/24/2022 18:13:56 - INFO - __main__ - Step 10 Global step 10 Train loss 17.924284 on epoch=4
06/24/2022 18:13:59 - INFO - __main__ - Step 20 Global step 20 Train loss 13.275493 on epoch=9
06/24/2022 18:14:01 - INFO - __main__ - Step 30 Global step 30 Train loss 8.351721 on epoch=14
06/24/2022 18:14:04 - INFO - __main__ - Step 40 Global step 40 Train loss 6.344050 on epoch=19
06/24/2022 18:14:06 - INFO - __main__ - Step 50 Global step 50 Train loss 4.842376 on epoch=24
06/24/2022 18:14:08 - INFO - __main__ - Global step 50 Train loss 10.147585 ACC 0.0 on epoch=24
06/24/2022 18:14:11 - INFO - __main__ - Step 60 Global step 60 Train loss 5.268292 on epoch=29
06/24/2022 18:14:14 - INFO - __main__ - Step 70 Global step 70 Train loss 2.946805 on epoch=34
06/24/2022 18:14:16 - INFO - __main__ - Step 80 Global step 80 Train loss 1.473765 on epoch=39
06/24/2022 18:14:19 - INFO - __main__ - Step 90 Global step 90 Train loss 1.840880 on epoch=44
06/24/2022 18:14:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.877876 on epoch=49
06/24/2022 18:14:21 - INFO - __main__ - Global step 100 Train loss 2.481524 ACC 0.5 on epoch=49
06/24/2022 18:14:25 - INFO - __main__ - Step 110 Global step 110 Train loss 1.142376 on epoch=54
06/24/2022 18:14:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.598720 on epoch=59
06/24/2022 18:14:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.580652 on epoch=64
06/24/2022 18:14:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.771167 on epoch=69
06/24/2022 18:14:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.725707 on epoch=74
06/24/2022 18:14:35 - INFO - __main__ - Global step 150 Train loss 0.763724 ACC 0.5 on epoch=74
06/24/2022 18:14:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.716130 on epoch=79
06/24/2022 18:14:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.579284 on epoch=84
06/24/2022 18:14:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.667934 on epoch=89
06/24/2022 18:14:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.514468 on epoch=94
06/24/2022 18:14:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.748785 on epoch=99
06/24/2022 18:14:48 - INFO - __main__ - Global step 200 Train loss 0.645320 ACC 0.46875 on epoch=99
06/24/2022 18:14:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.632611 on epoch=104
06/24/2022 18:14:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.727111 on epoch=109
06/24/2022 18:14:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.588828 on epoch=114
06/24/2022 18:14:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.469771 on epoch=119
06/24/2022 18:15:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.423510 on epoch=124
06/24/2022 18:15:00 - INFO - __main__ - Global step 250 Train loss 0.568366 ACC 0.5 on epoch=124
06/24/2022 18:15:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.507113 on epoch=129
06/24/2022 18:15:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.440493 on epoch=134
06/24/2022 18:15:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.482143 on epoch=139
06/24/2022 18:15:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.553277 on epoch=144
06/24/2022 18:15:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.431429 on epoch=149
06/24/2022 18:15:13 - INFO - __main__ - Global step 300 Train loss 0.482891 ACC 0.5625 on epoch=149
06/24/2022 18:15:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.472908 on epoch=154
06/24/2022 18:15:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.412311 on epoch=159
06/24/2022 18:15:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.370337 on epoch=164
06/24/2022 18:15:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.434194 on epoch=169
06/24/2022 18:15:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.485385 on epoch=174
06/24/2022 18:15:26 - INFO - __main__ - Global step 350 Train loss 0.435027 ACC 0.53125 on epoch=174
06/24/2022 18:15:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.414553 on epoch=179
06/24/2022 18:15:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.379088 on epoch=184
06/24/2022 18:15:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.431762 on epoch=189
06/24/2022 18:15:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.342448 on epoch=194
06/24/2022 18:15:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.386814 on epoch=199
06/24/2022 18:15:39 - INFO - __main__ - Global step 400 Train loss 0.390933 ACC 0.5 on epoch=199
06/24/2022 18:15:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.362945 on epoch=204
06/24/2022 18:15:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.417479 on epoch=209
06/24/2022 18:15:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.349373 on epoch=214
06/24/2022 18:15:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.332044 on epoch=219
06/24/2022 18:15:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.389674 on epoch=224
06/24/2022 18:15:52 - INFO - __main__ - Global step 450 Train loss 0.370303 ACC 0.59375 on epoch=224
06/24/2022 18:15:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.391228 on epoch=229
06/24/2022 18:15:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.282639 on epoch=234
06/24/2022 18:16:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.332979 on epoch=239
06/24/2022 18:16:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.411582 on epoch=244
06/24/2022 18:16:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.274404 on epoch=249
06/24/2022 18:16:05 - INFO - __main__ - Global step 500 Train loss 0.338566 ACC 0.5 on epoch=249
06/24/2022 18:16:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.348747 on epoch=254
06/24/2022 18:16:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.377781 on epoch=259
06/24/2022 18:16:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.312571 on epoch=264
06/24/2022 18:16:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.375691 on epoch=269
06/24/2022 18:16:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.349515 on epoch=274
06/24/2022 18:16:18 - INFO - __main__ - Global step 550 Train loss 0.352861 ACC 0.59375 on epoch=274
06/24/2022 18:16:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.335116 on epoch=279
06/24/2022 18:16:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.309525 on epoch=284
06/24/2022 18:16:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.316544 on epoch=289
06/24/2022 18:16:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.312939 on epoch=294
06/24/2022 18:16:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.336254 on epoch=299
06/24/2022 18:16:31 - INFO - __main__ - Global step 600 Train loss 0.322075 ACC 0.53125 on epoch=299
06/24/2022 18:16:31 - INFO - __main__ - save last model!
06/24/2022 18:16:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:16:32 - INFO - __main__ - Printing 3 examples
06/24/2022 18:16:32 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 18:16:32 - INFO - __main__ - ['Similar']
06/24/2022 18:16:32 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 18:16:32 - INFO - __main__ - ['Similar']
06/24/2022 18:16:32 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 18:16:32 - INFO - __main__ - ['Similar']
06/24/2022 18:16:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:16:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:16:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:16:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:16:32 - INFO - __main__ - Printing 3 examples
06/24/2022 18:16:32 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 18:16:32 - INFO - __main__ - ['Similar']
06/24/2022 18:16:32 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 18:16:32 - INFO - __main__ - ['Similar']
06/24/2022 18:16:32 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 18:16:32 - INFO - __main__ - ['Similar']
06/24/2022 18:16:32 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:16:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:16:32 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:16:34 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:16:34 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:16:34 - INFO - __main__ - Printing 3 examples
06/24/2022 18:16:34 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:16:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:16:34 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:16:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:16:34 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:16:34 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:16:34 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:16:34 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:16:35 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:16:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:16:36 - INFO - __main__ - Starting training!
06/24/2022 18:16:40 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_13_0.0003_8_predictions.txt
06/24/2022 18:16:40 - INFO - __main__ - ACC on test data: 0.5410
06/24/2022 18:16:40 - INFO - __main__ - prefix=medical_questions_pairs_16_13, lr=0.0003, bsz=8, dev_performance=0.59375, test_performance=0.5409836065573771
06/24/2022 18:16:40 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_13, lr=0.0002, bsz=8 ...
06/24/2022 18:16:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:16:41 - INFO - __main__ - Printing 3 examples
06/24/2022 18:16:41 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 18:16:41 - INFO - __main__ - ['Similar']
06/24/2022 18:16:41 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 18:16:41 - INFO - __main__ - ['Similar']
06/24/2022 18:16:41 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 18:16:41 - INFO - __main__ - ['Similar']
06/24/2022 18:16:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:16:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:16:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:16:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:16:41 - INFO - __main__ - Printing 3 examples
06/24/2022 18:16:41 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 18:16:41 - INFO - __main__ - ['Similar']
06/24/2022 18:16:41 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 18:16:41 - INFO - __main__ - ['Similar']
06/24/2022 18:16:41 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 18:16:41 - INFO - __main__ - ['Similar']
06/24/2022 18:16:41 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:16:41 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:16:41 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:16:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:16:45 - INFO - __main__ - Starting training!
06/24/2022 18:16:47 - INFO - __main__ - Step 10 Global step 10 Train loss 19.682741 on epoch=4
06/24/2022 18:16:50 - INFO - __main__ - Step 20 Global step 20 Train loss 13.492709 on epoch=9
06/24/2022 18:16:52 - INFO - __main__ - Step 30 Global step 30 Train loss 9.288360 on epoch=14
06/24/2022 18:16:55 - INFO - __main__ - Step 40 Global step 40 Train loss 7.208647 on epoch=19
06/24/2022 18:16:57 - INFO - __main__ - Step 50 Global step 50 Train loss 4.941666 on epoch=24
06/24/2022 18:16:58 - INFO - __main__ - Global step 50 Train loss 10.922825 ACC 0.0 on epoch=24
06/24/2022 18:17:01 - INFO - __main__ - Step 60 Global step 60 Train loss 5.410664 on epoch=29
06/24/2022 18:17:03 - INFO - __main__ - Step 70 Global step 70 Train loss 3.762475 on epoch=34
06/24/2022 18:17:06 - INFO - __main__ - Step 80 Global step 80 Train loss 2.627687 on epoch=39
06/24/2022 18:17:09 - INFO - __main__ - Step 90 Global step 90 Train loss 2.269985 on epoch=44
06/24/2022 18:17:11 - INFO - __main__ - Step 100 Global step 100 Train loss 1.578277 on epoch=49
06/24/2022 18:17:11 - INFO - __main__ - Global step 100 Train loss 3.129817 ACC 0.53125 on epoch=49
06/24/2022 18:17:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.932884 on epoch=54
06/24/2022 18:17:17 - INFO - __main__ - Step 120 Global step 120 Train loss 1.105725 on epoch=59
06/24/2022 18:17:19 - INFO - __main__ - Step 130 Global step 130 Train loss 1.224160 on epoch=64
06/24/2022 18:17:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.813686 on epoch=69
06/24/2022 18:17:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.771600 on epoch=74
06/24/2022 18:17:25 - INFO - __main__ - Global step 150 Train loss 0.969611 ACC 0.5 on epoch=74
06/24/2022 18:17:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.684528 on epoch=79
06/24/2022 18:17:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.781051 on epoch=84
06/24/2022 18:17:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.532825 on epoch=89
06/24/2022 18:17:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.507623 on epoch=94
06/24/2022 18:17:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.585097 on epoch=99
06/24/2022 18:17:38 - INFO - __main__ - Global step 200 Train loss 0.618225 ACC 0.53125 on epoch=99
06/24/2022 18:17:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.492887 on epoch=104
06/24/2022 18:17:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.596474 on epoch=109
06/24/2022 18:17:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.564930 on epoch=114
06/24/2022 18:17:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.524511 on epoch=119
06/24/2022 18:17:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.569750 on epoch=124
06/24/2022 18:17:50 - INFO - __main__ - Global step 250 Train loss 0.549710 ACC 0.53125 on epoch=124
06/24/2022 18:17:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.389267 on epoch=129
06/24/2022 18:17:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.340658 on epoch=134
06/24/2022 18:17:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.617800 on epoch=139
06/24/2022 18:18:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.678938 on epoch=144
06/24/2022 18:18:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.405357 on epoch=149
06/24/2022 18:18:03 - INFO - __main__ - Global step 300 Train loss 0.486404 ACC 0.4375 on epoch=149
06/24/2022 18:18:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.599077 on epoch=154
06/24/2022 18:18:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.405718 on epoch=159
06/24/2022 18:18:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.335553 on epoch=164
06/24/2022 18:18:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.513146 on epoch=169
06/24/2022 18:18:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.383430 on epoch=174
06/24/2022 18:18:16 - INFO - __main__ - Global step 350 Train loss 0.447384 ACC 0.59375 on epoch=174
06/24/2022 18:18:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.549532 on epoch=179
06/24/2022 18:18:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.456885 on epoch=184
06/24/2022 18:18:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.420833 on epoch=189
06/24/2022 18:18:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.448006 on epoch=194
06/24/2022 18:18:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.639728 on epoch=199
06/24/2022 18:18:29 - INFO - __main__ - Global step 400 Train loss 0.502997 ACC 0.5 on epoch=199
06/24/2022 18:18:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.311383 on epoch=204
06/24/2022 18:18:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.383047 on epoch=209
06/24/2022 18:18:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.403102 on epoch=214
06/24/2022 18:18:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.336082 on epoch=219
06/24/2022 18:18:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.417867 on epoch=224
06/24/2022 18:18:42 - INFO - __main__ - Global step 450 Train loss 0.370296 ACC 0.4375 on epoch=224
06/24/2022 18:18:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.372648 on epoch=229
06/24/2022 18:18:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.468271 on epoch=234
06/24/2022 18:18:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.451384 on epoch=239
06/24/2022 18:18:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.420683 on epoch=244
06/24/2022 18:18:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.351951 on epoch=249
06/24/2022 18:18:55 - INFO - __main__ - Global step 500 Train loss 0.412987 ACC 0.59375 on epoch=249
06/24/2022 18:18:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.291916 on epoch=254
06/24/2022 18:19:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.479914 on epoch=259
06/24/2022 18:19:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.328509 on epoch=264
06/24/2022 18:19:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.318320 on epoch=269
06/24/2022 18:19:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.327685 on epoch=274
06/24/2022 18:19:07 - INFO - __main__ - Global step 550 Train loss 0.349269 ACC 0.59375 on epoch=274
06/24/2022 18:19:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.358176 on epoch=279
06/24/2022 18:19:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.370271 on epoch=284
06/24/2022 18:19:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.323581 on epoch=289
06/24/2022 18:19:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.352941 on epoch=294
06/24/2022 18:19:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.327108 on epoch=299
06/24/2022 18:19:20 - INFO - __main__ - Global step 600 Train loss 0.346415 ACC 0.5625 on epoch=299
06/24/2022 18:19:20 - INFO - __main__ - save last model!
06/24/2022 18:19:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:19:21 - INFO - __main__ - Printing 3 examples
06/24/2022 18:19:21 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 18:19:21 - INFO - __main__ - ['Similar']
06/24/2022 18:19:21 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 18:19:21 - INFO - __main__ - ['Similar']
06/24/2022 18:19:21 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 18:19:21 - INFO - __main__ - ['Similar']
06/24/2022 18:19:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:19:21 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:19:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:19:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:19:21 - INFO - __main__ - Printing 3 examples
06/24/2022 18:19:21 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 18:19:21 - INFO - __main__ - ['Similar']
06/24/2022 18:19:21 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 18:19:21 - INFO - __main__ - ['Similar']
06/24/2022 18:19:21 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 18:19:21 - INFO - __main__ - ['Similar']
06/24/2022 18:19:21 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:19:21 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:19:21 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:19:23 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:19:23 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:19:23 - INFO - __main__ - Printing 3 examples
06/24/2022 18:19:23 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:19:23 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:19:23 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:19:23 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:19:23 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:19:23 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:19:23 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:19:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:19:24 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:19:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:19:25 - INFO - __main__ - Starting training!
06/24/2022 18:19:29 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_13_0.0002_8_predictions.txt
06/24/2022 18:19:29 - INFO - __main__ - ACC on test data: 0.4934
06/24/2022 18:19:29 - INFO - __main__ - prefix=medical_questions_pairs_16_13, lr=0.0002, bsz=8, dev_performance=0.59375, test_performance=0.4934426229508197
06/24/2022 18:19:29 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_13, lr=0.0001, bsz=8 ...
06/24/2022 18:19:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:19:30 - INFO - __main__ - Printing 3 examples
06/24/2022 18:19:30 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/24/2022 18:19:30 - INFO - __main__ - ['Similar']
06/24/2022 18:19:30 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/24/2022 18:19:30 - INFO - __main__ - ['Similar']
06/24/2022 18:19:30 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/24/2022 18:19:30 - INFO - __main__ - ['Similar']
06/24/2022 18:19:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:19:30 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:19:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:19:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:19:30 - INFO - __main__ - Printing 3 examples
06/24/2022 18:19:30 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/24/2022 18:19:30 - INFO - __main__ - ['Similar']
06/24/2022 18:19:30 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/24/2022 18:19:30 - INFO - __main__ - ['Similar']
06/24/2022 18:19:30 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/24/2022 18:19:30 - INFO - __main__ - ['Similar']
06/24/2022 18:19:30 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:19:30 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:19:30 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:19:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:19:34 - INFO - __main__ - Starting training!
06/24/2022 18:19:36 - INFO - __main__ - Step 10 Global step 10 Train loss 18.278038 on epoch=4
06/24/2022 18:19:38 - INFO - __main__ - Step 20 Global step 20 Train loss 14.903661 on epoch=9
06/24/2022 18:19:41 - INFO - __main__ - Step 30 Global step 30 Train loss 10.964771 on epoch=14
06/24/2022 18:19:43 - INFO - __main__ - Step 40 Global step 40 Train loss 9.879000 on epoch=19
06/24/2022 18:19:45 - INFO - __main__ - Step 50 Global step 50 Train loss 9.608874 on epoch=24
06/24/2022 18:19:47 - INFO - __main__ - Global step 50 Train loss 12.726869 ACC 0.0 on epoch=24
06/24/2022 18:19:50 - INFO - __main__ - Step 60 Global step 60 Train loss 8.619262 on epoch=29
06/24/2022 18:19:53 - INFO - __main__ - Step 70 Global step 70 Train loss 7.144987 on epoch=34
06/24/2022 18:19:55 - INFO - __main__ - Step 80 Global step 80 Train loss 5.844779 on epoch=39
06/24/2022 18:19:57 - INFO - __main__ - Step 90 Global step 90 Train loss 5.319704 on epoch=44
06/24/2022 18:20:00 - INFO - __main__ - Step 100 Global step 100 Train loss 5.779228 on epoch=49
06/24/2022 18:20:01 - INFO - __main__ - Global step 100 Train loss 6.541592 ACC 0.0 on epoch=49
06/24/2022 18:20:04 - INFO - __main__ - Step 110 Global step 110 Train loss 4.242386 on epoch=54
06/24/2022 18:20:06 - INFO - __main__ - Step 120 Global step 120 Train loss 3.699949 on epoch=59
06/24/2022 18:20:09 - INFO - __main__ - Step 130 Global step 130 Train loss 3.745583 on epoch=64
06/24/2022 18:20:11 - INFO - __main__ - Step 140 Global step 140 Train loss 2.707087 on epoch=69
06/24/2022 18:20:14 - INFO - __main__ - Step 150 Global step 150 Train loss 2.430984 on epoch=74
06/24/2022 18:20:14 - INFO - __main__ - Global step 150 Train loss 3.365198 ACC 0.5 on epoch=74
06/24/2022 18:20:17 - INFO - __main__ - Step 160 Global step 160 Train loss 2.539950 on epoch=79
06/24/2022 18:20:19 - INFO - __main__ - Step 170 Global step 170 Train loss 1.526165 on epoch=84
06/24/2022 18:20:22 - INFO - __main__ - Step 180 Global step 180 Train loss 1.988950 on epoch=89
06/24/2022 18:20:24 - INFO - __main__ - Step 190 Global step 190 Train loss 1.933303 on epoch=94
06/24/2022 18:20:27 - INFO - __main__ - Step 200 Global step 200 Train loss 1.547338 on epoch=99
06/24/2022 18:20:27 - INFO - __main__ - Global step 200 Train loss 1.907141 ACC 0.5 on epoch=99
06/24/2022 18:20:30 - INFO - __main__ - Step 210 Global step 210 Train loss 1.358816 on epoch=104
06/24/2022 18:20:32 - INFO - __main__ - Step 220 Global step 220 Train loss 1.100975 on epoch=109
06/24/2022 18:20:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.860793 on epoch=114
06/24/2022 18:20:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.936492 on epoch=119
06/24/2022 18:20:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.738376 on epoch=124
06/24/2022 18:20:40 - INFO - __main__ - Global step 250 Train loss 0.999090 ACC 0.5 on epoch=124
06/24/2022 18:20:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.565332 on epoch=129
06/24/2022 18:20:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.735053 on epoch=134
06/24/2022 18:20:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.758218 on epoch=139
06/24/2022 18:20:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.753237 on epoch=144
06/24/2022 18:20:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.664735 on epoch=149
06/24/2022 18:20:53 - INFO - __main__ - Global step 300 Train loss 0.695315 ACC 0.5 on epoch=149
06/24/2022 18:20:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.680291 on epoch=154
06/24/2022 18:20:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.608618 on epoch=159
06/24/2022 18:21:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.455861 on epoch=164
06/24/2022 18:21:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.652208 on epoch=169
06/24/2022 18:21:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.602944 on epoch=174
06/24/2022 18:21:05 - INFO - __main__ - Global step 350 Train loss 0.599984 ACC 0.5 on epoch=174
06/24/2022 18:21:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.662256 on epoch=179
06/24/2022 18:21:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.701811 on epoch=184
06/24/2022 18:21:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.601441 on epoch=189
06/24/2022 18:21:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.577416 on epoch=194
06/24/2022 18:21:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.443165 on epoch=199
06/24/2022 18:21:18 - INFO - __main__ - Global step 400 Train loss 0.597218 ACC 0.5 on epoch=199
06/24/2022 18:21:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.876150 on epoch=204
06/24/2022 18:21:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.553254 on epoch=209
06/24/2022 18:21:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.590446 on epoch=214
06/24/2022 18:21:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.615062 on epoch=219
06/24/2022 18:21:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.472269 on epoch=224
06/24/2022 18:21:31 - INFO - __main__ - Global step 450 Train loss 0.621437 ACC 0.5 on epoch=224
06/24/2022 18:21:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.544786 on epoch=229
06/24/2022 18:21:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.613339 on epoch=234
06/24/2022 18:21:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.444531 on epoch=239
06/24/2022 18:21:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.608501 on epoch=244
06/24/2022 18:21:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.542703 on epoch=249
06/24/2022 18:21:44 - INFO - __main__ - Global step 500 Train loss 0.550772 ACC 0.53125 on epoch=249
06/24/2022 18:21:47 - INFO - __main__ - Step 510 Global step 510 Train loss 1.064286 on epoch=254
06/24/2022 18:21:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.514524 on epoch=259
06/24/2022 18:21:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.786698 on epoch=264
06/24/2022 18:21:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.584300 on epoch=269
06/24/2022 18:21:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.504918 on epoch=274
06/24/2022 18:21:57 - INFO - __main__ - Global step 550 Train loss 0.690945 ACC 0.5 on epoch=274
06/24/2022 18:21:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.376171 on epoch=279
06/24/2022 18:22:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.509008 on epoch=284
06/24/2022 18:22:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.520775 on epoch=289
06/24/2022 18:22:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.548779 on epoch=294
06/24/2022 18:22:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.588860 on epoch=299
06/24/2022 18:22:10 - INFO - __main__ - Global step 600 Train loss 0.508719 ACC 0.5 on epoch=299
06/24/2022 18:22:10 - INFO - __main__ - save last model!
06/24/2022 18:22:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:22:11 - INFO - __main__ - Printing 3 examples
06/24/2022 18:22:11 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 18:22:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:11 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 18:22:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:11 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 18:22:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:22:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:22:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:22:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:22:11 - INFO - __main__ - Printing 3 examples
06/24/2022 18:22:11 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 18:22:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:11 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 18:22:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:11 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 18:22:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:11 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:22:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:22:11 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:22:12 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:22:13 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:22:13 - INFO - __main__ - Printing 3 examples
06/24/2022 18:22:13 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:22:13 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:13 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:22:13 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:13 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:22:13 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:13 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:22:13 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:22:14 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:22:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:22:15 - INFO - __main__ - Starting training!
06/24/2022 18:22:18 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_13_0.0001_8_predictions.txt
06/24/2022 18:22:18 - INFO - __main__ - ACC on test data: 0.5393
06/24/2022 18:22:19 - INFO - __main__ - prefix=medical_questions_pairs_16_13, lr=0.0001, bsz=8, dev_performance=0.53125, test_performance=0.5393442622950819
06/24/2022 18:22:19 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_21, lr=0.0005, bsz=8 ...
06/24/2022 18:22:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:22:19 - INFO - __main__ - Printing 3 examples
06/24/2022 18:22:19 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 18:22:19 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:19 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 18:22:19 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:19 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 18:22:19 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:22:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:22:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:22:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:22:20 - INFO - __main__ - Printing 3 examples
06/24/2022 18:22:20 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 18:22:20 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:20 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 18:22:20 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:20 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 18:22:20 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:22:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:22:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:22:20 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:22:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:22:23 - INFO - __main__ - Starting training!
06/24/2022 18:22:25 - INFO - __main__ - Step 10 Global step 10 Train loss 18.263361 on epoch=4
06/24/2022 18:22:28 - INFO - __main__ - Step 20 Global step 20 Train loss 14.034497 on epoch=9
06/24/2022 18:22:30 - INFO - __main__ - Step 30 Global step 30 Train loss 6.479559 on epoch=14
06/24/2022 18:22:33 - INFO - __main__ - Step 40 Global step 40 Train loss 4.363780 on epoch=19
06/24/2022 18:22:35 - INFO - __main__ - Step 50 Global step 50 Train loss 2.342908 on epoch=24
06/24/2022 18:22:35 - INFO - __main__ - Global step 50 Train loss 9.096821 ACC 0.5 on epoch=24
06/24/2022 18:22:38 - INFO - __main__ - Step 60 Global step 60 Train loss 1.596575 on epoch=29
06/24/2022 18:22:40 - INFO - __main__ - Step 70 Global step 70 Train loss 1.171517 on epoch=34
06/24/2022 18:22:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.752897 on epoch=39
06/24/2022 18:22:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.845210 on epoch=44
06/24/2022 18:22:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.488856 on epoch=49
06/24/2022 18:22:48 - INFO - __main__ - Global step 100 Train loss 0.971011 ACC 0.5 on epoch=49
06/24/2022 18:22:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.563414 on epoch=54
06/24/2022 18:22:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.514864 on epoch=59
06/24/2022 18:22:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.421917 on epoch=64
06/24/2022 18:22:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.525454 on epoch=69
06/24/2022 18:23:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.468731 on epoch=74
06/24/2022 18:23:01 - INFO - __main__ - Global step 150 Train loss 0.498876 ACC 0.5 on epoch=74
06/24/2022 18:23:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.503219 on epoch=79
06/24/2022 18:23:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.637239 on epoch=84
06/24/2022 18:23:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.713084 on epoch=89
06/24/2022 18:23:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.474673 on epoch=94
06/24/2022 18:23:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.540629 on epoch=99
06/24/2022 18:23:13 - INFO - __main__ - Global step 200 Train loss 0.573769 ACC 0.5 on epoch=99
06/24/2022 18:23:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.556621 on epoch=104
06/24/2022 18:23:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.430000 on epoch=109
06/24/2022 18:23:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.398162 on epoch=114
06/24/2022 18:23:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.427118 on epoch=119
06/24/2022 18:23:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.348531 on epoch=124
06/24/2022 18:23:26 - INFO - __main__ - Global step 250 Train loss 0.432086 ACC 0.5 on epoch=124
06/24/2022 18:23:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.359250 on epoch=129
06/24/2022 18:23:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.534651 on epoch=134
06/24/2022 18:23:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.377973 on epoch=139
06/24/2022 18:23:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.323877 on epoch=144
06/24/2022 18:23:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.445358 on epoch=149
06/24/2022 18:23:38 - INFO - __main__ - Global step 300 Train loss 0.408222 ACC 0.5 on epoch=149
06/24/2022 18:23:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.381192 on epoch=154
06/24/2022 18:23:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.389894 on epoch=159
06/24/2022 18:23:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.395334 on epoch=164
06/24/2022 18:23:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.342290 on epoch=169
06/24/2022 18:23:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.391013 on epoch=174
06/24/2022 18:23:50 - INFO - __main__ - Global step 350 Train loss 0.379945 ACC 0.4375 on epoch=174
06/24/2022 18:23:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.314263 on epoch=179
06/24/2022 18:23:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.397298 on epoch=184
06/24/2022 18:23:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.334125 on epoch=189
06/24/2022 18:24:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.364567 on epoch=194
06/24/2022 18:24:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.331093 on epoch=199
06/24/2022 18:24:03 - INFO - __main__ - Global step 400 Train loss 0.348269 ACC 0.5 on epoch=199
06/24/2022 18:24:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.314772 on epoch=204
06/24/2022 18:24:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.353407 on epoch=209
06/24/2022 18:24:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.324213 on epoch=214
06/24/2022 18:24:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.332473 on epoch=219
06/24/2022 18:24:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.305794 on epoch=224
06/24/2022 18:24:15 - INFO - __main__ - Global step 450 Train loss 0.326132 ACC 0.5 on epoch=224
06/24/2022 18:24:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.377612 on epoch=229
06/24/2022 18:24:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.333241 on epoch=234
06/24/2022 18:24:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.300635 on epoch=239
06/24/2022 18:24:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.331114 on epoch=244
06/24/2022 18:24:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.374797 on epoch=249
06/24/2022 18:24:28 - INFO - __main__ - Global step 500 Train loss 0.343480 ACC 0.5 on epoch=249
06/24/2022 18:24:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.293539 on epoch=254
06/24/2022 18:24:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.365738 on epoch=259
06/24/2022 18:24:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.332678 on epoch=264
06/24/2022 18:24:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.298177 on epoch=269
06/24/2022 18:24:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.308348 on epoch=274
06/24/2022 18:24:40 - INFO - __main__ - Global step 550 Train loss 0.319696 ACC 0.5 on epoch=274
06/24/2022 18:24:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.306472 on epoch=279
06/24/2022 18:24:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.311817 on epoch=284
06/24/2022 18:24:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.298542 on epoch=289
06/24/2022 18:24:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.351022 on epoch=294
06/24/2022 18:24:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.313188 on epoch=299
06/24/2022 18:24:52 - INFO - __main__ - Global step 600 Train loss 0.316208 ACC 0.5 on epoch=299
06/24/2022 18:24:52 - INFO - __main__ - save last model!
06/24/2022 18:24:53 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:24:53 - INFO - __main__ - Printing 3 examples
06/24/2022 18:24:53 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 18:24:53 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:24:53 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 18:24:53 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:24:53 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 18:24:53 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:24:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:24:53 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:24:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:24:53 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:24:53 - INFO - __main__ - Printing 3 examples
06/24/2022 18:24:53 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 18:24:53 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:24:53 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 18:24:53 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:24:53 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 18:24:53 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:24:53 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:24:53 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:24:53 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:24:55 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:24:55 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:24:55 - INFO - __main__ - Printing 3 examples
06/24/2022 18:24:55 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:24:55 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:24:55 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:24:55 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:24:55 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:24:55 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:24:55 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:24:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:24:56 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:24:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:24:57 - INFO - __main__ - Starting training!
06/24/2022 18:25:00 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_21_0.0005_8_predictions.txt
06/24/2022 18:25:00 - INFO - __main__ - ACC on test data: 0.4836
06/24/2022 18:25:00 - INFO - __main__ - prefix=medical_questions_pairs_16_21, lr=0.0005, bsz=8, dev_performance=0.5, test_performance=0.48360655737704916
06/24/2022 18:25:00 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_21, lr=0.0003, bsz=8 ...
06/24/2022 18:25:01 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:25:01 - INFO - __main__ - Printing 3 examples
06/24/2022 18:25:01 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 18:25:01 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:25:01 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 18:25:01 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:25:01 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 18:25:01 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:25:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:25:01 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:25:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:25:01 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:25:01 - INFO - __main__ - Printing 3 examples
06/24/2022 18:25:01 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 18:25:01 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:25:01 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 18:25:01 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:25:01 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 18:25:01 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:25:01 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:25:01 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:25:01 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:25:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:25:05 - INFO - __main__ - Starting training!
06/24/2022 18:25:07 - INFO - __main__ - Step 10 Global step 10 Train loss 18.263643 on epoch=4
06/24/2022 18:25:09 - INFO - __main__ - Step 20 Global step 20 Train loss 14.037119 on epoch=9
06/24/2022 18:25:12 - INFO - __main__ - Step 30 Global step 30 Train loss 8.006036 on epoch=14
06/24/2022 18:25:14 - INFO - __main__ - Step 40 Global step 40 Train loss 4.946561 on epoch=19
06/24/2022 18:25:17 - INFO - __main__ - Step 50 Global step 50 Train loss 3.834538 on epoch=24
06/24/2022 18:25:17 - INFO - __main__ - Global step 50 Train loss 9.817579 ACC 0.5 on epoch=24
06/24/2022 18:25:20 - INFO - __main__ - Step 60 Global step 60 Train loss 3.518132 on epoch=29
06/24/2022 18:25:22 - INFO - __main__ - Step 70 Global step 70 Train loss 2.115766 on epoch=34
06/24/2022 18:25:25 - INFO - __main__ - Step 80 Global step 80 Train loss 1.737482 on epoch=39
06/24/2022 18:25:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.932565 on epoch=44
06/24/2022 18:25:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.713135 on epoch=49
06/24/2022 18:25:30 - INFO - __main__ - Global step 100 Train loss 1.803416 ACC 0.5 on epoch=49
06/24/2022 18:25:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.820765 on epoch=54
06/24/2022 18:25:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.971918 on epoch=59
06/24/2022 18:25:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.557979 on epoch=64
06/24/2022 18:25:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.696874 on epoch=69
06/24/2022 18:25:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.503069 on epoch=74
06/24/2022 18:25:43 - INFO - __main__ - Global step 150 Train loss 0.710121 ACC 0.5 on epoch=74
06/24/2022 18:25:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.402096 on epoch=79
06/24/2022 18:25:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.727361 on epoch=84
06/24/2022 18:25:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.557570 on epoch=89
06/24/2022 18:25:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.455483 on epoch=94
06/24/2022 18:25:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.582960 on epoch=99
06/24/2022 18:25:56 - INFO - __main__ - Global step 200 Train loss 0.545094 ACC 0.625 on epoch=99
06/24/2022 18:25:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.486173 on epoch=104
06/24/2022 18:26:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.546660 on epoch=109
06/24/2022 18:26:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.514707 on epoch=114
06/24/2022 18:26:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.384908 on epoch=119
06/24/2022 18:26:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.368492 on epoch=124
06/24/2022 18:26:09 - INFO - __main__ - Global step 250 Train loss 0.460188 ACC 0.5625 on epoch=124
06/24/2022 18:26:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.364124 on epoch=129
06/24/2022 18:26:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.314649 on epoch=134
06/24/2022 18:26:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.260090 on epoch=139
06/24/2022 18:26:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.351116 on epoch=144
06/24/2022 18:26:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.267535 on epoch=149
06/24/2022 18:26:22 - INFO - __main__ - Global step 300 Train loss 0.311503 ACC 0.5 on epoch=149
06/24/2022 18:26:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.200758 on epoch=154
06/24/2022 18:26:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.226322 on epoch=159
06/24/2022 18:26:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.195642 on epoch=164
06/24/2022 18:26:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.158240 on epoch=169
06/24/2022 18:26:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.186647 on epoch=174
06/24/2022 18:26:34 - INFO - __main__ - Global step 350 Train loss 0.193522 ACC 0.5625 on epoch=174
06/24/2022 18:26:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.236325 on epoch=179
06/24/2022 18:26:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.243482 on epoch=184
06/24/2022 18:26:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.153677 on epoch=189
06/24/2022 18:26:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.137101 on epoch=194
06/24/2022 18:26:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.169841 on epoch=199
06/24/2022 18:26:47 - INFO - __main__ - Global step 400 Train loss 0.188085 ACC 0.46875 on epoch=199
06/24/2022 18:26:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.143465 on epoch=204
06/24/2022 18:26:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.157666 on epoch=209
06/24/2022 18:26:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.070675 on epoch=214
06/24/2022 18:26:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.110813 on epoch=219
06/24/2022 18:27:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.102770 on epoch=224
06/24/2022 18:27:00 - INFO - __main__ - Global step 450 Train loss 0.117077 ACC 0.5625 on epoch=224
06/24/2022 18:27:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.031396 on epoch=229
06/24/2022 18:27:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.059014 on epoch=234
06/24/2022 18:27:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.048769 on epoch=239
06/24/2022 18:27:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.049157 on epoch=244
06/24/2022 18:27:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.024348 on epoch=249
06/24/2022 18:27:14 - INFO - __main__ - Global step 500 Train loss 0.042537 ACC 0.5625 on epoch=249
06/24/2022 18:27:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.053218 on epoch=254
06/24/2022 18:27:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.022232 on epoch=259
06/24/2022 18:27:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.044093 on epoch=264
06/24/2022 18:27:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.027691 on epoch=269
06/24/2022 18:27:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.044003 on epoch=274
06/24/2022 18:27:26 - INFO - __main__ - Global step 550 Train loss 0.038247 ACC 0.5625 on epoch=274
06/24/2022 18:27:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.035635 on epoch=279
06/24/2022 18:27:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.027963 on epoch=284
06/24/2022 18:27:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.018374 on epoch=289
06/24/2022 18:27:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.018945 on epoch=294
06/24/2022 18:27:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.041791 on epoch=299
06/24/2022 18:27:39 - INFO - __main__ - Global step 600 Train loss 0.028542 ACC 0.59375 on epoch=299
06/24/2022 18:27:39 - INFO - __main__ - save last model!
06/24/2022 18:27:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:27:40 - INFO - __main__ - Printing 3 examples
06/24/2022 18:27:40 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 18:27:40 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:40 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 18:27:40 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:40 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 18:27:40 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:27:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:27:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:27:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:27:40 - INFO - __main__ - Printing 3 examples
06/24/2022 18:27:40 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 18:27:40 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:40 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 18:27:40 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:40 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 18:27:40 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:27:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:27:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:27:42 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:27:42 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:27:42 - INFO - __main__ - Printing 3 examples
06/24/2022 18:27:42 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:27:42 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:42 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:27:42 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:42 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:27:42 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:42 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:27:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:27:43 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:27:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:27:44 - INFO - __main__ - Starting training!
06/24/2022 18:27:48 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_21_0.0003_8_predictions.txt
06/24/2022 18:27:48 - INFO - __main__ - ACC on test data: 0.5820
06/24/2022 18:27:48 - INFO - __main__ - prefix=medical_questions_pairs_16_21, lr=0.0003, bsz=8, dev_performance=0.625, test_performance=0.5819672131147541
06/24/2022 18:27:48 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_21, lr=0.0002, bsz=8 ...
06/24/2022 18:27:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:27:49 - INFO - __main__ - Printing 3 examples
06/24/2022 18:27:49 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 18:27:49 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:49 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 18:27:49 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:49 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 18:27:49 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:27:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:27:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:27:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:27:49 - INFO - __main__ - Printing 3 examples
06/24/2022 18:27:49 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 18:27:49 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:49 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 18:27:49 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:49 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 18:27:49 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:27:49 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:27:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:27:49 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:27:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:27:53 - INFO - __main__ - Starting training!
06/24/2022 18:27:55 - INFO - __main__ - Step 10 Global step 10 Train loss 18.562302 on epoch=4
06/24/2022 18:27:57 - INFO - __main__ - Step 20 Global step 20 Train loss 15.307414 on epoch=9
06/24/2022 18:28:00 - INFO - __main__ - Step 30 Global step 30 Train loss 8.822958 on epoch=14
06/24/2022 18:28:02 - INFO - __main__ - Step 40 Global step 40 Train loss 6.952077 on epoch=19
06/24/2022 18:28:05 - INFO - __main__ - Step 50 Global step 50 Train loss 5.845090 on epoch=24
06/24/2022 18:28:07 - INFO - __main__ - Global step 50 Train loss 11.097969 ACC 0.0 on epoch=24
06/24/2022 18:28:10 - INFO - __main__ - Step 60 Global step 60 Train loss 5.000064 on epoch=29
06/24/2022 18:28:12 - INFO - __main__ - Step 70 Global step 70 Train loss 3.558408 on epoch=34
06/24/2022 18:28:15 - INFO - __main__ - Step 80 Global step 80 Train loss 3.453797 on epoch=39
06/24/2022 18:28:17 - INFO - __main__ - Step 90 Global step 90 Train loss 1.755476 on epoch=44
06/24/2022 18:28:20 - INFO - __main__ - Step 100 Global step 100 Train loss 1.932778 on epoch=49
06/24/2022 18:28:20 - INFO - __main__ - Global step 100 Train loss 3.140105 ACC 0.53125 on epoch=49
06/24/2022 18:28:23 - INFO - __main__ - Step 110 Global step 110 Train loss 1.734237 on epoch=54
06/24/2022 18:28:26 - INFO - __main__ - Step 120 Global step 120 Train loss 1.735260 on epoch=59
06/24/2022 18:28:28 - INFO - __main__ - Step 130 Global step 130 Train loss 1.665289 on epoch=64
06/24/2022 18:28:31 - INFO - __main__ - Step 140 Global step 140 Train loss 1.019130 on epoch=69
06/24/2022 18:28:33 - INFO - __main__ - Step 150 Global step 150 Train loss 1.048283 on epoch=74
06/24/2022 18:28:33 - INFO - __main__ - Global step 150 Train loss 1.440440 ACC 0.46875 on epoch=74
06/24/2022 18:28:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.752028 on epoch=79
06/24/2022 18:28:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.888602 on epoch=84
06/24/2022 18:28:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.739160 on epoch=89
06/24/2022 18:28:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.740384 on epoch=94
06/24/2022 18:28:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.505623 on epoch=99
06/24/2022 18:28:46 - INFO - __main__ - Global step 200 Train loss 0.725159 ACC 0.5 on epoch=99
06/24/2022 18:28:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.495626 on epoch=104
06/24/2022 18:28:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.564620 on epoch=109
06/24/2022 18:28:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.610662 on epoch=114
06/24/2022 18:28:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.490737 on epoch=119
06/24/2022 18:28:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.497882 on epoch=124
06/24/2022 18:28:59 - INFO - __main__ - Global step 250 Train loss 0.531905 ACC 0.46875 on epoch=124
06/24/2022 18:29:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.266937 on epoch=129
06/24/2022 18:29:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.470829 on epoch=134
06/24/2022 18:29:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.521633 on epoch=139
06/24/2022 18:29:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.452012 on epoch=144
06/24/2022 18:29:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.412301 on epoch=149
06/24/2022 18:29:11 - INFO - __main__ - Global step 300 Train loss 0.424742 ACC 0.5 on epoch=149
06/24/2022 18:29:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.323077 on epoch=154
06/24/2022 18:29:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.342232 on epoch=159
06/24/2022 18:29:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.378777 on epoch=164
06/24/2022 18:29:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.434085 on epoch=169
06/24/2022 18:29:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.326291 on epoch=174
06/24/2022 18:29:24 - INFO - __main__ - Global step 350 Train loss 0.360893 ACC 0.5625 on epoch=174
06/24/2022 18:29:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.202781 on epoch=179
06/24/2022 18:29:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.331332 on epoch=184
06/24/2022 18:29:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.373250 on epoch=189
06/24/2022 18:29:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.288604 on epoch=194
06/24/2022 18:29:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.256142 on epoch=199
06/24/2022 18:29:37 - INFO - __main__ - Global step 400 Train loss 0.290422 ACC 0.59375 on epoch=199
06/24/2022 18:29:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.223511 on epoch=204
06/24/2022 18:29:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.195553 on epoch=209
06/24/2022 18:29:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.205600 on epoch=214
06/24/2022 18:29:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.157791 on epoch=219
06/24/2022 18:29:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.165927 on epoch=224
06/24/2022 18:29:50 - INFO - __main__ - Global step 450 Train loss 0.189676 ACC 0.5625 on epoch=224
06/24/2022 18:29:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.172393 on epoch=229
06/24/2022 18:29:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.154651 on epoch=234
06/24/2022 18:29:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.161771 on epoch=239
06/24/2022 18:30:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.149664 on epoch=244
06/24/2022 18:30:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.112104 on epoch=249
06/24/2022 18:30:03 - INFO - __main__ - Global step 500 Train loss 0.150117 ACC 0.5625 on epoch=249
06/24/2022 18:30:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.066059 on epoch=254
06/24/2022 18:30:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.246545 on epoch=259
06/24/2022 18:30:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.094135 on epoch=264
06/24/2022 18:30:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.167022 on epoch=269
06/24/2022 18:30:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.428264 on epoch=274
06/24/2022 18:30:16 - INFO - __main__ - Global step 550 Train loss 0.200405 ACC 0.5 on epoch=274
06/24/2022 18:30:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.452737 on epoch=279
06/24/2022 18:30:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.182069 on epoch=284
06/24/2022 18:30:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.181526 on epoch=289
06/24/2022 18:30:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.150584 on epoch=294
06/24/2022 18:30:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.061458 on epoch=299
06/24/2022 18:30:28 - INFO - __main__ - Global step 600 Train loss 0.205675 ACC 0.5625 on epoch=299
06/24/2022 18:30:28 - INFO - __main__ - save last model!
06/24/2022 18:30:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:30:29 - INFO - __main__ - Printing 3 examples
06/24/2022 18:30:29 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 18:30:29 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:29 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 18:30:29 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:29 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 18:30:29 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:30:29 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:30:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:30:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:30:29 - INFO - __main__ - Printing 3 examples
06/24/2022 18:30:29 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 18:30:29 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:29 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 18:30:29 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:29 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 18:30:29 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:29 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:30:29 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:30:29 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:30:31 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:30:31 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:30:31 - INFO - __main__ - Printing 3 examples
06/24/2022 18:30:31 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:30:31 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:31 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:30:31 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:31 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:30:31 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:31 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:30:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:30:32 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:30:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:30:33 - INFO - __main__ - Starting training!
06/24/2022 18:30:37 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_21_0.0002_8_predictions.txt
06/24/2022 18:30:37 - INFO - __main__ - ACC on test data: 0.5443
06/24/2022 18:30:37 - INFO - __main__ - prefix=medical_questions_pairs_16_21, lr=0.0002, bsz=8, dev_performance=0.59375, test_performance=0.5442622950819672
06/24/2022 18:30:37 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_21, lr=0.0001, bsz=8 ...
06/24/2022 18:30:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:30:38 - INFO - __main__ - Printing 3 examples
06/24/2022 18:30:38 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/24/2022 18:30:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:38 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/24/2022 18:30:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:38 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/24/2022 18:30:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:30:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:30:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:30:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:30:38 - INFO - __main__ - Printing 3 examples
06/24/2022 18:30:38 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/24/2022 18:30:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/24/2022 18:30:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:38 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/24/2022 18:30:38 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:30:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:30:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:30:38 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:30:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:30:42 - INFO - __main__ - Starting training!
06/24/2022 18:30:44 - INFO - __main__ - Step 10 Global step 10 Train loss 18.439373 on epoch=4
06/24/2022 18:30:46 - INFO - __main__ - Step 20 Global step 20 Train loss 16.662292 on epoch=9
06/24/2022 18:30:49 - INFO - __main__ - Step 30 Global step 30 Train loss 11.408463 on epoch=14
06/24/2022 18:30:51 - INFO - __main__ - Step 40 Global step 40 Train loss 10.213072 on epoch=19
06/24/2022 18:30:54 - INFO - __main__ - Step 50 Global step 50 Train loss 8.507877 on epoch=24
06/24/2022 18:30:56 - INFO - __main__ - Global step 50 Train loss 13.046217 ACC 0.0 on epoch=24
06/24/2022 18:30:59 - INFO - __main__ - Step 60 Global step 60 Train loss 7.948514 on epoch=29
06/24/2022 18:31:01 - INFO - __main__ - Step 70 Global step 70 Train loss 7.483582 on epoch=34
06/24/2022 18:31:04 - INFO - __main__ - Step 80 Global step 80 Train loss 6.426381 on epoch=39
06/24/2022 18:31:06 - INFO - __main__ - Step 90 Global step 90 Train loss 5.832387 on epoch=44
06/24/2022 18:31:09 - INFO - __main__ - Step 100 Global step 100 Train loss 5.257264 on epoch=49
06/24/2022 18:31:10 - INFO - __main__ - Global step 100 Train loss 6.589625 ACC 0.0 on epoch=49
06/24/2022 18:31:13 - INFO - __main__ - Step 110 Global step 110 Train loss 5.508944 on epoch=54
06/24/2022 18:31:15 - INFO - __main__ - Step 120 Global step 120 Train loss 5.040240 on epoch=59
06/24/2022 18:31:18 - INFO - __main__ - Step 130 Global step 130 Train loss 3.638742 on epoch=64
06/24/2022 18:31:20 - INFO - __main__ - Step 140 Global step 140 Train loss 4.450998 on epoch=69
06/24/2022 18:31:23 - INFO - __main__ - Step 150 Global step 150 Train loss 3.419750 on epoch=74
06/24/2022 18:31:23 - INFO - __main__ - Global step 150 Train loss 4.411735 ACC 0.5 on epoch=74
06/24/2022 18:31:26 - INFO - __main__ - Step 160 Global step 160 Train loss 2.708899 on epoch=79
06/24/2022 18:31:28 - INFO - __main__ - Step 170 Global step 170 Train loss 2.158772 on epoch=84
06/24/2022 18:31:31 - INFO - __main__ - Step 180 Global step 180 Train loss 3.330396 on epoch=89
06/24/2022 18:31:33 - INFO - __main__ - Step 190 Global step 190 Train loss 1.817253 on epoch=94
06/24/2022 18:31:36 - INFO - __main__ - Step 200 Global step 200 Train loss 1.485292 on epoch=99
06/24/2022 18:31:36 - INFO - __main__ - Global step 200 Train loss 2.300122 ACC 0.4375 on epoch=99
06/24/2022 18:31:39 - INFO - __main__ - Step 210 Global step 210 Train loss 1.682502 on epoch=104
06/24/2022 18:31:41 - INFO - __main__ - Step 220 Global step 220 Train loss 1.908651 on epoch=109
06/24/2022 18:31:44 - INFO - __main__ - Step 230 Global step 230 Train loss 1.070876 on epoch=114
06/24/2022 18:31:46 - INFO - __main__ - Step 240 Global step 240 Train loss 1.173329 on epoch=119
06/24/2022 18:31:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.849381 on epoch=124
06/24/2022 18:31:49 - INFO - __main__ - Global step 250 Train loss 1.336948 ACC 0.46875 on epoch=124
06/24/2022 18:31:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.975271 on epoch=129
06/24/2022 18:31:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.646057 on epoch=134
06/24/2022 18:31:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.912192 on epoch=139
06/24/2022 18:31:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.538576 on epoch=144
06/24/2022 18:32:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.486090 on epoch=149
06/24/2022 18:32:02 - INFO - __main__ - Global step 300 Train loss 0.711637 ACC 0.53125 on epoch=149
06/24/2022 18:32:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.701052 on epoch=154
06/24/2022 18:32:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.682732 on epoch=159
06/24/2022 18:32:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.460010 on epoch=164
06/24/2022 18:32:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.680157 on epoch=169
06/24/2022 18:32:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.690994 on epoch=174
06/24/2022 18:32:15 - INFO - __main__ - Global step 350 Train loss 0.642989 ACC 0.5 on epoch=174
06/24/2022 18:32:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.546833 on epoch=179
06/24/2022 18:32:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.683377 on epoch=184
06/24/2022 18:32:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.558180 on epoch=189
06/24/2022 18:32:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.484317 on epoch=194
06/24/2022 18:32:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.549232 on epoch=199
06/24/2022 18:32:27 - INFO - __main__ - Global step 400 Train loss 0.564388 ACC 0.46875 on epoch=199
06/24/2022 18:32:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.405371 on epoch=204
06/24/2022 18:32:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.562581 on epoch=209
06/24/2022 18:32:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.551699 on epoch=214
06/24/2022 18:32:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.609054 on epoch=219
06/24/2022 18:32:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.529339 on epoch=224
06/24/2022 18:32:40 - INFO - __main__ - Global step 450 Train loss 0.531609 ACC 0.5625 on epoch=224
06/24/2022 18:32:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.472601 on epoch=229
06/24/2022 18:32:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.464552 on epoch=234
06/24/2022 18:32:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.416030 on epoch=239
06/24/2022 18:32:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.423940 on epoch=244
06/24/2022 18:32:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.468995 on epoch=249
06/24/2022 18:32:53 - INFO - __main__ - Global step 500 Train loss 0.449224 ACC 0.59375 on epoch=249
06/24/2022 18:32:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.455058 on epoch=254
06/24/2022 18:32:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.407140 on epoch=259
06/24/2022 18:33:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.364872 on epoch=264
06/24/2022 18:33:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.397310 on epoch=269
06/24/2022 18:33:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.475642 on epoch=274
06/24/2022 18:33:07 - INFO - __main__ - Global step 550 Train loss 0.420004 ACC 0.59375 on epoch=274
06/24/2022 18:33:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.375009 on epoch=279
06/24/2022 18:33:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.335675 on epoch=284
06/24/2022 18:33:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.381958 on epoch=289
06/24/2022 18:33:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.325211 on epoch=294
06/24/2022 18:33:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.310335 on epoch=299
06/24/2022 18:33:20 - INFO - __main__ - Global step 600 Train loss 0.345637 ACC 0.59375 on epoch=299
06/24/2022 18:33:20 - INFO - __main__ - save last model!
06/24/2022 18:33:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:33:21 - INFO - __main__ - Printing 3 examples
06/24/2022 18:33:21 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 18:33:21 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:21 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 18:33:21 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:21 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 18:33:21 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:33:21 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:33:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:33:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:33:21 - INFO - __main__ - Printing 3 examples
06/24/2022 18:33:21 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 18:33:21 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:21 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 18:33:21 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:21 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 18:33:21 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:21 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:33:21 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:33:21 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:33:22 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:33:23 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:33:23 - INFO - __main__ - Printing 3 examples
06/24/2022 18:33:23 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:33:23 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:23 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:33:23 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:23 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:33:23 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:23 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:33:23 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:33:24 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:33:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:33:25 - INFO - __main__ - Starting training!
06/24/2022 18:33:28 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_21_0.0001_8_predictions.txt
06/24/2022 18:33:28 - INFO - __main__ - ACC on test data: 0.5475
06/24/2022 18:33:29 - INFO - __main__ - prefix=medical_questions_pairs_16_21, lr=0.0001, bsz=8, dev_performance=0.59375, test_performance=0.5475409836065573
06/24/2022 18:33:29 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_42, lr=0.0005, bsz=8 ...
06/24/2022 18:33:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:33:29 - INFO - __main__ - Printing 3 examples
06/24/2022 18:33:29 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 18:33:29 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:29 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 18:33:29 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:29 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 18:33:29 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:33:29 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:33:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:33:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:33:30 - INFO - __main__ - Printing 3 examples
06/24/2022 18:33:30 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 18:33:30 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:30 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 18:33:30 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:30 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 18:33:30 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:33:30 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:33:30 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:33:30 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:33:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:33:34 - INFO - __main__ - Starting training!
06/24/2022 18:33:36 - INFO - __main__ - Step 10 Global step 10 Train loss 18.961933 on epoch=4
06/24/2022 18:33:38 - INFO - __main__ - Step 20 Global step 20 Train loss 12.575649 on epoch=9
06/24/2022 18:33:41 - INFO - __main__ - Step 30 Global step 30 Train loss 6.084550 on epoch=14
06/24/2022 18:33:43 - INFO - __main__ - Step 40 Global step 40 Train loss 3.695277 on epoch=19
06/24/2022 18:33:46 - INFO - __main__ - Step 50 Global step 50 Train loss 1.685230 on epoch=24
06/24/2022 18:33:46 - INFO - __main__ - Global step 50 Train loss 8.600528 ACC 0.46875 on epoch=24
06/24/2022 18:33:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.869144 on epoch=29
06/24/2022 18:33:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.801650 on epoch=34
06/24/2022 18:33:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.610696 on epoch=39
06/24/2022 18:33:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.517510 on epoch=44
06/24/2022 18:33:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.504199 on epoch=49
06/24/2022 18:33:59 - INFO - __main__ - Global step 100 Train loss 0.660640 ACC 0.53125 on epoch=49
06/24/2022 18:34:02 - INFO - __main__ - Step 110 Global step 110 Train loss 0.523091 on epoch=54
06/24/2022 18:34:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.514627 on epoch=59
06/24/2022 18:34:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.502278 on epoch=64
06/24/2022 18:34:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.424822 on epoch=69
06/24/2022 18:34:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.445343 on epoch=74
06/24/2022 18:34:12 - INFO - __main__ - Global step 150 Train loss 0.482032 ACC 0.46875 on epoch=74
06/24/2022 18:34:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.486935 on epoch=79
06/24/2022 18:34:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.530465 on epoch=84
06/24/2022 18:34:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.558376 on epoch=89
06/24/2022 18:34:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.482993 on epoch=94
06/24/2022 18:34:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.463036 on epoch=99
06/24/2022 18:34:25 - INFO - __main__ - Global step 200 Train loss 0.504361 ACC 0.5 on epoch=99
06/24/2022 18:34:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.396967 on epoch=104
06/24/2022 18:34:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.453045 on epoch=109
06/24/2022 18:34:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.474347 on epoch=114
06/24/2022 18:34:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.444363 on epoch=119
06/24/2022 18:34:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.397991 on epoch=124
06/24/2022 18:34:38 - INFO - __main__ - Global step 250 Train loss 0.433343 ACC 0.5 on epoch=124
06/24/2022 18:34:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.348556 on epoch=129
06/24/2022 18:34:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.341064 on epoch=134
06/24/2022 18:34:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.352123 on epoch=139
06/24/2022 18:34:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.267600 on epoch=144
06/24/2022 18:34:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.373072 on epoch=149
06/24/2022 18:34:51 - INFO - __main__ - Global step 300 Train loss 0.336483 ACC 0.5 on epoch=149
06/24/2022 18:34:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.438447 on epoch=154
06/24/2022 18:34:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.433199 on epoch=159
06/24/2022 18:34:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.366780 on epoch=164
06/24/2022 18:35:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.354309 on epoch=169
06/24/2022 18:35:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.329256 on epoch=174
06/24/2022 18:35:04 - INFO - __main__ - Global step 350 Train loss 0.384398 ACC 0.5 on epoch=174
06/24/2022 18:35:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.326118 on epoch=179
06/24/2022 18:35:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.367766 on epoch=184
06/24/2022 18:35:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.327120 on epoch=189
06/24/2022 18:35:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.322407 on epoch=194
06/24/2022 18:35:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.395563 on epoch=199
06/24/2022 18:35:17 - INFO - __main__ - Global step 400 Train loss 0.347795 ACC 0.5 on epoch=199
06/24/2022 18:35:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.336543 on epoch=204
06/24/2022 18:35:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.346210 on epoch=209
06/24/2022 18:35:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.356292 on epoch=214
06/24/2022 18:35:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.368002 on epoch=219
06/24/2022 18:35:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.361147 on epoch=224
06/24/2022 18:35:30 - INFO - __main__ - Global step 450 Train loss 0.353639 ACC 0.40625 on epoch=224
06/24/2022 18:35:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.326821 on epoch=229
06/24/2022 18:35:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.309720 on epoch=234
06/24/2022 18:35:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.318469 on epoch=239
06/24/2022 18:35:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.372709 on epoch=244
06/24/2022 18:35:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.362169 on epoch=249
06/24/2022 18:35:43 - INFO - __main__ - Global step 500 Train loss 0.337978 ACC 0.40625 on epoch=249
06/24/2022 18:35:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.314677 on epoch=254
06/24/2022 18:35:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.347817 on epoch=259
06/24/2022 18:35:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.352889 on epoch=264
06/24/2022 18:35:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.345101 on epoch=269
06/24/2022 18:35:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.287447 on epoch=274
06/24/2022 18:35:55 - INFO - __main__ - Global step 550 Train loss 0.329586 ACC 0.5 on epoch=274
06/24/2022 18:35:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.315179 on epoch=279
06/24/2022 18:36:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.338615 on epoch=284
06/24/2022 18:36:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.299081 on epoch=289
06/24/2022 18:36:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.349373 on epoch=294
06/24/2022 18:36:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.301313 on epoch=299
06/24/2022 18:36:08 - INFO - __main__ - Global step 600 Train loss 0.320713 ACC 0.4375 on epoch=299
06/24/2022 18:36:08 - INFO - __main__ - save last model!
06/24/2022 18:36:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:36:09 - INFO - __main__ - Printing 3 examples
06/24/2022 18:36:09 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 18:36:09 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:09 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 18:36:09 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:09 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 18:36:09 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:36:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:36:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:36:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:36:09 - INFO - __main__ - Printing 3 examples
06/24/2022 18:36:09 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 18:36:09 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:09 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 18:36:09 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:09 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 18:36:09 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:09 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:36:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:36:09 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:36:11 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:36:11 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:36:11 - INFO - __main__ - Printing 3 examples
06/24/2022 18:36:11 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:36:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:11 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:36:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:11 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:36:11 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:11 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:36:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:36:12 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:36:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:36:13 - INFO - __main__ - Starting training!
06/24/2022 18:36:17 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_42_0.0005_8_predictions.txt
06/24/2022 18:36:17 - INFO - __main__ - ACC on test data: 0.5246
06/24/2022 18:36:17 - INFO - __main__ - prefix=medical_questions_pairs_16_42, lr=0.0005, bsz=8, dev_performance=0.53125, test_performance=0.5245901639344263
06/24/2022 18:36:17 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_42, lr=0.0003, bsz=8 ...
06/24/2022 18:36:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:36:18 - INFO - __main__ - Printing 3 examples
06/24/2022 18:36:18 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 18:36:18 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:18 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 18:36:18 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:18 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 18:36:18 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:36:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:36:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:36:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:36:18 - INFO - __main__ - Printing 3 examples
06/24/2022 18:36:18 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 18:36:18 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:18 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 18:36:18 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:18 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 18:36:18 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:36:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:36:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:36:18 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:36:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:36:22 - INFO - __main__ - Starting training!
06/24/2022 18:36:24 - INFO - __main__ - Step 10 Global step 10 Train loss 19.066418 on epoch=4
06/24/2022 18:36:27 - INFO - __main__ - Step 20 Global step 20 Train loss 13.150807 on epoch=9
06/24/2022 18:36:29 - INFO - __main__ - Step 30 Global step 30 Train loss 8.661226 on epoch=14
06/24/2022 18:36:32 - INFO - __main__ - Step 40 Global step 40 Train loss 6.657041 on epoch=19
06/24/2022 18:36:34 - INFO - __main__ - Step 50 Global step 50 Train loss 5.034283 on epoch=24
06/24/2022 18:36:35 - INFO - __main__ - Global step 50 Train loss 10.513954 ACC 0.03125 on epoch=24
06/24/2022 18:36:38 - INFO - __main__ - Step 60 Global step 60 Train loss 3.616277 on epoch=29
06/24/2022 18:36:41 - INFO - __main__ - Step 70 Global step 70 Train loss 2.737835 on epoch=34
06/24/2022 18:36:43 - INFO - __main__ - Step 80 Global step 80 Train loss 2.284883 on epoch=39
06/24/2022 18:36:46 - INFO - __main__ - Step 90 Global step 90 Train loss 1.535193 on epoch=44
06/24/2022 18:36:48 - INFO - __main__ - Step 100 Global step 100 Train loss 1.252273 on epoch=49
06/24/2022 18:36:48 - INFO - __main__ - Global step 100 Train loss 2.285293 ACC 0.5 on epoch=49
06/24/2022 18:36:51 - INFO - __main__ - Step 110 Global step 110 Train loss 1.178820 on epoch=54
06/24/2022 18:36:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.921846 on epoch=59
06/24/2022 18:36:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.712031 on epoch=64
06/24/2022 18:36:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.617359 on epoch=69
06/24/2022 18:37:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.609783 on epoch=74
06/24/2022 18:37:01 - INFO - __main__ - Global step 150 Train loss 0.807968 ACC 0.46875 on epoch=74
06/24/2022 18:37:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.665826 on epoch=79
06/24/2022 18:37:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.471776 on epoch=84
06/24/2022 18:37:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.662234 on epoch=89
06/24/2022 18:37:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.473611 on epoch=94
06/24/2022 18:37:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.606719 on epoch=99
06/24/2022 18:37:14 - INFO - __main__ - Global step 200 Train loss 0.576033 ACC 0.46875 on epoch=99
06/24/2022 18:37:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.621628 on epoch=104
06/24/2022 18:37:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.500779 on epoch=109
06/24/2022 18:37:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.833501 on epoch=114
06/24/2022 18:37:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.413164 on epoch=119
06/24/2022 18:37:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.465381 on epoch=124
06/24/2022 18:37:26 - INFO - __main__ - Global step 250 Train loss 0.566891 ACC 0.46875 on epoch=124
06/24/2022 18:37:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.559025 on epoch=129
06/24/2022 18:37:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.397349 on epoch=134
06/24/2022 18:37:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.422440 on epoch=139
06/24/2022 18:37:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.448508 on epoch=144
06/24/2022 18:37:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.402397 on epoch=149
06/24/2022 18:37:39 - INFO - __main__ - Global step 300 Train loss 0.445944 ACC 0.5 on epoch=149
06/24/2022 18:37:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.392339 on epoch=154
06/24/2022 18:37:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.342200 on epoch=159
06/24/2022 18:37:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.377033 on epoch=164
06/24/2022 18:37:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.398965 on epoch=169
06/24/2022 18:37:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.315247 on epoch=174
06/24/2022 18:37:52 - INFO - __main__ - Global step 350 Train loss 0.365157 ACC 0.46875 on epoch=174
06/24/2022 18:37:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.464740 on epoch=179
06/24/2022 18:37:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.311458 on epoch=184
06/24/2022 18:37:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.390749 on epoch=189
06/24/2022 18:38:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.371069 on epoch=194
06/24/2022 18:38:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.396120 on epoch=199
06/24/2022 18:38:04 - INFO - __main__ - Global step 400 Train loss 0.386827 ACC 0.5 on epoch=199
06/24/2022 18:38:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.400960 on epoch=204
06/24/2022 18:38:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.343007 on epoch=209
06/24/2022 18:38:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.347875 on epoch=214
06/24/2022 18:38:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.297755 on epoch=219
06/24/2022 18:38:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.294800 on epoch=224
06/24/2022 18:38:17 - INFO - __main__ - Global step 450 Train loss 0.336880 ACC 0.5 on epoch=224
06/24/2022 18:38:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.286742 on epoch=229
06/24/2022 18:38:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.329251 on epoch=234
06/24/2022 18:38:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.415634 on epoch=239
06/24/2022 18:38:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.302130 on epoch=244
06/24/2022 18:38:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.333902 on epoch=249
06/24/2022 18:38:29 - INFO - __main__ - Global step 500 Train loss 0.333532 ACC 0.53125 on epoch=249
06/24/2022 18:38:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.407145 on epoch=254
06/24/2022 18:38:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.358394 on epoch=259
06/24/2022 18:38:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.355564 on epoch=264
06/24/2022 18:38:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.324464 on epoch=269
06/24/2022 18:38:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.332156 on epoch=274
06/24/2022 18:38:42 - INFO - __main__ - Global step 550 Train loss 0.355545 ACC 0.4375 on epoch=274
06/24/2022 18:38:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.386496 on epoch=279
06/24/2022 18:38:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.298065 on epoch=284
06/24/2022 18:38:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.313871 on epoch=289
06/24/2022 18:38:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.357980 on epoch=294
06/24/2022 18:38:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.358923 on epoch=299
06/24/2022 18:38:55 - INFO - __main__ - Global step 600 Train loss 0.343067 ACC 0.53125 on epoch=299
06/24/2022 18:38:55 - INFO - __main__ - save last model!
06/24/2022 18:38:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:38:56 - INFO - __main__ - Printing 3 examples
06/24/2022 18:38:56 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 18:38:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:38:56 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 18:38:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:38:56 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 18:38:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:38:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:38:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:38:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:38:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:38:56 - INFO - __main__ - Printing 3 examples
06/24/2022 18:38:56 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 18:38:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:38:56 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 18:38:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:38:56 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 18:38:56 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:38:56 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:38:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:38:56 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:38:58 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:38:58 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:38:58 - INFO - __main__ - Printing 3 examples
06/24/2022 18:38:58 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:38:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:38:58 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:38:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:38:58 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:38:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:38:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:38:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:38:59 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:39:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:39:00 - INFO - __main__ - Starting training!
06/24/2022 18:39:04 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_42_0.0003_8_predictions.txt
06/24/2022 18:39:04 - INFO - __main__ - ACC on test data: 0.5164
06/24/2022 18:39:04 - INFO - __main__ - prefix=medical_questions_pairs_16_42, lr=0.0003, bsz=8, dev_performance=0.53125, test_performance=0.5163934426229508
06/24/2022 18:39:04 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_42, lr=0.0002, bsz=8 ...
06/24/2022 18:39:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:39:05 - INFO - __main__ - Printing 3 examples
06/24/2022 18:39:05 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 18:39:05 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:39:05 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 18:39:05 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:39:05 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 18:39:05 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:39:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:39:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:39:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:39:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:39:05 - INFO - __main__ - Printing 3 examples
06/24/2022 18:39:05 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 18:39:05 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:39:05 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 18:39:05 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:39:05 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 18:39:05 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:39:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:39:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:39:05 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:39:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:39:09 - INFO - __main__ - Starting training!
06/24/2022 18:39:11 - INFO - __main__ - Step 10 Global step 10 Train loss 18.834593 on epoch=4
06/24/2022 18:39:13 - INFO - __main__ - Step 20 Global step 20 Train loss 12.742158 on epoch=9
06/24/2022 18:39:15 - INFO - __main__ - Step 30 Global step 30 Train loss 9.119041 on epoch=14
06/24/2022 18:39:18 - INFO - __main__ - Step 40 Global step 40 Train loss 6.506831 on epoch=19
06/24/2022 18:39:20 - INFO - __main__ - Step 50 Global step 50 Train loss 5.365610 on epoch=24
06/24/2022 18:39:22 - INFO - __main__ - Global step 50 Train loss 10.513645 ACC 0.0 on epoch=24
06/24/2022 18:39:24 - INFO - __main__ - Step 60 Global step 60 Train loss 4.927352 on epoch=29
06/24/2022 18:39:27 - INFO - __main__ - Step 70 Global step 70 Train loss 4.146152 on epoch=34
06/24/2022 18:39:30 - INFO - __main__ - Step 80 Global step 80 Train loss 3.565250 on epoch=39
06/24/2022 18:39:32 - INFO - __main__ - Step 90 Global step 90 Train loss 3.559921 on epoch=44
06/24/2022 18:39:35 - INFO - __main__ - Step 100 Global step 100 Train loss 1.718338 on epoch=49
06/24/2022 18:39:35 - INFO - __main__ - Global step 100 Train loss 3.583403 ACC 0.46875 on epoch=49
06/24/2022 18:39:38 - INFO - __main__ - Step 110 Global step 110 Train loss 1.706620 on epoch=54
06/24/2022 18:39:41 - INFO - __main__ - Step 120 Global step 120 Train loss 1.233156 on epoch=59
06/24/2022 18:39:44 - INFO - __main__ - Step 130 Global step 130 Train loss 1.393503 on epoch=64
06/24/2022 18:39:46 - INFO - __main__ - Step 140 Global step 140 Train loss 1.279847 on epoch=69
06/24/2022 18:39:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.932258 on epoch=74
06/24/2022 18:39:49 - INFO - __main__ - Global step 150 Train loss 1.309077 ACC 0.375 on epoch=74
06/24/2022 18:39:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.829624 on epoch=79
06/24/2022 18:39:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.562093 on epoch=84
06/24/2022 18:39:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.728968 on epoch=89
06/24/2022 18:39:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.808906 on epoch=94
06/24/2022 18:40:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.836460 on epoch=99
06/24/2022 18:40:02 - INFO - __main__ - Global step 200 Train loss 0.753210 ACC 0.375 on epoch=99
06/24/2022 18:40:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.641238 on epoch=104
06/24/2022 18:40:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.543882 on epoch=109
06/24/2022 18:40:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.773020 on epoch=114
06/24/2022 18:40:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.626340 on epoch=119
06/24/2022 18:40:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.499077 on epoch=124
06/24/2022 18:40:15 - INFO - __main__ - Global step 250 Train loss 0.616711 ACC 0.4375 on epoch=124
06/24/2022 18:40:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.467683 on epoch=129
06/24/2022 18:40:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.484771 on epoch=134
06/24/2022 18:40:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.637066 on epoch=139
06/24/2022 18:40:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.556505 on epoch=144
06/24/2022 18:40:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.443672 on epoch=149
06/24/2022 18:40:28 - INFO - __main__ - Global step 300 Train loss 0.517939 ACC 0.5 on epoch=149
06/24/2022 18:40:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.453857 on epoch=154
06/24/2022 18:40:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.403648 on epoch=159
06/24/2022 18:40:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.505191 on epoch=164
06/24/2022 18:40:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.455766 on epoch=169
06/24/2022 18:40:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.422730 on epoch=174
06/24/2022 18:40:41 - INFO - __main__ - Global step 350 Train loss 0.448239 ACC 0.46875 on epoch=174
06/24/2022 18:40:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.362104 on epoch=179
06/24/2022 18:40:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.551945 on epoch=184
06/24/2022 18:40:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.339837 on epoch=189
06/24/2022 18:40:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.494955 on epoch=194
06/24/2022 18:40:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.396682 on epoch=199
06/24/2022 18:40:53 - INFO - __main__ - Global step 400 Train loss 0.429105 ACC 0.5625 on epoch=199
06/24/2022 18:40:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.411655 on epoch=204
06/24/2022 18:40:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.334580 on epoch=209
06/24/2022 18:41:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.414032 on epoch=214
06/24/2022 18:41:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.420239 on epoch=219
06/24/2022 18:41:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.307533 on epoch=224
06/24/2022 18:41:07 - INFO - __main__ - Global step 450 Train loss 0.377608 ACC 0.46875 on epoch=224
06/24/2022 18:41:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.367273 on epoch=229
06/24/2022 18:41:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.428434 on epoch=234
06/24/2022 18:41:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.377663 on epoch=239
06/24/2022 18:41:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.373400 on epoch=244
06/24/2022 18:41:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.344655 on epoch=249
06/24/2022 18:41:19 - INFO - __main__ - Global step 500 Train loss 0.378285 ACC 0.53125 on epoch=249
06/24/2022 18:41:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.310999 on epoch=254
06/24/2022 18:41:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.412067 on epoch=259
06/24/2022 18:41:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.337864 on epoch=264
06/24/2022 18:41:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.383795 on epoch=269
06/24/2022 18:41:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.472202 on epoch=274
06/24/2022 18:41:32 - INFO - __main__ - Global step 550 Train loss 0.383386 ACC 0.46875 on epoch=274
06/24/2022 18:41:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.338203 on epoch=279
06/24/2022 18:41:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.339034 on epoch=284
06/24/2022 18:41:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.514689 on epoch=289
06/24/2022 18:41:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.352785 on epoch=294
06/24/2022 18:41:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.358937 on epoch=299
06/24/2022 18:41:45 - INFO - __main__ - Global step 600 Train loss 0.380729 ACC 0.5 on epoch=299
06/24/2022 18:41:45 - INFO - __main__ - save last model!
06/24/2022 18:41:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:41:45 - INFO - __main__ - Printing 3 examples
06/24/2022 18:41:45 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 18:41:45 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:45 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 18:41:45 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:45 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 18:41:45 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:41:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:41:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:41:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:41:45 - INFO - __main__ - Printing 3 examples
06/24/2022 18:41:45 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 18:41:45 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:45 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 18:41:45 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:45 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 18:41:45 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:45 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:41:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:41:45 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:41:47 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:41:47 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:41:47 - INFO - __main__ - Printing 3 examples
06/24/2022 18:41:48 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:41:48 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:48 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:41:48 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:48 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:41:48 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:48 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:41:48 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:41:48 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:41:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:41:49 - INFO - __main__ - Starting training!
06/24/2022 18:41:53 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_42_0.0002_8_predictions.txt
06/24/2022 18:41:53 - INFO - __main__ - ACC on test data: 0.5475
06/24/2022 18:41:53 - INFO - __main__ - prefix=medical_questions_pairs_16_42, lr=0.0002, bsz=8, dev_performance=0.5625, test_performance=0.5475409836065573
06/24/2022 18:41:53 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_42, lr=0.0001, bsz=8 ...
06/24/2022 18:41:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:41:54 - INFO - __main__ - Printing 3 examples
06/24/2022 18:41:54 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/24/2022 18:41:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:54 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/24/2022 18:41:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/24/2022 18:41:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:41:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:41:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:41:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:41:54 - INFO - __main__ - Printing 3 examples
06/24/2022 18:41:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/24/2022 18:41:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/24/2022 18:41:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/24/2022 18:41:54 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:41:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:41:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:41:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:41:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:41:58 - INFO - __main__ - Starting training!
06/24/2022 18:42:00 - INFO - __main__ - Step 10 Global step 10 Train loss 18.562811 on epoch=4
06/24/2022 18:42:02 - INFO - __main__ - Step 20 Global step 20 Train loss 17.588703 on epoch=9
06/24/2022 18:42:05 - INFO - __main__ - Step 30 Global step 30 Train loss 12.405912 on epoch=14
06/24/2022 18:42:07 - INFO - __main__ - Step 40 Global step 40 Train loss 9.968210 on epoch=19
06/24/2022 18:42:10 - INFO - __main__ - Step 50 Global step 50 Train loss 9.422848 on epoch=24
06/24/2022 18:42:12 - INFO - __main__ - Global step 50 Train loss 13.589697 ACC 0.0 on epoch=24
06/24/2022 18:42:15 - INFO - __main__ - Step 60 Global step 60 Train loss 8.298203 on epoch=29
06/24/2022 18:42:17 - INFO - __main__ - Step 70 Global step 70 Train loss 7.544498 on epoch=34
06/24/2022 18:42:20 - INFO - __main__ - Step 80 Global step 80 Train loss 7.275489 on epoch=39
06/24/2022 18:42:22 - INFO - __main__ - Step 90 Global step 90 Train loss 6.113791 on epoch=44
06/24/2022 18:42:25 - INFO - __main__ - Step 100 Global step 100 Train loss 5.188893 on epoch=49
06/24/2022 18:42:26 - INFO - __main__ - Global step 100 Train loss 6.884175 ACC 0.0 on epoch=49
06/24/2022 18:42:29 - INFO - __main__ - Step 110 Global step 110 Train loss 5.142051 on epoch=54
06/24/2022 18:42:31 - INFO - __main__ - Step 120 Global step 120 Train loss 5.106194 on epoch=59
06/24/2022 18:42:34 - INFO - __main__ - Step 130 Global step 130 Train loss 3.894529 on epoch=64
06/24/2022 18:42:36 - INFO - __main__ - Step 140 Global step 140 Train loss 3.269058 on epoch=69
06/24/2022 18:42:39 - INFO - __main__ - Step 150 Global step 150 Train loss 3.560943 on epoch=74
06/24/2022 18:42:39 - INFO - __main__ - Global step 150 Train loss 4.194555 ACC 0.5 on epoch=74
06/24/2022 18:42:42 - INFO - __main__ - Step 160 Global step 160 Train loss 2.360893 on epoch=79
06/24/2022 18:42:45 - INFO - __main__ - Step 170 Global step 170 Train loss 2.389047 on epoch=84
06/24/2022 18:42:47 - INFO - __main__ - Step 180 Global step 180 Train loss 1.828407 on epoch=89
06/24/2022 18:42:50 - INFO - __main__ - Step 190 Global step 190 Train loss 1.766296 on epoch=94
06/24/2022 18:42:52 - INFO - __main__ - Step 200 Global step 200 Train loss 1.622331 on epoch=99
06/24/2022 18:42:53 - INFO - __main__ - Global step 200 Train loss 1.993395 ACC 0.53125 on epoch=99
06/24/2022 18:42:55 - INFO - __main__ - Step 210 Global step 210 Train loss 1.208053 on epoch=104
06/24/2022 18:42:58 - INFO - __main__ - Step 220 Global step 220 Train loss 1.206966 on epoch=109
06/24/2022 18:43:00 - INFO - __main__ - Step 230 Global step 230 Train loss 1.037651 on epoch=114
06/24/2022 18:43:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.939018 on epoch=119
06/24/2022 18:43:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.745821 on epoch=124
06/24/2022 18:43:06 - INFO - __main__ - Global step 250 Train loss 1.027502 ACC 0.5625 on epoch=124
06/24/2022 18:43:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.902901 on epoch=129
06/24/2022 18:43:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.948832 on epoch=134
06/24/2022 18:43:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.741285 on epoch=139
06/24/2022 18:43:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.780971 on epoch=144
06/24/2022 18:43:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.792251 on epoch=149
06/24/2022 18:43:19 - INFO - __main__ - Global step 300 Train loss 0.833248 ACC 0.5 on epoch=149
06/24/2022 18:43:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.843359 on epoch=154
06/24/2022 18:43:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.778508 on epoch=159
06/24/2022 18:43:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.622054 on epoch=164
06/24/2022 18:43:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.488532 on epoch=169
06/24/2022 18:43:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.566319 on epoch=174
06/24/2022 18:43:32 - INFO - __main__ - Global step 350 Train loss 0.659755 ACC 0.5 on epoch=174
06/24/2022 18:43:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.553197 on epoch=179
06/24/2022 18:43:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.386755 on epoch=184
06/24/2022 18:43:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.510384 on epoch=189
06/24/2022 18:43:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.463568 on epoch=194
06/24/2022 18:43:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.450525 on epoch=199
06/24/2022 18:43:44 - INFO - __main__ - Global step 400 Train loss 0.472886 ACC 0.5 on epoch=199
06/24/2022 18:43:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.430169 on epoch=204
06/24/2022 18:43:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.427223 on epoch=209
06/24/2022 18:43:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.520932 on epoch=214
06/24/2022 18:43:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.446476 on epoch=219
06/24/2022 18:43:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.502495 on epoch=224
06/24/2022 18:43:57 - INFO - __main__ - Global step 450 Train loss 0.465459 ACC 0.5 on epoch=224
06/24/2022 18:44:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.585123 on epoch=229
06/24/2022 18:44:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.584479 on epoch=234
06/24/2022 18:44:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.427983 on epoch=239
06/24/2022 18:44:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.445800 on epoch=244
06/24/2022 18:44:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.587322 on epoch=249
06/24/2022 18:44:10 - INFO - __main__ - Global step 500 Train loss 0.526142 ACC 0.46875 on epoch=249
06/24/2022 18:44:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.376328 on epoch=254
06/24/2022 18:44:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.426703 on epoch=259
06/24/2022 18:44:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.520669 on epoch=264
06/24/2022 18:44:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.403046 on epoch=269
06/24/2022 18:44:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.525376 on epoch=274
06/24/2022 18:44:23 - INFO - __main__ - Global step 550 Train loss 0.450424 ACC 0.5 on epoch=274
06/24/2022 18:44:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.342869 on epoch=279
06/24/2022 18:44:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.361286 on epoch=284
06/24/2022 18:44:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.512545 on epoch=289
06/24/2022 18:44:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.428887 on epoch=294
06/24/2022 18:44:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.353433 on epoch=299
06/24/2022 18:44:36 - INFO - __main__ - Global step 600 Train loss 0.399804 ACC 0.5625 on epoch=299
06/24/2022 18:44:36 - INFO - __main__ - save last model!
06/24/2022 18:44:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:44:36 - INFO - __main__ - Printing 3 examples
06/24/2022 18:44:36 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 18:44:36 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:36 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 18:44:36 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:36 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 18:44:36 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:44:37 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:44:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:44:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:44:37 - INFO - __main__ - Printing 3 examples
06/24/2022 18:44:37 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 18:44:37 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:37 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 18:44:37 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:37 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 18:44:37 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:37 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:44:37 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:44:37 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:44:38 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:44:39 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:44:39 - INFO - __main__ - Printing 3 examples
06/24/2022 18:44:39 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:44:39 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:39 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:44:39 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:39 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:44:39 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:44:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:44:40 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:44:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:44:40 - INFO - __main__ - Starting training!
06/24/2022 18:44:44 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_42_0.0001_8_predictions.txt
06/24/2022 18:44:44 - INFO - __main__ - ACC on test data: 0.5295
06/24/2022 18:44:44 - INFO - __main__ - prefix=medical_questions_pairs_16_42, lr=0.0001, bsz=8, dev_performance=0.5625, test_performance=0.5295081967213114
06/24/2022 18:44:45 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_87, lr=0.0005, bsz=8 ...
06/24/2022 18:44:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:44:45 - INFO - __main__ - Printing 3 examples
06/24/2022 18:44:45 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 18:44:45 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:45 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 18:44:45 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:45 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 18:44:45 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:44:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:44:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:44:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:44:46 - INFO - __main__ - Printing 3 examples
06/24/2022 18:44:46 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 18:44:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:46 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 18:44:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:46 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 18:44:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:44:46 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:44:46 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:44:46 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:44:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:44:50 - INFO - __main__ - Starting training!
06/24/2022 18:44:52 - INFO - __main__ - Step 10 Global step 10 Train loss 17.920025 on epoch=4
06/24/2022 18:44:54 - INFO - __main__ - Step 20 Global step 20 Train loss 11.766829 on epoch=9
06/24/2022 18:44:56 - INFO - __main__ - Step 30 Global step 30 Train loss 6.230873 on epoch=14
06/24/2022 18:44:59 - INFO - __main__ - Step 40 Global step 40 Train loss 3.218684 on epoch=19
06/24/2022 18:45:01 - INFO - __main__ - Step 50 Global step 50 Train loss 1.796926 on epoch=24
06/24/2022 18:45:02 - INFO - __main__ - Global step 50 Train loss 8.186666 ACC 0.5 on epoch=24
06/24/2022 18:45:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.898172 on epoch=29
06/24/2022 18:45:07 - INFO - __main__ - Step 70 Global step 70 Train loss 1.322638 on epoch=34
06/24/2022 18:45:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.694838 on epoch=39
06/24/2022 18:45:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.848734 on epoch=44
06/24/2022 18:45:14 - INFO - __main__ - Step 100 Global step 100 Train loss 1.087901 on epoch=49
06/24/2022 18:45:14 - INFO - __main__ - Global step 100 Train loss 0.970457 ACC 0.5625 on epoch=49
06/24/2022 18:45:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.622989 on epoch=54
06/24/2022 18:45:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.653820 on epoch=59
06/24/2022 18:45:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.690055 on epoch=64
06/24/2022 18:45:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.620346 on epoch=69
06/24/2022 18:45:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.520343 on epoch=74
06/24/2022 18:45:27 - INFO - __main__ - Global step 150 Train loss 0.621511 ACC 0.5 on epoch=74
06/24/2022 18:45:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.560075 on epoch=79
06/24/2022 18:45:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.546418 on epoch=84
06/24/2022 18:45:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.406938 on epoch=89
06/24/2022 18:45:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.451675 on epoch=94
06/24/2022 18:45:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.489769 on epoch=99
06/24/2022 18:45:40 - INFO - __main__ - Global step 200 Train loss 0.490975 ACC 0.5 on epoch=99
06/24/2022 18:45:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.438331 on epoch=104
06/24/2022 18:45:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.436592 on epoch=109
06/24/2022 18:45:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.370192 on epoch=114
06/24/2022 18:45:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.357316 on epoch=119
06/24/2022 18:45:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.408101 on epoch=124
06/24/2022 18:45:53 - INFO - __main__ - Global step 250 Train loss 0.402107 ACC 0.5 on epoch=124
06/24/2022 18:45:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.442725 on epoch=129
06/24/2022 18:45:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.393109 on epoch=134
06/24/2022 18:46:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.446769 on epoch=139
06/24/2022 18:46:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.473394 on epoch=144
06/24/2022 18:46:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.435363 on epoch=149
06/24/2022 18:46:06 - INFO - __main__ - Global step 300 Train loss 0.438272 ACC 0.4375 on epoch=149
06/24/2022 18:46:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.497428 on epoch=154
06/24/2022 18:46:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.382843 on epoch=159
06/24/2022 18:46:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.442223 on epoch=164
06/24/2022 18:46:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.426356 on epoch=169
06/24/2022 18:46:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.421568 on epoch=174
06/24/2022 18:46:19 - INFO - __main__ - Global step 350 Train loss 0.434084 ACC 0.375 on epoch=174
06/24/2022 18:46:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.316172 on epoch=179
06/24/2022 18:46:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.354214 on epoch=184
06/24/2022 18:46:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.342788 on epoch=189
06/24/2022 18:46:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.390802 on epoch=194
06/24/2022 18:46:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.391589 on epoch=199
06/24/2022 18:46:32 - INFO - __main__ - Global step 400 Train loss 0.359113 ACC 0.5 on epoch=199
06/24/2022 18:46:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.362500 on epoch=204
06/24/2022 18:46:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.340375 on epoch=209
06/24/2022 18:46:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.279989 on epoch=214
06/24/2022 18:46:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.295504 on epoch=219
06/24/2022 18:46:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.362294 on epoch=224
06/24/2022 18:46:45 - INFO - __main__ - Global step 450 Train loss 0.328132 ACC 0.5 on epoch=224
06/24/2022 18:46:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.272496 on epoch=229
06/24/2022 18:46:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.337376 on epoch=234
06/24/2022 18:46:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.315464 on epoch=239
06/24/2022 18:46:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.307328 on epoch=244
06/24/2022 18:46:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.306854 on epoch=249
06/24/2022 18:46:58 - INFO - __main__ - Global step 500 Train loss 0.307903 ACC 0.5 on epoch=249
06/24/2022 18:47:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.330066 on epoch=254
06/24/2022 18:47:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.327094 on epoch=259
06/24/2022 18:47:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.317921 on epoch=264
06/24/2022 18:47:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.320449 on epoch=269
06/24/2022 18:47:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.296080 on epoch=274
06/24/2022 18:47:11 - INFO - __main__ - Global step 550 Train loss 0.318322 ACC 0.4375 on epoch=274
06/24/2022 18:47:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.285292 on epoch=279
06/24/2022 18:47:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.320262 on epoch=284
06/24/2022 18:47:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.303695 on epoch=289
06/24/2022 18:47:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.286744 on epoch=294
06/24/2022 18:47:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.277322 on epoch=299
06/24/2022 18:47:24 - INFO - __main__ - Global step 600 Train loss 0.294663 ACC 0.5 on epoch=299
06/24/2022 18:47:24 - INFO - __main__ - save last model!
06/24/2022 18:47:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:47:25 - INFO - __main__ - Printing 3 examples
06/24/2022 18:47:25 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 18:47:25 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:25 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 18:47:25 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:25 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 18:47:25 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:47:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:47:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:47:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:47:25 - INFO - __main__ - Printing 3 examples
06/24/2022 18:47:25 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 18:47:25 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:25 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 18:47:25 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:25 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 18:47:25 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:25 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:47:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:47:25 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:47:26 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:47:27 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:47:27 - INFO - __main__ - Printing 3 examples
06/24/2022 18:47:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:47:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:47:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:47:27 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:47:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:47:28 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:47:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:47:29 - INFO - __main__ - Starting training!
06/24/2022 18:47:32 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_87_0.0005_8_predictions.txt
06/24/2022 18:47:32 - INFO - __main__ - ACC on test data: 0.4984
06/24/2022 18:47:32 - INFO - __main__ - prefix=medical_questions_pairs_16_87, lr=0.0005, bsz=8, dev_performance=0.5625, test_performance=0.49836065573770494
06/24/2022 18:47:32 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_87, lr=0.0003, bsz=8 ...
06/24/2022 18:47:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:47:33 - INFO - __main__ - Printing 3 examples
06/24/2022 18:47:33 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 18:47:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 18:47:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:33 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 18:47:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:47:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:47:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:47:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:47:33 - INFO - __main__ - Printing 3 examples
06/24/2022 18:47:33 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 18:47:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:33 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 18:47:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:33 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 18:47:33 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:47:33 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:47:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:47:33 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:47:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:47:37 - INFO - __main__ - Starting training!
06/24/2022 18:47:39 - INFO - __main__ - Step 10 Global step 10 Train loss 18.821100 on epoch=4
06/24/2022 18:47:41 - INFO - __main__ - Step 20 Global step 20 Train loss 14.745058 on epoch=9
06/24/2022 18:47:44 - INFO - __main__ - Step 30 Global step 30 Train loss 9.716303 on epoch=14
06/24/2022 18:47:46 - INFO - __main__ - Step 40 Global step 40 Train loss 6.937131 on epoch=19
06/24/2022 18:47:49 - INFO - __main__ - Step 50 Global step 50 Train loss 4.869977 on epoch=24
06/24/2022 18:47:50 - INFO - __main__ - Global step 50 Train loss 11.017914 ACC 0.34375 on epoch=24
06/24/2022 18:47:53 - INFO - __main__ - Step 60 Global step 60 Train loss 3.488509 on epoch=29
06/24/2022 18:47:55 - INFO - __main__ - Step 70 Global step 70 Train loss 2.586156 on epoch=34
06/24/2022 18:47:58 - INFO - __main__ - Step 80 Global step 80 Train loss 1.857976 on epoch=39
06/24/2022 18:48:00 - INFO - __main__ - Step 90 Global step 90 Train loss 1.371373 on epoch=44
06/24/2022 18:48:03 - INFO - __main__ - Step 100 Global step 100 Train loss 1.151046 on epoch=49
06/24/2022 18:48:03 - INFO - __main__ - Global step 100 Train loss 2.091012 ACC 0.53125 on epoch=49
06/24/2022 18:48:06 - INFO - __main__ - Step 110 Global step 110 Train loss 1.334499 on epoch=54
06/24/2022 18:48:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.948972 on epoch=59
06/24/2022 18:48:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.751886 on epoch=64
06/24/2022 18:48:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.641573 on epoch=69
06/24/2022 18:48:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.673128 on epoch=74
06/24/2022 18:48:16 - INFO - __main__ - Global step 150 Train loss 0.870012 ACC 0.4375 on epoch=74
06/24/2022 18:48:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.466465 on epoch=79
06/24/2022 18:48:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.998029 on epoch=84
06/24/2022 18:48:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.637455 on epoch=89
06/24/2022 18:48:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.567941 on epoch=94
06/24/2022 18:48:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.606375 on epoch=99
06/24/2022 18:48:28 - INFO - __main__ - Global step 200 Train loss 0.655253 ACC 0.4375 on epoch=99
06/24/2022 18:48:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.445043 on epoch=104
06/24/2022 18:48:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.450402 on epoch=109
06/24/2022 18:48:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.559782 on epoch=114
06/24/2022 18:48:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.497239 on epoch=119
06/24/2022 18:48:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.590935 on epoch=124
06/24/2022 18:48:40 - INFO - __main__ - Global step 250 Train loss 0.508680 ACC 0.4375 on epoch=124
06/24/2022 18:48:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.535409 on epoch=129
06/24/2022 18:48:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.334390 on epoch=134
06/24/2022 18:48:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.494930 on epoch=139
06/24/2022 18:48:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.374430 on epoch=144
06/24/2022 18:48:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.496227 on epoch=149
06/24/2022 18:48:53 - INFO - __main__ - Global step 300 Train loss 0.447077 ACC 0.5 on epoch=149
06/24/2022 18:48:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.479455 on epoch=154
06/24/2022 18:48:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.345795 on epoch=159
06/24/2022 18:49:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.293321 on epoch=164
06/24/2022 18:49:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.389675 on epoch=169
06/24/2022 18:49:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.354846 on epoch=174
06/24/2022 18:49:05 - INFO - __main__ - Global step 350 Train loss 0.372618 ACC 0.46875 on epoch=174
06/24/2022 18:49:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.327195 on epoch=179
06/24/2022 18:49:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.253262 on epoch=184
06/24/2022 18:49:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.280916 on epoch=189
06/24/2022 18:49:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.326692 on epoch=194
06/24/2022 18:49:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.295488 on epoch=199
06/24/2022 18:49:18 - INFO - __main__ - Global step 400 Train loss 0.296711 ACC 0.59375 on epoch=199
06/24/2022 18:49:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.250063 on epoch=204
06/24/2022 18:49:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.284283 on epoch=209
06/24/2022 18:49:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.276925 on epoch=214
06/24/2022 18:49:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.282813 on epoch=219
06/24/2022 18:49:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.305707 on epoch=224
06/24/2022 18:49:31 - INFO - __main__ - Global step 450 Train loss 0.279958 ACC 0.53125 on epoch=224
06/24/2022 18:49:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.279039 on epoch=229
06/24/2022 18:49:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.173456 on epoch=234
06/24/2022 18:49:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.242912 on epoch=239
06/24/2022 18:49:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.184761 on epoch=244
06/24/2022 18:49:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.182638 on epoch=249
06/24/2022 18:49:43 - INFO - __main__ - Global step 500 Train loss 0.212561 ACC 0.53125 on epoch=249
06/24/2022 18:49:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.199103 on epoch=254
06/24/2022 18:49:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.225319 on epoch=259
06/24/2022 18:49:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.178884 on epoch=264
06/24/2022 18:49:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.166742 on epoch=269
06/24/2022 18:49:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.239903 on epoch=274
06/24/2022 18:49:56 - INFO - __main__ - Global step 550 Train loss 0.201990 ACC 0.53125 on epoch=274
06/24/2022 18:49:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.183556 on epoch=279
06/24/2022 18:50:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.252097 on epoch=284
06/24/2022 18:50:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.187334 on epoch=289
06/24/2022 18:50:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.130376 on epoch=294
06/24/2022 18:50:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.211305 on epoch=299
06/24/2022 18:50:09 - INFO - __main__ - Global step 600 Train loss 0.192934 ACC 0.53125 on epoch=299
06/24/2022 18:50:09 - INFO - __main__ - save last model!
06/24/2022 18:50:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:50:10 - INFO - __main__ - Printing 3 examples
06/24/2022 18:50:10 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 18:50:10 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:10 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 18:50:10 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:10 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 18:50:10 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:50:10 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:50:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:50:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:50:10 - INFO - __main__ - Printing 3 examples
06/24/2022 18:50:10 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 18:50:10 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:10 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 18:50:10 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:10 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 18:50:10 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:10 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:50:10 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:50:10 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:50:12 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:50:12 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:50:12 - INFO - __main__ - Printing 3 examples
06/24/2022 18:50:12 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:50:12 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:12 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:50:12 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:12 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:50:12 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:50:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:50:13 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:50:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:50:14 - INFO - __main__ - Starting training!
06/24/2022 18:50:17 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_87_0.0003_8_predictions.txt
06/24/2022 18:50:18 - INFO - __main__ - ACC on test data: 0.5393
06/24/2022 18:50:18 - INFO - __main__ - prefix=medical_questions_pairs_16_87, lr=0.0003, bsz=8, dev_performance=0.59375, test_performance=0.5393442622950819
06/24/2022 18:50:18 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_87, lr=0.0002, bsz=8 ...
06/24/2022 18:50:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:50:19 - INFO - __main__ - Printing 3 examples
06/24/2022 18:50:19 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 18:50:19 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:19 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 18:50:19 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:19 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 18:50:19 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:50:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:50:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:50:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:50:19 - INFO - __main__ - Printing 3 examples
06/24/2022 18:50:19 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 18:50:19 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:19 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 18:50:19 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:19 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 18:50:19 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:50:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:50:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:50:19 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:50:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:50:23 - INFO - __main__ - Starting training!
06/24/2022 18:50:25 - INFO - __main__ - Step 10 Global step 10 Train loss 18.086256 on epoch=4
06/24/2022 18:50:27 - INFO - __main__ - Step 20 Global step 20 Train loss 15.279223 on epoch=9
06/24/2022 18:50:30 - INFO - __main__ - Step 30 Global step 30 Train loss 10.153497 on epoch=14
06/24/2022 18:50:32 - INFO - __main__ - Step 40 Global step 40 Train loss 8.655247 on epoch=19
06/24/2022 18:50:35 - INFO - __main__ - Step 50 Global step 50 Train loss 5.895544 on epoch=24
06/24/2022 18:50:37 - INFO - __main__ - Global step 50 Train loss 11.613954 ACC 0.0 on epoch=24
06/24/2022 18:50:40 - INFO - __main__ - Step 60 Global step 60 Train loss 5.115770 on epoch=29
06/24/2022 18:50:42 - INFO - __main__ - Step 70 Global step 70 Train loss 4.735017 on epoch=34
06/24/2022 18:50:44 - INFO - __main__ - Step 80 Global step 80 Train loss 3.710901 on epoch=39
06/24/2022 18:50:47 - INFO - __main__ - Step 90 Global step 90 Train loss 3.222229 on epoch=44
06/24/2022 18:50:49 - INFO - __main__ - Step 100 Global step 100 Train loss 2.702595 on epoch=49
06/24/2022 18:50:50 - INFO - __main__ - Global step 100 Train loss 3.897302 ACC 0.5 on epoch=49
06/24/2022 18:50:53 - INFO - __main__ - Step 110 Global step 110 Train loss 2.224216 on epoch=54
06/24/2022 18:50:55 - INFO - __main__ - Step 120 Global step 120 Train loss 2.305759 on epoch=59
06/24/2022 18:50:58 - INFO - __main__ - Step 130 Global step 130 Train loss 1.116160 on epoch=64
06/24/2022 18:51:00 - INFO - __main__ - Step 140 Global step 140 Train loss 1.358167 on epoch=69
06/24/2022 18:51:02 - INFO - __main__ - Step 150 Global step 150 Train loss 1.419963 on epoch=74
06/24/2022 18:51:03 - INFO - __main__ - Global step 150 Train loss 1.684853 ACC 0.40625 on epoch=74
06/24/2022 18:51:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.866685 on epoch=79
06/24/2022 18:51:08 - INFO - __main__ - Step 170 Global step 170 Train loss 1.262867 on epoch=84
06/24/2022 18:51:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.866479 on epoch=89
06/24/2022 18:51:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.932430 on epoch=94
06/24/2022 18:51:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.765912 on epoch=99
06/24/2022 18:51:15 - INFO - __main__ - Global step 200 Train loss 0.938875 ACC 0.4375 on epoch=99
06/24/2022 18:51:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.580103 on epoch=104
06/24/2022 18:51:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.858506 on epoch=109
06/24/2022 18:51:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.906668 on epoch=114
06/24/2022 18:51:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.765738 on epoch=119
06/24/2022 18:51:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.592945 on epoch=124
06/24/2022 18:51:28 - INFO - __main__ - Global step 250 Train loss 0.740792 ACC 0.5 on epoch=124
06/24/2022 18:51:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.504193 on epoch=129
06/24/2022 18:51:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.547251 on epoch=134
06/24/2022 18:51:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.617583 on epoch=139
06/24/2022 18:51:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.471276 on epoch=144
06/24/2022 18:51:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.523454 on epoch=149
06/24/2022 18:51:41 - INFO - __main__ - Global step 300 Train loss 0.532751 ACC 0.5 on epoch=149
06/24/2022 18:51:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.450311 on epoch=154
06/24/2022 18:51:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.632990 on epoch=159
06/24/2022 18:51:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.484370 on epoch=164
06/24/2022 18:51:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.429331 on epoch=169
06/24/2022 18:51:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.404880 on epoch=174
06/24/2022 18:51:53 - INFO - __main__ - Global step 350 Train loss 0.480376 ACC 0.5 on epoch=174
06/24/2022 18:51:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.496269 on epoch=179
06/24/2022 18:51:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.341636 on epoch=184
06/24/2022 18:52:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.320308 on epoch=189
06/24/2022 18:52:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.422876 on epoch=194
06/24/2022 18:52:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.474894 on epoch=199
06/24/2022 18:52:06 - INFO - __main__ - Global step 400 Train loss 0.411197 ACC 0.5 on epoch=199
06/24/2022 18:52:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.479643 on epoch=204
06/24/2022 18:52:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.355010 on epoch=209
06/24/2022 18:52:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.366633 on epoch=214
06/24/2022 18:52:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.243061 on epoch=219
06/24/2022 18:52:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.266638 on epoch=224
06/24/2022 18:52:19 - INFO - __main__ - Global step 450 Train loss 0.342197 ACC 0.375 on epoch=224
06/24/2022 18:52:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.275746 on epoch=229
06/24/2022 18:52:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.261771 on epoch=234
06/24/2022 18:52:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.151529 on epoch=239
06/24/2022 18:52:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.198294 on epoch=244
06/24/2022 18:52:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.252483 on epoch=249
06/24/2022 18:52:31 - INFO - __main__ - Global step 500 Train loss 0.227965 ACC 0.46875 on epoch=249
06/24/2022 18:52:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.168207 on epoch=254
06/24/2022 18:52:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.154031 on epoch=259
06/24/2022 18:52:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.138211 on epoch=264
06/24/2022 18:52:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.317247 on epoch=269
06/24/2022 18:52:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.322699 on epoch=274
06/24/2022 18:52:44 - INFO - __main__ - Global step 550 Train loss 0.220079 ACC 0.40625 on epoch=274
06/24/2022 18:52:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.212347 on epoch=279
06/24/2022 18:52:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.120139 on epoch=284
06/24/2022 18:52:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.148459 on epoch=289
06/24/2022 18:52:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.172851 on epoch=294
06/24/2022 18:52:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.070342 on epoch=299
06/24/2022 18:52:57 - INFO - __main__ - Global step 600 Train loss 0.144828 ACC 0.4375 on epoch=299
06/24/2022 18:52:57 - INFO - __main__ - save last model!
06/24/2022 18:52:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:52:58 - INFO - __main__ - Printing 3 examples
06/24/2022 18:52:58 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 18:52:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:52:58 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 18:52:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:52:58 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 18:52:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:52:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:52:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:52:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:52:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:52:58 - INFO - __main__ - Printing 3 examples
06/24/2022 18:52:58 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 18:52:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:52:58 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 18:52:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:52:58 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 18:52:58 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:52:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:52:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:52:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:53:00 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:53:00 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:53:00 - INFO - __main__ - Printing 3 examples
06/24/2022 18:53:00 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:53:00 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:53:00 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:53:00 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:53:00 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:53:00 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:53:00 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:53:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:53:01 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:53:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:53:02 - INFO - __main__ - Starting training!
06/24/2022 18:53:04 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_87_0.0002_8_predictions.txt
06/24/2022 18:53:04 - INFO - __main__ - ACC on test data: 0.4836
06/24/2022 18:53:04 - INFO - __main__ - prefix=medical_questions_pairs_16_87, lr=0.0002, bsz=8, dev_performance=0.5, test_performance=0.48360655737704916
06/24/2022 18:53:04 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_87, lr=0.0001, bsz=8 ...
06/24/2022 18:53:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:53:05 - INFO - __main__ - Printing 3 examples
06/24/2022 18:53:05 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/24/2022 18:53:05 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:53:05 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/24/2022 18:53:05 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:53:05 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/24/2022 18:53:05 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:53:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:53:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:53:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:53:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:53:05 - INFO - __main__ - Printing 3 examples
06/24/2022 18:53:05 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/24/2022 18:53:05 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:53:05 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/24/2022 18:53:05 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:53:05 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/24/2022 18:53:05 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:53:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:53:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:53:05 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:53:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:53:10 - INFO - __main__ - Starting training!
06/24/2022 18:53:12 - INFO - __main__ - Step 10 Global step 10 Train loss 18.151890 on epoch=4
06/24/2022 18:53:14 - INFO - __main__ - Step 20 Global step 20 Train loss 16.951363 on epoch=9
06/24/2022 18:53:16 - INFO - __main__ - Step 30 Global step 30 Train loss 12.102943 on epoch=14
06/24/2022 18:53:19 - INFO - __main__ - Step 40 Global step 40 Train loss 9.925978 on epoch=19
06/24/2022 18:53:21 - INFO - __main__ - Step 50 Global step 50 Train loss 8.258475 on epoch=24
06/24/2022 18:53:24 - INFO - __main__ - Global step 50 Train loss 13.078130 ACC 0.0 on epoch=24
06/24/2022 18:53:26 - INFO - __main__ - Step 60 Global step 60 Train loss 7.005045 on epoch=29
06/24/2022 18:53:29 - INFO - __main__ - Step 70 Global step 70 Train loss 6.313018 on epoch=34
06/24/2022 18:53:31 - INFO - __main__ - Step 80 Global step 80 Train loss 5.952629 on epoch=39
06/24/2022 18:53:34 - INFO - __main__ - Step 90 Global step 90 Train loss 5.691124 on epoch=44
06/24/2022 18:53:36 - INFO - __main__ - Step 100 Global step 100 Train loss 4.523479 on epoch=49
06/24/2022 18:53:37 - INFO - __main__ - Global step 100 Train loss 5.897058 ACC 0.0 on epoch=49
06/24/2022 18:53:40 - INFO - __main__ - Step 110 Global step 110 Train loss 5.014492 on epoch=54
06/24/2022 18:53:42 - INFO - __main__ - Step 120 Global step 120 Train loss 3.374057 on epoch=59
06/24/2022 18:53:44 - INFO - __main__ - Step 130 Global step 130 Train loss 3.929295 on epoch=64
06/24/2022 18:53:47 - INFO - __main__ - Step 140 Global step 140 Train loss 3.558249 on epoch=69
06/24/2022 18:53:49 - INFO - __main__ - Step 150 Global step 150 Train loss 2.342378 on epoch=74
06/24/2022 18:53:49 - INFO - __main__ - Global step 150 Train loss 3.643694 ACC 0.5 on epoch=74
06/24/2022 18:53:52 - INFO - __main__ - Step 160 Global step 160 Train loss 2.202863 on epoch=79
06/24/2022 18:53:55 - INFO - __main__ - Step 170 Global step 170 Train loss 1.451550 on epoch=84
06/24/2022 18:53:57 - INFO - __main__ - Step 180 Global step 180 Train loss 2.377009 on epoch=89
06/24/2022 18:54:00 - INFO - __main__ - Step 190 Global step 190 Train loss 1.222351 on epoch=94
06/24/2022 18:54:02 - INFO - __main__ - Step 200 Global step 200 Train loss 1.365089 on epoch=99
06/24/2022 18:54:02 - INFO - __main__ - Global step 200 Train loss 1.723772 ACC 0.3125 on epoch=99
06/24/2022 18:54:05 - INFO - __main__ - Step 210 Global step 210 Train loss 1.187913 on epoch=104
06/24/2022 18:54:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.740759 on epoch=109
06/24/2022 18:54:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.939870 on epoch=114
06/24/2022 18:54:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.707799 on epoch=119
06/24/2022 18:54:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.725273 on epoch=124
06/24/2022 18:54:15 - INFO - __main__ - Global step 250 Train loss 0.860323 ACC 0.5 on epoch=124
06/24/2022 18:54:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.972792 on epoch=129
06/24/2022 18:54:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.854484 on epoch=134
06/24/2022 18:54:22 - INFO - __main__ - Step 280 Global step 280 Train loss 1.128851 on epoch=139
06/24/2022 18:54:25 - INFO - __main__ - Step 290 Global step 290 Train loss 1.169111 on epoch=144
06/24/2022 18:54:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.998678 on epoch=149
06/24/2022 18:54:27 - INFO - __main__ - Global step 300 Train loss 1.024783 ACC 0.46875 on epoch=149
06/24/2022 18:54:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.831976 on epoch=154
06/24/2022 18:54:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.580855 on epoch=159
06/24/2022 18:54:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.649603 on epoch=164
06/24/2022 18:54:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.623985 on epoch=169
06/24/2022 18:54:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.678319 on epoch=174
06/24/2022 18:54:40 - INFO - __main__ - Global step 350 Train loss 0.672948 ACC 0.5 on epoch=174
06/24/2022 18:54:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.577933 on epoch=179
06/24/2022 18:54:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.778366 on epoch=184
06/24/2022 18:54:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.580592 on epoch=189
06/24/2022 18:54:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.641571 on epoch=194
06/24/2022 18:54:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.523416 on epoch=199
06/24/2022 18:54:52 - INFO - __main__ - Global step 400 Train loss 0.620376 ACC 0.5 on epoch=199
06/24/2022 18:54:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.426716 on epoch=204
06/24/2022 18:54:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.478717 on epoch=209
06/24/2022 18:55:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.474622 on epoch=214
06/24/2022 18:55:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.497303 on epoch=219
06/24/2022 18:55:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.358406 on epoch=224
06/24/2022 18:55:05 - INFO - __main__ - Global step 450 Train loss 0.447153 ACC 0.5 on epoch=224
06/24/2022 18:55:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.568171 on epoch=229
06/24/2022 18:55:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.644406 on epoch=234
06/24/2022 18:55:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.481678 on epoch=239
06/24/2022 18:55:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.512976 on epoch=244
06/24/2022 18:55:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.537400 on epoch=249
06/24/2022 18:55:17 - INFO - __main__ - Global step 500 Train loss 0.548926 ACC 0.46875 on epoch=249
06/24/2022 18:55:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.524198 on epoch=254
06/24/2022 18:55:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.470055 on epoch=259
06/24/2022 18:55:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.467881 on epoch=264
06/24/2022 18:55:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.463273 on epoch=269
06/24/2022 18:55:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.565627 on epoch=274
06/24/2022 18:55:30 - INFO - __main__ - Global step 550 Train loss 0.498207 ACC 0.5 on epoch=274
06/24/2022 18:55:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.450928 on epoch=279
06/24/2022 18:55:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.414077 on epoch=284
06/24/2022 18:55:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.438960 on epoch=289
06/24/2022 18:55:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.559992 on epoch=294
06/24/2022 18:55:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.491731 on epoch=299
06/24/2022 18:55:42 - INFO - __main__ - Global step 600 Train loss 0.471137 ACC 0.59375 on epoch=299
06/24/2022 18:55:43 - INFO - __main__ - save last model!
06/24/2022 18:55:46 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:55:46 - INFO - __main__ - Start tokenizing ... 610 instances
06/24/2022 18:55:46 - INFO - __main__ - Printing 3 examples
06/24/2022 18:55:46 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/24/2022 18:55:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:55:46 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/24/2022 18:55:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:55:46 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/24/2022 18:55:46 - INFO - __main__ - ['Dissimilar']
06/24/2022 18:55:46 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:55:46 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:55:47 - INFO - __main__ - Loaded 610 examples from test data
06/24/2022 18:55:52 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_87_0.0001_8_predictions.txt
06/24/2022 18:55:52 - INFO - __main__ - ACC on test data: 0.5246
06/24/2022 18:55:52 - INFO - __main__ - prefix=medical_questions_pairs_16_87, lr=0.0001, bsz=8, dev_performance=0.59375, test_performance=0.5245901639344263
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (91351): No such process
Task: paws, Checkpoint: None, Identifier: T5-base-ft-nopara2para
Output directory () already exists and is not empty.
06/24/2022 18:55:57 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/24/2022 18:55:57 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/24/2022 18:55:57 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-paws
06/24/2022 18:55:57 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-paws
06/24/2022 18:55:57 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/24/2022 18:55:57 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/24/2022 18:55:57 - INFO - __main__ - args.device: cuda:0
06/24/2022 18:55:57 - INFO - __main__ - args.device: cuda:1
06/24/2022 18:55:57 - INFO - __main__ - Using 2 gpus
06/24/2022 18:55:57 - INFO - __main__ - Using 2 gpus
06/24/2022 18:55:57 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/24/2022 18:55:57 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/24/2022 18:56:02 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.0005, bsz=8 ...
06/24/2022 18:56:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:56:03 - INFO - __main__ - Printing 3 examples
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 18:56:03 - INFO - __main__ - Printing 3 examples
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:56:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:56:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:56:03 - INFO - __main__ - Printing 3 examples
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 18:56:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 18:56:03 - INFO - __main__ - Printing 3 examples
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:56:03 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 18:56:03 - INFO - __main__ - ['1']
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:56:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:56:03 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:56:03 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:56:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:56:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:56:08 - INFO - __main__ - Starting training!
06/24/2022 18:56:08 - INFO - __main__ - Starting training!
06/24/2022 18:56:10 - INFO - __main__ - Step 10 Global step 10 Train loss 18.488073 on epoch=4
06/24/2022 18:56:13 - INFO - __main__ - Step 20 Global step 20 Train loss 12.240129 on epoch=9
06/24/2022 18:56:15 - INFO - __main__ - Step 30 Global step 30 Train loss 8.048197 on epoch=14
06/24/2022 18:56:17 - INFO - __main__ - Step 40 Global step 40 Train loss 4.204375 on epoch=19
06/24/2022 18:56:20 - INFO - __main__ - Step 50 Global step 50 Train loss 3.076987 on epoch=24
06/24/2022 18:56:20 - INFO - __main__ - Global step 50 Train loss 9.211553 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 18:56:23 - INFO - __main__ - Step 60 Global step 60 Train loss 2.424563 on epoch=29
06/24/2022 18:56:25 - INFO - __main__ - Step 70 Global step 70 Train loss 2.539328 on epoch=34
06/24/2022 18:56:28 - INFO - __main__ - Step 80 Global step 80 Train loss 2.181306 on epoch=39
06/24/2022 18:56:30 - INFO - __main__ - Step 90 Global step 90 Train loss 1.503955 on epoch=44
06/24/2022 18:56:33 - INFO - __main__ - Step 100 Global step 100 Train loss 1.443496 on epoch=49
06/24/2022 18:56:33 - INFO - __main__ - Global step 100 Train loss 2.018530 Classification-F1 0.5076923076923077 on epoch=49
06/24/2022 18:56:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.764060 on epoch=54
06/24/2022 18:56:39 - INFO - __main__ - Step 120 Global step 120 Train loss 1.175244 on epoch=59
06/24/2022 18:56:41 - INFO - __main__ - Step 130 Global step 130 Train loss 1.134496 on epoch=64
06/24/2022 18:56:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.967894 on epoch=69
06/24/2022 18:56:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.937112 on epoch=74
06/24/2022 18:56:46 - INFO - __main__ - Global step 150 Train loss 0.995761 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 18:56:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.683671 on epoch=79
06/24/2022 18:56:51 - INFO - __main__ - Step 170 Global step 170 Train loss 1.046668 on epoch=84
06/24/2022 18:56:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.694055 on epoch=89
06/24/2022 18:56:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.959592 on epoch=94
06/24/2022 18:56:58 - INFO - __main__ - Step 200 Global step 200 Train loss 1.015592 on epoch=99
06/24/2022 18:56:59 - INFO - __main__ - Global step 200 Train loss 0.879916 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 18:57:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.796328 on epoch=104
06/24/2022 18:57:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.649964 on epoch=109
06/24/2022 18:57:06 - INFO - __main__ - Step 230 Global step 230 Train loss 1.199510 on epoch=114
06/24/2022 18:57:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.743072 on epoch=119
06/24/2022 18:57:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.760412 on epoch=124
06/24/2022 18:57:11 - INFO - __main__ - Global step 250 Train loss 0.829857 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 18:57:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.787524 on epoch=129
06/24/2022 18:57:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.996781 on epoch=134
06/24/2022 18:57:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.816902 on epoch=139
06/24/2022 18:57:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.859983 on epoch=144
06/24/2022 18:57:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.997016 on epoch=149
06/24/2022 18:57:23 - INFO - __main__ - Global step 300 Train loss 0.891641 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 18:57:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.538004 on epoch=154
06/24/2022 18:57:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.887252 on epoch=159
06/24/2022 18:57:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.970309 on epoch=164
06/24/2022 18:57:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.807468 on epoch=169
06/24/2022 18:57:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.657008 on epoch=174
06/24/2022 18:57:36 - INFO - __main__ - Global step 350 Train loss 0.772008 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 18:57:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.715020 on epoch=179
06/24/2022 18:57:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.490913 on epoch=184
06/24/2022 18:57:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.634690 on epoch=189
06/24/2022 18:57:46 - INFO - __main__ - Step 390 Global step 390 Train loss 1.061828 on epoch=194
06/24/2022 18:57:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.756035 on epoch=199
06/24/2022 18:57:48 - INFO - __main__ - Global step 400 Train loss 0.731697 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 18:57:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.570059 on epoch=204
06/24/2022 18:57:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.738593 on epoch=209
06/24/2022 18:57:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.639659 on epoch=214
06/24/2022 18:57:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.758286 on epoch=219
06/24/2022 18:58:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.646089 on epoch=224
06/24/2022 18:58:01 - INFO - __main__ - Global step 450 Train loss 0.670537 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 18:58:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.730290 on epoch=229
06/24/2022 18:58:06 - INFO - __main__ - Step 470 Global step 470 Train loss 1.032561 on epoch=234
06/24/2022 18:58:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.770202 on epoch=239
06/24/2022 18:58:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.702856 on epoch=244
06/24/2022 18:58:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.788700 on epoch=249
06/24/2022 18:58:13 - INFO - __main__ - Global step 500 Train loss 0.804922 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 18:58:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.630741 on epoch=254
06/24/2022 18:58:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.750160 on epoch=259
06/24/2022 18:58:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.757372 on epoch=264
06/24/2022 18:58:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.498567 on epoch=269
06/24/2022 18:58:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.420639 on epoch=274
06/24/2022 18:58:26 - INFO - __main__ - Global step 550 Train loss 0.611496 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 18:58:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.623642 on epoch=279
06/24/2022 18:58:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.559853 on epoch=284
06/24/2022 18:58:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.708903 on epoch=289
06/24/2022 18:58:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.595385 on epoch=294
06/24/2022 18:58:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.508255 on epoch=299
06/24/2022 18:58:38 - INFO - __main__ - Global step 600 Train loss 0.599208 Classification-F1 0.3333333333333333 on epoch=299
06/24/2022 18:58:38 - INFO - __main__ - save last model!
06/24/2022 18:58:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:58:39 - INFO - __main__ - Printing 3 examples
06/24/2022 18:58:39 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 18:58:39 - INFO - __main__ - ['1']
06/24/2022 18:58:39 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 18:58:39 - INFO - __main__ - ['1']
06/24/2022 18:58:39 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 18:58:39 - INFO - __main__ - ['1']
06/24/2022 18:58:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 18:58:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:58:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:58:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:58:39 - INFO - __main__ - Printing 3 examples
06/24/2022 18:58:39 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 18:58:39 - INFO - __main__ - ['1']
06/24/2022 18:58:39 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 18:58:39 - INFO - __main__ - ['1']
06/24/2022 18:58:39 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 18:58:39 - INFO - __main__ - ['1']
06/24/2022 18:58:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:58:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:58:39 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:58:41 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 18:58:41 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 18:58:41 - INFO - __main__ - Printing 3 examples
06/24/2022 18:58:41 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 18:58:41 - INFO - __main__ - ['0']
06/24/2022 18:58:41 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 18:58:41 - INFO - __main__ - ['1']
06/24/2022 18:58:41 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 18:58:41 - INFO - __main__ - ['1']
06/24/2022 18:58:41 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:58:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:58:43 - INFO - __main__ - Starting training!
06/24/2022 18:58:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:58:53 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 18:59:51 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_100_0.0005_8_predictions.txt
06/24/2022 18:59:51 - INFO - __main__ - Classification-F1 on test data: 0.4623
06/24/2022 18:59:51 - INFO - __main__ - prefix=paws_16_100, lr=0.0005, bsz=8, dev_performance=0.5076923076923077, test_performance=0.46229017965944685
06/24/2022 18:59:51 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.0003, bsz=8 ...
06/24/2022 18:59:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:59:52 - INFO - __main__ - Printing 3 examples
06/24/2022 18:59:52 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 18:59:52 - INFO - __main__ - ['1']
06/24/2022 18:59:52 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 18:59:52 - INFO - __main__ - ['1']
06/24/2022 18:59:52 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 18:59:52 - INFO - __main__ - ['1']
06/24/2022 18:59:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 18:59:52 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:59:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 18:59:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 18:59:52 - INFO - __main__ - Printing 3 examples
06/24/2022 18:59:52 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 18:59:52 - INFO - __main__ - ['1']
06/24/2022 18:59:52 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 18:59:52 - INFO - __main__ - ['1']
06/24/2022 18:59:52 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 18:59:52 - INFO - __main__ - ['1']
06/24/2022 18:59:52 - INFO - __main__ - Tokenizing Input ...
06/24/2022 18:59:52 - INFO - __main__ - Tokenizing Output ...
06/24/2022 18:59:52 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 18:59:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 18:59:56 - INFO - __main__ - Starting training!
06/24/2022 18:59:58 - INFO - __main__ - Step 10 Global step 10 Train loss 19.564877 on epoch=4
06/24/2022 19:00:01 - INFO - __main__ - Step 20 Global step 20 Train loss 14.314105 on epoch=9
06/24/2022 19:00:03 - INFO - __main__ - Step 30 Global step 30 Train loss 9.880041 on epoch=14
06/24/2022 19:00:06 - INFO - __main__ - Step 40 Global step 40 Train loss 7.545162 on epoch=19
06/24/2022 19:00:08 - INFO - __main__ - Step 50 Global step 50 Train loss 5.229483 on epoch=24
06/24/2022 19:00:09 - INFO - __main__ - Global step 50 Train loss 11.306735 Classification-F1 0.2074074074074074 on epoch=24
06/24/2022 19:00:11 - INFO - __main__ - Step 60 Global step 60 Train loss 3.636333 on epoch=29
06/24/2022 19:00:14 - INFO - __main__ - Step 70 Global step 70 Train loss 4.564093 on epoch=34
06/24/2022 19:00:17 - INFO - __main__ - Step 80 Global step 80 Train loss 3.496155 on epoch=39
06/24/2022 19:00:19 - INFO - __main__ - Step 90 Global step 90 Train loss 2.810601 on epoch=44
06/24/2022 19:00:22 - INFO - __main__ - Step 100 Global step 100 Train loss 2.644682 on epoch=49
06/24/2022 19:00:22 - INFO - __main__ - Global step 100 Train loss 3.430373 Classification-F1 0.3992490613266583 on epoch=49
06/24/2022 19:00:25 - INFO - __main__ - Step 110 Global step 110 Train loss 2.479713 on epoch=54
06/24/2022 19:00:28 - INFO - __main__ - Step 120 Global step 120 Train loss 2.144805 on epoch=59
06/24/2022 19:00:30 - INFO - __main__ - Step 130 Global step 130 Train loss 1.581690 on epoch=64
06/24/2022 19:00:33 - INFO - __main__ - Step 140 Global step 140 Train loss 1.670161 on epoch=69
06/24/2022 19:00:36 - INFO - __main__ - Step 150 Global step 150 Train loss 1.034819 on epoch=74
06/24/2022 19:00:36 - INFO - __main__ - Global step 150 Train loss 1.782238 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:00:38 - INFO - __main__ - Step 160 Global step 160 Train loss 1.173926 on epoch=79
06/24/2022 19:00:41 - INFO - __main__ - Step 170 Global step 170 Train loss 1.475846 on epoch=84
06/24/2022 19:00:44 - INFO - __main__ - Step 180 Global step 180 Train loss 1.243093 on epoch=89
06/24/2022 19:00:46 - INFO - __main__ - Step 190 Global step 190 Train loss 1.359110 on epoch=94
06/24/2022 19:00:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.834385 on epoch=99
06/24/2022 19:00:49 - INFO - __main__ - Global step 200 Train loss 1.217272 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:00:52 - INFO - __main__ - Step 210 Global step 210 Train loss 1.498053 on epoch=104
06/24/2022 19:00:54 - INFO - __main__ - Step 220 Global step 220 Train loss 1.514706 on epoch=109
06/24/2022 19:00:57 - INFO - __main__ - Step 230 Global step 230 Train loss 1.306730 on epoch=114
06/24/2022 19:00:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.629577 on epoch=119
06/24/2022 19:01:02 - INFO - __main__ - Step 250 Global step 250 Train loss 1.185808 on epoch=124
06/24/2022 19:01:02 - INFO - __main__ - Global step 250 Train loss 1.226975 Classification-F1 0.46843853820598 on epoch=124
06/24/2022 19:01:05 - INFO - __main__ - Step 260 Global step 260 Train loss 1.022391 on epoch=129
06/24/2022 19:01:08 - INFO - __main__ - Step 270 Global step 270 Train loss 1.302691 on epoch=134
06/24/2022 19:01:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.673943 on epoch=139
06/24/2022 19:01:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.851326 on epoch=144
06/24/2022 19:01:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.947689 on epoch=149
06/24/2022 19:01:16 - INFO - __main__ - Global step 300 Train loss 0.959608 Classification-F1 0.3816425120772947 on epoch=149
06/24/2022 19:01:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.949186 on epoch=154
06/24/2022 19:01:21 - INFO - __main__ - Step 320 Global step 320 Train loss 1.163329 on epoch=159
06/24/2022 19:01:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.928209 on epoch=164
06/24/2022 19:01:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.790151 on epoch=169
06/24/2022 19:01:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.944816 on epoch=174
06/24/2022 19:01:29 - INFO - __main__ - Global step 350 Train loss 0.955138 Classification-F1 0.3454545454545454 on epoch=174
06/24/2022 19:01:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.681252 on epoch=179
06/24/2022 19:01:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.788602 on epoch=184
06/24/2022 19:01:36 - INFO - __main__ - Step 380 Global step 380 Train loss 1.006303 on epoch=189
06/24/2022 19:01:39 - INFO - __main__ - Step 390 Global step 390 Train loss 1.079082 on epoch=194
06/24/2022 19:01:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.660370 on epoch=199
06/24/2022 19:01:42 - INFO - __main__ - Global step 400 Train loss 0.843122 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 19:01:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.823107 on epoch=204
06/24/2022 19:01:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.771528 on epoch=209
06/24/2022 19:01:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.539242 on epoch=214
06/24/2022 19:01:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.940730 on epoch=219
06/24/2022 19:01:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.841753 on epoch=224
06/24/2022 19:01:55 - INFO - __main__ - Global step 450 Train loss 0.783272 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 19:01:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.842726 on epoch=229
06/24/2022 19:02:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.822570 on epoch=234
06/24/2022 19:02:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.749368 on epoch=239
06/24/2022 19:02:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.784871 on epoch=244
06/24/2022 19:02:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.622654 on epoch=249
06/24/2022 19:02:08 - INFO - __main__ - Global step 500 Train loss 0.764438 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 19:02:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.868877 on epoch=254
06/24/2022 19:02:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.701646 on epoch=259
06/24/2022 19:02:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.561440 on epoch=264
06/24/2022 19:02:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.653917 on epoch=269
06/24/2022 19:02:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.702932 on epoch=274
06/24/2022 19:02:21 - INFO - __main__ - Global step 550 Train loss 0.697763 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 19:02:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.634953 on epoch=279
06/24/2022 19:02:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.784902 on epoch=284
06/24/2022 19:02:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.486673 on epoch=289
06/24/2022 19:02:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.511099 on epoch=294
06/24/2022 19:02:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.787423 on epoch=299
06/24/2022 19:02:34 - INFO - __main__ - Global step 600 Train loss 0.641010 Classification-F1 0.4554554554554554 on epoch=299
06/24/2022 19:02:34 - INFO - __main__ - save last model!
06/24/2022 19:02:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:02:35 - INFO - __main__ - Printing 3 examples
06/24/2022 19:02:35 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 19:02:35 - INFO - __main__ - ['1']
06/24/2022 19:02:35 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 19:02:35 - INFO - __main__ - ['1']
06/24/2022 19:02:35 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 19:02:35 - INFO - __main__ - ['1']
06/24/2022 19:02:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:02:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:02:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:02:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:02:35 - INFO - __main__ - Printing 3 examples
06/24/2022 19:02:35 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 19:02:35 - INFO - __main__ - ['1']
06/24/2022 19:02:35 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 19:02:35 - INFO - __main__ - ['1']
06/24/2022 19:02:35 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 19:02:35 - INFO - __main__ - ['1']
06/24/2022 19:02:35 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:02:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:02:35 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:02:37 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:02:38 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:02:38 - INFO - __main__ - Printing 3 examples
06/24/2022 19:02:38 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:02:38 - INFO - __main__ - ['0']
06/24/2022 19:02:38 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:02:38 - INFO - __main__ - ['1']
06/24/2022 19:02:38 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:02:38 - INFO - __main__ - ['1']
06/24/2022 19:02:38 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:02:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:02:39 - INFO - __main__ - Starting training!
06/24/2022 19:02:42 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:02:49 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:03:49 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_100_0.0003_8_predictions.txt
06/24/2022 19:03:49 - INFO - __main__ - Classification-F1 on test data: 0.4501
06/24/2022 19:03:49 - INFO - __main__ - prefix=paws_16_100, lr=0.0003, bsz=8, dev_performance=0.46843853820598, test_performance=0.4500660469126676
06/24/2022 19:03:49 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.0002, bsz=8 ...
06/24/2022 19:03:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:03:50 - INFO - __main__ - Printing 3 examples
06/24/2022 19:03:50 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 19:03:50 - INFO - __main__ - ['1']
06/24/2022 19:03:50 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 19:03:50 - INFO - __main__ - ['1']
06/24/2022 19:03:50 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 19:03:50 - INFO - __main__ - ['1']
06/24/2022 19:03:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:03:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:03:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:03:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:03:50 - INFO - __main__ - Printing 3 examples
06/24/2022 19:03:50 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 19:03:50 - INFO - __main__ - ['1']
06/24/2022 19:03:50 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 19:03:50 - INFO - __main__ - ['1']
06/24/2022 19:03:50 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 19:03:50 - INFO - __main__ - ['1']
06/24/2022 19:03:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:03:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:03:50 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:03:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:03:54 - INFO - __main__ - Starting training!
06/24/2022 19:03:56 - INFO - __main__ - Step 10 Global step 10 Train loss 18.980835 on epoch=4
06/24/2022 19:03:58 - INFO - __main__ - Step 20 Global step 20 Train loss 15.102415 on epoch=9
06/24/2022 19:04:00 - INFO - __main__ - Step 30 Global step 30 Train loss 11.595715 on epoch=14
06/24/2022 19:04:03 - INFO - __main__ - Step 40 Global step 40 Train loss 8.434122 on epoch=19
06/24/2022 19:04:05 - INFO - __main__ - Step 50 Global step 50 Train loss 7.040746 on epoch=24
06/24/2022 19:04:06 - INFO - __main__ - Global step 50 Train loss 12.230765 Classification-F1 0.08440647711334948 on epoch=24
06/24/2022 19:04:09 - INFO - __main__ - Step 60 Global step 60 Train loss 5.368169 on epoch=29
06/24/2022 19:04:12 - INFO - __main__ - Step 70 Global step 70 Train loss 4.601288 on epoch=34
06/24/2022 19:04:14 - INFO - __main__ - Step 80 Global step 80 Train loss 3.595553 on epoch=39
06/24/2022 19:04:16 - INFO - __main__ - Step 90 Global step 90 Train loss 3.001110 on epoch=44
06/24/2022 19:04:19 - INFO - __main__ - Step 100 Global step 100 Train loss 2.110503 on epoch=49
06/24/2022 19:04:19 - INFO - __main__ - Global step 100 Train loss 3.735324 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:04:22 - INFO - __main__ - Step 110 Global step 110 Train loss 2.834332 on epoch=54
06/24/2022 19:04:25 - INFO - __main__ - Step 120 Global step 120 Train loss 1.803958 on epoch=59
06/24/2022 19:04:27 - INFO - __main__ - Step 130 Global step 130 Train loss 1.780046 on epoch=64
06/24/2022 19:04:29 - INFO - __main__ - Step 140 Global step 140 Train loss 1.349424 on epoch=69
06/24/2022 19:04:32 - INFO - __main__ - Step 150 Global step 150 Train loss 2.147232 on epoch=74
06/24/2022 19:04:32 - INFO - __main__ - Global step 150 Train loss 1.982998 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:04:35 - INFO - __main__ - Step 160 Global step 160 Train loss 2.074554 on epoch=79
06/24/2022 19:04:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.225931 on epoch=84
06/24/2022 19:04:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.903302 on epoch=89
06/24/2022 19:04:42 - INFO - __main__ - Step 190 Global step 190 Train loss 1.666656 on epoch=94
06/24/2022 19:04:44 - INFO - __main__ - Step 200 Global step 200 Train loss 1.225490 on epoch=99
06/24/2022 19:04:45 - INFO - __main__ - Global step 200 Train loss 1.419187 Classification-F1 0.4554554554554554 on epoch=99
06/24/2022 19:04:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.748977 on epoch=104
06/24/2022 19:04:50 - INFO - __main__ - Step 220 Global step 220 Train loss 1.080450 on epoch=109
06/24/2022 19:04:52 - INFO - __main__ - Step 230 Global step 230 Train loss 1.004850 on epoch=114
06/24/2022 19:04:55 - INFO - __main__ - Step 240 Global step 240 Train loss 1.097284 on epoch=119
06/24/2022 19:04:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.877564 on epoch=124
06/24/2022 19:04:58 - INFO - __main__ - Global step 250 Train loss 0.961825 Classification-F1 0.3454545454545454 on epoch=124
06/24/2022 19:05:00 - INFO - __main__ - Step 260 Global step 260 Train loss 1.373724 on epoch=129
06/24/2022 19:05:02 - INFO - __main__ - Step 270 Global step 270 Train loss 1.160308 on epoch=134
06/24/2022 19:05:05 - INFO - __main__ - Step 280 Global step 280 Train loss 1.026951 on epoch=139
06/24/2022 19:05:07 - INFO - __main__ - Step 290 Global step 290 Train loss 1.042904 on epoch=144
06/24/2022 19:05:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.964290 on epoch=149
06/24/2022 19:05:10 - INFO - __main__ - Global step 300 Train loss 1.113635 Classification-F1 0.28888888888888886 on epoch=149
06/24/2022 19:05:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.918783 on epoch=154
06/24/2022 19:05:15 - INFO - __main__ - Step 320 Global step 320 Train loss 1.008956 on epoch=159
06/24/2022 19:05:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.696177 on epoch=164
06/24/2022 19:05:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.753930 on epoch=169
06/24/2022 19:05:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.878734 on epoch=174
06/24/2022 19:05:22 - INFO - __main__ - Global step 350 Train loss 0.851316 Classification-F1 0.43529411764705883 on epoch=174
06/24/2022 19:05:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.959418 on epoch=179
06/24/2022 19:05:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.710231 on epoch=184
06/24/2022 19:05:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.425614 on epoch=189
06/24/2022 19:05:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.769840 on epoch=194
06/24/2022 19:05:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.474217 on epoch=199
06/24/2022 19:05:35 - INFO - __main__ - Global step 400 Train loss 0.667864 Classification-F1 0.4682306940371457 on epoch=199
06/24/2022 19:05:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.465213 on epoch=204
06/24/2022 19:05:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.476833 on epoch=209
06/24/2022 19:05:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.512757 on epoch=214
06/24/2022 19:05:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.363752 on epoch=219
06/24/2022 19:05:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.221288 on epoch=224
06/24/2022 19:05:48 - INFO - __main__ - Global step 450 Train loss 0.407969 Classification-F1 0.4420512820512821 on epoch=224
06/24/2022 19:05:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.270928 on epoch=229
06/24/2022 19:05:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.516473 on epoch=234
06/24/2022 19:05:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.394848 on epoch=239
06/24/2022 19:05:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.181207 on epoch=244
06/24/2022 19:06:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.319979 on epoch=249
06/24/2022 19:06:00 - INFO - __main__ - Global step 500 Train loss 0.336687 Classification-F1 0.4420512820512821 on epoch=249
06/24/2022 19:06:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.372477 on epoch=254
06/24/2022 19:06:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.086943 on epoch=259
06/24/2022 19:06:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.205025 on epoch=264
06/24/2022 19:06:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.152850 on epoch=269
06/24/2022 19:06:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.168420 on epoch=274
06/24/2022 19:06:13 - INFO - __main__ - Global step 550 Train loss 0.197143 Classification-F1 0.3816425120772947 on epoch=274
06/24/2022 19:06:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.181192 on epoch=279
06/24/2022 19:06:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.405202 on epoch=284
06/24/2022 19:06:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.045189 on epoch=289
06/24/2022 19:06:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.099061 on epoch=294
06/24/2022 19:06:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.112723 on epoch=299
06/24/2022 19:06:25 - INFO - __main__ - Global step 600 Train loss 0.168673 Classification-F1 0.41700404858299595 on epoch=299
06/24/2022 19:06:25 - INFO - __main__ - save last model!
06/24/2022 19:06:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:06:26 - INFO - __main__ - Printing 3 examples
06/24/2022 19:06:26 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 19:06:26 - INFO - __main__ - ['1']
06/24/2022 19:06:26 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 19:06:26 - INFO - __main__ - ['1']
06/24/2022 19:06:26 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 19:06:26 - INFO - __main__ - ['1']
06/24/2022 19:06:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:06:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:06:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:06:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:06:26 - INFO - __main__ - Printing 3 examples
06/24/2022 19:06:26 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 19:06:26 - INFO - __main__ - ['1']
06/24/2022 19:06:26 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 19:06:26 - INFO - __main__ - ['1']
06/24/2022 19:06:26 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 19:06:26 - INFO - __main__ - ['1']
06/24/2022 19:06:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:06:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:06:26 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:06:28 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:06:28 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:06:28 - INFO - __main__ - Printing 3 examples
06/24/2022 19:06:28 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:06:28 - INFO - __main__ - ['0']
06/24/2022 19:06:28 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:06:28 - INFO - __main__ - ['1']
06/24/2022 19:06:28 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:06:28 - INFO - __main__ - ['1']
06/24/2022 19:06:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:06:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:06:31 - INFO - __main__ - Starting training!
06/24/2022 19:06:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:06:40 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:07:39 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_100_0.0002_8_predictions.txt
06/24/2022 19:07:39 - INFO - __main__ - Classification-F1 on test data: 0.4975
06/24/2022 19:07:39 - INFO - __main__ - prefix=paws_16_100, lr=0.0002, bsz=8, dev_performance=0.4682306940371457, test_performance=0.4974788327502479
06/24/2022 19:07:39 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.0001, bsz=8 ...
06/24/2022 19:07:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:07:40 - INFO - __main__ - Printing 3 examples
06/24/2022 19:07:40 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 19:07:40 - INFO - __main__ - ['1']
06/24/2022 19:07:40 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 19:07:40 - INFO - __main__ - ['1']
06/24/2022 19:07:40 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 19:07:40 - INFO - __main__ - ['1']
06/24/2022 19:07:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:07:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:07:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:07:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:07:40 - INFO - __main__ - Printing 3 examples
06/24/2022 19:07:40 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 19:07:40 - INFO - __main__ - ['1']
06/24/2022 19:07:40 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 19:07:40 - INFO - __main__ - ['1']
06/24/2022 19:07:40 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 19:07:40 - INFO - __main__ - ['1']
06/24/2022 19:07:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:07:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:07:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:07:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:07:44 - INFO - __main__ - Starting training!
06/24/2022 19:07:46 - INFO - __main__ - Step 10 Global step 10 Train loss 18.157490 on epoch=4
06/24/2022 19:07:48 - INFO - __main__ - Step 20 Global step 20 Train loss 16.850668 on epoch=9
06/24/2022 19:07:51 - INFO - __main__ - Step 30 Global step 30 Train loss 13.300146 on epoch=14
06/24/2022 19:07:53 - INFO - __main__ - Step 40 Global step 40 Train loss 12.095674 on epoch=19
06/24/2022 19:07:56 - INFO - __main__ - Step 50 Global step 50 Train loss 10.740697 on epoch=24
06/24/2022 19:07:58 - INFO - __main__ - Global step 50 Train loss 14.228934 Classification-F1 0.006896551724137932 on epoch=24
06/24/2022 19:08:00 - INFO - __main__ - Step 60 Global step 60 Train loss 8.936110 on epoch=29
06/24/2022 19:08:03 - INFO - __main__ - Step 70 Global step 70 Train loss 7.996220 on epoch=34
06/24/2022 19:08:05 - INFO - __main__ - Step 80 Global step 80 Train loss 7.652712 on epoch=39
06/24/2022 19:08:08 - INFO - __main__ - Step 90 Global step 90 Train loss 8.114249 on epoch=44
06/24/2022 19:08:10 - INFO - __main__ - Step 100 Global step 100 Train loss 6.861159 on epoch=49
06/24/2022 19:08:11 - INFO - __main__ - Global step 100 Train loss 7.912091 Classification-F1 0.10077519379844961 on epoch=49
06/24/2022 19:08:14 - INFO - __main__ - Step 110 Global step 110 Train loss 6.685088 on epoch=54
06/24/2022 19:08:16 - INFO - __main__ - Step 120 Global step 120 Train loss 6.298478 on epoch=59
06/24/2022 19:08:19 - INFO - __main__ - Step 130 Global step 130 Train loss 5.211701 on epoch=64
06/24/2022 19:08:21 - INFO - __main__ - Step 140 Global step 140 Train loss 4.320987 on epoch=69
06/24/2022 19:08:23 - INFO - __main__ - Step 150 Global step 150 Train loss 4.430605 on epoch=74
06/24/2022 19:08:24 - INFO - __main__ - Global step 150 Train loss 5.389372 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:08:27 - INFO - __main__ - Step 160 Global step 160 Train loss 3.346350 on epoch=79
06/24/2022 19:08:29 - INFO - __main__ - Step 170 Global step 170 Train loss 3.140928 on epoch=84
06/24/2022 19:08:31 - INFO - __main__ - Step 180 Global step 180 Train loss 3.390441 on epoch=89
06/24/2022 19:08:34 - INFO - __main__ - Step 190 Global step 190 Train loss 3.170716 on epoch=94
06/24/2022 19:08:36 - INFO - __main__ - Step 200 Global step 200 Train loss 2.604563 on epoch=99
06/24/2022 19:08:37 - INFO - __main__ - Global step 200 Train loss 3.130600 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:08:39 - INFO - __main__ - Step 210 Global step 210 Train loss 2.664571 on epoch=104
06/24/2022 19:08:41 - INFO - __main__ - Step 220 Global step 220 Train loss 1.834392 on epoch=109
06/24/2022 19:08:44 - INFO - __main__ - Step 230 Global step 230 Train loss 2.289062 on epoch=114
06/24/2022 19:08:46 - INFO - __main__ - Step 240 Global step 240 Train loss 1.880731 on epoch=119
06/24/2022 19:08:49 - INFO - __main__ - Step 250 Global step 250 Train loss 1.504866 on epoch=124
06/24/2022 19:08:49 - INFO - __main__ - Global step 250 Train loss 2.034724 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:08:52 - INFO - __main__ - Step 260 Global step 260 Train loss 2.902151 on epoch=129
06/24/2022 19:08:54 - INFO - __main__ - Step 270 Global step 270 Train loss 2.743124 on epoch=134
06/24/2022 19:08:56 - INFO - __main__ - Step 280 Global step 280 Train loss 1.949452 on epoch=139
06/24/2022 19:08:59 - INFO - __main__ - Step 290 Global step 290 Train loss 2.160208 on epoch=144
06/24/2022 19:09:01 - INFO - __main__ - Step 300 Global step 300 Train loss 2.126773 on epoch=149
06/24/2022 19:09:02 - INFO - __main__ - Global step 300 Train loss 2.376342 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 19:09:04 - INFO - __main__ - Step 310 Global step 310 Train loss 2.177492 on epoch=154
06/24/2022 19:09:07 - INFO - __main__ - Step 320 Global step 320 Train loss 1.319015 on epoch=159
06/24/2022 19:09:09 - INFO - __main__ - Step 330 Global step 330 Train loss 1.321043 on epoch=164
06/24/2022 19:09:11 - INFO - __main__ - Step 340 Global step 340 Train loss 1.499443 on epoch=169
06/24/2022 19:09:14 - INFO - __main__ - Step 350 Global step 350 Train loss 1.378266 on epoch=174
06/24/2022 19:09:14 - INFO - __main__ - Global step 350 Train loss 1.539052 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 19:09:17 - INFO - __main__ - Step 360 Global step 360 Train loss 1.529958 on epoch=179
06/24/2022 19:09:19 - INFO - __main__ - Step 370 Global step 370 Train loss 1.710218 on epoch=184
06/24/2022 19:09:21 - INFO - __main__ - Step 380 Global step 380 Train loss 1.526869 on epoch=189
06/24/2022 19:09:24 - INFO - __main__ - Step 390 Global step 390 Train loss 1.277701 on epoch=194
06/24/2022 19:09:26 - INFO - __main__ - Step 400 Global step 400 Train loss 1.052454 on epoch=199
06/24/2022 19:09:27 - INFO - __main__ - Global step 400 Train loss 1.419440 Classification-F1 0.4458874458874459 on epoch=199
06/24/2022 19:09:30 - INFO - __main__ - Step 410 Global step 410 Train loss 1.237007 on epoch=204
06/24/2022 19:09:32 - INFO - __main__ - Step 420 Global step 420 Train loss 1.341453 on epoch=209
06/24/2022 19:09:34 - INFO - __main__ - Step 430 Global step 430 Train loss 1.557869 on epoch=214
06/24/2022 19:09:37 - INFO - __main__ - Step 440 Global step 440 Train loss 1.065002 on epoch=219
06/24/2022 19:09:39 - INFO - __main__ - Step 450 Global step 450 Train loss 1.395208 on epoch=224
06/24/2022 19:09:40 - INFO - __main__ - Global step 450 Train loss 1.319308 Classification-F1 0.4920634920634921 on epoch=224
06/24/2022 19:09:42 - INFO - __main__ - Step 460 Global step 460 Train loss 1.080009 on epoch=229
06/24/2022 19:09:45 - INFO - __main__ - Step 470 Global step 470 Train loss 1.163825 on epoch=234
06/24/2022 19:09:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.857817 on epoch=239
06/24/2022 19:09:50 - INFO - __main__ - Step 490 Global step 490 Train loss 1.176219 on epoch=244
06/24/2022 19:09:52 - INFO - __main__ - Step 500 Global step 500 Train loss 1.108552 on epoch=249
06/24/2022 19:09:53 - INFO - __main__ - Global step 500 Train loss 1.077284 Classification-F1 0.4231177094379639 on epoch=249
06/24/2022 19:09:55 - INFO - __main__ - Step 510 Global step 510 Train loss 1.002236 on epoch=254
06/24/2022 19:09:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.987150 on epoch=259
06/24/2022 19:10:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.931881 on epoch=264
06/24/2022 19:10:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.754783 on epoch=269
06/24/2022 19:10:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.560630 on epoch=274
06/24/2022 19:10:05 - INFO - __main__ - Global step 550 Train loss 0.847336 Classification-F1 0.4181818181818182 on epoch=274
06/24/2022 19:10:08 - INFO - __main__ - Step 560 Global step 560 Train loss 1.015641 on epoch=279
06/24/2022 19:10:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.739580 on epoch=284
06/24/2022 19:10:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.786541 on epoch=289
06/24/2022 19:10:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.991137 on epoch=294
06/24/2022 19:10:17 - INFO - __main__ - Step 600 Global step 600 Train loss 1.004584 on epoch=299
06/24/2022 19:10:18 - INFO - __main__ - Global step 600 Train loss 0.907497 Classification-F1 0.3191489361702127 on epoch=299
06/24/2022 19:10:18 - INFO - __main__ - save last model!
06/24/2022 19:10:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:10:18 - INFO - __main__ - Printing 3 examples
06/24/2022 19:10:18 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:10:18 - INFO - __main__ - ['1']
06/24/2022 19:10:18 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:10:18 - INFO - __main__ - ['1']
06/24/2022 19:10:18 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:10:18 - INFO - __main__ - ['1']
06/24/2022 19:10:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:10:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:10:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:10:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:10:18 - INFO - __main__ - Printing 3 examples
06/24/2022 19:10:18 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:10:18 - INFO - __main__ - ['1']
06/24/2022 19:10:18 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:10:18 - INFO - __main__ - ['1']
06/24/2022 19:10:18 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:10:18 - INFO - __main__ - ['1']
06/24/2022 19:10:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:10:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:10:19 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:10:20 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:10:20 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:10:20 - INFO - __main__ - Printing 3 examples
06/24/2022 19:10:20 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:10:20 - INFO - __main__ - ['0']
06/24/2022 19:10:20 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:10:20 - INFO - __main__ - ['1']
06/24/2022 19:10:20 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:10:20 - INFO - __main__ - ['1']
06/24/2022 19:10:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:10:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:10:22 - INFO - __main__ - Starting training!
06/24/2022 19:10:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:10:32 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:11:31 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_100_0.0001_8_predictions.txt
06/24/2022 19:11:31 - INFO - __main__ - Classification-F1 on test data: 0.5202
06/24/2022 19:11:31 - INFO - __main__ - prefix=paws_16_100, lr=0.0001, bsz=8, dev_performance=0.4920634920634921, test_performance=0.5201521714015664
06/24/2022 19:11:31 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.0005, bsz=8 ...
06/24/2022 19:11:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:11:32 - INFO - __main__ - Printing 3 examples
06/24/2022 19:11:32 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:11:32 - INFO - __main__ - ['1']
06/24/2022 19:11:32 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:11:32 - INFO - __main__ - ['1']
06/24/2022 19:11:32 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:11:32 - INFO - __main__ - ['1']
06/24/2022 19:11:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:11:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:11:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:11:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:11:32 - INFO - __main__ - Printing 3 examples
06/24/2022 19:11:32 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:11:32 - INFO - __main__ - ['1']
06/24/2022 19:11:32 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:11:32 - INFO - __main__ - ['1']
06/24/2022 19:11:32 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:11:32 - INFO - __main__ - ['1']
06/24/2022 19:11:32 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:11:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:11:32 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:11:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:11:35 - INFO - __main__ - Starting training!
06/24/2022 19:11:37 - INFO - __main__ - Step 10 Global step 10 Train loss 18.760315 on epoch=4
06/24/2022 19:11:40 - INFO - __main__ - Step 20 Global step 20 Train loss 12.115448 on epoch=9
06/24/2022 19:11:42 - INFO - __main__ - Step 30 Global step 30 Train loss 8.715551 on epoch=14
06/24/2022 19:11:44 - INFO - __main__ - Step 40 Global step 40 Train loss 4.820919 on epoch=19
06/24/2022 19:11:47 - INFO - __main__ - Step 50 Global step 50 Train loss 3.682077 on epoch=24
06/24/2022 19:11:47 - INFO - __main__ - Global step 50 Train loss 9.618863 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 19:11:50 - INFO - __main__ - Step 60 Global step 60 Train loss 3.287867 on epoch=29
06/24/2022 19:11:52 - INFO - __main__ - Step 70 Global step 70 Train loss 2.385054 on epoch=34
06/24/2022 19:11:55 - INFO - __main__ - Step 80 Global step 80 Train loss 2.082311 on epoch=39
06/24/2022 19:11:57 - INFO - __main__ - Step 90 Global step 90 Train loss 1.386418 on epoch=44
06/24/2022 19:11:59 - INFO - __main__ - Step 100 Global step 100 Train loss 1.347488 on epoch=49
06/24/2022 19:12:00 - INFO - __main__ - Global step 100 Train loss 2.097827 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:12:02 - INFO - __main__ - Step 110 Global step 110 Train loss 1.132566 on epoch=54
06/24/2022 19:12:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.908656 on epoch=59
06/24/2022 19:12:07 - INFO - __main__ - Step 130 Global step 130 Train loss 1.371471 on epoch=64
06/24/2022 19:12:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.816266 on epoch=69
06/24/2022 19:12:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.858946 on epoch=74
06/24/2022 19:12:12 - INFO - __main__ - Global step 150 Train loss 1.017581 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:12:15 - INFO - __main__ - Step 160 Global step 160 Train loss 1.042486 on epoch=79
06/24/2022 19:12:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.715730 on epoch=84
06/24/2022 19:12:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.849375 on epoch=89
06/24/2022 19:12:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.751550 on epoch=94
06/24/2022 19:12:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.938317 on epoch=99
06/24/2022 19:12:25 - INFO - __main__ - Global step 200 Train loss 0.859492 Classification-F1 0.5134502923976608 on epoch=99
06/24/2022 19:12:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.783956 on epoch=104
06/24/2022 19:12:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.505614 on epoch=109
06/24/2022 19:12:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.511568 on epoch=114
06/24/2022 19:12:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.578023 on epoch=119
06/24/2022 19:12:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.433735 on epoch=124
06/24/2022 19:12:37 - INFO - __main__ - Global step 250 Train loss 0.562579 Classification-F1 0.3992490613266583 on epoch=124
06/24/2022 19:12:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.454726 on epoch=129
06/24/2022 19:12:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.456554 on epoch=134
06/24/2022 19:12:45 - INFO - __main__ - Step 280 Global step 280 Train loss 5.177397 on epoch=139
06/24/2022 19:12:47 - INFO - __main__ - Step 290 Global step 290 Train loss 1.995462 on epoch=144
06/24/2022 19:12:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.148491 on epoch=149
06/24/2022 19:12:50 - INFO - __main__ - Global step 300 Train loss 1.846526 Classification-F1 0.6875 on epoch=149
06/24/2022 19:12:53 - INFO - __main__ - Step 310 Global step 310 Train loss 1.237569 on epoch=154
06/24/2022 19:12:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.706731 on epoch=159
06/24/2022 19:12:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.806047 on epoch=164
06/24/2022 19:13:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.744628 on epoch=169
06/24/2022 19:13:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.654146 on epoch=174
06/24/2022 19:13:03 - INFO - __main__ - Global step 350 Train loss 0.829824 Classification-F1 0.25581395348837205 on epoch=174
06/24/2022 19:13:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.648310 on epoch=179
06/24/2022 19:13:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.711762 on epoch=184
06/24/2022 19:13:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.625237 on epoch=189
06/24/2022 19:13:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.565655 on epoch=194
06/24/2022 19:13:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.528193 on epoch=199
06/24/2022 19:13:15 - INFO - __main__ - Global step 400 Train loss 0.615831 Classification-F1 0.5555555555555556 on epoch=199
06/24/2022 19:13:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.634955 on epoch=204
06/24/2022 19:13:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.531699 on epoch=209
06/24/2022 19:13:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.697413 on epoch=214
06/24/2022 19:13:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.459813 on epoch=219
06/24/2022 19:13:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.495675 on epoch=224
06/24/2022 19:13:27 - INFO - __main__ - Global step 450 Train loss 0.563911 Classification-F1 0.5625 on epoch=224
06/24/2022 19:13:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.488927 on epoch=229
06/24/2022 19:13:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.656352 on epoch=234
06/24/2022 19:13:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.428672 on epoch=239
06/24/2022 19:13:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.337406 on epoch=244
06/24/2022 19:13:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.413742 on epoch=249
06/24/2022 19:13:40 - INFO - __main__ - Global step 500 Train loss 0.465020 Classification-F1 0.46843853820598 on epoch=249
06/24/2022 19:13:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.320089 on epoch=254
06/24/2022 19:13:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.451703 on epoch=259
06/24/2022 19:13:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.495477 on epoch=264
06/24/2022 19:13:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.249954 on epoch=269
06/24/2022 19:13:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.245073 on epoch=274
06/24/2022 19:13:52 - INFO - __main__ - Global step 550 Train loss 0.352459 Classification-F1 0.6113360323886641 on epoch=274
06/24/2022 19:13:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.292085 on epoch=279
06/24/2022 19:13:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.291854 on epoch=284
06/24/2022 19:13:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.232937 on epoch=289
06/24/2022 19:14:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.233480 on epoch=294
06/24/2022 19:14:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.243884 on epoch=299
06/24/2022 19:14:05 - INFO - __main__ - Global step 600 Train loss 0.258848 Classification-F1 0.5835835835835835 on epoch=299
06/24/2022 19:14:05 - INFO - __main__ - save last model!
06/24/2022 19:14:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:14:05 - INFO - __main__ - Printing 3 examples
06/24/2022 19:14:05 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:14:05 - INFO - __main__ - ['1']
06/24/2022 19:14:05 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:14:05 - INFO - __main__ - ['1']
06/24/2022 19:14:05 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:14:05 - INFO - __main__ - ['1']
06/24/2022 19:14:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:14:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:14:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:14:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:14:05 - INFO - __main__ - Printing 3 examples
06/24/2022 19:14:05 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:14:05 - INFO - __main__ - ['1']
06/24/2022 19:14:05 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:14:05 - INFO - __main__ - ['1']
06/24/2022 19:14:05 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:14:05 - INFO - __main__ - ['1']
06/24/2022 19:14:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:14:06 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:14:06 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:14:07 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:14:07 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:14:07 - INFO - __main__ - Printing 3 examples
06/24/2022 19:14:07 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:14:07 - INFO - __main__ - ['0']
06/24/2022 19:14:07 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:14:07 - INFO - __main__ - ['1']
06/24/2022 19:14:07 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:14:07 - INFO - __main__ - ['1']
06/24/2022 19:14:07 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:14:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:14:09 - INFO - __main__ - Starting training!
06/24/2022 19:14:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:14:19 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:15:19 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_13_0.0005_8_predictions.txt
06/24/2022 19:15:19 - INFO - __main__ - Classification-F1 on test data: 0.0903
06/24/2022 19:15:19 - INFO - __main__ - prefix=paws_16_13, lr=0.0005, bsz=8, dev_performance=0.6875, test_performance=0.09033191360243632
06/24/2022 19:15:19 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.0003, bsz=8 ...
06/24/2022 19:15:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:15:20 - INFO - __main__ - Printing 3 examples
06/24/2022 19:15:20 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:15:20 - INFO - __main__ - ['1']
06/24/2022 19:15:20 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:15:20 - INFO - __main__ - ['1']
06/24/2022 19:15:20 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:15:20 - INFO - __main__ - ['1']
06/24/2022 19:15:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:15:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:15:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:15:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:15:20 - INFO - __main__ - Printing 3 examples
06/24/2022 19:15:20 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:15:20 - INFO - __main__ - ['1']
06/24/2022 19:15:20 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:15:20 - INFO - __main__ - ['1']
06/24/2022 19:15:20 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:15:20 - INFO - __main__ - ['1']
06/24/2022 19:15:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:15:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:15:20 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:15:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:15:24 - INFO - __main__ - Starting training!
06/24/2022 19:15:26 - INFO - __main__ - Step 10 Global step 10 Train loss 18.989279 on epoch=4
06/24/2022 19:15:29 - INFO - __main__ - Step 20 Global step 20 Train loss 15.593657 on epoch=9
06/24/2022 19:15:31 - INFO - __main__ - Step 30 Global step 30 Train loss 9.811989 on epoch=14
06/24/2022 19:15:34 - INFO - __main__ - Step 40 Global step 40 Train loss 5.878953 on epoch=19
06/24/2022 19:15:36 - INFO - __main__ - Step 50 Global step 50 Train loss 5.563643 on epoch=24
06/24/2022 19:15:36 - INFO - __main__ - Global step 50 Train loss 11.167505 Classification-F1 0.21658986175115205 on epoch=24
06/24/2022 19:15:39 - INFO - __main__ - Step 60 Global step 60 Train loss 4.619464 on epoch=29
06/24/2022 19:15:42 - INFO - __main__ - Step 70 Global step 70 Train loss 3.961968 on epoch=34
06/24/2022 19:15:44 - INFO - __main__ - Step 80 Global step 80 Train loss 2.474783 on epoch=39
06/24/2022 19:15:47 - INFO - __main__ - Step 90 Global step 90 Train loss 2.873657 on epoch=44
06/24/2022 19:15:49 - INFO - __main__ - Step 100 Global step 100 Train loss 1.715405 on epoch=49
06/24/2022 19:15:49 - INFO - __main__ - Global step 100 Train loss 3.129055 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:15:52 - INFO - __main__ - Step 110 Global step 110 Train loss 2.100524 on epoch=54
06/24/2022 19:15:55 - INFO - __main__ - Step 120 Global step 120 Train loss 1.505673 on epoch=59
06/24/2022 19:15:57 - INFO - __main__ - Step 130 Global step 130 Train loss 1.843402 on epoch=64
06/24/2022 19:16:00 - INFO - __main__ - Step 140 Global step 140 Train loss 1.573747 on epoch=69
06/24/2022 19:16:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.983626 on epoch=74
06/24/2022 19:16:02 - INFO - __main__ - Global step 150 Train loss 1.601395 Classification-F1 0.3073593073593074 on epoch=74
06/24/2022 19:16:05 - INFO - __main__ - Step 160 Global step 160 Train loss 1.368447 on epoch=79
06/24/2022 19:16:07 - INFO - __main__ - Step 170 Global step 170 Train loss 1.396451 on epoch=84
06/24/2022 19:16:10 - INFO - __main__ - Step 180 Global step 180 Train loss 1.220609 on epoch=89
06/24/2022 19:16:12 - INFO - __main__ - Step 190 Global step 190 Train loss 1.258261 on epoch=94
06/24/2022 19:16:15 - INFO - __main__ - Step 200 Global step 200 Train loss 1.128282 on epoch=99
06/24/2022 19:16:15 - INFO - __main__ - Global step 200 Train loss 1.274410 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:16:18 - INFO - __main__ - Step 210 Global step 210 Train loss 1.009148 on epoch=104
06/24/2022 19:16:20 - INFO - __main__ - Step 220 Global step 220 Train loss 1.113517 on epoch=109
06/24/2022 19:16:23 - INFO - __main__ - Step 230 Global step 230 Train loss 1.051020 on epoch=114
06/24/2022 19:16:25 - INFO - __main__ - Step 240 Global step 240 Train loss 1.005140 on epoch=119
06/24/2022 19:16:28 - INFO - __main__ - Step 250 Global step 250 Train loss 1.160240 on epoch=124
06/24/2022 19:16:28 - INFO - __main__ - Global step 250 Train loss 1.067813 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:16:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.363951 on epoch=129
06/24/2022 19:16:33 - INFO - __main__ - Step 270 Global step 270 Train loss 1.121725 on epoch=134
06/24/2022 19:16:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.734535 on epoch=139
06/24/2022 19:16:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.782341 on epoch=144
06/24/2022 19:16:40 - INFO - __main__ - Step 300 Global step 300 Train loss 1.258429 on epoch=149
06/24/2022 19:16:41 - INFO - __main__ - Global step 300 Train loss 1.052196 Classification-F1 0.4181818181818182 on epoch=149
06/24/2022 19:16:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.840894 on epoch=154
06/24/2022 19:16:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.832283 on epoch=159
06/24/2022 19:16:49 - INFO - __main__ - Step 330 Global step 330 Train loss 1.145375 on epoch=164
06/24/2022 19:16:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.825061 on epoch=169
06/24/2022 19:16:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.797064 on epoch=174
06/24/2022 19:16:54 - INFO - __main__ - Global step 350 Train loss 0.888136 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 19:16:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.802375 on epoch=179
06/24/2022 19:16:59 - INFO - __main__ - Step 370 Global step 370 Train loss 1.160099 on epoch=184
06/24/2022 19:17:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.816765 on epoch=189
06/24/2022 19:17:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.739395 on epoch=194
06/24/2022 19:17:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.724374 on epoch=199
06/24/2022 19:17:07 - INFO - __main__ - Global step 400 Train loss 0.848602 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 19:17:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.369251 on epoch=204
06/24/2022 19:17:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.983281 on epoch=209
06/24/2022 19:17:14 - INFO - __main__ - Step 430 Global step 430 Train loss 1.000612 on epoch=214
06/24/2022 19:17:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.931226 on epoch=219
06/24/2022 19:17:19 - INFO - __main__ - Step 450 Global step 450 Train loss 1.133998 on epoch=224
06/24/2022 19:17:20 - INFO - __main__ - Global step 450 Train loss 0.883674 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 19:17:22 - INFO - __main__ - Step 460 Global step 460 Train loss 1.066952 on epoch=229
06/24/2022 19:17:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.859396 on epoch=234
06/24/2022 19:17:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.626426 on epoch=239
06/24/2022 19:17:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.733350 on epoch=244
06/24/2022 19:17:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.740199 on epoch=249
06/24/2022 19:17:32 - INFO - __main__ - Global step 500 Train loss 0.805265 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 19:17:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.624112 on epoch=254
06/24/2022 19:17:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.957918 on epoch=259
06/24/2022 19:17:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.544566 on epoch=264
06/24/2022 19:17:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.698383 on epoch=269
06/24/2022 19:17:45 - INFO - __main__ - Step 550 Global step 550 Train loss 1.025905 on epoch=274
06/24/2022 19:17:45 - INFO - __main__ - Global step 550 Train loss 0.770177 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 19:17:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.676705 on epoch=279
06/24/2022 19:17:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.844380 on epoch=284
06/24/2022 19:17:53 - INFO - __main__ - Step 580 Global step 580 Train loss 1.167803 on epoch=289
06/24/2022 19:17:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.611662 on epoch=294
06/24/2022 19:17:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.862417 on epoch=299
06/24/2022 19:17:58 - INFO - __main__ - Global step 600 Train loss 0.832593 Classification-F1 0.3333333333333333 on epoch=299
06/24/2022 19:17:58 - INFO - __main__ - save last model!
06/24/2022 19:17:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:17:59 - INFO - __main__ - Printing 3 examples
06/24/2022 19:17:59 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:17:59 - INFO - __main__ - ['1']
06/24/2022 19:17:59 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:17:59 - INFO - __main__ - ['1']
06/24/2022 19:17:59 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:17:59 - INFO - __main__ - ['1']
06/24/2022 19:17:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:17:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:17:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:17:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:17:59 - INFO - __main__ - Printing 3 examples
06/24/2022 19:17:59 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:17:59 - INFO - __main__ - ['1']
06/24/2022 19:17:59 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:17:59 - INFO - __main__ - ['1']
06/24/2022 19:17:59 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:17:59 - INFO - __main__ - ['1']
06/24/2022 19:17:59 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:17:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:17:59 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:18:00 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:18:01 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:18:01 - INFO - __main__ - Printing 3 examples
06/24/2022 19:18:01 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:18:01 - INFO - __main__ - ['0']
06/24/2022 19:18:01 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:18:01 - INFO - __main__ - ['1']
06/24/2022 19:18:01 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:18:01 - INFO - __main__ - ['1']
06/24/2022 19:18:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:18:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:18:03 - INFO - __main__ - Starting training!
06/24/2022 19:18:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:18:13 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:19:11 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_13_0.0003_8_predictions.txt
06/24/2022 19:19:11 - INFO - __main__ - Classification-F1 on test data: 0.4041
06/24/2022 19:19:11 - INFO - __main__ - prefix=paws_16_13, lr=0.0003, bsz=8, dev_performance=0.4181818181818182, test_performance=0.4040976231912496
06/24/2022 19:19:11 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.0002, bsz=8 ...
06/24/2022 19:19:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:19:12 - INFO - __main__ - Printing 3 examples
06/24/2022 19:19:12 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:19:12 - INFO - __main__ - ['1']
06/24/2022 19:19:12 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:19:12 - INFO - __main__ - ['1']
06/24/2022 19:19:12 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:19:12 - INFO - __main__ - ['1']
06/24/2022 19:19:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:19:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:19:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:19:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:19:12 - INFO - __main__ - Printing 3 examples
06/24/2022 19:19:12 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:19:12 - INFO - __main__ - ['1']
06/24/2022 19:19:12 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:19:12 - INFO - __main__ - ['1']
06/24/2022 19:19:12 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:19:12 - INFO - __main__ - ['1']
06/24/2022 19:19:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:19:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:19:13 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:19:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:19:17 - INFO - __main__ - Starting training!
06/24/2022 19:19:19 - INFO - __main__ - Step 10 Global step 10 Train loss 18.273840 on epoch=4
06/24/2022 19:19:21 - INFO - __main__ - Step 20 Global step 20 Train loss 16.040924 on epoch=9
06/24/2022 19:19:24 - INFO - __main__ - Step 30 Global step 30 Train loss 12.143332 on epoch=14
06/24/2022 19:19:26 - INFO - __main__ - Step 40 Global step 40 Train loss 9.210495 on epoch=19
06/24/2022 19:19:29 - INFO - __main__ - Step 50 Global step 50 Train loss 5.968013 on epoch=24
06/24/2022 19:19:29 - INFO - __main__ - Global step 50 Train loss 12.327322 Classification-F1 0.16880341880341881 on epoch=24
06/24/2022 19:19:32 - INFO - __main__ - Step 60 Global step 60 Train loss 4.674319 on epoch=29
06/24/2022 19:19:34 - INFO - __main__ - Step 70 Global step 70 Train loss 3.472973 on epoch=34
06/24/2022 19:19:37 - INFO - __main__ - Step 80 Global step 80 Train loss 3.442551 on epoch=39
06/24/2022 19:19:39 - INFO - __main__ - Step 90 Global step 90 Train loss 3.214423 on epoch=44
06/24/2022 19:19:42 - INFO - __main__ - Step 100 Global step 100 Train loss 2.372362 on epoch=49
06/24/2022 19:19:42 - INFO - __main__ - Global step 100 Train loss 3.435326 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:19:45 - INFO - __main__ - Step 110 Global step 110 Train loss 2.117404 on epoch=54
06/24/2022 19:19:48 - INFO - __main__ - Step 120 Global step 120 Train loss 2.197869 on epoch=59
06/24/2022 19:19:50 - INFO - __main__ - Step 130 Global step 130 Train loss 1.757640 on epoch=64
06/24/2022 19:19:53 - INFO - __main__ - Step 140 Global step 140 Train loss 1.856669 on epoch=69
06/24/2022 19:19:55 - INFO - __main__ - Step 150 Global step 150 Train loss 1.692746 on epoch=74
06/24/2022 19:19:56 - INFO - __main__ - Global step 150 Train loss 1.924466 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:19:58 - INFO - __main__ - Step 160 Global step 160 Train loss 1.381038 on epoch=79
06/24/2022 19:20:01 - INFO - __main__ - Step 170 Global step 170 Train loss 1.336419 on epoch=84
06/24/2022 19:20:03 - INFO - __main__ - Step 180 Global step 180 Train loss 1.370529 on epoch=89
06/24/2022 19:20:06 - INFO - __main__ - Step 190 Global step 190 Train loss 1.232659 on epoch=94
06/24/2022 19:20:09 - INFO - __main__ - Step 200 Global step 200 Train loss 1.112050 on epoch=99
06/24/2022 19:20:09 - INFO - __main__ - Global step 200 Train loss 1.286539 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:20:11 - INFO - __main__ - Step 210 Global step 210 Train loss 1.131809 on epoch=104
06/24/2022 19:20:14 - INFO - __main__ - Step 220 Global step 220 Train loss 1.727566 on epoch=109
06/24/2022 19:20:16 - INFO - __main__ - Step 230 Global step 230 Train loss 1.188752 on epoch=114
06/24/2022 19:20:19 - INFO - __main__ - Step 240 Global step 240 Train loss 1.006001 on epoch=119
06/24/2022 19:20:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.803380 on epoch=124
06/24/2022 19:20:22 - INFO - __main__ - Global step 250 Train loss 1.171502 Classification-F1 0.5307917888563051 on epoch=124
06/24/2022 19:20:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.879805 on epoch=129
06/24/2022 19:20:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.663818 on epoch=134
06/24/2022 19:20:30 - INFO - __main__ - Step 280 Global step 280 Train loss 1.069009 on epoch=139
06/24/2022 19:20:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.868480 on epoch=144
06/24/2022 19:20:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.842965 on epoch=149
06/24/2022 19:20:35 - INFO - __main__ - Global step 300 Train loss 0.864815 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 19:20:38 - INFO - __main__ - Step 310 Global step 310 Train loss 1.047143 on epoch=154
06/24/2022 19:20:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.928325 on epoch=159
06/24/2022 19:20:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.836877 on epoch=164
06/24/2022 19:20:45 - INFO - __main__ - Step 340 Global step 340 Train loss 1.186984 on epoch=169
06/24/2022 19:20:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.831804 on epoch=174
06/24/2022 19:20:48 - INFO - __main__ - Global step 350 Train loss 0.966227 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 19:20:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.758772 on epoch=179
06/24/2022 19:20:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.774523 on epoch=184
06/24/2022 19:20:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.843773 on epoch=189
06/24/2022 19:20:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.635545 on epoch=194
06/24/2022 19:21:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.886021 on epoch=199
06/24/2022 19:21:01 - INFO - __main__ - Global step 400 Train loss 0.779727 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 19:21:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.688743 on epoch=204
06/24/2022 19:21:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.820245 on epoch=209
06/24/2022 19:21:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.480946 on epoch=214
06/24/2022 19:21:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.878393 on epoch=219
06/24/2022 19:21:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.677200 on epoch=224
06/24/2022 19:21:14 - INFO - __main__ - Global step 450 Train loss 0.709106 Classification-F1 0.2727272727272727 on epoch=224
06/24/2022 19:21:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.716182 on epoch=229
06/24/2022 19:21:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.719746 on epoch=234
06/24/2022 19:21:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.896613 on epoch=239
06/24/2022 19:21:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.609989 on epoch=244
06/24/2022 19:21:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.673100 on epoch=249
06/24/2022 19:21:27 - INFO - __main__ - Global step 500 Train loss 0.723126 Classification-F1 0.5195195195195195 on epoch=249
06/24/2022 19:21:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.573552 on epoch=254
06/24/2022 19:21:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.838834 on epoch=259
06/24/2022 19:21:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.489635 on epoch=264
06/24/2022 19:21:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.258673 on epoch=269
06/24/2022 19:21:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.441423 on epoch=274
06/24/2022 19:21:40 - INFO - __main__ - Global step 550 Train loss 0.520423 Classification-F1 0.5933528836754642 on epoch=274
06/24/2022 19:21:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.466849 on epoch=279
06/24/2022 19:21:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.410597 on epoch=284
06/24/2022 19:21:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.665573 on epoch=289
06/24/2022 19:21:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.277529 on epoch=294
06/24/2022 19:21:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.312874 on epoch=299
06/24/2022 19:21:54 - INFO - __main__ - Global step 600 Train loss 0.426684 Classification-F1 0.46843853820598 on epoch=299
06/24/2022 19:21:54 - INFO - __main__ - save last model!
06/24/2022 19:21:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:21:54 - INFO - __main__ - Printing 3 examples
06/24/2022 19:21:54 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:21:54 - INFO - __main__ - ['1']
06/24/2022 19:21:54 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:21:54 - INFO - __main__ - ['1']
06/24/2022 19:21:54 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:21:54 - INFO - __main__ - ['1']
06/24/2022 19:21:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:21:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:21:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:21:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:21:54 - INFO - __main__ - Printing 3 examples
06/24/2022 19:21:54 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:21:54 - INFO - __main__ - ['1']
06/24/2022 19:21:54 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:21:54 - INFO - __main__ - ['1']
06/24/2022 19:21:54 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:21:54 - INFO - __main__ - ['1']
06/24/2022 19:21:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:21:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:21:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:21:56 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:21:56 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:21:56 - INFO - __main__ - Printing 3 examples
06/24/2022 19:21:56 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:21:56 - INFO - __main__ - ['0']
06/24/2022 19:21:56 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:21:56 - INFO - __main__ - ['1']
06/24/2022 19:21:56 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:21:56 - INFO - __main__ - ['1']
06/24/2022 19:21:56 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:21:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:21:58 - INFO - __main__ - Starting training!
06/24/2022 19:22:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:22:08 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:23:07 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_13_0.0002_8_predictions.txt
06/24/2022 19:23:07 - INFO - __main__ - Classification-F1 on test data: 0.4806
06/24/2022 19:23:07 - INFO - __main__ - prefix=paws_16_13, lr=0.0002, bsz=8, dev_performance=0.5933528836754642, test_performance=0.48062281959358544
06/24/2022 19:23:07 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.0001, bsz=8 ...
06/24/2022 19:23:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:23:08 - INFO - __main__ - Printing 3 examples
06/24/2022 19:23:08 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 19:23:08 - INFO - __main__ - ['1']
06/24/2022 19:23:08 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 19:23:08 - INFO - __main__ - ['1']
06/24/2022 19:23:08 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 19:23:08 - INFO - __main__ - ['1']
06/24/2022 19:23:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:23:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:23:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:23:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:23:08 - INFO - __main__ - Printing 3 examples
06/24/2022 19:23:08 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 19:23:08 - INFO - __main__ - ['1']
06/24/2022 19:23:08 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 19:23:08 - INFO - __main__ - ['1']
06/24/2022 19:23:08 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 19:23:08 - INFO - __main__ - ['1']
06/24/2022 19:23:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:23:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:23:08 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:23:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:23:11 - INFO - __main__ - Starting training!
06/24/2022 19:23:13 - INFO - __main__ - Step 10 Global step 10 Train loss 18.636131 on epoch=4
06/24/2022 19:23:16 - INFO - __main__ - Step 20 Global step 20 Train loss 16.268793 on epoch=9
06/24/2022 19:23:18 - INFO - __main__ - Step 30 Global step 30 Train loss 12.236886 on epoch=14
06/24/2022 19:23:20 - INFO - __main__ - Step 40 Global step 40 Train loss 11.255623 on epoch=19
06/24/2022 19:23:23 - INFO - __main__ - Step 50 Global step 50 Train loss 8.993055 on epoch=24
06/24/2022 19:23:25 - INFO - __main__ - Global step 50 Train loss 13.478098 Classification-F1 0.006791171477079796 on epoch=24
06/24/2022 19:23:28 - INFO - __main__ - Step 60 Global step 60 Train loss 9.676052 on epoch=29
06/24/2022 19:23:30 - INFO - __main__ - Step 70 Global step 70 Train loss 7.999013 on epoch=34
06/24/2022 19:23:32 - INFO - __main__ - Step 80 Global step 80 Train loss 6.637070 on epoch=39
06/24/2022 19:23:35 - INFO - __main__ - Step 90 Global step 90 Train loss 5.825637 on epoch=44
06/24/2022 19:23:37 - INFO - __main__ - Step 100 Global step 100 Train loss 5.261481 on epoch=49
06/24/2022 19:23:37 - INFO - __main__ - Global step 100 Train loss 7.079851 Classification-F1 0.22695035460992907 on epoch=49
06/24/2022 19:23:41 - INFO - __main__ - Step 110 Global step 110 Train loss 5.420087 on epoch=54
06/24/2022 19:23:43 - INFO - __main__ - Step 120 Global step 120 Train loss 3.888200 on epoch=59
06/24/2022 19:23:45 - INFO - __main__ - Step 130 Global step 130 Train loss 4.414083 on epoch=64
06/24/2022 19:23:48 - INFO - __main__ - Step 140 Global step 140 Train loss 3.250174 on epoch=69
06/24/2022 19:23:50 - INFO - __main__ - Step 150 Global step 150 Train loss 3.179623 on epoch=74
06/24/2022 19:23:51 - INFO - __main__ - Global step 150 Train loss 4.030434 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:23:53 - INFO - __main__ - Step 160 Global step 160 Train loss 3.728525 on epoch=79
06/24/2022 19:23:56 - INFO - __main__ - Step 170 Global step 170 Train loss 3.692624 on epoch=84
06/24/2022 19:23:58 - INFO - __main__ - Step 180 Global step 180 Train loss 2.515836 on epoch=89
06/24/2022 19:24:01 - INFO - __main__ - Step 190 Global step 190 Train loss 2.121382 on epoch=94
06/24/2022 19:24:03 - INFO - __main__ - Step 200 Global step 200 Train loss 1.789983 on epoch=99
06/24/2022 19:24:03 - INFO - __main__ - Global step 200 Train loss 2.769670 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:24:06 - INFO - __main__ - Step 210 Global step 210 Train loss 2.262825 on epoch=104
06/24/2022 19:24:08 - INFO - __main__ - Step 220 Global step 220 Train loss 2.051809 on epoch=109
06/24/2022 19:24:11 - INFO - __main__ - Step 230 Global step 230 Train loss 2.353955 on epoch=114
06/24/2022 19:24:13 - INFO - __main__ - Step 240 Global step 240 Train loss 2.745810 on epoch=119
06/24/2022 19:24:16 - INFO - __main__ - Step 250 Global step 250 Train loss 2.467023 on epoch=124
06/24/2022 19:24:16 - INFO - __main__ - Global step 250 Train loss 2.376285 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:24:18 - INFO - __main__ - Step 260 Global step 260 Train loss 1.458655 on epoch=129
06/24/2022 19:24:21 - INFO - __main__ - Step 270 Global step 270 Train loss 1.492821 on epoch=134
06/24/2022 19:24:23 - INFO - __main__ - Step 280 Global step 280 Train loss 1.493459 on epoch=139
06/24/2022 19:24:26 - INFO - __main__ - Step 290 Global step 290 Train loss 1.995163 on epoch=144
06/24/2022 19:24:28 - INFO - __main__ - Step 300 Global step 300 Train loss 1.289229 on epoch=149
06/24/2022 19:24:28 - INFO - __main__ - Global step 300 Train loss 1.545865 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 19:24:31 - INFO - __main__ - Step 310 Global step 310 Train loss 1.316971 on epoch=154
06/24/2022 19:24:33 - INFO - __main__ - Step 320 Global step 320 Train loss 1.281198 on epoch=159
06/24/2022 19:24:36 - INFO - __main__ - Step 330 Global step 330 Train loss 1.247386 on epoch=164
06/24/2022 19:24:38 - INFO - __main__ - Step 340 Global step 340 Train loss 1.453127 on epoch=169
06/24/2022 19:24:40 - INFO - __main__ - Step 350 Global step 350 Train loss 1.191969 on epoch=174
06/24/2022 19:24:41 - INFO - __main__ - Global step 350 Train loss 1.298130 Classification-F1 0.3816425120772947 on epoch=174
06/24/2022 19:24:44 - INFO - __main__ - Step 360 Global step 360 Train loss 1.664808 on epoch=179
06/24/2022 19:24:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.895225 on epoch=184
06/24/2022 19:24:49 - INFO - __main__ - Step 380 Global step 380 Train loss 1.078724 on epoch=189
06/24/2022 19:24:51 - INFO - __main__ - Step 390 Global step 390 Train loss 1.371546 on epoch=194
06/24/2022 19:24:54 - INFO - __main__ - Step 400 Global step 400 Train loss 1.299597 on epoch=199
06/24/2022 19:24:54 - INFO - __main__ - Global step 400 Train loss 1.261980 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 19:24:56 - INFO - __main__ - Step 410 Global step 410 Train loss 1.081830 on epoch=204
06/24/2022 19:24:59 - INFO - __main__ - Step 420 Global step 420 Train loss 1.022288 on epoch=209
06/24/2022 19:25:01 - INFO - __main__ - Step 430 Global step 430 Train loss 1.157412 on epoch=214
06/24/2022 19:25:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.971906 on epoch=219
06/24/2022 19:25:06 - INFO - __main__ - Step 450 Global step 450 Train loss 1.163168 on epoch=224
06/24/2022 19:25:06 - INFO - __main__ - Global step 450 Train loss 1.079321 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 19:25:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.843273 on epoch=229
06/24/2022 19:25:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.932024 on epoch=234
06/24/2022 19:25:13 - INFO - __main__ - Step 480 Global step 480 Train loss 1.117956 on epoch=239
06/24/2022 19:25:16 - INFO - __main__ - Step 490 Global step 490 Train loss 1.030971 on epoch=244
06/24/2022 19:25:18 - INFO - __main__ - Step 500 Global step 500 Train loss 1.013367 on epoch=249
06/24/2022 19:25:18 - INFO - __main__ - Global step 500 Train loss 0.987518 Classification-F1 0.539313399778516 on epoch=249
06/24/2022 19:25:21 - INFO - __main__ - Step 510 Global step 510 Train loss 1.069781 on epoch=254
06/24/2022 19:25:24 - INFO - __main__ - Step 520 Global step 520 Train loss 1.440531 on epoch=259
06/24/2022 19:25:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.940743 on epoch=264
06/24/2022 19:25:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.704118 on epoch=269
06/24/2022 19:25:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.723825 on epoch=274
06/24/2022 19:25:31 - INFO - __main__ - Global step 550 Train loss 0.975800 Classification-F1 0.46843853820598 on epoch=274
06/24/2022 19:25:34 - INFO - __main__ - Step 560 Global step 560 Train loss 1.204557 on epoch=279
06/24/2022 19:25:36 - INFO - __main__ - Step 570 Global step 570 Train loss 1.206268 on epoch=284
06/24/2022 19:25:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.954299 on epoch=289
06/24/2022 19:25:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.682955 on epoch=294
06/24/2022 19:25:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.763562 on epoch=299
06/24/2022 19:25:44 - INFO - __main__ - Global step 600 Train loss 0.962328 Classification-F1 0.5901477832512315 on epoch=299
06/24/2022 19:25:44 - INFO - __main__ - save last model!
06/24/2022 19:25:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:25:44 - INFO - __main__ - Printing 3 examples
06/24/2022 19:25:44 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:25:44 - INFO - __main__ - ['1']
06/24/2022 19:25:44 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:25:44 - INFO - __main__ - ['1']
06/24/2022 19:25:44 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:25:44 - INFO - __main__ - ['1']
06/24/2022 19:25:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:25:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:25:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:25:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:25:45 - INFO - __main__ - Printing 3 examples
06/24/2022 19:25:45 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:25:45 - INFO - __main__ - ['1']
06/24/2022 19:25:45 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:25:45 - INFO - __main__ - ['1']
06/24/2022 19:25:45 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:25:45 - INFO - __main__ - ['1']
06/24/2022 19:25:45 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:25:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:25:45 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:25:47 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:25:47 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:25:47 - INFO - __main__ - Printing 3 examples
06/24/2022 19:25:47 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:25:47 - INFO - __main__ - ['0']
06/24/2022 19:25:47 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:25:47 - INFO - __main__ - ['1']
06/24/2022 19:25:47 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:25:47 - INFO - __main__ - ['1']
06/24/2022 19:25:47 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:25:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:25:48 - INFO - __main__ - Starting training!
06/24/2022 19:25:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:25:59 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:26:57 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_13_0.0001_8_predictions.txt
06/24/2022 19:26:57 - INFO - __main__ - Classification-F1 on test data: 0.4997
06/24/2022 19:26:57 - INFO - __main__ - prefix=paws_16_13, lr=0.0001, bsz=8, dev_performance=0.5901477832512315, test_performance=0.499737493437336
06/24/2022 19:26:57 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.0005, bsz=8 ...
06/24/2022 19:26:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:26:58 - INFO - __main__ - Printing 3 examples
06/24/2022 19:26:58 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:26:58 - INFO - __main__ - ['1']
06/24/2022 19:26:58 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:26:58 - INFO - __main__ - ['1']
06/24/2022 19:26:58 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:26:58 - INFO - __main__ - ['1']
06/24/2022 19:26:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:26:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:26:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:26:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:26:58 - INFO - __main__ - Printing 3 examples
06/24/2022 19:26:58 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:26:58 - INFO - __main__ - ['1']
06/24/2022 19:26:58 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:26:58 - INFO - __main__ - ['1']
06/24/2022 19:26:58 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:26:58 - INFO - __main__ - ['1']
06/24/2022 19:26:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:26:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:26:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:27:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:27:02 - INFO - __main__ - Starting training!
06/24/2022 19:27:05 - INFO - __main__ - Step 10 Global step 10 Train loss 17.274071 on epoch=4
06/24/2022 19:27:07 - INFO - __main__ - Step 20 Global step 20 Train loss 13.891212 on epoch=9
06/24/2022 19:27:09 - INFO - __main__ - Step 30 Global step 30 Train loss 7.150117 on epoch=14
06/24/2022 19:27:12 - INFO - __main__ - Step 40 Global step 40 Train loss 3.365751 on epoch=19
06/24/2022 19:27:14 - INFO - __main__ - Step 50 Global step 50 Train loss 3.062154 on epoch=24
06/24/2022 19:27:15 - INFO - __main__ - Global step 50 Train loss 8.948662 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 19:27:17 - INFO - __main__ - Step 60 Global step 60 Train loss 2.588110 on epoch=29
06/24/2022 19:27:20 - INFO - __main__ - Step 70 Global step 70 Train loss 1.328197 on epoch=34
06/24/2022 19:27:22 - INFO - __main__ - Step 80 Global step 80 Train loss 1.152042 on epoch=39
06/24/2022 19:27:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.965765 on epoch=44
06/24/2022 19:27:27 - INFO - __main__ - Step 100 Global step 100 Train loss 1.025919 on epoch=49
06/24/2022 19:27:28 - INFO - __main__ - Global step 100 Train loss 1.412007 Classification-F1 0.5076923076923077 on epoch=49
06/24/2022 19:27:31 - INFO - __main__ - Step 110 Global step 110 Train loss 1.143474 on epoch=54
06/24/2022 19:27:33 - INFO - __main__ - Step 120 Global step 120 Train loss 1.003026 on epoch=59
06/24/2022 19:27:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.865615 on epoch=64
06/24/2022 19:27:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.885773 on epoch=69
06/24/2022 19:27:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.695284 on epoch=74
06/24/2022 19:27:41 - INFO - __main__ - Global step 150 Train loss 0.918634 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:27:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.837942 on epoch=79
06/24/2022 19:27:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.731949 on epoch=84
06/24/2022 19:27:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.450592 on epoch=89
06/24/2022 19:27:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.477154 on epoch=94
06/24/2022 19:27:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.507266 on epoch=99
06/24/2022 19:27:54 - INFO - __main__ - Global step 200 Train loss 0.600981 Classification-F1 0.5307917888563051 on epoch=99
06/24/2022 19:27:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.847844 on epoch=104
06/24/2022 19:28:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.956674 on epoch=109
06/24/2022 19:28:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.754661 on epoch=114
06/24/2022 19:28:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.366173 on epoch=119
06/24/2022 19:28:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.339032 on epoch=124
06/24/2022 19:28:08 - INFO - __main__ - Global step 250 Train loss 0.652877 Classification-F1 0.4385964912280702 on epoch=124
06/24/2022 19:28:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.455700 on epoch=129
06/24/2022 19:28:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.337170 on epoch=134
06/24/2022 19:28:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.456443 on epoch=139
06/24/2022 19:28:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.462760 on epoch=144
06/24/2022 19:28:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.394782 on epoch=149
06/24/2022 19:28:20 - INFO - __main__ - Global step 300 Train loss 0.421371 Classification-F1 0.49090909090909085 on epoch=149
06/24/2022 19:28:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.182850 on epoch=154
06/24/2022 19:28:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.229538 on epoch=159
06/24/2022 19:28:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.190890 on epoch=164
06/24/2022 19:28:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.189502 on epoch=169
06/24/2022 19:28:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.163849 on epoch=174
06/24/2022 19:28:33 - INFO - __main__ - Global step 350 Train loss 0.191326 Classification-F1 0.5333333333333333 on epoch=174
06/24/2022 19:28:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.145782 on epoch=179
06/24/2022 19:28:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.066831 on epoch=184
06/24/2022 19:28:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.110449 on epoch=189
06/24/2022 19:28:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.114463 on epoch=194
06/24/2022 19:28:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.050190 on epoch=199
06/24/2022 19:28:47 - INFO - __main__ - Global step 400 Train loss 0.097543 Classification-F1 0.4589371980676329 on epoch=199
06/24/2022 19:28:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.062793 on epoch=204
06/24/2022 19:28:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.049545 on epoch=209
06/24/2022 19:28:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.068164 on epoch=214
06/24/2022 19:28:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.006516 on epoch=219
06/24/2022 19:28:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.038918 on epoch=224
06/24/2022 19:29:00 - INFO - __main__ - Global step 450 Train loss 0.045187 Classification-F1 0.5076923076923077 on epoch=224
06/24/2022 19:29:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.025903 on epoch=229
06/24/2022 19:29:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.005597 on epoch=234
06/24/2022 19:29:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004013 on epoch=239
06/24/2022 19:29:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.041036 on epoch=244
06/24/2022 19:29:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.016925 on epoch=249
06/24/2022 19:29:13 - INFO - __main__ - Global step 500 Train loss 0.018695 Classification-F1 0.4181818181818182 on epoch=249
06/24/2022 19:29:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.003082 on epoch=254
06/24/2022 19:29:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001200 on epoch=259
06/24/2022 19:29:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001539 on epoch=264
06/24/2022 19:29:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000848 on epoch=269
06/24/2022 19:29:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002296 on epoch=274
06/24/2022 19:29:25 - INFO - __main__ - Global step 550 Train loss 0.001793 Classification-F1 0.4909862142099682 on epoch=274
06/24/2022 19:29:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000330 on epoch=279
06/24/2022 19:29:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003348 on epoch=284
06/24/2022 19:29:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.007378 on epoch=289
06/24/2022 19:29:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.068133 on epoch=294
06/24/2022 19:29:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003413 on epoch=299
06/24/2022 19:29:38 - INFO - __main__ - Global step 600 Train loss 0.016520 Classification-F1 0.4181818181818182 on epoch=299
06/24/2022 19:29:38 - INFO - __main__ - save last model!
06/24/2022 19:29:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:29:39 - INFO - __main__ - Printing 3 examples
06/24/2022 19:29:39 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:29:39 - INFO - __main__ - ['1']
06/24/2022 19:29:39 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:29:39 - INFO - __main__ - ['1']
06/24/2022 19:29:39 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:29:39 - INFO - __main__ - ['1']
06/24/2022 19:29:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:29:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:29:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:29:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:29:39 - INFO - __main__ - Printing 3 examples
06/24/2022 19:29:39 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:29:39 - INFO - __main__ - ['1']
06/24/2022 19:29:39 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:29:39 - INFO - __main__ - ['1']
06/24/2022 19:29:39 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:29:39 - INFO - __main__ - ['1']
06/24/2022 19:29:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:29:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:29:39 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:29:41 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:29:41 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:29:41 - INFO - __main__ - Printing 3 examples
06/24/2022 19:29:41 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:29:41 - INFO - __main__ - ['0']
06/24/2022 19:29:41 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:29:41 - INFO - __main__ - ['1']
06/24/2022 19:29:41 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:29:41 - INFO - __main__ - ['1']
06/24/2022 19:29:41 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:29:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:29:43 - INFO - __main__ - Starting training!
06/24/2022 19:29:45 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:29:53 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:30:53 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_21_0.0005_8_predictions.txt
06/24/2022 19:30:53 - INFO - __main__ - Classification-F1 on test data: 0.4685
06/24/2022 19:30:53 - INFO - __main__ - prefix=paws_16_21, lr=0.0005, bsz=8, dev_performance=0.5333333333333333, test_performance=0.46851376745822937
06/24/2022 19:30:53 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.0003, bsz=8 ...
06/24/2022 19:30:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:30:54 - INFO - __main__ - Printing 3 examples
06/24/2022 19:30:54 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:30:54 - INFO - __main__ - ['1']
06/24/2022 19:30:54 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:30:54 - INFO - __main__ - ['1']
06/24/2022 19:30:54 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:30:54 - INFO - __main__ - ['1']
06/24/2022 19:30:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:30:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:30:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:30:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:30:54 - INFO - __main__ - Printing 3 examples
06/24/2022 19:30:54 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:30:54 - INFO - __main__ - ['1']
06/24/2022 19:30:54 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:30:54 - INFO - __main__ - ['1']
06/24/2022 19:30:54 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:30:54 - INFO - __main__ - ['1']
06/24/2022 19:30:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:30:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:30:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:30:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:30:59 - INFO - __main__ - Starting training!
06/24/2022 19:31:01 - INFO - __main__ - Step 10 Global step 10 Train loss 18.257887 on epoch=4
06/24/2022 19:31:03 - INFO - __main__ - Step 20 Global step 20 Train loss 15.065611 on epoch=9
06/24/2022 19:31:06 - INFO - __main__ - Step 30 Global step 30 Train loss 8.665182 on epoch=14
06/24/2022 19:31:08 - INFO - __main__ - Step 40 Global step 40 Train loss 6.343405 on epoch=19
06/24/2022 19:31:11 - INFO - __main__ - Step 50 Global step 50 Train loss 4.519876 on epoch=24
06/24/2022 19:31:11 - INFO - __main__ - Global step 50 Train loss 10.570392 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 19:31:14 - INFO - __main__ - Step 60 Global step 60 Train loss 4.178642 on epoch=29
06/24/2022 19:31:16 - INFO - __main__ - Step 70 Global step 70 Train loss 3.215014 on epoch=34
06/24/2022 19:31:19 - INFO - __main__ - Step 80 Global step 80 Train loss 2.809184 on epoch=39
06/24/2022 19:31:21 - INFO - __main__ - Step 90 Global step 90 Train loss 2.724012 on epoch=44
06/24/2022 19:31:24 - INFO - __main__ - Step 100 Global step 100 Train loss 2.341506 on epoch=49
06/24/2022 19:31:24 - INFO - __main__ - Global step 100 Train loss 3.053672 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:31:27 - INFO - __main__ - Step 110 Global step 110 Train loss 1.596172 on epoch=54
06/24/2022 19:31:29 - INFO - __main__ - Step 120 Global step 120 Train loss 1.792162 on epoch=59
06/24/2022 19:31:32 - INFO - __main__ - Step 130 Global step 130 Train loss 1.329898 on epoch=64
06/24/2022 19:31:34 - INFO - __main__ - Step 140 Global step 140 Train loss 1.616809 on epoch=69
06/24/2022 19:31:37 - INFO - __main__ - Step 150 Global step 150 Train loss 1.020216 on epoch=74
06/24/2022 19:31:37 - INFO - __main__ - Global step 150 Train loss 1.471052 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:31:39 - INFO - __main__ - Step 160 Global step 160 Train loss 1.013776 on epoch=79
06/24/2022 19:31:42 - INFO - __main__ - Step 170 Global step 170 Train loss 1.230460 on epoch=84
06/24/2022 19:31:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.812347 on epoch=89
06/24/2022 19:31:47 - INFO - __main__ - Step 190 Global step 190 Train loss 1.048815 on epoch=94
06/24/2022 19:31:49 - INFO - __main__ - Step 200 Global step 200 Train loss 1.034082 on epoch=99
06/24/2022 19:31:50 - INFO - __main__ - Global step 200 Train loss 1.027896 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:31:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.676410 on epoch=104
06/24/2022 19:31:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.656824 on epoch=109
06/24/2022 19:31:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.723811 on epoch=114
06/24/2022 19:32:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.963222 on epoch=119
06/24/2022 19:32:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.983047 on epoch=124
06/24/2022 19:32:03 - INFO - __main__ - Global step 250 Train loss 0.800663 Classification-F1 0.4385964912280702 on epoch=124
06/24/2022 19:32:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.830445 on epoch=129
06/24/2022 19:32:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.860552 on epoch=134
06/24/2022 19:32:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.796031 on epoch=139
06/24/2022 19:32:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.864819 on epoch=144
06/24/2022 19:32:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.691149 on epoch=149
06/24/2022 19:32:16 - INFO - __main__ - Global step 300 Train loss 0.808599 Classification-F1 0.3191489361702127 on epoch=149
06/24/2022 19:32:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.594157 on epoch=154
06/24/2022 19:32:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.644697 on epoch=159
06/24/2022 19:32:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.457326 on epoch=164
06/24/2022 19:32:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.692143 on epoch=169
06/24/2022 19:32:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.775566 on epoch=174
06/24/2022 19:32:29 - INFO - __main__ - Global step 350 Train loss 0.632778 Classification-F1 0.4589371980676329 on epoch=174
06/24/2022 19:32:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.592881 on epoch=179
06/24/2022 19:32:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.775918 on epoch=184
06/24/2022 19:32:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.371163 on epoch=189
06/24/2022 19:32:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.578915 on epoch=194
06/24/2022 19:32:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.429777 on epoch=199
06/24/2022 19:32:43 - INFO - __main__ - Global step 400 Train loss 0.549731 Classification-F1 0.4385964912280702 on epoch=199
06/24/2022 19:32:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.427920 on epoch=204
06/24/2022 19:32:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.731084 on epoch=209
06/24/2022 19:32:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.500697 on epoch=214
06/24/2022 19:32:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.232082 on epoch=219
06/24/2022 19:32:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.134614 on epoch=224
06/24/2022 19:32:55 - INFO - __main__ - Global step 450 Train loss 0.405280 Classification-F1 0.46843853820598 on epoch=224
06/24/2022 19:32:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.112991 on epoch=229
06/24/2022 19:33:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.186540 on epoch=234
06/24/2022 19:33:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.167116 on epoch=239
06/24/2022 19:33:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.059800 on epoch=244
06/24/2022 19:33:09 - INFO - __main__ - Step 500 Global step 500 Train loss 1.046728 on epoch=249
06/24/2022 19:33:09 - INFO - __main__ - Global step 500 Train loss 0.314635 Classification-F1 0.3992490613266583 on epoch=249
06/24/2022 19:33:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.289367 on epoch=254
06/24/2022 19:33:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.216632 on epoch=259
06/24/2022 19:33:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.165672 on epoch=264
06/24/2022 19:33:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.287084 on epoch=269
06/24/2022 19:33:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.182542 on epoch=274
06/24/2022 19:33:22 - INFO - __main__ - Global step 550 Train loss 0.228259 Classification-F1 0.46843853820598 on epoch=274
06/24/2022 19:33:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.203007 on epoch=279
06/24/2022 19:33:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.206908 on epoch=284
06/24/2022 19:33:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.137098 on epoch=289
06/24/2022 19:33:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.395856 on epoch=294
06/24/2022 19:33:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.158605 on epoch=299
06/24/2022 19:33:34 - INFO - __main__ - Global step 600 Train loss 0.220295 Classification-F1 0.5151515151515151 on epoch=299
06/24/2022 19:33:35 - INFO - __main__ - save last model!
06/24/2022 19:33:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:33:35 - INFO - __main__ - Printing 3 examples
06/24/2022 19:33:35 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:33:35 - INFO - __main__ - ['1']
06/24/2022 19:33:35 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:33:35 - INFO - __main__ - ['1']
06/24/2022 19:33:35 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:33:35 - INFO - __main__ - ['1']
06/24/2022 19:33:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:33:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:33:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:33:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:33:35 - INFO - __main__ - Printing 3 examples
06/24/2022 19:33:35 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:33:35 - INFO - __main__ - ['1']
06/24/2022 19:33:35 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:33:35 - INFO - __main__ - ['1']
06/24/2022 19:33:35 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:33:35 - INFO - __main__ - ['1']
06/24/2022 19:33:35 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:33:35 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:33:35 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:33:37 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:33:38 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:33:38 - INFO - __main__ - Printing 3 examples
06/24/2022 19:33:38 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:33:38 - INFO - __main__ - ['0']
06/24/2022 19:33:38 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:33:38 - INFO - __main__ - ['1']
06/24/2022 19:33:38 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:33:38 - INFO - __main__ - ['1']
06/24/2022 19:33:38 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:33:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:33:39 - INFO - __main__ - Starting training!
06/24/2022 19:33:42 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:33:49 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:34:49 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_21_0.0003_8_predictions.txt
06/24/2022 19:34:49 - INFO - __main__ - Classification-F1 on test data: 0.4710
06/24/2022 19:34:50 - INFO - __main__ - prefix=paws_16_21, lr=0.0003, bsz=8, dev_performance=0.5151515151515151, test_performance=0.4709934995299648
06/24/2022 19:34:50 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.0002, bsz=8 ...
06/24/2022 19:34:51 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:34:51 - INFO - __main__ - Printing 3 examples
06/24/2022 19:34:51 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:34:51 - INFO - __main__ - ['1']
06/24/2022 19:34:51 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:34:51 - INFO - __main__ - ['1']
06/24/2022 19:34:51 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:34:51 - INFO - __main__ - ['1']
06/24/2022 19:34:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:34:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:34:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:34:51 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:34:51 - INFO - __main__ - Printing 3 examples
06/24/2022 19:34:51 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:34:51 - INFO - __main__ - ['1']
06/24/2022 19:34:51 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:34:51 - INFO - __main__ - ['1']
06/24/2022 19:34:51 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:34:51 - INFO - __main__ - ['1']
06/24/2022 19:34:51 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:34:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:34:51 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:34:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:34:55 - INFO - __main__ - Starting training!
06/24/2022 19:34:57 - INFO - __main__ - Step 10 Global step 10 Train loss 18.620905 on epoch=4
06/24/2022 19:34:59 - INFO - __main__ - Step 20 Global step 20 Train loss 15.388863 on epoch=9
06/24/2022 19:35:01 - INFO - __main__ - Step 30 Global step 30 Train loss 11.354353 on epoch=14
06/24/2022 19:35:04 - INFO - __main__ - Step 40 Global step 40 Train loss 8.287121 on epoch=19
06/24/2022 19:35:06 - INFO - __main__ - Step 50 Global step 50 Train loss 7.021180 on epoch=24
06/24/2022 19:35:06 - INFO - __main__ - Global step 50 Train loss 12.134485 Classification-F1 0.14473684210526316 on epoch=24
06/24/2022 19:35:09 - INFO - __main__ - Step 60 Global step 60 Train loss 6.042306 on epoch=29
06/24/2022 19:35:12 - INFO - __main__ - Step 70 Global step 70 Train loss 4.367318 on epoch=34
06/24/2022 19:35:14 - INFO - __main__ - Step 80 Global step 80 Train loss 4.167357 on epoch=39
06/24/2022 19:35:17 - INFO - __main__ - Step 90 Global step 90 Train loss 3.071975 on epoch=44
06/24/2022 19:35:19 - INFO - __main__ - Step 100 Global step 100 Train loss 3.188432 on epoch=49
06/24/2022 19:35:19 - INFO - __main__ - Global step 100 Train loss 4.167478 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:35:22 - INFO - __main__ - Step 110 Global step 110 Train loss 3.193263 on epoch=54
06/24/2022 19:35:24 - INFO - __main__ - Step 120 Global step 120 Train loss 2.685688 on epoch=59
06/24/2022 19:35:27 - INFO - __main__ - Step 130 Global step 130 Train loss 2.502991 on epoch=64
06/24/2022 19:35:29 - INFO - __main__ - Step 140 Global step 140 Train loss 2.817993 on epoch=69
06/24/2022 19:35:32 - INFO - __main__ - Step 150 Global step 150 Train loss 1.915219 on epoch=74
06/24/2022 19:35:32 - INFO - __main__ - Global step 150 Train loss 2.623031 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:35:34 - INFO - __main__ - Step 160 Global step 160 Train loss 1.651995 on epoch=79
06/24/2022 19:35:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.776486 on epoch=84
06/24/2022 19:35:39 - INFO - __main__ - Step 180 Global step 180 Train loss 1.548394 on epoch=89
06/24/2022 19:35:42 - INFO - __main__ - Step 190 Global step 190 Train loss 1.355190 on epoch=94
06/24/2022 19:35:44 - INFO - __main__ - Step 200 Global step 200 Train loss 1.595068 on epoch=99
06/24/2022 19:35:44 - INFO - __main__ - Global step 200 Train loss 1.585427 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:35:47 - INFO - __main__ - Step 210 Global step 210 Train loss 1.497940 on epoch=104
06/24/2022 19:35:49 - INFO - __main__ - Step 220 Global step 220 Train loss 1.179557 on epoch=109
06/24/2022 19:35:52 - INFO - __main__ - Step 230 Global step 230 Train loss 1.084847 on epoch=114
06/24/2022 19:35:54 - INFO - __main__ - Step 240 Global step 240 Train loss 1.366999 on epoch=119
06/24/2022 19:35:57 - INFO - __main__ - Step 250 Global step 250 Train loss 1.620858 on epoch=124
06/24/2022 19:35:57 - INFO - __main__ - Global step 250 Train loss 1.350040 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:35:59 - INFO - __main__ - Step 260 Global step 260 Train loss 1.144199 on epoch=129
06/24/2022 19:36:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.837045 on epoch=134
06/24/2022 19:36:04 - INFO - __main__ - Step 280 Global step 280 Train loss 1.231829 on epoch=139
06/24/2022 19:36:06 - INFO - __main__ - Step 290 Global step 290 Train loss 1.025651 on epoch=144
06/24/2022 19:36:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.754085 on epoch=149
06/24/2022 19:36:09 - INFO - __main__ - Global step 300 Train loss 0.998562 Classification-F1 0.3191489361702127 on epoch=149
06/24/2022 19:36:12 - INFO - __main__ - Step 310 Global step 310 Train loss 1.056789 on epoch=154
06/24/2022 19:36:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.693726 on epoch=159
06/24/2022 19:36:16 - INFO - __main__ - Step 330 Global step 330 Train loss 1.089097 on epoch=164
06/24/2022 19:36:19 - INFO - __main__ - Step 340 Global step 340 Train loss 1.279687 on epoch=169
06/24/2022 19:36:21 - INFO - __main__ - Step 350 Global step 350 Train loss 1.306084 on epoch=174
06/24/2022 19:36:22 - INFO - __main__ - Global step 350 Train loss 1.085077 Classification-F1 0.41700404858299595 on epoch=174
06/24/2022 19:36:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.738485 on epoch=179
06/24/2022 19:36:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.746167 on epoch=184
06/24/2022 19:36:29 - INFO - __main__ - Step 380 Global step 380 Train loss 1.103486 on epoch=189
06/24/2022 19:36:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.677232 on epoch=194
06/24/2022 19:36:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.764150 on epoch=199
06/24/2022 19:36:34 - INFO - __main__ - Global step 400 Train loss 0.805904 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 19:36:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.969161 on epoch=204
06/24/2022 19:36:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.986785 on epoch=209
06/24/2022 19:36:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.733802 on epoch=214
06/24/2022 19:36:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.627604 on epoch=219
06/24/2022 19:36:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.592623 on epoch=224
06/24/2022 19:36:47 - INFO - __main__ - Global step 450 Train loss 0.781995 Classification-F1 0.28744939271255066 on epoch=224
06/24/2022 19:36:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.791320 on epoch=229
06/24/2022 19:36:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.627774 on epoch=234
06/24/2022 19:36:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.645169 on epoch=239
06/24/2022 19:36:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.605172 on epoch=244
06/24/2022 19:36:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.783778 on epoch=249
06/24/2022 19:36:59 - INFO - __main__ - Global step 500 Train loss 0.690643 Classification-F1 0.46843853820598 on epoch=249
06/24/2022 19:37:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.517364 on epoch=254
06/24/2022 19:37:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.471180 on epoch=259
06/24/2022 19:37:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.710557 on epoch=264
06/24/2022 19:37:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.557061 on epoch=269
06/24/2022 19:37:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.398903 on epoch=274
06/24/2022 19:37:12 - INFO - __main__ - Global step 550 Train loss 0.531013 Classification-F1 0.5270935960591133 on epoch=274
06/24/2022 19:37:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.864571 on epoch=279
06/24/2022 19:37:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.794682 on epoch=284
06/24/2022 19:37:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.528410 on epoch=289
06/24/2022 19:37:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.439317 on epoch=294
06/24/2022 19:37:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.515220 on epoch=299
06/24/2022 19:37:25 - INFO - __main__ - Global step 600 Train loss 0.628440 Classification-F1 0.33793103448275863 on epoch=299
06/24/2022 19:37:25 - INFO - __main__ - save last model!
06/24/2022 19:37:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:37:26 - INFO - __main__ - Printing 3 examples
06/24/2022 19:37:26 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:37:26 - INFO - __main__ - ['1']
06/24/2022 19:37:26 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:37:26 - INFO - __main__ - ['1']
06/24/2022 19:37:26 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:37:26 - INFO - __main__ - ['1']
06/24/2022 19:37:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:37:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:37:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:37:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:37:26 - INFO - __main__ - Printing 3 examples
06/24/2022 19:37:26 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:37:26 - INFO - __main__ - ['1']
06/24/2022 19:37:26 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:37:26 - INFO - __main__ - ['1']
06/24/2022 19:37:26 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:37:26 - INFO - __main__ - ['1']
06/24/2022 19:37:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:37:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:37:26 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:37:28 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:37:28 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:37:28 - INFO - __main__ - Printing 3 examples
06/24/2022 19:37:28 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:37:28 - INFO - __main__ - ['0']
06/24/2022 19:37:28 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:37:28 - INFO - __main__ - ['1']
06/24/2022 19:37:28 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:37:28 - INFO - __main__ - ['1']
06/24/2022 19:37:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:37:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:37:30 - INFO - __main__ - Starting training!
06/24/2022 19:37:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:37:40 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:38:39 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_21_0.0002_8_predictions.txt
06/24/2022 19:38:39 - INFO - __main__ - Classification-F1 on test data: 0.4797
06/24/2022 19:38:39 - INFO - __main__ - prefix=paws_16_21, lr=0.0002, bsz=8, dev_performance=0.5270935960591133, test_performance=0.4797438873306056
06/24/2022 19:38:39 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.0001, bsz=8 ...
06/24/2022 19:38:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:38:40 - INFO - __main__ - Printing 3 examples
06/24/2022 19:38:40 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 19:38:40 - INFO - __main__ - ['1']
06/24/2022 19:38:40 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 19:38:40 - INFO - __main__ - ['1']
06/24/2022 19:38:40 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 19:38:40 - INFO - __main__ - ['1']
06/24/2022 19:38:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:38:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:38:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:38:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:38:40 - INFO - __main__ - Printing 3 examples
06/24/2022 19:38:40 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 19:38:40 - INFO - __main__ - ['1']
06/24/2022 19:38:40 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 19:38:40 - INFO - __main__ - ['1']
06/24/2022 19:38:40 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 19:38:40 - INFO - __main__ - ['1']
06/24/2022 19:38:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:38:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:38:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:38:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:38:44 - INFO - __main__ - Starting training!
06/24/2022 19:38:46 - INFO - __main__ - Step 10 Global step 10 Train loss 18.692295 on epoch=4
06/24/2022 19:38:49 - INFO - __main__ - Step 20 Global step 20 Train loss 17.613937 on epoch=9
06/24/2022 19:38:51 - INFO - __main__ - Step 30 Global step 30 Train loss 13.956186 on epoch=14
06/24/2022 19:38:54 - INFO - __main__ - Step 40 Global step 40 Train loss 10.920215 on epoch=19
06/24/2022 19:38:56 - INFO - __main__ - Step 50 Global step 50 Train loss 10.642973 on epoch=24
06/24/2022 19:38:58 - INFO - __main__ - Global step 50 Train loss 14.365120 Classification-F1 0.016 on epoch=24
06/24/2022 19:39:00 - INFO - __main__ - Step 60 Global step 60 Train loss 8.959156 on epoch=29
06/24/2022 19:39:03 - INFO - __main__ - Step 70 Global step 70 Train loss 9.069086 on epoch=34
06/24/2022 19:39:05 - INFO - __main__ - Step 80 Global step 80 Train loss 7.757186 on epoch=39
06/24/2022 19:39:08 - INFO - __main__ - Step 90 Global step 90 Train loss 7.111947 on epoch=44
06/24/2022 19:39:11 - INFO - __main__ - Step 100 Global step 100 Train loss 5.805356 on epoch=49
06/24/2022 19:39:11 - INFO - __main__ - Global step 100 Train loss 7.740546 Classification-F1 0.10483351235230934 on epoch=49
06/24/2022 19:39:14 - INFO - __main__ - Step 110 Global step 110 Train loss 6.124685 on epoch=54
06/24/2022 19:39:17 - INFO - __main__ - Step 120 Global step 120 Train loss 4.523122 on epoch=59
06/24/2022 19:39:19 - INFO - __main__ - Step 130 Global step 130 Train loss 4.307140 on epoch=64
06/24/2022 19:39:22 - INFO - __main__ - Step 140 Global step 140 Train loss 4.911710 on epoch=69
06/24/2022 19:39:24 - INFO - __main__ - Step 150 Global step 150 Train loss 4.108825 on epoch=74
06/24/2022 19:39:25 - INFO - __main__ - Global step 150 Train loss 4.795096 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:39:28 - INFO - __main__ - Step 160 Global step 160 Train loss 4.292697 on epoch=79
06/24/2022 19:39:30 - INFO - __main__ - Step 170 Global step 170 Train loss 3.957476 on epoch=84
06/24/2022 19:39:33 - INFO - __main__ - Step 180 Global step 180 Train loss 3.585941 on epoch=89
06/24/2022 19:39:35 - INFO - __main__ - Step 190 Global step 190 Train loss 3.197808 on epoch=94
06/24/2022 19:39:38 - INFO - __main__ - Step 200 Global step 200 Train loss 3.032729 on epoch=99
06/24/2022 19:39:38 - INFO - __main__ - Global step 200 Train loss 3.613330 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:39:41 - INFO - __main__ - Step 210 Global step 210 Train loss 2.381713 on epoch=104
06/24/2022 19:39:43 - INFO - __main__ - Step 220 Global step 220 Train loss 1.834528 on epoch=109
06/24/2022 19:39:46 - INFO - __main__ - Step 230 Global step 230 Train loss 3.314250 on epoch=114
06/24/2022 19:39:48 - INFO - __main__ - Step 240 Global step 240 Train loss 2.644478 on epoch=119
06/24/2022 19:39:51 - INFO - __main__ - Step 250 Global step 250 Train loss 1.998127 on epoch=124
06/24/2022 19:39:51 - INFO - __main__ - Global step 250 Train loss 2.434619 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:39:54 - INFO - __main__ - Step 260 Global step 260 Train loss 2.074090 on epoch=129
06/24/2022 19:39:57 - INFO - __main__ - Step 270 Global step 270 Train loss 2.367315 on epoch=134
06/24/2022 19:39:59 - INFO - __main__ - Step 280 Global step 280 Train loss 1.677291 on epoch=139
06/24/2022 19:40:02 - INFO - __main__ - Step 290 Global step 290 Train loss 2.105669 on epoch=144
06/24/2022 19:40:04 - INFO - __main__ - Step 300 Global step 300 Train loss 1.643373 on epoch=149
06/24/2022 19:40:05 - INFO - __main__ - Global step 300 Train loss 1.973548 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 19:40:07 - INFO - __main__ - Step 310 Global step 310 Train loss 1.448235 on epoch=154
06/24/2022 19:40:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.956200 on epoch=159
06/24/2022 19:40:13 - INFO - __main__ - Step 330 Global step 330 Train loss 1.807747 on epoch=164
06/24/2022 19:40:15 - INFO - __main__ - Step 340 Global step 340 Train loss 1.287284 on epoch=169
06/24/2022 19:40:18 - INFO - __main__ - Step 350 Global step 350 Train loss 1.854182 on epoch=174
06/24/2022 19:40:18 - INFO - __main__ - Global step 350 Train loss 1.470730 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 19:40:21 - INFO - __main__ - Step 360 Global step 360 Train loss 1.528572 on epoch=179
06/24/2022 19:40:23 - INFO - __main__ - Step 370 Global step 370 Train loss 1.758467 on epoch=184
06/24/2022 19:40:26 - INFO - __main__ - Step 380 Global step 380 Train loss 1.566288 on epoch=189
06/24/2022 19:40:28 - INFO - __main__ - Step 390 Global step 390 Train loss 1.439713 on epoch=194
06/24/2022 19:40:31 - INFO - __main__ - Step 400 Global step 400 Train loss 1.423964 on epoch=199
06/24/2022 19:40:31 - INFO - __main__ - Global step 400 Train loss 1.543401 Classification-F1 0.3816425120772947 on epoch=199
06/24/2022 19:40:34 - INFO - __main__ - Step 410 Global step 410 Train loss 1.410772 on epoch=204
06/24/2022 19:40:37 - INFO - __main__ - Step 420 Global step 420 Train loss 1.073613 on epoch=209
06/24/2022 19:40:39 - INFO - __main__ - Step 430 Global step 430 Train loss 1.177377 on epoch=214
06/24/2022 19:40:42 - INFO - __main__ - Step 440 Global step 440 Train loss 1.739299 on epoch=219
06/24/2022 19:40:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.988757 on epoch=224
06/24/2022 19:40:45 - INFO - __main__ - Global step 450 Train loss 1.277964 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 19:40:47 - INFO - __main__ - Step 460 Global step 460 Train loss 1.366991 on epoch=229
06/24/2022 19:40:50 - INFO - __main__ - Step 470 Global step 470 Train loss 1.064601 on epoch=234
06/24/2022 19:40:52 - INFO - __main__ - Step 480 Global step 480 Train loss 1.231667 on epoch=239
06/24/2022 19:40:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.971896 on epoch=244
06/24/2022 19:40:57 - INFO - __main__ - Step 500 Global step 500 Train loss 1.451910 on epoch=249
06/24/2022 19:40:58 - INFO - __main__ - Global step 500 Train loss 1.217413 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 19:41:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.904204 on epoch=254
06/24/2022 19:41:03 - INFO - __main__ - Step 520 Global step 520 Train loss 1.475807 on epoch=259
06/24/2022 19:41:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.739236 on epoch=264
06/24/2022 19:41:08 - INFO - __main__ - Step 540 Global step 540 Train loss 1.002265 on epoch=269
06/24/2022 19:41:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.975372 on epoch=274
06/24/2022 19:41:11 - INFO - __main__ - Global step 550 Train loss 1.019377 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 19:41:13 - INFO - __main__ - Step 560 Global step 560 Train loss 1.164303 on epoch=279
06/24/2022 19:41:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.607590 on epoch=284
06/24/2022 19:41:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.988737 on epoch=289
06/24/2022 19:41:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.847035 on epoch=294
06/24/2022 19:41:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.996812 on epoch=299
06/24/2022 19:41:24 - INFO - __main__ - Global step 600 Train loss 0.920895 Classification-F1 0.3333333333333333 on epoch=299
06/24/2022 19:41:24 - INFO - __main__ - save last model!
06/24/2022 19:41:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:41:24 - INFO - __main__ - Printing 3 examples
06/24/2022 19:41:24 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:41:24 - INFO - __main__ - ['1']
06/24/2022 19:41:24 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:41:24 - INFO - __main__ - ['1']
06/24/2022 19:41:24 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:41:24 - INFO - __main__ - ['1']
06/24/2022 19:41:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:41:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:41:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:41:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:41:25 - INFO - __main__ - Printing 3 examples
06/24/2022 19:41:25 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:41:25 - INFO - __main__ - ['1']
06/24/2022 19:41:25 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:41:25 - INFO - __main__ - ['1']
06/24/2022 19:41:25 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:41:25 - INFO - __main__ - ['1']
06/24/2022 19:41:25 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:41:25 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:41:25 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:41:26 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:41:27 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:41:27 - INFO - __main__ - Printing 3 examples
06/24/2022 19:41:27 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:41:27 - INFO - __main__ - ['0']
06/24/2022 19:41:27 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:41:27 - INFO - __main__ - ['1']
06/24/2022 19:41:27 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:41:27 - INFO - __main__ - ['1']
06/24/2022 19:41:27 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:41:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:41:29 - INFO - __main__ - Starting training!
06/24/2022 19:41:31 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:41:38 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:42:36 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_21_0.0001_8_predictions.txt
06/24/2022 19:42:36 - INFO - __main__ - Classification-F1 on test data: 0.3440
06/24/2022 19:42:37 - INFO - __main__ - prefix=paws_16_21, lr=0.0001, bsz=8, dev_performance=0.3816425120772947, test_performance=0.34403874656089367
06/24/2022 19:42:37 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.0005, bsz=8 ...
06/24/2022 19:42:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:42:38 - INFO - __main__ - Printing 3 examples
06/24/2022 19:42:38 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:42:38 - INFO - __main__ - ['1']
06/24/2022 19:42:38 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:42:38 - INFO - __main__ - ['1']
06/24/2022 19:42:38 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:42:38 - INFO - __main__ - ['1']
06/24/2022 19:42:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:42:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:42:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:42:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:42:38 - INFO - __main__ - Printing 3 examples
06/24/2022 19:42:38 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:42:38 - INFO - __main__ - ['1']
06/24/2022 19:42:38 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:42:38 - INFO - __main__ - ['1']
06/24/2022 19:42:38 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:42:38 - INFO - __main__ - ['1']
06/24/2022 19:42:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:42:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:42:38 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:42:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:42:41 - INFO - __main__ - Starting training!
06/24/2022 19:42:44 - INFO - __main__ - Step 10 Global step 10 Train loss 18.415697 on epoch=4
06/24/2022 19:42:46 - INFO - __main__ - Step 20 Global step 20 Train loss 12.560564 on epoch=9
06/24/2022 19:42:48 - INFO - __main__ - Step 30 Global step 30 Train loss 8.240090 on epoch=14
06/24/2022 19:42:51 - INFO - __main__ - Step 40 Global step 40 Train loss 6.562280 on epoch=19
06/24/2022 19:42:53 - INFO - __main__ - Step 50 Global step 50 Train loss 4.344825 on epoch=24
06/24/2022 19:42:54 - INFO - __main__ - Global step 50 Train loss 10.024692 Classification-F1 0.3191489361702127 on epoch=24
06/24/2022 19:42:56 - INFO - __main__ - Step 60 Global step 60 Train loss 2.394375 on epoch=29
06/24/2022 19:42:59 - INFO - __main__ - Step 70 Global step 70 Train loss 2.610148 on epoch=34
06/24/2022 19:43:02 - INFO - __main__ - Step 80 Global step 80 Train loss 2.104713 on epoch=39
06/24/2022 19:43:04 - INFO - __main__ - Step 90 Global step 90 Train loss 2.002684 on epoch=44
06/24/2022 19:43:07 - INFO - __main__ - Step 100 Global step 100 Train loss 1.921515 on epoch=49
06/24/2022 19:43:07 - INFO - __main__ - Global step 100 Train loss 2.206687 Classification-F1 0.3191489361702127 on epoch=49
06/24/2022 19:43:09 - INFO - __main__ - Step 110 Global step 110 Train loss 1.945712 on epoch=54
06/24/2022 19:43:12 - INFO - __main__ - Step 120 Global step 120 Train loss 1.616817 on epoch=59
06/24/2022 19:43:14 - INFO - __main__ - Step 130 Global step 130 Train loss 1.389851 on epoch=64
06/24/2022 19:43:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.907677 on epoch=69
06/24/2022 19:43:20 - INFO - __main__ - Step 150 Global step 150 Train loss 1.937530 on epoch=74
06/24/2022 19:43:20 - INFO - __main__ - Global step 150 Train loss 1.559518 Classification-F1 0.4385964912280702 on epoch=74
06/24/2022 19:43:23 - INFO - __main__ - Step 160 Global step 160 Train loss 1.491960 on epoch=79
06/24/2022 19:43:26 - INFO - __main__ - Step 170 Global step 170 Train loss 1.197883 on epoch=84
06/24/2022 19:43:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.986143 on epoch=89
06/24/2022 19:43:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.730025 on epoch=94
06/24/2022 19:43:33 - INFO - __main__ - Step 200 Global step 200 Train loss 1.078399 on epoch=99
06/24/2022 19:43:33 - INFO - __main__ - Global step 200 Train loss 1.096882 Classification-F1 0.5733333333333335 on epoch=99
06/24/2022 19:43:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.727629 on epoch=104
06/24/2022 19:43:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.554098 on epoch=109
06/24/2022 19:43:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.715226 on epoch=114
06/24/2022 19:43:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.842000 on epoch=119
06/24/2022 19:43:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.799902 on epoch=124
06/24/2022 19:43:47 - INFO - __main__ - Global step 250 Train loss 0.727771 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:43:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.658164 on epoch=129
06/24/2022 19:43:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.868900 on epoch=134
06/24/2022 19:43:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.767971 on epoch=139
06/24/2022 19:43:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.845734 on epoch=144
06/24/2022 19:44:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.947716 on epoch=149
06/24/2022 19:44:00 - INFO - __main__ - Global step 300 Train loss 0.817697 Classification-F1 0.3043478260869565 on epoch=149
06/24/2022 19:44:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.703301 on epoch=154
06/24/2022 19:44:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.897266 on epoch=159
06/24/2022 19:44:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.698976 on epoch=164
06/24/2022 19:44:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.432472 on epoch=169
06/24/2022 19:44:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.812874 on epoch=174
06/24/2022 19:44:13 - INFO - __main__ - Global step 350 Train loss 0.708978 Classification-F1 0.39999999999999997 on epoch=174
06/24/2022 19:44:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.639367 on epoch=179
06/24/2022 19:44:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.531936 on epoch=184
06/24/2022 19:44:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.686203 on epoch=189
06/24/2022 19:44:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.937829 on epoch=194
06/24/2022 19:44:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.706576 on epoch=199
06/24/2022 19:44:26 - INFO - __main__ - Global step 400 Train loss 0.700382 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 19:44:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.594252 on epoch=204
06/24/2022 19:44:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.667805 on epoch=209
06/24/2022 19:44:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.754670 on epoch=214
06/24/2022 19:44:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.560917 on epoch=219
06/24/2022 19:44:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.411428 on epoch=224
06/24/2022 19:44:39 - INFO - __main__ - Global step 450 Train loss 0.597814 Classification-F1 0.4666666666666667 on epoch=224
06/24/2022 19:44:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.631295 on epoch=229
06/24/2022 19:44:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.409600 on epoch=234
06/24/2022 19:44:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.588607 on epoch=239
06/24/2022 19:44:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.438521 on epoch=244
06/24/2022 19:44:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.437250 on epoch=249
06/24/2022 19:44:52 - INFO - __main__ - Global step 500 Train loss 0.501055 Classification-F1 0.4817813765182186 on epoch=249
06/24/2022 19:44:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.564293 on epoch=254
06/24/2022 19:44:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.428594 on epoch=259
06/24/2022 19:45:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.501571 on epoch=264
06/24/2022 19:45:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.441739 on epoch=269
06/24/2022 19:45:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.434799 on epoch=274
06/24/2022 19:45:05 - INFO - __main__ - Global step 550 Train loss 0.474199 Classification-F1 0.39999999999999997 on epoch=274
06/24/2022 19:45:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.374310 on epoch=279
06/24/2022 19:45:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.349503 on epoch=284
06/24/2022 19:45:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.423204 on epoch=289
06/24/2022 19:45:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.387028 on epoch=294
06/24/2022 19:45:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.300222 on epoch=299
06/24/2022 19:45:18 - INFO - __main__ - Global step 600 Train loss 0.366853 Classification-F1 0.4682306940371457 on epoch=299
06/24/2022 19:45:18 - INFO - __main__ - save last model!
06/24/2022 19:45:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:45:19 - INFO - __main__ - Printing 3 examples
06/24/2022 19:45:19 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:45:19 - INFO - __main__ - ['1']
06/24/2022 19:45:19 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:45:19 - INFO - __main__ - ['1']
06/24/2022 19:45:19 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:45:19 - INFO - __main__ - ['1']
06/24/2022 19:45:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:45:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:45:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:45:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:45:19 - INFO - __main__ - Printing 3 examples
06/24/2022 19:45:19 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:45:19 - INFO - __main__ - ['1']
06/24/2022 19:45:19 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:45:19 - INFO - __main__ - ['1']
06/24/2022 19:45:19 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:45:19 - INFO - __main__ - ['1']
06/24/2022 19:45:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:45:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:45:20 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:45:21 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:45:21 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:45:21 - INFO - __main__ - Printing 3 examples
06/24/2022 19:45:21 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:45:21 - INFO - __main__ - ['0']
06/24/2022 19:45:21 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:45:21 - INFO - __main__ - ['1']
06/24/2022 19:45:21 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:45:21 - INFO - __main__ - ['1']
06/24/2022 19:45:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:45:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:45:24 - INFO - __main__ - Starting training!
06/24/2022 19:45:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:45:33 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:46:32 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_42_0.0005_8_predictions.txt
06/24/2022 19:46:32 - INFO - __main__ - Classification-F1 on test data: 0.4748
06/24/2022 19:46:32 - INFO - __main__ - prefix=paws_16_42, lr=0.0005, bsz=8, dev_performance=0.5733333333333335, test_performance=0.47480251691372055
06/24/2022 19:46:32 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.0003, bsz=8 ...
06/24/2022 19:46:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:46:33 - INFO - __main__ - Printing 3 examples
06/24/2022 19:46:33 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:46:33 - INFO - __main__ - ['1']
06/24/2022 19:46:33 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:46:33 - INFO - __main__ - ['1']
06/24/2022 19:46:33 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:46:33 - INFO - __main__ - ['1']
06/24/2022 19:46:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:46:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:46:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:46:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:46:33 - INFO - __main__ - Printing 3 examples
06/24/2022 19:46:33 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:46:33 - INFO - __main__ - ['1']
06/24/2022 19:46:33 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:46:33 - INFO - __main__ - ['1']
06/24/2022 19:46:33 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:46:33 - INFO - __main__ - ['1']
06/24/2022 19:46:33 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:46:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:46:33 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:46:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:46:37 - INFO - __main__ - Starting training!
06/24/2022 19:46:39 - INFO - __main__ - Step 10 Global step 10 Train loss 18.146641 on epoch=4
06/24/2022 19:46:41 - INFO - __main__ - Step 20 Global step 20 Train loss 17.455408 on epoch=9
06/24/2022 19:46:43 - INFO - __main__ - Step 30 Global step 30 Train loss 10.627536 on epoch=14
06/24/2022 19:46:46 - INFO - __main__ - Step 40 Global step 40 Train loss 7.425112 on epoch=19
06/24/2022 19:46:48 - INFO - __main__ - Step 50 Global step 50 Train loss 4.683543 on epoch=24
06/24/2022 19:46:48 - INFO - __main__ - Global step 50 Train loss 11.667647 Classification-F1 0.14695121951219514 on epoch=24
06/24/2022 19:46:51 - INFO - __main__ - Step 60 Global step 60 Train loss 3.192115 on epoch=29
06/24/2022 19:46:53 - INFO - __main__ - Step 70 Global step 70 Train loss 3.082392 on epoch=34
06/24/2022 19:46:56 - INFO - __main__ - Step 80 Global step 80 Train loss 2.919779 on epoch=39
06/24/2022 19:46:58 - INFO - __main__ - Step 90 Global step 90 Train loss 2.222760 on epoch=44
06/24/2022 19:47:01 - INFO - __main__ - Step 100 Global step 100 Train loss 1.996280 on epoch=49
06/24/2022 19:47:01 - INFO - __main__ - Global step 100 Train loss 2.682665 Classification-F1 0.3191489361702127 on epoch=49
06/24/2022 19:47:04 - INFO - __main__ - Step 110 Global step 110 Train loss 1.307583 on epoch=54
06/24/2022 19:47:07 - INFO - __main__ - Step 120 Global step 120 Train loss 1.796091 on epoch=59
06/24/2022 19:47:09 - INFO - __main__ - Step 130 Global step 130 Train loss 1.526505 on epoch=64
06/24/2022 19:47:11 - INFO - __main__ - Step 140 Global step 140 Train loss 1.177732 on epoch=69
06/24/2022 19:47:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.759973 on epoch=74
06/24/2022 19:47:14 - INFO - __main__ - Global step 150 Train loss 1.313577 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:47:17 - INFO - __main__ - Step 160 Global step 160 Train loss 1.399680 on epoch=79
06/24/2022 19:47:19 - INFO - __main__ - Step 170 Global step 170 Train loss 1.140395 on epoch=84
06/24/2022 19:47:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.787084 on epoch=89
06/24/2022 19:47:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.793913 on epoch=94
06/24/2022 19:47:27 - INFO - __main__ - Step 200 Global step 200 Train loss 1.236342 on epoch=99
06/24/2022 19:47:27 - INFO - __main__ - Global step 200 Train loss 1.071483 Classification-F1 0.3125 on epoch=99
06/24/2022 19:47:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.785909 on epoch=104
06/24/2022 19:47:32 - INFO - __main__ - Step 220 Global step 220 Train loss 1.114986 on epoch=109
06/24/2022 19:47:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.645546 on epoch=114
06/24/2022 19:47:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.894590 on epoch=119
06/24/2022 19:47:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.793587 on epoch=124
06/24/2022 19:47:40 - INFO - __main__ - Global step 250 Train loss 0.846924 Classification-F1 0.3764102564102564 on epoch=124
06/24/2022 19:47:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.460832 on epoch=129
06/24/2022 19:47:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.501075 on epoch=134
06/24/2022 19:47:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.597297 on epoch=139
06/24/2022 19:47:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.432567 on epoch=144
06/24/2022 19:47:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.323675 on epoch=149
06/24/2022 19:47:53 - INFO - __main__ - Global step 300 Train loss 0.463089 Classification-F1 0.4817813765182186 on epoch=149
06/24/2022 19:47:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.391158 on epoch=154
06/24/2022 19:47:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.342239 on epoch=159
06/24/2022 19:48:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.348064 on epoch=164
06/24/2022 19:48:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.176082 on epoch=169
06/24/2022 19:48:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.196226 on epoch=174
06/24/2022 19:48:05 - INFO - __main__ - Global step 350 Train loss 0.290754 Classification-F1 0.4554554554554554 on epoch=174
06/24/2022 19:48:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.248122 on epoch=179
06/24/2022 19:48:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.214113 on epoch=184
06/24/2022 19:48:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.090292 on epoch=189
06/24/2022 19:48:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.054683 on epoch=194
06/24/2022 19:48:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.102190 on epoch=199
06/24/2022 19:48:18 - INFO - __main__ - Global step 400 Train loss 0.141880 Classification-F1 0.4554554554554554 on epoch=199
06/24/2022 19:48:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.112474 on epoch=204
06/24/2022 19:48:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.194737 on epoch=209
06/24/2022 19:48:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.089241 on epoch=214
06/24/2022 19:48:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.014979 on epoch=219
06/24/2022 19:48:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.013099 on epoch=224
06/24/2022 19:48:30 - INFO - __main__ - Global step 450 Train loss 0.084906 Classification-F1 0.4817813765182186 on epoch=224
06/24/2022 19:48:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.205569 on epoch=229
06/24/2022 19:48:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.031146 on epoch=234
06/24/2022 19:48:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.024180 on epoch=239
06/24/2022 19:48:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.022113 on epoch=244
06/24/2022 19:48:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.112653 on epoch=249
06/24/2022 19:48:43 - INFO - __main__ - Global step 500 Train loss 0.079132 Classification-F1 0.40566959921798634 on epoch=249
06/24/2022 19:48:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.113925 on epoch=254
06/24/2022 19:48:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000163 on epoch=259
06/24/2022 19:48:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.121624 on epoch=264
06/24/2022 19:48:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.089117 on epoch=269
06/24/2022 19:48:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.256208 on epoch=274
06/24/2022 19:48:55 - INFO - __main__ - Global step 550 Train loss 0.116207 Classification-F1 0.4980392156862745 on epoch=274
06/24/2022 19:48:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.023881 on epoch=279
06/24/2022 19:49:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.043260 on epoch=284
06/24/2022 19:49:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.005349 on epoch=289
06/24/2022 19:49:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.051746 on epoch=294
06/24/2022 19:49:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.032114 on epoch=299
06/24/2022 19:49:08 - INFO - __main__ - Global step 600 Train loss 0.031270 Classification-F1 0.4009852216748768 on epoch=299
06/24/2022 19:49:08 - INFO - __main__ - save last model!
06/24/2022 19:49:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:49:09 - INFO - __main__ - Printing 3 examples
06/24/2022 19:49:09 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:49:09 - INFO - __main__ - ['1']
06/24/2022 19:49:09 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:49:09 - INFO - __main__ - ['1']
06/24/2022 19:49:09 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:49:09 - INFO - __main__ - ['1']
06/24/2022 19:49:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:49:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:49:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:49:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:49:09 - INFO - __main__ - Printing 3 examples
06/24/2022 19:49:09 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:49:09 - INFO - __main__ - ['1']
06/24/2022 19:49:09 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:49:09 - INFO - __main__ - ['1']
06/24/2022 19:49:09 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:49:09 - INFO - __main__ - ['1']
06/24/2022 19:49:09 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:49:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:49:09 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:49:11 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:49:11 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:49:11 - INFO - __main__ - Printing 3 examples
06/24/2022 19:49:11 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:49:11 - INFO - __main__ - ['0']
06/24/2022 19:49:11 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:49:11 - INFO - __main__ - ['1']
06/24/2022 19:49:11 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:49:11 - INFO - __main__ - ['1']
06/24/2022 19:49:11 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:49:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:49:13 - INFO - __main__ - Starting training!
06/24/2022 19:49:15 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:49:23 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:50:22 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_42_0.0003_8_predictions.txt
06/24/2022 19:50:23 - INFO - __main__ - Classification-F1 on test data: 0.5130
06/24/2022 19:50:23 - INFO - __main__ - prefix=paws_16_42, lr=0.0003, bsz=8, dev_performance=0.4980392156862745, test_performance=0.5129683411450097
06/24/2022 19:50:23 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.0002, bsz=8 ...
06/24/2022 19:50:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:50:24 - INFO - __main__ - Printing 3 examples
06/24/2022 19:50:24 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:50:24 - INFO - __main__ - ['1']
06/24/2022 19:50:24 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:50:24 - INFO - __main__ - ['1']
06/24/2022 19:50:24 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:50:24 - INFO - __main__ - ['1']
06/24/2022 19:50:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:50:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:50:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:50:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:50:24 - INFO - __main__ - Printing 3 examples
06/24/2022 19:50:24 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:50:24 - INFO - __main__ - ['1']
06/24/2022 19:50:24 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:50:24 - INFO - __main__ - ['1']
06/24/2022 19:50:24 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:50:24 - INFO - __main__ - ['1']
06/24/2022 19:50:24 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:50:24 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:50:24 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:50:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:50:28 - INFO - __main__ - Starting training!
06/24/2022 19:50:30 - INFO - __main__ - Step 10 Global step 10 Train loss 17.851278 on epoch=4
06/24/2022 19:50:33 - INFO - __main__ - Step 20 Global step 20 Train loss 15.504946 on epoch=9
06/24/2022 19:50:35 - INFO - __main__ - Step 30 Global step 30 Train loss 11.111494 on epoch=14
06/24/2022 19:50:38 - INFO - __main__ - Step 40 Global step 40 Train loss 8.841539 on epoch=19
06/24/2022 19:50:40 - INFO - __main__ - Step 50 Global step 50 Train loss 7.882529 on epoch=24
06/24/2022 19:50:41 - INFO - __main__ - Global step 50 Train loss 12.238357 Classification-F1 0.08125 on epoch=24
06/24/2022 19:50:43 - INFO - __main__ - Step 60 Global step 60 Train loss 6.318281 on epoch=29
06/24/2022 19:50:46 - INFO - __main__ - Step 70 Global step 70 Train loss 6.618601 on epoch=34
06/24/2022 19:50:48 - INFO - __main__ - Step 80 Global step 80 Train loss 3.784927 on epoch=39
06/24/2022 19:50:51 - INFO - __main__ - Step 90 Global step 90 Train loss 3.061241 on epoch=44
06/24/2022 19:50:53 - INFO - __main__ - Step 100 Global step 100 Train loss 2.039476 on epoch=49
06/24/2022 19:50:54 - INFO - __main__ - Global step 100 Train loss 4.364505 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:50:57 - INFO - __main__ - Step 110 Global step 110 Train loss 2.844899 on epoch=54
06/24/2022 19:50:59 - INFO - __main__ - Step 120 Global step 120 Train loss 2.104368 on epoch=59
06/24/2022 19:51:02 - INFO - __main__ - Step 130 Global step 130 Train loss 2.287822 on epoch=64
06/24/2022 19:51:04 - INFO - __main__ - Step 140 Global step 140 Train loss 2.253255 on epoch=69
06/24/2022 19:51:07 - INFO - __main__ - Step 150 Global step 150 Train loss 1.880979 on epoch=74
06/24/2022 19:51:07 - INFO - __main__ - Global step 150 Train loss 2.274265 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:51:09 - INFO - __main__ - Step 160 Global step 160 Train loss 2.432750 on epoch=79
06/24/2022 19:51:12 - INFO - __main__ - Step 170 Global step 170 Train loss 1.427901 on epoch=84
06/24/2022 19:51:14 - INFO - __main__ - Step 180 Global step 180 Train loss 1.950796 on epoch=89
06/24/2022 19:51:17 - INFO - __main__ - Step 190 Global step 190 Train loss 1.460444 on epoch=94
06/24/2022 19:51:20 - INFO - __main__ - Step 200 Global step 200 Train loss 1.171338 on epoch=99
06/24/2022 19:51:20 - INFO - __main__ - Global step 200 Train loss 1.688646 Classification-F1 0.5307917888563051 on epoch=99
06/24/2022 19:51:23 - INFO - __main__ - Step 210 Global step 210 Train loss 1.096462 on epoch=104
06/24/2022 19:51:25 - INFO - __main__ - Step 220 Global step 220 Train loss 1.612038 on epoch=109
06/24/2022 19:51:28 - INFO - __main__ - Step 230 Global step 230 Train loss 1.053031 on epoch=114
06/24/2022 19:51:30 - INFO - __main__ - Step 240 Global step 240 Train loss 1.246426 on epoch=119
06/24/2022 19:51:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.782562 on epoch=124
06/24/2022 19:51:33 - INFO - __main__ - Global step 250 Train loss 1.158104 Classification-F1 0.3816425120772947 on epoch=124
06/24/2022 19:51:35 - INFO - __main__ - Step 260 Global step 260 Train loss 1.030382 on epoch=129
06/24/2022 19:51:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.954537 on epoch=134
06/24/2022 19:51:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.818372 on epoch=139
06/24/2022 19:51:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.962917 on epoch=144
06/24/2022 19:51:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.762413 on epoch=149
06/24/2022 19:51:46 - INFO - __main__ - Global step 300 Train loss 0.905724 Classification-F1 0.4285714285714286 on epoch=149
06/24/2022 19:51:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.899073 on epoch=154
06/24/2022 19:51:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.708256 on epoch=159
06/24/2022 19:51:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.734953 on epoch=164
06/24/2022 19:51:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.870285 on epoch=169
06/24/2022 19:51:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.523443 on epoch=174
06/24/2022 19:51:59 - INFO - __main__ - Global step 350 Train loss 0.747202 Classification-F1 0.4458874458874459 on epoch=174
06/24/2022 19:52:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.675477 on epoch=179
06/24/2022 19:52:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.611644 on epoch=184
06/24/2022 19:52:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.668930 on epoch=189
06/24/2022 19:52:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.401790 on epoch=194
06/24/2022 19:52:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.473429 on epoch=199
06/24/2022 19:52:12 - INFO - __main__ - Global step 400 Train loss 0.566254 Classification-F1 0.4554554554554554 on epoch=199
06/24/2022 19:52:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.380026 on epoch=204
06/24/2022 19:52:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.314176 on epoch=209
06/24/2022 19:52:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.365109 on epoch=214
06/24/2022 19:52:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.211138 on epoch=219
06/24/2022 19:52:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.251382 on epoch=224
06/24/2022 19:52:24 - INFO - __main__ - Global step 450 Train loss 0.304366 Classification-F1 0.4554554554554554 on epoch=224
06/24/2022 19:52:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.258097 on epoch=229
06/24/2022 19:52:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.189710 on epoch=234
06/24/2022 19:52:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.109103 on epoch=239
06/24/2022 19:52:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.228926 on epoch=244
06/24/2022 19:52:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.111600 on epoch=249
06/24/2022 19:52:37 - INFO - __main__ - Global step 500 Train loss 0.179487 Classification-F1 0.39139139139139134 on epoch=249
06/24/2022 19:52:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.272710 on epoch=254
06/24/2022 19:52:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.096440 on epoch=259
06/24/2022 19:52:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.183630 on epoch=264
06/24/2022 19:52:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.089816 on epoch=269
06/24/2022 19:52:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.018733 on epoch=274
06/24/2022 19:52:50 - INFO - __main__ - Global step 550 Train loss 0.132266 Classification-F1 0.43529411764705883 on epoch=274
06/24/2022 19:52:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.072256 on epoch=279
06/24/2022 19:52:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.030310 on epoch=284
06/24/2022 19:52:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.043136 on epoch=289
06/24/2022 19:53:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.146801 on epoch=294
06/24/2022 19:53:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.084742 on epoch=299
06/24/2022 19:53:03 - INFO - __main__ - Global step 600 Train loss 0.075449 Classification-F1 0.4009852216748768 on epoch=299
06/24/2022 19:53:03 - INFO - __main__ - save last model!
06/24/2022 19:53:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:53:04 - INFO - __main__ - Printing 3 examples
06/24/2022 19:53:04 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:53:04 - INFO - __main__ - ['1']
06/24/2022 19:53:04 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:53:04 - INFO - __main__ - ['1']
06/24/2022 19:53:04 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:53:04 - INFO - __main__ - ['1']
06/24/2022 19:53:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:53:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:53:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:53:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:53:04 - INFO - __main__ - Printing 3 examples
06/24/2022 19:53:04 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:53:04 - INFO - __main__ - ['1']
06/24/2022 19:53:04 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:53:04 - INFO - __main__ - ['1']
06/24/2022 19:53:04 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:53:04 - INFO - __main__ - ['1']
06/24/2022 19:53:04 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:53:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:53:04 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:53:06 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:53:06 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:53:06 - INFO - __main__ - Printing 3 examples
06/24/2022 19:53:06 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:53:06 - INFO - __main__ - ['0']
06/24/2022 19:53:06 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:53:06 - INFO - __main__ - ['1']
06/24/2022 19:53:06 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:53:06 - INFO - __main__ - ['1']
06/24/2022 19:53:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:53:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:53:07 - INFO - __main__ - Starting training!
06/24/2022 19:53:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:53:18 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:54:17 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_42_0.0002_8_predictions.txt
06/24/2022 19:54:17 - INFO - __main__ - Classification-F1 on test data: 0.5129
06/24/2022 19:54:17 - INFO - __main__ - prefix=paws_16_42, lr=0.0002, bsz=8, dev_performance=0.5307917888563051, test_performance=0.5129098200691231
06/24/2022 19:54:17 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.0001, bsz=8 ...
06/24/2022 19:54:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:54:18 - INFO - __main__ - Printing 3 examples
06/24/2022 19:54:18 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 19:54:18 - INFO - __main__ - ['1']
06/24/2022 19:54:18 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 19:54:18 - INFO - __main__ - ['1']
06/24/2022 19:54:18 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 19:54:18 - INFO - __main__ - ['1']
06/24/2022 19:54:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:54:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:54:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:54:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:54:18 - INFO - __main__ - Printing 3 examples
06/24/2022 19:54:18 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 19:54:18 - INFO - __main__ - ['1']
06/24/2022 19:54:18 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 19:54:18 - INFO - __main__ - ['1']
06/24/2022 19:54:18 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 19:54:18 - INFO - __main__ - ['1']
06/24/2022 19:54:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:54:18 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:54:18 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:54:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:54:22 - INFO - __main__ - Starting training!
06/24/2022 19:54:24 - INFO - __main__ - Step 10 Global step 10 Train loss 18.557276 on epoch=4
06/24/2022 19:54:26 - INFO - __main__ - Step 20 Global step 20 Train loss 18.294590 on epoch=9
06/24/2022 19:54:28 - INFO - __main__ - Step 30 Global step 30 Train loss 13.522710 on epoch=14
06/24/2022 19:54:31 - INFO - __main__ - Step 40 Global step 40 Train loss 10.957727 on epoch=19
06/24/2022 19:54:33 - INFO - __main__ - Step 50 Global step 50 Train loss 8.933477 on epoch=24
06/24/2022 19:54:35 - INFO - __main__ - Global step 50 Train loss 14.053156 Classification-F1 0.02888888888888889 on epoch=24
06/24/2022 19:54:38 - INFO - __main__ - Step 60 Global step 60 Train loss 8.450672 on epoch=29
06/24/2022 19:54:40 - INFO - __main__ - Step 70 Global step 70 Train loss 7.614158 on epoch=34
06/24/2022 19:54:43 - INFO - __main__ - Step 80 Global step 80 Train loss 7.283175 on epoch=39
06/24/2022 19:54:45 - INFO - __main__ - Step 90 Global step 90 Train loss 6.686679 on epoch=44
06/24/2022 19:54:47 - INFO - __main__ - Step 100 Global step 100 Train loss 6.988896 on epoch=49
06/24/2022 19:54:48 - INFO - __main__ - Global step 100 Train loss 7.404716 Classification-F1 0.042539682539682544 on epoch=49
06/24/2022 19:54:51 - INFO - __main__ - Step 110 Global step 110 Train loss 6.046941 on epoch=54
06/24/2022 19:54:53 - INFO - __main__ - Step 120 Global step 120 Train loss 5.188281 on epoch=59
06/24/2022 19:54:56 - INFO - __main__ - Step 130 Global step 130 Train loss 5.343556 on epoch=64
06/24/2022 19:54:58 - INFO - __main__ - Step 140 Global step 140 Train loss 4.781535 on epoch=69
06/24/2022 19:55:00 - INFO - __main__ - Step 150 Global step 150 Train loss 4.159587 on epoch=74
06/24/2022 19:55:01 - INFO - __main__ - Global step 150 Train loss 5.103980 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:55:03 - INFO - __main__ - Step 160 Global step 160 Train loss 3.429219 on epoch=79
06/24/2022 19:55:06 - INFO - __main__ - Step 170 Global step 170 Train loss 2.979606 on epoch=84
06/24/2022 19:55:08 - INFO - __main__ - Step 180 Global step 180 Train loss 3.223781 on epoch=89
06/24/2022 19:55:11 - INFO - __main__ - Step 190 Global step 190 Train loss 2.902179 on epoch=94
06/24/2022 19:55:13 - INFO - __main__ - Step 200 Global step 200 Train loss 2.432151 on epoch=99
06/24/2022 19:55:14 - INFO - __main__ - Global step 200 Train loss 2.993387 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 19:55:16 - INFO - __main__ - Step 210 Global step 210 Train loss 2.171896 on epoch=104
06/24/2022 19:55:18 - INFO - __main__ - Step 220 Global step 220 Train loss 2.757163 on epoch=109
06/24/2022 19:55:21 - INFO - __main__ - Step 230 Global step 230 Train loss 1.465657 on epoch=114
06/24/2022 19:55:23 - INFO - __main__ - Step 240 Global step 240 Train loss 1.855921 on epoch=119
06/24/2022 19:55:26 - INFO - __main__ - Step 250 Global step 250 Train loss 1.904064 on epoch=124
06/24/2022 19:55:26 - INFO - __main__ - Global step 250 Train loss 2.030940 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:55:28 - INFO - __main__ - Step 260 Global step 260 Train loss 1.724907 on epoch=129
06/24/2022 19:55:31 - INFO - __main__ - Step 270 Global step 270 Train loss 2.288544 on epoch=134
06/24/2022 19:55:33 - INFO - __main__ - Step 280 Global step 280 Train loss 2.282082 on epoch=139
06/24/2022 19:55:36 - INFO - __main__ - Step 290 Global step 290 Train loss 1.392456 on epoch=144
06/24/2022 19:55:38 - INFO - __main__ - Step 300 Global step 300 Train loss 1.558609 on epoch=149
06/24/2022 19:55:38 - INFO - __main__ - Global step 300 Train loss 1.849319 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 19:55:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.835732 on epoch=154
06/24/2022 19:55:43 - INFO - __main__ - Step 320 Global step 320 Train loss 1.277493 on epoch=159
06/24/2022 19:55:46 - INFO - __main__ - Step 330 Global step 330 Train loss 1.348766 on epoch=164
06/24/2022 19:55:48 - INFO - __main__ - Step 340 Global step 340 Train loss 1.288175 on epoch=169
06/24/2022 19:55:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.812210 on epoch=174
06/24/2022 19:55:51 - INFO - __main__ - Global step 350 Train loss 1.512475 Classification-F1 0.3191489361702127 on epoch=174
06/24/2022 19:55:53 - INFO - __main__ - Step 360 Global step 360 Train loss 1.598080 on epoch=179
06/24/2022 19:55:56 - INFO - __main__ - Step 370 Global step 370 Train loss 1.654667 on epoch=184
06/24/2022 19:55:58 - INFO - __main__ - Step 380 Global step 380 Train loss 1.192274 on epoch=189
06/24/2022 19:56:01 - INFO - __main__ - Step 390 Global step 390 Train loss 1.187700 on epoch=194
06/24/2022 19:56:03 - INFO - __main__ - Step 400 Global step 400 Train loss 1.374914 on epoch=199
06/24/2022 19:56:03 - INFO - __main__ - Global step 400 Train loss 1.401527 Classification-F1 0.46843853820598 on epoch=199
06/24/2022 19:56:06 - INFO - __main__ - Step 410 Global step 410 Train loss 1.329824 on epoch=204
06/24/2022 19:56:09 - INFO - __main__ - Step 420 Global step 420 Train loss 1.346727 on epoch=209
06/24/2022 19:56:11 - INFO - __main__ - Step 430 Global step 430 Train loss 1.261116 on epoch=214
06/24/2022 19:56:14 - INFO - __main__ - Step 440 Global step 440 Train loss 1.399519 on epoch=219
06/24/2022 19:56:16 - INFO - __main__ - Step 450 Global step 450 Train loss 1.017519 on epoch=224
06/24/2022 19:56:16 - INFO - __main__ - Global step 450 Train loss 1.270941 Classification-F1 0.5 on epoch=224
06/24/2022 19:56:19 - INFO - __main__ - Step 460 Global step 460 Train loss 1.043504 on epoch=229
06/24/2022 19:56:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.827864 on epoch=234
06/24/2022 19:56:24 - INFO - __main__ - Step 480 Global step 480 Train loss 1.380969 on epoch=239
06/24/2022 19:56:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.861887 on epoch=244
06/24/2022 19:56:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.892295 on epoch=249
06/24/2022 19:56:29 - INFO - __main__ - Global step 500 Train loss 1.001304 Classification-F1 0.5333333333333333 on epoch=249
06/24/2022 19:56:32 - INFO - __main__ - Step 510 Global step 510 Train loss 1.088401 on epoch=254
06/24/2022 19:56:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.881608 on epoch=259
06/24/2022 19:56:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.784313 on epoch=264
06/24/2022 19:56:40 - INFO - __main__ - Step 540 Global step 540 Train loss 1.229812 on epoch=269
06/24/2022 19:56:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.888308 on epoch=274
06/24/2022 19:56:43 - INFO - __main__ - Global step 550 Train loss 0.974488 Classification-F1 0.3191489361702127 on epoch=274
06/24/2022 19:56:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.917416 on epoch=279
06/24/2022 19:56:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.956340 on epoch=284
06/24/2022 19:56:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.603555 on epoch=289
06/24/2022 19:56:53 - INFO - __main__ - Step 590 Global step 590 Train loss 1.068368 on epoch=294
06/24/2022 19:56:55 - INFO - __main__ - Step 600 Global step 600 Train loss 1.037902 on epoch=299
06/24/2022 19:56:56 - INFO - __main__ - Global step 600 Train loss 0.916716 Classification-F1 0.3191489361702127 on epoch=299
06/24/2022 19:56:56 - INFO - __main__ - save last model!
06/24/2022 19:56:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:56:56 - INFO - __main__ - Printing 3 examples
06/24/2022 19:56:56 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 19:56:56 - INFO - __main__ - ['0']
06/24/2022 19:56:56 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 19:56:56 - INFO - __main__ - ['0']
06/24/2022 19:56:56 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 19:56:56 - INFO - __main__ - ['0']
06/24/2022 19:56:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 19:56:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:56:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:56:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:56:57 - INFO - __main__ - Printing 3 examples
06/24/2022 19:56:57 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 19:56:57 - INFO - __main__ - ['0']
06/24/2022 19:56:57 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 19:56:57 - INFO - __main__ - ['0']
06/24/2022 19:56:57 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 19:56:57 - INFO - __main__ - ['0']
06/24/2022 19:56:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:56:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:56:57 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:56:58 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 19:56:59 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 19:56:59 - INFO - __main__ - Printing 3 examples
06/24/2022 19:56:59 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 19:56:59 - INFO - __main__ - ['0']
06/24/2022 19:56:59 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 19:56:59 - INFO - __main__ - ['1']
06/24/2022 19:56:59 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 19:56:59 - INFO - __main__ - ['1']
06/24/2022 19:56:59 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:57:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:57:00 - INFO - __main__ - Starting training!
06/24/2022 19:57:03 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:57:10 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 19:58:11 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_42_0.0001_8_predictions.txt
06/24/2022 19:58:11 - INFO - __main__ - Classification-F1 on test data: 0.4823
06/24/2022 19:58:11 - INFO - __main__ - prefix=paws_16_42, lr=0.0001, bsz=8, dev_performance=0.5333333333333333, test_performance=0.48225621076699576
06/24/2022 19:58:11 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.0005, bsz=8 ...
06/24/2022 19:58:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:58:12 - INFO - __main__ - Printing 3 examples
06/24/2022 19:58:12 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 19:58:12 - INFO - __main__ - ['0']
06/24/2022 19:58:12 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 19:58:12 - INFO - __main__ - ['0']
06/24/2022 19:58:12 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 19:58:12 - INFO - __main__ - ['0']
06/24/2022 19:58:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 19:58:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:58:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 19:58:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 19:58:12 - INFO - __main__ - Printing 3 examples
06/24/2022 19:58:12 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 19:58:12 - INFO - __main__ - ['0']
06/24/2022 19:58:12 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 19:58:12 - INFO - __main__ - ['0']
06/24/2022 19:58:12 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 19:58:12 - INFO - __main__ - ['0']
06/24/2022 19:58:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 19:58:13 - INFO - __main__ - Tokenizing Output ...
06/24/2022 19:58:13 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 19:58:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 19:58:17 - INFO - __main__ - Starting training!
06/24/2022 19:58:19 - INFO - __main__ - Step 10 Global step 10 Train loss 17.859043 on epoch=4
06/24/2022 19:58:21 - INFO - __main__ - Step 20 Global step 20 Train loss 15.006889 on epoch=9
06/24/2022 19:58:24 - INFO - __main__ - Step 30 Global step 30 Train loss 7.998172 on epoch=14
06/24/2022 19:58:26 - INFO - __main__ - Step 40 Global step 40 Train loss 4.753346 on epoch=19
06/24/2022 19:58:29 - INFO - __main__ - Step 50 Global step 50 Train loss 3.231876 on epoch=24
06/24/2022 19:58:29 - INFO - __main__ - Global step 50 Train loss 9.769865 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 19:58:32 - INFO - __main__ - Step 60 Global step 60 Train loss 3.102000 on epoch=29
06/24/2022 19:58:34 - INFO - __main__ - Step 70 Global step 70 Train loss 2.131850 on epoch=34
06/24/2022 19:58:37 - INFO - __main__ - Step 80 Global step 80 Train loss 2.029642 on epoch=39
06/24/2022 19:58:39 - INFO - __main__ - Step 90 Global step 90 Train loss 1.796295 on epoch=44
06/24/2022 19:58:42 - INFO - __main__ - Step 100 Global step 100 Train loss 1.786660 on epoch=49
06/24/2022 19:58:42 - INFO - __main__ - Global step 100 Train loss 2.169289 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 19:58:45 - INFO - __main__ - Step 110 Global step 110 Train loss 1.242304 on epoch=54
06/24/2022 19:58:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.814367 on epoch=59
06/24/2022 19:58:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.961770 on epoch=64
06/24/2022 19:58:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.817905 on epoch=69
06/24/2022 19:58:55 - INFO - __main__ - Step 150 Global step 150 Train loss 1.221562 on epoch=74
06/24/2022 19:58:55 - INFO - __main__ - Global step 150 Train loss 1.011582 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 19:58:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.989435 on epoch=79
06/24/2022 19:59:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.923997 on epoch=84
06/24/2022 19:59:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.810751 on epoch=89
06/24/2022 19:59:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.817802 on epoch=94
06/24/2022 19:59:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.626176 on epoch=99
06/24/2022 19:59:08 - INFO - __main__ - Global step 200 Train loss 0.833632 Classification-F1 0.3992490613266583 on epoch=99
06/24/2022 19:59:11 - INFO - __main__ - Step 210 Global step 210 Train loss 1.051739 on epoch=104
06/24/2022 19:59:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.973524 on epoch=109
06/24/2022 19:59:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.729989 on epoch=114
06/24/2022 19:59:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.915619 on epoch=119
06/24/2022 19:59:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.638589 on epoch=124
06/24/2022 19:59:22 - INFO - __main__ - Global step 250 Train loss 0.861892 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 19:59:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.773546 on epoch=129
06/24/2022 19:59:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.969468 on epoch=134
06/24/2022 19:59:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.892037 on epoch=139
06/24/2022 19:59:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.821866 on epoch=144
06/24/2022 19:59:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.586067 on epoch=149
06/24/2022 19:59:35 - INFO - __main__ - Global step 300 Train loss 0.808597 Classification-F1 0.3191489361702127 on epoch=149
06/24/2022 19:59:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.737805 on epoch=154
06/24/2022 19:59:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.704131 on epoch=159
06/24/2022 19:59:42 - INFO - __main__ - Step 330 Global step 330 Train loss 1.006131 on epoch=164
06/24/2022 19:59:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.842118 on epoch=169
06/24/2022 19:59:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.615074 on epoch=174
06/24/2022 19:59:48 - INFO - __main__ - Global step 350 Train loss 0.781052 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 19:59:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.693873 on epoch=179
06/24/2022 19:59:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.572551 on epoch=184
06/24/2022 19:59:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.731572 on epoch=189
06/24/2022 19:59:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.644168 on epoch=194
06/24/2022 20:00:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.586765 on epoch=199
06/24/2022 20:00:01 - INFO - __main__ - Global step 400 Train loss 0.645786 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 20:00:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.637972 on epoch=204
06/24/2022 20:00:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.584678 on epoch=209
06/24/2022 20:00:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.616338 on epoch=214
06/24/2022 20:00:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.556887 on epoch=219
06/24/2022 20:00:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.576462 on epoch=224
06/24/2022 20:00:14 - INFO - __main__ - Global step 450 Train loss 0.594468 Classification-F1 0.5270935960591133 on epoch=224
06/24/2022 20:00:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.659956 on epoch=229
06/24/2022 20:00:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.359030 on epoch=234
06/24/2022 20:00:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.447731 on epoch=239
06/24/2022 20:00:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.399155 on epoch=244
06/24/2022 20:00:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.437029 on epoch=249
06/24/2022 20:00:27 - INFO - __main__ - Global step 500 Train loss 0.460580 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 20:00:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.365010 on epoch=254
06/24/2022 20:00:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.409087 on epoch=259
06/24/2022 20:00:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.369446 on epoch=264
06/24/2022 20:00:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.400745 on epoch=269
06/24/2022 20:00:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.414147 on epoch=274
06/24/2022 20:00:40 - INFO - __main__ - Global step 550 Train loss 0.391687 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 20:00:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.451596 on epoch=279
06/24/2022 20:00:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.394467 on epoch=284
06/24/2022 20:00:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.351680 on epoch=289
06/24/2022 20:00:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.330711 on epoch=294
06/24/2022 20:00:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.309988 on epoch=299
06/24/2022 20:00:54 - INFO - __main__ - Global step 600 Train loss 0.367688 Classification-F1 0.3333333333333333 on epoch=299
06/24/2022 20:00:54 - INFO - __main__ - save last model!
06/24/2022 20:00:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:00:54 - INFO - __main__ - Printing 3 examples
06/24/2022 20:00:54 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 20:00:54 - INFO - __main__ - ['0']
06/24/2022 20:00:54 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 20:00:54 - INFO - __main__ - ['0']
06/24/2022 20:00:54 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 20:00:54 - INFO - __main__ - ['0']
06/24/2022 20:00:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 20:00:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:00:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 20:00:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:00:54 - INFO - __main__ - Printing 3 examples
06/24/2022 20:00:54 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 20:00:54 - INFO - __main__ - ['0']
06/24/2022 20:00:54 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 20:00:54 - INFO - __main__ - ['0']
06/24/2022 20:00:54 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 20:00:54 - INFO - __main__ - ['0']
06/24/2022 20:00:54 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:00:54 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:00:54 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 20:00:57 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 20:00:57 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 20:00:57 - INFO - __main__ - Printing 3 examples
06/24/2022 20:00:57 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 20:00:57 - INFO - __main__ - ['0']
06/24/2022 20:00:57 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 20:00:57 - INFO - __main__ - ['1']
06/24/2022 20:00:57 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 20:00:57 - INFO - __main__ - ['1']
06/24/2022 20:00:57 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 20:00:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 20:00:58 - INFO - __main__ - Starting training!
06/24/2022 20:01:01 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:01:09 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 20:02:07 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_87_0.0005_8_predictions.txt
06/24/2022 20:02:07 - INFO - __main__ - Classification-F1 on test data: 0.4960
06/24/2022 20:02:07 - INFO - __main__ - prefix=paws_16_87, lr=0.0005, bsz=8, dev_performance=0.5270935960591133, test_performance=0.4960101252214757
06/24/2022 20:02:07 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.0003, bsz=8 ...
06/24/2022 20:02:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:02:08 - INFO - __main__ - Printing 3 examples
06/24/2022 20:02:08 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 20:02:08 - INFO - __main__ - ['0']
06/24/2022 20:02:08 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 20:02:08 - INFO - __main__ - ['0']
06/24/2022 20:02:08 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 20:02:08 - INFO - __main__ - ['0']
06/24/2022 20:02:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 20:02:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:02:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 20:02:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:02:08 - INFO - __main__ - Printing 3 examples
06/24/2022 20:02:08 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 20:02:08 - INFO - __main__ - ['0']
06/24/2022 20:02:08 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 20:02:08 - INFO - __main__ - ['0']
06/24/2022 20:02:08 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 20:02:08 - INFO - __main__ - ['0']
06/24/2022 20:02:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:02:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:02:08 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 20:02:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 20:02:12 - INFO - __main__ - Starting training!
06/24/2022 20:02:14 - INFO - __main__ - Step 10 Global step 10 Train loss 18.225842 on epoch=4
06/24/2022 20:02:17 - INFO - __main__ - Step 20 Global step 20 Train loss 13.344610 on epoch=9
06/24/2022 20:02:19 - INFO - __main__ - Step 30 Global step 30 Train loss 9.280280 on epoch=14
06/24/2022 20:02:22 - INFO - __main__ - Step 40 Global step 40 Train loss 6.520228 on epoch=19
06/24/2022 20:02:24 - INFO - __main__ - Step 50 Global step 50 Train loss 6.311319 on epoch=24
06/24/2022 20:02:24 - INFO - __main__ - Global step 50 Train loss 10.736455 Classification-F1 0.3992490613266583 on epoch=24
06/24/2022 20:02:27 - INFO - __main__ - Step 60 Global step 60 Train loss 3.786195 on epoch=29
06/24/2022 20:02:30 - INFO - __main__ - Step 70 Global step 70 Train loss 3.719945 on epoch=34
06/24/2022 20:02:32 - INFO - __main__ - Step 80 Global step 80 Train loss 3.598518 on epoch=39
06/24/2022 20:02:35 - INFO - __main__ - Step 90 Global step 90 Train loss 1.741688 on epoch=44
06/24/2022 20:02:38 - INFO - __main__ - Step 100 Global step 100 Train loss 2.130688 on epoch=49
06/24/2022 20:02:38 - INFO - __main__ - Global step 100 Train loss 2.995407 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 20:02:41 - INFO - __main__ - Step 110 Global step 110 Train loss 1.496953 on epoch=54
06/24/2022 20:02:43 - INFO - __main__ - Step 120 Global step 120 Train loss 2.168131 on epoch=59
06/24/2022 20:02:46 - INFO - __main__ - Step 130 Global step 130 Train loss 1.757914 on epoch=64
06/24/2022 20:02:49 - INFO - __main__ - Step 140 Global step 140 Train loss 2.018715 on epoch=69
06/24/2022 20:02:51 - INFO - __main__ - Step 150 Global step 150 Train loss 1.710997 on epoch=74
06/24/2022 20:02:51 - INFO - __main__ - Global step 150 Train loss 1.830542 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 20:02:54 - INFO - __main__ - Step 160 Global step 160 Train loss 1.228947 on epoch=79
06/24/2022 20:02:56 - INFO - __main__ - Step 170 Global step 170 Train loss 1.407626 on epoch=84
06/24/2022 20:02:59 - INFO - __main__ - Step 180 Global step 180 Train loss 1.207727 on epoch=89
06/24/2022 20:03:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.824724 on epoch=94
06/24/2022 20:03:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.887260 on epoch=99
06/24/2022 20:03:04 - INFO - __main__ - Global step 200 Train loss 1.111257 Classification-F1 0.3043478260869565 on epoch=99
06/24/2022 20:03:07 - INFO - __main__ - Step 210 Global step 210 Train loss 1.367212 on epoch=104
06/24/2022 20:03:09 - INFO - __main__ - Step 220 Global step 220 Train loss 1.227192 on epoch=109
06/24/2022 20:03:12 - INFO - __main__ - Step 230 Global step 230 Train loss 1.160066 on epoch=114
06/24/2022 20:03:14 - INFO - __main__ - Step 240 Global step 240 Train loss 1.125609 on epoch=119
06/24/2022 20:03:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.862723 on epoch=124
06/24/2022 20:03:17 - INFO - __main__ - Global step 250 Train loss 1.148561 Classification-F1 0.4920634920634921 on epoch=124
06/24/2022 20:03:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.764751 on epoch=129
06/24/2022 20:03:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.922346 on epoch=134
06/24/2022 20:03:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.950643 on epoch=139
06/24/2022 20:03:28 - INFO - __main__ - Step 290 Global step 290 Train loss 1.140175 on epoch=144
06/24/2022 20:03:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.918683 on epoch=149
06/24/2022 20:03:31 - INFO - __main__ - Global step 300 Train loss 0.939319 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 20:03:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.972561 on epoch=154
06/24/2022 20:03:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.951689 on epoch=159
06/24/2022 20:03:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.831334 on epoch=164
06/24/2022 20:03:41 - INFO - __main__ - Step 340 Global step 340 Train loss 1.005059 on epoch=169
06/24/2022 20:03:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.799221 on epoch=174
06/24/2022 20:03:44 - INFO - __main__ - Global step 350 Train loss 0.911973 Classification-F1 0.4920634920634921 on epoch=174
06/24/2022 20:03:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.882938 on epoch=179
06/24/2022 20:03:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.959942 on epoch=184
06/24/2022 20:03:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.473711 on epoch=189
06/24/2022 20:03:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.709475 on epoch=194
06/24/2022 20:03:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.786066 on epoch=199
06/24/2022 20:03:57 - INFO - __main__ - Global step 400 Train loss 0.762427 Classification-F1 0.3992490613266583 on epoch=199
06/24/2022 20:03:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.844013 on epoch=204
06/24/2022 20:04:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.628994 on epoch=209
06/24/2022 20:04:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.607219 on epoch=214
06/24/2022 20:04:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.704033 on epoch=219
06/24/2022 20:04:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.437128 on epoch=224
06/24/2022 20:04:10 - INFO - __main__ - Global step 450 Train loss 0.644277 Classification-F1 0.34310850439882695 on epoch=224
06/24/2022 20:04:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.699322 on epoch=229
06/24/2022 20:04:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.616526 on epoch=234
06/24/2022 20:04:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.428785 on epoch=239
06/24/2022 20:04:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.540092 on epoch=244
06/24/2022 20:04:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.534123 on epoch=249
06/24/2022 20:04:22 - INFO - __main__ - Global step 500 Train loss 0.563770 Classification-F1 0.4980392156862745 on epoch=249
06/24/2022 20:04:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.518935 on epoch=254
06/24/2022 20:04:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.637745 on epoch=259
06/24/2022 20:04:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.488528 on epoch=264
06/24/2022 20:04:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.433396 on epoch=269
06/24/2022 20:04:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.406900 on epoch=274
06/24/2022 20:04:36 - INFO - __main__ - Global step 550 Train loss 0.497101 Classification-F1 0.37254901960784315 on epoch=274
06/24/2022 20:04:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.422179 on epoch=279
06/24/2022 20:04:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.291620 on epoch=284
06/24/2022 20:04:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.211562 on epoch=289
06/24/2022 20:04:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.548309 on epoch=294
06/24/2022 20:04:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.422108 on epoch=299
06/24/2022 20:04:49 - INFO - __main__ - Global step 600 Train loss 0.379156 Classification-F1 0.4554554554554554 on epoch=299
06/24/2022 20:04:49 - INFO - __main__ - save last model!
06/24/2022 20:04:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:04:50 - INFO - __main__ - Printing 3 examples
06/24/2022 20:04:50 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 20:04:50 - INFO - __main__ - ['0']
06/24/2022 20:04:50 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 20:04:50 - INFO - __main__ - ['0']
06/24/2022 20:04:50 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 20:04:50 - INFO - __main__ - ['0']
06/24/2022 20:04:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 20:04:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:04:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 20:04:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:04:50 - INFO - __main__ - Printing 3 examples
06/24/2022 20:04:50 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 20:04:50 - INFO - __main__ - ['0']
06/24/2022 20:04:50 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 20:04:50 - INFO - __main__ - ['0']
06/24/2022 20:04:50 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 20:04:50 - INFO - __main__ - ['0']
06/24/2022 20:04:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:04:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:04:50 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 20:04:52 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 20:04:52 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 20:04:52 - INFO - __main__ - Printing 3 examples
06/24/2022 20:04:52 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 20:04:52 - INFO - __main__ - ['0']
06/24/2022 20:04:52 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 20:04:52 - INFO - __main__ - ['1']
06/24/2022 20:04:52 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 20:04:52 - INFO - __main__ - ['1']
06/24/2022 20:04:52 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 20:04:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 20:04:54 - INFO - __main__ - Starting training!
06/24/2022 20:04:56 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:05:04 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 20:06:03 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_87_0.0003_8_predictions.txt
06/24/2022 20:06:03 - INFO - __main__ - Classification-F1 on test data: 0.5090
06/24/2022 20:06:03 - INFO - __main__ - prefix=paws_16_87, lr=0.0003, bsz=8, dev_performance=0.4980392156862745, test_performance=0.508974710935986
06/24/2022 20:06:03 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.0002, bsz=8 ...
06/24/2022 20:06:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:06:04 - INFO - __main__ - Printing 3 examples
06/24/2022 20:06:04 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 20:06:04 - INFO - __main__ - ['0']
06/24/2022 20:06:04 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 20:06:04 - INFO - __main__ - ['0']
06/24/2022 20:06:04 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 20:06:04 - INFO - __main__ - ['0']
06/24/2022 20:06:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 20:06:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:06:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 20:06:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:06:04 - INFO - __main__ - Printing 3 examples
06/24/2022 20:06:04 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 20:06:04 - INFO - __main__ - ['0']
06/24/2022 20:06:04 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 20:06:04 - INFO - __main__ - ['0']
06/24/2022 20:06:04 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 20:06:04 - INFO - __main__ - ['0']
06/24/2022 20:06:04 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:06:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:06:04 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 20:06:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 20:06:08 - INFO - __main__ - Starting training!
06/24/2022 20:06:10 - INFO - __main__ - Step 10 Global step 10 Train loss 18.272266 on epoch=4
06/24/2022 20:06:12 - INFO - __main__ - Step 20 Global step 20 Train loss 15.873416 on epoch=9
06/24/2022 20:06:14 - INFO - __main__ - Step 30 Global step 30 Train loss 12.299232 on epoch=14
06/24/2022 20:06:17 - INFO - __main__ - Step 40 Global step 40 Train loss 9.629784 on epoch=19
06/24/2022 20:06:19 - INFO - __main__ - Step 50 Global step 50 Train loss 7.503863 on epoch=24
06/24/2022 20:06:20 - INFO - __main__ - Global step 50 Train loss 12.715712 Classification-F1 0.12903225806451615 on epoch=24
06/24/2022 20:06:23 - INFO - __main__ - Step 60 Global step 60 Train loss 6.649984 on epoch=29
06/24/2022 20:06:25 - INFO - __main__ - Step 70 Global step 70 Train loss 5.499284 on epoch=34
06/24/2022 20:06:28 - INFO - __main__ - Step 80 Global step 80 Train loss 4.948716 on epoch=39
06/24/2022 20:06:30 - INFO - __main__ - Step 90 Global step 90 Train loss 3.758120 on epoch=44
06/24/2022 20:06:33 - INFO - __main__ - Step 100 Global step 100 Train loss 3.719437 on epoch=49
06/24/2022 20:06:33 - INFO - __main__ - Global step 100 Train loss 4.915108 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 20:06:36 - INFO - __main__ - Step 110 Global step 110 Train loss 2.443645 on epoch=54
06/24/2022 20:06:39 - INFO - __main__ - Step 120 Global step 120 Train loss 2.979368 on epoch=59
06/24/2022 20:06:41 - INFO - __main__ - Step 130 Global step 130 Train loss 2.847932 on epoch=64
06/24/2022 20:06:44 - INFO - __main__ - Step 140 Global step 140 Train loss 2.053156 on epoch=69
06/24/2022 20:06:46 - INFO - __main__ - Step 150 Global step 150 Train loss 1.530029 on epoch=74
06/24/2022 20:06:47 - INFO - __main__ - Global step 150 Train loss 2.370826 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 20:06:49 - INFO - __main__ - Step 160 Global step 160 Train loss 2.485966 on epoch=79
06/24/2022 20:06:52 - INFO - __main__ - Step 170 Global step 170 Train loss 2.394474 on epoch=84
06/24/2022 20:06:54 - INFO - __main__ - Step 180 Global step 180 Train loss 2.121197 on epoch=89
06/24/2022 20:06:57 - INFO - __main__ - Step 190 Global step 190 Train loss 1.760669 on epoch=94
06/24/2022 20:06:59 - INFO - __main__ - Step 200 Global step 200 Train loss 1.457062 on epoch=99
06/24/2022 20:06:59 - INFO - __main__ - Global step 200 Train loss 2.043874 Classification-F1 0.3191489361702127 on epoch=99
06/24/2022 20:07:02 - INFO - __main__ - Step 210 Global step 210 Train loss 1.277126 on epoch=104
06/24/2022 20:07:04 - INFO - __main__ - Step 220 Global step 220 Train loss 1.533648 on epoch=109
06/24/2022 20:07:07 - INFO - __main__ - Step 230 Global step 230 Train loss 1.254734 on epoch=114
06/24/2022 20:07:09 - INFO - __main__ - Step 240 Global step 240 Train loss 1.287117 on epoch=119
06/24/2022 20:07:12 - INFO - __main__ - Step 250 Global step 250 Train loss 1.488904 on epoch=124
06/24/2022 20:07:12 - INFO - __main__ - Global step 250 Train loss 1.368306 Classification-F1 0.5307917888563051 on epoch=124
06/24/2022 20:07:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.969119 on epoch=129
06/24/2022 20:07:18 - INFO - __main__ - Step 270 Global step 270 Train loss 1.470936 on epoch=134
06/24/2022 20:07:20 - INFO - __main__ - Step 280 Global step 280 Train loss 1.118436 on epoch=139
06/24/2022 20:07:23 - INFO - __main__ - Step 290 Global step 290 Train loss 1.289152 on epoch=144
06/24/2022 20:07:25 - INFO - __main__ - Step 300 Global step 300 Train loss 1.359090 on epoch=149
06/24/2022 20:07:25 - INFO - __main__ - Global step 300 Train loss 1.241347 Classification-F1 0.3454545454545454 on epoch=149
06/24/2022 20:07:28 - INFO - __main__ - Step 310 Global step 310 Train loss 1.135232 on epoch=154
06/24/2022 20:07:30 - INFO - __main__ - Step 320 Global step 320 Train loss 1.072848 on epoch=159
06/24/2022 20:07:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.876339 on epoch=164
06/24/2022 20:07:35 - INFO - __main__ - Step 340 Global step 340 Train loss 1.029675 on epoch=169
06/24/2022 20:07:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.958574 on epoch=174
06/24/2022 20:07:38 - INFO - __main__ - Global step 350 Train loss 1.014534 Classification-F1 0.4458874458874459 on epoch=174
06/24/2022 20:07:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.940025 on epoch=179
06/24/2022 20:07:43 - INFO - __main__ - Step 370 Global step 370 Train loss 1.077780 on epoch=184
06/24/2022 20:07:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.966345 on epoch=189
06/24/2022 20:07:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.773342 on epoch=194
06/24/2022 20:07:51 - INFO - __main__ - Step 400 Global step 400 Train loss 1.001989 on epoch=199
06/24/2022 20:07:51 - INFO - __main__ - Global step 400 Train loss 0.951896 Classification-F1 0.3992490613266583 on epoch=199
06/24/2022 20:07:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.647254 on epoch=204
06/24/2022 20:07:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.974119 on epoch=209
06/24/2022 20:07:58 - INFO - __main__ - Step 430 Global step 430 Train loss 1.089622 on epoch=214
06/24/2022 20:08:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.692288 on epoch=219
06/24/2022 20:08:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.870436 on epoch=224
06/24/2022 20:08:03 - INFO - __main__ - Global step 450 Train loss 0.854744 Classification-F1 0.5151515151515151 on epoch=224
06/24/2022 20:08:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.930491 on epoch=229
06/24/2022 20:08:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.982710 on epoch=234
06/24/2022 20:08:11 - INFO - __main__ - Step 480 Global step 480 Train loss 1.022246 on epoch=239
06/24/2022 20:08:13 - INFO - __main__ - Step 490 Global step 490 Train loss 1.034307 on epoch=244
06/24/2022 20:08:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.949914 on epoch=249
06/24/2022 20:08:16 - INFO - __main__ - Global step 500 Train loss 0.983934 Classification-F1 0.4458874458874459 on epoch=249
06/24/2022 20:08:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.681453 on epoch=254
06/24/2022 20:08:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.677282 on epoch=259
06/24/2022 20:08:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.976046 on epoch=264
06/24/2022 20:08:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.743706 on epoch=269
06/24/2022 20:08:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.721542 on epoch=274
06/24/2022 20:08:29 - INFO - __main__ - Global step 550 Train loss 0.760006 Classification-F1 0.3043478260869565 on epoch=274
06/24/2022 20:08:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.714352 on epoch=279
06/24/2022 20:08:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.956482 on epoch=284
06/24/2022 20:08:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.914236 on epoch=289
06/24/2022 20:08:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.730023 on epoch=294
06/24/2022 20:08:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.827095 on epoch=299
06/24/2022 20:08:42 - INFO - __main__ - Global step 600 Train loss 0.828438 Classification-F1 0.36374269005847953 on epoch=299
06/24/2022 20:08:42 - INFO - __main__ - save last model!
06/24/2022 20:08:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:08:43 - INFO - __main__ - Printing 3 examples
06/24/2022 20:08:43 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 20:08:43 - INFO - __main__ - ['0']
06/24/2022 20:08:43 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 20:08:43 - INFO - __main__ - ['0']
06/24/2022 20:08:43 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 20:08:43 - INFO - __main__ - ['0']
06/24/2022 20:08:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/24/2022 20:08:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:08:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 20:08:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:08:43 - INFO - __main__ - Printing 3 examples
06/24/2022 20:08:43 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 20:08:43 - INFO - __main__ - ['0']
06/24/2022 20:08:43 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 20:08:43 - INFO - __main__ - ['0']
06/24/2022 20:08:43 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 20:08:43 - INFO - __main__ - ['0']
06/24/2022 20:08:43 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:08:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:08:43 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 20:08:45 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 20:08:45 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 20:08:45 - INFO - __main__ - Printing 3 examples
06/24/2022 20:08:45 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 20:08:45 - INFO - __main__ - ['0']
06/24/2022 20:08:45 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 20:08:45 - INFO - __main__ - ['1']
06/24/2022 20:08:45 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 20:08:45 - INFO - __main__ - ['1']
06/24/2022 20:08:45 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 20:08:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 20:08:47 - INFO - __main__ - Starting training!
06/24/2022 20:08:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:08:57 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 20:09:57 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_87_0.0002_8_predictions.txt
06/24/2022 20:09:57 - INFO - __main__ - Classification-F1 on test data: 0.4902
06/24/2022 20:09:57 - INFO - __main__ - prefix=paws_16_87, lr=0.0002, bsz=8, dev_performance=0.5307917888563051, test_performance=0.49024843888584413
06/24/2022 20:09:57 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.0001, bsz=8 ...
06/24/2022 20:09:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:09:58 - INFO - __main__ - Printing 3 examples
06/24/2022 20:09:58 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 20:09:58 - INFO - __main__ - ['0']
06/24/2022 20:09:58 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 20:09:58 - INFO - __main__ - ['0']
06/24/2022 20:09:58 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 20:09:58 - INFO - __main__ - ['0']
06/24/2022 20:09:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 20:09:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:09:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/24/2022 20:09:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 20:09:58 - INFO - __main__ - Printing 3 examples
06/24/2022 20:09:58 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 20:09:58 - INFO - __main__ - ['0']
06/24/2022 20:09:58 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 20:09:58 - INFO - __main__ - ['0']
06/24/2022 20:09:58 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 20:09:58 - INFO - __main__ - ['0']
06/24/2022 20:09:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 20:09:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:09:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 20:10:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/24/2022 20:10:02 - INFO - __main__ - Starting training!
06/24/2022 20:10:04 - INFO - __main__ - Step 10 Global step 10 Train loss 18.306149 on epoch=4
06/24/2022 20:10:07 - INFO - __main__ - Step 20 Global step 20 Train loss 17.737345 on epoch=9
06/24/2022 20:10:09 - INFO - __main__ - Step 30 Global step 30 Train loss 13.588193 on epoch=14
06/24/2022 20:10:11 - INFO - __main__ - Step 40 Global step 40 Train loss 12.880322 on epoch=19
06/24/2022 20:10:14 - INFO - __main__ - Step 50 Global step 50 Train loss 11.720693 on epoch=24
06/24/2022 20:10:16 - INFO - __main__ - Global step 50 Train loss 14.846539 Classification-F1 0.0035842293906810036 on epoch=24
06/24/2022 20:10:19 - INFO - __main__ - Step 60 Global step 60 Train loss 10.600663 on epoch=29
06/24/2022 20:10:21 - INFO - __main__ - Step 70 Global step 70 Train loss 10.135930 on epoch=34
06/24/2022 20:10:24 - INFO - __main__ - Step 80 Global step 80 Train loss 8.409504 on epoch=39
06/24/2022 20:10:26 - INFO - __main__ - Step 90 Global step 90 Train loss 8.314714 on epoch=44
06/24/2022 20:10:29 - INFO - __main__ - Step 100 Global step 100 Train loss 7.789214 on epoch=49
06/24/2022 20:10:29 - INFO - __main__ - Global step 100 Train loss 9.050005 Classification-F1 0.22621082621082617 on epoch=49
06/24/2022 20:10:32 - INFO - __main__ - Step 110 Global step 110 Train loss 7.426347 on epoch=54
06/24/2022 20:10:35 - INFO - __main__ - Step 120 Global step 120 Train loss 7.053820 on epoch=59
06/24/2022 20:10:37 - INFO - __main__ - Step 130 Global step 130 Train loss 6.932936 on epoch=64
06/24/2022 20:10:40 - INFO - __main__ - Step 140 Global step 140 Train loss 5.963532 on epoch=69
06/24/2022 20:10:42 - INFO - __main__ - Step 150 Global step 150 Train loss 4.702482 on epoch=74
06/24/2022 20:10:42 - INFO - __main__ - Global step 150 Train loss 6.415824 Classification-F1 0.33086419753086416 on epoch=74
06/24/2022 20:10:45 - INFO - __main__ - Step 160 Global step 160 Train loss 5.050243 on epoch=79
06/24/2022 20:10:48 - INFO - __main__ - Step 170 Global step 170 Train loss 5.136830 on epoch=84
06/24/2022 20:10:50 - INFO - __main__ - Step 180 Global step 180 Train loss 4.403351 on epoch=89
06/24/2022 20:10:53 - INFO - __main__ - Step 190 Global step 190 Train loss 4.580024 on epoch=94
06/24/2022 20:10:55 - INFO - __main__ - Step 200 Global step 200 Train loss 3.403796 on epoch=99
06/24/2022 20:10:56 - INFO - __main__ - Global step 200 Train loss 4.514849 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 20:10:59 - INFO - __main__ - Step 210 Global step 210 Train loss 2.830732 on epoch=104
06/24/2022 20:11:01 - INFO - __main__ - Step 220 Global step 220 Train loss 3.097867 on epoch=109
06/24/2022 20:11:04 - INFO - __main__ - Step 230 Global step 230 Train loss 3.519270 on epoch=114
06/24/2022 20:11:06 - INFO - __main__ - Step 240 Global step 240 Train loss 2.716503 on epoch=119
06/24/2022 20:11:09 - INFO - __main__ - Step 250 Global step 250 Train loss 2.987255 on epoch=124
06/24/2022 20:11:09 - INFO - __main__ - Global step 250 Train loss 3.030325 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 20:11:11 - INFO - __main__ - Step 260 Global step 260 Train loss 2.051555 on epoch=129
06/24/2022 20:11:14 - INFO - __main__ - Step 270 Global step 270 Train loss 2.565394 on epoch=134
06/24/2022 20:11:17 - INFO - __main__ - Step 280 Global step 280 Train loss 2.017922 on epoch=139
06/24/2022 20:11:19 - INFO - __main__ - Step 290 Global step 290 Train loss 2.356881 on epoch=144
06/24/2022 20:11:22 - INFO - __main__ - Step 300 Global step 300 Train loss 2.688838 on epoch=149
06/24/2022 20:11:22 - INFO - __main__ - Global step 300 Train loss 2.336118 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 20:11:24 - INFO - __main__ - Step 310 Global step 310 Train loss 2.871899 on epoch=154
06/24/2022 20:11:27 - INFO - __main__ - Step 320 Global step 320 Train loss 2.263367 on epoch=159
06/24/2022 20:11:29 - INFO - __main__ - Step 330 Global step 330 Train loss 2.359102 on epoch=164
06/24/2022 20:11:32 - INFO - __main__ - Step 340 Global step 340 Train loss 1.909289 on epoch=169
06/24/2022 20:11:34 - INFO - __main__ - Step 350 Global step 350 Train loss 1.955052 on epoch=174
06/24/2022 20:11:34 - INFO - __main__ - Global step 350 Train loss 2.271742 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 20:11:37 - INFO - __main__ - Step 360 Global step 360 Train loss 2.413523 on epoch=179
06/24/2022 20:11:39 - INFO - __main__ - Step 370 Global step 370 Train loss 1.963431 on epoch=184
06/24/2022 20:11:42 - INFO - __main__ - Step 380 Global step 380 Train loss 1.403795 on epoch=189
06/24/2022 20:11:44 - INFO - __main__ - Step 390 Global step 390 Train loss 1.771543 on epoch=194
06/24/2022 20:11:47 - INFO - __main__ - Step 400 Global step 400 Train loss 1.273161 on epoch=199
06/24/2022 20:11:47 - INFO - __main__ - Global step 400 Train loss 1.765090 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 20:11:50 - INFO - __main__ - Step 410 Global step 410 Train loss 1.727518 on epoch=204
06/24/2022 20:11:52 - INFO - __main__ - Step 420 Global step 420 Train loss 1.359388 on epoch=209
06/24/2022 20:11:55 - INFO - __main__ - Step 430 Global step 430 Train loss 1.855217 on epoch=214
06/24/2022 20:11:57 - INFO - __main__ - Step 440 Global step 440 Train loss 1.188078 on epoch=219
06/24/2022 20:12:00 - INFO - __main__ - Step 450 Global step 450 Train loss 1.814466 on epoch=224
06/24/2022 20:12:00 - INFO - __main__ - Global step 450 Train loss 1.588933 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 20:12:03 - INFO - __main__ - Step 460 Global step 460 Train loss 1.531227 on epoch=229
06/24/2022 20:12:05 - INFO - __main__ - Step 470 Global step 470 Train loss 1.783478 on epoch=234
06/24/2022 20:12:08 - INFO - __main__ - Step 480 Global step 480 Train loss 1.074816 on epoch=239
06/24/2022 20:12:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.900765 on epoch=244
06/24/2022 20:12:13 - INFO - __main__ - Step 500 Global step 500 Train loss 1.783712 on epoch=249
06/24/2022 20:12:13 - INFO - __main__ - Global step 500 Train loss 1.414799 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 20:12:15 - INFO - __main__ - Step 510 Global step 510 Train loss 1.005900 on epoch=254
06/24/2022 20:12:18 - INFO - __main__ - Step 520 Global step 520 Train loss 1.252774 on epoch=259
06/24/2022 20:12:21 - INFO - __main__ - Step 530 Global step 530 Train loss 1.314011 on epoch=264
06/24/2022 20:12:23 - INFO - __main__ - Step 540 Global step 540 Train loss 1.187949 on epoch=269
06/24/2022 20:12:26 - INFO - __main__ - Step 550 Global step 550 Train loss 1.057229 on epoch=274
06/24/2022 20:12:26 - INFO - __main__ - Global step 550 Train loss 1.163573 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 20:12:28 - INFO - __main__ - Step 560 Global step 560 Train loss 1.342266 on epoch=279
06/24/2022 20:12:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.995254 on epoch=284
06/24/2022 20:12:33 - INFO - __main__ - Step 580 Global step 580 Train loss 1.034240 on epoch=289
06/24/2022 20:12:36 - INFO - __main__ - Step 590 Global step 590 Train loss 1.318752 on epoch=294
06/24/2022 20:12:38 - INFO - __main__ - Step 600 Global step 600 Train loss 1.700005 on epoch=299
06/24/2022 20:12:39 - INFO - __main__ - Global step 600 Train loss 1.278103 Classification-F1 0.3333333333333333 on epoch=299
06/24/2022 20:12:39 - INFO - __main__ - save last model!
06/24/2022 20:12:41 - INFO - __main__ - Loading checkpoint on the fly
06/24/2022 20:12:41 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 20:12:41 - INFO - __main__ - Printing 3 examples
06/24/2022 20:12:41 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 20:12:41 - INFO - __main__ - ['0']
06/24/2022 20:12:41 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 20:12:41 - INFO - __main__ - ['1']
06/24/2022 20:12:41 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 20:12:41 - INFO - __main__ - ['1']
06/24/2022 20:12:41 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/24/2022 20:12:46 - INFO - __main__ - Tokenizing Output ...
06/24/2022 20:12:53 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 20:13:47 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-paws/paws_16_87_0.0001_8_predictions.txt
06/24/2022 20:13:47 - INFO - __main__ - Classification-F1 on test data: 0.2107
06/24/2022 20:13:48 - INFO - __main__ - prefix=paws_16_87, lr=0.0001, bsz=8, dev_performance=0.3333333333333333, test_performance=0.21071556614054676
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (91404): No such process
