nohup: ignoring input
Task: wiki_split, Checkpoint: models/upstream-reptile-random-3e-5-2-5000-5e-1-10/last-model.pt, Identifier: T5-large-reptile-random-3e-5-2-5000-5e-1-10
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_reptile_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:26555
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_3hspfuvy/none_egmq9t1i
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=26555
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_3hspfuvy/none_egmq9t1i/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_3hspfuvy/none_egmq9t1i/attempt_0/1/error.json
Output directory () already exists and is not empty.
02/24/2022 14:43:58 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-reptile-random-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-reptile-random-3e-5-2-5000-5e-1-10/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-random-3e-5-2-5000-5e-1-10/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
02/24/2022 14:43:58 - INFO - __main__ - models/T5-large-reptile-random-3e-5-2-5000-5e-1-10/singletask-wiki_split
02/24/2022 14:43:58 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-reptile-random-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-reptile-random-3e-5-2-5000-5e-1-10/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-random-3e-5-2-5000-5e-1-10/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
02/24/2022 14:43:58 - INFO - __main__ - models/T5-large-reptile-random-3e-5-2-5000-5e-1-10/singletask-wiki_split
02/24/2022 14:43:59 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
02/24/2022 14:43:59 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
02/24/2022 14:43:59 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
02/24/2022 14:43:59 - INFO - __main__ - args.device: cuda:0
02/24/2022 14:43:59 - INFO - __main__ - Using 2 gpus
02/24/2022 14:43:59 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
02/24/2022 14:43:59 - INFO - __main__ - args.device: cuda:1
02/24/2022 14:43:59 - INFO - __main__ - Using 2 gpus
02/24/2022 14:43:59 - INFO - __main__ - Fine-tuning the following samples: ['wiki_split_32_100', 'wiki_split_32_13', 'wiki_split_32_21', 'wiki_split_32_42', 'wiki_split_32_87']
02/24/2022 14:43:59 - INFO - __main__ - Fine-tuning the following samples: ['wiki_split_32_100', 'wiki_split_32_13', 'wiki_split_32_21', 'wiki_split_32_42', 'wiki_split_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
02/24/2022 14:44:04 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.5, bsz=8 ...
02/24/2022 14:44:05 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 14:44:05 - INFO - __main__ - Printing 3 examples
02/24/2022 14:44:05 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/24/2022 14:44:05 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/24/2022 14:44:05 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/24/2022 14:44:05 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/24/2022 14:44:05 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/24/2022 14:44:05 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/24/2022 14:44:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/24/2022 14:44:05 - INFO - __main__ - Tokenizing Output ...
02/24/2022 14:44:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/24/2022 14:44:05 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 14:44:05 - INFO - __main__ - Printing 3 examples
02/24/2022 14:44:05 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/24/2022 14:44:05 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/24/2022 14:44:05 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/24/2022 14:44:05 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/24/2022 14:44:05 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/24/2022 14:44:05 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/24/2022 14:44:05 - INFO - __main__ - Tokenizing Input ...
02/24/2022 14:44:05 - INFO - __main__ - Tokenizing Output ...
02/24/2022 14:44:05 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 14:44:05 - INFO - __main__ - Printing 3 examples
02/24/2022 14:44:05 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/24/2022 14:44:05 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/24/2022 14:44:05 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/24/2022 14:44:05 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/24/2022 14:44:05 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/24/2022 14:44:05 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/24/2022 14:44:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/24/2022 14:44:05 - INFO - __main__ - Tokenizing Output ...
02/24/2022 14:44:05 - INFO - __main__ - Loaded 32 examples from dev data
02/24/2022 14:44:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/24/2022 14:44:05 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 14:44:05 - INFO - __main__ - Printing 3 examples
02/24/2022 14:44:05 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/24/2022 14:44:05 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/24/2022 14:44:05 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/24/2022 14:44:05 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/24/2022 14:44:05 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/24/2022 14:44:05 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/24/2022 14:44:05 - INFO - __main__ - Tokenizing Input ...
02/24/2022 14:44:05 - INFO - __main__ - Tokenizing Output ...
02/24/2022 14:44:05 - INFO - __main__ - Loaded 32 examples from dev data
02/24/2022 14:44:20 - INFO - __main__ - load prompt embedding from ckpt
02/24/2022 14:44:20 - INFO - __main__ - load prompt embedding from ckpt
02/24/2022 14:44:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/24/2022 14:44:21 - INFO - __main__ - Starting training!
02/24/2022 14:44:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/24/2022 14:44:26 - INFO - __main__ - Starting training!
02/24/2022 14:44:29 - INFO - __main__ - Step 10 Global step 10 Train loss 0.95 on epoch=4
02/24/2022 14:44:31 - INFO - __main__ - Step 20 Global step 20 Train loss 0.87 on epoch=9
02/24/2022 14:44:34 - INFO - __main__ - Step 30 Global step 30 Train loss 0.79 on epoch=14
02/24/2022 14:44:36 - INFO - __main__ - Step 40 Global step 40 Train loss 0.74 on epoch=19
02/24/2022 14:44:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.71 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
02/24/2022 14:44:47 - INFO - __main__ - Global step 50 Train loss 0.81 Rouge-L 0.6677980234578946 on epoch=24
02/24/2022 14:44:47 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.6677980234578946 on epoch=24, global_step=50
02/24/2022 14:44:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.68 on epoch=29
02/24/2022 14:44:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.63 on epoch=34
02/24/2022 14:44:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=39
02/24/2022 14:44:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.61 on epoch=44
02/24/2022 14:44:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.60 on epoch=49
02/24/2022 14:45:09 - INFO - __main__ - Global step 100 Train loss 0.63 Rouge-L 0.8179903030792105 on epoch=49
02/24/2022 14:45:09 - INFO - __main__ - Saving model with best Rouge-L: 0.6677980234578946 -> 0.8179903030792105 on epoch=49, global_step=100
02/24/2022 14:45:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.57 on epoch=54
02/24/2022 14:45:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=59
02/24/2022 14:45:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=64
02/24/2022 14:45:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=69
02/24/2022 14:45:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=74
02/24/2022 14:45:31 - INFO - __main__ - Global step 150 Train loss 0.54 Rouge-L 0.8771928001470316 on epoch=74
02/24/2022 14:45:31 - INFO - __main__ - Saving model with best Rouge-L: 0.8179903030792105 -> 0.8771928001470316 on epoch=74, global_step=150
02/24/2022 14:45:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=79
02/24/2022 14:45:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=84
02/24/2022 14:45:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=89
02/24/2022 14:45:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=94
02/24/2022 14:45:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=99
02/24/2022 14:45:54 - INFO - __main__ - Global step 200 Train loss 0.50 Rouge-L 0.8774527034047512 on epoch=99
02/24/2022 14:45:54 - INFO - __main__ - Saving model with best Rouge-L: 0.8771928001470316 -> 0.8774527034047512 on epoch=99, global_step=200
02/24/2022 14:45:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=104
02/24/2022 14:45:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=109
02/24/2022 14:46:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=114
02/24/2022 14:46:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=119
02/24/2022 14:46:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=124
02/24/2022 14:46:16 - INFO - __main__ - Global step 250 Train loss 0.46 Rouge-L 0.8788326798882661 on epoch=124
02/24/2022 14:46:16 - INFO - __main__ - Saving model with best Rouge-L: 0.8774527034047512 -> 0.8788326798882661 on epoch=124, global_step=250
02/24/2022 14:46:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=129
02/24/2022 14:46:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=134
02/24/2022 14:46:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=139
02/24/2022 14:46:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=144
02/24/2022 14:46:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=149
02/24/2022 14:46:38 - INFO - __main__ - Global step 300 Train loss 0.44 Rouge-L 0.8788894540459741 on epoch=149
02/24/2022 14:46:38 - INFO - __main__ - Saving model with best Rouge-L: 0.8788326798882661 -> 0.8788894540459741 on epoch=149, global_step=300
02/24/2022 14:46:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=154
02/24/2022 14:46:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=159
02/24/2022 14:46:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=164
02/24/2022 14:46:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=169
02/24/2022 14:46:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
02/24/2022 14:47:01 - INFO - __main__ - Global step 350 Train loss 0.41 Rouge-L 0.8676896062569427 on epoch=174
02/24/2022 14:47:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=179
02/24/2022 14:47:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=184
02/24/2022 14:47:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=189
02/24/2022 14:47:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=194
02/24/2022 14:47:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=199
02/24/2022 14:47:23 - INFO - __main__ - Global step 400 Train loss 0.39 Rouge-L 0.8768835312636769 on epoch=199
02/24/2022 14:47:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=204
02/24/2022 14:47:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=209
02/24/2022 14:47:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=214
02/24/2022 14:47:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=219
02/24/2022 14:47:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=224
02/24/2022 14:47:44 - INFO - __main__ - Global step 450 Train loss 0.37 Rouge-L 0.8778035853141104 on epoch=224
02/24/2022 14:47:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=229
02/24/2022 14:47:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=234
02/24/2022 14:47:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=239
02/24/2022 14:47:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=244
02/24/2022 14:47:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=249
02/24/2022 14:48:06 - INFO - __main__ - Global step 500 Train loss 0.34 Rouge-L 0.8760407119445719 on epoch=249
02/24/2022 14:48:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.33 on epoch=254
02/24/2022 14:48:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=259
02/24/2022 14:48:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=264
02/24/2022 14:48:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=269
02/24/2022 14:48:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.32 on epoch=274
02/24/2022 14:48:28 - INFO - __main__ - Global step 550 Train loss 0.33 Rouge-L 0.8770245021432252 on epoch=274
02/24/2022 14:48:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=279
02/24/2022 14:48:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=284
02/24/2022 14:48:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=289
02/24/2022 14:48:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=294
02/24/2022 14:48:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=299
02/24/2022 14:48:49 - INFO - __main__ - Global step 600 Train loss 0.31 Rouge-L 0.8830968976650289 on epoch=299
02/24/2022 14:48:49 - INFO - __main__ - Saving model with best Rouge-L: 0.8788894540459741 -> 0.8830968976650289 on epoch=299, global_step=600
02/24/2022 14:48:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=304
02/24/2022 14:48:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=309
02/24/2022 14:48:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.30 on epoch=314
02/24/2022 14:48:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=319
02/24/2022 14:49:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=324
02/24/2022 14:49:11 - INFO - __main__ - Global step 650 Train loss 0.30 Rouge-L 0.8769548183567546 on epoch=324
02/24/2022 14:49:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.29 on epoch=329
02/24/2022 14:49:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=334
02/24/2022 14:49:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=339
02/24/2022 14:49:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.29 on epoch=344
02/24/2022 14:49:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.28 on epoch=349
02/24/2022 14:49:33 - INFO - __main__ - Global step 700 Train loss 0.29 Rouge-L 0.8770503263716247 on epoch=349
02/24/2022 14:49:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=354
02/24/2022 14:49:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=359
02/24/2022 14:49:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.28 on epoch=364
02/24/2022 14:49:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=369
02/24/2022 14:49:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=374
02/24/2022 14:49:55 - INFO - __main__ - Global step 750 Train loss 0.27 Rouge-L 0.8759393319055664 on epoch=374
02/24/2022 14:49:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=379
02/24/2022 14:49:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.27 on epoch=384
02/24/2022 14:50:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.27 on epoch=389
02/24/2022 14:50:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=394
02/24/2022 14:50:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=399
02/24/2022 14:50:17 - INFO - __main__ - Global step 800 Train loss 0.27 Rouge-L 0.8732684939797059 on epoch=399
02/24/2022 14:50:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=404
02/24/2022 14:50:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=409
02/24/2022 14:50:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=414
02/24/2022 14:50:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=419
02/24/2022 14:50:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=424
02/24/2022 14:50:39 - INFO - __main__ - Global step 850 Train loss 0.25 Rouge-L 0.8746214414424204 on epoch=424
02/24/2022 14:50:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=429
02/24/2022 14:50:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=434
02/24/2022 14:50:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=439
02/24/2022 14:50:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=444
02/24/2022 14:50:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=449
02/24/2022 14:51:01 - INFO - __main__ - Global step 900 Train loss 0.24 Rouge-L 0.874257616319859 on epoch=449
02/24/2022 14:51:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=454
02/24/2022 14:51:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=459
02/24/2022 14:51:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=464
02/24/2022 14:51:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=469
02/24/2022 14:51:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=474
02/24/2022 14:51:24 - INFO - __main__ - Global step 950 Train loss 0.23 Rouge-L 0.8698569237044932 on epoch=474
02/24/2022 14:51:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=479
02/24/2022 14:51:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=484
02/24/2022 14:51:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=489
02/24/2022 14:51:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=494
02/24/2022 14:51:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=499
02/24/2022 14:51:46 - INFO - __main__ - Global step 1000 Train loss 0.22 Rouge-L 0.8771278499890067 on epoch=499
02/24/2022 14:51:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=504
02/24/2022 14:51:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=509
02/24/2022 14:51:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=514
02/24/2022 14:51:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=519
02/24/2022 14:51:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=524
02/24/2022 14:52:08 - INFO - __main__ - Global step 1050 Train loss 0.21 Rouge-L 0.8891371776119305 on epoch=524
02/24/2022 14:52:08 - INFO - __main__ - Saving model with best Rouge-L: 0.8830968976650289 -> 0.8891371776119305 on epoch=524, global_step=1050
02/24/2022 14:52:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=529
02/24/2022 14:52:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=534
02/24/2022 14:52:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=539
02/24/2022 14:52:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=544
02/24/2022 14:52:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=549
02/24/2022 14:52:31 - INFO - __main__ - Global step 1100 Train loss 0.20 Rouge-L 0.8782845023308837 on epoch=549
02/24/2022 14:52:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=554
02/24/2022 14:52:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=559
02/24/2022 14:52:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=564
02/24/2022 14:52:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=569
02/24/2022 14:52:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=574
02/24/2022 14:52:54 - INFO - __main__ - Global step 1150 Train loss 0.19 Rouge-L 0.8902845466440937 on epoch=574
02/24/2022 14:52:54 - INFO - __main__ - Saving model with best Rouge-L: 0.8891371776119305 -> 0.8902845466440937 on epoch=574, global_step=1150
02/24/2022 14:52:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=579
02/24/2022 14:52:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=584
02/24/2022 14:53:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=589
02/24/2022 14:53:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=594
02/24/2022 14:53:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=599
02/24/2022 14:53:17 - INFO - __main__ - Global step 1200 Train loss 0.18 Rouge-L 0.8803848866316594 on epoch=599
02/24/2022 14:53:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=604
02/24/2022 14:53:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=609
02/24/2022 14:53:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=614
02/24/2022 14:53:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=619
02/24/2022 14:53:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=624
02/24/2022 14:53:41 - INFO - __main__ - Global step 1250 Train loss 0.17 Rouge-L 0.8790984948899166 on epoch=624
02/24/2022 14:53:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=629
02/24/2022 14:53:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=634
02/24/2022 14:53:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=639
02/24/2022 14:53:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.17 on epoch=644
02/24/2022 14:53:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=649
02/24/2022 14:54:03 - INFO - __main__ - Global step 1300 Train loss 0.17 Rouge-L 0.8745730031211134 on epoch=649
02/24/2022 14:54:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=654
02/24/2022 14:54:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=659
02/24/2022 14:54:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=664
02/24/2022 14:54:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.14 on epoch=669
02/24/2022 14:54:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=674
02/24/2022 14:54:26 - INFO - __main__ - Global step 1350 Train loss 0.16 Rouge-L 0.881322340858776 on epoch=674
02/24/2022 14:54:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=679
02/24/2022 14:54:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=684
02/24/2022 14:54:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=689
02/24/2022 14:54:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=694
02/24/2022 14:54:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=699
02/24/2022 14:54:50 - INFO - __main__ - Global step 1400 Train loss 0.15 Rouge-L 0.8712863004927857 on epoch=699
02/24/2022 14:54:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=704
02/24/2022 14:54:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=709
02/24/2022 14:54:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=714
02/24/2022 14:54:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=719
02/24/2022 14:55:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=724
02/24/2022 14:55:12 - INFO - __main__ - Global step 1450 Train loss 0.14 Rouge-L 0.8793450726335474 on epoch=724
02/24/2022 14:55:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=729
02/24/2022 14:55:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=734
02/24/2022 14:55:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=739
02/24/2022 14:55:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=744
02/24/2022 14:55:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=749
02/24/2022 14:55:35 - INFO - __main__ - Global step 1500 Train loss 0.13 Rouge-L 0.8786761684101607 on epoch=749
02/24/2022 14:55:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=754
02/24/2022 14:55:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=759
02/24/2022 14:55:42 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=764
02/24/2022 14:55:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=769
02/24/2022 14:55:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=774
02/24/2022 14:55:57 - INFO - __main__ - Global step 1550 Train loss 0.13 Rouge-L 0.866689598987811 on epoch=774
02/24/2022 14:56:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=779
02/24/2022 14:56:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=784
02/24/2022 14:56:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=789
02/24/2022 14:56:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=794
02/24/2022 14:56:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.11 on epoch=799
02/24/2022 14:56:20 - INFO - __main__ - Global step 1600 Train loss 0.12 Rouge-L 0.8761748622855239 on epoch=799
02/24/2022 14:56:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=804
02/24/2022 14:56:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=809
02/24/2022 14:56:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=814
02/24/2022 14:56:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=819
02/24/2022 14:56:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=824
02/24/2022 14:56:42 - INFO - __main__ - Global step 1650 Train loss 0.12 Rouge-L 0.8663457406200687 on epoch=824
02/24/2022 14:56:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=829
02/24/2022 14:56:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=834
02/24/2022 14:56:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=839
02/24/2022 14:56:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=844
02/24/2022 14:56:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=849
02/24/2022 14:57:05 - INFO - __main__ - Global step 1700 Train loss 0.12 Rouge-L 0.887411299571827 on epoch=849
02/24/2022 14:57:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=854
02/24/2022 14:57:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.10 on epoch=859
02/24/2022 14:57:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=864
02/24/2022 14:57:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=869
02/24/2022 14:57:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=874
02/24/2022 14:57:27 - INFO - __main__ - Global step 1750 Train loss 0.11 Rouge-L 0.8692790599490766 on epoch=874
02/24/2022 14:57:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=879
02/24/2022 14:57:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=884
02/24/2022 14:57:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=889
02/24/2022 14:57:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=894
02/24/2022 14:57:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=899
02/24/2022 14:57:50 - INFO - __main__ - Global step 1800 Train loss 0.11 Rouge-L 0.8853781975123911 on epoch=899
02/24/2022 14:57:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=904
02/24/2022 14:57:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=909
02/24/2022 14:57:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=914
02/24/2022 14:57:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.10 on epoch=919
02/24/2022 14:58:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=924
02/24/2022 14:58:12 - INFO - __main__ - Global step 1850 Train loss 0.10 Rouge-L 0.8785145907673039 on epoch=924
02/24/2022 14:58:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=929
02/24/2022 14:58:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=934
02/24/2022 14:58:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=939
02/24/2022 14:58:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=944
02/24/2022 14:58:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=949
02/24/2022 14:58:34 - INFO - __main__ - Global step 1900 Train loss 0.10 Rouge-L 0.8788815962073037 on epoch=949
02/24/2022 14:58:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=954
02/24/2022 14:58:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=959
02/24/2022 14:58:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=964
02/24/2022 14:58:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=969
02/24/2022 14:58:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=974
02/24/2022 14:58:59 - INFO - __main__ - Global step 1950 Train loss 0.10 Rouge-L 0.8680461429279975 on epoch=974
02/24/2022 14:59:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=979
02/24/2022 14:59:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=984
02/24/2022 14:59:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=989
02/24/2022 14:59:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=994
02/24/2022 14:59:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=999
02/24/2022 14:59:11 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 14:59:11 - INFO - __main__ - Printing 3 examples
02/24/2022 14:59:11 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/24/2022 14:59:11 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/24/2022 14:59:11 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/24/2022 14:59:11 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/24/2022 14:59:11 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/24/2022 14:59:11 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/24/2022 14:59:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/24/2022 14:59:11 - INFO - __main__ - Tokenizing Output ...
02/24/2022 14:59:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/24/2022 14:59:11 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 14:59:11 - INFO - __main__ - Printing 3 examples
02/24/2022 14:59:11 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/24/2022 14:59:11 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/24/2022 14:59:11 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/24/2022 14:59:11 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/24/2022 14:59:11 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/24/2022 14:59:11 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/24/2022 14:59:11 - INFO - __main__ - Tokenizing Input ...
02/24/2022 14:59:11 - INFO - __main__ - Tokenizing Output ...
02/24/2022 14:59:11 - INFO - __main__ - Loaded 32 examples from dev data
02/24/2022 14:59:21 - INFO - __main__ - Global step 2000 Train loss 0.09 Rouge-L 0.8691092738991733 on epoch=999
02/24/2022 14:59:21 - INFO - __main__ - save last model!
02/24/2022 14:59:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/24/2022 14:59:21 - INFO - __main__ - Start tokenizing ... 5000 instances
02/24/2022 14:59:21 - INFO - __main__ - Printing 3 examples
02/24/2022 14:59:21 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
02/24/2022 14:59:21 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
02/24/2022 14:59:21 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
02/24/2022 14:59:21 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
02/24/2022 14:59:21 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
02/24/2022 14:59:21 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
02/24/2022 14:59:21 - INFO - __main__ - Tokenizing Input ...
02/24/2022 14:59:23 - INFO - __main__ - load prompt embedding from ckpt
02/24/2022 14:59:24 - INFO - __main__ - Tokenizing Output ...
02/24/2022 14:59:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/24/2022 14:59:24 - INFO - __main__ - Starting training!
02/24/2022 14:59:29 - INFO - __main__ - Loaded 5000 examples from test data
02/24/2022 15:28:20 - INFO - __main__ - Saved prediction in models/T5-large-reptile-random-3e-5-2-5000-5e-1-10/singletask-wiki_split/wiki_split_32_100_0.5_8_predictions.txt
02/24/2022 15:28:26 - INFO - __main__ - Rouge-L on test data: 0.8745
02/24/2022 15:28:26 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.5, bsz=8, dev_performance=0.8902845466440937, test_performance=0.874501447102768
02/24/2022 15:28:26 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.4, bsz=8 ...
02/24/2022 15:28:27 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 15:28:27 - INFO - __main__ - Printing 3 examples
02/24/2022 15:28:27 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/24/2022 15:28:27 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/24/2022 15:28:27 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/24/2022 15:28:27 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/24/2022 15:28:27 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/24/2022 15:28:27 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/24/2022 15:28:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/24/2022 15:28:27 - INFO - __main__ - Tokenizing Output ...
02/24/2022 15:28:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/24/2022 15:28:27 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 15:28:27 - INFO - __main__ - Printing 3 examples
02/24/2022 15:28:27 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/24/2022 15:28:27 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/24/2022 15:28:27 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/24/2022 15:28:27 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/24/2022 15:28:27 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/24/2022 15:28:27 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/24/2022 15:28:27 - INFO - __main__ - Tokenizing Input ...
02/24/2022 15:28:28 - INFO - __main__ - Tokenizing Output ...
02/24/2022 15:28:28 - INFO - __main__ - Loaded 32 examples from dev data
02/24/2022 15:28:41 - INFO - __main__ - load prompt embedding from ckpt
02/24/2022 15:28:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/24/2022 15:28:42 - INFO - __main__ - Starting training!
02/24/2022 15:28:48 - INFO - __main__ - Step 10 Global step 10 Train loss 0.97 on epoch=4
02/24/2022 15:28:51 - INFO - __main__ - Step 20 Global step 20 Train loss 0.97 on epoch=9
02/24/2022 15:28:53 - INFO - __main__ - Step 30 Global step 30 Train loss 0.87 on epoch=14
02/24/2022 15:28:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.82 on epoch=19
02/24/2022 15:28:57 - INFO - __main__ - Step 50 Global step 50 Train loss 0.79 on epoch=24
02/24/2022 15:29:08 - INFO - __main__ - Global step 50 Train loss 0.88 Rouge-L 0.6596796756084554 on epoch=24
02/24/2022 15:29:08 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.6596796756084554 on epoch=24, global_step=50
02/24/2022 15:29:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.74 on epoch=29
02/24/2022 15:29:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.71 on epoch=34
02/24/2022 15:29:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.72 on epoch=39
02/24/2022 15:29:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.68 on epoch=44
02/24/2022 15:29:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.67 on epoch=49
02/24/2022 15:29:28 - INFO - __main__ - Global step 100 Train loss 0.70 Rouge-L 0.6883920273281048 on epoch=49
02/24/2022 15:29:28 - INFO - __main__ - Saving model with best Rouge-L: 0.6596796756084554 -> 0.6883920273281048 on epoch=49, global_step=100
02/24/2022 15:29:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.66 on epoch=54
02/24/2022 15:29:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.64 on epoch=59
02/24/2022 15:29:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.64 on epoch=64
02/24/2022 15:29:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.61 on epoch=69
02/24/2022 15:29:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.61 on epoch=74
02/24/2022 15:29:51 - INFO - __main__ - Global step 150 Train loss 0.63 Rouge-L 0.8245760711374517 on epoch=74
02/24/2022 15:29:51 - INFO - __main__ - Saving model with best Rouge-L: 0.6883920273281048 -> 0.8245760711374517 on epoch=74, global_step=150
02/24/2022 15:29:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=79
02/24/2022 15:29:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.58 on epoch=84
02/24/2022 15:29:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=89
02/24/2022 15:30:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=94
02/24/2022 15:30:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=99
02/24/2022 15:30:14 - INFO - __main__ - Global step 200 Train loss 0.56 Rouge-L 0.8599391588775614 on epoch=99
02/24/2022 15:30:14 - INFO - __main__ - Saving model with best Rouge-L: 0.8245760711374517 -> 0.8599391588775614 on epoch=99, global_step=200
02/24/2022 15:30:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=104
02/24/2022 15:30:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=109
02/24/2022 15:30:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=114
02/24/2022 15:30:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=119
02/24/2022 15:30:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=124
02/24/2022 15:30:40 - INFO - __main__ - Global step 250 Train loss 0.51 Rouge-L 0.8765198914293914 on epoch=124
02/24/2022 15:30:40 - INFO - __main__ - Saving model with best Rouge-L: 0.8599391588775614 -> 0.8765198914293914 on epoch=124, global_step=250
02/24/2022 15:30:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=129
02/24/2022 15:30:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=134
02/24/2022 15:30:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=139
02/24/2022 15:30:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=144
02/24/2022 15:30:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=149
02/24/2022 15:31:04 - INFO - __main__ - Global step 300 Train loss 0.46 Rouge-L 0.8753826790027214 on epoch=149
02/24/2022 15:31:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=154
02/24/2022 15:31:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=159
02/24/2022 15:31:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=164
02/24/2022 15:31:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=169
02/24/2022 15:31:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=174
02/24/2022 15:31:28 - INFO - __main__ - Global step 350 Train loss 0.43 Rouge-L 0.8830134036500994 on epoch=174
02/24/2022 15:31:28 - INFO - __main__ - Saving model with best Rouge-L: 0.8765198914293914 -> 0.8830134036500994 on epoch=174, global_step=350
02/24/2022 15:31:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=179
02/24/2022 15:31:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=184
02/24/2022 15:31:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=189
02/24/2022 15:31:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=194
02/24/2022 15:31:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=199
02/24/2022 15:31:51 - INFO - __main__ - Global step 400 Train loss 0.40 Rouge-L 0.8776446424600817 on epoch=199
02/24/2022 15:31:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=204
02/24/2022 15:31:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=209
02/24/2022 15:31:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=214
02/24/2022 15:32:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=219
02/24/2022 15:32:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=224
02/24/2022 15:32:14 - INFO - __main__ - Global step 450 Train loss 0.37 Rouge-L 0.8829207965795762 on epoch=224
02/24/2022 15:32:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=229
02/24/2022 15:32:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=234
02/24/2022 15:32:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=239
02/24/2022 15:32:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=244
02/24/2022 15:32:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=249
02/24/2022 15:32:36 - INFO - __main__ - Global step 500 Train loss 0.36 Rouge-L 0.8821583907515926 on epoch=249
02/24/2022 15:32:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=254
02/24/2022 15:32:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.35 on epoch=259
02/24/2022 15:32:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.35 on epoch=264
02/24/2022 15:32:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=269
02/24/2022 15:32:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=274
02/24/2022 15:32:59 - INFO - __main__ - Global step 550 Train loss 0.35 Rouge-L 0.8828676435884302 on epoch=274
02/24/2022 15:33:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.34 on epoch=279
02/24/2022 15:33:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=284
02/24/2022 15:33:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=289
02/24/2022 15:33:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=294
02/24/2022 15:33:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=299
02/24/2022 15:33:21 - INFO - __main__ - Global step 600 Train loss 0.32 Rouge-L 0.8861713745271373 on epoch=299
02/24/2022 15:33:21 - INFO - __main__ - Saving model with best Rouge-L: 0.8830134036500994 -> 0.8861713745271373 on epoch=299, global_step=600
02/24/2022 15:33:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.32 on epoch=304
02/24/2022 15:33:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=309
02/24/2022 15:33:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=314
02/24/2022 15:33:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=319
02/24/2022 15:33:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=324
02/24/2022 15:33:43 - INFO - __main__ - Global step 650 Train loss 0.31 Rouge-L 0.880576008208253 on epoch=324
02/24/2022 15:33:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.31 on epoch=329
02/24/2022 15:33:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=334
02/24/2022 15:33:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=339
02/24/2022 15:33:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.30 on epoch=344
02/24/2022 15:33:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=349
02/24/2022 15:34:06 - INFO - __main__ - Global step 700 Train loss 0.30 Rouge-L 0.8870768398854627 on epoch=349
02/24/2022 15:34:06 - INFO - __main__ - Saving model with best Rouge-L: 0.8861713745271373 -> 0.8870768398854627 on epoch=349, global_step=700
02/24/2022 15:34:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=354
02/24/2022 15:34:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=359
02/24/2022 15:34:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.28 on epoch=364
02/24/2022 15:34:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=369
02/24/2022 15:34:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.29 on epoch=374
02/24/2022 15:34:28 - INFO - __main__ - Global step 750 Train loss 0.29 Rouge-L 0.8866003785436918 on epoch=374
02/24/2022 15:34:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=379
02/24/2022 15:34:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.29 on epoch=384
02/24/2022 15:34:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=389
02/24/2022 15:34:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=394
02/24/2022 15:34:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=399
02/24/2022 15:34:51 - INFO - __main__ - Global step 800 Train loss 0.27 Rouge-L 0.8845564520107418 on epoch=399
02/24/2022 15:34:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=404
02/24/2022 15:34:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=409
02/24/2022 15:34:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=414
02/24/2022 15:35:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=419
02/24/2022 15:35:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=424
02/24/2022 15:35:13 - INFO - __main__ - Global step 850 Train loss 0.27 Rouge-L 0.8847098987959283 on epoch=424
02/24/2022 15:35:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=429
02/24/2022 15:35:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=434
02/24/2022 15:35:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=439
02/24/2022 15:35:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=444
02/24/2022 15:35:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=449
02/24/2022 15:35:36 - INFO - __main__ - Global step 900 Train loss 0.26 Rouge-L 0.886474674609953 on epoch=449
02/24/2022 15:35:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.25 on epoch=454
02/24/2022 15:35:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=459
02/24/2022 15:35:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=464
02/24/2022 15:35:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=469
02/24/2022 15:35:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=474
02/24/2022 15:35:58 - INFO - __main__ - Global step 950 Train loss 0.25 Rouge-L 0.8819906099418224 on epoch=474
02/24/2022 15:36:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=479
02/24/2022 15:36:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=484
02/24/2022 15:36:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=489
02/24/2022 15:36:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=494
02/24/2022 15:36:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=499
02/24/2022 15:36:22 - INFO - __main__ - Global step 1000 Train loss 0.24 Rouge-L 0.87218466030036 on epoch=499
02/24/2022 15:36:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=504
02/24/2022 15:36:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=509
02/24/2022 15:36:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=514
02/24/2022 15:36:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=519
02/24/2022 15:36:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=524
02/24/2022 15:36:44 - INFO - __main__ - Global step 1050 Train loss 0.23 Rouge-L 0.878202335405282 on epoch=524
02/24/2022 15:36:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=529
02/24/2022 15:36:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=534
02/24/2022 15:36:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=539
02/24/2022 15:36:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=544
02/24/2022 15:36:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=549
02/24/2022 15:37:08 - INFO - __main__ - Global step 1100 Train loss 0.22 Rouge-L 0.8843922410295062 on epoch=549
02/24/2022 15:37:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=554
02/24/2022 15:37:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=559
02/24/2022 15:37:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=564
02/24/2022 15:37:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=569
02/24/2022 15:37:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=574
02/24/2022 15:37:30 - INFO - __main__ - Global step 1150 Train loss 0.21 Rouge-L 0.8741624273348253 on epoch=574
02/24/2022 15:37:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=579
02/24/2022 15:37:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=584
02/24/2022 15:37:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=589
02/24/2022 15:37:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=594
02/24/2022 15:37:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=599
02/24/2022 15:37:52 - INFO - __main__ - Global step 1200 Train loss 0.20 Rouge-L 0.8771850032062867 on epoch=599
02/24/2022 15:37:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=604
02/24/2022 15:37:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=609
02/24/2022 15:37:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=614
02/24/2022 15:38:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=619
02/24/2022 15:38:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=624
02/24/2022 15:38:15 - INFO - __main__ - Global step 1250 Train loss 0.19 Rouge-L 0.875987149907546 on epoch=624
02/24/2022 15:38:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=629
02/24/2022 15:38:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=634
02/24/2022 15:38:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=639
02/24/2022 15:38:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=644
02/24/2022 15:38:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=649
02/24/2022 15:38:38 - INFO - __main__ - Global step 1300 Train loss 0.19 Rouge-L 0.8789824699629708 on epoch=649
02/24/2022 15:38:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=654
02/24/2022 15:38:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=659
02/24/2022 15:38:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=664
02/24/2022 15:38:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=669
02/24/2022 15:38:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=674
02/24/2022 15:38:59 - INFO - __main__ - Global step 1350 Train loss 0.18 Rouge-L 0.8662767120430586 on epoch=674
02/24/2022 15:39:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=679
02/24/2022 15:39:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=684
02/24/2022 15:39:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=689
02/24/2022 15:39:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=694
02/24/2022 15:39:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=699
02/24/2022 15:39:21 - INFO - __main__ - Global step 1400 Train loss 0.17 Rouge-L 0.8730630444936589 on epoch=699
02/24/2022 15:39:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=704
02/24/2022 15:39:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=709
02/24/2022 15:39:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=714
02/24/2022 15:39:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=719
02/24/2022 15:39:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=724
02/24/2022 15:39:43 - INFO - __main__ - Global step 1450 Train loss 0.17 Rouge-L 0.8885652790970938 on epoch=724
02/24/2022 15:39:43 - INFO - __main__ - Saving model with best Rouge-L: 0.8870768398854627 -> 0.8885652790970938 on epoch=724, global_step=1450
02/24/2022 15:39:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=729
02/24/2022 15:39:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=734
02/24/2022 15:39:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=739
02/24/2022 15:39:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=744
02/24/2022 15:39:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=749
02/24/2022 15:40:06 - INFO - __main__ - Global step 1500 Train loss 0.16 Rouge-L 0.8848232921412715 on epoch=749
02/24/2022 15:40:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=754
02/24/2022 15:40:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=759
02/24/2022 15:40:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=764
02/24/2022 15:40:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=769
02/24/2022 15:40:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=774
02/24/2022 15:40:28 - INFO - __main__ - Global step 1550 Train loss 0.16 Rouge-L 0.8715438636547126 on epoch=774
02/24/2022 15:40:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=779
02/24/2022 15:40:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=784
02/24/2022 15:40:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.15 on epoch=789
02/24/2022 15:40:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=794
02/24/2022 15:40:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=799
02/24/2022 15:40:50 - INFO - __main__ - Global step 1600 Train loss 0.15 Rouge-L 0.8761963829486644 on epoch=799
02/24/2022 15:40:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.16 on epoch=804
02/24/2022 15:40:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=809
02/24/2022 15:40:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=814
02/24/2022 15:40:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=819
02/24/2022 15:41:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=824
02/24/2022 15:41:12 - INFO - __main__ - Global step 1650 Train loss 0.15 Rouge-L 0.8712367247315137 on epoch=824
02/24/2022 15:41:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=829
02/24/2022 15:41:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=834
02/24/2022 15:41:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=839
02/24/2022 15:41:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=844
02/24/2022 15:41:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=849
02/24/2022 15:41:37 - INFO - __main__ - Global step 1700 Train loss 0.14 Rouge-L 0.8676805506121659 on epoch=849
02/24/2022 15:41:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=854
02/24/2022 15:41:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=859
02/24/2022 15:41:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=864
02/24/2022 15:41:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=869
02/24/2022 15:41:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=874
02/24/2022 15:42:02 - INFO - __main__ - Global step 1750 Train loss 0.13 Rouge-L 0.8595222934015762 on epoch=874
02/24/2022 15:42:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=879
02/24/2022 15:42:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=884
02/24/2022 15:42:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=889
02/24/2022 15:42:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=894
02/24/2022 15:42:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=899
02/24/2022 15:42:24 - INFO - __main__ - Global step 1800 Train loss 0.13 Rouge-L 0.8780113198998025 on epoch=899
02/24/2022 15:42:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=904
02/24/2022 15:42:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=909
02/24/2022 15:42:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=914
02/24/2022 15:42:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=919
02/24/2022 15:42:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=924
02/24/2022 15:42:46 - INFO - __main__ - Global step 1850 Train loss 0.12 Rouge-L 0.8780602349078833 on epoch=924
02/24/2022 15:42:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=929
02/24/2022 15:42:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=934
02/24/2022 15:42:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=939
02/24/2022 15:42:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=944
02/24/2022 15:42:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=949
02/24/2022 15:43:08 - INFO - __main__ - Global step 1900 Train loss 0.12 Rouge-L 0.8794377531278719 on epoch=949
02/24/2022 15:43:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=954
02/24/2022 15:43:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=959
02/24/2022 15:43:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=964
02/24/2022 15:43:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=969
02/24/2022 15:43:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=974
02/24/2022 15:43:30 - INFO - __main__ - Global step 1950 Train loss 0.11 Rouge-L 0.8656928838170629 on epoch=974
02/24/2022 15:43:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=979
02/24/2022 15:43:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=984
02/24/2022 15:43:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=989
02/24/2022 15:43:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=994
02/24/2022 15:43:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=999
02/24/2022 15:43:44 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 15:43:44 - INFO - __main__ - Printing 3 examples
02/24/2022 15:43:44 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/24/2022 15:43:44 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/24/2022 15:43:44 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/24/2022 15:43:44 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/24/2022 15:43:44 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/24/2022 15:43:44 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/24/2022 15:43:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/24/2022 15:43:44 - INFO - __main__ - Tokenizing Output ...
02/24/2022 15:43:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/24/2022 15:43:44 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 15:43:44 - INFO - __main__ - Printing 3 examples
02/24/2022 15:43:44 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/24/2022 15:43:44 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/24/2022 15:43:44 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/24/2022 15:43:44 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/24/2022 15:43:44 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/24/2022 15:43:44 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/24/2022 15:43:44 - INFO - __main__ - Tokenizing Input ...
02/24/2022 15:43:44 - INFO - __main__ - Tokenizing Output ...
02/24/2022 15:43:44 - INFO - __main__ - Loaded 32 examples from dev data
02/24/2022 15:43:55 - INFO - __main__ - Global step 2000 Train loss 0.11 Rouge-L 0.881195735268306 on epoch=999
02/24/2022 15:43:55 - INFO - __main__ - save last model!
02/24/2022 15:43:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/24/2022 15:43:55 - INFO - __main__ - Start tokenizing ... 5000 instances
02/24/2022 15:43:55 - INFO - __main__ - Printing 3 examples
02/24/2022 15:43:55 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
02/24/2022 15:43:55 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
02/24/2022 15:43:55 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
02/24/2022 15:43:55 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
02/24/2022 15:43:55 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
02/24/2022 15:43:55 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
02/24/2022 15:43:55 - INFO - __main__ - Tokenizing Input ...
02/24/2022 15:43:57 - INFO - __main__ - Tokenizing Output ...
02/24/2022 15:43:58 - INFO - __main__ - load prompt embedding from ckpt
02/24/2022 15:43:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/24/2022 15:43:59 - INFO - __main__ - Starting training!
02/24/2022 15:44:03 - INFO - __main__ - Loaded 5000 examples from test data
02/24/2022 16:13:13 - INFO - __main__ - Saved prediction in models/T5-large-reptile-random-3e-5-2-5000-5e-1-10/singletask-wiki_split/wiki_split_32_100_0.4_8_predictions.txt
02/24/2022 16:13:19 - INFO - __main__ - Rouge-L on test data: 0.8772
02/24/2022 16:13:19 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.4, bsz=8, dev_performance=0.8885652790970938, test_performance=0.8772010860235594
02/24/2022 16:13:19 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.3, bsz=8 ...
02/24/2022 16:13:20 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 16:13:20 - INFO - __main__ - Printing 3 examples
02/24/2022 16:13:20 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/24/2022 16:13:20 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/24/2022 16:13:20 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/24/2022 16:13:20 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/24/2022 16:13:20 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/24/2022 16:13:20 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/24/2022 16:13:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/24/2022 16:13:20 - INFO - __main__ - Tokenizing Output ...
02/24/2022 16:13:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/24/2022 16:13:20 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 16:13:20 - INFO - __main__ - Printing 3 examples
02/24/2022 16:13:20 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/24/2022 16:13:20 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/24/2022 16:13:20 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/24/2022 16:13:20 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/24/2022 16:13:20 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/24/2022 16:13:20 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/24/2022 16:13:20 - INFO - __main__ - Tokenizing Input ...
02/24/2022 16:13:20 - INFO - __main__ - Tokenizing Output ...
02/24/2022 16:13:20 - INFO - __main__ - Loaded 32 examples from dev data
02/24/2022 16:13:32 - INFO - __main__ - load prompt embedding from ckpt
02/24/2022 16:13:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/24/2022 16:13:33 - INFO - __main__ - Starting training!
02/24/2022 16:13:36 - INFO - __main__ - Step 10 Global step 10 Train loss 1.00 on epoch=4
02/24/2022 16:13:38 - INFO - __main__ - Step 20 Global step 20 Train loss 0.93 on epoch=9
02/24/2022 16:13:41 - INFO - __main__ - Step 30 Global step 30 Train loss 0.89 on epoch=14
02/24/2022 16:13:43 - INFO - __main__ - Step 40 Global step 40 Train loss 0.87 on epoch=19
02/24/2022 16:13:45 - INFO - __main__ - Step 50 Global step 50 Train loss 0.84 on epoch=24
02/24/2022 16:13:55 - INFO - __main__ - Global step 50 Train loss 0.91 Rouge-L 0.6981517262678147 on epoch=24
02/24/2022 16:13:55 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.6981517262678147 on epoch=24, global_step=50
02/24/2022 16:13:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.77 on epoch=29
02/24/2022 16:14:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.73 on epoch=34
02/24/2022 16:14:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.71 on epoch=39
02/24/2022 16:14:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.68 on epoch=44
02/24/2022 16:14:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.67 on epoch=49
02/24/2022 16:14:17 - INFO - __main__ - Global step 100 Train loss 0.71 Rouge-L 0.7038465500416022 on epoch=49
02/24/2022 16:14:17 - INFO - __main__ - Saving model with best Rouge-L: 0.6981517262678147 -> 0.7038465500416022 on epoch=49, global_step=100
02/24/2022 16:14:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.66 on epoch=54
02/24/2022 16:14:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.65 on epoch=59
02/24/2022 16:14:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.63 on epoch=64
02/24/2022 16:14:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.63 on epoch=69
02/24/2022 16:14:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.62 on epoch=74
02/24/2022 16:14:39 - INFO - __main__ - Global step 150 Train loss 0.64 Rouge-L 0.7067792762638787 on epoch=74
02/24/2022 16:14:39 - INFO - __main__ - Saving model with best Rouge-L: 0.7038465500416022 -> 0.7067792762638787 on epoch=74, global_step=150
02/24/2022 16:14:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.61 on epoch=79
02/24/2022 16:14:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.61 on epoch=84
02/24/2022 16:14:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.59 on epoch=89
02/24/2022 16:14:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.59 on epoch=94
02/24/2022 16:14:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.59 on epoch=99
02/24/2022 16:15:01 - INFO - __main__ - Global step 200 Train loss 0.60 Rouge-L 0.788014683474231 on epoch=99
02/24/2022 16:15:01 - INFO - __main__ - Saving model with best Rouge-L: 0.7067792762638787 -> 0.788014683474231 on epoch=99, global_step=200
02/24/2022 16:15:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.57 on epoch=104
02/24/2022 16:15:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=109
02/24/2022 16:15:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=114
02/24/2022 16:15:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=119
02/24/2022 16:15:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=124
02/24/2022 16:15:24 - INFO - __main__ - Global step 250 Train loss 0.56 Rouge-L 0.8358847579865991 on epoch=124
02/24/2022 16:15:24 - INFO - __main__ - Saving model with best Rouge-L: 0.788014683474231 -> 0.8358847579865991 on epoch=124, global_step=250
02/24/2022 16:15:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.54 on epoch=129
02/24/2022 16:15:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=134
02/24/2022 16:15:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.55 on epoch=139
02/24/2022 16:15:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=144
02/24/2022 16:15:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.54 on epoch=149
02/24/2022 16:15:47 - INFO - __main__ - Global step 300 Train loss 0.54 Rouge-L 0.848774324527278 on epoch=149
02/24/2022 16:15:47 - INFO - __main__ - Saving model with best Rouge-L: 0.8358847579865991 -> 0.848774324527278 on epoch=149, global_step=300
02/24/2022 16:15:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=154
02/24/2022 16:15:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=159
02/24/2022 16:15:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=164
02/24/2022 16:15:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=169
02/24/2022 16:15:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=174
02/24/2022 16:16:10 - INFO - __main__ - Global step 350 Train loss 0.51 Rouge-L 0.8531306749793567 on epoch=174
02/24/2022 16:16:10 - INFO - __main__ - Saving model with best Rouge-L: 0.848774324527278 -> 0.8531306749793567 on epoch=174, global_step=350
02/24/2022 16:16:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=179
02/24/2022 16:16:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=184
02/24/2022 16:16:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=189
02/24/2022 16:16:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=194
02/24/2022 16:16:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=199
02/24/2022 16:16:34 - INFO - __main__ - Global step 400 Train loss 0.49 Rouge-L 0.8625355311884237 on epoch=199
02/24/2022 16:16:34 - INFO - __main__ - Saving model with best Rouge-L: 0.8531306749793567 -> 0.8625355311884237 on epoch=199, global_step=400
02/24/2022 16:16:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=204
02/24/2022 16:16:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=209
02/24/2022 16:16:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=214
02/24/2022 16:16:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=219
02/24/2022 16:16:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=224
02/24/2022 16:16:58 - INFO - __main__ - Global step 450 Train loss 0.47 Rouge-L 0.8483661503797161 on epoch=224
02/24/2022 16:17:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=229
02/24/2022 16:17:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=234
02/24/2022 16:17:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=239
02/24/2022 16:17:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=244
02/24/2022 16:17:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=249
02/24/2022 16:17:23 - INFO - __main__ - Global step 500 Train loss 0.45 Rouge-L 0.8583500797741956 on epoch=249
02/24/2022 16:17:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=254
02/24/2022 16:17:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=259
02/24/2022 16:17:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=264
02/24/2022 16:17:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=269
02/24/2022 16:17:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=274
02/24/2022 16:17:48 - INFO - __main__ - Global step 550 Train loss 0.43 Rouge-L 0.8666149304300126 on epoch=274
02/24/2022 16:17:48 - INFO - __main__ - Saving model with best Rouge-L: 0.8625355311884237 -> 0.8666149304300126 on epoch=274, global_step=550
02/24/2022 16:17:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=279
02/24/2022 16:17:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=284
02/24/2022 16:17:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=289
02/24/2022 16:17:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=294
02/24/2022 16:17:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=299
02/24/2022 16:18:11 - INFO - __main__ - Global step 600 Train loss 0.42 Rouge-L 0.8612207817261669 on epoch=299
02/24/2022 16:18:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=304
02/24/2022 16:18:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=309
02/24/2022 16:18:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=314
02/24/2022 16:18:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=319
02/24/2022 16:18:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=324
02/24/2022 16:18:34 - INFO - __main__ - Global step 650 Train loss 0.41 Rouge-L 0.8548773564663943 on epoch=324
02/24/2022 16:18:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=329
02/24/2022 16:18:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=334
02/24/2022 16:18:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=339
02/24/2022 16:18:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=344
02/24/2022 16:18:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=349
02/24/2022 16:18:59 - INFO - __main__ - Global step 700 Train loss 0.40 Rouge-L 0.8622280671640112 on epoch=349
02/24/2022 16:19:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=354
02/24/2022 16:19:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=359
02/24/2022 16:19:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=364
02/24/2022 16:19:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=369
02/24/2022 16:19:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=374
02/24/2022 16:19:22 - INFO - __main__ - Global step 750 Train loss 0.39 Rouge-L 0.87078008888656 on epoch=374
02/24/2022 16:19:22 - INFO - __main__ - Saving model with best Rouge-L: 0.8666149304300126 -> 0.87078008888656 on epoch=374, global_step=750
02/24/2022 16:19:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=379
02/24/2022 16:19:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=384
02/24/2022 16:19:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=389
02/24/2022 16:19:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=394
02/24/2022 16:19:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=399
02/24/2022 16:19:47 - INFO - __main__ - Global step 800 Train loss 0.38 Rouge-L 0.8681099394013112 on epoch=399
02/24/2022 16:19:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=404
02/24/2022 16:19:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=409
02/24/2022 16:19:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=414
02/24/2022 16:19:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=419
02/24/2022 16:19:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=424
02/24/2022 16:20:11 - INFO - __main__ - Global step 850 Train loss 0.37 Rouge-L 0.8729711989086805 on epoch=424
02/24/2022 16:20:11 - INFO - __main__ - Saving model with best Rouge-L: 0.87078008888656 -> 0.8729711989086805 on epoch=424, global_step=850
02/24/2022 16:20:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=429
02/24/2022 16:20:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=434
02/24/2022 16:20:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=439
02/24/2022 16:20:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=444
02/24/2022 16:20:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=449
02/24/2022 16:20:35 - INFO - __main__ - Global step 900 Train loss 0.36 Rouge-L 0.8801451921282428 on epoch=449
02/24/2022 16:20:35 - INFO - __main__ - Saving model with best Rouge-L: 0.8729711989086805 -> 0.8801451921282428 on epoch=449, global_step=900
02/24/2022 16:20:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.34 on epoch=454
02/24/2022 16:20:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=459
02/24/2022 16:20:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=464
02/24/2022 16:20:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=469
02/24/2022 16:20:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=474
02/24/2022 16:20:59 - INFO - __main__ - Global step 950 Train loss 0.34 Rouge-L 0.8764314627090266 on epoch=474
02/24/2022 16:21:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=479
02/24/2022 16:21:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=484
02/24/2022 16:21:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=489
02/24/2022 16:21:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=494
02/24/2022 16:21:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.33 on epoch=499
02/24/2022 16:21:22 - INFO - __main__ - Global step 1000 Train loss 0.34 Rouge-L 0.8793080958187799 on epoch=499
02/24/2022 16:21:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=504
02/24/2022 16:21:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=509
02/24/2022 16:21:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=514
02/24/2022 16:21:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=519
02/24/2022 16:21:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=524
02/24/2022 16:21:46 - INFO - __main__ - Global step 1050 Train loss 0.33 Rouge-L 0.8785603283989423 on epoch=524
02/24/2022 16:21:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=529
02/24/2022 16:21:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.31 on epoch=534
02/24/2022 16:21:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=539
02/24/2022 16:21:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=544
02/24/2022 16:21:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=549
02/24/2022 16:22:08 - INFO - __main__ - Global step 1100 Train loss 0.32 Rouge-L 0.8844418107133564 on epoch=549
02/24/2022 16:22:08 - INFO - __main__ - Saving model with best Rouge-L: 0.8801451921282428 -> 0.8844418107133564 on epoch=549, global_step=1100
02/24/2022 16:22:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=554
02/24/2022 16:22:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.31 on epoch=559
02/24/2022 16:22:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.32 on epoch=564
02/24/2022 16:22:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=569
02/24/2022 16:22:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.31 on epoch=574
02/24/2022 16:22:32 - INFO - __main__ - Global step 1150 Train loss 0.31 Rouge-L 0.882223615064686 on epoch=574
02/24/2022 16:22:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=579
02/24/2022 16:22:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=584
02/24/2022 16:22:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=589
02/24/2022 16:22:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.30 on epoch=594
02/24/2022 16:22:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=599
02/24/2022 16:22:55 - INFO - __main__ - Global step 1200 Train loss 0.30 Rouge-L 0.8834222434276765 on epoch=599
02/24/2022 16:22:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.29 on epoch=604
02/24/2022 16:22:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.29 on epoch=609
02/24/2022 16:23:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=614
02/24/2022 16:23:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=619
02/24/2022 16:23:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=624
02/24/2022 16:23:19 - INFO - __main__ - Global step 1250 Train loss 0.29 Rouge-L 0.8823988225727135 on epoch=624
02/24/2022 16:23:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.31 on epoch=629
02/24/2022 16:23:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=634
02/24/2022 16:23:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.30 on epoch=639
02/24/2022 16:23:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.29 on epoch=644
02/24/2022 16:23:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.28 on epoch=649
02/24/2022 16:23:44 - INFO - __main__ - Global step 1300 Train loss 0.29 Rouge-L 0.8845703917541804 on epoch=649
02/24/2022 16:23:44 - INFO - __main__ - Saving model with best Rouge-L: 0.8844418107133564 -> 0.8845703917541804 on epoch=649, global_step=1300
02/24/2022 16:23:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=654
02/24/2022 16:23:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.29 on epoch=659
02/24/2022 16:23:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=664
02/24/2022 16:23:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.27 on epoch=669
02/24/2022 16:23:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.27 on epoch=674
02/24/2022 16:24:06 - INFO - __main__ - Global step 1350 Train loss 0.28 Rouge-L 0.8817619447786413 on epoch=674
02/24/2022 16:24:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=679
02/24/2022 16:24:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.28 on epoch=684
02/24/2022 16:24:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=689
02/24/2022 16:24:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.28 on epoch=694
02/24/2022 16:24:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=699
02/24/2022 16:24:29 - INFO - __main__ - Global step 1400 Train loss 0.27 Rouge-L 0.8768732410099309 on epoch=699
02/24/2022 16:24:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=704
02/24/2022 16:24:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=709
02/24/2022 16:24:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=714
02/24/2022 16:24:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.26 on epoch=719
02/24/2022 16:24:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.27 on epoch=724
02/24/2022 16:24:51 - INFO - __main__ - Global step 1450 Train loss 0.27 Rouge-L 0.8863109134541046 on epoch=724
02/24/2022 16:24:51 - INFO - __main__ - Saving model with best Rouge-L: 0.8845703917541804 -> 0.8863109134541046 on epoch=724, global_step=1450
02/24/2022 16:24:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=729
02/24/2022 16:24:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=734
02/24/2022 16:24:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=739
02/24/2022 16:25:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=744
02/24/2022 16:25:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=749
02/24/2022 16:25:13 - INFO - __main__ - Global step 1500 Train loss 0.26 Rouge-L 0.8825831841586484 on epoch=749
02/24/2022 16:25:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.23 on epoch=754
02/24/2022 16:25:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.24 on epoch=759
02/24/2022 16:25:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=764
02/24/2022 16:25:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=769
02/24/2022 16:25:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=774
02/24/2022 16:25:36 - INFO - __main__ - Global step 1550 Train loss 0.25 Rouge-L 0.8820157004860285 on epoch=774
02/24/2022 16:25:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=779
02/24/2022 16:25:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=784
02/24/2022 16:25:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=789
02/24/2022 16:25:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=794
02/24/2022 16:25:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=799
02/24/2022 16:25:58 - INFO - __main__ - Global step 1600 Train loss 0.24 Rouge-L 0.8874634104281712 on epoch=799
02/24/2022 16:25:58 - INFO - __main__ - Saving model with best Rouge-L: 0.8863109134541046 -> 0.8874634104281712 on epoch=799, global_step=1600
02/24/2022 16:26:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=804
02/24/2022 16:26:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=809
02/24/2022 16:26:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=814
02/24/2022 16:26:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.23 on epoch=819
02/24/2022 16:26:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=824
02/24/2022 16:26:20 - INFO - __main__ - Global step 1650 Train loss 0.24 Rouge-L 0.8853269399722278 on epoch=824
02/24/2022 16:26:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=829
02/24/2022 16:26:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=834
02/24/2022 16:26:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.24 on epoch=839
02/24/2022 16:26:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=844
02/24/2022 16:26:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=849
02/24/2022 16:26:42 - INFO - __main__ - Global step 1700 Train loss 0.23 Rouge-L 0.883915343996524 on epoch=849
02/24/2022 16:26:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=854
02/24/2022 16:26:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=859
02/24/2022 16:26:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=864
02/24/2022 16:26:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=869
02/24/2022 16:26:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=874
02/24/2022 16:27:05 - INFO - __main__ - Global step 1750 Train loss 0.23 Rouge-L 0.8881035192298669 on epoch=874
02/24/2022 16:27:05 - INFO - __main__ - Saving model with best Rouge-L: 0.8874634104281712 -> 0.8881035192298669 on epoch=874, global_step=1750
02/24/2022 16:27:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=879
02/24/2022 16:27:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=884
02/24/2022 16:27:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=889
02/24/2022 16:27:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=894
02/24/2022 16:27:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=899
02/24/2022 16:27:27 - INFO - __main__ - Global step 1800 Train loss 0.22 Rouge-L 0.8802477403832614 on epoch=899
02/24/2022 16:27:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=904
02/24/2022 16:27:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=909
02/24/2022 16:27:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.22 on epoch=914
02/24/2022 16:27:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=919
02/24/2022 16:27:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=924
02/24/2022 16:27:50 - INFO - __main__ - Global step 1850 Train loss 0.21 Rouge-L 0.8783200943476778 on epoch=924
02/24/2022 16:27:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=929
02/24/2022 16:27:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=934
02/24/2022 16:27:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=939
02/24/2022 16:27:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=944
02/24/2022 16:28:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=949
02/24/2022 16:28:16 - INFO - __main__ - Global step 1900 Train loss 0.21 Rouge-L 0.8790927387516878 on epoch=949
02/24/2022 16:28:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=954
02/24/2022 16:28:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=959
02/24/2022 16:28:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=964
02/24/2022 16:28:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=969
02/24/2022 16:28:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=974
02/24/2022 16:28:37 - INFO - __main__ - Global step 1950 Train loss 0.21 Rouge-L 0.8752496297644563 on epoch=974
02/24/2022 16:28:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=979
02/24/2022 16:28:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=984
02/24/2022 16:28:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=989
02/24/2022 16:28:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=994
02/24/2022 16:28:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=999
02/24/2022 16:28:49 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 16:28:49 - INFO - __main__ - Printing 3 examples
02/24/2022 16:28:49 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/24/2022 16:28:49 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/24/2022 16:28:49 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/24/2022 16:28:49 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/24/2022 16:28:49 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/24/2022 16:28:49 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/24/2022 16:28:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/24/2022 16:28:49 - INFO - __main__ - Tokenizing Output ...
02/24/2022 16:28:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/24/2022 16:28:49 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 16:28:49 - INFO - __main__ - Printing 3 examples
02/24/2022 16:28:49 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/24/2022 16:28:49 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/24/2022 16:28:49 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/24/2022 16:28:49 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/24/2022 16:28:49 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/24/2022 16:28:49 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/24/2022 16:28:49 - INFO - __main__ - Tokenizing Input ...
02/24/2022 16:28:49 - INFO - __main__ - Tokenizing Output ...
02/24/2022 16:28:50 - INFO - __main__ - Loaded 32 examples from dev data
02/24/2022 16:28:59 - INFO - __main__ - Global step 2000 Train loss 0.20 Rouge-L 0.8775616794096478 on epoch=999
02/24/2022 16:28:59 - INFO - __main__ - save last model!
02/24/2022 16:28:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/24/2022 16:28:59 - INFO - __main__ - Start tokenizing ... 5000 instances
02/24/2022 16:28:59 - INFO - __main__ - Printing 3 examples
02/24/2022 16:28:59 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
02/24/2022 16:28:59 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
02/24/2022 16:28:59 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
02/24/2022 16:28:59 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
02/24/2022 16:28:59 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
02/24/2022 16:28:59 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
02/24/2022 16:28:59 - INFO - __main__ - Tokenizing Input ...
02/24/2022 16:29:01 - INFO - __main__ - Tokenizing Output ...
02/24/2022 16:29:02 - INFO - __main__ - load prompt embedding from ckpt
02/24/2022 16:29:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/24/2022 16:29:03 - INFO - __main__ - Starting training!
02/24/2022 16:29:07 - INFO - __main__ - Loaded 5000 examples from test data
02/24/2022 16:58:10 - INFO - __main__ - Saved prediction in models/T5-large-reptile-random-3e-5-2-5000-5e-1-10/singletask-wiki_split/wiki_split_32_100_0.3_8_predictions.txt
02/24/2022 16:58:15 - INFO - __main__ - Rouge-L on test data: 0.8818
02/24/2022 16:58:16 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.3, bsz=8, dev_performance=0.8881035192298669, test_performance=0.8818152803985697
02/24/2022 16:58:16 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.2, bsz=8 ...
02/24/2022 16:58:17 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 16:58:17 - INFO - __main__ - Printing 3 examples
02/24/2022 16:58:17 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/24/2022 16:58:17 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/24/2022 16:58:17 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/24/2022 16:58:17 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/24/2022 16:58:17 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/24/2022 16:58:17 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/24/2022 16:58:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/24/2022 16:58:17 - INFO - __main__ - Tokenizing Output ...
02/24/2022 16:58:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/24/2022 16:58:17 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 16:58:17 - INFO - __main__ - Printing 3 examples
02/24/2022 16:58:17 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/24/2022 16:58:17 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/24/2022 16:58:17 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/24/2022 16:58:17 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/24/2022 16:58:17 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/24/2022 16:58:17 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/24/2022 16:58:17 - INFO - __main__ - Tokenizing Input ...
02/24/2022 16:58:17 - INFO - __main__ - Tokenizing Output ...
02/24/2022 16:58:17 - INFO - __main__ - Loaded 32 examples from dev data
02/24/2022 16:58:29 - INFO - __main__ - load prompt embedding from ckpt
02/24/2022 16:58:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/24/2022 16:58:30 - INFO - __main__ - Starting training!
02/24/2022 16:58:33 - INFO - __main__ - Step 10 Global step 10 Train loss 0.97 on epoch=4
02/24/2022 16:58:35 - INFO - __main__ - Step 20 Global step 20 Train loss 0.92 on epoch=9
02/24/2022 16:58:38 - INFO - __main__ - Step 30 Global step 30 Train loss 0.87 on epoch=14
02/24/2022 16:58:40 - INFO - __main__ - Step 40 Global step 40 Train loss 0.84 on epoch=19
02/24/2022 16:58:42 - INFO - __main__ - Step 50 Global step 50 Train loss 0.84 on epoch=24
02/24/2022 16:58:56 - INFO - __main__ - Global step 50 Train loss 0.89 Rouge-L 0.6465096531329282 on epoch=24
02/24/2022 16:58:56 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.6465096531329282 on epoch=24, global_step=50
02/24/2022 16:58:59 - INFO - __main__ - Step 60 Global step 60 Train loss 0.81 on epoch=29
02/24/2022 16:59:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.77 on epoch=34
02/24/2022 16:59:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.76 on epoch=39
02/24/2022 16:59:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.76 on epoch=44
02/24/2022 16:59:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.72 on epoch=49
02/24/2022 16:59:16 - INFO - __main__ - Global step 100 Train loss 0.77 Rouge-L 0.6650319151589268 on epoch=49
02/24/2022 16:59:16 - INFO - __main__ - Saving model with best Rouge-L: 0.6465096531329282 -> 0.6650319151589268 on epoch=49, global_step=100
02/24/2022 16:59:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.72 on epoch=54
02/24/2022 16:59:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.70 on epoch=59
02/24/2022 16:59:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.67 on epoch=64
02/24/2022 16:59:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.68 on epoch=69
02/24/2022 16:59:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.67 on epoch=74
02/24/2022 16:59:37 - INFO - __main__ - Global step 150 Train loss 0.69 Rouge-L 0.6812994580535583 on epoch=74
02/24/2022 16:59:37 - INFO - __main__ - Saving model with best Rouge-L: 0.6650319151589268 -> 0.6812994580535583 on epoch=74, global_step=150
02/24/2022 16:59:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.66 on epoch=79
02/24/2022 16:59:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.66 on epoch=84
02/24/2022 16:59:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.65 on epoch=89
02/24/2022 16:59:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.65 on epoch=94
02/24/2022 16:59:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=99
02/24/2022 16:59:58 - INFO - __main__ - Global step 200 Train loss 0.65 Rouge-L 0.7207457541042943 on epoch=99
02/24/2022 16:59:58 - INFO - __main__ - Saving model with best Rouge-L: 0.6812994580535583 -> 0.7207457541042943 on epoch=99, global_step=200
02/24/2022 17:00:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.63 on epoch=104
02/24/2022 17:00:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.62 on epoch=109
02/24/2022 17:00:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.61 on epoch=114
02/24/2022 17:00:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.61 on epoch=119
02/24/2022 17:00:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.60 on epoch=124
02/24/2022 17:00:21 - INFO - __main__ - Global step 250 Train loss 0.62 Rouge-L 0.8060297547286582 on epoch=124
02/24/2022 17:00:21 - INFO - __main__ - Saving model with best Rouge-L: 0.7207457541042943 -> 0.8060297547286582 on epoch=124, global_step=250
02/24/2022 17:00:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.58 on epoch=129
02/24/2022 17:00:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=134
02/24/2022 17:00:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.58 on epoch=139
02/24/2022 17:00:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.59 on epoch=144
02/24/2022 17:00:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=149
02/24/2022 17:00:44 - INFO - __main__ - Global step 300 Train loss 0.58 Rouge-L 0.8617125712284976 on epoch=149
02/24/2022 17:00:44 - INFO - __main__ - Saving model with best Rouge-L: 0.8060297547286582 -> 0.8617125712284976 on epoch=149, global_step=300
02/24/2022 17:00:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.56 on epoch=154
02/24/2022 17:00:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=159
02/24/2022 17:00:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.55 on epoch=164
02/24/2022 17:00:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.56 on epoch=169
02/24/2022 17:00:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.55 on epoch=174
02/24/2022 17:01:09 - INFO - __main__ - Global step 350 Train loss 0.56 Rouge-L 0.8705168781533594 on epoch=174
02/24/2022 17:01:09 - INFO - __main__ - Saving model with best Rouge-L: 0.8617125712284976 -> 0.8705168781533594 on epoch=174, global_step=350
02/24/2022 17:01:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=179
02/24/2022 17:01:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.54 on epoch=184
02/24/2022 17:01:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=189
02/24/2022 17:01:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=194
02/24/2022 17:01:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=199
02/24/2022 17:01:32 - INFO - __main__ - Global step 400 Train loss 0.53 Rouge-L 0.8779254201136975 on epoch=199
02/24/2022 17:01:32 - INFO - __main__ - Saving model with best Rouge-L: 0.8705168781533594 -> 0.8779254201136975 on epoch=199, global_step=400
02/24/2022 17:01:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=204
02/24/2022 17:01:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.52 on epoch=209
02/24/2022 17:01:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=214
02/24/2022 17:01:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=219
02/24/2022 17:01:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=224
02/24/2022 17:01:56 - INFO - __main__ - Global step 450 Train loss 0.51 Rouge-L 0.8786872675483682 on epoch=224
02/24/2022 17:01:56 - INFO - __main__ - Saving model with best Rouge-L: 0.8779254201136975 -> 0.8786872675483682 on epoch=224, global_step=450
02/24/2022 17:01:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=229
02/24/2022 17:02:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=234
02/24/2022 17:02:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=239
02/24/2022 17:02:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=244
02/24/2022 17:02:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=249
02/24/2022 17:02:19 - INFO - __main__ - Global step 500 Train loss 0.49 Rouge-L 0.8817161912468774 on epoch=249
02/24/2022 17:02:19 - INFO - __main__ - Saving model with best Rouge-L: 0.8786872675483682 -> 0.8817161912468774 on epoch=249, global_step=500
02/24/2022 17:02:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=254
02/24/2022 17:02:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=259
02/24/2022 17:02:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=264
02/24/2022 17:02:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=269
02/24/2022 17:02:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=274
02/24/2022 17:02:40 - INFO - __main__ - Global step 550 Train loss 0.47 Rouge-L 0.8889198122204237 on epoch=274
02/24/2022 17:02:40 - INFO - __main__ - Saving model with best Rouge-L: 0.8817161912468774 -> 0.8889198122204237 on epoch=274, global_step=550
02/24/2022 17:02:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=279
02/24/2022 17:02:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=284
02/24/2022 17:02:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=289
02/24/2022 17:02:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=294
02/24/2022 17:02:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=299
02/24/2022 17:03:03 - INFO - __main__ - Global step 600 Train loss 0.47 Rouge-L 0.8804446526065172 on epoch=299
02/24/2022 17:03:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=304
02/24/2022 17:03:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=309
02/24/2022 17:03:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=314
02/24/2022 17:03:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=319
02/24/2022 17:03:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=324
02/24/2022 17:03:24 - INFO - __main__ - Global step 650 Train loss 0.45 Rouge-L 0.886984511239062 on epoch=324
02/24/2022 17:03:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=329
02/24/2022 17:03:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=334
02/24/2022 17:03:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=339
02/24/2022 17:03:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=344
02/24/2022 17:03:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=349
02/24/2022 17:03:46 - INFO - __main__ - Global step 700 Train loss 0.43 Rouge-L 0.8798765752942701 on epoch=349
02/24/2022 17:03:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=354
02/24/2022 17:03:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=359
02/24/2022 17:03:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=364
02/24/2022 17:03:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=369
02/24/2022 17:03:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=374
02/24/2022 17:04:08 - INFO - __main__ - Global step 750 Train loss 0.42 Rouge-L 0.8850507511607802 on epoch=374
02/24/2022 17:04:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=379
02/24/2022 17:04:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=384
02/24/2022 17:04:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=389
02/24/2022 17:04:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=394
02/24/2022 17:04:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=399
02/24/2022 17:04:29 - INFO - __main__ - Global step 800 Train loss 0.40 Rouge-L 0.8869921424597489 on epoch=399
02/24/2022 17:04:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=404
02/24/2022 17:04:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=409
02/24/2022 17:04:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=414
02/24/2022 17:04:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=419
02/24/2022 17:04:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=424
02/24/2022 17:04:51 - INFO - __main__ - Global step 850 Train loss 0.40 Rouge-L 0.8931181934376794 on epoch=424
02/24/2022 17:04:51 - INFO - __main__ - Saving model with best Rouge-L: 0.8889198122204237 -> 0.8931181934376794 on epoch=424, global_step=850
02/24/2022 17:04:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=429
02/24/2022 17:04:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=434
02/24/2022 17:04:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=439
02/24/2022 17:05:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=444
02/24/2022 17:05:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=449
02/24/2022 17:05:13 - INFO - __main__ - Global step 900 Train loss 0.38 Rouge-L 0.8864261364075616 on epoch=449
02/24/2022 17:05:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=454
02/24/2022 17:05:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=459
02/24/2022 17:05:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=464
02/24/2022 17:05:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=469
02/24/2022 17:05:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=474
02/24/2022 17:05:34 - INFO - __main__ - Global step 950 Train loss 0.37 Rouge-L 0.8864956570148989 on epoch=474
02/24/2022 17:05:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=479
02/24/2022 17:05:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=484
02/24/2022 17:05:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=489
02/24/2022 17:05:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=494
02/24/2022 17:05:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=499
02/24/2022 17:05:56 - INFO - __main__ - Global step 1000 Train loss 0.36 Rouge-L 0.8841563991205743 on epoch=499
02/24/2022 17:05:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=504
02/24/2022 17:06:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=509
02/24/2022 17:06:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=514
02/24/2022 17:06:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=519
02/24/2022 17:06:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=524
02/24/2022 17:06:17 - INFO - __main__ - Global step 1050 Train loss 0.35 Rouge-L 0.8919560289550515 on epoch=524
02/24/2022 17:06:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=529
02/24/2022 17:06:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=534
02/24/2022 17:06:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=539
02/24/2022 17:06:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.33 on epoch=544
02/24/2022 17:06:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.33 on epoch=549
02/24/2022 17:06:39 - INFO - __main__ - Global step 1100 Train loss 0.34 Rouge-L 0.8906415179586056 on epoch=549
02/24/2022 17:06:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=554
02/24/2022 17:06:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=559
02/24/2022 17:06:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=564
02/24/2022 17:06:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=569
02/24/2022 17:06:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=574
02/24/2022 17:07:02 - INFO - __main__ - Global step 1150 Train loss 0.33 Rouge-L 0.8888213386562208 on epoch=574
02/24/2022 17:07:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.32 on epoch=579
02/24/2022 17:07:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.32 on epoch=584
02/24/2022 17:07:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=589
02/24/2022 17:07:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=594
02/24/2022 17:07:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=599
02/24/2022 17:07:24 - INFO - __main__ - Global step 1200 Train loss 0.32 Rouge-L 0.8842624580070246 on epoch=599
02/24/2022 17:07:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=604
02/24/2022 17:07:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=609
02/24/2022 17:07:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=614
02/24/2022 17:07:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.31 on epoch=619
02/24/2022 17:07:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.32 on epoch=624
02/24/2022 17:07:46 - INFO - __main__ - Global step 1250 Train loss 0.32 Rouge-L 0.8898058019594914 on epoch=624
02/24/2022 17:07:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.30 on epoch=629
02/24/2022 17:07:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=634
02/24/2022 17:07:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=639
02/24/2022 17:07:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=644
02/24/2022 17:07:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=649
02/24/2022 17:08:08 - INFO - __main__ - Global step 1300 Train loss 0.31 Rouge-L 0.8882336867515372 on epoch=649
02/24/2022 17:08:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=654
02/24/2022 17:08:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.31 on epoch=659
02/24/2022 17:08:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=664
02/24/2022 17:08:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=669
02/24/2022 17:08:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=674
02/24/2022 17:08:31 - INFO - __main__ - Global step 1350 Train loss 0.31 Rouge-L 0.8904862147847865 on epoch=674
02/24/2022 17:08:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=679
02/24/2022 17:08:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=684
02/24/2022 17:08:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.30 on epoch=689
02/24/2022 17:08:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=694
02/24/2022 17:08:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=699
02/24/2022 17:08:53 - INFO - __main__ - Global step 1400 Train loss 0.31 Rouge-L 0.8895323049217458 on epoch=699
02/24/2022 17:08:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=704
02/24/2022 17:08:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=709
02/24/2022 17:09:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.29 on epoch=714
02/24/2022 17:09:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.29 on epoch=719
02/24/2022 17:09:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=724
02/24/2022 17:09:15 - INFO - __main__ - Global step 1450 Train loss 0.29 Rouge-L 0.8891382798699147 on epoch=724
02/24/2022 17:09:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=729
02/24/2022 17:09:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=734
02/24/2022 17:09:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=739
02/24/2022 17:09:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=744
02/24/2022 17:09:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.28 on epoch=749
02/24/2022 17:09:37 - INFO - __main__ - Global step 1500 Train loss 0.29 Rouge-L 0.8864165070124883 on epoch=749
02/24/2022 17:09:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=754
02/24/2022 17:09:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=759
02/24/2022 17:09:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.28 on epoch=764
02/24/2022 17:09:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=769
02/24/2022 17:09:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=774
02/24/2022 17:09:59 - INFO - __main__ - Global step 1550 Train loss 0.28 Rouge-L 0.8907274082077659 on epoch=774
02/24/2022 17:10:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.28 on epoch=779
02/24/2022 17:10:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=784
02/24/2022 17:10:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.28 on epoch=789
02/24/2022 17:10:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=794
02/24/2022 17:10:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=799
02/24/2022 17:10:22 - INFO - __main__ - Global step 1600 Train loss 0.27 Rouge-L 0.8838613014789236 on epoch=799
02/24/2022 17:10:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.27 on epoch=804
02/24/2022 17:10:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=809
02/24/2022 17:10:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=814
02/24/2022 17:10:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=819
02/24/2022 17:10:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.26 on epoch=824
02/24/2022 17:10:44 - INFO - __main__ - Global step 1650 Train loss 0.27 Rouge-L 0.8875941397654532 on epoch=824
02/24/2022 17:10:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.26 on epoch=829
02/24/2022 17:10:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=834
02/24/2022 17:10:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=839
02/24/2022 17:10:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=844
02/24/2022 17:10:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=849
02/24/2022 17:11:07 - INFO - __main__ - Global step 1700 Train loss 0.26 Rouge-L 0.884586327418128 on epoch=849
02/24/2022 17:11:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.26 on epoch=854
02/24/2022 17:11:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=859
02/24/2022 17:11:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=864
02/24/2022 17:11:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=869
02/24/2022 17:11:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=874
02/24/2022 17:11:29 - INFO - __main__ - Global step 1750 Train loss 0.25 Rouge-L 0.886397381401767 on epoch=874
02/24/2022 17:11:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=879
02/24/2022 17:11:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.25 on epoch=884
02/24/2022 17:11:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=889
02/24/2022 17:11:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=894
02/24/2022 17:11:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.25 on epoch=899
02/24/2022 17:11:52 - INFO - __main__ - Global step 1800 Train loss 0.25 Rouge-L 0.8809821306245178 on epoch=899
02/24/2022 17:11:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=904
02/24/2022 17:11:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=909
02/24/2022 17:11:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=914
02/24/2022 17:12:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=919
02/24/2022 17:12:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.25 on epoch=924
02/24/2022 17:12:14 - INFO - __main__ - Global step 1850 Train loss 0.24 Rouge-L 0.885261283860401 on epoch=924
02/24/2022 17:12:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=929
02/24/2022 17:12:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=934
02/24/2022 17:12:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.25 on epoch=939
02/24/2022 17:12:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=944
02/24/2022 17:12:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.23 on epoch=949
02/24/2022 17:12:37 - INFO - __main__ - Global step 1900 Train loss 0.24 Rouge-L 0.8899621047077024 on epoch=949
02/24/2022 17:12:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=954
02/24/2022 17:12:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.24 on epoch=959
02/24/2022 17:12:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=964
02/24/2022 17:12:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=969
02/24/2022 17:12:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.24 on epoch=974
02/24/2022 17:13:00 - INFO - __main__ - Global step 1950 Train loss 0.24 Rouge-L 0.8896643086452538 on epoch=974
02/24/2022 17:13:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.24 on epoch=979
02/24/2022 17:13:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=984
02/24/2022 17:13:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.24 on epoch=989
02/24/2022 17:13:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.23 on epoch=994
02/24/2022 17:13:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=999
02/24/2022 17:13:13 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 17:13:13 - INFO - __main__ - Printing 3 examples
02/24/2022 17:13:13 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
02/24/2022 17:13:13 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
02/24/2022 17:13:13 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
02/24/2022 17:13:13 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
02/24/2022 17:13:13 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
02/24/2022 17:13:13 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
02/24/2022 17:13:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/24/2022 17:13:13 - INFO - __main__ - Tokenizing Output ...
02/24/2022 17:13:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/24/2022 17:13:13 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 17:13:13 - INFO - __main__ - Printing 3 examples
02/24/2022 17:13:13 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
02/24/2022 17:13:13 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
02/24/2022 17:13:13 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
02/24/2022 17:13:13 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
02/24/2022 17:13:13 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
02/24/2022 17:13:13 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
02/24/2022 17:13:13 - INFO - __main__ - Tokenizing Input ...
02/24/2022 17:13:13 - INFO - __main__ - Tokenizing Output ...
02/24/2022 17:13:13 - INFO - __main__ - Loaded 32 examples from dev data
02/24/2022 17:13:23 - INFO - __main__ - Global step 2000 Train loss 0.24 Rouge-L 0.8865222506252675 on epoch=999
02/24/2022 17:13:23 - INFO - __main__ - save last model!
02/24/2022 17:13:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/24/2022 17:13:23 - INFO - __main__ - Start tokenizing ... 5000 instances
02/24/2022 17:13:23 - INFO - __main__ - Printing 3 examples
02/24/2022 17:13:23 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
02/24/2022 17:13:23 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
02/24/2022 17:13:23 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
02/24/2022 17:13:23 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
02/24/2022 17:13:23 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
02/24/2022 17:13:23 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
02/24/2022 17:13:23 - INFO - __main__ - Tokenizing Input ...
02/24/2022 17:13:25 - INFO - __main__ - Tokenizing Output ...
02/24/2022 17:13:26 - INFO - __main__ - load prompt embedding from ckpt
02/24/2022 17:13:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/24/2022 17:13:26 - INFO - __main__ - Starting training!
02/24/2022 17:13:31 - INFO - __main__ - Loaded 5000 examples from test data
02/24/2022 17:42:41 - INFO - __main__ - Saved prediction in models/T5-large-reptile-random-3e-5-2-5000-5e-1-10/singletask-wiki_split/wiki_split_32_100_0.2_8_predictions.txt
02/24/2022 17:42:46 - INFO - __main__ - Rouge-L on test data: 0.8872
02/24/2022 17:42:47 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.2, bsz=8, dev_performance=0.8931181934376794, test_performance=0.8871540578788571
02/24/2022 17:42:47 - INFO - __main__ - Running ... prefix=wiki_split_32_13, lr=0.5, bsz=8 ...
02/24/2022 17:42:48 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 17:42:48 - INFO - __main__ - Printing 3 examples
02/24/2022 17:42:48 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
02/24/2022 17:42:48 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
02/24/2022 17:42:48 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
02/24/2022 17:42:48 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
02/24/2022 17:42:48 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
02/24/2022 17:42:48 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
02/24/2022 17:42:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/24/2022 17:42:48 - INFO - __main__ - Tokenizing Output ...
02/24/2022 17:42:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/24/2022 17:42:48 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 17:42:48 - INFO - __main__ - Printing 3 examples
02/24/2022 17:42:48 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
02/24/2022 17:42:48 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
02/24/2022 17:42:48 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
02/24/2022 17:42:48 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
02/24/2022 17:42:48 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
02/24/2022 17:42:48 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
02/24/2022 17:42:48 - INFO - __main__ - Tokenizing Input ...
02/24/2022 17:42:48 - INFO - __main__ - Tokenizing Output ...
02/24/2022 17:42:48 - INFO - __main__ - Loaded 32 examples from dev data
02/24/2022 17:43:02 - INFO - __main__ - load prompt embedding from ckpt
02/24/2022 17:43:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/24/2022 17:43:02 - INFO - __main__ - Starting training!
02/24/2022 17:43:05 - INFO - __main__ - Step 10 Global step 10 Train loss 0.87 on epoch=4
02/24/2022 17:43:08 - INFO - __main__ - Step 20 Global step 20 Train loss 0.78 on epoch=9
02/24/2022 17:43:10 - INFO - __main__ - Step 30 Global step 30 Train loss 0.71 on epoch=14
02/24/2022 17:43:12 - INFO - __main__ - Step 40 Global step 40 Train loss 0.67 on epoch=19
02/24/2022 17:43:14 - INFO - __main__ - Step 50 Global step 50 Train loss 0.66 on epoch=24
02/24/2022 17:43:23 - INFO - __main__ - Global step 50 Train loss 0.74 Rouge-L 0.646046815212021 on epoch=24
02/24/2022 17:43:23 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.646046815212021 on epoch=24, global_step=50
02/24/2022 17:43:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.63 on epoch=29
02/24/2022 17:43:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=34
02/24/2022 17:43:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=39
02/24/2022 17:43:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=44
02/24/2022 17:43:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=49
02/24/2022 17:43:45 - INFO - __main__ - Global step 100 Train loss 0.56 Rouge-L 0.8397566337544834 on epoch=49
02/24/2022 17:43:45 - INFO - __main__ - Saving model with best Rouge-L: 0.646046815212021 -> 0.8397566337544834 on epoch=49, global_step=100
02/24/2022 17:43:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=54
02/24/2022 17:43:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=59
02/24/2022 17:43:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=64
02/24/2022 17:43:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=69
02/24/2022 17:43:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=74
02/24/2022 17:44:08 - INFO - __main__ - Global step 150 Train loss 0.47 Rouge-L 0.8671239529462781 on epoch=74
02/24/2022 17:44:08 - INFO - __main__ - Saving model with best Rouge-L: 0.8397566337544834 -> 0.8671239529462781 on epoch=74, global_step=150
02/24/2022 17:44:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=79
02/24/2022 17:44:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=84
02/24/2022 17:44:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
02/24/2022 17:44:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=94
02/24/2022 17:44:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=99
02/24/2022 17:44:31 - INFO - __main__ - Global step 200 Train loss 0.41 Rouge-L 0.8656939762542779 on epoch=99
02/24/2022 17:44:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=104
02/24/2022 17:44:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=109
02/24/2022 17:44:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=114
02/24/2022 17:44:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=119
02/24/2022 17:44:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=124
02/24/2022 17:44:54 - INFO - __main__ - Global step 250 Train loss 0.37 Rouge-L 0.8714832927901737 on epoch=124
02/24/2022 17:44:54 - INFO - __main__ - Saving model with best Rouge-L: 0.8671239529462781 -> 0.8714832927901737 on epoch=124, global_step=250
02/24/2022 17:44:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.34 on epoch=129
02/24/2022 17:44:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.34 on epoch=134
02/24/2022 17:45:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=139
02/24/2022 17:45:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.33 on epoch=144
02/24/2022 17:45:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.33 on epoch=149
02/24/2022 17:45:17 - INFO - __main__ - Global step 300 Train loss 0.33 Rouge-L 0.8691829497393588 on epoch=149
02/24/2022 17:45:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=154
02/24/2022 17:45:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.32 on epoch=159
02/24/2022 17:45:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=164
02/24/2022 17:45:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.31 on epoch=169
02/24/2022 17:45:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
02/24/2022 17:45:39 - INFO - __main__ - Global step 350 Train loss 0.31 Rouge-L 0.877279687794855 on epoch=174
02/24/2022 17:45:40 - INFO - __main__ - Saving model with best Rouge-L: 0.8714832927901737 -> 0.877279687794855 on epoch=174, global_step=350
02/24/2022 17:45:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=179
02/24/2022 17:45:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.29 on epoch=184
02/24/2022 17:45:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=189
02/24/2022 17:45:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=194
02/24/2022 17:45:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=199
02/24/2022 17:46:02 - INFO - __main__ - Global step 400 Train loss 0.29 Rouge-L 0.8809812487146813 on epoch=199
02/24/2022 17:46:02 - INFO - __main__ - Saving model with best Rouge-L: 0.877279687794855 -> 0.8809812487146813 on epoch=199, global_step=400
02/24/2022 17:46:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=204
02/24/2022 17:46:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=209
02/24/2022 17:46:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=214
02/24/2022 17:46:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
02/24/2022 17:46:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
02/24/2022 17:46:25 - INFO - __main__ - Global step 450 Train loss 0.27 Rouge-L 0.8728144126419952 on epoch=224
02/24/2022 17:46:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
02/24/2022 17:46:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=234
02/24/2022 17:46:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=239
02/24/2022 17:46:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=244
02/24/2022 17:46:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
02/24/2022 17:46:48 - INFO - __main__ - Global step 500 Train loss 0.25 Rouge-L 0.8781763027424142 on epoch=249
02/24/2022 17:46:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
02/24/2022 17:46:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=259
02/24/2022 17:46:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
02/24/2022 17:46:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=269
02/24/2022 17:46:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=274
02/24/2022 17:47:11 - INFO - __main__ - Global step 550 Train loss 0.23 Rouge-L 0.8769685947051762 on epoch=274
02/24/2022 17:47:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=279
02/24/2022 17:47:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
02/24/2022 17:47:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=289
02/24/2022 17:47:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=294
02/24/2022 17:47:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.22 on epoch=299
02/24/2022 17:47:33 - INFO - __main__ - Global step 600 Train loss 0.22 Rouge-L 0.8781394871685546 on epoch=299
02/24/2022 17:47:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=304
02/24/2022 17:47:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=309
02/24/2022 17:47:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
02/24/2022 17:47:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=319
02/24/2022 17:47:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=324
02/24/2022 17:47:56 - INFO - __main__ - Global step 650 Train loss 0.21 Rouge-L 0.8735560352469149 on epoch=324
02/24/2022 17:47:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=329
02/24/2022 17:48:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=334
02/24/2022 17:48:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=339
02/24/2022 17:48:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=344
02/24/2022 17:48:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=349
02/24/2022 17:48:19 - INFO - __main__ - Global step 700 Train loss 0.19 Rouge-L 0.8746162862663962 on epoch=349
02/24/2022 17:48:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=354
02/24/2022 17:48:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=359
02/24/2022 17:48:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=364
02/24/2022 17:48:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=369
02/24/2022 17:48:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.18 on epoch=374
02/24/2022 17:48:42 - INFO - __main__ - Global step 750 Train loss 0.18 Rouge-L 0.8719980755504573 on epoch=374
02/24/2022 17:48:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=379
02/24/2022 17:48:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=384
02/24/2022 17:48:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=389
02/24/2022 17:48:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=394
02/24/2022 17:48:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.16 on epoch=399
02/24/2022 17:49:04 - INFO - __main__ - Global step 800 Train loss 0.17 Rouge-L 0.8771243078608473 on epoch=399
02/24/2022 17:49:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=404
02/24/2022 17:49:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.17 on epoch=409
02/24/2022 17:49:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=414
02/24/2022 17:49:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=419
02/24/2022 17:49:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.16 on epoch=424
02/24/2022 17:49:27 - INFO - __main__ - Global step 850 Train loss 0.16 Rouge-L 0.8665007335365763 on epoch=424
02/24/2022 17:49:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.15 on epoch=429
02/24/2022 17:49:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=434
02/24/2022 17:49:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=439
02/24/2022 17:49:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=444
02/24/2022 17:49:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=449
02/24/2022 17:49:50 - INFO - __main__ - Global step 900 Train loss 0.15 Rouge-L 0.8709837883011724 on epoch=449
02/24/2022 17:49:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=454
02/24/2022 17:49:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.15 on epoch=459
02/24/2022 17:49:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=464
02/24/2022 17:49:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=469
02/24/2022 17:50:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=474
02/24/2022 17:50:12 - INFO - __main__ - Global step 950 Train loss 0.15 Rouge-L 0.873330847501174 on epoch=474
02/24/2022 17:50:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.14 on epoch=479
02/24/2022 17:50:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=484
02/24/2022 17:50:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=489
02/24/2022 17:50:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=494
02/24/2022 17:50:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=499
02/24/2022 17:50:35 - INFO - __main__ - Global step 1000 Train loss 0.14 Rouge-L 0.8694526976065761 on epoch=499
02/24/2022 17:50:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=504
02/24/2022 17:50:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=509
02/24/2022 17:50:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.13 on epoch=514
02/24/2022 17:50:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=519
02/24/2022 17:50:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=524
02/24/2022 17:50:58 - INFO - __main__ - Global step 1050 Train loss 0.13 Rouge-L 0.8707334538719902 on epoch=524
02/24/2022 17:51:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=529
02/24/2022 17:51:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=534
02/24/2022 17:51:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=539
02/24/2022 17:51:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=544
02/24/2022 17:51:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=549
02/24/2022 17:51:21 - INFO - __main__ - Global step 1100 Train loss 0.13 Rouge-L 0.8693036472457428 on epoch=549
02/24/2022 17:51:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=554
02/24/2022 17:51:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=559
02/24/2022 17:51:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=564
02/24/2022 17:51:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=569
02/24/2022 17:51:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=574
02/24/2022 17:51:43 - INFO - __main__ - Global step 1150 Train loss 0.14 Rouge-L 0.8585731443323339 on epoch=574
02/24/2022 17:51:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.16 on epoch=579
02/24/2022 17:51:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=584
02/24/2022 17:51:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=589
02/24/2022 17:51:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=594
02/24/2022 17:51:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=599
02/24/2022 17:52:06 - INFO - __main__ - Global step 1200 Train loss 0.13 Rouge-L 0.8663648523326133 on epoch=599
02/24/2022 17:52:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=604
02/24/2022 17:52:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=609
02/24/2022 17:52:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=614
02/24/2022 17:52:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=619
02/24/2022 17:52:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=624
02/24/2022 17:52:29 - INFO - __main__ - Global step 1250 Train loss 0.11 Rouge-L 0.8723989755489883 on epoch=624
02/24/2022 17:52:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=629
02/24/2022 17:52:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.10 on epoch=634
02/24/2022 17:52:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=639
02/24/2022 17:52:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=644
02/24/2022 17:52:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=649
02/24/2022 17:52:52 - INFO - __main__ - Global step 1300 Train loss 0.10 Rouge-L 0.8731453593492158 on epoch=649
02/24/2022 17:52:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=654
02/24/2022 17:52:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=659
02/24/2022 17:52:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=664
02/24/2022 17:53:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=669
02/24/2022 17:53:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=674
02/24/2022 17:53:15 - INFO - __main__ - Global step 1350 Train loss 0.10 Rouge-L 0.8694087563247925 on epoch=674
02/24/2022 17:53:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=679
02/24/2022 17:53:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=684
02/24/2022 17:53:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=689
02/24/2022 17:53:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=694
02/24/2022 17:53:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.09 on epoch=699
02/24/2022 17:53:38 - INFO - __main__ - Global step 1400 Train loss 0.10 Rouge-L 0.8711067749480725 on epoch=699
02/24/2022 17:53:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=704
02/24/2022 17:53:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=709
02/24/2022 17:53:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=714
02/24/2022 17:53:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=719
02/24/2022 17:53:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=724
02/24/2022 17:54:01 - INFO - __main__ - Global step 1450 Train loss 0.09 Rouge-L 0.8738456843141685 on epoch=724
02/24/2022 17:54:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=729
02/24/2022 17:54:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=734
02/24/2022 17:54:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=739
02/24/2022 17:54:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=744
02/24/2022 17:54:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=749
02/24/2022 17:54:24 - INFO - __main__ - Global step 1500 Train loss 0.09 Rouge-L 0.8620243885014462 on epoch=749
02/24/2022 17:54:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=754
02/24/2022 17:54:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=759
02/24/2022 17:54:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=764
02/24/2022 17:54:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=769
02/24/2022 17:54:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=774
02/24/2022 17:54:46 - INFO - __main__ - Global step 1550 Train loss 0.09 Rouge-L 0.8779658247812736 on epoch=774
02/24/2022 17:54:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=779
02/24/2022 17:54:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.09 on epoch=784
02/24/2022 17:54:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=789
02/24/2022 17:54:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=794
02/24/2022 17:54:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=799
02/24/2022 17:55:09 - INFO - __main__ - Global step 1600 Train loss 0.08 Rouge-L 0.8762161651277465 on epoch=799
02/24/2022 17:55:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=804
02/24/2022 17:55:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=809
02/24/2022 17:55:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=814
02/24/2022 17:55:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=819
02/24/2022 17:55:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=824
02/24/2022 17:55:32 - INFO - __main__ - Global step 1650 Train loss 0.08 Rouge-L 0.8676866598036195 on epoch=824
02/24/2022 17:55:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=829
02/24/2022 17:55:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=834
02/24/2022 17:55:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=839
02/24/2022 17:55:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=844
02/24/2022 17:55:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=849
02/24/2022 17:55:55 - INFO - __main__ - Global step 1700 Train loss 0.08 Rouge-L 0.8637320248440489 on epoch=849
02/24/2022 17:55:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=854
02/24/2022 17:55:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=859
02/24/2022 17:56:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=864
02/24/2022 17:56:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=869
02/24/2022 17:56:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=874
02/24/2022 17:56:18 - INFO - __main__ - Global step 1750 Train loss 0.07 Rouge-L 0.867402674092332 on epoch=874
02/24/2022 17:56:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=879
02/24/2022 17:56:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=884
02/24/2022 17:56:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=889
02/24/2022 17:56:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=894
02/24/2022 17:56:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=899
02/24/2022 17:56:41 - INFO - __main__ - Global step 1800 Train loss 0.07 Rouge-L 0.8653471756982762 on epoch=899
02/24/2022 17:56:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=904
02/24/2022 17:56:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=909
02/24/2022 17:56:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=914
02/24/2022 17:56:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=919
02/24/2022 17:56:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=924
02/24/2022 17:57:03 - INFO - __main__ - Global step 1850 Train loss 0.07 Rouge-L 0.8750410716010384 on epoch=924
02/24/2022 17:57:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=929
02/24/2022 17:57:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=934
02/24/2022 17:57:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=939
02/24/2022 17:57:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=944
02/24/2022 17:57:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=949
02/24/2022 17:57:26 - INFO - __main__ - Global step 1900 Train loss 0.07 Rouge-L 0.8676240015707469 on epoch=949
02/24/2022 17:57:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=954
02/24/2022 17:57:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=959
02/24/2022 17:57:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=964
02/24/2022 17:57:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=969
02/24/2022 17:57:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=974
02/24/2022 17:57:49 - INFO - __main__ - Global step 1950 Train loss 0.06 Rouge-L 0.8692984178436303 on epoch=974
02/24/2022 17:57:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=979
02/24/2022 17:57:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=984
02/24/2022 17:57:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=989
02/24/2022 17:57:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=994
02/24/2022 17:58:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=999
02/24/2022 17:58:02 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 17:58:02 - INFO - __main__ - Printing 3 examples
02/24/2022 17:58:02 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
02/24/2022 17:58:02 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
02/24/2022 17:58:02 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
02/24/2022 17:58:02 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
02/24/2022 17:58:02 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
02/24/2022 17:58:02 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
02/24/2022 17:58:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/24/2022 17:58:02 - INFO - __main__ - Tokenizing Output ...
02/24/2022 17:58:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/24/2022 17:58:02 - INFO - __main__ - Start tokenizing ... 32 instances
02/24/2022 17:58:02 - INFO - __main__ - Printing 3 examples
02/24/2022 17:58:02 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
02/24/2022 17:58:02 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
02/24/2022 17:58:02 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
02/24/2022 17:58:02 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
02/24/2022 17:58:02 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
02/24/2022 17:58:02 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
02/24/2022 17:58:02 - INFO - __main__ - Tokenizing Input ...
02/24/2022 17:58:02 - INFO - __main__ - Tokenizing Output ...
02/24/2022 17:58:02 - INFO - __main__ - Loaded 32 examples from dev data
02/24/2022 17:58:12 - INFO - __main__ - Global step 2000 Train loss 0.07 Rouge-L 0.8728730029853742 on epoch=999
02/24/2022 17:58:12 - INFO - __main__ - save last model!
02/24/2022 17:58:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/24/2022 17:58:12 - INFO - __main__ - Start tokenizing ... 5000 instances
02/24/2022 17:58:12 - INFO - __main__ - Printing 3 examples
02/24/2022 17:58:12 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
02/24/2022 17:58:12 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
02/24/2022 17:58:12 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
02/24/2022 17:58:12 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
02/24/2022 17:58:12 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
02/24/2022 17:58:12 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
02/24/2022 17:58:12 - INFO - __main__ - Tokenizing Input ...
02/24/2022 17:58:14 - INFO - __main__ - load prompt embedding from ckpt
02/24/2022 17:58:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/24/2022 17:58:15 - INFO - __main__ - Starting training!
02/24/2022 17:58:15 - INFO - __main__ - Tokenizing Output ...
02/24/2022 17:58:20 - INFO - __main__ - Loaded 5000 examples from test data
